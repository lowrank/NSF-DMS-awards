"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1205271","Collaborative Research:  Model diagnostics in regression and Tobit regression models with measurement error","DMS","STATISTICS","09/01/2012","05/16/2013","Hira Koul","MI","Michigan State University","Continuing Grant","Gabor Szekely","08/31/2016","$185,000.00","","koul@stt.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","","$0.00","Statistical modeling for relationships between a collection of predictors and a response is often implemented by regression analysis. In the classical regression model, both predictors and response variables are assumed to be directly observable. In measurement error regression models, predictors cannot be observed directly, instead, some surrogates are observed. In Tobit regression models, the response variable is observed only when it is above some threshold.  The development of useful and optimal inference procedures in the presence of measurement errors in regression and Tobit regression models is of major concern in theoretical and applied  statistics. Despite this need, the study of goodness-of-fit and lack-of-fit tests in the measurement error regression models and Tobit regression models with/without measurement errors has lagged behind. In this project, the investigators analyze goodness-of-fit tests for the distributions of the random components of errors-in-variables and Berkson measurement error regression models, and some nonparametric estimators of regression functions in Tobit regression models with or without these measurement errors. Furthermore, the investigators develop and analyze lack-of-fit and goodness-of-fit tests in Tobit regression models with these measurement errors. The investigators make available some new, useful, and optimal inference procedures in these models with an in-depth understanding of their theoretical properties to a wide professional audience in statistics and related disciplines. This project is at the cutting edge of model checking in the presence of measurement error in predictors in regression and Tobit regression models. It advances and enriches the statistical theory and methodology, thereby helping to fill a significant void and well recognized theoretical gap that exists in statistics. <br/><br/>Measurement errors are very prevalent in the health sciences, physical sciences, economics, and the social sciences. For example, when investigating the effect of diet on breast cancer, one of the predictor variables studied for predicting breast cancer is the long-term saturated fat intake which cannot be measured precisely. Instead, the surrogate of a 24 hour diet recall for each patient is often used in this type of investigation. Similarly, the exact amount of radiation a person is exposed to when studying the effect of radiation exposure on humans is often measured with error.  In labor studies, when investigating the relationship between women's working status and their background information,  such as age, education and working experience, the effect of measurement errors is present in the education variables (such as mother's and father's education experience).  Tobit regression models, which are used in these types of studies, often suffer from the measurement error problem. Most empirical studies involving Tobit regression models tend to ignore the measurement errors, which usually leads to biased and inefficient statistical conclusions. The research focus of this project, which helps in assessing the accuracy of a regression model or of a model for the distributions of random components in the presence of measurement errors, helps to develop more accurate statistical inference for these and other similar examples."
"1149355","CAREER: A new and pragmatic framework for modeling and predicting conditional quantiles in data-sparse regions","DMS","STATISTICS","09/01/2012","08/13/2014","Huixia Wang","NC","North Carolina State University","Continuing Grant","Gabor Szekely","04/30/2015","$226,608.00","","judywang@gwu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","1045","$0.00","Quantile regression provides a valuable semiparametric tool for modeling the conditional quantiles of a response variable given predictors. However, making inference for quantile regression is challenging in data-sparse regions such as at low or high quantiles with quantile levels close to 0 or 1. In the proposed research, the Principal Investigator (PI) aims to develop theory and methodology for quantile regression in data-sparse regions, which opens up a significant new direction in quantile regression. For estimating extreme conditional quantiles of the response distribution, the PI plans to develop extrapolation methods based on a novel application of the extreme value theory. In data sparse areas, the formulation of models plays a critical role. The PI will study models with different levels of complexity, which calls for different techniques for quantifying the tail quantiles. New theory and methods for joint quantile estimation and inter-quantile shrinkage will be developed to improve statistical efficiency by sharing information across multiple quantile functions. The PI will also study tail quantile regression for dependent data, where the common understanding about the impact of dependence on statistical inference needs to be re-examined. As a result, new and efficient methods to incorporate tail dependence will be proposed.<br/><br/><br/>An important problem in many fields is the modeling and prediction of events that are rare but have significant consequences. Unexpectedly heavy rainfall, large portfolio loss, and dangerously low birth weight are some examples of rare events. For such events, scientists are particularly interested in modeling and estimating the tail quantiles of the underlying distribution rather than the central summaries such as the mean or median. The proposed methodologies will have broad and valuable applications in studies of rare events in climate sciences, risk management in finance, studies of infant birth weights, and prediction of insurance claims. The PI will integrate research and education by developing advanced topics courses, engaging graduate and undergraduate students, especially those from under-represented groups, in the project, and reaching out to the K-12 education levels by training high school teachers."
"1216197","2012 International Conference on Robust Statistics (ICORS2012)","DMS","STATISTICS","08/01/2012","03/26/2012","Huixia Wang","NC","North Carolina State University","Standard Grant","Gabor Szekely","07/31/2013","$8,000.00","","judywang@gwu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","7556","$0.00","The International Conference on Robust Statistics (ICORS) has been an annual international conference for the past 12 years.  The ICORS meeting for 2012 (ICORS2012) will be held in the University of Vermont, Burlington, Vermont from August 5-August 10, 2012. The conference will span five days with double sessions for most of the days. The lectures will include invited speakers, chosen with the help of suggestions from the steering committee; theme based invited sessions organized by the individual organizers, contributed sessions and poster sessions. The funds from NSF are primarily to support approximately 15 junior researchers and graduate students from within the United States.  The conference is cosponsored by the Minerva Foundation.<br/><br/>The aim of the ICORS conferences is to bring together researchers interested in robust statistics, data analysis and related areas. This includes theoretical and applied statisticians as well as data analysts from other fields, and includes leading experts as well as junior researchers and graduate students.  The ICORS conferences create a forum to discuss recent progress and emerging ideas in the area and encourage informal contacts and discussions among all the participants. The ICORS2012 will be the first such conference to be held in the United States. This is important since it will encourage a larger participation of researchers, and in particular young researchers, from the United States, to engage in research interaction and exchange of ideas with their colleagues from other countries. It is anticipated that the conference will continue to also have a large international representation. The ICORS2012 will contribute to the enhancement and improvement of scientific activities through presentations of latest research results, intensive discussion after presentations, and informal exchange of ideas. The meeting will help collaboration among researchers of diverse backgrounds and geographic locations. The meeting will also contribute to educational activities, as it will attract a substantial number of junior researchers and students to interact with senior researchers in the field. The conference Web site is http://www.rci.rutgers.edu/~dtyler/ICORS2012/ ."
"1209112","Statistical inference for gene regulation with genetical genomic data","DMS","STATISTICS","06/15/2012","04/04/2014","Yuehua Cui","MI","Michigan State University","Continuing Grant","Gabor Szekely","05/31/2016","$208,886.00","Ping-Shou Zhong","cuiy@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","","$0.00","Genetical genomics, the combined analysis of genetic and gene expression data, holds great promise in elucidating gene regulation and predicting gene networks associated with complex phenotypes. In this project, the investigators aim to develop novel statistical inference procedures and computational tools to understand different gene regulation mechanisms such as cis- and trans-regulation, nonlinear regulation as well as joint regulation of multiple genes from a systems biology perspective. Both parametric and nonparametric inference procedures which incorporate pathway or gene set information are developed to identify novel regulators and to gain novel insights into gene regulation underlying developmental diversity. Statistical tests in a high-dimensional nonparametric regression, and penalized and empirical likelihood methods in semi-parametric models are proposed while their asymptotic distributions are evaluated. Dense SNP genotype and next-generation RNA-Seq data are combined under the proposed framework while accounting for the discrete nature of the sequence mapping reads. <br/><br/>The genetic bases of complex traits often involve multiple inherited genetic factors that function in a network basis. Gene regulation is thought to play a pivotal role in determining trait variations by promoting or reducing the expression of functional genes directly or indirectly related to the phenotype. Drawing on methodology from statistics, genetics and systems biology, this project contributes to knowledge about gene regulation by providing biologically meaningful statistical infrastructures: developing novel statistical devices to handle various complexities in genetic and gene expression data (e.g., RNA-seq), and incorporating pathway information to enhance model biological relevance. The developed models have particular power to attack long-standing genetic questions regarding the interplay between gene expression and regulation. The success will greatly advance the discovery of novel genes, regulators and pathways to facilitate identification of drug targets to enhance public health, and help animal and plant breeders to improve trait quality. Computational tools are made available for public use free of charge. The research is integrated into education to train new generations in statistical genetics, and is widely disseminated through publications, presentations, online software and collaborations with biologists.<br/>"
"1149415","CAREER: New Topics in Functional Data Analysis","DMS","STATISTICS","06/01/2012","12/14/2011","Yehua Li","GA","University of Georgia Research Foundation Inc","Continuing Grant","Gabor Szekely","02/28/2013","$75,674.00","","yehuali@iastate.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","1045","$0.00","Functional data are random vectors in a functional space, which are usually observed on discrete points and measured with error. Functional data are also deeply connected to other types of correlated data, such as spatial data, since they can both be modeled as stochastic processes. In the proposed research, the investigator will broaden the applications of functional data analysis by proposing a new functional data approach to model spatio-temporal point pattern data from disease surveillance applications, where the spatio-temporal random effects are modeled as latent functional processes. Motivated by scientific problems in colon carcinogenesis experiments and hypertension studies, the investigator proposes new dimension reduction methods, which are widely applicable to semiparametric regression problems with functional predictors. The investigator proposes new estimation procedures based on spline approximation and roughness penalties, and will also investigate the model selection and inference problems related to these methods. The investigator also proposes to model longitudinal clinical trial data by the functional analysis of covariance models, where different treatment effects are represented by nonparametric functions in time. The proposed nonparametric hypothesis test can be used to detect the treatment effects. Specifically, the investigator will study the effect of the within-subject correlation on the power of the test.<br/><br/>The proposed functional data approach to disease surveillance data will help to model the relationship between disease occurrence and some environmental variables (such as pollution level), estimate the time trend in the disease rate, and predict the unknown risk factors represented by latent random effects. The results will help disease control agencies and local officials to gain better understanding of the disease risk and develop better public health policies, such as emission or water quality control policies. The proposed dimension reduction methods provide the much needed statistical tools in the semiparametric regression problems in colon carcinogenesis and hypertension studies. The proposed nonparametric hypothesis testing procedure for functional analysis of covariance models answers a fundamental question in clinical trials, which is to compare the effectiveness of different treatments. The investigator will provide free and user-friendly software to scientific researchers and incorporate his research activity with graduate education. To further disseminate the research results and motivate new ideas, the investigator will develop a new course on functional data analysis and organize a research workshop."
"1217801","Design and Analysis of Experiments","DMS","STATISTICS","07/01/2012","03/01/2012","John Stufken","GA","University of Georgia Research Foundation Inc","Standard Grant","Gabor Szekely","06/30/2013","$20,000.00","Abhyuday Mandal","jstufken@gmu.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","7556","$0.00","This award provides participant support costs for the conference Design and Analysis of Experiments 2012 (DAE 2012) at the University of Georgia from October 17, 2012 to October 20, 2012. The conference features leading researchers of international repute, promising new researchers, and graduate students actively pursuing research in the area of design and analysis of experiments. Participants represent academia, industry and government, and the conference offers a forum for interaction, discussion, and exchange of ideas on novel research in this area. Special sessions and activities to facilitate mentoring of junior researchers are an important focus of the conference. Themes of DAE 2012 include computer experiments, designs in marketing and business, designs in the pharmaceutical industry, algorithms for finding efficient designs, fractional factorial designs, designs for generalized linear models, designs for nonlinear models, designs for correlated data, and design robustness. The award provides partial support for approximately 40 participants.<br/><br/>Design and analysis of experiments are indispensable in the scientific process. Experiments are an integral part of the discovery process in virtually all scientific disciplines and in engineering and manufacturing. Rapid developments have taken place in research on design and analysis of experiments, both in more traditional areas of application and, especially, in entirely new areas of application. This conference, DAE 2012, helps the advancement of cutting edge research in design and analysis of experiments by bringing together senior and junior researchers from academia, industry and government. DAE 2012 prepares future researchers in design and analysis of experiments through extensive mentoring activities that stimulate interactions between senior and junior participants. The conference makes significant efforts to broaden the participation of under-represented groups. Indirectly, by virtue of the fact that design and analysis of experiments play critical roles in the process of discovery and improvement in science, engineering and manufacturing, activities at the conference have an enormous impact on society. The conference website address is http://www.stat.uga.edu/dae2012."
"1208238","Collaborative Research: Multidimensional Curve Estimation for Diffusion MRI","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","08/01/2012","05/06/2014","Lyudmila Sakhanenko","MI","Michigan State University","Standard Grant","Gabor Szekely","07/31/2016","$115,954.00","","sakhanen@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1253, 1269","1515","$0.00","Integral curves are natural models for a variety of biological phenomena, from neuron fibers in brain imaging data to jet streams in atmospheric data. Traditionally they have been modeled as solutions to differential equations defined on fields of direction vectors that are observed with noise in a 3D domain. But advances in imaging technology now provide much more complex directional information--functions defined on the 3D sphere-at each location in the domain. Integral curves traced from this enhanced directional data have the potential to dramatically increase our understanding of biological phenomena such as brain connectivity, but the statistical properties of integral curve estimators for this cutting-edge data are not well understood. Therefore in this project the investigators will provide a solid theoretical foundation for integral curve estimation in 3D fields of complex directional data and apply it to large corpuses of real data sets from ongoing scientific studies. The primary plan will be to model directional data locally using high-order supersymmetric tensors, and pose integral curve estimation in terms of ODEs defined on the field of their pseudo-eigenvectors. The investigators will show that the proposed integral curve estimators enjoy optimal convergence rates in a minimax sense, and prove that balloon estimators of the pseudo-eigenvector fields will lead to improved convergence. Then integral curve estimators will be linked to accompanying random processes to allow construction of uniform confidence bands around point estimates for curves; and adaptive estimation of these confidence bands will be explored to make them practically useful. The investigators will then study whether estimation may be improved further by selecting arbitrary 3D measurement locations, possibly using enhanced imaging techniques. Finally, a test for branching of integral curves will be constructed, for example at locations where axon fibers diverge or cross.<br/><br/><br/>The proposed work has the potential to dramatically increase the usefulness of diffusion magnetic resonance imaging (MRI) data, a technology with tremendous potential to probe the ""wiring diagram"" of the brain-- its connectivity-- in living people. Currently, brain connectivity measurements are widely regarded as brittle, complicated, and difficult to validate. For each individual receiving a diffusion MRI scan, the investigators will estimate curves describing the trajectories of axon fibers, the electrical ""wires"" of the brain. These fibers connect brain regions into distributed networks that give rise to thought; the evolution of this brain wiring in response to normal development, gene expression, aging, disease, drugs, and environmental factors is of primary interest to a broad swath of neuroscience. Simply providing scientific end-users with a sense of whether or not they should believe the estimated fiber trajectories provided to them by computer programs will greatly enhance their ability to make confident decisions about relations between such trajectories and other scientific data. In addition, the proposed methodology is also relevant in meteorology. There, isolines, fronts, jetstreams, and pressure troughs in weather data can be modeled by similar curve trajectories that can be used to enhance existing weather maps. Finally, this proposal has an exciting educational impact. The investigators, a statistician and a computer scientist with neuroscience training, envision building an interdisciplinary team of promising young researchers in statistics and neuroimaging who gain exposure to both the mathematical and neuroscience aspects of curve estimation through joined group meetings, graduate courses, and web resources related to theory and applications. This unique cross-pollination will prepare the trainees to contribute to the broadly interdisciplinary research teams that are ascendant in the sciences."
"1205546","A complete sufficient dimension folding theory with novel methods","DMS","STATISTICS","08/01/2012","07/20/2012","Xiangrong Yin","GA","University of Georgia Research Foundation Inc","Standard Grant","Gabor Szekely","07/31/2015","$110,000.00","","yinxiangrong@uky.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","","$0.00","This proposal is aimed at developing a general formulation and the related methods for sufficient dimension folding where predictors are matrix-/array- valued, and where a specific functional (or parameter) of the conditional distribution is of interest. The past two decades have seen vigorous development of the sufficient dimension reduction methods for vector-valued predictors, and have accrued a striking record of their successful applications. However, many data are matrix-/array-valued, sufficient dimension reduction for vector-valued predictors applying to such data will lose its sufficiency and structure, resulting difficulties in interpretation, and to a large extent these methods treat the conditional distribution as the object of interest, without discriminating between parameter of interest and nuisance parameter. The investigator proposes a new paradigm for sufficient dimension folding for matrix-/array-valued predictors that focuses on a functional of the conditional distribution, which can be any one in a very wide class that covers most of applications. In addition, the investigator proposes to develop a coherent collection of associated techniques for estimation, computation, and asymptotic inference. <br/> <br/>Recently, high throughput technologies that produce massive amount of complex and high-dimensional data are increasingly prevalent in such diverse areas as  business, government administration, environmental studies, machine learning, and bioinformatics. These provide considerable momentum in the Statistics community to develop new theories and methodologies, that are capable of discovering critical evidence from high-dimensional, complex structural and massive data. Sufficient Dimension Folding is a new area of statistical research that arose amidst, and has been propelled by, these new demands. The investigator proposes to formulate the theories and methodologies of sufficient dimension folding so that they can be specifically tailored to target to be estimated. This new paradigm not only synthesizes, broadens, and deepens the recent advances in sufficient dimension folding, but brings the understanding of sufficient dimension folding on a par with classical statistical inference theory, by following the tradition of sufficiency, efficiency, information, parameter of interests, and nuisance parameters, which are the key ideas that had helped to propel classical inference to its maturity."
"1205276","Collaborative Research: Model Diagnostics in Regression and Tobit Regression Models with Measurement Errors","DMS","STATISTICS, EPSCoR Co-Funding","09/01/2012","08/31/2012","Weixing Song","KS","Kansas State University","Standard Grant","Gabor Szekely","08/31/2015","$140,000.00","","weixing@ksu.edu","1601 VATTIER STREET","MANHATTAN","KS","665062504","7855326804","MPS","1269, 9150","9150","$0.00","Statistical modeling for relationships between a collection of predictors and a response is often implemented by regression analysis. In the classical regression model, both predictors and response variables are assumed to be directly observable. In measurement error regression models, predictors cannot be observed directly, instead, some surrogates are observed. In Tobit regression models, the response variable is observed only when it is above some threshold.  The development of useful and optimal inference procedures in the presence of measurement errors in regression and Tobit regression models is of major concern in theoretical and applied  statistics. Despite this need, the study of goodness-of-fit and lack-of-fit tests in the measurement error regression models and Tobit regression models with/without measurement errors has lagged behind. In this project, the investigators analyze goodness-of-fit tests for the distributions of the random components of errors-in-variables and Berkson measurement error regression models, and some nonparametric estimators of regression functions in Tobit regression models with or without these measurement errors. Furthermore, the investigators develop and analyze lack-of-fit and goodness-of-fit tests in Tobit regression models with these measurement errors. The investigators make available some new, useful, and optimal inference procedures in these models with an in-depth understanding of their theoretical properties to a wide professional audience in statistics and related disciplines. This project is at the cutting edge of model checking in the presence of measurement error in predictors in regression and Tobit regression models. It advances and enriches the statistical theory and methodology, thereby helping to fill a significant void and well recognized theoretical gap that exists in statistics. <br/><br/>Measurement errors are very prevalent in the health sciences, physical sciences, economics, and the social sciences. For example, when investigating the effect of diet on breast cancer, one of the predictor variables studied for predicting breast cancer is the long-term saturated fat intake which cannot be measured precisely. Instead, the surrogate of a 24 hour diet recall for each patient is often used in this type of investigation. Similarly, the exact amount of radiation a person is exposed to when studying the effect of radiation exposure on humans is often measured with error.  In labor studies, when investigating the relationship between women's working status and their background information,  such as age, education and working experience, the effect of measurement errors is present in the education variables (such as mother's and father's education experience).  Tobit regression models, which are used in these types of studies, often suffer from the measurement error problem. Most empirical studies involving Tobit regression models tend to ignore the measurement errors, which usually leads to biased and inefficient statistical conclusions. The research focus of this project, which helps in assessing the accuracy of a regression model or of a model for the distributions of random components in the presence of measurement errors, helps to develop more accurate statistical inference for these and other similar examples."
"1208847","Collaborative Research: Axially symmetric processes and intrinsic random functions on the sphere","DMS","STATISTICS, EPSCoR Co-Funding","09/15/2012","09/08/2012","Haimeng Zhang","MS","Mississippi State University","Standard Grant","Gabor Szekely","01/31/2014","$64,718.00","","h_zhang5@uncg.edu","245 BARR AVE","MISSISSIPPI STATE","MS","39762","6623257404","MPS","1269, 9150","9150","$0.00","In spatial statistics, a wide variety of methods and models have been developed in Euclidean spaces. Additional theory and methods are needed for analyzing processes and phenomena on the sphere, many of which are of utmost importance in the geophysical sciences. Data from global networks of in situ and satellite sensors, for instance, are used to monitor a wide array of important climatological variables such as temperature and precipitation. The goal of this project is to study random processes on the sphere beyond the usual homogeneity assumption using two approaches. The first approach is to consider axially symmetry on the sphere, where the random process exhibits longitudinal symmetry rather than being rotation invariant on the entire sphere.  The second approach of this research extends the intrinsic random functions and generalized covariance functions to processes on the sphere. The notion of intrinsic random functions has been developed in order to handle a process with an unknown and possibly non-constant mean function while preserving a linkage to stationarity. The investigators plan to achieve a fundamental understanding of the covariance structures of the non-homogenous processes on the sphere and to develop associated estimation procedures. In addition, these approaches are extended to more sophisticated situations including multivariate random processes and spatio-temporal processes on the sphere. All of these methods and models are applied to global-scale temperature data from surface and satellite sensors.<br/><br/>The models and methods developed here provide new tools for improving our understanding of global-scale phenomena in general and multi- decadal temperature variations in particular.  The motivation of this project comes from a desire to understand the statistical characteristics of global-scale temperature variations during the instrumental (1880s onward) and satellite (1979 onward) periods. As a result, two important and widely used data sets are analyzed here:(1) tropospheric temperature data from National Oceanic and Atmospheric Administration satellite-based Microwave Sounding Unit and (2) surface-based instrumental data from the Hadley Centre in the United Kingdom and Climatic Research Unit at the University of East Anglia. The geostatistical analysis of these data sets on the sphere provides profound information on the state of our changing planetary environment. The project also establishes and encourages research and education collaborations between Indiana University and Mississippi State University."
"1203076","Travel Support for the 11th ISBA World Meeting on Bayesian Statistics","DMS","STATISTICS","01/01/2012","12/08/2011","Vanja Dukic","CO","University of Colorado at Boulder","Standard Grant","Gabor Szekely","12/31/2012","$15,000.00","","Vanja.Dukic@Colorado.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1269","7556","$0.00","In this application, the PI requests support for the World Meeting of the International Society for Bayesian Analysis (ISBA), to be held in Kyoto, Japan, June 25-29, 2012.  A central theme of the conference will be Bayesian modeling, Bayesian theory, and applications. The applications will range from biology to finance to technology, with specific focus on challenging large-scale problems. The funds sought are to support the travel expenses of junior investigators, i.e., persons pursuing a PhD or DrPH in statistics, biostatistics, applied mathematics, or a closely related field, or who have received such a degree within the five years preceding the conference. Such investigators are often doing research that is among the most novel, interesting, and important for international dissemination, yet lack the travel funds necessary to attend such a conference, since they have not yet established a publication track record sufficient to attract external travel and other funding for their work. Refereed versions of the conference's best papers (both invited and contributed) will be published in Bayesian Analysis, the official journal of ISBA (available online at ba.stat.cmu.edu). Further disseminatation of conference-related  materials will be done as widely as possible, by posting some of the presentations on the ISBA web site at www.bayesian.org.<br/><br/>Statisticians play an indispensable role in the analysis of biomedical, environmental, financial, and public health data, from the study design stage all the way through to final analysis and report-writing. Statisticians also serve on a myriad of scientific review and advisory panels, as well as provide statistical training and consulting to substantive area researchers. Finally, the development of new statistical methodology for interpretation of data from clinical, observational, and laboratory studies is a key area of statistical endeavor.  As a result, scholarly conferences where new ideas can be exchanged are important for statistical science to move forward.  International meetings -- like the World Meeting of the International Society for Bayesian Analysis (ISBA) -- while of course rarer due to their size and expense, are particularly beneficial since they permit exposure to ideas and colleagues the attendee might not ordinary see or even read about in the familiar journals of one's own country. The benefit of and need for attendance at such meetings by junior statistical researchers is particularly great, since they contribute greatly to their professional development and help ``level the playing field'' with more established senior investigators. The anticipated benefits from the conference include promoting the continued development of statistical theory and applications in engineering, finance, technology, environmental science, biomedicine and other areas, and exploring the interplay of classical and Bayesian statistical methods in the context of specific areas of research, with particular emphasis on large-scale computation. Disseminatation of conference-related  materials will be done as widely as possible, by publishing in ""Bayesian Analysis"", the official journal of ISBA available online at ba.stat.cmu.edu, and posting some of the presentations on the ISBA web site at www.bayesian.org."
"1209118","Collaborative Research: Statistical Modeling and Inference for High-dimensional Multi-Subject Neuroimaging Data","DMS","ADVANCES IN BIO INFORMATICS, STATISTICS, Cross-BIO Activities, MSPA-INTERDISCIPLINARY, Robust Intelligence","09/01/2012","08/30/2012","Tingting Zhang","VA","University of Virginia Main Campus","Standard Grant","Gabor Szekely","08/31/2015","$101,600.00","James Coan","TIZ67@pitt.edu","1001 EMMET ST N","CHARLOTTESVILLE","VA","229034833","4349244270","MPS","1165, 1269, 7275, 7454, 7495","1165, 8007","$0.00","This project consists of two components, each motivated by the inference problem for functional magnetic resonance imaging (fMRI) data. In the first part, within the framework of generalized functional linear model (GFLM), a flexible semi-parametric model for neural hemodynamic response in the form of slope functions is introduced. To accommodate the variation of brain activity across different regions, stimulus types, and subjects, the new approach assumes the slope functions share the same but unknown functional shape for a given region and stimulus, while having subject-specific height, time to peak, and width.  Several fast algorithms based on B-spline smoothing are proposed to estimate the model parameters for whole-brain analysis. The second part of the research focuses on building a novel Bayesian variable selection framework to study the relationship between individual traits and brain activity. The spline estimates of the brain hemodynamic responses from the first part are taken as predictors in a regression model where the response is the individual traits. Two types of priors are introduced jointly to achieve simultaneous variable selection and clustering.<br/><br/>FMRI is one of the most effective neuroimaging technologies for understanding brain activity. In recent years, fMRI data collected from complex studies with multiple subjects have been widely used in psychological and medical research. This project will provide tools for modeling, analysis and computation for this type of fMRI data. Project findings will advance basic understanding of the inter-relations between nature and nurture in shaping individual differences in brain function and behavior, and suggest new directions for interdisciplinary research that combines statistics, neuroscience and psychology. The open source R/Matlab software developed from the research will provide valuable data analysis and educational tools for the scientific community."
"1209164","Adaptive Design Based upon Covariate Information: New Designs and Their Properties","DMS","STATISTICS","07/01/2012","06/28/2012","Feifang Hu","VA","University of Virginia Main Campus","Standard Grant","Gabor Szekely","05/31/2014","$110,000.00","","feifang@gwu.edu","1001 EMMET ST N","CHARLOTTESVILLE","VA","229034833","4349244270","MPS","1269","","$0.00","Covariate information is usually available and often plays a critical role in a clinical study.  The stratified permuted block design and the classical covariate-adaptive designs have been widely employed to balance important covariates in clinical trials. Both designs have some serious drawbacks. In addition, there is no theoretical justification of the covariate-adaptive designs in the literature. In this project, two new families of adaptive designs are proposed and their properties are studied. The first family of designs overcomes the drawbacks of  the stratified permuted block design and the classical covariate-adaptive design, and hence provides better balance. The second family of designs is proposed to detect the interaction between treatment and covariate more efficiently. Also the investigator introduces a new technique (called ""drift conditions"") to study the asymptotic properties of covariate-adaptive designs. This project will produce new sequential tools for solving many practical problems. The proposed methods will be applied to some specific applications.<br/><br/>The objective of this project is to develop new methods for clinical trials based upon covariate information.  With today's advanced technology, it becomes easier and easier  to collect useful covariate information in sequential experiments. For example, scientists have identified many new biomarkers that may link to certain diseases over the past several decades. Since one is now able to collect information on important biomarkers (covariate information) of each patient, it becomes more and more important to incorporate information on covariates into the design of clinical trials. The investigator will propose two new families of adaptive designs and study their properties. The first family of designs overcomes the drawbacks of the classical covariate-adaptive designs. The second family of designs is proposed to detect the interaction between treatment and covariate more efficiently. Upon completion of this project, one will be able to apply new designs in clinical trials for personalized medicine. The research project will produce some advanced statistical tools, which may be applied in many fields including drug development, medical studies, industrial experiments, economics and finance."
"1207771","Mining structured tensor data","DMS","STATISTICS","07/01/2012","04/01/2014","Xiaotong Shen","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","06/30/2016","$200,165.00","","xshen@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","Structured modeling situations, with multivariate data involving tensors, are treated using constrained likelihood approaches, as an efficient means to exploit lower-dimensional structure for high-order analysis.  In gene network analysis, for example, a constrained approach helps reveal gene-gene relations in a context of multiple graphical models. Special attention is devoted to the appropriate choice of the constraints for adaptation to a variety of structures. The general theme of the proposed project is the development of statistical methods of practical utility, both in prediction and estimation. In particular, the proposed project develops methods for (a) multiple graphical models for structure extraction, and for (b) high-order analysis of tensor data. The proposed research is primarily motivated by challenging problems that arise in gene network analysis and collaborative filtering, where one central issue is how to leverage and utilize lower-dimensional structure to battle high statistical uncertainty in a discovery process.  New techniques are proposed and investigated, both computationally and statistically, which target biomedical and engineering problems. In (a) and (b), our effort will be on classification and regression, and on structure adaptation through tensor decomposition and factorization, with most effort focused towards condition specific extraction of lower-dimensional structure.<br/><br/>Modern scientific and engineering investigation, as in biomedical research and computer vision, now produces enormous data that aim to simultaneously explore relations among hundreds and thousands interacting units.  This project proposes methods for treating the new scientific environment. The project develops technology that is directly applicable to applied research, particularly in automatic machine processing and data mining, biomedical research, advertisement, and economics. Plans for technology transfer are described, in addition to an educational program that will train students in statistical learning and data mining. Educational activities include developing a course, and attracting undergraduate students to research."
"1209226","Functional Linear Models and Functional Time Series","DMS","STATISTICS","09/01/2012","09/03/2014","Alexander Aue","CA","University of California-Davis","Continuing Grant","Gabor Szekely","08/31/2015","$200,000.00","Thomas Chun Man Lee","alexaue@wald.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","The proposed research provides innovative methodology for the statistical inference for functional data with a focus on functional regressions and functional time series. The investigators will thereby extend the range of functional data analysis beyond the currently available models. This is relevant because the proposed models arise in a natural way in a number of applications such as engineering, geophysics, economics and finance. A characteristic common to many functional data applications is the presence of dependence. The theory of functional data, however, is as of today mainly focused on independent processes with notable exceptions being given by the functional autoregressive and linear processes. This proposal represents a comprehensive research plan for developing new estimation and model selection methods for different classes of functional linear and (novel) functional time series models.  It contains the following parts: (1) develop practical algorithms for partitioning a sequence of functional data into subsequences of homogeneous curves, (2) develop methods for parameter estimation for general functional autoregressive processes of known orders, (3) develop automatic methods for selecting the order of a functional autoregressive process, (4) apply model selection techniques to select best fitting models in the fully functional linear model setting, (5) construct estimation and model selection methods for the novel functional autoregressive model with exogenous covariates, and (6) develop online monitoring procedure for both functional linear and autoregressive models. This will require the development of sophisticated new statistical methodology, requiring the refinement and extension of the theory of (vector-valued) Hilbert space-valued observations. The research will also include a significant innovative computational component. To aid the dissemination of results, the investigators plan to make the relevant software freely available via the Internet. Completion of the proposal will give statisticians and practitioners new tools for analyzing different forms of functional data. <br/><br/><br/>The research in this proposal is interdisciplinary in nature, with applications in diverse fields such as engineering (floodplain management), geophysics (magnetic field readings of magnetometers), finance and economics (tick-by-tick transaction data). The research is therefore of immediate interest for practitioners and will further connect statistics and fields of science with a significant statistical component. It will also advance mathematical and computational statistics. The proposed research will produce doctoral students, among them female and minority students, theoretically and practically versed in both statistics and an area of application. The training and involvement of undergraduate students in this research is also included through regular coursework, independent study and projects."
"1150318","CAREER: High-Dimensional Variable Selection in Nonlinear Models and Classification with Correlated Data","DMS","STATISTICS","08/01/2012","06/13/2016","Yingying Fan","CA","University of Southern California","Continuing Grant","Gabor Szekely","07/31/2017","$400,000.00","","fanyingy@usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","MPS","1269","1045","$0.00","Estimation and prediction with large-scale data sets commonly arise in statistics and related fields and pose great challenges. To address these challenges, four interrelated research topics are proposed for investigation. First, the investigator proposes robust variable selection methods for heavy-tailed data in the ultra-high dimensional setting of dimensionality increasing exponentially with the sample size. To address the heavy-tailedness, regularization methods with robust losses and general penalty functions in various model settings are investigated. The risk properties of these methods are studied and the optimality of penalty function and loss function is characterized. Robust independence screening methods are also proposed and studied. Second, variable selection in high-dimensional functional regression models with functional predictors and/or functional response is investigated. Model fitting procedures are proposed and sampling properties of the proposed methods are thoroughly investigated. Third, the investigator studies the regularization parameter selection in penalized empirical risk minimization in both settings of correctly specified and misspecified models in ultra-high dimensions. The appropriate tradeoff between the model fitting and model complexity is characterized. This study also answers the question on whether conventional model selection criteria such as AIC and BIC continue to work in ultra-high dimensions.  Fourth, high-dimensional classification with correlated features is extensively studied under the unified framework of thresholding classification rules, and the optimal choice of threshold that minimizes the classification error is identified. The investigator studies Gaussian classification and generalizes the methods and results to the case of correlated discrete features.<br/><br/>Thanks to the advent of modern technologies such as the handwritten digital recognition and single-nucleotide polymorphism (SNP) genotyping experiments, massive data sets with a large number of variables are becoming more and more common in various scientific fields such as computational biology, economics, finance, machine learning, and climatology. How to effectively analyze these data sets poses great challenges in both methodology and computation that are not present in smaller scale studies. A major goal of this proposal is to propose new or extended methodologies and investigate their sampling properties in depth and width for high-dimensional model building and model evaluation in various settings of regression and classification problems. The PI has broad research interests in many fields outside statistics such as computational biology, finance, econometrics, and machine learning. The proposed methods will be tested on real data sets and extended to these different areas. In addition, the PI plans to develop software packages to implement the proposed methods, and make them publicly available. The proposed work will benefit a broad range of scientists and researchers in various fields. The PI also plans to integrate education activities with the proposed research, such as involving minority students, undergraduate students, and graduate students in the proposed projects and incorporating cutting-edge high-dimensional statistical methods into new courses."
"1208874","Quantile regression with random censoring","DMS","STATISTICS","06/01/2012","05/29/2012","Yijian Huang","GA","Emory University","Standard Grant","Gabor Szekely","05/31/2016","$180,000.00","","yhuang5@emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1269","","$0.00","This project aims at developing quantile regression methodology for time-to-event data with random censoring. As a primary statistical tool to assess functional covariate effects, quantile regression has been extensively developed for uncensored data and for data with fixed censoring. However,time-to-event data are often subject to random censoring where the censoring time is observed only for censored individuals but not for uncensored ones. Such censoring poses substantial challenges for the estimation and inference, particularly under the standard conditional independence censoring mechanism that the censoring time and survival time are conditionally independent given the covariates. Building on recent advances, the investigator will pursue three important research directions. First, the model will be generalized to accommodate time-dependent covariates, which are common in survival studies. Second, two-stage sampling, including case-cohort and case-control designs, will be accommodated for censored quantile regression. These designs may have tremendous cost savings in large-scale studies. Finally, data-driven transformations of the survival time will be incorporated in the model for further flexibility. Statistical and computational methods will be developed for the resulting nonlinear quantile regression model. The proposed work will significantly advance the existing censored quantile regression methodology.<br/> <br/>Time-to-event data arise in survival studies from a wide range of fields including biomedical sciences, engineering, economics, sociology, public health, and demography. Scientific questions of interest are often concerned with how survival time might be affected by covariates, i.e., predictors; for instance, whether HIV-infected individuals may survive longer with a new intervention. Standard survival analysis techniques, as routinely adopted, presume constant effects of covariates over time. Such a practice has contributed to many scientific controversies in circumstances where effects of interest may evolve over time. This research tackles the problem by adopting censored quantile regression, which accommodates varying effects of covariates in a realistic and robust fashion. The proposed developments will complement standard survival analysis techniques and have the potential to substantially influence survival analysis in practice."
"1339098","CAREER: Statistical Inference in Algebraic Models with Singularities","DMS","STATISTICS","09/01/2012","04/03/2013","Mathias Drton","WA","University of Washington","Continuing Grant","Gabor Szekely","06/30/2013","$32,854.00","","md5@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, 1045, 1187, OTHR","$0.00","NSF CAREER proposal DMS-0746265<br/><br/>This project is concerned with statistical models whose parameter spaces have singularities.  The investigator studies how singularities impact the behavior of existing statistical methods and develops new techniques for adequate assessment of statistical significance.  The focus is on algebraic statistical models, that is, models that have (semi-)algebraic sets as parameter spaces.  The class of algebraic models comprises many of the singular models employed in practice and can be studied using tools from computational algebraic geometry.  Importantly, the well-behaved local geometry of semi-algebraic sets makes it possible to obtain general results without having to assume difficult to verify regularity conditions.  The statistical techniques under study include classical procedures from likelihood inference such as likelihood ratio and Wald tests as well as information criteria.<br/><br/>Modern scientific studies often require analysis of data on several jointly observed variables.  Statistical models of dependence relationships among the different variables are often formulated using additional variables that are not observable (or hidden).  A common feature of hidden variable models is that their statistical properties are not entirely understood because of a lack of smoothness properties that makes them irregular.  This is the primary motivation for this project that develops theory and methods that have a bearing on problems such as determining the number and type of unobserved variables to be included in a statistical model.  Such problems arise in particular in applications in the social sciences where key concepts such as intelligence are not directly observable, and in computational biology where hidden variables are employed, for example, when DNA of present-day species is used to validate evolutionary theories that involve extinct species.  More broadly, the work is relevant for any study, medical or otherwise, in which the existence of influential unobserved variables cannot be excluded.<br/>"
"1317118","CAREER: New Topics in Functional Data Analysis","DMS","STATISTICS","08/03/2012","03/09/2016","Yehua Li","IA","Iowa State University","Continuing Grant","Gabor Szekely","05/31/2018","$365,156.00","","yehuali@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","1045","$0.00","Functional data are random vectors in a functional space, which are usually observed on discrete points and measured with error. Functional data are also deeply connected to other types of correlated data, such as spatial data, since they can both be modeled as stochastic processes. In the proposed research, the investigator will broaden the applications of functional data analysis by proposing a new functional data approach to model spatio-temporal point pattern data from disease surveillance applications, where the spatio-temporal random effects are modeled as latent functional processes. Motivated by scientific problems in colon carcinogenesis experiments and hypertension studies, the investigator proposes new dimension reduction methods, which are widely applicable to semiparametric regression problems with functional predictors. The investigator proposes new estimation procedures based on spline approximation and roughness penalties, and will also investigate the model selection and inference problems related to these methods. The investigator also proposes to model longitudinal clinical trial data by the functional analysis of covariance models, where different treatment effects are represented by nonparametric functions in time. The proposed nonparametric hypothesis test can be used to detect the treatment effects. Specifically, the investigator will study the effect of the within-subject correlation on the power of the test.<br/><br/>The proposed functional data approach to disease surveillance data will help to model the relationship between disease occurrence and some environmental variables (such as pollution level), estimate the time trend in the disease rate, and predict the unknown risk factors represented by latent random effects. The results will help disease control agencies and local officials to gain better understanding of the disease risk and develop better public health policies, such as emission or water quality control policies. The proposed dimension reduction methods provide the much needed statistical tools in the semiparametric regression problems in colon carcinogenesis and hypertension studies. The proposed nonparametric hypothesis testing procedure for functional analysis of covariance models answers a fundamental question in clinical trials, which is to compare the effectiveness of different treatments. The investigator will provide free and user-friendly software to scientific researchers and incorporate his research activity with graduate education. To further disseminate the research results and motivate new ideas, the investigator will develop a new course on functional data analysis and organize a research workshop."
"1231069","Developing Novel Statistical Methods in NeuroImaging","DMS","STATISTICS","04/15/2012","04/10/2012","Hernando Ombao","CA","University of California-Irvine","Standard Grant","Jia Li","03/31/2013","$16,500.00","","hombao@uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1269","7556","$0.00","The proposed project is a workshop on ""Developing Novel Statistical Methods in NeuroImaging"". The plan is to hold the workshop on July 24-26, 2012 at the University of California at San Diego which will immediately precede the Joint Statistical Meeting (JSM 2012) in San Diego, California. The primary goal of the workshop is to identify open problems in statistical research that emerge from current challenges in neuroimaging. The analysis of brain data presents statistical challenges because of its massiveness, high dimensionality and complex spatio-temporal dependence structure. It is anticipated that open lines of statistical research will be identified -- especially in the areas of time series, spatial analysis, dimension reduction, statistical learning, functional data analysis, statistical computation and foundations of statistical inference. <br/><br/>To achieve the stated workshop goal, the first step is to equip quantitative scientists (statisticians and mathematicians) with the necessary theoretical background in neuroscience, thereby familiarizing them with the data. Several lectures will be delivered by leading neuroscientists. The second step is to cover the state-of-the-art statistical methods for the analysis of brain imaging data and to identify the limitations of the current approaches. Scientists from diverse research areas and career stages will meet in order to identify and discuss key emerging areas of interdisciplinary neuroimaging research where mathematical sciences promise to play an important role in the coming years.  The plan is to utilize funds from the NSF to provide partial support to young researchers (including recent PhD grads, PhD and post-doctoral students). Junior statisticians who belong to under-represented groups (e.g., women and minorities) are encouraged to apply.  Details on the workshop are available online: http://www.ics.uci.edu/~hombao/neurostatsw2012.html."
"1209232","Advanced Statistical Methods and Computation for Emerging Challenges in Astrophysics and Astronomy.","DMS","STATISTICS","07/01/2012","08/20/2014","Yaming Yu","CA","University of California-Irvine","Continuing Grant","Gabor Szekely","06/30/2015","$236,000.00","Thomas Chun Man Lee","yamingy@uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1269","","$0.00","The California-Boston AstroStatistics Collaboration is developing model-based strategies for statistical inference in astronomy and astrophysics. They specifically design highly structured models to account for particular complexities in the sources and data generation mechanisms with the goal of answering specific scientific questions as to the underlying astronomical and physical processes. This strategy requires state-of-the-art statistical inference, sophisticated scientific computing, and careful model-checking procedures. They will employ, extend and publicize inferential and efficient computational methods under highly-structured models that involve multi-scale structure and/or multiple levels of latent variables and incomplete data. Such models are ideally suited to account for the many physical and instrumental filters of the data generation mechanisms in astrophysics. The collaboration specifically aims to develop a mixture of parametrized and flexible multi-scale models that can be combined with complex computer-models to describe spectral, spatial, and timing data, either marginally or jointly. In astronomy, for example, the analyses of data from the same observation, but in different regimes (images, spectra, and time series) are typically conducted separately. This simplifies analysis, but sacrifices information, for example as to how a spectrum varies over time or across an image. The Collaboration proposes to develop coherent methods for multi-regime data including the joint use of high throughput spatio-spectral data to isolate and identify complex solar features and the analysis of systematic temporal variance in spectra from stellar coronae. They also propose to embed complex computer models into highly structured models, a strategy which allows the combination of multiple computer models along with physics-based parametric and/or flexible multi-scale models to derive comprehensive methods that address complexities in both the astronomical sources and the instrumentation. Building such highly structured models requires subtle tradeoffs between complexity and practicality and fitting them poses significant computational challenges. This proposal includes a suite of research projects that aim to produce efficient tailored Monte Carlo methods. <br/><br/>Dramatic advances in space-based instrumentation over the past decade have led to the deployment of a new generation of telescopes with unprecedented capabilities. Such instruments are often tailored to meet specific scientific goals and are increasing both the quality and the quantity of data available to astronomers. Massive new surveys are resulting in enormous new catalogs containing terabytes of data, in high resolution spectrography, imaging, and time-series across the electromagnetic spectrum, and in ultra high resolution imaging of explosive dynamic processes in the solar atmosphere. Scientists wish to draw conclusions as to the physical environment and structure of astronomical source, the processes and laws which govern the birth and death of planets, stars, and galaxies, and ultimately the structure and evolution of the universe. This combination of complex instrumentation and complex science leads to massive data analytic and data-mining challenges for astronomers. The California-Boston AstroStatistics Collaboration plans to tackle these challenges using principled statistical methods derived from carefully designed astronomical and mathematical models. As the Collaboration develops methods and distributes free software, it will also educate the astronomical community as to the benefit of sophisticated statistical methods. It is expected that a fundamental impact of the proposed research will be more general acceptance and use of appropriate methods among astronomers. The Collaboration not only aims to develop new methods for astronomy but plans to use these problems as springboards in the development of new general statistical methods, especially in signal processing, multilevel modeling, computer modeling, and computational statistics. The collaboration will use the statistical challenges posed in astronomy as a testing ground for new sophisticated inferential and computational techniques that will help solve complex data analytic challenges throughout the natural, social, medical, and engineering sciences."
"1238351","Collaborative Research: Applied Probability and Time Series Modeling","DMS","STATISTICS","01/16/2012","06/18/2013","Hernando Ombao","CA","University of California-Irvine","Continuing Grant","Gabor Szekely","05/31/2015","$97,215.00","","hombao@uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1269","9150","$0.00","An investigation of the properties of Levy-driven CARMA (continuous-time ARMA) processes will be undertaken and efficient methods of inference developed. The results will be applied to the study of stochastic volatility models with Levy-driven CARMA volatility that have applications that go beyond finance to turbulence and some neuroscience processes. Time series in which the parameters are constant over time-intervals between change-points constitute an important class of non-stationary time series which has been found particularly useful in hydrology, seismology, neuroscience, environmental science and finance. Properties and applications of a new estimation technique based on the minimization of the minimum description length of a model that includes the number of change-points and their locations as parameters will be developed and extended to cover a general class of processes with structural breaks. It is hoped that this technique can also be adapted for detection of both additive and innovational outliers. Linear and nonlinear models for multivariate time series, with a view towards modeling temporal brain dynamics, will also play a major role in this research proposal. These models include a mixture of possibly nonlinear vector autoregressions and a class of not necessarily causal vector autoregressions. The latter class, although linear, exhibits features previously only associated with nonlinear models and allows for the possibility of foresight in the sense of dependence of one or more components of future shocks. <br/><br/>In the last fifteen years, there has been a widely-recognized need for the development of new models and techniques for the analysis of time series data from scientific, engineering, biomedical, financial, and neuroscience applications. Some of the features required of these new models are nonlinearity, complex dependence structures, strong deviations from normality and non-stationarity. In neuroscience, environmental and financial modeling there is also a demand for continuous-time models which incorporate these features. The current proposal addresses these needs. It seeks to enhance understanding of the physical, biomedical, and economic processes represented by the models. The development of efficient estimation and simulation techniques will be an essential component of the research."
"1159005","FRG: Collaborative Research: Unified statistical theory for the analysis and discovery of complex networks","DMS","STATISTICS","06/01/2012","03/29/2012","Elizaveta Levina","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","05/31/2016","$206,579.00","","elevina@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","1616","$0.00","The investigators will develop a unified nonparametric theoretical framework to study stochastic network models and design scalable algorithms (and software) to fit these models. They intend to carry out the development and validation of their methods with collaborators in biology who have gathered extensive new data for the assessment of protein structure and determination of biological pathways particularly in Drosophila. Such problems are omnipresent in genomics and they expect their methods to carry over widely. They will also use networks with uncertainty measures to study relationships between words and phrases in a newspaper database in order to provide media analysts with automatic and scalable algorithms. In all cases they will focus on methods of general statistical confidence which have been lacking in work so far. <br/><br/>Our world is connected through relationships, among ""actors"" who can be people, organizations, words, genes, proteins, and more. Advancements of information technology have enabled collection of massive amounts of data in all disciplines for us to build relationships between these actors. These relationships can be effectively described as networks, and properties or patterns in these networks can be random or knowledge. Responding to this recent data availability and a huge potential for knowledge discovery, research in networks is attracting much attention from researchers in physics, social science, computer science, and probability. While contributing to the development of core statistical research, the proposed research will directly impact the interdisciplinary field of network analysis and the study of complex networks. The applications of their research results are diverse and well beyond the two fields studied in the proposal: genomics and media analysis. They include national security, communications, sociology, political science, and infectious disease. The statistical tools developed are unifying and could change how many scientists approach network analysis. As a result, statistical research will become more prominent in the networks community."
"1208837","Conference and Summer School: Algebraic Statistics in the Alleghenies","DMS","ALGEBRA,NUMBER THEORY,AND COM, STATISTICS","05/01/2012","04/04/2012","Jason Morton","PA","Pennsylvania State Univ University Park","Standard Grant","Haiyan Cai","04/30/2013","$36,000.00","Seth Sullivant, Sonja Petrovic, Aleksandra Slavkovic, Manfred Denker","morton@math.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1264, 1269","7556","$0.00","The event ""Conference and Summer School: Algebraic Statistics in the Alleghenies"" will be held at the Pennsylvania State University, University Park, PA, from June 9 to 15, 2012 (URL: http://www.math.psu.edu/morton/aspsu2012/). Algebraic statistics exploits algebraic geometry and related fields to solve problems in statistics and its applications. Methods from algebraic statistics have been successfully applied to address many problems including construction of Markov bases, theoretical study of phylogenetic mixture models, ecological inference, identifiability problems for graphical models, Bayesian integrals and singular learning theory, social networks, and coalescent theory. In addition to algebraic statistics' successes in solving statistical problems, its research objectives have driven theoretical developments in algebra.<br/><br/>Traditionally, applied mathematics has focused on branches of mathematics including differential equations and analysis.  Instead, algebraic statistics advocates algebraic geometry, a well-developed and ancient field of mathematics, as a tool for solving problems in statistics and its applications.  Many statistical models have the structure of algebraic varieties.  This observation catalyzed rapid growth in this area over the past fifteen years.  Over this time it has become clear that algebraic structures are ubiquitous in statistics.  Hence advanced tools from algebra profitably address statistical questions. The purpose of this grant is to support a seven day conference and summer school Algebraic Statistics in the Alleghenies at the Pennsylvania State University, June 9-15, 2012. More than 100 participants are anticipated, which would make this the largest meeting yet on Algebraic Statistics that had been held in the US or abroad."
"1209059","Variable Selection, Variable Screening and Dimension Reduction","DMS","STATISTICS","08/15/2012","05/22/2013","Michael Akritas","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","07/31/2016","$140,000.00","","mga@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","The objectives of this proposal are (1) to enrich the variable selection alternatives by merging ideas from the variable selection and dimension reduction  areas, and (2) develop variable screening procedures using alternative methods of association learning. The specific goals include: a) develop post-dimension reduction model checking procedures, b) develop variable selection procedures using the model checking procedures and  multiple testing ideas, c) develop dimension reduction procedures for very high dimensional data using matrix regularization, d) develop variable screening based on partial correlation learning, e) develop variable screening using nonparametric association learning.<br/><br/>In many areas of contemporary research, including gene expression and proteomics studies, biomedical imaging, functional magnetic resonance imaging, tomography, tumor classifications, signal processing, image analysis,  finance, text retrieval and climate studies, the data collected include a large number of variables with only a few of them being relevant for prediction. Procedures for identifying the relevant predictors have mostly been developed under certain model assumptions and may fail to discern the relevance of some important predictors. The proposed research aims at developing variable screening and variable selection procedures which do not rely on potentially restrictive modeling assumptions."
"1208939","Composite Estimating Function Approaches to GeoCopula Models for  Complex Spatially Correlated Data","DMS","STATISTICS","07/01/2012","05/23/2012","Peter Song","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","06/30/2015","$170,000.00","","pxsong@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Recent advances in technologies have given subject-matter scientists powerful tools to conduct large scale experiments and collect complex spatially correlated data. This proposal focuses on the development of statistical models and methods of composite likelihood for analyzing such high-dimensional complex data. In particular, the investigator plans to achieve three research goals: To develop a unified framework of multivariate copula models with flexible and desirable correlation structures for spatio-temporal data, spatial-clustered data and spatial-network data; to develop a feasible and computationally tractable estimation and inference methods as well as algorithms in the proposed models; and to establish the large-sample theory for the proposed composite estimating function (JCEF) estimator. <br/> <br/>All the proposed statistical models and methods will be applied to analyze data from practical studies to facilitate the understanding of subject-matter sciences and ultimately to improve human knowledge and quality of life. The investigator has close connections with researchers in other fields such as Epidemiology, Environmental Health Sciences, Infectious Diseases and Policy in Transportation Safety at University of Michigan. He has been working closely with these scientists who will serve as local users of the methodologies and provide valuable feedback. The project is also devoted to substantial educational initiatives that will involve undergraduate and graduate students and expose them to state-of-the-art research in various interdisciplinary topics related to the proposed research. These include new courses, short courses at major conferences, summer workshops, mentoring, and software development. These and other dissemination activities will increase awareness of modern powerful methods for data analysis among scientists from other fields."
"1203216","Conference on Statistical Learning and Data Mining","DMS","STATISTICS","01/01/2012","12/05/2011","Ji Zhu","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","12/31/2012","$25,000.00","","jizhu@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","7556","$0.00","An international conference on ""Statistical Learning and Data Mining"" will be held June 4-7, 2012 on the Ann Arbor campus of the University of Michigan. The objective is to bring together researchers in statistical learning and data mining from academia, industry, and government in a relaxed and stimulating atmosphere focused on the development of statistical learning theory, methods and applications. The conference will feature three plenary talks by internationally prominent researchers whose work are cutting-edge in the field of statistical learning and data mining. Eighteen invited breakout sessions, each with three talks, will cover additional topics with great interest to the field. These include Computational Advertisement, Function Estimation, High-dimensional Methods, Structured Learning, Graphical Models, Learning Theory, Model Selection, Covariance Estimation, Network Analysis, Computational Biology, Signal and Image Processing and Data Mining Applications. There will also be seven contributed paper sessions and two contributed poster sessions where junior investigators and graduate students are expected to participate.<br/><br/>Statistical learning is a relatively new discipline, evolving from machine learning methods of artificial intelligence and multivariate statistics. The general goals of statistical learning are discovery, classification and prediction, often in very high, effectively infinite, dimensional contexts. The advent of powerful computers with accompanying massive data sets has brought the discipline to the forefront of statistical theory and practice. The major goal of the proposed conference is to present some of the most important recent advances in the field and to discuss future research directions. A major part of the conference focuses on bringing statistical research leaders together with students, postdoctoral fellows, and young academics in a stimulating environment. The funding from the NSF will mainly support graduate students and junior researchers in American universities to attend the conference and present either a talk or a poster. The conference is expected to accelerate interactions and collaborations among researchers in the important area of statistical learning and data mining, and thereby lead to the development of new and more effective methods of modeling and inference."
"1208965","Conference on Long-Range Dependence, Self-Similarity, and Heavy Tails","DMS","STATISTICS","04/01/2012","03/13/2012","Stilian Stoev","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Haiyan Cai","03/31/2013","$5,000.00","","sstoev@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","7556","$0.00","The PI is co-organizing a conference on long-range dependence, self-similarity, and heavy tails, which will take place in Research Triangle Park, North Carolina in April 2012. More details can be found on the conference web-site http://lrd2012.web.unc.edu. The three themes of the conference are studied and arise across a wide range of disciplines and applications, including probability, statistics, geophysics, telecommunications, engineering, economics, finance and insurance. Many of these fields were literally transformed by the use of models involving long-range dependence and heavy tails. For example, with a dramatic shift from circuit to packet switching in modern communication networks, these models took over the dominant place from the models based on Poisson assumptions and exponential distributions. New exciting areas of application such as social networks are posing new challenges where power laws, scaling limits, and self-similarity phenomena emerge in a new light.<br/> <br/>The conference will feature talks from world renowned experts as well as poster presentations by students and junior researchers.  It is expected to have an important and long lasting scientific, educational, and socioeconomic impact. The conference will foster the exchange of ideas between probabilists, theoretical and applied statisticians, other scientists, as well as students and practitioners. This will likely lead to formulating new problems relevant to important applications, thereby stimulating the development of new fundamental theoretical research. Many of these problems are expected to have a broad educational, scientific, and general impact on society. For example, in connection to understanding risk, gauging uncertainty in complex setting, or taking advantage of emerging network structures. Most importantly, the event would introduce young researchers and graduate students to important areas of research.<br/>"
"1209007","Collaborative Research:  Estimation, Inference, and Computation for Finite Nonparametric Mixtures","DMS","STATISTICS","08/15/2012","08/13/2014","David Hunter","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","07/31/2015","$23,463.00","","drh20@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","This project aims to develop theory and methods for estimation of the functional components and weights in the nonparametric multivariate finite mixtures. These mixtures only assume that the components are drawn from some family of multivariate density functions without any parametric specification. The investigators, who are already active in this emerging area of research, adapt a number of estimation methods that are known from finite parametric mixture theory to the nonparametric context. The PI and the co-PI propose a number of practically feasible and fast algorithms that can be used to compute the resulting estimators in practice. Finally, both investigators show how to obtain large-sample asymptotic results for the proposed estimators. <br/><br/>Finite nonparametric mixtures of distributions can provide answers to many practically important questions. As an example, they can be used to help a physician in establishing the definitive diagnosis in case of a complex medical condition with a number of possible diagnoses. An example of such a situation is a patient with a possible heart attack where other differential diagnoses are also possible. Developmental psychology provides another useful example. Indeed, study of cognitive development in children, in particular identification of strategies used by children to accomplish various tasks, can also be modeled easily using these mixtures. This has important implication for developmental psychology, providing answers to many difficult questions faced by child psychologists while helping children mature and develop in an optimal way. The PI and the co-PI propose a number of efficient algorithms to estimate these mixtures and accomplish the practical tasks mentioned above. These algorithms will be publicly available and easy to use as part of the R software package called mixtools."
"1208785","Dense and Sparse Methods in High-Dimensional Data Analysis","DMS","STATISTICS","08/01/2012","07/02/2012","Lee Dicker","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","07/31/2016","$159,995.00","","ldicker@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","Many methods for high-dimensional data analysis begin with the assumption that the parameter of interest is, in some sense, sparse.  Furthermore, the performance of many of these methods depends on the sparsity of the underlying parameters.  However, statistical methods for checking sparsity assumptions and determining the implications of the absence or near-absence of sparsity are lacking.  The driving goal of this project is to develop practical statistical tools for identifying situations where the relevant parameters are in fact sparse, or where sparse methods for high-dimensional data analysis may be applied effectively.  Problems considered in this project will primarily be studied within the context of the linear model and the Gaussian location model.  Methods will be assessed by decision theoretic-like criteria (e.g. asymptotic minimaxity).  A null model based on dense (non-sparse) signals and dense estimation and prediction methods will be developed and thoroughly studied.  This will provide a rich framework for sparsity testing, where the aim is to identify settings in which sparse methods are likely to be successful.  Specific sparsity testing procedures will be proposed and analyzed.  <br/><br/>High-dimensional data analysis is one of the most active areas of current statistical research.  Much of this research has been driven by technological advances that have enabled researchers to collect vast datasets with relative ease in a variety of scientific disciplines, including astrophysics, geological sciences, molecular biology, and genomics.  In high-dimensional datasets, many features are measured for each unit of observation (e.g. thousands of gene expression levels may be measured for each individual in a genomic study).  Sparsity plays a major role in much of the research on high-dimensional data analysis.  Broadly speaking, sparsity measures the degree to which a specified outcome may be described by relatively few features.  Sparse methods for high-dimensional data analysis attempt to leverage sparsity in the underlying dataset and have proven to be very effective in many applications, especially in engineering and signal processing.  On the other hand, the performance of sparse methods has been more mixed in other important applications where high-dimensional data are abundant, such as genomics.  In this project, the investigator will develop statistical methods for characterizing and identifying situations where sparse methods can be successfully applied.  This will be achieved by developing tools for determining the level of sparsity in high-dimensional datasets.  These methods, when applied to a given dataset, will help researchers determine the validity of subsequent statistical analyses and the potential benefits of using sparse methods for these analyses.  This research is likely to have significant implications for understanding reproducibility in high-dimensional data analysis and broad applications in the analysis of genomic data.  The methods developed during the course of this project will be utilized in collaborative work with highly experienced researchers in genomics."
"1209085","Collaborative Research:Modeling and Analysis of Fracture Network for Shale Gas Development and Its Environmental Impact","DMS","STATISTICS, OPPORTUNITIES FOR RESEARCH CMG","09/01/2012","08/22/2013","Rong Chen","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","08/31/2016","$100,000.00","","rongchen@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269, 7215","","$0.00","Hydraulic fracturing stimulation is one of the key technologies in shale gas development. Though it has been widely used in oil and gas industry for many years, it remains a great challenge to quantitatively characterize the hydraulic fracturing induced fracture network in shale gas development due to the complex shale gas formation and the lack of observational data. Consequently, it is extremely difficult to evaluate and predict its efficiency and environmental impacts. In this collaborative research project the investigators study the hydraulic stimulation induced fracture network through advanced statistical analysis and mathematical modelling. Armed with micro-seismic data and production yield curve, they develop a statistical dynamical system for the fracture network, mimicking the network formation process under imposed hydraulic pressure. A Bayesian framework is used for parameter inferences, utilizing prior knowledge of the geological structure of the field under study. A mathematical subsurface flow model and an advanced numerical method, the sub-region method, is used to link a given fracture network to the production yield curve, providing vital information of the unobservable fracture network underground.<br/><br/>By combining creative statistical analysis with advanced mathematical modelling and numerical method, the project addresses a very challenging application with important national interests. The project yields a better understanding of the hydraulic stimulation process, provides useful guidance for control and optimization of hydraulic fracturing process as well as assessing its environmental risks and consequences. Such knowledge helps policy makers to make more informed decision and more accurate cost analysis. The project spans both research and education aspects, including training of undergraduate, doctoral and post-doctoral students in interdisciplinary research, crossing the boundary of statistical analysis and numerical analysis, on an important application. The resulting software, available in public domain, will be used as an educational, research, and engineering tool well beyond the project duration."
"1209014","STATISTICAL INFERENCE WITH HIGH-DIMENSIONAL DATA","DMS","STATISTICS","07/01/2012","03/29/2012","Cun-Hui Zhang","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","06/30/2016","$357,024.00","","czhang@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","The proposed research will develop new methodologies and algorithms for statistical inference of low-dimensional parameters with high-dimensional data. A low-dimensional projection estimator will be further developed in linear regression and extended to more general high-dimensional statistical models, including generalized linear models, the proportional hazards model, large matrix models and more. The project will investigate consistency and asymptotic normality of the proposed estimators, test of significance, confidence intervals and regions, their efficiency in terms minimum Fisher information, and their tolerance to multiplicity adjustments. This research will directly connect the fields of semi-parametric methods and high-dimensional data, producing a locally uniform and efficient framework of statistical inference.  <br/><br/>High-dimensional data is an area of intense current interest in statistical research and practice due to the rapid development of information technologies and their applications to modern scientific experiments. Important fields with an abundance of high-dimensional data include bioinformatics, signal processing, neural imaging, communications networks and more. In many such scientific and engineering applications, the number of unknowns, and thus the complexity of the problem, is a function of the number of features: genetic components in bioinformatics, brain regions or voxels in neural imaging, or computers and routers in the Internet. A longstanding challenge in high-dimensional data is statistical inference in situations where the number of features is far greater than the number of samples in the data. Existing methodologies for testing the significance of a feature commonly rely on a uniform signal strength assumption: Each feature has either no effect or an effect stronger than an inflated noise level after adjustments for the uncertainty of the set of effective features. However, this uniform signal strength assumption is, unfortunately, seldom supported by either the data or the underlying science, especially in applications in biology, medicine, and communication and social networks. The proposed research will focus on a new approach to the above mentioned longstanding problem of statistical inference with high-dimensional data. It will develop practical methods, efficient algorithms, statistical software, and solid theory for test of significance and confidence regions for low-dimensional functions of features, even when the dimension of data is high. The methodologies developed in the proposed research will be directly relevant to common applications where modern information technologies prosper."
"1209091","Second Order Inference for Nonstationary Time Series","DMS","STATISTICS","08/15/2012","06/14/2013","Han Xiao","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","07/31/2016","$119,074.00","","hxiao@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","The proposed research is on second order inferences for causal nonstationary processes. While a causal stationary process can be viewed as generated by filtering a set of past innovations, one can allow the filter to be time-changing, and henceforth introduce nonstationarity. Two sets of problems are considered. First, for linear models with nonstationary errors, the investigator addresses the estimation of covariance matrices of the least square estimates, as well as general M-estimates. Second, the PI studies the estimation of time-varying covariance functions, time-varying spectrum and covariance matrices of the observed time series. Simultaneous inferences of autocovariance functions and spectra can be used to study their patterns and trends, and are also of interests. The study requires several tools to be developed for nonstationary processes, including empirical processes, Gaussian approximations, strong invariance principles and large deviations for quadratic forms.<br/><br/>Stationarity has played an important role in classical time series analysis, which basically says that the overall structure does not change over time. However, in many scientific fields, including economics, engineering, environmental science, finance, and neuroscience etc, it is not realistic to believe the observed time series are stationary. Results from the proposed research will be useful in understanding the nature of the data from various disciplines, making forecasts and conclusions.<br/>Furthermore, the second order inferences in the proposal are general and fundamental, and will facilitate further statistical analysis of nonstationary time series."
"1209166","Random Matrix Theory and High Dimensional Statistics","DMS","STATISTICS","09/01/2012","06/09/2014","Tiefeng Jiang","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","08/31/2016","$180,001.00","","jiang040@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","High dimensional data are becoming increasingly available from a wide <br/>range of scientific investigations, including genomics, bioinformatics, <br/>engineering, and climate studies. Sound analysis of such datasets poses <br/>many statistical challenges. It calls for new statistical theory and methods <br/>as well as new technical tools. In this collaborative research project, the <br/>investigators will first develop results and  technical tools in random matrix <br/>theory and then take a unified approach using the technical tools developed <br/>to study several important problems in high dimensional statistics <br/>as well as applications in signal processing, physics, and mathematics. <br/><br/>The statistical and scientific objectives outlined in this proposal are interdisciplinary <br/>and will establish connections among different fields - random matrix theory, high <br/>dimensional statistics, signal processing, physics, and mathematics. The research <br/>will also provide technical tools as well as methodology, to researchers in other <br/>scientific fields who collect and analyze high dimensional data. These include, <br/>but are not limited to, genomics, biostatistics, and electrical engineering. The procedures and algorithms developed in this project will be implemented and softwares developed will be made freely and publicly available on the web as open source<br/>code along with the associated research reports so as to facilitate the dissemination of knowledge."
"1209191","Estimation of Functionals of High Dimensional Covariance Matrices","DMS","STATISTICS","09/01/2012","08/18/2014","Huibin Zhou","CT","Yale University","Continuing Grant","Gabor Szekely","01/31/2016","$299,999.00","","huibin.zhou@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","","$0.00","In a wide range of applications involving covariance, people are interested in certain aspects, i.e., functionals, of the covariance structure. Despite recent progress on methodological work on covariance matrices estimation, there has been remarkably little fundamental theoretical and methodological studies on optimal estimation of functionals of high dimensional covariance matrices. The goal of this proposal is to develop a coherent theory to unveil the precision to which covariance matrix functionals can be estimated, to develop general methodologies for optimal estimation of functionals of covariance matrices, to establish the asymptotic equivalence between covariance matrices estimation and Gaussian sequence models, and to address applications that arise in finance, bioinformatics, genomics, and meteorology etc. The research presented in this proposal will significantly advance the theoretical understanding of estimating functionals of large scale covariance matrices. In particular, the asymptotic equivalence theory to be developed will help build a close and inspiring connection between matrices estimation and classical Gaussian sequence models which had been well studied in the past thirty years, and help carry over results and methodologies developed in Gaussian sequence models to matrices estimation. The proposed optimal estimation procedures for those fundamentally important statistics methodologies, for example, principal components analysis, graphical model, and linear and quadratic discriminant analysis, will provide more accurate estimation and prediction rules in a wide range of applications.<br/><br/>With the emergence of high dimensional data from modern technologies, estimating large scale covariance matrices and their functionals is becoming a crucial problem in many fields including climate studies, genomics and proteomics, functional magnetic resonance imaging, portfolio allocation and risk management. The traditional sample covariance matrix estimator has been used frequently in practice when analyzing high dimensional data, which may result in poor performance and invalid conclusions. To overcome the difficulty associated with the high dimensionality, regularized methods have been developed in recent years.  A central role of covariance matrix in statistical analysis and its wide range of important statistical applications ensure that the progress the investigator and his colleagues make towards their proposed objectives will have a great impact in the broad scientific community which includes astronomy, bioinformatics, finance, genomics, meteorology and clinical research. Research results from this proposal will be disseminated through research articles, workshops and seminar series to researchers in other disciplines. The project will integrate research and education by teaching monograph courses and organizing workshops and seminars to help graduate students and postdocs, particularly minority, women, and domestic students and young researchers, work on this topic."
"1201458","Combinatorics of Manifolds and Stacks with Torus Actions","DMS","OFFICE OF MULTIDISCIPLINARY AC, GEOMETRIC ANALYSIS, STATISTICS, Combinatorics","08/15/2012","08/22/2013","Rebecca Goldin","VA","George Mason University","Continuing Grant","Tomek Bartoszynski","07/31/2016","$150,000.00","","rgoldin@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1253, 1265, 1269, 7970","","$0.00","Under this project, the principal investigator (PI) will develop the combinatorics for geometric spaces (manifolds and smooth stacks) with Lie group actions and, in a complimentary program, explore how the combinatorics constrain the geometric objects and their classification. The focus is on Schubert calculus in a generalized setting, and 'stacky' objects with torus actions. The PI will also use geometry and inferential statistics on graphs to try to derive neuronal classification through connectivity. The overarching theme is to bring powerful geometric tools and algebraic invariants to bear on geometrically motivated combinatorial and graph theoretic questions, and to allow the combinatorics to inform the geometric structures. The project also supports the PI's efforts to increase statistical literacy and public education through her work with journalists and the public on basic statistical and scientific reasoning.<br/> <br/>This project promotes the mutual beneficial relationship between geometry and combinatorics, or the theory of counting; an excellent example is describing how to count intersections of certain geometric spaces. Geometric spaces with a lot of symmetry can be described using methods developed in combinatorics, and these descriptions can in turn shed light on the geometry. Similarly, geometric spaces motivate the exploration of specific aspects of combinatorics. The project promotes the use of geometric and statistical techniques to study an important question in neuroscience:  how our brains are organized from a graph-theoretic point of view. Finally, the project supports the PI as Director of Research at STATS (Statistical Assessment Service, www.stats.org), where she works with journalists on how to present mathematical and statistical ideas to the public in an informative and honest way.  These efforts impact how public policy and legislation is formed; they encourages the public to become more educated regarding mathematics and quantitative reasoning concepts, even basic ones such as the difference between correlation and causation; and they show the public that mathematics and academia have an important impact on society."
"1209111","Statistical Analysis of Incomplete lifetime Data: Theory, Stochastic Models and Empirical Likelihood","DMS","STATISTICS","08/15/2012","11/30/2016","Grace Yang","MD","University of Maryland, College Park","Continuing Grant","Gabor Szekely","01/31/2018","$250,000.00","","gly@math.umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1269","","$0.00","The investigator plans to further the development of the theory and statistical methods for the analysis of incomplete lifetime data with special focus on censored data in survival analysis.  The proposed research consists of two interrelated research thrusts: (A) The development of the empirical likelihood (EL)-based inference procedures.  Motivated by the proven advantages of the EL method, a novel approach is proposed.  The PI and her collaborator plan to develop asymptotically optimal statistical procedures that are computationally feasible and efficient, and (B) Extension of the Fix-Neyman competing risks model which focuses on a feature of recurrent events of recovery and relapse of a disease in the model.  For many diseases, such as breast cancer and aplastic anemia (AA), recovery and relapse are important events affecting a patient's survival probability.  Most of the currently employed competing risks models lack this feature.  The research team plans to develop statistical inference procedures under the new model for censored data.  The extended model can be applied to other types of recurrent events such as in epidemiology surveys (current status data) and engineering reliability. <br/> <br/>Lifetime or ""time-to-event"" data, commonly collected in science and engineering, could be time to loss of immunity, survival time of a cancer patient after a treatment, time to failure of a bridge and others.  Due to sampling methods, sampling subjects, experimental protocols and limitations of recording instruments and possibly other reasons, data sets often contain a significant number of incompletely observed lifetimes.  Incomplete data may include right or left censored, interval censored and truncation data.  Without proper corrections for incompleteness, data analysis and uncertainty measures would produce biased and unreliable scientific findings.  It is therefore of paramount importance to develop sound statistical methods and theory for the analyses of incomplete data.  Despite significant advances in theory and applications, burgeoning applications in diverse science fields continues to present new challenging mathematical problems and computational issues.  For example, the proposed extension would extend the popular Kaplan-Meier estimator  by including recovery and relapse data in the prediction of a patient's survival probability.  It is hoped that better utilization of available data would yield more accurate prediction of survival probability for some diseases and help to identify important factors affecting a patient's survival.  Developing numerical solutions will be an integral part of the project.  Algorithms will be developed for data analysis.  The success of the project will advance the statistical theory of incomplete lifetime data and its applications.  Novel use of the empirical likelihood method will result in computationally efficient algorithms for applications.  Research and education for this project are inseparable.  Training of graduate students and recruiting students from under represented group are planned."
"1209124","Collaborative Research: Modeling and Analysis of Fracture Network for Shale Gas Development and Its Environmental Impact","DMS","STATISTICS, OPPORTUNITIES FOR RESEARCH CMG","09/01/2012","08/12/2013","Guan Qin","TX","University of Houston","Continuing Grant","Gabor Szekely","08/31/2016","$100,000.00","","gqin@uwyo.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1269, 7215","9150","$0.00","Hydraulic fracturing stimulation is one of the key technologies in shale gas development. Though it has been widely used in oil and gas industry for many years, it remains a great challenge to quantitatively characterize the hydraulic fracturing induced fracture network in shale gas development due to the complex shale gas formation and the lack of observational data. Consequently, it is extremely difficult to evaluate and predict its efficiency and environmental impacts. In this collaborative research project the investigators study the hydraulic stimulation induced fracture network through advanced statistical analysis and mathematical modelling. Armed with micro-seismic data and production yield curve, they develop a statistical dynamical system for the fracture network, mimicking the network formation process under imposed hydraulic pressure. A Bayesian framework is used for parameter inferences, utilizing prior knowledge of the geological structure of the field under study. A mathematical subsurface flow model and an advanced numerical method, the sub-region method, is used to link a given fracture network to the production yield curve, providing vital information of the unobservable fracture network underground. <br/><br/>By combining creative statistical analysis with advanced mathematical modelling and numerical method, the project addresses a very challenging application with important national interests. The project yields a better understanding of the hydraulic stimulation process, provides useful guidance for control and optimization of hydraulic fracturing process as well as assessing its environmental risks and consequences. Such knowledge helps policy makers to make more informed decision and more accurate cost analysis. The project spans both research and education aspects, including training of undergraduate, doctoral and post-doctoral students in interdisciplinary research, crossing the boundary of statistical analysis and numerical analysis, on an important application. The resulting software, available in public domain, will be used as an educational, research, and engineering tool well beyond the project duration."
"1208786","Collaborative Research: New Developments for Analysis of Two-Way Structured Functional Data","DMS","STATISTICS","09/01/2012","09/03/2014","T. Siva Tian","TX","University of Houston","Continuing Grant","Gabor Szekely","08/31/2015","$74,999.00","","ttian@times.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1269","","$0.00","Most existing methodologies of functional data analysis are limited to data structured in one domain, such as time. Two-way structured functional data are indexed by two functional domains, such as space and time, and each domain has its own notion of regularity, such as smoothness or sparsity. Fully considering the two-way structure of the data will lead to more accurate analysis results. Recent work by the lead PI on two-way regularization has provided some preliminary results and a good starting point for investigating new methodology of dimension reduction, feature extraction, regression and classification for two-way functional data. This research team plans to further develop the methodology for two-way functional data in several important directions:(a) Develop a Reproducing Kernel Hilbert Space theory for two-way regularized singular value decomposition (SVD); (b) Develop novel dimension reduction methods for data that are indexed by general domains such as manifolds; (c) Develop dimension reduction methods that effectively analyze discrete two-way functional data; (d) Develop two-way regularization methods for solving the magnetoencephalography (MEG) inverse problem; (e) Develop new classifiers for diagnosis of mental disorders using dynamic MEG images as predictors; (f) Develop robust methods that are resistant to outliers. The success of the research will add a new dimension to functional data analysis and significantly enrich the field.<br/><br/>Two-way structured functional data arise in various disciplines, including medicine, social sciences, earth sciences, economics, and business. But few existing methodologies fully take into account the two-way structure of this type of functional data. The novel statistical methods developed in this research will provide valuable tools for efficient use of such data. They will provide better understanding of scientific, social and economic phenomena and make more accurate predictions. In particular, the two-way reguarlized SVD provides a new analysis of variance method for analyzing high throughput bioinformatics data and for discovering interactions among biomarkers and clinical variables that are associated with a disease phenotype. The new MEG inverse solvers will facilitate noninvasive presurgical mapping of functional areas of the brain. The new classifiers will help diagonosis and assessment of mental diseases using dynamic MEG images. The proposed activities involve training of Ph.D. students who participate in the proposed projects and mentoring of the female, junior statistician P.I. in a psychology department. Research results will be disseminated through collaborative work, academic presentations, and journal publications. Web pages will be created to enable quick access to user-friendly and accessible software implementations of new methods as well as technical reports and relevant references."
"1209194","Robust and Relevant Model Evaluation: Principles and Techniques for Handling Weak Prior Information and Contaminated Data","DMS","STATISTICS, Methodology, Measuremt & Stats","09/01/2012","06/02/2014","Steven MacEachern","OH","Ohio State University","Continuing Grant","Gabor Szekely","08/31/2016","$320,000.00","Yoonkyung Lee, Xinyi Xu","snm@stat.osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269, 1333","","$0.00","This research concerns the development of innovative model comparison and model evaluation methods that focus on the most relevant features of the data and that are robust to deficiencies of model and data.  The advent of modern, automated technology for data collection and of cheap, near-boundless storage capacity provides access to a previously undreamt wealth of data.  The parallel development of sophisticated models which allow one to combine many sources of information and the computational strategies and horsepower which allow one to fit the models would seem to facilitate near-perfect decision-making.  However, the wealth of data aggravates the problems caused by data contamination, and the complexity of model aggravates the difficulty of specifying the prior on the parameters.  Handling data contamination and constructing methods robust to the lack of prior information pose fundamental statistical challenges.  In this project, the investigators illustrate the deficiency of the current leading model evaluation/model comparison methods, and then propose a set of new tools to alleviate the problems.  The proposed research consists of the following two specific aims.  1.  To develop reliable methods for Bayesian model comparison when prior information is lacking.  The common practices of using an improper noninformative prior distribution or a vague proper prior distribution are effective in estimation, however they break down for Bayesian hypothesis testing problems where model choice is sensitive to details of the prior distribution.  To tackle this difficulty, the investigators propose a remedy, the calibrated Bayes factor.  The calibrated Bayes factor does not need extensive subjective evaluation, yields an analysis that better mimics the performance of the Bayes factor under a ''reasonable default'' prior, and is widely applicable in a large variety of model comparison problems.  2.  To develop robust methods for model evaluation and model fitting in the presence of contaminated data.  Contaminated data comes in many forms, including observations potentially from recording mistakes or from irrelevant populations.  The contaminating process might be unstable, which makes standard statistical modeling infeasible.  For model fitting, this project develops and implements restricted-likelihood, which leads to estimation strategies that focus on the most relevant features of the data and that are robust to ''bad data''.  For model evaluation, this project develops an adaptive loss (scoring) paradigm for cross-validation, which produces robust results and yields superior finite-sample performance by stabilizing the evaluation.  <br/><br/>Model evaluation and model comparison are used on a daily basis in both scientific and corporate decision-making settings.  These techniques help researchers judge which theory best describes the phenomenon, help health professionals identify which risk factors are related to disease incidence, and help corporate managers decide which business strategy results in increased sales or better customer retention.  However, most of the current model evaluation and model comparison methods neglect deficiencies in data or suffer from the lack of parameter information.  The proposed research provides powerful methodological tools for robust model preference, model evaluation, and model fitting in these difficult situations.  It can help people in various fields better extract information from massive data sets, and thus optimize their decision making.  Specific applications in health studies, psychological experiments and machine learning will proceed along with development of the new methodology.  The general methodology is also applicable to many other scientific and technical areas, such as genomics, climatology, and economics, where large data sets are collected and robust model evaluation is desirable.  The investigators are well-positioned to disseminate the project's results.  They have been actively involved in research groups at the intersection of Statistics and the social sciences, engineering/computer science, and marketing.  They are also key members of a joint industry-university center dedicated to provide and disseminate research relevant to the insurance industry.  Results from this project will be spread to other communities through the investigators' interactions with these groups."
"1209155","Collaborative Research: Numerical algebra and statistical inference","DMS","STATISTICS","07/01/2012","05/08/2014","Shayn Mukherjee","NC","Duke University","Continuing Grant","Gabor Szekely","06/30/2015","$150,001.00","","sayan@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","","$0.00","The investigators have two aims in this proposal that fall at the interface of numerical algebra and statistical inference. The first aim is to extend the use of randomized approximation in a variety of dimension reduction methods that rely on numerical linear algebra both supervised and unsupervised as well as linear and nonlinear and develop a statistical bases for these methods in addition to the computational motivation of being applicable to massive data. The other motivation is to extend these statistical methods for dimension reduction to multiway data using numerical multilinear algebra, a recent new development in numerical analysis. These projects will increase interaction between statistical inference and numerical analysis and benefit both fields, providing new perspectives to how we view and perform data analysis.<br/><br/>Numerical methods with statistical implications are central to a variety of technologies used by the general population. These technologies include Google's pagerank algorithm, genetic methods used to find genetic variation related to disease, compressing of medical images for storage and treatment, as well as applications in geostatistics. In all the previous cases the fundamental idea is to condense massive data in a useful summary with respect to a desired goal. The two ideas in this proposal are (1) to study how numerical methods that scale to the massive data generated in modern scientific, engineering, and social applications impose statistical assumptions or models on the data, (2) to study more complex interactions or properties of the data than examined in current methods. The motivation behind the first aim is to understand how numerical approximations required for computational scaling as we collect more data impact the information that can be extracted from these data -- for what type of data and applications do certain numerical approximations work well. The motivation behind the second aim is to go beyond the broad category of standard statistical methods take into account the relation between pairs of objects -- two web pages that are linked for Google's pagerank, the correlation between two genes or two loci in genetics applications. The question behind this aim is whether richer sources of information can be extracted by examining the links between three web pages or three loci. The research involved in this aim consists of the development of computationally efficient algebraic methods to extract this information and understanding the statistical models implemented by these methods."
"1208833","Collaborative Research: Prior-free probabilistic inferential methods for ""large-p-small-n"" linear regression problems","DMS","STATISTICS","06/15/2012","05/03/2013","Ryan Martin","IL","University of Illinois at Chicago","Continuing Grant","Gabor Szekely","05/31/2015","$85,000.00","","rgmarti3@ncsu.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","","$0.00","The investigators study prior-free probabilistic inference with ""large p, small n"" regression analysis.  This is made possible in the new framework of Inferential Models (IMs) proposed recently by the investigators.  Statistical results produced by IMs are probabilistic and have desirable frequency properties.  In this study, the investigators develop IM-based methods for linear and certain non-linear regression analysis.  A sequence of topics in the context of large-p-small-n regression to be investigated include (1) variable selection in Gaussian regression models; (2) robust Student-t regression; and (3) binary regression models.<br/><br/>Linear regression is one of the most commonly used methodologies in statistical applications.  However, desirable prior-free and frequency-calibrated probabilistic inference, particularly in the important variable selection context, has not been available until the recent development of IMs.  The IM framework provides a new and promising alternative to the well-known Bayesian and frequentist methods for various high-dimensional problems researchers currently face.  In this study, the investigators develop new statistical methods and computing software, generating useful tools for applied statisticians and scientists who are challenged by very-high-dimensional data in carrying out regression analysis."
"1322797","CAREER:  Optimal Design of Experiments for Generalized Linear Models","DMS","STATISTICS","10/01/2012","04/15/2013","Min Yang","IL","University of Illinois at Chicago","Continuing Grant","Gabor Szekely","05/31/2015","$224,541.00","","myang2@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","0000, 1045, 1187, OTHR","$0.00","In this work, the PI is to develop a novel approach for identifying optimal and efficient designs under Generalized Linear Models (GLMs) including the fundamental logistic, probit and loglinear models and some other nonlinear models. Specifically, this project intends to achieve the following three objectives: (i) Identify optimal designs for GLMs under non-homogeneous subjects. In this study, optimal designs are derived for models in which the subjects are divided into two or more groups using one or more factors, allowing the intercept or slope to vary from one group to another. Random subject effects can also be allowed for differences among subjects within groups. (ii) Identify optimal designs for GLMs with multiple covariates. There are very few optimality results for GLMs with more than one covariate. The PI will study optimal designs for GLMs when multiple covariates exist. These models can also account for subject heterogeneity. (iii) Identify optimal designs for some other nonlinear models. In nonlinear models, most optimal results were derived under D-optimality for all parameters. The PI will investigate a general approach to identify optimal designs for nonlinear models with three or more parameters under commonly used optimality criteria when all or some of the parameters are of interest. The proposed research will have a tremendous impact because it will fill several gaps in the literature: the models in the proposed research accommodate heterogeneity among subjects and multiple covariates; general solutions for optimal designs of nonlinear models with three parameters will be provided. The technique in the proposed research is innovative in that it yields very general results that go beyond solving problems on a case-by-case basis. It helps to identify the support of locally optimal designs for many of the commonly studied models and can be applied for all the common optimality criteria based on information matrices. It works both with a constrained and unconstrained design region. Furthermore, it can be applied to multistage experiments, where an initial experiment may be used to provide a better idea of the unknown parameters.<br/> <br/>GLMs and other nonlinear models have been used in a wide range of social and natural science fields, such as biological sciences, pharmaceutical research, agricultural science, economics, marketing, etc. The results of this study will have a deep impact on the application of GLMs in these fields. For example, when the findings are applied to the design of clinical trials during new drug discovery and development, they will significantly reduce the time, money, and number of patients needed in these trials. In fact, this research can help the U.S. Food and Drug Administration to improve its guidelines for clinical trials. To effectively disseminate the results of this research, the PI will develop a user-friendly software package targeting non-expert users. To successfully integrate research and education, the PI will develop advanced experimental design courses at the University of Missouri-Columbia incorporating findings of this project. Graduate students will be trained to study optimal designs in the new fields, under the PI?s guidance. Finally, the proposed research has the potential to stimulate new research and to provide tools for identifying optimal designs under GLMs or nonlinear models used in other areas, such as longitudinal data analysis and survival analysis.<br/> <br/><br/>"
"1207444","Generalized Partially Additive Models For High-Dimensional Data","DMS","STATISTICS","08/01/2012","07/25/2012","Hua Liang","NY","University of Rochester","Standard Grant","Gabor Szekely","04/30/2014","$99,994.00","","hliang@gwu.edu","910 GENESEE ST","ROCHESTER","NY","146113847","5852754031","MPS","1269","","$0.00","The investigator studies generalized additive partially linear models (GAPLM) with the aim of developing efficient and flexible estimation and inference methods, variable selection procedures,model specification tests, and model structure checks, and studies applications of these methods for biomedical research. Specifically speaking, he (a) is developing a genuine method that is able to select important parametric and nonparametric components that are numerically stable , even when the numbers of the nonparametric and parametric components diverge; (b) is developing model specification tests for GAPLM; (c) studies model structure determination for GAPLM; (d) studies marginal GAPLM for correlated data; and (e) applies the advanced models and proposed methods to analyze gene data for study of the relationship between certain diseases and genes, including the identification of signature gene expression profiles of cancer cells in response to different drug treatments, the prediction of genetic risks through integrating knowledge from genetic variations in the genome and genomic markers, and validation of epigenetic codes from data collected through next generation sequencing platforms.<br/><br/><br/>The proposed models and methods are motivated by the investigator's study of gene and other potentially useful biomarkers in cancer clinical trials. The results of this project can help identify important gene expression profiles and cancer cells and trace the disease progression in cancer research. The theoretic results contribute to the advancement of the statistical theory on variable selections and semi-parametric inference with high-dimensional covariates."
"1208715","Computing Environments for Statistics","DMS","STATISTICS, CI REUSE","09/01/2012","06/19/2014","Luke-jon Tierney","IA","University of Iowa","Continuing Grant","Gabor Szekely","08/31/2016","$244,916.00","","luke@stat.uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1269, 6892","7433, 9150","$0.00","The investigator explores and develops new principles for the design of statistical software to take advantage of modern computing power. Particular emphasis is placed on exploring the effective use of parallel computing and compilation.  Pilot implementations are incorporated in open source statistical software systems.<br/><br/>Effective statistical methodology made available through statistical software is critical to the ability of researchers to make maximal use of experimental and observational data, and for the ability of instructors to teach good research practices.  The design principles developed by this research lead to software that improves the ability of researchers, instructors, and other users of statistical methodology to apply this methodology more effectively in scientific research and teaching and to take full advantage of modern high-performance computational resources.  These principles also lead to software frameworks that can be used to more rapidly deliver new statistical methodology to end users.  Applications in a range of areas serve as testbeds for methods and principles developed in this research."
"1256768","2013 International Conference on Statistics, Science, and Society: New Challenges and Opportunities","DMS","STATISTICS","09/01/2012","09/01/2012","Nalini Ravishanker","CT","University of Connecticut","Standard Grant","Gabor Szekely","08/31/2013","$25,000.00","","nalini.ravishanker@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","7556","$0.00","""Statistics, Science, and Society: New Challenges and Opportunities (SSSNCO)"" is a four day international conference which will serve as the official meeting of the International Indian Statistical Association (IISA). Held January 2-5, 2013 in Chennai, India, this conference provides the international statistical community a unique opportunity to discuss new areas of research and exchange ideas.  The conference will include a strong mentoring component for students and young researchers working in areas in Probability and Statistics.  The SSSNCO conference theme is very aptly titled the Impact of Statistical Science on Society, and the conference will feature a discussion of new challenges and opportunities for statisticians in a data-centric world. The conference aims to provide a forum for leading experts and young researchers to discuss recent progress in statistical theory and applications, thereby providing new directions for statistical inference in various fields. The conference will have an excellent slate of plenary, invited and contributed talks, and poster sessions, covering a wide variety of topics of current statistical relevance and research activity.  These include, but are not limited to, Bayesian Computing, Bioinformatics, Biomedical applications, Clinical Trials, Data Mining, Multiple Comparisons, Nonparametric methods, Reliability, Stochastic Modeling, Survival Analysis, and Time Series.  In addition, panel discussions and student paper competitions will be held, providing an opportunity for junior faculty, graduate and undergraduate students not only to showcase their research work but also to benefit from some special sessions devoted to statistics education and funding opportunities, all of which should lead to acceleration and enhancement of research for participants.<br/><br/>The main objective of the 2013 SSSNCO conference is to bring together both well established and emerging young researchers from around the world who are actively pursuing theoretical and methodological research in statistics and their applications in various related fields. Workshops, focusing on research and education, with a strong focus on mentoring students and young researchers, have been planned at the conference. The mentoring component in education and research will be strongly emphasized. These goals will have a strong impact on researchers and educators in probability and statistics, especially the next generation of professionals by interaction with peers and senior professionals from international institutions.  The funds will be used to provide partial travel support for students and junior researchers from US academic institutions to attend the conference. While selecting recipients, special consideration will be given to women and minority students. Participants receiving support will be required to actively take part in the conference by presenting a poster or a talk, as well as attending the workshops and the panel discussions.<br/><br/>Conference web page:<br/>http://www.iisaconference.info/"
"1209022","Statistical Inferences, Computing, and Applications of Semiparametric Accelerated Failure Time Models","DMS","STATISTICS","08/15/2012","08/14/2012","Jun Yan","CT","University of Connecticut","Standard Grant","Gabor Szekely","07/31/2016","$129,999.00","Sangwook Kang","jun.yan@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","","$0.00","Accelerated failure time (AFT) models are much less utilized in practice than relative risk models because of difficulty in inference and limited availability in standard software.  The investigators develop 1) generalized estimating equations (GEE) for multivariate AFT models with application to adolescent depression, 2) induced smoothing rank-based approach and least squares approach for AFT models with covariates missing by design, 3) regularized estimation for AFT models with high dimensional covariates, and 4) an open source, high-quality, and user-friendly software implementation for inferences with AFT models.  The GEE approach is incorporated into an iterative procedure to estimate the regression coefficients in multivariate AFT models, initializing from a consistent and asymptotically normal estimator obtained with induced smoothing.  Inferences with covariates missing by design proceed with appropriately constructed selection weights for estimating functions.  Regularized estimation is done by minimizing an objective function, where three novel choices of risk functions are combined with a variety of penalty functions, including nonconvex ones such as minimax concave penalty.  Software implementation will be made available as R packages.<br/><br/>Methodological development on AFT models is far behind that on relative risk models due to computational and inferential challenges. The investigators shorten the gap with a comprehensive collection of methodologies and software implementation for AFT models in practical settings that are frequently encountered in biomedical, epidemiological, and social science studies. The methodologies and software implementation are expected to have an influential impact on the practice of failure time modeling.  The open source implementation provides a realistic alternative to the relative risk model for censored data regression.  Applications of the methods to ongoing collaborative projects that motivated the proposed research have cross-boundary effects.  A bivariate AFT model for the duration of depression and the duration of major stressors offers a novel perspective to gain insight into onset and maintenance of depressive episodes. The project is naturally integrated with education through undergraduate/graduate student thesis advising, graduate level courses, and short courses at conferences in both the statistics community and the psychology community. The publicly available software makes the cutting-edge statistical methodology accessible to those who need them in scientific discoveries."
"1206522","First Conference of the International Society for NonParametric Statistics","DMS","STATISTICS","03/01/2012","02/14/2012","Dimitris Politis","CA","University of California-San Diego","Standard Grant","Gabor Szekely","02/28/2013","$18,000.00","","dpolitis@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","7556","$0.00","A 5-day international conference,``The First Conference of the International Society for NonParametric Statistics'' is being planned to be held in Halkidiki, Greece, June 15-19, 2012.<br/><br/>NonParametric Statistics is not a new subject. Nevertheless, it has become more prominent in the 21st century since data have become abundant, and the computing power to analyze them is generally available. The aim of the PI and his collaborators is to bring together researchers from the U.S., Europe and other parts of the world in order to foster research on nonparametric inference with special emphasis on emerging fields such as:<br/>a) Nonparametric methodology for high dimensional data.<br/>b) Bootstrap, Empirical Likelihood, and related methods.<br/>c) Data depth and and related multivariate nonparametric methods.<br/>d) Dimension reduction, model selection, and related methods.<br/>e) Nonparametric machine learning.<br/>f) Nonparametric models for high dimensional mixed and random effects designs. <br/>g) Empirical process and semiparametrics.<br/>h) Bayesian Nonparametrics.<br/>i) Nonparametric methods for time series and Econometrics.<br/>j) Nonparametric methods for spatial processes.<br/>k) Wavelets and nonparametric functional estimation.<br/>l) Nonstandard Asymptotics in nonparametric inference.<br/><br/>The purpose of the conference is to facilitate the exchange of research ideas on all actively growing areas of nonparametric statistics, thereb giving the development of this important field of statistics a timely boost. This will also be the first conference of the newly formed International Society for NonParametric Statistics (ISNPS) whose mission is ""to foster the research and practice of nonparametric statistics, and to promote the dissemination of new developments in the field via conferences, books and journal publications."" ISNPS has a distinguished Advisory Committee that includes R.Beran, P.Bickel, R. Carroll, D. Cook, P. Hall, W. Hardle, R. Johnson, B. Lindsay, E. Parzen, P. Robinson, M. Rosenblatt, G. Roussas, T. SubbaRao, and G. Wahba, as well as a Charting Committee consisting of over 50 prominent researchers from all over the world. In particular, theprogram will provide a forum for junior and senior researchers alike to gain valuable experience interacting with leaders in their respective sub-areas,and for building new collaborations between researchers in Nonparametric Statistics from the US with researchers from all over the world."
"1205296","Regularization for High Dimensional Inference and Sparse Recovery","DMS","STATISTICS","09/01/2012","07/30/2014","Jelena Bradic","CA","University of California-San Diego","Continuing Grant","Gabor Szekely","08/31/2015","$120,000.00","","jbradic@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","","$0.00","This proposal aims at answering pertinent questions to identifying sparse subsets of high dimensional covariate spaces, in the context of regularization methods, when simple least square loss functions are not well suited. The broad goal is to understand the fundamental interactions between nonlinear and/or censored structure of the statistical model, the regularization scheme and the intrinsic dimensionality of the problem. Specifically, the PI aims at (1) Identifying new statistical problems and regularization schemes, with complex nonlinear and time-to-event structure with high dimensional covariate space that are tuned to the characteristics of the data; (2) Developing novel non-asymptotic oracle bounds on the behavior of regularized estimators where techniques of random matrix theory, especially high probability bounds on various matrix norms, and approximation theory, will be utilized to enhance understanding of the effects of dimensionality on the non-asymptotic properties; (3) Investigating  new non-asymptotic  bounds on risk of  semiparametric methods  where the statistical model is possibly misspecified; (4) Analyzing and developing models that use special interplay between censoring rate, sample size and dimensionality of the problem and importantly  (5) Introducing new algorithms that optimally and efficiently solve the investigated large scale problems.<br/><br/><br/>Explosion of microarray technologies has lead to vast number of large-scale   genome-wide association studies where simultaneous analysis of a large number of SNPs is pertinent to discovering genetic identification of complex diseases. Presence and importance of time to event component calls for significant advances in statistical methodology for both NP dimensionality and censored structure. This research proposal aims at developing innovative and effective statistical methods for such complex data with special impact in genetic, public health and bioinformatic sciences, where censoring and vast number of gene interactions make identification of misbehaving genes very difficult. Moreover, the developments of this inter-disciplinary project will enhance new scientific discoveries, make new collaborative connections with practitioners and will promote teaching and training of graduate students on the contemporary state-of-the-art machine learning techniques applied to semiparametric models and censored data. To promote the progress of science, PI will make explicit collaborations of department of Mathematics, Biostatistics division of the Medical School at UCSD and Supercomputer Center in San Diego. Through dissemination of the results of this proposal, PI plans to expose biology to mathematics majors and promote science among underrepresented groups and women in mathematics."
"1205158","14th Meeting of New Researchers in Statistics and Probability","DMS","STATISTICS","01/01/2012","12/08/2011","Ery Arias-Castro","CA","University of California-San Diego","Standard Grant","Gabor Szekely","12/31/2013","$24,713.00","","eariasca@math.ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","7556","$0.00","This proposal describes plans for the 14th Meeting of New Researchers in Statistics and Probability, a conference series sponsored by the Institute of Mathematical Statistics (IMS), to be organized by and held for junior researchers.  The primary objective is to provide a much needed venue for interaction among new researchers.  The proposed meeting will take place over three days, July 26-28, 2012, at the University of California, San Diego, in La Jolla, California.  The meeting will be held immediately preceding the Joint Statistical Meetings (JSM) in downtown San Diego.  Participants will be statisticians and probabilists who have received their Ph.D. within the past five years or are expecting to receive their degree within the same year.  Each participant will present a talk or poster.  Topics will cover a variety of areas in statistics and probability, from theory and methods to applications.  Senior speakers will give plenary talks for inspiration and other senior researchers will take part in discussion panels covering topics of importance for young people embarking on an academic/research career, such as teaching, mentoring, publishing and funding.<br/><br/>This conference series is explicitly aimed at training the future leaders and workers in statistics and probability.  In helping to create networks of new researchers, it lays the groundwork for future collaboration and the informal exchange of ideas and knowledge.  It is also critical for professional cohesion, which is particularly important as research fields become more and more specialized.  Meeting people outside of one's research specialty area, and learning about their research, favors a more comprehensive view of research, which is important when taking editor positions and other professional service activities.  The conference also attracts participants with different backgrounds and nationalities, and underrepresented groups --- women, minorities and people with disabilities --- are explicitly encouraged to attend.<br/>Conference website: http://www.math.ucsd.edu/~nrc2012/index.html"
"1205185","2012 Dr. David Blackwell Memorial Conference","DMS","PROBABILITY, STATISTICS","02/15/2012","01/25/2012","Abdul-Aziz Yakubu","DC","Howard University","Standard Grant","Gabor Szekely","01/31/2014","$35,000.00","Tepper Gill","ayakubu@howard.edu","2400 6TH ST NW","WASHINGTON","DC","20059","2028064759","MPS","1263, 1269","7556","$0.00","To honor Dr. David Blackwell's numerous contributions to mathematics and statistics, the American Statistical Society, the University of California at Berkeley, and Howard University convene the David Blackwell Memorial Conference at Howard University on April 19-20, 2012 in the Blackburn Center. Dr. David Blackwell was an outstanding African-American mathematician and statistician who made profound contributions to several areas of mathematics, game theory, probability theory, information theory, dynamic programming, logic and statistics.  This significant event organized by three collaborating institutions simultaneously recognizes Dr. Blackwell's legacy as well as advances in mathematics and statistics. The two-day conference brings together leading theoretical and applied mathematicians, statisticians, and other scientists to:<br/>(1)    Recognize and acknowledge Dr. Blackwell's myriad contributions to scholarship in multiple areas and disciplines.<br/>(2)    Discuss how Dr. Blackwell's work shapes and influences the field-at-large and their respective scholarship.<br/>(3)    Explore and develop new ideas and approaches to questions arising from mathematics and statistics.<br/>(4)    Provide a forum for a diverse group of undergraduate students, graduate students, postdoctoral fellows and junior faculty to interact and collaborate with experts in the field of mathematics and statistics.<br/><br/>Evidence of Dr. Blackwell's intellectual contributions is visible in many arenas and scholars continue to build on that knowledge base. In so doing, conference participants continuously underscore and reinforce the intellectual merit of Dr. Blackwell's and their own work.  The conference is an opportunity for intellectual exchange and discourse among scholars from diverse backgrounds.  As new generations of scholars from increasingly diverse backgrounds enter the fields of mathematics and statistics and more broadly, science, technology, engineering and mathematics (STEM), through the Dr. David Blackwell Memorial conference they become members of the community of scholars who innovate and make breakthroughs within their respective fields. The conference website is  http://sites.google.com/site/conferenceblackwell/"
"1208225","Constrained Group Selection and Structure Estimation in Semiparametric Models","DMS","STATISTICS","07/01/2012","05/30/2012","Jian Huang","IA","University of Iowa","Standard Grant","Gabor Szekely","06/30/2015","$159,681.00","","jian-huang@uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1269","9150","$0.00","This application proposes a class of novel constrained group selection methods in high-dimensional models when there are natural constraints on the parameters. The proposed project is expected to stimulate new research directions for studying several important statistical modeling and analysis problems, which include structure estimation and variable selection in semiparametric additive models, varying coefficient models,  and survival analysis models  in high-dimensional settings where the number of variables is larger than the sample size. The proposed project will also yield new methods for integrative analysis of multiple genomic datasets and genome wide association studies. Theoretical properties of the proposed methods in high-dimensional settings and computational algorithms will be developed. Analysis of high-dimensional data presents new and challenging theoretical and computational questions in statistics. Standard methods assuming the number of variables  is fixed and much smaller than the sample size are not applicable to high-dimensional models. The proposed methods are expected to be able to correctly select the important groups and correctly estimate model structures with high probability in sparse, high-dimensional settings. <br/><br/>High-dimensional data arise in many diverse fields of sciences and humanities, including biology, economics, finance, information technology, and health sciences. In all these fields, feature selection is a crucial step in the process of knowledge discovery from data. In genetic and genomic research, with rapid advances in biotechnology, more and more big data sets are being generated. The identification of statistically and biologically significant patterns from high-dimensional and noisy data sets is becoming a major challenge. The development of statistical methods that can deal with high-dimensional problems in estimating the relationship between clinical outcomes and genetic data will contribute to better understanding of the genetic basis of diseases, better diagnoses, and better survival prediction. The proposed methods will be applied to the analysis of high-dimensional censored survival data, longitudinal data, genome wide association studies (GWAS) and integrative analysis of multiple genomic datasets. Censored and longitudinal data arise in many clinical and biomedical studies. GWAS and integrative analysis are important methods for identifying disease susceptibility genes for common and complex diseases. The ultimate goal of clinical and genetic research is to understand the relationships between risk factors and phenotypes for developing new approaches to prevention, diagnosis and treatment of disease. This project aims to translate novel statistical approaches into new methodologies for analyzing high-dimensional clinical and genomic data that are important in achieving this goal. The methods and results from the proposed project will be incorporated into a graduate course on high-dimensional data analysis. The investigator will broadly disseminate the results to the scientific community by submitting papers to scientific journals and making them and the computer programs publicly available on the internet. The investigator will also present the results in scientific conferences and workshops."
"1208857","High-dimensional structured regression","DMS","STATISTICS","08/01/2012","08/19/2014","Jonathan Taylor","CA","Stanford University","Continuing Grant","Gabor Szekely","07/31/2016","$299,892.00","","jonathan.taylor@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","This project is focused on structured high dimensional regression with the term structure meant to distinguish the methods from other methods such as l1 minimization methods. Generally speaking, this structure is assumed a priori and is chosen on the basis of finding an interpretable solution to a regression problem. In this project, structure often refers to spatial structure found in areas of application such as neuroimaging or astronomical data. The project has two principal goals. First to develop scalable, flexible algorithms and software implementations for fitting such structured models. Secondly, to understand the statistical performance of such models as well as the algorithms used to fit such models.<br/><br/>The results of the research proposed in this project will allow researcher in the field of neuroscience to improve neuroscientists' ability to predict behavior based on fMRI or other spatio-temporally structured data."
"1208787","Statistical Theory and Methodology","DMS","STATISTICS","07/01/2012","05/27/2014","Bradley Efron","CA","Stanford University","Continuing Grant","Gabor Szekely","06/30/2016","$499,912.00","Persi Diaconis","brad@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","Statistical Theory and Methodology<br/>Persi Diaconis and Bradley Efron<br/><br/>The long-term goal of this project has been the development of useful tools for scientific inference, both in probability and statistics. The present proposal focuses on exponential families of probability distributions, especially as they apply to complex applications such as graphical models and high-dimensional Bayesian/frequentist computation. Specific projects include exponential families for graphs, including the effects of covariate information, and ways of using the frequentist bootstrap to compute Bayesian inferential distributions.<br/><br/>Exponential families include most of the well-known probability distributions -- binomial, Poisson, normal etc, -- so exponential family results have a wide scope of application. They live at the core of applied and theoretical statistics, and are of growing importance in probability theory. Graphical models, which include studies of the internet, gene expression data, and social networks such as Facebook, use exponential family structure to simplify analysis of the enormous data sets involved. Bayesian applications of the theory, increasingly popular in the ""Big Data"" world, are investigated here in terms of the bootstrap, a frequentist device, with the goal of connecting the two approaches."
"1208164","Flexible and Adaptive Statistical Modeling","DMS","STATISTICS","06/01/2012","06/16/2015","Robert Tibshirani","CA","Stanford University","Continuing Grant","Gabor Szekely","05/31/2016","$499,968.00","","tibs@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","The investigator and his colleagues will study selected topics in high-dimensional regression, classification and hypothesis testing. Specifically, these include a sparse factor model for high-dimensional regression and survival analysis, large scale interaction testing with applications to genomics, the computational algorithms for the fused lasso, and a research monograph on L1-constrained (Lasso) methods in statistics.<br/><br/>This work will help scientists working in biotech and other areas, who generate large scale datasets, to interpret and uncover the important patterns in their data. This should help scientists and doctors to discover the biological bases of many diseases, and improve prognosis and treatment selection for patients."
"1249556","Travel Support for the 59th Session of the International Statistical Institute, Hong Kong","DMS","STATISTICS","09/15/2012","09/01/2012","Ronald Wasserstein","VA","American Statistical Association","Standard Grant","Gabor Szekely","08/31/2013","$22,846.00","","ron@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","7556","$0.00","The American Statistical Association (ASA) will use this NSF award as a travel grant for approximately 10 United States participants to attend the 59th Session of the International Statistical Institute (ISI) in Hong Kong from August 25-30, 2013.  The ISI meeting includes the meetings of the Bernoulli Society, International Association for Official Statistics (IAOS), International Association for Statistical Computing (IASC), International Association of Survey Statisticians (IASS), and the International Association for Statistical Education (IASE).  Thus it is an umbrella meeting with sessions of interest for thousands of statisticians.  The travel grants will provide support to defray transportation costs for individuals selected from institutions and non-profit associations. An emphasis of the award is to encourage and provide the opportunity for younger statisticians to participate in the meeting.<br/> <br/>Participants will be notified of the availability of the travel grant through Amstat News, a membership publication of the ASA, and on the ASA Home Page on the Internet. Notices will be provided to university and college statistics and mathematics departments to encourage younger statisticians to apply for the travel grant. A review and selection committee will be established to review the applications and select grantees.  The committee will be comprised of three or more ASA members and will convene at the ASA office in Alexandria, Virginia.  Special consideration will be given to statisticians who have recently received their Ph.D.'s and to women and minorities.<br/><br/>ISI World Statistics Congresses web page:<br/>http://www.isi-web.org/about-isi/isi-wscs"
"1208273","Application of Statistics in Assessing Nanoreliability","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","08/15/2012","05/13/2014","Nader Ebrahimi","IL","Northern Illinois University","Continuing Grant","Gabor Szekely","07/31/2016","$130,000.00","","nader@math.niu.edu","1425 W LINCOLN HWY","DEKALB","IL","601152828","8157531581","MPS","1253, 1269","","$0.00","Modern programs for improving reliability of existing nanocomponents and nanosystems, and for assuring continued high reliability for the next generation of these systems, require stochastic modeling and statistical methods for predicting and assessing various aspects of the reliability. Recent work, by using on/off type models that capture interactions between atoms of a nanocomponent, arrived at an integrated probabilistic model, which was used to assess reliabilities for two nanocomponent structure functions. Also, by using the notion of copula, a model that captures interactions between atoms was developed and used to assess the reliability function of a nanocomponent having the series structure. A goal of this proposal is to extend and refine both models by accounting for the more complex dependencies among atoms and assess nanocomponents reliability/reliability function with different structure functions based on those models. The proposed modeling framework will advance understanding of nanocomponents failure in different environments and will lead to novel approaches to assessing nanocomponents reliability/reliability function. Another direction of proposed research aims to assess nanosystems reliability/reliability function. Motivated by palladium nanowire nanonetwork-based hydrogen sensors, the original aim was to assess the reliability of a nanonetwork. Here, the nanonetwork plays a role of a nanosystem which is composed of ultra palladiuim nanowires where each nanowire is a nanocomponent of the nanonetwork/nanosystem. The nanonetwork topology was assumed to be a square lattice and the reliability, which differs from the traditional definition, is defined as the probability that a nanosystem percolates for a certain number of cycles of hydrogen exposure without failure. The investigator proposes to extend and refine previous work by accounting for interactions that might exist between nanocomponents as well as considering different topologies for a nanosystem. These advances will not only provide methods to assess reliability for specific nanosystem/nanonetwork topologies, but also will bring about new perspectives to nanosystem's reliability evaluation.<br/><br/>Nanoproducts have great potential in many industrial applications that involve electronics, sensors, solar cells, super-strong materials, coatings, drug delivery and medicine. In today's market, the number of consumer products with nano-sized components and systems is growing exponentially. Similar to nanotechnology's success in consumer products and other sectors, nanoproducts have the potential also to improve the environment by direct applications to detect, prevent and remove pollutants as well as indirectly by using nanotechnology to design cleaner industrial processes and create environmentally responsible products. Due to the fact that nanoproducts account for a high proportion of costs, knowing the reliability of such products is necessary to guarantee the advancement of nanotechnology. This project represents a multi-disciplinary effort that develops stochastic models and statistical tools for assessing nanoproducts reliability. This, in turn, will lead to a better understanding of design optimization against reliability which is always crucial in technology and will be even more so in complex nanotechnologies."
"1206321","Statistical Methodology for Stochastic Systems with Parameters Jumps and Applications to Economics, Genetics and Engineering","DMS","STATISTICS, COFFES","09/01/2012","08/29/2012","Haipeng Xing","NY","SUNY at Stony Brook","Standard Grant","Gabor Szekely","08/31/2016","$184,257.00","","haipeng.xing@stonybrook.edu","W5510 FRANKS MELVILLE MEMORIAL L","STONY BROOK","NY","117940001","6316329949","MPS","1269, 7552","","$0.00","Statistical inference problems in complex stochastic systems with parameter jumps arise in science and engineering, including economics, finance, genetics, industrial quality control, and public health. An important ingredient in the solution to these problems is efficient estimation of time-varying parameters with unknown jumps. In the proposed research, the investigator studies some newly emerged stochastic models with unknown parameter jumps in different disciplines and develops the related inference procedure. In particular, four types of problems in different areas are studied in the proposal. The first investigates a semi-parametric change-point regression model and its inference procedure for abrupt changes of covariate effect in longitudinal studies. The second develops a credit rating transition model in the presence of unknown structural breaks and an estimation procedure for the analysis of the relationship between the structural breaks in the U.S. credit market and macroeconomic and firm-specific covariates. The third considers a class of Markov switching models with stochastic regimes and their applications in economic analysis of business cycles and recurrent copy number variation analysis in genomic studies. The fourth problem discusses surveillance rules in sequential surveillance problems and their applications in risk management. The investigator will show how these challenging problems in different areas can be unified and solved by the developed statistical models and inference procedures.   <br/><br/>Complex stochastic systems with unknown parameter jumps are often encountered in various scientific and engineering practices including economics, finance, biology, risk management and control. While systems with smoothly changing parameters have been discussed intensively in the literature, recent advances in natural and social sciences show the growing importance of stochastic systems with unknown parameter jumps. In current genomic research, DNA copy number variations are key genetic events in the development and progression of numerous diseases including cancer, HIV acquisition, and Alzheimer and Parkinson's disease, and an important step in studying these genetic events is to identify the regions of variations. In economic studies, the authorities are keen to have a more detailed and quantitative characterization of the real economic states, instead of some simple descriptions such as booming or recession that are commonly discussed in the economic literature, so that proper monetary and fiscal policies can be issued. In financial studies, the 2008-2009 financial crisis raises the immediate needs for the regulatory authorities that the credit market and banking systems should be regulated and monitored based on solid statistical and econometric models and procedures, and hence an early warning system should be established to surveillance the stability of financial and economic systems.  The proposed research is one of the first attempts to explore the possibility of building quantitative and implementable early-warning systems for financial markets and economic activities, which aggregates microeconomic information among individual firms and banks and macroeconomic statistics from general economic activities."
"1208994","Collaborative Research: Estimation, Inference, and Computation for Finite Nonparametric Mixtures","DMS","STATISTICS","08/15/2012","06/02/2014","Michael Levine","IN","Purdue University","Continuing Grant","Gabor Szekely","07/31/2016","$86,537.00","","mlevins@stat.purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1269","","$0.00","This project aims to develop theory and methods for estimation of the functional components and weights in the nonparametric multivariate finite mixtures. These mixtures only assume that the components are drawn from some family of multivariate density functions without any parametric specification. The investigators, who are already active in this emerging area of research, adapt a number of estimation methods that are known from finite parametric mixture theory to the nonparametric context. The PI and the co-PI propose a number of practically feasible and fast algorithms that can be used to compute the resulting estimators in practice. Finally, both investigators show how to obtain large-sample asymptotic results for the proposed estimators.<br/><br/>Finite nonparametric mixtures of distributions can provide answers to many practically important questions. As an example, they can be used to help a physician in establishing the definitive diagnosis in case of a complex medical condition with a number of possible diagnoses. An example of such a situation is a patient with a possible heart attack where other differential diagnoses are also possible. Developmental psychology provides another useful example. Indeed, study of cognitive development in children, in particular identification of strategies used by children to accomplish various tasks, can also be modeled easily using these mixtures. This has important implication for developmental psychology, providing answers to many difficult questions faced by child psychologists while helping children mature and develop in an optimal way. The PI and the co-PI propose a number of efficient algorithms to estimate these mixtures and accomplish the practical tasks mentioned above. These algorithms will be publicly available and easy to use as part of the R software package called mixtools."
"1207759","Nonparametric Prediction and Structure Discovery for Spatial Dynamics","DMS","STATISTICS","09/01/2012","07/30/2014","Cosma Shalizi","PA","Carnegie-Mellon University","Continuing Grant","Gabor Szekely","08/31/2015","$180,000.00","","cshalizi@cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","This project develops new methods for non-parametric prediction, filtering, and structure discovery, primarily for spatio-temporal data but also in a range of other settings with high-dimensional observations, such as networks.  There are two novel aspects to the investigation's approach.  First, rather than trying to predict spatio-temporal data globally, or according to a fixed pattern, it exploits the dynamics of the system to develop a novel form of local prediction which still captures long-scale structure.  Second, while conventional non-parametric smoothing is based on the usual geometry of the space of predictor variables, this is supplemented smoothing together input points which have similar predictive consequences, in effect discovering a new geometry.  This approach, which draws on earlier work on information theory in nonlinear dynamics, allows for accurate forecasting of the evolution of large spatio-temporal systems in a computationally efficient manner.  It also allows for the automatic discovery of complex higher-level structures in such data. <br/><br/>Scientific data increasingly comes as complex measurements spread over space and time.  Scientists need ways to forecast how such systems will evolve, and to automatically separate important (but perhaps subtle) patterns from inconsequential ""background"" of the system, since the structures are often crucial to understanding the dynamics.  This project tackles both of these challenging statistical problems together.  It combines idea from information theory and nonlinear physics with modern tools of flexible statistical modeling to discover the intrinsic dynamics of the system from the data itself, and uses these structures for both prediction and filtering.  Areas of potential application include neuroscience, fluid dynamics, and ecology, where it would help forecast the behavior of complex systems, and help to find the organized structures which are keys to controlling that behavior."
"1208354","Estimating Low Dimensional Structure in Point Clouds","DMS","STATISTICS","07/01/2012","05/29/2012","Isabella Verdinelli","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","06/30/2016","$232,150.00","Larry Wasserman, Christopher Genovese","isabella@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","This project will develop computationally efficient estimation methods with accompanying theory for the problem of identifying low-dimensional structure in point-cloud data, both low and high dimensional.  A canonical example is a noisy sample from a manifold. The investigators will develop minimax lower bounds for the estimation problem and construct estimators that achieve these lower bounds. They will then implement these methods in a practically useful form nd apply them to several important scientific problems.<br/><br/>Datasets sometimes contain hidden, low-dimensional structure such as clusters, filaments and low dimensional surfaces. The goal of this project to develop rigorously justified, computationally efficient methods for extracting such structure from data.  The developed methods will be applied to a diverse set of problems in astrophysics,seismology, biology, and neuroscience.  The project will advance knowledge in several fields including computational geometry, machine learning, and statistics."
"1208983","Collaborative Research: Statistical Modeling and Inference for High-dimensional Multi-Subject Neuroimaging Data","DMS","ADVANCES IN BIO INFORMATICS, STATISTICS, Cross-BIO Activities, MSPA-INTERDISCIPLINARY","09/01/2012","08/30/2012","Fan Li","NC","Duke University","Standard Grant","Gabor Szekely","08/31/2015","$71,100.00","","fli@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1165, 1269, 7275, 7454","1165, 8007","$0.00","This project consists of two components, each motivated by the inference problem for functional magnetic resonance imaging (fMRI) data. In the first part, within the framework of generalized functional linear model (GFLM), a flexible semi-parametric model for neural hemodynamic response in the form of slope functions is introduced. To accommodate the variation of brain activity across different regions, stimulus types, and subjects, the new approach assumes the slope functions share the same but unknown functional shape for a given region and stimulus, while having subject-specific height, time to peak, and width.  Several fast algorithms based on B-spline smoothing are proposed to estimate the model parameters for whole-brain analysis. The second part of the research focuses on building a novel Bayesian variable selection framework to study the relationship between individual traits and brain activity. The spline estimates of the brain hemodynamic responses from the first part are taken as predictors in a regression model where the response is the individual traits. Two types of priors are introduced jointly to achieve simultaneous variable selection and clustering.<br/><br/>FMRI is one of the most effective neuroimaging technologies for understanding brain activity. In recent years, fMRI data collected from complex studies with multiple subjects have been widely used in psychological and medical research. This project will provide tools for modeling, analysis and computation for this type of fMRI data. Project findings will advance basic understanding of the inter-relations between nature and nurture in shaping individual differences in brain function and behavior, and suggest new directions for interdisciplinary research that combines statistics, neuroscience and psychology. The open source R/Matlab software developed from the research will provide valuable data analysis and educational tools for the scientific community."
"1207711","Association, Regression and Diagnostic Accuracy Analyses of Competing Risks Data","DMS","STATISTICS","09/01/2012","08/29/2012","Yu Cheng","PA","University of Pittsburgh","Standard Grant","Gabor Szekely","08/31/2015","$99,935.00","","yucheng@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","","$0.00","Competing risks commonly occur in analyzing time-to-event outcomes with composite endpoints. Due to dependent censoring imposed by competing events, standard methods for dealing with usual independent censoring, such as censoring imposed by time limits on the duration of observation, may not be applicable. In this proposal, the investigator discusses two projects that address challenges arising from the analyses of competing risks data. The first project aims to quantify the association between two lung infection times using the Cystic Fibrosis Foundation registry data, where the event times are left truncated and competing-risk censored. Conditional cause-specific hazard (CSH) functions and conditional cumulative incidence function (CIFs) are considered to incorporate left truncation. An extended Dabrowska method is proposed to estimate the bivariate conditional survival function, and then used to estimate the bivariate conditional CIF. Nonparametric association analysis is subsequently carried out based on association measures that are quantified through conditional cumulative CSH functions and CIFs. The goal of the second project is to explore an important intrinsic relationship between CIFs in a regression setting, and propose a flexible parametric regression model that explicitly takes into account the additivity constraint on the CIFs. The parametric model adopts a modified logistic model as baseline CIFs and a generalized odds-rate model for covariate effects. This model explicitly takes into account the constraint that a subject with any given prognostic factors should eventually fail from one of the causes so the asymptotes of the CIFs should add up to one. <br/><br/>There is limited research on association analysis of bivariate competing risks data and no prior work for left-truncated competing risks data, a common situation when registry data are used to quantify the association between two events of interest. Regression models based on CIFs have been well studied to evaluate covariate effects on the event of interest in the presence of competing-risk censoring. However, existing methods do not explicitly account for the additivity constraint on CIFs, resulting in interpretation issues. The proposed two projects address each of these methodological gaps and are expected to enhance our understanding of the two areas. The proposed projects have been motivated by real problems that the PI encountered in collaborations with researchers from other areas, and are designed to address those practical issues. The projects can be applied in such diverse fields as medicine, public health, reliability studies in engineering, actuarial sciences and finance. The PI is actively working with graduate students and expects some of them will get involved with the proposed research for their dissertations. Hence the proposed work will be naturally integrated with education through graduate student advising and training."
"1208959","A New Paradigm in Joint Registration, Analysis and Modeling of Function Data","DMS","STATISTICS","07/01/2012","05/21/2012","Anuj Srivastava","FL","Florida State University","Standard Grant","Gabor Szekely","06/30/2015","$250,000.00","Wei Wu","anuj@stat.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","","$0.00","The dual problems of registration and analysis of functional data are very important in statistical analysis.  They play important roles in applications involving phase-amplitude separation of real-valued functions, shape analysis of curves and surfaces, registration of 2D and 3D images, and analysis of longitudinal data with values on nonlinear manifolds. The PIs develop a novel framework that will lead to statistically consistent functional analysis and will substantially improve algorithmic performances over the current methods. The key novelty is to use Riemannian methods and distance-based objective functions that are: (1) designed for measuring registration levels of functions explicitly, (2) studied in the quotient spaces of functions modulo the registration groups, and (3) formulated for ensuing statistical analysis/modeling. Here one represents registration variability by actions of the domain-warping groups on function spaces, chooses an appropriate Riemannian metric (such that the group actions are by isometries) and studies novel mathematical representations that enable statistical analysis under such metrics. The main advantage is that both registration and analysis, e.g, computation of mean, covariance, PCA, are performed jointly under the same metric rather than the current practice of using sequential and disjoint steps. Preliminary results on some subproblems including real-valued function registration and shape analysis of curves are shown to be superior, both empirically and theoretically, to the current approaches. The goal is to broaden this research to a larger class of registration problems with similar fundamental solutions. <br/><br/>High-dimensional functional data is becoming omnipresent in today's society and one needs to nonlinearly align such observations in time and space, in order to improve data analysis, statistical modeling, and inferences. This project represents a multi-disciplinary effort that will develop both basic statistical science and computational tools for registration of functional data. This, in turn, will impact such data-rich applications as development of accurate growth charts for children, gene expression analysis, face recognition using images and videos, detection and evaluation of brain disorders (e.g. Alzheimer) using medical images, and human activity recognition using surveillance video data. The novelty and potential high returns of this project come from the variety of tools utilized---from differential geometry and statistics to imaging science."
"1206464","Statistical Inferences on Massive Data","DMS","STATISTICS","06/01/2012","04/25/2016","Jianqing Fan","NJ","Princeton University","Continuing Grant","Gabor Szekely","05/31/2018","$600,000.00","","jqfan@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","","$0.00","The proposal plans to develop novel statistical theory and methods for processing massive data.  Four interrelated avenues are proposed for theoretical research and methodological developments:  High-dimensional variable selection, large covariance estimation, large-scale hypothesis testing, and nonparametric statistical learning. In particular, novel statistical techniques are proposed to answer the following important questions:  how to screen genes and risk factors with some acquired knowledge, what are the advantages of folded concave penalized methods, how to estimate the benchmark for classifications and regressions, how to deal with outliers, dependence data, and endogenous measurements, how to use homogeneity of geographical neighborhoods to enhance forecasting and inferences, how to assess uncertainty of risk measurements, how to conduct sparse principal component analysis, how to control the false discovery rates under arbitrary dependence, how to use nonparametric methods to enhance the flexibility of high-dimensional statistical learning.  In addition, a novel statistical model, motivated from a financial economics theory, is proposed for estimating large covariance matrices for better understanding risk correlations and for better assessment of risks.  The methods for testing the presence of endogenous variables and the celebrated multifactor pricing models are also presented and will be thoroughly investigated. <br/><br/>Massive data collections have become routine in exploring the frontiers of science, in one case genomic studies and in another case measuring economic risks.  The proposed research will advance our knowledge on understanding molecular mechanisms, biological processes, genetic associations, brain functions, social networks, economic and financial risks, supply and demands, and hence increase economic and global competitiveness. In addition, the proposed novel statistical techniques can be applied to other biological and engineering problems. The project will integrate research and education by working closely with senior undergraduate students, graduate students and postdoctoral fellows, and increase the collaborations between academia and industry by working closely with industrial partners and developing publicly available computer code for processing massive data with sound theoretical supports. The results will be disseminated broadly through presentations at seminars, conferences, professional association meetings, and the internet."
"1208853","Collaborative Research: Axially symmetric processes and intrinsic random functions on the sphere","DMS","STATISTICS","09/15/2012","09/08/2012","Chunfeng Huang","IN","Indiana University","Standard Grant","Gabor Szekely","08/31/2015","$153,864.00","Scott Robeson","huang48@indiana.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","MPS","1269","","$0.00","In spatial statistics, a wide variety of methods and models have been developed in Euclidean spaces. Additional theory and methods are needed for analyzing processes and phenomena on the sphere, many of which are of utmost importance in the geophysical sciences. Data from global networks of in situ and satellite sensors, for instance, are used to monitor a wide array of important climatological variables such as temperature and precipitation. The goal of this project is to study random processes on the sphere beyond the usual homogeneity assumption using two approaches. The first approach is to consider axially symmetry on the sphere, where the random process exhibits longitudinal symmetry rather than being rotation invariant on the entire sphere.  The second approach of this research extends the intrinsic random functions and generalized covariance functions to processes on the sphere. The notion of intrinsic random functions has been developed in order to handle a process with an unknown and possibly non-constant mean function while preserving a linkage to stationarity. The investigators plan to achieve a fundamental understanding of the covariance structures of the non-homogenous processes on the sphere and to develop associated estimation procedures. In addition, these approaches are extended to more sophisticated situations including multivariate random processes and spatio-temporal processes on the sphere. All of these methods and models are applied to global-scale temperature data from surface and satellite sensors.<br/><br/>The models and methods developed here provide new tools for improving our understanding of global-scale phenomena in general and multi-decadal temperature variations in particular.  The motivation of this project comes from a desire to understand the statistical characteristics of global-scale temperature variations during the instrumental (1880s onward) and satellite (1979 onward) periods. As a result, two important and widely used data sets are analyzed here:(1) tropospheric temperature data from National Oceanic and Atmospheric Administration satellite-based Microwave Sounding Unit and (2) surface-based instrumental data from the Hadley Centre in the United Kingdom and Climatic Research Unit at the University of East Anglia. The geostatistical analysis of these data sets on the sphere provides profound information on the state of our changing planetary environment. The project also establishes and encourages research and education collaborations between Indiana University and Mississippi State University."
"1216466","A NISS/ASA Writing Workshop for New Researchers","DMS","STATISTICS","06/15/2012","05/31/2012","Rebecca Nichols","VA","American Statistical Association","Standard Grant","Gabor Szekely","12/31/2013","$22,000.00","","rebecca@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","7556","$0.00","This award will support a Writing Workshop for Junior Researchers, which will be held at the Joint Statistical Meetings, July 29-August 2, 2012, in San Deigo, CA. The workshop will train junior researchers in effective technical writing. The goal is to help new researchers in the statistical sciences who seek to publish their research or to present their research plans in the form of grant proposals for federal funding. Researchers, especially new researchers, often have difficulty disseminating their research results not because of the quality of the research but rather because of inappropriate choices of publication venues for the particular research and/or because of poor presentation of technical material to the chosen audience. The National Institute of Statistical Sciences and the American Statistical Association will manage the Workshop. <br/> <br/>This workshop will open with tutorial sessions on the organization of material for a technical article or grant application, on technical writing techniques and on the specific missions and audiences of key journals in the statistical sciences. Then each participating new researcher will work individually with an experienced journal editor as mentor to address these issues on an individualized basis for draft of the new researcher's work in progress. Revisions following this guidance will be critiqued by the mentor to assure that the new researcher's implementation of writing techniques has been successful before the article or the grant proposal is submitted for review."
"1151692","CAREER: Bootstrap M-estimation in Semi-Nonparametric Models","DMS","STATISTICS","07/01/2012","04/04/2016","Guang Cheng","IN","Purdue University","Continuing Grant","Gabor Szekely","06/30/2018","$400,000.00","","guangcheng@ucla.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1269","1045","$0.00","The PI deals with the bootstrap inferential strategies for two broad classes of bootstrap methods in the context of semi-nonparametric models. As a general-purpose approach to statistical inferences, the bootstrap has found wide applications in semi-nonparametric models. Unfortunately, systematic theoretical studies on the bootstrap inferences are extremely limited, especially when the nonparametric component is not root-n estimable. Two classes of bootstrap methods are considered: the exchangeably weighted bootstrap (EWB) and the model-based bootstrap (also known as the parametric bootstrap). The PI proves that the EWB consistently estimates the asymptotic variance of the Euclidean estimate and is theoretically valid in drawing semiparametric inferences in the framework of penalized M-estimation. However, the EWB may become invalid in drawing inferences for nonparametric components. Hence, the PI considers the model-based bootstrap, and theoretically justifies it as an universally valid inference procedure for all the parameters in semi-nonparametric models. The proposed research also involves the development of advanced empirical processes tools. The above research lays the theoretical foundation for the general semi-nonparametric inferences via various bootstrap sampling schemes, and establishes a general framework for non-standard asymptotic theory concerning the nonparametric components.<br/><br/>The immediate need for fast and efficiently extracting information from all the dimensions of modern massive data sets gives rise to the increasing popularity of the semi-nonparametric models. For example, to understand the recent financial crisis, the semi-nonparametric copula models are applied to address tail dependence among shocks to different financial series and also to recover the shapes of the impact curve for individual financial series. The proposed research promotes the use of semi-nonparametric models in analyzing modern complex data by developing a series of innovative and valid bootstrap inferential tools, and eventually gain substantial scientific productivity across various disciplines. Statistical science benefits from the increasing number of researchers trained in semi-nonparametric modelling both from the statistical and scientific viewpoints. This would include the students funded by this work, broader collaborating research and educational activities. The above research also produces easy-to-implement software for the public."
"1149677","CAREER: STATISTICAL INFERENCE FOR TOPOLOGICAL AND GEOMETRIC DATA ANALYSIS","DMS","STATISTICS","06/01/2012","05/26/2016","Alessandro Rinaldo","PA","Carnegie-Mellon University","Continuing Grant","Gabor Szekely","05/31/2018","$400,000.00","","arinaldo@cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","1045","$0.00","The research objective of this proposal is to develop new theories and methods for estimating topological and geometric features of lower-dimensional sets based on noisy high-dimensional data.  To this end, the investigator has formulated two separate but highly interdependent sets of research goals.  The first set of research goals is the integration of statistical theory with methods of topological data analysis. Recent breakthroughs in computational topology have made it possible to compute topological invariants of sets from a collection of points in Euclidean spaces. Though the potential for high-dimensional statistical inference  of these new types of data summaries is significant, their statistical properties are still largely unexplored.  The investigator proposes to 1) to develop a comprehensive theory of minimax (and adaptive) estimation of topological properties of sets and 2) to create statistical procedures for non-parametric testing and de-noising based on topological invariants. The second set of research goals pertains to the traditional geometric data-analytic task of clustering in high-dimensions, and it is aimed at advancing the theory and practice of high-density clustering. Recent progress in the theory of clustering has demonstrated  that clustering using density estimation can perform well in high-dimensional settings, and that the notion of high-density clustering provides a natural probabilistic framework for describing and analyzing clustering problems in great generality. Thus, the investigator intends 1) to generalize and refine the high-density clustering problem under weak conditions on the data-generating mechanism and 2) to investigate the theory and use of data resampling techniques for parameter tuning in high-density clustering and density estimation. A common thread in the proposed research is the reliance on density estimation, as a tool for both accurate high-dimensional clustering and smoothing/de-noising of topological features. <br/><br/><br/>In the last few decades, advances in data acquisition technologies have led to an explosion in the collection and diffusion of large-scale datasets, across a variety of scientific fields. The unprecedented magnitude and complexity of modern databases pose formidable challenges to statisticians, both of theoretical and methodological nature, and has required the development of new statistical tools for data analysis. Modern high-dimensional statistics is predicated on the key assumption that, while the data are observed in a high-dimensional space, the intrinsic complexity of the data-generating mechanism is in fact significantly smaller and, therefore, learnable in computationally efficient ways. This research proposal capitalizes on this premise, and describes an array of methods for summarizing, discriminating, visualizing and clustering high-dimensional noisy data and for extracting salient low-dimensional features. The proposed research encompasses several novel and open research problems at the interface of mathematics, computer science, statistics and machine learning. The procedures studied in the proposal are of broad applicability and promise to be used in a multitude of scientific areas, such as medical imaging, neuroscience, astrophysics, biology, genetics, geophysics and sensor networks, just to name a few. The broader impact of this project also includes interdisciplinary training of students in statistics, mathematics and computer science."
"1219417","Workshop on Statistical Analysis of Neural Data (SAND)","DMS","STATISTICS","03/01/2012","02/14/2012","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","02/28/2013","$20,000.00","","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","7556","$0.00","The workshop Statistical Analysis of Neural Data (SAND6) is planned for May 31-June 2, 2012 at the Center for the Neural Basis of Cognition, run jointly by Carnegie Mellon University and the University of Pittsburgh.<br/>SAND6 will bring together neurophysiologists, statisticians, physicists, and computer scientists who are interested in quantitative analysis of neuronal data. There will be 5 scientific sessions, at which 8 keynote investigators and 16 junior investigators will speak. There will also be a poster session. Authors will be encouraged to submit papers to the Journal of Computational Neuroscience. <br/><br/>SAND6 is the sixth workshop in a series that began in 2002. The workshops are held in even years during the spring at the Center for the Neural Basis of Cognition, run jointly by Carnegie Mellon University and the University of Pittsburgh. The objectives of the workshop are to define important problems in neuronal data analysis and useful strategies for attacking them; foster communication between experimental neuroscientists and those trained in statistical and computational methods; and provide further dissemination of the findings presented at the workshop via a set of peer-reviewed articles. Secondary objectives are to encourage young researchers, including graduate students, to present their work; expose young researchers to important challenges and opportunities in this interdisciplinary domain, while providing a small meeting atmosphere to facilitate the interaction of young researchers with senior colleagues; and include as participants women, under-represented minorities and persons with disabilities who might benefit from the small workshop environment."
"1208315","Rare and Weak Signals in Big Data: How to Find Them and How to Use Them","DMS","STATISTICS","08/01/2012","08/09/2014","Jiashun Jin","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","07/31/2017","$119,999.00","","jiashun@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","A research effort is proposed to create new tools for high dimensional data analysis, focusing on the very challenging regime where signals are both rare and weak. In particular,  the proposer proposes to:   (a).  Develop graphlet screening as a new tool for high dimensional variable selection, introduce a new theoretic framework for assessing the optimality of variable selection, and show that graphlet screening achieves the optimal rate of convergence in terms of Hamming distance of the selection errors.  (b). Develop a new method of spectral clustering by using the recent idea of Higher Criticism thresholding, and investigates the fundamental limits for several problems related to low-rank matrix recovery, including high dimensional clustering, sparse Principle Component Analysis, and a testing problem related to the underlying large-size covariance matrix. (c) Extend and apply the proposed methods and theory to the analysis of Big data  generated in  various scientific fields,  including genomics and machine learning. <br/><br/>We are often said that we are entering the era of 'Big Data', where massive datasets consisting of millions of observations are mined for associations and patterns. What is never said about this pervasive trend is that, unfortunately, the signal we are looking for is usually very rare and weak and is hard to find, and it is easy to be fooled. The project introduces new ideas, new tools, and novel theory that are appropriate for rare and weak signals in Big Data, and apply the theory and methods to various scientific fields, including genomics and machine learning."
"1157813","Summer Research Conference in Statistics and Biostatistics","DMS","STATISTICS","04/01/2012","12/14/2011","Donald Edwards","SC","University South Carolina Research Foundation","Standard Grant","Nandini Kannan","03/31/2013","$10,000.00","","edwards@stat.sc.edu","915 BULL ST","COLUMBIA","SC","292084009","8037777093","MPS","1269","7556, 9150","$0.00","The investigator and his colleagues will organize and conduct a Summer Research Conference (SRC) in statistics and biostatistics at Jekyll Island, Georgia June 3-6, 2012.  The SRC is an annual conference sponsored by the Southern Regional Council on Statistics (SRCOS).  Its purpose is to encourage the interchange and mutual understanding of current research ideas in statistics and biostatistics, and to give motivation and direction to further research progress.  The project will focus on young researchers, placing them in close proximity to leaders in the field for person-to-person interactions in a manner not possible at most other meetings; these leaders, if from the southern region, pay their own way and contribute their time.  Speakers will present formal research talks with adequate time allowed for clarification, amplification, and further informal discussions in small groups.  Under the travel support provided by this award students and young faculty who do not have financial support for their travel will attend and also present posters to be reviewed by more experienced researchers.<br/><br/><br/>The Southern Regional Council on Statistics is a consortium of statistics and biostatistics programs in the South, stretching as far west as Texas and as far north as Maryland.  It currently has 43 member programs at 39 universities in 16 states in the region.  This project will fund student and young-investigator travel to the 2012 Summer Research Conference (SRC) sponsored by SRCOS; see http://www.sph.emory.edu/srcos/research.htm for an archive of many recent SRCs.  The meeting is particularly valuable for students and faculty from smaller regional schools at drivable distances, affording them the opportunity to participate and interact closely with internationally-known leaders in the field without the cost of travel to distant national or international venues.  It will strengthen the research of the statistics and biostatistics community as a whole, and particularly in the underdeveloped southern region."
"1209023","Statistical methods for screening individual childhood growth paths","DMS","STATISTICS","09/15/2012","09/11/2012","Ying Wei","NY","Columbia University","Standard Grant","Nandini Kannan","08/31/2016","$180,000.00","","yw2148@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","","$0.00","Pediatric growth charts have been widely used in clinics and medical centers to monitor the growth of infants, children, and adolescents to compare individual values with the reference population. The existing methods focus on screening of a single measurement at a fixed time. As infants and children are often followed up regularly to ensure normative growth, each child usually exhibits a growth path. The main objective of the proposed research is to develop statistical methods to construct growth charts for screening individual growth paths. The key challenge here is to rank the underlying growth paths, which are only observed limited number of times with varying measurement time spacings. Two potential approaches are proposed. The first one obtains a lower dimensional approximation of growth paths, and ranks the individual growth paths based on their projection scores in this lower space. The second one defines statistical depth for growth paths, which in turn provides a ranking of individual paths. For both approaches, the investigators will study their theoretical properties, such as consistency, convergence rate, and asymptotic distributions, and develop related inference tools. As growth data are often collected nation wide, they have large sample sizes. To make the proposed methods feasible in practice, computing time is a critical issue. Therefore, the investigators also plan to develop fast algorithms for both methods to improve the computational efficiency. In addition, as shown in previous studies, covariate-adjusted methods can effectively enhance screening performance. For example, parental information plays a significant role in children's growth. The investigators will extend the developed methods for the two approaches to incorporate covariate information. Overall, the statistical methods to be employed for this proposal cover: statistical methods for growth chart construction, longitudinal data analysis, functional data analysis, singular value decomposition and principal component analysis, statistical depth, quantile regression, nonparametric and semi-parametric modeling, and mixed effect models. <br/><br/>The proposed research will produce broad interdisciplinary contributions.  Although the study was motivated by pediatric growth screening, the applications of the proposed methods certainly go beyond that.  Both the PI and the co-investigator have been involved in various collaborative projects in epidemiology, HIV research, genetics, cancer research and environmental science. Growth data can be viewed as varying-location longitudinal data, which commonly exist in those applications. Hence, the proposed methods can lead to more accurate and more comprehensive approaches for problems in these areas of research. Additionally, the general methodologies to be developed are of definite statistical importance and have not been studied satisfactorily to date. The research plan described in this proposal has both broad methodological and applied merits. The results obtained from this project are expected to be widely disseminated through publications in international scientific journals in statistics, public health and medicine, presentations in domestic and international conferences, seminar talks in universities, and collaborations with clinical researchers. The investigators plan to develop new courses related to quantile regression and data depth, and part of the course materials will be based on the proposed research. In addition, the investigators also believe that an important way of disseminating new methodology is to provide easily available and user-friendly computer software. The proposed methods will be implemented as an R package which will be freely available on-line."
"1209136","Collaborative Research: Numerical algebra and statistical inference","DMS","STATISTICS","07/01/2012","08/29/2014","Lek-Heng Lim","IL","University of Chicago","Continuing Grant","Nandini Kannan","06/30/2017","$150,000.00","","lekheng@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","","$0.00","The investigators have two aims in this proposal that fall at the interface of numerical algebra and statistical inference. The first aim is to extend the use of randomized approximation in a variety of dimension reduction methods that rely on numerical linear algebra both supervised and unsupervised as well as linear and nonlinear and develop a statistical bases for these methods in addition to the computational motivation of being applicable to massive data. The other motivation is to extend these statistical methods for dimension reduction to multiway data using numerical multilinear algebra, a recent new development in numerical analysis. These projects will increase interaction between statistical inference and numerical analysis and benefit both fields, providing new perspectives to how we view and perform data analysis.<br/><br/>Numerical methods with statistical implications are central to a variety of technologies used by the general population. These technologies include Google's pagerank algorithm, genetic methods used to find genetic variation related to disease, compressing of medical images for storage and treatment, as well as applications in geostatistics. In all the previous cases the fundamental idea is to condense massive data in a useful summary with respect to a desired goal. The two ideas in this proposal are (1) to study how numerical methods that scale to the massive data generated in modern scientific, engineering, and social applications impose statistical assumptions or models on the data, (2) to study more complex interactions or properties of the data than examined in current methods. The motivation behind the first aim is to understand how numerical approximations required for computational scaling as we collect more data impact the information that can be extracted from these data -- for what type of data and applications do certain numerical approximations work well. The motivation behind the second aim is to go beyond the broad category of standard statistical methods take into account the relation between pairs of objects -- two web pages that are linked for Google's pagerank, the correlation between two genes or two loci in genetics applications. The question behind this aim is whether richer sources of information can be extracted by examining the links between three web pages or three loci. The research involved in this aim consists of the development of computationally efficient algebraic methods to extract this information and understanding the statistical models implemented by these methods."
"1149692","CAREER: Bridging High-Frequency Data Analysis and Continuous-time Features of Levy Models","DMS","PROBABILITY, APPLIED MATHEMATICS, STATISTICS","06/01/2012","04/13/2015","Jose Figueroa-Lopez","IN","Purdue University","Continuing Grant","Gabor Szekely","11/30/2015","$325,381.00","","figueroa@math.wustl.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1263, 1266, 1269","1045","$0.00","Motivated by recent theoretical findings in the field, the investigator identifies some key open problems of the asymptotic behavior of Levy processes in short time and connects them to two important statistical problems commonly appearing in applications: parametric estimation and change-point detection for Levy models.  For finite random samples, methods such as maximum likelihood estimation and cumulative sum (CUSUM) sequential rules are known to be optimal for dealing with the two previously mentioned problems. Although one expects that optimality would be preserved when the time span between consecutive observations of a Levy process shrinks to zero, there exist important examples showing this not always to be the case. The mystery behind these counterintuitive results is closely connected to the ""fine"" distributional properties of Levy processes in short time. Rather than directly attacking the two proposed problems in continuous time, the investigator builds on the well-studied analogous problems in discrete time and fill in the infinite time continuum by analyzing their evolution when the time span between consecutive observations is made increasingly small. This bottom-up approach is not only appealing but also useful since in practice one would like to determine the performance of statistical methods for high-frequency observations rather than for continuous-time observations, which are arguably never available. The focus on Levy processes is motivated by the fact that the latter are the simplest stochastic models displaying abrupt changes while still preserving the parsimonious statistical properties of their increments. Extensions to other multi-factor stochastic models driven by Levy processes are also contemplated. <br/><br/>Automatic high-frequency monitoring systems of natural and social phenomena are increasingly used in engineering applications, financial markets, and environmental studies. Therefore, there is an increasing need for efficient and accurate statistical and computational methods for the high-frequency data generated by these systems. Two important issues arise with this need: understanding the meaning of statistical efficiency in a high-frequency sampling setting and analyzing the optimality of some of the commonly used statistical methods when applied to high-frequency data. The undertaken research responds to these two pressing problems. The project's outcomes have important applications in pricing of financial derivatives, calibration of financial models, monitoring of navigation system, intrusion detection in computer networks, and more. Educational impacts include providing summer research experiences for undergraduates and developing teaching/computational resources for interdisciplinary topics in statistics, probability, and mathematical finance. These activities involve graduate students and target the participation of underrepresented groups in sciences."
"1160319","FRG: Collaborative Research: Unified statistical theory for the analysis and discovery of complex networks","DMS","STATISTICS","06/01/2012","03/29/2012","Peter Bickel","CA","University of California-Berkeley","Standard Grant","Gabor Szekely","05/31/2017","$1,199,916.00","Bin Yu, Laurent El Ghaoui, Haiyan Huang, Noureddine El Karoui","bickel@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","1616","$0.00","The investigators will develop a unified nonparametric theoretical framework to study stochastic network models and design scalable algorithms (and software) to fit these models. They intend to carry out the development and validation of their methods with collaborators in biology who have gathered extensive new data for the assessment of protein structure and determination of biological pathways particularly in Drosophila. Such problems are omnipresent in genomics and they expect their methods to carry over widely.  They will also use networks with uncertainty measures to study relationships between words and phrases in a newspaper database in order to provide media analysts with automatic and scalable algorithms. In all cases they will focus on methods of general statistical confidence which have been lacking in work so far.<br/><br/>Our world is connected through relationships, among ""actors"" who can be people, organizations, words, genes, proteins, and more. Advancements of information technology have enabled collection of massive amounts of data in all disciplines for us to build relationships between these actors. These relationships can be effectively described as networks, and properties or patterns in these networks can be random or knowledge. Responding to this recent data availability and a huge potential for knowledge discovery, research in networks is attracting much attention from researchers in physics, social science, computer science, and probability. While contributing to the development of core statistical research, the proposed research will directly impact the interdisciplinary field of network analysis and the study of complex networks. The applications of their research results are diverse and well beyond the two fields studied in the proposal: genomics and media analysis. They include national security, communications, sociology, political science, and infectious disease. The statistical tools developed are unifying and could change how many scientists approach network analysis. As a result, statistical research will become more prominent in the networks community."
"1208978","Generalized Semiparametric Regression with Longitudinal Data","DMS","STATISTICS","09/01/2012","05/30/2012","Yanqing Sun","NC","University of North Carolina at Charlotte","Standard Grant","Gabor Szekely","08/31/2015","$120,000.00","","yasun@uncc.edu","9201 UNIVERSITY CITY BLVD","CHARLOTTE","NC","282230001","7046871888","MPS","1269","","$0.00","This proposal explores the generalized semiparametric regression model (GSRM) for longitudinal data. The GSRM model allows the effects of some covariates to be constant and others to be time-varying. The model is an extension of the generalized linear model for cross-sectional data. Different link functions can be selected to provide a rich family of models for longitudinal data. Both categorical and continuous longitudinal responses can be modeled with appropriately chosen link functions.  Statistical analysis of longitudinal data often involves modeling treatment effects on clinically relevant longitudinal biomarkers since an initial event (the time origin). The proposed research includes two parts with important applications. In the first part, the investigator proposes to examine the GSRM model when the time origin is observed for all subjects.  In the second part, the exact time origin may be unknown.  The GSRM model provides a big platform for model building and variable selection. The investigator proposes a sampling adjusted profile local linear estimation approach. The nonparametric components of the model will be estimated using the local linear estimating equations and the parametric components are to be estimated through weighted profile estimating functions.  In the situation where the exact time origin may be unknown, an EM procedure based on the missing data principle will be investigated.  The proposed method will automatically adjust for heterogeneity of sampling times, allowing the sampling strategy to depend on the past sampling history as well as possibly time-dependent covariates without specifically modeling such dependence. Many important issues will be investigated, including variance estimation, hypothesis testing of covariate effects, weight function and bandwidth selections, and goodness of fit. The estimation and hypothesis testing of the link function will also be investigated. The proposed research will be applied to real examples from AIDS clinical trials and vaccine efficacy trials.<br/><br/>Longitudinal data are common in medical and public health research. Statistical analysis of longitudinal data often involves modeling treatment effects on clinically relevant longitudinal biomarkers since an initial event. The proposed research investigates a unified approach to the generalized semiparametric regression model for longitudinal data for both the situations where the time of the initial event is known and where the exact time of the initial event may be censored. The proposed research is motivated by real problems in AIDS clinical trials and HIV vaccine efficacy trials. By pursuing the directions outlined in the proposal, significant progress could be made in building biologically interpretable models and in developing statistically efficient methods to deal with the complexity of longitudinal data. The proposed research will contribute to efforts to overcome the medical and public health challenges facing the world today."
"1302798","Variable Selection Methods in High Dimensional Feature Space","DMS","STATISTICS","08/31/2012","10/15/2012","Rui Song","NC","North Carolina State University","Standard Grant","Gabor Szekely","12/31/2013","$29,441.00","","rsong@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","","$0.00","With rapid advances of computing power and other modern technology, high-throughput data of unprecedented size and complexity are becoming a commonplace in diverse fields. Examples include data from genetic, microarrays, proteomics, fMRI, cancer clinical trials and high frequency financial data. These high dimensional data characterize many important contemporary problems in statistics and feature selection play pivotal roles in these problems. This research project aims to develop cutting-edge statistical theory and methods for high dimensional variable selections. In particular, the PI proposes the following interrelated research topics for investigation: (1) grouped-variables screening with sparse linear models; (2) nonparametric components screening with sparse additive models; (3) parametric components screening with sparse semiparametric models and(4) their further extensions. The proposed methods will be studied theoretically for their sure screening behavior and compared with some of the existing methods empirically in terms of computational expediency, statistical accuracy and algorithmic stability.<br/><br/>The outlined research project on variable selection in high dimensions tries to tackle fundamental problems in statistical learning and will stimulate interests from a large group of scientists and researchers in diverse fields of sciences, engineering and humanities ranging from genomics and health sciences to economics and finance. Another key aspect of this project is the integration of research and education, which will be achieved by developing two new courses on statistical learning and non-, semi-parametric inference and proposing specific projects for students during the teaching of classes. It will enable the participation of all citizens from various disciplines, including underrepresented groups of students."
"1148643","RTG:  Statistics in the 21st Century - Objects, Geometry and Computing","DMS","STATISTICS, WORKFORCE IN THE MATHEMAT SCI","09/01/2012","09/03/2019","Wolfgang Polonik","CA","University of California-Davis","Continuing Grant","Gabor Szekely","08/31/2020","$1,999,858.00","Hans-Georg Mueller, Prabir Burman, Thomas Chun Man Lee, Jie Peng","wpolonik@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269, 7335","7301","$0.00","RTG: Statistics in the 21st Century - Objects, Geometry and Computing<br/><br/>Images, matrices, functions, trajectories, trees, or graphs are examples of objects increasingly encountered in modern data analysis. Statistical analysis of such object data requires skills and knowledge that are not (yet) part of a standard training of a statistician. This includes modern data handling skills, such as accessing web services, or manipulating large data in complex formats; skills needed for working in a multi-disciplinary scientific environment; and also mathematical skills, in particular involving notions of geometry, shape or topology.  The project provides training to undergraduate and graduate students, addressing these issues through data analysis applied to important scientific questions, exposure to research in advanced methodologies, corresponding relevant mathematical theories, and computing.  Graduate students and postdoctoral fellows also receive training in teaching, mentoring, scientific writing and communication skills, and also in other aspects that are essential for a successful academic career, such as grant writing and job application.<br/><br/>Continuous technological advances go hand in hand with a rapid increase of novel, complex data types, and the importance of developing skills necessary for their analysis cannot be overstated. Aspects of geometry and computing often lie in the center of such an analysis. The project is thus critically advancing training related to the analysis of such modern data types, thereby striving to provide a model for a broad, modern training in statistics. The project builds on the experiences and the strengths of the senior personnel, the affiliated members and collaborators from scientific fields including applied mathematics, astronomy, bio-demography, computer science, and neuroscience. The training activities will not only further strengthen the statistics program at UC Davis, but will also make critical contributions to enhancing the workforce in the STEM disciplines."
"1149312","CAREER: Phylogenomics - New Computational Methods through Stochastic Modeling and Analysis","DMS","PROBABILITY, STATISTICS, MATHEMATICAL BIOLOGY","09/01/2012","07/18/2016","Sebastien Roch","WI","University of Wisconsin-Madison","Continuing Grant","Junping Wang","08/31/2018","$444,405.00","","roch@math.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1263, 1269, 7334","1045, 9251","$0.00","In this project, modeling and analysis techniques from probability theory will be used to study several important computational problems in the area of phylogenomics, i.e., the integration of genome analysis and systematic studies.  Various mechanisms such as hybridization events, lateral gene transfers, gene duplications and losses, and incomplete lineage sorting commonly lead to incongruences between inferred gene genealogies.  As a result, one is led to consider forests of gene histories as well as more complex network representations of the evolutionary history of life.  The main goals of the research are to improve large-scale likelihood-based gene tree estimation, develop computational methods to assemble species phylogenies from gene histories, and detect network-like signal in molecular data.  Drawing on a combination of ideas from discrete probability, algorithms, and mathematical statistics, novel methodologies will be developed that are both statistically accurate and computationally efficient for these challenging inference problems.<br/><br/>Biologists face major statistical and computational challenges in modeling, analyzing, and interpreting the massive genetic datasets produced by next-generation technologies, including genomic variation within populations, whole genomes from multiple species, and environmental samples.  In particular, high-throughput sequencing is transforming the reconstruction of the Tree of Life, a fundamental problem in biology which provides insights into the study of evolution, adaptation, and speciation.  Through the development, implementation, and broad dissemination of new practical algorithms for phylogenomic studies based on mathematical analysis, this project will help advance the state of knowledge in evolutionary biology and contribute to the numerous benefits to society of phylogenetic research. Integration of research and education is a major component of this proposal.  In addition to providing training for graduate students and postdoctoral researchers, new undergraduate and graduate courses will be developed and research experiences for undergraduates will be an important part of the project."
"1209152","Bayesian Learning with Structured Sparsity","DMS","STATISTICS","08/15/2012","08/19/2014","Feng Liang","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Gabor Szekely","07/31/2015","$135,181.00","","liangf@uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","","$0.00","A common premise held in high-dimensional data analysis is that only a small fraction of the covariates/variables is relevant. This is termed as the sparsity assumption meaning that the unknown parameter, which needs to be inferred from the data, is sparse. In many real world applications, there is in addition some side information on the structure dependence among elements of this high-dimensional parameter. For example, they may be grouped, ordered, or linked on a graph. Consequently such structure constraints should be incorporated into the ordinary sparsity assumption, which is what we call the structured sparsity. The scientific foci of this proposal are to develop novel Bayesian theory, methodology and computational tools to adaptively utilize structure information into statistical inference, such as variable selection, estimation, and dimension reduction. <br/><br/>Many problems in modern statistics can be formulated as recovering some unknown high-dimensional parameter from the noisy data. In some real world applications, besides the observed data, there is a kind of side information termed as ""structured sparsity"" on the unknown parameter. The ultimate goal of this project is to develop novel theory, methodology and algorithms to adaptively utilize such side information for statistical inference. The statistical and computational challenges addressed in this project are a response to practical problems arising from the Principle  Investigator's collaborative research on neuroimaging analysis, binoinformatics, and marketing science. The proposed methodological development and statistics research will result in useful methods for broader applications."
"1207808","Complexity Penalization in High Dimensional Matrix Estimation Problems","DMS","STATISTICS","07/01/2012","05/12/2014","Vladimir Koltchinskii","GA","Georgia Tech Research Corporation","Continuing Grant","Gabor Szekely","06/30/2016","$300,000.00","","vlad@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","","$0.00","The investigator will study a variety of problems of estimation of large matrices based on noisy measurements of linear functionals of these matrices. The main focus is on the problems where the target matrix is either low rank, or it can be well approximated by low rank matrices. The proposed estimation methods are based on empirical risk minimization with complexity penalties that favor low rank solutions and the objective is to obtain sharp bounds on the estimation error (in particular, low rank oracle inequalities) that show how it depends on the important parameters of the problem such as the level of the noise, the sample size, the size and the rank of the target matrix. The problems to be studied include: (a) new low rank oracle inequalities for trace regression providing a bridge between known results in the noiseless case and in the noisy case for more complex models of design distribution; (b) estimation of density matrix in quantum state tomography with a goal of studying both the least squares method in matrix regression setting and the maximum likelihood method for more general measurement models with proper complexity penalization; (c) estimation (learning) of low rank kernels on graphs and manifolds with a goal of developing new methods of predicting similarities between vertices of a graph or points in an unknown manifold embedded in a Euclidean space.<br/><br/><br/>This project is in a very active interdisciplinary area of high-dimensional matrix estimation that is borderline between statistics, mathematics and computer science, and it will facilitate research collaborations between these areas. It will provide a better understanding of subtle aspects of high-dimensional problems of matrix estimation and of complexity regularization in these problems. The problem of estimation of large matrices is very basic in high-dimensional statistics and in a variety of its applications in such areas as signal and image processing, compressed sensing, bioinformatics, quantum information and quantum statistics, high-dimensional data visualization and visual analytics. In these problems, it is of importance to find low-dimensional structures in high-dimensional data that reflect basic relationships between the variables describing complex high-dimensional systems. In the case of matrix problems, finding such structures can be reduced to estimation of low rank matrices and the proposed project will result in a better understanding of the existing methods as well as in the development of new methods of low rank matrix recovery."
"1212830","International Conference on Advances in Interdisciplinary Statistics and Combinatorics","DMS","STATISTICS","09/01/2012","02/14/2012","Sat Gupta","NC","University of North Carolina Greensboro","Standard Grant","Gabor Szekely","08/31/2014","$20,000.00","Scott Richter, Jan Rychtar","sngupta@uncg.edu","1000 SPRING GARDEN STREET","GREENSBORO","NC","274125068","3363345878","MPS","1269","7556","$0.00","The proposed project is a statistics conference ""International Conference on Advances in Interdisciplinary Statistics and Combinatorics (AISC 2012)"". The plan is to hold the conference on the campus of The University of North Carolina at Greensboro, Greensboro, NC during October 5-7, 2012. The main objective of the conference is to promote interdisciplinary research involving statistical techniques. These techniques are becoming increasingly important in all fields of scientific discovery. A unique feature of the proposed conference is to bring together nationally recognized researchers from many fields such as anthropology, biology, economics, education, environmental science, information systems, insurance, mathematics, medicine, psychology, and public health. The common thread is the statistical methods used in their respective research programs. The statisticians benefit by getting exposed to real problems from other disciplines and researchers from other disciplines benefit by learning more about various statistical techniques. The PI and both Co-PIs are currently involved in several collaborative research grants and have experienced firsthand the significance of interdisciplinary research.<br/> <br/>An important objective of the conference is to groom young researchers by providing them an opportunity to interact with established researchers in their respective fields. These young researchers include undergraduate researchers, graduate student researchers and new PhDs specializing in statistical sciences. They are given the opportunity to make oral presentations as opposed to poster presentations. The investigators are specifically trying to attract women and minority candidates by giving them higher priority in awarding financial support. The organizers plan to contact appropriate departments at North Carolina Agricultural and Technical State University in Greensboro, Winston-Salem State University in Winston-Salem, and North Carolina Central University in Durham, all historically black universities. They also plan to place conference announcement on the web pages of the Committee on Women in Statistics and the Committee on Minorities in Statistics (both committees of the American Statistical Association).The organizers also plan to make special efforts to accommodate the needs of the participants with disabilities. More details about the conference are available at the website   http://www.uncg.edu/mat/aisc/"
"1150435","CAREER: Nonparametric methods in multiple dimensions: shape restrictions, bootstrap and beyond","DMS","STATISTICS","07/01/2012","07/01/2016","Bodhisattva Sen","NY","Columbia University","Continuing Grant","Nandini Kannan","06/30/2017","$400,032.00","","bodhi@stat.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","1045","$0.00","This proposal deals with some problems on estimation and inference using nonparametric methods. Nonparametric procedures have become increasingly popular in the theory and practice of statistics in recent times primarily because of their fundamental advantages over parametric methods: greater flexibility and more 'data-driven' features. In this proposal, the investigator studies three core directions of statistical research in this area: (A) Nonparametric function estimation under shape restrictions, (B) Dimension reduction using semi/non-parametric techniques, and (C) Bootstrap based inference in non-standard problems. The main motivation for this research is in developing nonparametric procedures that are completely automated (free from tuning parameters, e.g., smoothing bandwidths) but still flexible enough to incorporate data-driven features. A major part of this proposal deals with nonparametric methods applicable to multivariate data, an area that has received relatively less attention, though often felt to be necessary in performing real data analysis.<br/><br/>With the advancement in modern computing facilities and the explosion in collection of large scale data sets, the use of more complicated/intricate statistical procedures involving numerical optimization techniques are becoming increasingly popular. However, a complete theoretical analysis of most of these procedures is still largely unavailable. This research aims at understanding the theoretical and computational aspects of some of these statistical procedures, and quantifying the uncertainties involved in such stochastic optimization problems. The intended applications of the proposed research are diverse, ranging from detecting the advent of global warming, to estimating the radial velocity distribution of stars in a galaxy, to developing inferential techniques for binary choice models (of special interest to econometricians), and would involve collaborations at different levels with statisticians, biostatisticians, epidemiologists, econometricians and astronomers."
"1208421","In-depth development and assessment of covariance models for multivariate nonstationary processes on a sphere","DMS","STATISTICS","07/01/2012","06/26/2014","Mikyoung Jun","TX","Texas A&M University","Continuing Grant","Gabor Szekely","06/30/2016","$100,000.00","","mjun@central.uh.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1269","","$0.00","With the advancement of science and technology, many geophysical problems involve data sets on a global scale with multiple variables of interest. As a consequence, there is a pressing need for flexible nonstationary covariance models for multivariate processes defined on a sphere. On the other hand, the development of covariance models for geostatistical data should accompany methodologies for thorough goodness-of-fit diagnostics of the proposed models. The goal of this project is to provide in-depth development of multivariate spatial (-temporal) covariance models on a spherical domain that address certain nonstationary properties of the covariance structures, in small and large scale, paired with thorough goodness-of-fit diagnostics of the proposed models. Proposed covariance models are flexible to capture complex nonstationarity, such as varying smoothness and geometric anisotropy. The investigator develops Bayesian and non-Bayesian methods for goodness-of-fit diagnostics and studies the effect of plug-in estimators of covariance models. The investigator applies the models developed in this project to the problem of validating multiple climate model outputs from the CMIP5 archive with thorough diagnostics of the model fits. Different covariance structures over land and the ocean, as well as their dependence on latitude, are thoroughly investigated. The investigator also explores efficient computation methods for large data sets. <br/><br/>The motivation of this project is the scientific problem of combining multiple climate model outputs and studying the relationships between different climate variables. A great deal of effort is being invested in developing state-of-the-art climate models by climate scientists worldwide, and a new set of climate model outputs (CMIP5-Coupled Model Intercomparison Project Phase 5), that are the basis of the results in the fifth assessment report of the Intergovernmental Panel on Climate Change (IPCC), are becoming available. This project will provide climate scientists and climate modelers with useful tools to intercompare and combine climate models and test the improvement of the CMIP5 results over the previous CMIP3 results. Statistical models developed in this project will enable more accurate assessments of the relationships between multiple climate variables such as temperature and precipitation, and future climate change."
"1209017","Multivariate Methods for High-Dimensional Transposable Data","DMS","STATISTICS","06/01/2012","05/30/2012","Genevera Allen","TX","William Marsh Rice University","Standard Grant","Gabor Szekely","05/31/2015","$120,000.00","","gallen@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","","$0.00","High-dimensional data is prevalent in areas such as medicine, finance, environmental studies, imaging, networking, and the Internet. Making sense of this massive amount of data holds the key to critical scientific questions such as discovering biomarkers related to disease and evaluating the effects of climate change. The proposed research will use statistical learning techniques to develop novel multivariate approaches incorporating sparsity (variable selection), smoothness, and accounting for known structure in high-dimensional data. Extending multivariate methods for structured data improves signal recovery and feature selection. These techniques will be useful in spatio-temporal and image data, for example, where strong correlations from known structure obscure dimension reduction. Much attention has been given to multivariate methods for matrix data. The proposed research will extend many modern multivariate techniques such as sparse principal components analysis to the multi-dimensional or tensor framework. Finally, this proposal seeks to generalize much of the existing literature on regularized multivariate methods such as principal components analysis, canonical correlations analysis and linear discriminant analysis by illustrating how to encourage many types of regularization. This proposal also seeks to develop algorithmic and computational frameworks that will allow researchers to apply these methods to modern massive data sets.<br/><br/>As multivariate analysis techniques enjoy nearly universal application, the theory and methods developed in this proposal will have wide ranging significance in many applied fields. The methods developed are in part motivated by and will have immediate impact in neuroimaging studies, cancer genomics, and metabolomics studies.  Other areas where this methodology will prove beneficial include climate studies, remote sensing, networking, engineering, finance, and imaging. Results of this research will be disseminated through the release of open-source, publicly available software and will be incorporated in course material of an advanced graduate course on statistical learning."
"1208735","Bayesian Decision Theoretic Methods for Some High-Dimensional Multiple Inference Problems","DMS","STATISTICS","07/15/2012","06/05/2013","Zhigen Zhao","PA","Temple University","Continuing Grant","Gabor Szekely","09/30/2015","$174,976.00","Sanat Sarkar","zhaozhg@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","MPS","1269","","$0.00","The project covers some outstanding and important inference problems that statisticians face when analyzing high-dimensional data from brain imaging, next generation sequencing, atmospheric science, astronomical studies, and many other scientific investigations. Miss-detecting a strong signal in these experiments, particularly when the signals are sparse, is often a more severe error than miss-detecting a weak signal, and this error gets more severe as the signal gets stronger. This is an important issue which has not been fully utilized in the existing procedures designed for simultaneous testing of multiple hypotheses. Also, selective inference using multiple confidence intervals is an emerging area of statistical research whose importance is being realized very recently. However, while analyzing high-dimensional data with sparse signals, the existing intervals designed to provide estimates of the selected significant signals can become non-informative in the sense of miss-covering the true signal or covering zero too often  if the sparse nature of the data is not properly taken into account. This research project seeks to develop new and innovative methods taking a Bayesian decision theoretic viewpoint which is particularly well suited to tackle these issues. It focuses on the following two broad areas of research: (i) Developing new multiple testing methods controlling false discoveries incorporating the severity of type II errors, and (ii) developing new multiple confidence intervals for selected parameters under zero-inflated mixture prior.<br/> <br/>This project will be expected to have a broad impact on the theory and practice of statistics. It can produce novel methodologies to detect true signals in modern and high-dimensional scientific investigations, and pave the way for better use of statistics towards meeting modern societal and scientific needs. For instance, understanding vegetation changes under seasonal variability is crucial for more effective land use management when coping with climate changes and food security. This project can potentially offer new methodologies towards addressing that sustainability issue. Also, there is an increasing demand for sophisticated statistical tools to have better understanding of astronomical behaviors based on the influx of data created by the advent of new technologies. Again, this project can potentially meet that demand. The results will be disseminated through presentations and discussions at national and international conferences, and visits to other institutions. The software to be developed under this project will be made available, free of charge, to the scientific community."
"1208488","Models and Computational Strategies in Statistical Bioinformatics","DMS","STATISTICS","09/15/2012","08/14/2013","James Booth","NY","Cornell University","Continuing Grant","Gabor Szekely","08/31/2014","$94,895.00","Martin Wells","jb383@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","","$0.00","The investigator and his colleagues will develop methods for high-throughput biological data that provide innovative extensions of modern statistical building blocks, including the use of random effects for regularization, shrinkage estimation, Bayesian statistics, and mixtures for posterior classification and prediction. Novel modifications of the expectation-maximization algorithm are proposed for scalable and efficient model fitting and inference in several important biological applications.<br/><br/>The goal of this project is to develop new statistical models, computational algorithms, and decision theoretic analysis of estimates for high-throughput biological data. The new methods can be applied in the analysis of microarrays, RNA sequencing counts, label-free shotgun proteomics, metabolomics, the identification of quantitative trait loci and association mapping."
"1207651","Extending the use of machine learning algorithms to Sufficient Dimension Reduction","DMS","STATISTICS","09/15/2012","11/14/2013","Andreas Artemiou","MI","Michigan Technological University","Continuing Grant","Gabor Szekely","08/31/2013","$19,385.00","","aartemio@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","MPS","1269","","$0.00","The investigator develops new methodologies for sufficient dimension reduction both for linear and nonlinear dimension reduction problems.  A method developed recently by the investigator and his collaborators utilizes classic two-class Support Vector Machine algorithms to develop a new class of algorithms for linear and nonlinear sufficient dimension reduction under a unified framework.  In this work the investigator extends this methodology in several directions. First, different extensions of the classic Support Vector Machine algorithms are used to improve the performance and the asymptotic properties of the original method. Second, the method is extended to algorithms that allow for multi-class classification.  Third, the investigator develops new method-specific and method-free variable selection methodologies for sufficient dimension techniques based on ideas in the machine learning literature.  Finally, new algorithms for the order determination of the dimension reduction space based on the new methodology are developed.<br/><br/>Recent advancements in computer science have increased computer power and, subsequently, the capability of storing large datasets efficiently.  Thus, to analyze large datasets effectively in many sciences, like Biology, Meteorology, Genetics and Economics, new techniques are needed to reduce the dimensionality of the datasets.  This work creates new algorithms to reduce the dimensionality of large datasets effectively, for both linear or nonlinear relationships between variables.  These techniques transform a high-dimensional regression or classification problem to a lower-dimensional one, which helps to identify hidden relationships among variables.  The methodology being developed will be an efficient tool for scientists working with large datasets, and it will open new research frontiers to statisticians to develop new ideas in the area of dimension reduction."
"1208917","Collaborative Research:  Multidimensional Curve Estimation for Diffusion MRI","DMS","STATISTICS","08/01/2012","07/25/2012","Owen Carmichael","CA","University of California-Davis","Standard Grant","Gabor Szekely","06/30/2014","$48,380.00","","owen.carmichael@pbrc.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","","$0.00","Integral curves are natural models for a variety of biological phenomena, from neuron fibers in brain imaging data to jet streams in atmospheric data. Traditionally they have been modeled as solutions to differential equations defined on fields of direction vectors that are observed with noise in a 3D domain. But advances in imaging technology now provide much more complex directional information--functions defined on the 3D sphere-at each location in the domain. Integral curves traced from this enhanced directional data have the potential to dramatically increase our understanding of biological phenomena such as brain connectivity, but the statistical properties of integral curve estimators for this cutting-edge data are not well understood. Therefore in this project the investigators will provide a solid theoretical foundation for integral curve estimation in 3D fields of complex directional data and apply it to large corpuses of real data sets from ongoing scientific studies. The primary plan will be to model directional data locally using high-order supersymmetric tensors, and pose integral curve estimation in terms of ODEs defined on the field of their pseudo-eigenvectors. The investigators will show that the proposed integral curve estimators enjoy optimal convergence rates in a minimax sense, and prove that balloon estimators of the pseudo-eigenvector fields will lead to improved convergence. Then integral curve estimators will be linked to accompanying random processes to allow construction of uniform confidence bands around point estimates for curves; and adaptive estimation of these confidence bands will be explored to make them practically useful. The investigators will then study whether estimation may be improved further by selecting arbitrary 3D measurement locations, possibly using enhanced imaging techniques. Finally, a test for branching of integral curves will be constructed, for example at locations where axon fibers diverge or cross.<br/><br/><br/>The proposed work has the potential to dramatically increase the usefulness of diffusion magnetic resonance imaging (MRI) data, a technology with tremendous potential to probe the ""wiring diagram"" of the brain-- its connectivity-- in living people. Currently, brain connectivity measurements are widely regarded as brittle, complicated, and difficult to validate. For each individual receiving a diffusion MRI scan, the investigators will estimate curves describing the trajectories of axon fibers, the electrical ""wires"" of the brain. These fibers connect brain regions into distributed networks that give rise to thought; the evolution of this brain wiring in response to normal development, gene expression, aging, disease, drugs, and environmental factors is of primary interest to a broad swath of neuroscience. Simply providing scientific end-users with a sense of whether or not they should believe the estimated fiber trajectories provided to them by computer programs will greatly enhance their ability to make confident decisions about relations between such trajectories and other scientific data. In addition, the proposed methodology is also relevant in meteorology. There, isolines, fronts, jetstreams, and pressure troughs in weather data can be modeled by similar curve trajectories that can be used to enhance existing weather maps. Finally, this proposal has an exciting educational impact. The investigators, a statistician and a computer scientist with neuroscience training, envision building an interdisciplinary team of promising young researchers in statistics and neuroimaging who gain exposure to both the mathematical and neuroscience aspects of curve estimation through joined group meetings, graduate courses, and web resources related to theory and applications. This unique cross-pollination will prepare the trainees to contribute to the broadly interdisciplinary research teams that are ascendant in the sciences."
"1208771","Novel statistical models for text mining with applications to Chinese history and texts","DMS","STATISTICS","07/01/2012","07/02/2014","Jun Liu","MA","Harvard University","Continuing Grant","Gabor Szekely","06/30/2016","$400,000.00","Peter Bol, Ke Deng","jliu@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","","$0.00","In this project, the investigators study a series of challenging problems of extracting information from Chinese text, including: (1) word/phrase discovery, (2) text segmentation, (3) technical term recognition, and (4) association discovery among technical terms. Different from alphabetical languages such as English, Chinese has many special properties: no word boundaries, no clear definition of words, traditionally no punctuation, and a unique grammar. Thus, it is problematic to apply most methods developed for alphabetical languages directly to Chinese. Moreover, the available methods for analyzing Chinese text in the literature have many limitations. Instead the investigators propose an advanced word dictionary model (AWDM) that can simultaneously achieve word discovery, text segmentation and technical term recognition, which are traditionally studied separately. The idea is to build up a word dictionary first by enumerating all word candidates satisfying a certain criterion from the texts and assign to each word candidate a latent word type label representing different types of technical terms (such as names, addresses, office titles, time labels, as well as background texts) and corresponding word usage frequencies. Then, a Markov dependence model among different words and word types is given to model the potential grammatical and semantic structure of the texts. With the help of the training data (i.e., lists of known technical terms), the AWDM can automatically select the most meaningful words from the huge space of word candidates, determine the word type for each word based on not only the content of the word but also the context around the word, and segment the texts based on both grammatical and semantic information. Compared to the existing methods in the literature, the AWDM enjoys a better efficiency due to the joint modeling of the grammatical and semantic information and the integrated analysis of word discovery, text segmentation and technical term recognition. Combined with other text mining tools, such as topic models and theme dictionary models, the proposed method will lead to a powerful multi-level (Chinese character level, word/phrase level, theme level, topic level) analysis platform for Chinese texts.<br/><br/>With the explosive growth of the internet and digital technologies, large quantities of digitalized Chinese texts can be easily collected. For example, lots of Chinese historical documents written in traditional Chinese are now available in digital form; and, public media such as new papers, forums, blogs and microblogs, are producing huge amounts of Chinese text every day. Thus there is great appeal in developing text mining tools to automatically extract information from these data and create new knowledge. The ideas and approaches in this project may have significant impacts on how Chinese history will be studied. An efficient and reliable method for extracting information from the ever growing databases of digitized historical documents will enable researchers to analyze change over time based on large numbers of disaggregated data points, something impractical in the past. Furthermore, although originally designed for Chinese, these approaches have the potential to be applied to other Asian languages similar to Chinese, such as Japanese and Korean, and thus provide a powerful multi-language platform for the study of Asian history. In addition, the novel way of combatting the challenges in recognizing named entities studied in this project also has the potential to be extended to alphabetical languages such as English. Finally, the ideas and approaches studied in this project have the potential to be generalized into a systematic tool that digests any data flow of Chinese texts, and outputs a structured database that contains key information about the individuals and organizations described by the input data, thus making it easier for researchers to discover social network of all kinds of ""units"" in our social life. Various item association patterns discovered by our algorithms are also invaluable to the study of public media and sociology, and may help reveal new important epidemiological events and societal trends in a timely fashion. These types of information can have important implications in business decision making and governmental policy making."
"1209103","Bayesian Computation, Guaranteed Efficient (or Intractable)","DMS","STATISTICS","07/15/2012","06/10/2014","Dawn Woodard","NY","Cornell University","Continuing Grant","Gabor Szekely","06/30/2015","$150,000.00","","woodard@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","","$0.00","This research and education program will characterize and reduce the error associated with Markov chain and other computational methods in Bayesian statistics.  The introduction of simulation-based computational methods has allowed the field of Bayesian statistics to dramatically expand; despite this, the biggest challenge for its wider adoption is still its computational difficulty.  Understanding of the error associated with Bayesian computational methods has lagged far behind their use, and in some cases all available methods are too inefficient. The goals of this project are (1) distinguish classes of computationally tractable Bayesian models from those of computationally intractable models, (2) develop methods for guaranteed efficient computation for tractable models, and (3) introduce alternatives to intractable models that allow efficient computation. Tractability means that there exists a computational method that scales efficiently with the number of parameters, observations, or other statistical quantities. For motivation and application of the techniques, statistical problems arising in management of commercial and nonprofit operations will be used; case studies include Emergency Medical Services operations and (distributed computing) data-center operations.  Statistical challenges in these contexts include estimation on road networks and fine-scale spatio-temporal demand estimation.<br/><br/>When statistics are used to guide business, engineering, or public policy decisions, for instance, there may be large amounts of information and data from different sources that have to be drawn together to inform those decisions.  These large and various sources of information must be analyzed in a way that does not require a large amount of computing time.  This program advances understanding regarding what analyses we can expect to be able to do quickly; it develops more efficient computational methods for those analyses; and in the case of analyses that are simply too time-consuming it develops easier alternatives.  Case studies and scientific approaches from this project will be incorporated into the classroom, and Master's and Ph.D. students will be an integral part of the research process."
"1201539","Travel grants for U.S. participants to attend WCPS 2012, Istanbul, Turkey","DMS","PROBABILITY, STATISTICS","02/01/2012","01/30/2012","Raisa Feldman","CA","University of California-Santa Barbara","Standard Grant","Tomek Bartoszynski","01/31/2013","$59,508.00","","feldman@pstat.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","MPS","1263, 1269","7556","$0.00","Travel support is requested for about twenty U.S. participants (recent Ph.D.s, women, and members of underrepresented minority groups) to attend the 8th World Congress on Probability and Statistics to be held in Istanbul, Turkey from 9th to 14th of July, 2012. <br/><br/>The Congress is organized under the auspices of the Bernoulli Society for Mathematical Statistics and Probability and the Institute of Mathematical Statistics. It is the major worldwide event bringing together researchers working in the fields of statistics, probability and stochastic processes and is held every four years.<br/><br/>An important part of the intellectual development of researchers at the beginning of their career is to attend international conferences where they have an opportunity to interacts with people working in a wide range of topics. The conference covers a wide range of active research areas and  features 14 Invited Plenary Lectures presented by leading specialists. In addition, there will be 40 special Invited Sessions, consisting of three talks each, as well as Contributed and Poster Sessions covering many aspects of applied and theoretical probability, statistics and stochastic processes."
"1209057","Dimension Reduction Through Index Models","DMS","STATISTICS","06/01/2012","05/21/2012","Peter Radchenko","CA","University of Southern California","Standard Grant","Gabor Szekely","05/31/2015","$129,585.00","","radchenk@marshall.usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","1269","","$0.00","Historically statistics has dealt with extracting as much information as possible from a small data set. However, much of modern statistical research focusses on data sets that have enormous numbers of predictors.  This phenomenon is a direct result of recent technological advances that have affected various fields of research, such as image processing, computational biology, and finance.  This proposal addresses a very important question of fitting nonlinear regression models in high-dimensional situations, where the number predictors may be much larger than the number of observations.  Unlike linear or generalized linear models, high-dimensional nonlinear regression is a very young research area that requires systematic and extensive development.  Due to the curse of dimensionality, most of the work in this area has been conducted under the assumption that the regression function has a simple additive structure.  The investigator proposes novel methodology for fitting index type regression models in high dimensions.  The new methods cover models that are either complimentary or strictly more general than the additive models studied before.  For each of the methods the proposal presents a computationally efficient fitting algorithm and lays out a plan for establishing theoretical results.<br/><br/>The proposed research is expected to have a broad impact on the practice and education of statistics and related fields.  Disciplines such as Computational Biology, Finance, Marketing and Machine Learning are highly interested in the type of methodology that is targeted in this proposal.   The investigator plans to systematically develop software for implementing the proposed methods through free software packages and then make them readily available to researchers in the aforementioned fields.  The proposed research will also have an impact on the growth and development of the new USC Statistics Ph.D. program.  Several students in this young program will be involved in methodology research, algorithm development, and theoretical investigation.  They will get hands on experience and guidance in the very important field of high-dimensional statistical inference."
"1208799","Building a theoretical and methodological framework for collaborative statistical inference and learning: multi-party and multiphase paradigms","DMS","STATISTICS","09/01/2012","09/02/2014","Xiao-Li Meng","MA","Harvard University","Continuing Grant","Gabor Szekely","08/31/2016","$360,000.00","","meng@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","","$0.00","Scientific data almost always undergo filtering, imputation, and other forms of preprocessing before they are analyzed. When such steps are taken, the data analysis becomes a collaborative endeavor by all parties involved in data collection, preprocessing, and inference. This research terms such settings as falling within the multiparty and multiphase paradigms for statistical inference and learning. These settings are rife with subtleties and pitfalls. Each party does not and often cannot have a perfect understanding of the entire phenomenon at hand; the final results will inevitably contain some combination of their judgments, and some preprocessing can irreversibly destroy information from the raw data.  Building upon his previous theory and methods for dealing with uncongeniality with multiple imputation, the PI and his students aim to develop a set of statistical theory and methods to understand such problems and to provide better preprocessing, inferences, and learning. Their ultimate goals include providing methods for assessing the validity of such collaborative analyses, guidance on statistically-principled preprocessing, and a rich new theory of statistical learning and inference with multiple parties. The theoretical framework they develop can shed light on principles and methods for constructing more useful scientific databases, handling complex measurement processes, and analyzing massive datasets.<br/><br/>With the dramatic increases in the size, diversity, and complexity of data available for scientific discoveries, medical advances, education reforms and evidence-based policy making, the entire enterprise of quantitative scientific inquiry has been presented with unprecedented challenges and opportunities. The vast majority of current inquiries are not made by a single individual or even a single team. In particular, the analysis of scientific data depends heavily on preprocessing in practice. Following data collection, raw data is typically transformed into a more easily handled form. Such transformations range from innocuous to highly destructive. When poorly executed, they can destroy huge scientific investments by rendering them useless for future analyses. These dangers are rising in importance as scientists and funding agencies emphasize the construction of scientific repositories and ""big data"". Despite its importance, preprocessing is poorly understood from a theoretical perspective. Even among statisticians, conventional wisdom and informal guidance are the norm. The PI and his students will work to close this gap, building a theory of preprocessing and collaborative inference. This theory aims to guide the construction of scientific repositories and the analysis of massive datasets generated by the latest technologies. It can also open the doors to greater collaboration and access to high-quality scientific data, broadening the scientific enterprise."
"1208952","Collaborative Research: New Developments for Analysis of Two-way Structured Functional Data","DMS","STATISTICS","09/01/2012","06/30/2014","Jianhua Huang","TX","Texas A&M University","Continuing Grant","Gabor Szekely","08/31/2016","$225,104.00","","jianhua@stat.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1269","","$0.00","Most existing methodologies of functional data analysis are limited to data structured in one domain, such as time. Two-way structured functional data are indexed by two functional domains, such as space and time, and each domain has its own notion of regularity, such as smoothness or sparsity. Fully considering the two-way structure of the data will lead to more accurate analysis results. Recent work by the lead PI on two-way regularization has provided some preliminary results and a good starting point for investigating new methodology of dimension reduction, feature extraction, regression and classification for two-way functional data. This research team plans to further develop the methodology for two-way functional data in several important directions:(a) Develop a Reproducing Kernel Hilbert Space theory for two-way regularized singular value decomposition (SVD); (b) Develop novel dimension reduction methods for data that are indexed by general domains such as manifolds; (c) Develop dimension reduction methods that effectively analyze discrete two-way functional data; (d) Develop two-way regularization methods for solving the magnetoencephalography (MEG) inverse problem; (e) Develop new classifiers for diagnosis of mental disorders using dynamic MEG images as predictors; (f) Develop robust methods that are resistant to outliers. The success of the research will add a new dimension to functional data analysis and significantly enrich the field.<br/><br/>Two-way structured functional data arise in various disciplines, including medicine, social sciences, earth sciences, economics, and business. But few existing methodologies fully take into account the two-way structure of this type of functional data. The novel statistical methods developed in this research will provide valuable tools for efficient use of such data. They will provide better understanding of scientific, social and economic phenomena and make more accurate predictions. In particular, the two-way reguarlized SVD provides a new analysis of variance method for analyzing high throughput bioinformatics data and for discovering interactions among biomarkers and clinical variables that are associated with a disease phenotype. The new MEG inverse solvers will facilitate noninvasive presurgical mapping of functional areas of the brain. The new classifiers will help diagonosis and assessment of mental diseases using dynamic MEG images. The proposed activities involve training of Ph.D. students who participate in the proposed projects and mentoring of the female, junior statistician P.I. in a psychology department. Research results will be disseminated through collaborative work, academic presentations, and journal publications. Web pages will be created to enable quick access to user-friendly and accessible software implementations of new methods as well as technical reports and relevant references."
"1241502","Workshop on New Directions in Monte Carlo Methods","DMS","STATISTICS","09/01/2012","09/01/2012","Hani Doss","FL","University of Florida","Standard Grant","Gabor Szekely","08/31/2013","$8,576.00","James Hobert, Kshitij Khare","doss@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","7556","$0.00","The workshop will be held January 18-19, 2013, on the campus of the University of Florida.  Although Monte Carlo methods have existed for a long time, the problems to which they are applied have changed dramatically.  Monte Carlo methods are now routinely applied to very complex problems, for instance Bayesian regression models with a large number of predictors and Bayesian hierarchical models involving many levels.  To address this increased complexity, recent research on Monte Carlo approaches has proceeded in several new directions.  For example, because in highly complex models it is no longer possible to analytically devise Monte Carlo algorithms which are optimal or near-optimal, researchers have developed ""adaptive MCMC algorithms"" which, as they are running, automatically evolve into algorithms which are optimal for the current problem.  Another example involves Bayesian model selection, where researchers have many models that can be used to explain the data, and they wish to select the best one.  In a Bayesian approach, a prior is placed on the set of potential models, and the researcher wishes to obtain the posterior distribution of the models, and the parameters for the models.  Because the parameters for the different models may have different dimensions, Markov chains for estimating posterior distributions must be ""transdimensional.""  In this workshop, twelve distinguished individuals who work in Monte Carlo simulation review the current state of the field and present their recent work.  A number of young researchers will also participate in the workshop and present their work in poster sessions.<br/><br/>Monte Carlo simulation is a methodology that uses random sampling to arrive at numerical approximations to quantities that cannot be computed exactly.  The methodology allows researchers to use extremely complex statistical models: if a potentially useful model is so complicated that it is not possible to obtain exact solutions, the model can still be considered if one is willing to use approximate solutions provided by Monte Carlo simulation.  Recent advances in computing power have made Monte Carlo simulation increasingly accurate and useful, but many unsolved problems remain.  The workshop provides an excellent opportunity for established researchers in the field, as well as newcomers, to discuss the significant developments that have taken place in the last decade; to discuss what works and what does not; and to identify important problems and new research directions."
"1208896","Geostatistical Modeling of Spatial Discrete Data","DMS","STATISTICS","09/01/2012","07/30/2014","Victor DeOliveira","TX","University of Texas at San Antonio","Continuing Grant","Gabor Szekely","08/31/2016","$149,991.00","","victor.deoliveira@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","MPS","1269","","$0.00","Statistical methods for the analysis of spatial discrete data are relatively underdeveloped when compared to methods for continuous data. This is a notable methodological gap since the former are routinely collected in the earth and social sciences. For instance, death counts due to different causes are collected on a regular basis by government agencies throughout the entire U.S. and classified according to different demographic variables, such as age, gender and race. This project aims at filling this gap by developing a comprehensive study of models for geostatistical discrete data. The project consists of three parts. First, a class of hierarchical spatial models is developed that seeks to ameliorate some limitations identified by the investigator of currently used models. Some of these limitations, relating to the spatial association structures representable by these models, are especially severe when the data consist mostly of small counts, precisely the case when models describing the discreteness of the data are most needed. The properties of these new models and likelihood based methods to fit them are studied. Second, a class of non-hierarchical spatial models is developed that seeks to represent  a wide range of spatial discrete data, not just counts, having spatial association structures that are complementary to those in the class of hierarchical spatial models. The models in this class are constructed by separately modeling the marginal and spatial association structures, using an approach akin to copulas. The properties of these models and likelihood based methods to fit them are also studied. Third, a recently proposed Bayesian method to assess goodness-of-fit of statistical models is studied and its soundness for use in the aforementioned classes of models explored. The method, based on a distributional identity between pivotal quantities evaluated at different parameter values, is applicable to both hierarchical and non-hierarchical models. Developing such methods is a pressing need since formal methods to assess model adequacy of spatial models are notoriously lacking.<br/><br/>Spatial data are nowadays routinely collected in many earth and social sciences, such as ecology, epidemiology, demography and geography, but methodology for the analysis of discrete data (say death counts) is much less developed than the corresponding methodology for the analysis of continuous data (say temperature). The investigator proposes to fill this gap by constructing new classes of models that on the one hand ameliorate some limitations identified by the investigator of currently used models, and on the other hand increase the data patterns represented by the models. The project will also develop methodology to assess model adequacy for the newly proposed models, a ubiquitous task in science since any model is an imperfect representation of the phenomenon under study. The statistical methodology developed in the course of this project would have immediate methodological and practical impacts on the earth and social sciences, where spatial discrete data are routinely collected but models and methods for their analysis are scarce. The proposed classes of models will substantially increase the arsenal of tools available to spatial data analysts and the possibility of representing a wide range of behaviors for spatial discrete data. Graduate students will be engaged in the project which will contribute to their statistical training in Bayesian methods and Spatial Statistics, as well as the projection into the future of the Ph.D. program in Applied Statistics at the University of Texas at San Antonio."
"1208791","Collaborative Research: Advanced Statistical Methods and Computation for Emerging Challenges in Astrophysics and Astronomy","DMS","STATISTICS","07/01/2012","08/20/2014","Xiao-Li Meng","MA","Harvard University","Continuing Grant","Gabor Szekely","06/30/2015","$164,000.00","","meng@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","","$0.00","The California-Boston AstroStatistics Collaboration is developing model-based strategies for statistical inference in astronomy and astrophysics.  They specifically design highly structured models to account for particular complexities in the sources and data generation mechanisms with the goal of answering specific scientific questions as to the underlying astronomical and physical processes. This strategy requires state-of-the-art statistical inference, sophisticated scientific computing, and careful model-checking procedures. They will employ, extend and publicize inferential and efficient computational methods under highly-structured models that involve multi-scale structure and/or multiple levels of latent variables and incomplete data.  Such models are ideally suited to account for the many physical and instrumental filters of the data generation mechanisms in astrophysics. The collaboration specifically aims to develop a mixture of parametrized and flexible multi-scale models that can be combined with complex computer-models to describe spectral, spatial, and timing data, either marginally or jointly.  In astronomy, for example, the analyses of data from the same observation, but in different regimes (images, spectra, and time series) are typically conducted separately. This simplifies analysis, but sacrifices information, for example as to how a spectrum varies over time or across an image. The Collaboration proposes to develop coherent methods for multi-regime data including the joint use of high throughput spatio-spectral data to isolate and identify complex solar features and the analysis of systematic temporal variance in spectra from stellar coronae. They also propose to embed complex computer models into highly structured models, a strategy which allows the combination of multiple computer models along with physics-based parametric and/or flexible multi-scale models to derive comprehensive methods that address complexities in both the astronomical sources and the instrumentation.  Building such highly structured models requires subtle tradeoffs between complexity and practicality and fitting them poses significant computational challenges. This proposal includes a suite of research projects that aim to produce efficient tailored Monte Carlo methods.<br/><br/>Dramatic advances in space-based instrumentation over the past decade have led to the deployment of a new generation of telescopes with unprecedented capabilities.  Such instruments are often tailored to meet specific scientific goals and are increasing both the quality and the quantity of data available to astronomers.  Massive new surveys are resulting in enormous new catalogs containing terabytes of data, in high resolution spectrography, imaging, and time-series across the electromagnetic spectrum, and in ultra high resolution imaging of explosive dynamic processes in the solar atmosphere.  Scientists wish to draw conclusions as to the physical environment and structure of astronomical source, the processes and laws which govern the birth and death of planets, stars, and galaxies, and ultimately the structure and evolution of the universe.  This combination of complex instrumentation and complex science leads to massive data analytic and data-mining challenges for astronomers.   The California-Boston AstroStatistics Collaboration plans to tackle these challenges using principled statistical methods derived from carefully designed astronomical and mathematical models.  As the Collaboration develops methods and distributes free software, it will also educate the astronomical community as to the benefit of sophisticated statistical methods.  It is expected that a fundamental impact of the proposed research will be more general acceptance and use of appropriate methods among astronomers. The Collaboration not only aims to develop new methods for astronomy but plans to use these problems as springboards in the development of new general statistical methods, especially in signal processing, multilevel modeling, computer modeling, and computational statistics. The collaboration will use the statistical challenges posed in astronomy as a testing ground for new sophisticated inferential and computational techniques that will help solve complex data analytic challenges throughout the natural, social, medical, and engineering sciences."
"1208841","Collaborative Research: Prior-free probabilistic inferential methods for ""large-p-small-n"" linear regression problems","DMS","STATISTICS","06/15/2012","05/10/2013","Chuanhai Liu","IN","Purdue University","Continuing Grant","Gabor Szekely","05/31/2015","$85,000.00","","chuanhai@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","","$0.00","The investigators study prior-free probabilistic inference with ""large p, small n"" regression analysis.  This is made possible in the new framework of Inferential Models (IMs) proposed recently by the investigators.  Statistical results produced by IMs are probabilistic and have desirable frequency properties.  In this study, the investigators develop IM-based methods for linear and certain non-linear regression analysis.  A sequence of topics in the context of large-p-small-n regression to be investigated include (1) variable selection in Gaussian regression models; (2) robust Student-t regression; and (3) binary regression models.<br/><br/>Linear regression is one of the most commonly used methodologies in statistical applications.  However, desirable prior-free and frequency-calibrated probabilistic inference, particularly in the important variable selection context, has not been available until the recent development of IMs.  The IM framework provides a new and promising alternative to the well-known Bayesian and frequentist methods for various high-dimensional problems researchers currently face.  In this study, the investigators develop new statistical methods and computing software, generating useful tools for applied statisticians and scientists who are challenged by very-high-dimensional data in carrying out regression analysis."
"1208968","Modeling and Analysis of Genomic Imprinting and Maternal Effects","DMS","STATISTICS, Cross-BIO Activities, MATHEMATICAL BIOLOGY, MSPA-INTERDISCIPLINARY","09/01/2012","07/12/2017","Shili Lin","OH","Ohio State University","Standard Grant","Gabor Szekely","08/31/2018","$220,000.00","","shili@stat.osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","MPS","1269, 7275, 7334, 7454","1112, 7465, 8007","$0.00","Genetic imprinting and maternal genotype effects are two epigenetic phenomenon, which may lead to heritable changes in gene expression or cellular phenotype without altering the underlying DNA sequence. These epigenetic factors, also known as parent-of-origin effects, have been increasingly explored for their roles in complex traits as part of a concerted effort to find the ""missing heritability"". Research on parent-of-origin effects has taken on a new dimension as the next generation sequencing technology becomes wildly available. However, despite the great biological and technological progress and innovation, statistical methods are lacking behind.  Most existing methods for studying imprinting/maternal effects are restricted to data from nuclear families. The applicability of such methods is further hindered by the need to make strong but unrealistic assumptions to reduce the number of parameters to avoid overparametrization. In addition, imprinting and maternal effects are confounded and maternal effect is believed to be heterogeneous; all these further complicate modeling and analysis efforts.  This project takes up this challenging problem and aims to develop novel statistical and computational models/methods and software for extended families and nuclear families without making the strong but unrealistic assumptions. For nuclear family data from retrospective studies, in addition to case families, the study design also considers controls, which can be in the form of control nuclear families or internal controls from unaffected siblings of case families.  This novel design makes it possible to formulate a partial likelihood approach, wherein the likelihood component of interest is free of the nuisance parameters.  This circumvents the problem of overparametrization and unrealistic assumptions that plague existing methods.  For extended pedigrees from prospective studies, the focus is on the development of a method for incorporating heterogeneous maternal effects and addressing the issue of missing data, which is common for extended pedigrees.  Methods proposed will be implemented in software that will be made publicly available.<br/><br/>The importance of epigenetics in the twenty first century cannot be overstated.  To borrow science writer David Shenk's words, epigenetics is ``perhaps the most important  discovery in the science of heredity since the gene''.  In this regard, genomic imprinting and maternal effects,  two aspects of the epigenetic process, holds important roles as they are essential for normal mammalian growth and development but can also cause devastating diseases and birth defects. Genetic imprinting refers to the epigenetic marking of the parental origin of a gene, which leads to the same DNA sequence being expressed differently depending on whether it is inherited from the mother or from the father.  It is well known that Prader?Willi syndrome and Angelman syndrome are genetic disorders involving genetic imprinting. Maternal effect refers to the influence of the prenatal environment provided by a mother, which may arise from the expression levels of some genes carried by the child being altered by the additional genetic materials passed from the mother during pregnancy. Biological research increasingly reveals the presence and importance of maternal effect in many diseases such as childhood cancer and birth defects. This projects aims to develop novel statistical methods and computational software that circumvent difficulties in identifying genes that bear imprinting and/or maternal effects using nuclear and extended family data.  The tools developed will be made available to the larger scientific community to aid scientific discoveries and finding treatments for genetic diseases.  This project will also contribute to the training of the next generation of researchers in a cutting-edge interdisciplinary research area that fuses knowledge in biology, statistics and computer science."
"1208982","Random Matrix Theory and High Dimensional Statistics","DMS","STATISTICS","09/01/2012","06/09/2014","T. Tony Cai","PA","University of Pennsylvania","Continuing Grant","Gabor Szekely","08/31/2016","$254,944.00","","tcai@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","","$0.00","High dimensional data are becoming increasingly available from a wide <br/>range of scientific investigations, including genomics, bioinformatics, <br/>engineering, and climate studies. Sound analysis of such datasets poses <br/>many statistical challenges. It calls for new statistical theory and methods <br/>as well as new technical tools. In this collaborative research project, the <br/>investigators will first develop results and  technical tools in random matrix <br/>theory and then take a unified approach using the technical tools developed <br/>to study several important problems in high dimensional statistics <br/>as well as applications in signal processing, physics, and mathematics. <br/><br/>The statistical and scientific objectives outlined in this proposal are interdisciplinary <br/>and will establish connections among different fields - random matrix theory, high <br/>dimensional statistics, signal processing, physics, and mathematics. The research <br/>will also provide technical tools as well as methodology, to researchers in other <br/>scientific fields who collect and analyze high dimensional data. These include, <br/>but are not limited to, genomics, biostatistics, and electrical engineering. The procedures and algorithms developed in this project will be implemented and softwares developed will be made freely and publicly available on the web as open source<br/>code along with the associated research reports so as to facilitate the dissemination of knowledge."
"1209161","Bayesian Methods for Socio-Spatial Point Patterns and Networks","DMS","STATISTICS","09/15/2012","09/11/2012","Catherine Calder","OH","Ohio State University","Standard Grant","Gabor Szekely","08/31/2016","$180,000.00","Christopher Browning","calder@austin.utexas.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","MPS","1269","","$0.00","This project seeks to make contributions to statistical science in the areas of spatial statistics, network analysis, and Bayesian parametric and nonparametric modeling.  Underlying the methodological aspects of the research is a particular area of application, contextual effects and exposure analysis research.  Statistical methodology will be developed to improve the ability to characterize human activity patterns and social networks using state-of-the-art large-scale sample survey data. Specific contributions include the development of novel Bayesian stochastic models for multivariate spatial point patterns based on dependent nonparametric latent intensity functions. Statistical models will also be developed for spatially-referenced social networks; these models describe relational ties based on proximity in a random deformation of geographic space. Both classes of models will be thoroughly studied from both a theoretical and empirical perspective. <br/><br/>The statistical research focus of this project is deeply rooted in an area of application bridging the social, geographic, and health sciences.  Research on contextual effects aims to understand the consequences of social and environmental exposures on individual outcomes.  Efforts to quantify the influence of the multiple contexts to which individuals are exposed are limited due the inherent complexities of existing data sources, as well as those currently being collected using state-of-the-art GPS-based technologies.  This project aims to overcome these data challenges by introducing novel statistical methods and is expected to contribute generally to the field of statistics, as well as the motivating area of application.  Educational and training components of the project will reflect this multidisciplinary research setting."
"1209142","Bayesian Inference via Markov Chains, Diffusion Processes, and Distributed Computing","DMS","STATISTICS","06/01/2012","05/21/2012","Radu Herbei","OH","Ohio State University","Standard Grant","Gabor Szekely","05/31/2017","$149,869.00","","herbei@stat.osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","MPS","1269","","$0.00","The investigator studies long-time behavior for diffusion processes and exact sampling algorithms under various scenarios, to address the following main challenges that arise in a high-dimensional, non-linear  Markov chain Monte Carlo (MCMC) setting: (1) data/model-grid discrepancy, (2) high computing cost for likelihood evaluation, (3) distance of the (dependent) samples from the desired target distribution, (4) tuning the MCMC, and (5) high computing cost associated with the serial nature of a MCMC procedure. The target distribution explored by the Markov chain is the posterior distribution resulting from a Bayesian approach to an inverse problem, where the forward model is a differential equation. Such situations often arise in earth sciences and biological applications. For challenge (1), the investigator uses diffusion processes and Feynman-Kac representations for solutions of partial differential equations to eliminate the need for ""matching"" the data to the physical model grid as it is often done in spatial settings. For challenges (2) and (3), the investigator uses suitably generated Bernoulli random variates to overcome the need for likelihood evaluation. In addition, the investigator develops drift and minorisation conditions for the corresponding Markov chain transition kernel. These conditions are used in a perfect sampling algorithm to obtain independent and identically distributed draws from the posterior distribution. The investigator addresses challenge (4) by carefully constructing diffusion processes converging in total variation distance to the posterior distribution. He uses distributed computing via graphical processors (GPU) to overcome the computational challenges associated with high-dimensional MCMC algorithms.<br/><br/>In modern science, researchers build probability models under complex settings to assess and understand the variability of a physical system under study. For example, in weather prediction, scientists compute probabilities of various events of interest in the near future. Such models are explored and summarized using computationally intensive methods. It is extremely important that the outputs of these algorithms have the desired confidence, in order to be useful for the decision making process. The investigator develops efficient computational methods for exploring complex probability models by combining methodological advances with highly efficient parallel computing using graphical processors. The project has an educational aspect in that it involves graduate students under the supervision of the investigator."
"1206693","A New Treatment to Dimension Reduction via Semiparametrics","DMS","STATISTICS","07/01/2012","04/19/2012","YANYUAN MA","TX","Texas A&M University","Standard Grant","Gabor Szekely","10/31/2014","$300,969.00","","yzm63@psu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1269","","$0.00","The principal investigator (P.I.) will study six topics in dimension reduction problems, develop new methodologies and analyze their properties and performances. Topic One concerns central space estimation. The P.I. will initiate a totally different approach from the current literature, using a semiparametric treatment. The new view point results in a complete class of estimators for the central space which contains all possible estimators. In addition, it relaxes various conditions currently required in the existing methods. Finally, it illustrates the relations between various existing estimators and reveals the underlying reason which enables all these estimators to function. Topic Two concerns central mean space estimation. The P.I. will establish parallel results to Topic One, using the same general idea but via different analytic derivation. Topic Three concerns two common conditions in dimension reduction: the linearity condition and the constant variance condition. The P.I. will reveal an astonishing discovery that these conditions are not only redundant, but also detrimental. They are redundant in that consistent estimation can still be carried out of these conditions are relaxed. They are detrimental in two ways. 1. If these conditions are not met, but are falsely assumed, the classical estimators are biased. 2. If these conditions are met and are used, the classic estimators will have inflated variances  compared to the same estimator without using these conditions. This research will prompt re-evaluation of these popular conditions and divert the current research trend of further exploiting these conditions. Topic Four concerns statistical inference and efficient estimation of the central space. The P.I. will devise a convenient parameterization and proposeto convert the space estimation problem to an equivalent parameter estimation problem which facilitates statistical inference. The P.I. will also establish the semiparametric efficiency bound for the central space estimation and will derive the optimal efficient estimator. She will further provide theoretical proof of the root-$n$ convergence rate and the efficiency. Topic Five concerns inference and efficient estimation for the central mean space. The P.I. will establish parallel results to Topic Four, illustrate the difference between the two problems and highlight their drastically different analytic results. Topic Six concerns deciding the dimension of the reduced space. The P.I. will propose three methods tailored to different estimation procedures and examine their usefulness.<br/><br/>The series of projects in this proposal will shed new light on the dimension reduction problems and resolve some of the most fundamental and outstanding issues in this field. Accompanying the modern development of sciences and technologies, richer and more complex data have become a common phenomenon. Data sets from medical science, genetics, environmental science, social behavioral science including internet behavior studies easily contain a large amount of variables that dimension reduction is unavoidable. The new methodologies will generate wide interest and have important application in these fields. They will also provoke further studies and development in related semiparametric problems and computing methods in statistical sciences itself."
"1208962","Functional Depth and Quantiles: Limit Theory, Comparisons and Applications","DMS","STATISTICS","09/01/2012","08/08/2014","Joel Zinn","TX","Texas A&M University","Continuing Grant","Gabor Szekely","08/31/2015","$100,000.00","","jzinn@math.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1269","","$0.00","This investigator proposes to study functional depth and functional quantiles using techniques developed for Empirical Process theory. Some recent techniques developed by this investigator and his colleagues allow one to obtain central limit theorems for quantile processes formed from functional data, and even can be applied to introduce new methods in the study of finite dimensional data, obtaining interesting variations on already existing examples of data depth. Moreover, within the context of functional depth,  this has uncovered some unforeseen problems. One such problem is that for natural processes, such as Brownian motion, and for some natural definitions of depth, one might have depth which is identically zero. Together with colleagues, this investigator has introduced a certain type of smoothing which allows one to eliminate this problem in many special cases. However, it is still necessary to develop a general, realistic, and usable approach for a wide variety of circumstances. So, in addition to developing an asymptotic theory for functional data, this project proposes to develop a coherent methodology for smoothing/modifying the data to allow for such analyses.<br/><br/>The contemporary statistician must deal with data that appears in many quite different forms. Much energy has been spent on one-dimensional data, and researchers have achieved a great deal of success. While there are still many important questions in this area, the analysis of finite-dimensional data has also become a vibrant and important area for research. One critical difference between one-dimensional data and finite dimensional data is the lack of an obvious ordering of the data  in dimensions greater than one. To alleviate this problem, and better analyze (finite) multidimensional data, the concept of data depth has been introduced and studied by many authors. There are a variety of examples of such depth, each with its own set of good properties. One can choose a particular data depth to analyze a given data problem, depending on which properties are the most important. One can also compare various forms of depth on the same set of data to find contrasts that otherwise may not be apparent.  As this area has matured, so has the ability of the researcher to better fit the type of depth to the problem at hand. Even more recently, due to improved computing tools, real time monitoring of many processes is available and consequently, there is a growing need to analyze such data. This is often referred to as functional data. In mathematical parlance, this is considered infinite dimensional data. Data of this type occur, for example, in medicine, neuroscience, chemometrics, signal transmission, stock markets and meteorology. A robust methodology is important to successfully handle the resulting problems, and as is to be expected, this requires methods beyond those used to study the finite dimensional situation.  It is not surprising to a researcher in statistics or mathematics, that like many other infinite dimensional problems, infinite dimensional depth is fraught with differences and difficulties not found in the finite dimensional setting. The techniques proposed by this investigator to study functional data partially rely on a theory in which this investigator has made many contributions, and which is, now, quite well developed. These techniques have already allowed this investigator and his colleagues to uncover some of the difficulties that must be overcome."

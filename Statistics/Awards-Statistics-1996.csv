"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"9625496","Mathematical Sciences:  Inference for Nonparametric         Regresssion","DMS","STATISTICS","07/01/1996","06/17/1996","Randall Eubank","TX","Texas A&M Research Foundation","Standard Grant","Joseph M. Rosenblatt","06/30/1999","$60,000.00","","eubank@math.asu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","DMS 96-25496  Eubank  Problems of inference for parametric and nonparametric regression are investigated.  Tests of goodness-of-fit for parametric regression models are proposed that derive from the comparison of parametric and nonparametric estimators.  This is accomplished for rather general models that include generalized linear and time series models.  New methods are developed for analytic, asymptotic comparison of these nonparametric type tests through the use of intermediate asymptotic efficiency.  These testing ideas are extended to more general goodness-of-fit problems, such as testing for additivity, where the null model is even nonparametric. Other inference problems are also studied, including several proposals for the construction of asymptotically valid confidence intervals for nonparametric regression that have good finite sample properties.    It is often reasonable to believe in many areas of science, such as biology, engineering, psychology, etc., that the data being collected is produced by two components: a nonrandom component, representing a characteristic of nature common to all individuals or subjects, and a random component that accounts for individual variation.  The nonrandom component represents the reproducible or predictable aspect of the phenomenon being studied and is therefore of considerable interest. In many cases, from theoretical considerations or past experience, one can postulate a mathematical framework or model that is believed to describe the nonrandom component. When such models are correct they can provide useful summary and predictive tools. However, an incorrect model can lead to misleading conclusions and inaccurate predictions of future events. Thus, it is important to have methodology for assessing the accuracy of postulated models for data. This research focuses on the development of new tools for this purpose that rely on the comparison of two types of estimators for the nonrandom component: namely, an estimator that is computed under the assumpt ion that the postulated model is correct and a very flexible estimator that does not employ this assumption. The theoretical statistical issues that are addressed in this work include the development of objective criteria for comparing the two estimators and the determination of values for these criteria which indicate that the data does not support a postulated model. Other problems that are considered include the use of the data to construct intervals that have a known, specified, chance of containing the unknown, nonrandom component of the data."
"9626532","Mathematical Sciences:  Leveraged Bootstrap","DMS","STATISTICS","06/01/1996","05/15/1996","Jian-Jian Ren","NE","University of Nebraska-Lincoln","Standard Grant","James E. Gentle","05/31/1999","$63,000.00","","jjren@umd.edu","2200 VINE ST BOX 830861","LINCOLN","NE","685032427","4024723171","MPS","1269","0000, OTHR","$0.00","DMS 96-26532  Ren     The objective of this research is to investigate a new resampling   method, called the Leveraged Bootstrap. The Leveraged Bootstrap facilitates   research in a broad class of nonparametric and semiparametric    statistics using various types of censored data, including right censored   data, doubly censored data, interval censored data, etc.. This method does   not depend on the extensions of the usual statistics for complete data to   incomplete data, and is simple, computationally efficient and easily    applicable to a wide of variety of statistical inference problems with    different types of censored data. In this research, the following issues   are considered: (i) how the leveraged bootstrap should be applied in   practice; (ii) the efficiency of the leveraged bootstrap; (iii) comparison   with other methods. Specifically, the investigator studies the application   of the leveraged bootstrap in some statistical inference problems such as the   empirical likelihood methods in constructing confidence bands and tests,    and the hypothesis testing problems which are associated with several   statistical models.   %%%   Various types of censored data are generally referred to as   incomplete data in statistics literature. For instance, a patient who has   died from heart disease cannot go on to die from lung cancer. In such a   case, the survival time of lung cancer of the patient is incomplete.   Recently, doubly censored data and interval censored data have been   encountered in some very important clinical trials such as breast cancer   research and AIDS research. The statistical research on these types of   censored data still generally lags behind that on right censored data.   The principle difficulties in statistical analysis using these complicated    types of censored data are that the usual methods developed for complete   data often do not have direct extensions to incomplete data, and that the   usual nonparametric bootstrap can be quite computationally time  consuming   in certain cases. This research is to develop some new statistical methods   which are accurate, computationally efficient and easily applicable for   different types of censored data.  ***"
"9626113","Nonparametric Modeling and Prediction for Time Series       Analysis","DMS","STATISTICS","06/15/1996","06/14/1996","Rong Chen","TX","Texas A&M Research Foundation","Standard Grant","Joseph M. Rosenblatt","05/31/1999","$65,000.00","","rongchen@stat.rutgers.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","DMS9626113  Chen  This research is concerned with nonparametric model building  procedures and nonparametric prediction methods in nonlinear time  series analysis.  The first objective of  this research is to  develop a new nonparametric modeling procedure for nonlinear time  series.  The investigator studies the functional coefficient  autoregressive models and makes the model easier to use in  practice.  In particular, a weighted local linear regression  procedure is studied.  This procedure differs from the classical  local linear regression for curve fitting  where the response  function is of interest.  Here, estimating the coefficient  functions are of main interest.  A procedure for detecting  discontinuities in the coefficient functions is studied as well.  The second objective of this research is concerned with   multi-step predictions using nonparametric smoothing techniques.  The investigator studies the properties of a multi-stage  nonparametric predictor, which is closely related to the  iterative integration procedures for multi-step prediction.  Preliminary study shows that the new method does improve the  accuracy of the prediction.  The first goal is to show that the  predictor is applicable to a wide class of nonlinear AR models.  The second goal is to investigate the practical implementation of  the method, particularly the automatic bandwidth selection method  and prediction strategy.  This research is concerned with model building procedures and  prediction methods in nonlinear time series analysis.  A time  series is a set of data observed over a period of time.  For  example, daily ozone and pollutant readings for environmental  study, quarterly unemployment rate or GNP for economical study  and noisy telecommunication signals are all subjects of time  series analysis.  Time series analysis tries to reveal the   generating mechanism of the observed time series and to provide  sensible methods to predict future observations based on current  and past information.  Linear ti me series models assumes the  future observations relate to the current and past observations  in simple linear functions while nonlinear models assume complex  relationship.  In this research, the investigator follows the  principle of `letting the data speak for themselves' and develops  modeling procedures for nonlinear time series.  It is used to  overcome the difficulty encountered in real applications of  choosing an appropriate model.  The second objective of this  research is concerned with multi-step predictions for nonlinear  time series. Nonlinear time series models have been shown to have  certain advantages in multi-step forecasting over linear models.  In this research, the investigator studies the properties of a  new predictor that improves the prediction accuracy.  There are  sufficient reasons to believe that the results of this research  should have significant contributions in nonlinear time series  analysis, which has many important applications in the fields of  economics, telecommunication, meteorology, environment and many  others."
"9626865","Mathematical Sciences:  Workshop:  Statistical Image        Analysis - July 1-5, 1996","DMS","STATISTICS","06/01/1996","05/15/1996","Lynne Billard","GA","University of Georgia Research Foundation Inc","Standard Grant","Stephen M. Samuels","05/31/1997","$15,000.00","","lynne@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","Billard    The proposal requests support for fifteen U.S-based individuals from academic and nonprofit institutions to participate in the Workshop on Statistical Image Analysis to be held in Sydney, Australia, July 9-12, 1996.  The workshop will be held in conjunction with the Sydney International Statistical Congress.  About one-third of those supported will be those with an official capacity and two-thirds will be new researchers, especially those traditionally underrepresented groups such as women and minorities.  Availability of awards will be advertised in publications of the ASA, the IMS and the Biometric Society.  Awardees will be selected by an Awards Selection Committee."
"9628290","Mathematical Sciences:  International Biometric Conference","DMS","STATISTICS","06/01/1996","05/24/1996","Lynne Billard","GA","University of Georgia Research Foundation Inc","Standard Grant","Stephen M. Samuels","05/31/1997","$15,000.00","","lynne@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","Billard    The proposal requests support for fifteen U.S-based individuals from academic and nonprofit institutions to participate in the International Biometric Conference to be held in   Amsterdam Netherlands on July 1-5, 1996.  This is the biennial meeting of the International Biometric Society.  About one-third of those supported will be those with an official capacity and two-thirds will be new researchers, especially those traditionally underrepresented groups such as women and minorities.  Availability of awards will be advertised in publications of the ASA, the IMS and the Biometric Society.  Awardees will be selected by an Awards Selection Committee."
"9629283","Workshops: Pathways to the Future","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","10/01/1996","10/07/1999","Lynne Billard","GA","University of Georgia Research Foundation Inc","Standard Grant","William B. Smith","09/30/2000","$48,000.00","","lynne@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1260, 1269","0000, OTHR","$0.00","9629283  Billard    ABSTRACT    Support is provided for three Pathways to the Future workshops to be held in conjunction with the annual Joint Statistical Meetings in Anaheim in 1997, Dallas in 1998 and Baltimore in 1999.  Since 1988 these workshops have assisted young women researchers in Statistics and Probability, providing a forum and a mentoring experience.  A committee of alumnae and other senior researchers will assist in locating and contacting potential award recipients, with the goal being that every young woman researcher who wishes to participate should have the opportunity to do so."
"9523878","Establishing Chemometric and Statistical Foundations of     Receptor Models","DMS","STATISTICS, STATISTICAL AND SIMULATIONS","05/01/1996","05/14/1999","Clifford Spiegelman","TX","Texas A&M Research Foundation","Standard Grant","Joseph M. Rosenblatt","04/30/2000","$140,000.00","Ronald Henry","cliff@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269, 1956","0000, 9198, EGCH, OTHR","$0.00","         Receptor modeling is a cross-disciplinary research area  that finds chemical   fingerprints of pollution sources.  Receptor models use methods  from chemometrics,   chemistry, statistics, and image analysis, as well as many other  fields.  These models   commonly use principal component analysis, and factor methods as  their main ingredients.   Receptor modeling requires good scientific knowledge to  reasonably model observed data.   For example, knowing that  acetylene is a tracer for motor vehicle emissions is essential to   successful receptor modeling.  The methods that this research  develops will be used to  provide fingerprint identification of  pollution sources.  The research will also provide a  firm  mathematical basis for the methods.            This research provides a firm chemometric and statistical  basis for receptor modeling  and is an important contribution to the Federal Strategic Area of   the Environment.  Receptor models will serve as unbiased  scientific evidence for identifying pollution sources. Receptor  models should be able to stand untarnished in legislative and  court proceedings."
"9626115","Mathematical Sciences:  Problems in Simple and Complex      Block Designs","DMS","STATISTICS","08/01/1996","01/06/2000","John Morgan","VA","Old Dominion University Research Foundation","Standard Grant","Joseph M. Rosenblatt","07/31/2000","$60,000.00","","jpmorgan@vt.edu","4111 MONARCH WAY","NORFOLK","VA","235082561","7576834293","MPS","1269","0000, 9148, MANU, OTHR","$0.00","DMS 9626115  Morgan     This project examines design problems for comparing a set of   treatments under a variety of blocking structures.  The simplest of   these is that of a single blocking variable which partitions   n experimental units into blocks of  k units each, k being less than   the total number of treatments. It has long been believed that a   necessary condition for a good design for this structure is that    the assignment be binary. This project is expanding the scope for   which the alternative idea of  nonbinary  treatment assignment can   yield  good designs, where ""good"" may be expressed in terms of the   standard mathematically formulated optimality criteria    and, in some cases, ability of the design to maintain important   treatment structure in the information matrix. For situations with   more than one blocking factor present, the range of conceivable   relationships among them is quite large. The investigator is also   studying the interplay between nesting, crossing,  and other   blocking factors, with the goal of identifying classes of these more   complex structures for which optimal design conditions can be   determined with existing methods. Design constructions are being   formulated for each setting as well.   %%%   The general problem of comparing a set of experimental conditions,   or  ""treatments,"" when faced with heterogeneity in the available   units of material on which the experiment is to be performed, is   common to many disparate fields of scientific and industrial inquiry.   The technique of  blocking, one of the early great contributions of    statistics and the experimental process, specifically addresses this   problem. Typically using values of some identifiable nuisance   factor, the experimental units are partitioned into homogeneous   subsets called blocks. Comparison of measurements occurring in   the same block can then be made with greater precision than those   in different blocks, the heterogeneity having  been eliminated from   such compar isons. Thus the experiment  is improved, in the very   practical sense that it gives information  that is more reliable. The   design problem itself,  which is the topic of this study, is to   determine which treatments are assigned to which units in which   blocks, said assignment being driven by the desire to maximize the   overall quality of  information the experiment will ultimately   produce. The problem naturally becomes more complicated as more   nuisance factors, with various relationships among one another, are   introduced or identified. By studying the situation mathematically,   large families of good designs are produced which are then   applicable to a host of experimental situations. Applications run the   gamut of experimental inquiry, from the manufacturing realm to the   agricultural.   ***"
"9626750","Survival Analysis and Related Topics","DMS","STATISTICS","07/01/1996","06/21/1996","Zhiliang Ying","NJ","Rutgers University New Brunswick","Standard Grant","Joseph M. Rosenblatt","06/30/1999","$56,553.00","","zying@stat.columbia.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","9197, EGCH","$0.00","Ying    The research develops statistical models, methodologies and  related theory to address issues arising from design,  modeling and analysis of survival data,  which are often subject to censoring, truncation and/or  other types of missingness. It introduces new hazard-based  regression models for survival data that extend and  complement the standard proportional hazards regression,  and yet still possess essential good features that the  latter has. Besides the usual right-censored data, it  also provides solutions to such important and difficult  problems as interval censoring, random effects and  multivariate survival times by exploiting model variation  and developing suitable analytic tools. It involves finding  computationally convenient representations and establishing  sharp theoretical properties for the analysis of discrete  (survival) data, particularly of those that can be summarized  by two-by-two or more general contingency tables. New methods  and theory tailored to retrospective studies, which are often  summarized as the case-control and case-cohort regression problems,  are also developed.    Survival data are collected in many scientific  investigations, including biomedical follow-up studies,  industrial life tests, study of economic durations,  astronomy, demography among others. Development of suitable  statistical methods is crucial to handling of such data so that  valid scientific conclusions can be derived. This project  develops several novel approaches, some of which are suitable  for identification of health related risk factors, such as exposure to  hazardous environment, consumption of certain foods etc., and  others are applicable to evaluation of therapeutic effects as well  cost effectiveness of medical treatments. These new methods may impact  other areas of public and scientific interests, an example of which is  the study of factors influencing unemployment duration."
"9632662","Using CAVE Technology to Explore High-Dimensional Data","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS, SIGNAL PROCESSING SYS PROGRAM","03/15/1996","03/20/1996","Dianne Cook","IA","Iowa State University","Standard Grant","Joseph M. Rosenblatt","02/28/1999","$50,000.00","","dicook@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269, 1271, 4720","9218, 9237, 9263, HPCC","$0.00","This research explores visualization methods for examining  high-dimensional data with low-dimensional projections with CAVE  Virtual Reality technology. Dynamic graphical methods for  exploratory  data analysis are examined in a 3-d immersible environment. The  methods are based on generating motion graphics from continuous  sequences of 3-d projections of high-dimensional data.  Interactive  user control is considered in the form of controlling the path of  the  3-d sequences in the data space.     Virtual Reality offers the construction and realization of an  artificial 3-dimensional world that has important potential use  in  scientific areas. Statistics is the science of making sense of  complicated data. For example, typical environmental data sets  may  contain thousands of data values on hundreds of variables.  Graphical  methods are needed for exploring and determining relationships  among  variables such as tree crown health, topography, temperature,  population density and atmospheric pollutants. This research  explores  statistical graphical methods for visualizing many variables  simultaneously using CAVE Virtual Reality technology. The  research  relates to the Federal Strategic Area of Improved Environmental  Quality in that the examined methods can provide insights for  examining spatially measured environmental data. It also relates  to  the Federal Strategic Area of Harnessing Information Technology  because it examines using Virtual Reality technology for  organizing  and synthesizing information."
"9626249","Estimating Equations and Second-Order Theories","DMS","STATISTICS","06/01/1996","06/06/1996","Bing Li","PA","Pennsylvania State Univ University Park","Standard Grant","Joseph M. Rosenblatt","05/31/1999","$63,000.00","","bing@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, OTHR","$0.00","DMS 96-266249  Li       The research is  focused on  two problems related to second order theories:    one on the  assessment of accuracy and the other on the refinement of estimating   equations.  The  accuracy  of an estimate is determined  partly by the quality of the estimator and partly by chance.  Traditionally  the  chance element was  accounted for by the conditional  or the  Bayesian  approach,  which require an ancillary or a prior distribution. However, the  chance  element can exist  without either.  The first part of the research  tackles this  problem by direct estimation of loss via asymptotic expansions and geometric  analyses.  A  particular  result  obtained  in this  direction is that  the  inverted  observed information  best approximates  the squared  error.  The  second part of the research is concerned with improving the  second-order accuracy   of an estimating equation.  Two methods were  previously  proposed,  but they are  inapplicable  to an important  type of  situations,  which  motivates  this  project.  In addition, several previously unknown properties of estimating   equations are investigated in the light of their  analogy with the    second-order  properties  of the classical maximum  likelihood  estimator.  This research will  yield  deeper  understanding  and  more  effective use of semiparametric methods.  %%%       The recent advances in science,  particularly in medical, biological,  sociological,  and ecological studies, have drastically increased the scale  and complexity  of data sets.  This change,  hand in hand with the ever increasing  computer power, gives new challenges to traditional statistical methodologies.  One  area of  studies  that these challenges  have brought about is that of  estimating equations, which is the focus of the present research. Estimating  equations  allow scientists  to model directly the  parameters which are of  the most  interest  without making  excessive  assumptions  (as traditional  methods often do),  whose violat ions would  impair the inference  about the  parameters.  Estimating equations are especially useful for data  sets with complicated  dependence  structures, such as longitudinal studies  and the studies of plants  scattered in a natural environment.  The studies  of estimating  equations have  undergone vigorous  advances during the past  decades,  most of which,  however,  are concerned with what might be called  the coarser-level aspects (or first-order aspects).  Whereas the studies of  the  finer-level  aspects (or second-order properties)  have just  begun to  catch up. The second-order aspects of estimating equations are systematically  investigated in the present research."
"9524770","Mathematical Sciences:  Statistics in Atmospheric Sciences","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, LARGE-SCALE DYNAMIC METEOROLOG, GLOBAL CHANGE","07/01/1996","04/29/1996","Peter Guttorp","WA","University of Washington","Standard Grant","Joseph M. Rosenblatt","12/31/1999","$399,511.00","Christopher Bretherton, Donald Percival, James Hughes","guttorp@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1253, 1269, 1527, 1577","0000, 1325, EGCH, OTHR","$0.00"," 9524770  Guttorp    The problem of subgrid scale variability in general circulation models,  and the generally poor quality of the precipitation part of such models,  indicate the need for precipitation models based on large-scale atmospheric  processes.  A realistic stochastic model of precipitation can be based   on meteorologically homogeneous weather states, each driving a simple   stochastic model of precipitation. In order to determine appropriate weather   states, a variant of canonical correlation analysis, appropriate for dependent   data, is developed.  Statistical theory allowing a space-time decomposition   of atmospheric fields with attendant standard error assessment is developed.  When applied to the leading canonical variates, such a method can isolate spatial   and temporal scales of interest. A slightly different approach uses a hidden   Markov model, driven by atmospheric data in a less explicit form than expressed   above. This model is fairly accurate on relatively small spatial scales, and for  temporally homogeneous parts of the year.  This hidden Markov model approach  is extended to a seasonal model, and its performance compared with the weather state approach, as well as with general meteorological forecast models.  %%%  The  general circulation models used to assess climate change have relatively  poor performance when it comes to precipitation. Furthermore, in order to  assess the effect on regional hydrology of climate changes, models with a finer  resolution than the circulation models are needed, since the hydrologic  processes usually operate on a much smaller scale than the large-scale  atmospheric processes that dominate the global climate models. Rather than  developing precise deterministic models of rainfall, the researchers build  a partly probability-based class of models, and compare it with  meteorological forecast models and applied to both weather data and circulation  model outputs.  ***"
"9532039","Mathematical Sciences:  Estimation in Semiparametric Models and Empirical Processes","DMS","STATISTICS","07/01/1996","08/03/1999","Jon Wellner","WA","University of Washington","Standard Grant","John Stufken","09/30/2002","$138,176.00","","jaw@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, OTHR","$0.00","DMS 95-32039:  Wellner    This research involves the study of  empirical processes,  bootstrap methods in statistics, semiparametric models, and   inverse problems and nonstandard asymptotics.  The research will   involve limit theory for infinite - dimensional  M - estimates  and   related bootstrap methods, preservation theorems for uniform  Donsker   classes of functions, uniform in P bootstrap limit theorems,   estimation of monotone and convex functions, convergence of  iterative   convex minorant algorithms, and the behavior of global  functionals in   models with interval censoring.    %%%  For the analysis of failure time data which are subject to right   censoring and/or left truncation, many methods are now available.    However, in many biomedical studies, including those arising from    HIV/AIDS studies, failure times are censored or truncated in more    complicated ways.  One such complication is ``interval  censoring'',   in which one knows that the event of interest occurs within a  random   time interval, but not the exact timing: for example,  HIV  positivity   or the drop of the CD4 count below a certain threshold may only  be   determined to be between two clinical visits, but not the exact  time  may remain unknown.   This research will involve developing new   statistical method for data arising from these and other more   complicated censoring mechanisms ranging from animal   carcinogenesis experiments to data in sociology and   econometrics modelling.  A detailed study and evaluation    of the properties of methods recently proposed by other  statisticians   will be completed.   ***"
"9626347","Biased Sampling and Confidence","DMS","STATISTICS","07/01/1996","01/14/1998","Michael Woodroofe","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Joseph M. Rosenblatt","06/30/2000","$188,993.00","","michaelw@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","DMS 9626347  Woodroofe      The research concerns biased sampling models and   exponential time series.  In biased sampling models, the probability   of including a subject in the study may depend on variables of   interest--for example, the size of the subject.  Interest centers on   cases in which the inclusion probabilities depend on the variables of   interest in a monotone way, but without assuming that the inclusion   probabilities are known or known up to a few parameters.  The   research involves developing tests for the presence of bias,   developing estimation procedures when bias is present and   studying the properties of both tests and estimators.  Likelihood   and penalized likelihood are used to develop the tests and   estimators, and the properties are studied through a combination of   asymptotic analysis and simulation.  An exponential time series is a   stochastic process whose finite dimensional distributions form   exponential families.  For such process, the sampling distributions   of normalized maximum likelihood estimators are asymptotically  normal, under quite general conditions.  The research involves   developing higher order approximations to the these sampling   distributions and using the refined approximations to form   corrected confidence sets.  Mathematically, the results take the form   of very weak expansions in which a Bayesian approach is used   to obtain approximations to sampling distributions.      Biased samples arise frequently in investigations that involve   searching for hidden objects, since the probability of finding an   object may depend on its properties.  For example, astronomers are   more likely to find a large bright galaxy than a small dim   one, and geologists are more likely to find a large oil well than a   small one.  Previous work on such problems has concentrated on   the case in which the inclusion probabilities depend on the variables   of interest in a known way.  The new research involves developing   data analyses that are appr opriate when the latter   relationship is not known and includes methods for estimating the   relationship from observed data.  The relationship is important,   because the number of objects not found depends on it in crucial   way.  The research considers exponential time series which  include classical time series, like stock prices and weather, but also   many others like adaptively designed experiments (experiments   that design themselves) and sequential clinical trials."
"9625732","Asymptotic Approximations in Probability and Statistics","DMS","PROBABILITY, STATISTICS","07/01/1996","02/26/1998","Sandor Csorgo","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","K Crank","06/30/1999","$96,000.00","","csorgo@sol.cc.u-szeged.hu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1263, 1269","0000, OTHR","$0.00","9625732 Csorgo  ABSTRACT  In probability, the investigator studies a concrete construction of an infinitely divisible approximation to the distribution of a sum of independent and identically distributed random variables with a completely arbitrary distribution. This construction, motivated by a purely probabilistic approach to the problem, has the potential to achieve the best possible overall rate of such approximations. In statistics, the investigator studies three different and unrelated groups of problems. One of these is concerned with the overall asymptotic normality, at the best possible rate of convergence, of certain least squares estimators for the exponent of a regularly varying tail of a distribution function, by means of the quantile-transformation method. The second is the exploration of universal large-sample Gaussian approximations to cumulative-hazard and product-limit processes under random censorship without any conditions on the estimated distribution or the censoring mechanism, where the investigator uses a combination of strong-approximation and martingale-theoretic analyses. The third group of problems is concerned with rounding off a recent theory of nonparametric curve estimation under short- and long-range dependent Gaussian subordination by answering the outstanding questions for random-design regression.   The investigator's Probability project studies an approximation to sums of independent observations, which, unlike the usual approximations in terms of bell-shaped curves-- responsible only for the middle portions of the data -- attempts to incorporate the effects of the extremes as well. In Statistics, the investigator is studying three different groups of problems. The first one is concerned with the robust estimation of measures of ""thickness of the tails"" of data sets. This problem is important in various applied statistical and actuarial situations, such as the assessment of the variability of stock-price changes and the distribution of large claims in insuran ce business, particularly in bankruptcy re-insurance. The second group of statistical problems is to investigate the basic large-sample behavior of estimation when the observed data are incomplete due to the presence of nuisance effects and/or the necessity of an early termination of data collection; situations commonly referred to as random censorship. Here the investigator's anticipated results aim at greatly stretching the limits of concrete applicability of statistical estimation techniques for survival analyses with randomly censored data. These are of great interest in medical follow-up studies and reliability engineering. In the third group of problems in Statistics, the investigator extends his recent theory to cover all major curve-estimation patterns (density estimation, fixed-design and random-design regression) with long-memory data sets. The complete theory will explore the basic distributional properties of commonly used curve-estimation methods in time series analysis when the observations may be long-range dependent, as in many typical applied problems in geophysics and hydrology."
"9626102","Adaptive Sampling","DMS","STATISTICS, Methodology, Measuremt & Stats","07/01/1996","05/22/2000","Steven Thompson","PA","Pennsylvania State Univ University Park","Standard Grant","Joseph M. Rosenblatt","06/30/2001","$200,000.00","","skt@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269, 1333","9278, EGCH","$0.00","    DMS 9626102  Thompson    Adaptive sampling designs are designs in which the procedure for  selecting the units to include in the sample may depend on  observations made during the survey.  For some populations,  adaptive strategies produce substantial increases in precision of  estimates compared to conventional designs with equivalent sample  sizes.  The research investigates new classes of adaptive designs  and inference procedures and develops the needed basic theory.   The research involves a combination of design-based and  model-based approaches, and includes investigations in adaptive  cluster sampling, adaptive allocation, adaptive graph sampling,  optimal sampling strategies, multivariate methods in adaptive  sampling including rapid-assessment strategies, and nonsampling  errors in adaptive sampling.    With adaptive sampling designs, the procedure for selecting the  sample can be changed during a survey in response to observed  patterns in the population.  For example, in a survey of a rare,  endangered animal species, whenever members of the species are  detected, additional observations may be made at neighboring  sites.  In environmental pollution assessment studies, an  adaptive plan allows additional observations to be made in the  vicinity of observed ""hot spots.""  Similarly, in surveys of   hard-to-access human populations, social links between  individuals may be adaptively used in obtaining the sample.  Such  designs are in marked contrast to conventional survey designs, in  which the sites or people to be included in the sample can be  determined prior to making any observations or obtaining any  responses.  Research in adaptive sampling has many important  applications in environmental studies, including surveys of  endangered species, natural resources, and environmental  pollutants, as well as for scientific studies of hidden or  hard-to-access human populations.  Advantages of adaptive  sampling strategies include the potential for obtaining better  estimates of popu lation quantities with a given amount of  sampling effort.  Adaptive cluster sampling designs, for example,  have been shown to be highly efficient relative to conventional  designs for sampling populations that are very unevenly  distributed.  Adaptive designs can significantly increase the  ""yield"" of the sample, so that, for example, more animals of the  rare species are observed, while at the same time permitting  unbiased estimation of quantities such as the total number of the  animals in the population.  Adaptive designs can be used to  increase the probability of finding the values of most interest   to investigators, such as the highest concentrations of a  pollutant.  In many scientific studies of hidden or  hard-to-access human populations, adaptive link-tracing designs  provide the only practical means for obtaining a sample large  enough for study, so that effective methods for making estimates  from such samples are vitally important.  ***  DMS 9626102  Thompson    Adaptive sampling designs are designs in which the procedure for   selecting the units to include in the sample may depend on  observations   made during the survey.  For some populations, adaptive  strategies  produce substantial increases in precision of estimates compared  to conventional designs with equivalent sample sizes.  The  research  investigates new classes of adaptive designs and inference  procedures  and develops the needed basic theory.  The research involves a  combination of design-based and model-based approaches, and  includes investigations in adaptive cluster sampling, adaptive  allocation, adaptive graph sampling, optimal sampling strategies,  multivariate methods in adaptive sampling including   rapid-assessment strategies, and nonsampling errors in adaptive  sampling.     %%%  With adaptive sampling designs, the procedure for selecting the   sample can be changed during a survey in response to observed  patterns in the population.  For example, in a survey of a rare,   endangered animal species,  whenever members of  the species are detected, additional observations may be made  at neighboring sites.  In environmental pollution assessment  studies,  an adaptive plan allows additional observations to be made in  the vicinity of observed ""hot spots.""  Similarly, in surveys of   hard-to-access human populations, social links between  individuals   may be adaptively used in obtaining the sample.  Such designs are  in  marked contrast to conventional survey designs, in which the  sites   or people to be included in the sample can be determined prior to    making any observations or obtaining any responses.    Research in adaptive sampling has many important applications in   environmental studies, including surveys of endangered species,  natural   resources, and environmental pollutants,  as well as for  scientific studies  of hidden or hard-to-access human populations.    Advantages of adaptive sampling strategies include the potential  for  obtaining better estimates of population quantities with a given  amount   of sampling effort. Adaptive cluster sampling designs, for  example, have   been shown to be   highly efficient relative to conventional  designs  for sampling populations that are very unevenly distributed.    Adaptive designs can significantly increase the ""yield"" of the  sample,   so that, for example, more animals of the rare species are  observed, while   at the same time permitting unbiased estimation of quantities  such as the   total number of the animals in the population.  Adaptive designs  can be   used to increase  the probability of finding the values of most  interest   to investigators, such as the highest  concentrations of a  pollutant.    In many scientific studies of hidden or hard-to-access human  populations,   adaptive link-tracing designs provide the only practical means  for obtaining   a sample large enough for study, so that effective methods for  making estimates  from such samples are vitally important.    ***"
"9626189","Multivariate Estimation for Astronomy","DMS","EXTRAGALACTIC ASTRON & COSMOLO, STATISTICS","08/01/1996","07/24/1996","Gutti Babu","PA","Pennsylvania State Univ University Park","Standard Grant","Joseph M. Rosenblatt","09/30/1999","$95,000.00","Eric Feigelson","babu@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1217, 1269","0000, OTHR","$0.00","DMS 9626189  Babu     Many important problems in astronomical research require models that are  often composite and nonlinear, far more complex than the models assumed in  standard multivariate analysis.  While these models can sometimes be  calculated analytically, they often can only be represented by Monte Carlo  simulations, particularly when selection biases are involved in the data  collection.  In such cases, the empirical distribution does not approach the  underlying population distribution and standard parameter estimation is  inapplicable.  The investigators study an estimation procedure, based on  projections of multivariate datasets on to 1-dimensional spaces, to derive  optimal values and constraints on the hidden parameters   Kolmogorov-Smirnov-type statistics based on the projections will be used to  compare the data with models.  Asymptotic consistency and the asymptotic  distribution of the new statistics will be evaluated using approximation  theory of empirical processes.  When applied to astronomical problems,  `best-fit' model parameters may be computed even for biased multivariate  data and complicated models.  %%%      Many important problems in astronomical research involve applying  complicated astrophysical models to datasets with many variables:  the  evolution of galaxies since the Big Bang, the distribution of matter in our  Galaxy, the age of the oldest stars in globular clusters.  The astronomer  seeks insight into the validity of the models and the range of model  parameters consistent with the data.  The models are often very complex, and  real datasets frequently suffer from known selection biases (e.g. only the  brighter galaxies are detected).  We develop and apply the mathematical  and statistical tools necessary to treat such problems.  This project is part of a   long-term effort to promote intellectual integration of two disciplines:  statistics,  which has sophisticated tools for understanding data; and astronomy, which  confronts fundamental questions a bout our physical Universe.  ***"
"9624940","Mathematical Sciences:  Advances in Resampling              and Data-Depth","DMS","STATISTICS","09/01/1996","08/19/1996","Kesar Singh","NJ","Rutgers University New Brunswick","Standard Grant","James E. Gentle","08/31/1998","$40,000.00","","kesar@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00"," DMS 9624940   Singh                               This research is on six subtopics in the area of resampling and  data-depth:  robustification of the bootstrap; second order  balanced confidence regions and bands based on resampling and  data depth; semiparamtric models and the bootstrap; resampling on  bioequivalence; inference spectrum; and descriptive statistics  and inferences based on data-depth.  These areas have been the  investigator's main focus of research in the past decade.   Efron's bootstrap in particular is the dominant subject  throughout the research.  The research may seem mathematical in  nature, however the potential impact on statistical methodology  has been the key factor in the selection of these research  problems.  Besides being of independent interest, the notion of  data-depth has an important application in bootstrap methodology.   The use of data-depth in multivariate descriptive-statistics is  being further explored in the research.    Bootstrap is a new technology in statistics, discovered by B.  Efron in 1979, which has made tremendous advances in the eighties  and the nineties.  This PI has been involved with these advances  since the early years of this discovery.  Bootstrap has had  impact in nearly every area of statistical sciences and this  research is geared toward its further advancement.  One of the  ideas included in this research is the construction of a spectrum  of statistical conclusions using bootstrap when some of the  responses in a survey are missing.  One other thrust of this  research is towards pictorial and other types of summaries when  data have many, many components.  The latter research is based on  what is known as data-depth, which is essentially a measure of  centrality of a datum with respect to the totality of the  data-cloud.  Other topics involve efficient computing when a  methodology is too computer intensive."
"9626601","Statistical Inferences and Markov Chains, Admissibility,    and Strong Inconsistency","DMS","STATISTICS","07/15/1996","07/11/1996","Morris Eaton","MN","University of Minnesota-Twin Cities","Standard Grant","Joseph M. Rosenblatt","06/30/2000","$80,967.00","","eaton002@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","DMS 96-26601  Eaton          This research involves the evaluation of statistical inferences       which are expressed as data dependent probability distributions.       Inferences of particular interest include both posterior        distributions and predictive distributions which are derived        from improper prior distributions coupled with the formal application       of Bayes Theorem. Questions concerning the admissibility of        inferences have lead to the development of new and powerful       techniques based on a fundamental connection between issues in        statistical decision theory and recurrence/transcience issues       involving symmetric Markov Chains. This fundamental connection is       further developed in this research along with the somewhat allied        noton of strong inconsistency for inferences. Applications of this       research include the evaluation and improvement of inferential       techniques in high dimensional models such as those which arise       in large scale environmental models and those attempting to describe       global change.  %%%       Many problems of statistics, such as those which arise in environmental       studies, involve the simultaneous description and/or simultaneous       prediction of many variables. In such cases, one often speaks of        ""high dimensional problems"". This research involves the construction,       evaluation and comparison of statistical methods which are relevant       in these high dimensional problems. Recent developments have shown       that the problems which arise in this research are, rather surprisingly,       closely connected with certain problems which arise in a seemingly       unrelated area of probability. This connection promises to yield       exciting new insights regarding both practical and theoretical issues       involving high dimensional problems.  ***"
"9626159","Bayesian Wavelet Modeling with Applications in Turbulence","DMS","APPLIED MATHEMATICS, STATISTICS, GLOBAL CHANGE","08/01/1996","07/24/1996","Brani Vidakovic","NC","Duke University","Standard Grant","Joseph M. Rosenblatt","07/31/1999","$65,000.00","Gabriel Katul","brani@tamu.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1266, 1269, 1577","1325, 9189, EGCH","$0.00","DMS 9626159  Vidakovic  This research connects recent theoretical advances in both nonlinear wavelet shrinkage theory and Bayesian statistical wavelet modeling to time series measurements of stochastic phenomena.  The researchers study:  (i)  cost measures that serve as criteria for the best wavelet basis selection, (ii)  statistical models in the wavelet domain that range from Hilbert space projections to coherent Bayesian models, and (iii)  the model induced shrinkage in the wavelet domain.  It is demonstrated that wavelet regression and density estimation are excellent tools in denoising and parsimonious description of complex dynamic processes, such as turbulence.  The turbulence  measurements are characterized by  local ``bursts'' in  time and frequency domains are providing an ideal media for testing shrinkage methods, best basis choice, and wavelet modeling. The existence of power laws, consistent with Kolmogorov's K41 theory is used to assess the proposed shrinkage methods.    Wavelets are novel building blocks and excellent descriptors of many complex phenomena in a variety of scientific fields.  This research applies wavelets to an important and omnipresent phenomenon arising in hydrology and atmospheric science:  the turbulence.  Recent theoretical advances in statistical modeling and estimation are combined with the power of several intrinsic properties of wavelets to produce superb tools for modeling, denoising, and analyzing complex turbulence measurements."
"9634300","Mathematical Sciences:  Global Change Research Program","DMS","STATISTICS","09/01/1996","08/26/1996","Robert Lempert","CA","Rand Corporation","Standard Grant","K Crank","08/31/1999","$100,000.00","","Robert_Lempert@rand.org","1776 MAIN ST","SANTA MONICA","CA","904013208","3103930411","MPS","1269","1317, EGCH","$0.00","9634300  Lempert     Although technology issues appear vital to climate-change policy, the Integrated   Assessment community currently lacks a suitable framework for considering the   implications of new technology for policy choices. The fundamental problem is the   unpredictability of the evolution of technology over the time spans of relevance. Currently   available decision-analysis tools are predicated on the ability to craft some credible best-  estimate prediction of the future. However, policy problems such as climate change face   extreme uncertainties for which it is not possible to construct well-characterized best   estimates. In such cases, prediction-based policy analysis is often unable to resolve   acrimonious debate among stakeholders who believe in different best estimates, and often   results in constrained thinking about possible policy options. In their recent work the   investigators have demonstrated an alternative framework for policy analysis that does not   depend on making best-estimate predictions of the future. The approach is based on two   concepts called adaptive strategies and exploratory modeling. Exploratory modeling treats   problems having massive uncertainty by conducting a large number of computer simulation-  experiments on many plausible formulations of the problem, rather than using computer   resources to increase the resolution of a single best-estimate model. Adaptive-decision   strategies focus on modeling environmental policies where decision-makers can make   midcourse corrections based on observations of the relevant environmental and economic   systems. In this research project the investigators will use the exploratory modeling/adaptive   strategies framework to examine how the potential of new technology should affect near-  term climate change policy choices. The research has two specific goals: 1) demonstrate and   advance the capabilities of the exploratory modeling/ adaptive strategies framework, and 2)   examine a central climate-change po licy question -- under what conditions should society   pursue technology-specific policies (e.g., tax credits, subsidies, or government   procurements) in addition to carbon taxes?    Policy-makers must often make decisions whose ultimate success depends on factors that   cannot be predicted at the time the decision is made. However, currently available decision-  analysis tools used to help policy-makers compare alternative policies are predicated on the   ability to craft some credible best-estimate prediction of the future.  Under conditions of   extreme uncertainty, prediction-based policy analysis is often unable to resolve acrimonious   debate among stakeholders who believe in different best estimates, and often results in   constrained thinking about possible policy options. Climate change provides an important   example. Future technology developments could have profound impacts on climate-change   policy, but no one can accurately predict the impacts of technology on the future costs of   reducing greenhouse-gas emissions, nor the effects that various policies will have on these   costs. In recent work, the investigators have demonstrated a new framework for policy   analysis that does not depend on making best-estimate predictions of the future. The   approach is based on two concepts, called adaptive strategies and exploratory modeling,   that take advantage of the new analytic capabilities provided by recent computer   technology. Exploratory modeling treats problems having extreme uncertainty by   conducting computer simulation-experiments on a very large number of plausible estimates   of the future and using sampling strategies and data visualization to find important patterns   in the results. Adaptive-decision focus on modeling environmental policies where decision-  makers can make midcourse corrections based on observations of the relevant   environmental and economic systems. In this research project, the investigators will use the   exploratory modeling/adaptive strategies  framework to examine how the potential of new   technology should affect near-term climate change policy choices. In particular, the project   will: 1) demonstrate and advance the capabilities of the exploratory modeling/adaptive   strategies framework, and 2) examine a central climate-change policy question -- under   what conditions should society pursue technology-specific policies (e.g., tax credits,   subsidies, or government procurements) in addition to carbon taxes?"
"9622749","Mathematical Sciences:  Global Optimization for             Multidimensional Scaling","DMS","STATISTICS, Methodology, Measuremt & Stats","07/15/1996","07/19/1996","Michael Trosset","AZ","University of Arizona","Standard Grant","James E. Gentle","10/16/1998","$59,527.00","","mtrosset@indiana.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1269, 1333","0000, OTHR","$0.00","9622749  Trosset   Numerical algorithms for multidimensional scaling (MDS) construct  geometric configurations from dissimilarity data by attempting to  solve specific optimization problems.  It has long been widely  believed that most MDS problems are plagued by the existence of  local solutions that are not global solutions, and considerable  effort has been expended addressing this difficulty.  Recently,  various researchers have proposed searching for global solutions  by such ""off-the-shelf"" global optimization methods as simulated  annealing, tunneling, and continuation.  These methods are quite  computationally expensive and research by the principal  investigator suggests that they may be largely unnecessary.  This  research involves properly reformulating the structure of many  important MDS problems such that local searches, which invariably  find global solutions, can be exploited.  This research concerns computational methods for multidimensional  scaling, a collection of statistical techniques for  mathematically constructing geometric configurations of objects  from information about the distances between those objects.  For  example, a chemist might want to ""construct"" a molecule with  certain interatomic distances, or a psychologist might want to  geometrically represent a set of stimuli using data about the  differences that human subjects perceived between pairs of  stimuli.  Such problems pose formidable computational challenges.   The focus of this research is on developing more efficient  methods for computing optimal solutions of these problems."
"9625576","Mathematical Sciences:  Nonlinear Demographic Dynamics:     Mathematical Models, Biological Experiments, and Data       Analyses","DMS","POPULATION DYNAMICS, APPLIED MATHEMATICS, STATISTICS, COMPUTATIONAL MATHEMATICS","08/15/1996","05/11/1998","Jim Cushing","AZ","University of Arizona","Continuing Grant","Michael Steuerwalt","07/31/1999","$355,000.00","Robert Costantino, Brian Dennis","cushing@math.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1174, 1266, 1269, 1271","9169, 9263, EGCH","$0.00","9625576  Cushing       A central question in population biology is that of  understanding and explaining observed fluctuations in animal  numbers.  The study of nonlinear dynamics has opened the way to a  new phase of population research in which experiments are focused  directly on phenomena such as equilibria, periodic and aperiodic  cycles, and chaos.  The investigators undertake a spectrum of  activities essential to the testing of nonlinear population  theory: from the translation of biology into the formal language  of mathematics, to the analysis of mathematical models, to the  development and application of statistical techniques for the  analysis of data, to the design and implementation of biological  experiments.  Laboratory populations of flour beetles of the genus  Tribolium are used in the experiments.  By means of their studies  the investigators provide rigorous experimental tests of  nonlinear population phenomena and behavior.  These include: (1)  dynamical transitions from stable equilibria, to invariant loops  (aperiodicities), to period locking, to strange attractors and  chaos; (2) transient and intermittent dynamics with aims towards  defining practical concepts of intermittency for use with  stochastic population models and the testing of some of the  unusual transient behaviors forecast by stochastic nonlinear  models; (3) the dynamics of meta-populations using beetle  populations linked by migration; and (4) the dynamical behaviors  that can be produced by the interaction of environmental  periodicities with nonlinear demographic effects.       The investigators study how biological populations (in  particular, populations of insects) fluctuate in time and how  different circumstances can lead to drastically different, and  sometimes unexpected, changes in these fluctuations.  This study  is carried out by means of an interdisciplinary program that  integrates the use of sophisticated mathematical models and  statistical analysis with the design and implementation of  l aboratory experiments using species of beetles that are  economically important insect pests.  The investigators seek to  describe and explain a variety of patterns in population  fluctuations, ranging from those that are regular and predictable  to those that are irregular and ""chaotic."" They seek to  understand the environmental conditions that give rise to these  various kinds of population behavior.  This understanding is  essential if the impact on biological populations of  environmental perturbations and manipulations (by Man or by  Nature) is to be predicted.  These impacts have far-reaching  consequences, ranging from food production and pest control to  wildlife management and the conservation of species diversity."
"9625383","Mathematical Sciences:  Problems in Hierarchical Model      Determination","DMS","STATISTICS","07/01/1996","02/12/1998","Alan Gelfand","CT","University of Connecticut","Continuing Grant","Joseph M. Rosenblatt","06/30/1999","$135,000.00","","alan@stat.duke.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","0000, OTHR","$0.00","Gelfand    Recent computational advances have made it feasible to fit   hierarchical models in a wide range of serious applications.  With   this capacity various issues in model determination arise.  This   research studies the selection of a best model within a collection of   such models.  It concerns itself with the trade-off between goodness   of fit and parsimony and the inappropriateness or shortcomings of   customary criteria.  The research also addresses the matter of model   adequacy.  With hierarchical models, model failures can occur at each   stage.  Approaches for identifying the existence of such failures and   their nature are examined by the investigator.  Finally, for   hierarchical models hyperprior specification is often vague.  This   work considers conditions under which the resultant posterior is a   proper distribution.    The modern intellectual contribution of statistics is modeling and   inference.  In working with scientists from other fields, the   statistician helps to formulate appropriate models for the   scientists' problems and data as well as to indicate how inference can   be developed which provides answers to the questions which motivated   the scientists' investigations.  With the wide availability of high   speed computing, increasingly sophisticated models are being   considered permitting the research effort to focus on the   determination of the most appropriate model for the application   rather than having to settle for an obviously inadequate one.    Nonetheless, models must offer some simplification or else they are   no easier to understand than the complex phenomenon they are   modeling.  This research addresses several important issues in the   process of determining a satisfying model."
"9626784","Design of Experiments and Canonical Moments","DMS","STATISTICS","07/15/1996","07/24/1996","William Studden","IN","Purdue Research Foundation","Standard Grant","Joseph M. Rosenblatt","06/30/2000","$66,000.00","","studden@stat.purdue.edu","1281 WIN HENTSCHEL BLVD","WEST LAFAYETTE","IN","479064182","3174946200","MPS","1269","0000, OTHR","$0.00","DMS 9626784  Studden  This research is composed of two related parts:  one part deals  with the design of experiments and the other deals with canonical  moments in Statistics and Probability.  The research in the  design area involves the analysis of complex computer codes, the  use of rational functions in regression theory and the analysis  of rigid designs.  Complex computer codes are analysed  asympotically in order to optimally choose input values to use  for further prediction purposes.  The theory involves the use of  stochastic processes, multidimensional interpolation theory and  approximation theory. The second part involves the use of  normalized moments of distributions on the real line called  canonical moments. These have been used successfully in designing  experiments in one dimensional polynomial regression.  The  present study investigates some unanswered questions in moment  theory and random walks.  Experimental design problems in general involve how to choose  which experiments to conduct to obtain maximal information at  minimal cost.The present research involves the analysis of  complex computer codes where the problem is to decide on optimal  input values to predict the outcome at other possible input  values.  This relates significantly to high performance computing  which is one of NSF's current strategic areas."
"9626265","Mathematical Sciences:  Multi-Dimensional Statistical       Analysis","DMS","STATISTICS","07/15/1996","06/25/1996","Ingram Olkin","CA","Stanford University","Standard Grant","Joseph M. Rosenblatt","06/30/2000","$102,000.00","","IOLKIN@STAT.STANFORD.EDU","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","9146, 9187, EGCH, MANU","$0.00","DMS 9626265  Olkin     This research involves the study of life distributions from the  point of view of model selection.  One phase involves methods for  generating parameters in families.  Given a random variable with  distribution and survival functions, there are a number of ways  to impose parameters, in some instances by taking a function of  the random variable, and in some cases by taking a function of  the distribution or survival function.  The following are some  examples: (1) Location and scale parameters; (2) Frailty and  resilience parameters; (3) Power parameter; (4) Convolution  parameter (for special families such as the gamma family); (5)  Geometric-extreme stable parameter.  The investigators study  order relations for these parametric families.  For example, for  which parametric families are the hazard functions ordered; for  which families is there a stochastic or convex ordering.  The  introduction of parameters and their characteristics and their  orderings is fundamental to an insight concerning how to extend  the results to multivariate distributions.    Although the normal distribution plays a central role in many  applications and in statistical theory, it does not serve as  a model for lifetime data arising in industrial applications such  as manufacturing data, materials data, reliability data, nor does  it serve as a model for survival analysis or risk analysis in the  health or environmental sciences.  This research focuses on how  to generate non-normal parametric families that will provide an  insight into behavior patterns and prediction for such data.  In  particular, the methods being developed serve to upgrade risk  and reliability analyses in engineering applications.  A second  stage in this research is the development of multivariate  versions that take account of dependencies among measurements."
"9631278","Mathematical Sciences/GIG:  ""Group Infrastructure Grant for Stanford Statistics""","DMS","OFFICE OF MULTIDISCIPLINARY AC, INFRASTRUCTURE PROGRAM, STATISTICS","09/01/1996","08/02/1996","Iain Johnstone","CA","Stanford University","Standard Grant","Lloyd E. Douglas","08/31/2002","$1,000,000.00","Art Owen, Ingram Olkin","imj@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1253, 1260, 1269","0000, OTHR","$0.00","The Department of Statistics at Stanford University will enhance its research and research training programs by developing the following, interconnected, aspects of its infrastructure:  A.  Graduate Student Support with Interdisciplinary Orientation.   Building on a base of faculty joint appointments in application areas and prior experience, Stanford will expand its training of interdisciplinary Ph.D.s.  by supporting four additional students. There is an extensive network of faculty contacts in relevant departments at Stanford, and support in the curricular structure for building competence in the application area while acquiring a thorough research training in statistics.  B.  Summer Mentoring programs for Women and Minority faculty and for Post-doctoral scholars.   The department will continue and expand a highly successful program to help launch the careers of pre-tenure women and minority faculty and post-doctoral scholars by involving them in the already active program of visitors, seminars and research at Stanford each summer.  C. Computer Infrastructure.  The department aims to provide a research computing environment for its faculty, students and visitors that enables maximum productivity. The department will acquire hardware and software on a staged basis in support of the core equipment already in place. Examples include X terminals to maximize student and visitor access to computing, and extra disk and memory to support increasingly memory intensive research, such as in statistical image processing."
"9632955","Mathematical Sciences:  Support for the 45th Annual Gordon  Research Conference on Statistics in Chemistry and          Chemical Engineering; August 25-30, 1996; Oxford, England","DMS","STATISTICS, Proc Sys, Reac Eng & Mol Therm","06/01/1996","05/07/1996","Lyle Ungar","RI","Gordon Research Conferences","Standard Grant","James E. Gentle","05/31/1997","$10,000.00","","ungar@central.cis.upenn.edu","5586 POST RD","EAST GREENWICH","RI","028183454","4017834011","MPS","1269, 1403","0000, OTHR","$0.00","9632955  Ungar        The Gordon Research Conference on Statistics in Chemistry and Chemical   Engineering has met annually since 1951.  It provides an important and   highly stimulating opportunity for the exchange of new ideas among   researchers and workers in statistics, chemistry and chemical   engineering.  In order to further the exchange of ideas, the 1996   Conference will, for the first time, be held outside the US, in Oxford   (UK).  This will bring US scientists in contact with the leading   scientists in the fields of statistics, chemistry and chemical   engineering in Europe.  The 1996 Conference will work along two lines.   On the one hand, outstanding statisticians present novel and   up-to-date topics relevant for chemists and chemical engineers and on   the other hand, outstanding chemists and chemical engineers present   real-life problems and proposed solutions.  This approach maximizes   the interaction between the different groups: chemists and chemical   engineers learn new statistical methods and statisticians hear about   complex real-life problems.   %%%   The Gordol Research Conference on Statistics in Chemistry and Chemical   Engineering lies on the interface between statistics, chemistry and   chemical engineering.  Hence, the conference attracts a broad spectrum   of participants.  The resulting cross-fertilization of ideas from   different areas of science and engineering significantly benefits all   participants.  Research statisticians are exposed to real problems in   chemistry and chemical engineering; applied chemists and statisticians   learn first hand about new statistical research which they can use in   their everyday work; and research chemists and chemical engineers can   discuss their statistical ideas with eminent statisticians.   Statistical methodology, as covered in this conference, provides   important quantitative tools in numerous fields of research, including   meeting national goals for improved competitiveness in the chemical   industries via  enhanced product quality and productivity, and   recognition and avoidance of environmental hazards.  This conference   will allow open presentation and discussion of recent research on the   frontiers of statistics and its relevance for all areas of chemistry   and chemical engineering. ***"
"9626135","Model Uncertainty in Prediction, Variable Selection and     Related Decision Problems","DMS","STATISTICS","07/01/1996","06/18/1996","Merlise Clyde","NC","Duke University","Standard Grant","Joseph M. Rosenblatt","06/30/2000","$79,000.00","","clyde@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, OTHR","$0.00","DMS 9626135  Clyde     Statistical predictions based on complex models may be very sensitive to  modeling assumptions, such as choice of covariates.  As a result,  choosing a single model may not lead to satisfactory predictions and may  significantly underestimate prediction intervals due to not  incorporating uncertainty about the model choice into the final answer.  Model uncertainty often outweighs other sources of uncertainty in  problems, but is often ignored. Bayesian methods offer a very  effective and conceptually appealing alternative: predictions and  inferences can be based on a set of models rather than a single model;  each model contributes proportionally to the support it receives from  the observed data. This research involves Bayesian methods for  stochastically searching high dimensional model spaces.  As the number  of models is very large, the challenge is therefore that of finding  efficient ways of exploring the space of models, selecting plausible  ones, and attributing to each of them a weight (approximating the  posterior probability) for the mixing-based prediction or other utility  calculations.  Examples for the methodology include  applications in wavelets and generalized additive models: calibration  and prediction in spectroscopy using wavelet packets; determining the  influence of particulate matter on mortality adjusting for other  covariates in the presence of model uncertainty; and variable selection  and prediction in binary regression models for seedling survival.  Model  averaging using importance sampling to sample from high dimensional  model spaces is an effective solution.  This approach is extended to  selecting transformations of variables, subspace selection and  thresholding in wavelets, and generalized additive models in the  applications described above.  Methods for sampling models with a  probability proportional to their expected utility are also developed.  %%%  Finding and using models to describe data is a fundamental problem in  both statis tics and the sciences.  Statistical predictions may be very  sensitive to the set of explanatory variables included in a model.  Selecting a particular model based on selecting a subset of the  explanatory variables and using this model for prediction, may lead to  riskier decisions due to not incorporating uncertainty about model  choice into the final answer.  Model uncertainty often outweighs other  sources of uncertainty in problems, but is usually ignored.  In this  research, predictions and inferences can be based on a set of models  rather than a single model; each model contributes to the decision  proportionally to the support it receives from the observed data. As the  number of possible models is very large, the challenge is therefore that  of finding efficient ways of exploring the space of models, selecting  plausible ones, and attributing to each of them a weight for the  weighted prediction or other decisions. The methodological developments  are driven by the following applications: calibration and prediction in  spectroscopy; and determining the influence of particulate matter on  mortality adjusting for other meteorological variables when there is  uncertainty about which variables should be included in the prediction  model.  ***"
"9626829","Spatial and Spatial-temporal Bayesian Point Process Models  for Bioabudance and Other Applications","DMS","STATISTICS","08/01/1996","08/01/1996","Robert Wolpert","NC","Duke University","Standard Grant","Joseph M. Rosenblatt","07/31/1999","$66,000.00","","rlw@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","1057, 9169, CVIS, EGCH","$0.00","DMS 9626819  Wolpert  Bayesian hierarchical models are introduced to account for uncertainty and spatial variation in the underlying intensity measure for point process models.  Inhomogeneous gamma process random fields and, more generally, Markov random fields with infinitely-divisible distributions are used to construct positively autocorrelated intensity measures for spatial Poisson point processes, used in turn to model the number and location of individual events.  A data augmentation scheme and Markov chain Monte Carlo numerical methods are employed to generate samples from Bayesian posterior and predictive distributions.  The methods are developed in both continuous and discrete settings, and are applied to problems in forest ecology and other fields.  Spatial patterns are an important aspect of statistical data in many fields of investigation-- disease mapping, where spatial patterns may help us learn about causes or patterns of susceptibility to specific diseases; agriculture, where spatial patterns, influenced by soil types, economics, and regional agricultural traditions, may help us predict yield; and forest management, where spatial patterns help us learn about past land-use and help us anticipate problems (for example, susceptibility to insect infestations) and management opportunities (the harvesting of overly populous species).  The present research exploits recent advances in computing hardware and algorithms and in mathematical probability theory to develop new and better statistical models and numerical algorithms for exploring spatial pattern data.  The new statistical models and numerical algorithms are applied to problems in the Environment (studying changing patterns of forest speciation and biodiversity in this Federal Strategic Area), Disease Mapping, and Transportation Theory (helping to predict commuter traffic flow in an evolving urban environment, supporting the Federal Strategic Area of Civil Infrastructure)."
"9626266","Artificial and Approximate Likelihoods","DMS","STATISTICS","08/01/1996","07/16/1996","Per Mykland","IL","University of Chicago","Standard Grant","Joseph M. Rosenblatt","07/31/2000","$120,000.00","","mykland@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00","  DMS 9626266  Mykland    The project seeks to extend the use of likelihood methods to  semi- and nonparametric situations.  Important questions include  how to define and assess the accuracy of the likelihood ratio and  R-star statistics.  Of particular interest so far has been the  development of the dual likelihood, of Bartlett identities for  martingales, and of embedding techniques which permit the  derivation of asymptotic expansions for martingales.  Currently,  the investigators are expanding the theory to cover  non-martingale situations, by considering criterion functions  which are approximately likelihoods.  This covers a much broader  spectrum of data analysis problems.  It is desirable to describe  what types of inference can be covered by this, and what  corrections over likelihood inference that ought to be used when  carrying out procedures based on this approach.  The study  concerns both existing procedures (such as empirical and point  process ""likelihoods""), and at new constructions which arise from  the artificial likelihood point of view.  In particular, the  ""design-your-own likelihood"" is being investigated, with  particular reference to resampling based criterion functions.   The implications for problems in financial engineering and  investment under uncertainty are being studied as part of the  project.  Policy makers in both business and goverment are faced  with the need to take decisions under uncertainty.  Firms invest  in new plants, for instance, with imperfect knowledge of current  and future market conditions for their products.  Regulations  concerning the environment, as another example, often try to  affect systems that are so complex that even with the best models  and scientific studies, there is tremendous uncertainty about the  effects of one's actions.  Decisions in such circumstances not  only require estimates and predictions, but also a maximally  accurate quantification of how far away such estimates are likely  to be from the actual figures.  This  project is about a new  technology for doing this, one that substantially improves the  reliability of such assessments.  It is based on a statistical  theory (""likelihood inference"") first developed in Britain in the  1920s, but which has only in the last few years been opened up to  the more complex and vaguely specified systems often faced by  policy makers."
"9626134","Model Uncertainty and Bayesian Analysis of Multivariate     Time Series","DMS","STATISTICS, Economics, Methodology, Measuremt & Stats","08/01/1996","07/22/1996","Ruey Tsay","IL","University of Chicago","Standard Grant","Joseph M. Rosenblatt","07/31/2000","$105,000.00","","ruey.tsay@gsb.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269, 1320, 1333","0000, OTHR","$0.00","DMS 9626134  Tsay    This research involves two main topics in data analysis for  processing   information and making inference. The first main topic is  Bayesian   modeling of multivariate time series. The research seeks to  develop  a unified modeling approach for a wide variety of vector models.   In   particular, the investigator proposes a nice method for  simultaneous   dentification of Kronecker indexes and parameters of a vector  process,   and considers common features such as common trends,   common business cycles, and common nonlinearity in a large  system.   For co-integrated series, the method identifies the number   of common trends  at the stage of  model specification and,  hence,   differs from the existing methods that  test co-integration rank  under   a given model. The research employs Gibbs sampler in estimation;  it uses   closed-form conditional posterior distributions for linear  parameters, but   augments the sampler with other Markov chain methods such as the   Hasting-Metropolis algorithm and the griddy Gibbs to handle  nonlinear   parameters. The second topic is model uncertainty via data  perturbation   in time series analysis. The research relaxes the commonly  imposed   assumption of existence of a model and uses data perturbation to  investigate   model uncertainty. The investigator uses single and simultaneous   data-perturbation to study the impact of model uncertainty on  model selection,   parameter inference, and prediction. Based on the  Kullback-Leibler   measure, the research  produces a selection criterion that has  three components,   namely ""goodness of fit,"" ""complexity penalty,"" and  ""uncertainty  measure.""    This result differs from most of the existing selection criteria  that only contain   the first two components. Furthermore, the research shows that  the impact of   ""uncertainty measure""is most pronounced when the sample size is  small.   %%%  This research involves two topics in the analysis of a large  system   for  processing information a nd making informed decisions The  results obtained   are widely applicable in many scientific fields. For example,  they can be used in   stochastic process control in manufacturing, to monitor  simultaneously air   pollution indices at several locations in an environmental study,  to study common   business cycles in  macro-economics, and to study common  seasonality and trends   in ozone concentrations.  The research also seeks efficient  computational methods   to perform high dimensional optimization for statistical  estimation. The first topic   of the research is analysis of multiple processes in which the  investigator develops   a unified modeling approach that can identify the complete  dynamic structure   of a large system. The structure may contain important common  characteristics   such as common trends, common business cycles, and common  nonlinearity.   When common trends are present, the method identifies the number   of common trends  simultaneously with other relationships between  the processes.     The second topic of the research is the role of model uncertainty  in statistical inference.    The inference may be prediction, trend estimation, or assessing  the effects of a change   in an input variable on the output variables.    ***"
"9626181","Statistical Quality Control for Multistage Processes","DMS","STATISTICS, PRODUCTION SYSTEMS","08/01/1996","04/23/1998","Mark Schervish","PA","Carnegie-Mellon University","Continuing Grant","Joseph M. Rosenblatt","07/31/1999","$126,000.00","Andrzej Strojwas","mark@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 1465","9146, MANU","$0.00","    The statistical modeling of multistage sequential manufacturing  processes requires models both for processes running as desired  and for processes suffering from faults.  In addition, tools are  needed for monitoring and controlling the process, for diagnosing  faults, and for predicting future data.  This research project is  developing such models and tools.  A starting point is a  hierarchical mixture of recursive regression models with Markov  transition structure having the faulty processes as absorbing  states. Developing the models requires understanding the  sensitivity of predictions to the parameters of the models.  For  example, the use of absorbing states for faulty processes,  although intuitively appealing and mathematically simple, does  lead to some undesirable results that need to be addressed.   Prediction requires numerical methods for adequately summarizing  uncertainty from many sources.  In particular, it is often  necessary to predict yield several stages in the future  for an ongoing process, and the marginal distributions for the  output are not available in closed form.  Monitoring ongoing processes requires the development of  appropriate summary statistics and displays.  Diagnosis and  monitoring both require the ability to partition uncertainty  amongst several competing possibilities (such as different  possible faults) and make decisions based on the relative costs  of the possible errors.    %%%  The research develops a general framework for modeling multistage  sequential manufacturing processes.  The framework facilitates  monitoring of ongoing processes, prediction of future and current  yield, and diagnosis of faults.  In a multistage process with  in-line measurements, information becomes available sequentially  and one needs to update one's state of knowledge as the  information arrives.  This leads naturally to the use of a  Bayesian statistical approach to the modeling of uncertainties in  such a process.  The process level problems addressed by this   research are those of in-line measurement choice, control chart  construction, learning about immature processes, and process  control.  The research will lead to more sensitive monitoring  tools and more comprehensive prediction and diagnosis tools that  should help to improve the competitiveness of American  manufacturers in high technology areas such as very large scale  integrated circuit production.   ***"
"9529348","International Conference on Forensic Statistics,            June 30 to July 3, 1996  at the University of Edinburgh,    Scotland","DMS","STATISTICS, Methodology, Measuremt & Stats, LSS-Law And Social Sciences","04/01/1996","03/15/1996","Stephen Fienberg","PA","Carnegie-Mellon University","Standard Grant","James E. Gentle","03/31/1997","$25,000.00","David Kaye, Joseph Gastwirth","fienberg@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 1333, 1372","0000, OTHR","$0.00",":    The International Conference on Forensic Statistics, scheduled  for the University of Edinburgh June 30-July 3, 1996, is the  third in a series of tri-annual conferences intended to provide a  forum for the interaction of researchers in the areas of law,  forensic science, social sciences, and statistics.  Among the  goals of this conference are (a) the fostering of interaction  among research specialists in different disciplines, (b) advances  in topics of current interest to the courts, and (c) the  development of a firm scientific basis for the use of statistical  evidence in the courts.    Presentations scheduled include reports on the ethics of expert  testimony, DNA fingerprinting and its forensic uses, inferring  causality, statistical evidence of environmental harm, the use of  econometric models in anti-trust litigation, and the judicial  reception of meta-analysis for combining statistical information  across scientific studies, especially those involving   the harmful effects of exposure to drugs and environmental  hazards.    This travel grant supports the participation in the Conference by  several leading American statisticians, forensic scientists, and  social scientists as well as several promising young scientists  who are currently beginning to pursue research in this  interdisciplinary field and have sought the opportunity to  present their work in an international context.                                                                                                                                                                                                                                                    Over the past four decades statistics and statistical methods  have played increasingly important roles in the evaluation of  forensic evidence and in the presentation  of scientific evidence  more broadly in the courts.  American scientist have played a  prominent role in this development and have led initiatives  intended to improve the quality of statistics as  evidence as well  as to improve the interaction between statisticians and other  scientists as they prepare materials for use in a legal context.   While expert testimony in the American legal system has unique  features, important aspects of the science associated with the  expert testimony transcend national boundaries. The Third  International Conference on Forensic Statistics, to be held on  June 3 to July 3 1996, is a unique forum for the interaction of  researchers in the areas of law, forensic science, social  sciences, and statistics and this travel grant   supports the participation of several leading American  researchers from these different fields of interest.  Among the  topics of current federal strategic interest featured in the  conference presentations are biotechnology and its role in the  manufacture of drugs as well as the evaluation of forensic   evidence, and the assessment of harmful effects as a result of  environmental exposure."
"9530932","Mathematical Sciences:  Confidence Regions in Multivariate  Calibration","DMS","STATISTICS","09/01/1996","07/12/1996","Thomas Mathew","MD","University of Maryland Baltimore County","Standard Grant","Joseph M. Rosenblatt","08/31/1999","$74,868.00","","mathew@umbc.edu","1000 HILLTOP CIR","BALTIMORE","MD","212500001","4104553140","MPS","1269","0000, 9181, BIOT, OTHR","$0.00","DMS 9530932  Mathew     The problem of multivariate calibration can be briefly described as   follows. Two types of measurements can be made on each item or individual   in a study: one of them, denoted by a vector x, is expensive, or time   consuming or sometimes even impossible to obtain, and a second measurement,   denoted by a vector y, is more easily obtained. A calibration curve  is constructed by fitting a statistical model that establishes the   relationship between y and x, based on data that is available on both of   these variables. The calibration curve is then used in a further study for   statistical inference concerning an unknown value of x after observing y.   This research involves the construction of confidence regions for the  unknown value of x. Different types of confidence regions are needed   depending on the application. If the calibration curve is used for   constructing a confidence region for a single unknown value of x, we have   a single use confidence region. On the other hand, if the calibration   curve is used to construct confidence regions for a sequence of unknown   values of x, corresponding to a sequence of measurements y, we have   multiple use confidence regions. Simultaneous confidence regions may   also be required when we have a given set of unknown values of x,   corresponding to a given set of independent y measurements. The   construction of such confidence regions will be investigated   under various assumptions on the  model that relates y and x.  %%%  Calibration is a widely used procedure in the chemical and engineering   sciences. Typical problems where calibration arises deal with the  measurement of the concentration of a chemical in a sample, or the amount of  drug in a serum sample, where the direct measurement of these quantities of  interest is usually difficult or expensive. An indirect response is usually  easy to obtain and once a relationship is established between the response and  the quantity of interest, this relationship can be us ed to estimate the  quantity of interest, when its value is unknown. This research is aimed at  obtaining an interval for the unknown quantity of interest. In particular, the  interval can be used to conclude, for example, if the unknown concentration   of the chemical exceeds a certain limit.   ***"
"9626691","Multiple Imputation Inferences with Public-Use Data Files   and Frequentist Properties of Bayesian Procedures","DMS","STATISTICS, Methodology, Measuremt & Stats","08/01/1996","07/24/1996","Xiao-Li Meng","IL","University of Chicago","Standard Grant","Joseph M. Rosenblatt","07/31/2000","$167,000.00","","meng@stat.harvard.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269, 1333","0000, 1303, EGCH, OTHR","$0.00","DMS 9626691  Meng  This is a comprehensive research program on multiple imputation  methodology.  Multiple imputation methodology is  the most   effective inferential method available for handling the common   and complex problem of nonresponse in sample surveys, especially  those that produce public-use data files shared by many users.    The multiple imputation framework was established under the  Bayesian perspective, mainly because the Bayesian approach  provides a coherent and flexible general framework for  constructing sophisticated imputation models that incorporate all  available information.  However, the fact that the public-use  data files are designed to be shared by many users requires that  the procedures used for creating multiple imputations, and for  analyzing the multiply-imputed data sets, must have good  frequentist properties.  Thus, the study and use of multiple  imputation highlights and requires the melding of the Bayesian  and frequentist perspectives, thus posing many challenging and  intriguing research problems at the intersection of these two  perspectives.  To effectively tackle these problems, this  research is conducted simultaneously at two levels. At the  general level, the research examines new robust frequentist  properties of Bayesian procedures.  At the specific level, the  research studies the use of these properties in multiple  imputation, with a focus on constructing new procedures as well  as justifying existing ones under more general conditions.  Specific topics include confidence validity under uncongenial  multiple imputation inferences, frequentist properties of   posterior predictive p-values, and unbiased imputations with  single observation unbiased priors.  This research studies the important and complicated problem of  nonresponse, a problem inherent to all  sample surveys. The most  serious problem caused by nonresponse is nonresponse bias, that  is, those people who do not respond are systematically different  from those who do respond.  Su ch systematic differences have been  repeatedly documented in the social, economic, and statistical  literature, for example, on self-reporting of income.  If the  bias is not corrected and only the answers from respondents are  used, a very distorted picture of the characteristics (e.g.,  average annual income of households) of the underlying population  is likely to be obtained. Correcting for such systematic  distortion, especially for large public-data files, is a  very complex and demanding task.  The basic task is to reduce the  nonresponse bias by using available information (e.g.,  demographic information) on the nonrespondents to predict their  missing values.  Since we have uncertainty in our prediction, we  need more than one prediction, i.e. imputation, to honestly  display the uncertainty. With more than one imputation, it  becomes straightforward for an individual user to estimate   the loss of information due to nonresponse and thus obtain valid  statistical inference using only standard complete-data analysis  procedures.  It is obvious that the quality of the imputation  model has direct impact on the quality of the subsequent  statistical analyses. A main aim of this research is to provide  better and more flexible methodologies for constructing  imputation models; the significance of such a research is  highlighted by the fact that the analyses of public-use data  files typically have a profound impact on our society because the  conclusions from these analyses are typically used to answer  questions in economics, education, demographic studies, public  health and policy, sociology, political science, among others.   Another aim of this research is to explore the possible use of  the methodologies developed for multiple imputation to  missing-data problems in other content areas, such as the problem  of handling the missing observations in ultraviolet radiation  measurements, which are crucial for accessing global atmospheric  changes due to ozone depletion."
"9626108","Non-parametric Inferences and Related Topics","DMS","STATISTICS","08/15/1996","08/07/1996","Jiayang Sun","OH","Case Western Reserve University","Standard Grant","Joseph M. Rosenblatt","10/31/1999","$60,000.00","","jsun21@gmu.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","MPS","1269","0000, OTHR","$0.00","DMS 9626108  Sun   This research involves the following non-parametric problems:  BUMP HUNTING PROBLEMS, where tests  of unimodality  against multi-modality  and related estimators  are studied; CONFIDENCE BANDS for the mean response function  in generalized linear models; inferences in PROJECTION PURSUIT; and  RANDOM FIELDS, where the maxima of Gaussian random  fields  are studied using large deviations and techniques from  differential geometry.  All of these problems involve some nontrivial computations besides  methodology development and theoretical analysis. The investigator also studies efficient  algorithms and applies the techniques to air pollution data and other high dimensional and categorical data.  Modes or bumps in a density  estimate of a data set may reveal interesting structures of the data.  For example, in evolutionary biology they may indicate that a certain kind of species will be a dominating factor in stabilizing ""natural selection"", and in high-energy physics they may show the evidence of ""partial-wave scattering amplitudes"" or even a new particle. BUMP HUNTING techniques studied in this research help decide whether the modes or bumps are real features of the underline distribution of the data or simply a result of sampling fluctuation.  RANDOM FIELDS and GENERALIZED LINEAR MODELS can be used to model air pollution data and many other data.  PROJECTION PURSUIT is a powerful technique that searches for interesting structures of a  high dimensional data via lower dimensional projections.  Another  objective of the research is to determine the amount of confidence that may be attached to an estimate based on such techniques."
"9626658","Estimation and Testing in Non-regular Likelihood Problems   Via Adapting Spacing Method and Empirical Process Techniques","DMS","STATISTICS","07/01/1996","06/17/1996","Yongzhao Shao","NY","Columbia University","Standard Grant","Joseph M. Rosenblatt","06/30/1999","$80,681.00","","shaoy01@nyu.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","0000, OTHR","$0.00","DMS 96-26658  Shao  The methods of maximum likelihood estimation and likelihood ratio  test have constituted the cornerstone of statistical inference.  In numerous cases, the maximum likelihood method generates  estimates  with optimal asymptotic properties and the likelihood  ratio test is known to be powerful.  However, there are many  important practical situations where these methods break down.  This research focuses on investigating non-regular likelihood  problems including estimation problems where likelihood functions  may be unbounded (e.g., efficient estimation of a unimodal  distribution) as well as hypothesis testing problems where the  likelihood ratio test statistics do not have chi-square type  limiting distributions (e.g., testing homogeneity in Tukey  models).  This research involves generalizing the maximum product  of spacings method, which is a natural way to rectify the  problems caused by the unboundedness of likelihood functions and  to preserve the essential asymptotic optimalities usually  possessed by the maximum likelihood method.  In addition, the  research involves adapting and extending modern empirical process  techniques to characterize the asymptotic behavior of the  rescaled likelihood ratio statistic.  Scientists and engineers often build models based upon various  prior assumptions about the underlying conditions.  Very often,  these assumptions fail to  hold in real applications.  If the  model is too sensitive to small departures from those prior  assumptions, the results from such models will not be reliable,  and may be misleading with far-reaching adverse consequences.  It  is, therefore, very important to develop efficient methods to  check the validity of model assumptions, and to develop  techniques that are not constrained by unrealistic assumptions.  This research concerns efficient procedures for testing model  validity and introduces ""robust"" statistical models which remain  effective even when prior assumptions are not exactly true.  Such  proced ures have broad applications ranging from biotechnology to  high-speed computing system.  In particular, advancement in this  research area would be crucial in fostering modern manufacturing,  for it relates to developing optimal procedures for assessing  reliability and quality control.  In this respect, the research  plays an integral role in the promotion of economic growth by  improving manufacturing processes, thereby ensuring U.S.  competitiveness in world markets."
"9796129","NSF Young Investigator","DMS","STATISTICS","12/01/1996","07/27/1999","Andrew Gelman","NY","Columbia University","Continuing Grant","Joseph M. Rosenblatt","07/31/2000","$87,390.00","","gelman@stat.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","0000, 9297, OTHR","$0.00",""
"9625440","Mathematical Sciences:  Implementation of Accurate Methods  for Practical Inference","DMS","STATISTICS","07/01/1996","04/07/1998","George Casella","NY","Cornell Univ - State: AWDS MADE PRIOR MAY 2010","Continuing Grant","Joseph M. Rosenblatt","06/30/2000","$336,153.00","Martin Wells, Thomas DiCiccio","casella@stat.ufl.edu","373 Pine Tree Road","Ithica","NY","148502488","6072555014","MPS","1269","0000, OTHR","$0.00","DMS 9625440  Casella    Although much research effort has been expended on developing   accurate approximation techniques such as saddlepoint and improved   likelihood-based procedures, less effort has been devoted to assessing the   types of inferences that can be achieved in practice by using these methods.   When this assessment is made, it is seen that the available inferences are   severely limited in scope by both statistical issues and computational   complexity. These two sources of limitation are intertwined, as   computational difficulties can arise from the inherent demands of   valid frequentist procedures. Ensuring the correctness   of frequentist inferences can be computationally intensive, requiring   many cumbersome evaluations of the complex expressions that derive   from higher-order asymptotic approximations. In this research, these   difficulties are overcome by a synthesis of frequentist and Bayesian   inference, as the latter approach is simpler in outlook and implementation.    In particular, the computational problem is addressed by adapting   sampling-based techniques, such as Markov Chain Monte Carlo, to attain   the higher-order approximations. The result will be improved inferences   in a wide variety of practical problems; examples include logistic regression,   censored data models, and variance component estimation.   %%%  In more complicated statistical models, statisticians have typically relied   on approximate methods of inference, primarily because exact methods   can be both difficult to derive and complex to compute. However,   the validity of these approximate methods rests on the sample size being   large, which means that such methods may not be accurate in problems   with small samples. Now that inexpensive computational power has   become widely available, statisticians are attempting to use the more   realistic and complex models. For example, models used in analyzing   global environmental change, or DNA assessment, are quite complex.   The focus of t his research is to develop statistical methods that offer   both accuracy and computational tractability.  This work necessarily blends   high-performance computing with modern statistical methodology.   ***"
"9625350","Mathematical Sciences:  Semi-parametric Methods for         Longitudinal Data Analysis","DMS","STATISTICS","07/01/1996","07/21/1999","Naomi Altman","NY","Cornell Univ - State: AWDS MADE PRIOR MAY 2010","Standard Grant","Joseph M. Rosenblatt","06/30/2000","$50,000.00","","nsa1@psu.edu","373 Pine Tree Road","Ithica","NY","148502488","6072555014","MPS","1269","0000, OTHR","$0.00","DMS 9625350  Altman      Linear and generalized linear mixed models are powerful tools for  analysis of longitudinal response curves.  This research focuses  on semi-parametric extensions of these models: recovery of the  underlying curve via self-modeling methods,  estimation and  inference for parameters which summarize covariate effects and  the use of the curves as data in statistical routines such as  analysis of designed experiments, discriminant analysis and  clustering.  A novel feature of the representation is separate  parametrization of the time and response axes which allows the  covariates to act on the response both by changes in the level of  response and by time dilation or contraction.  Estimation and  inferential techniques are under development.  The methodology  has applications in the many areas in which mixed models are  used:  medical and epidemiological research, environmental  studies, public policy assessment, and economics, to name just a  few.     In many studies the response of each individual can be thought of  as a curve over time.  Examples include the the progression of  HIV infection in patients under different treatment programs and  the degradation of pesticides in different soils under different   environmental conditions.  Similar types of data are used for  public policy assessment, economics, psychology, pharmokinetics  and numerous other fields.  Understanding the evolution of  response over time can be critical to interpreting the effects of  treatments and other influences.  Recent advances in statistical  modeling have greatly improved the efficiency of estimating  treatment effects but require that the investigator specify the  shape of the response curve and the types of treatment effects  expected prior to analyzing the data.  The methods developed for  this project allows the shape of the response curve to be  determined from the observed data, while retaining simple  measures of treatment effects.  A novel feature is that  treatments which stretch  the time scale of the response (for  example, by slowing disease progression) are handled in a natural  way.  Related work covered by this project involves the use of  response curves to find subgroups with similar response evolution  and for classification of individuals into groups, such as  healthy and diseased."
"9626762","Nonparametric Estimation in Engineering","DMS","STATISTICS","07/01/1996","06/27/1996","David Ruppert","NY","Cornell University","Standard Grant","Joseph M. Rosenblatt","06/30/1999","$50,000.00","","dr24@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","0000, 9188, EGCH, OTHR","$0.00","DMS 9626762  Ruppert    The research is on nonparametric estimation of conditional  expectations, conditional variances, and conditional densities, i.e.,  estimation without assuming that these functions belong to a  parametric family.  The work uses local regression, a particularly  simple and effective nonparametric method, where one fits parametric  models locally at each point in the domain of the function in order to  estimate the function at that point.  By varying the point one achieves  a globally nonparametric estimator.  The degree of  ""localness"" is  achieved by a bandwidth or smoothing parameter.  The work includes (a)  multivariate local polynomial regression where the local neighborhoods  are determined by a symmetric, positive definite bandwidth matrix that  must be estimated; (b) non-polynomial local models for estimating  functions with poles, jump discontinuities, or discontinuous  derivatives; and (c) semiparametric estimation of correlation functions  and other functions of scientific interest by local fitting of  estimating equations.  %%%  Present applications of this research on nonparametric estimation are  mostly in engineering but include nutrition.  These applications  include density-estimation radiosity in computer graphics, analysis of  simulation data from the pdf method of modeling turbulence, estimating  volatilities in financial engineering, combining local regression  smoothing and kriging for spatial data analysis, and modeling the  correlation between nutrient uptake as measured by both food frequency  questionnaires and more accurate, but also more expensive, food records.  In the strategic area of environmental science, this work has been  applied to two very different methods of monitoring air pollution,  LIDAR and biomonitoring.  ***"
"9625396","Saddlepoint Methods in Statistics","DMS","APPLIED MATHEMATICS, STATISTICS","07/15/1996","04/01/1998","Ronald Butler","CO","Colorado State University","Continuing Grant","Joseph M. Rosenblatt","06/30/2000","$99,000.00","","rbutler@smu.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1266, 1269","0000, OTHR","$0.00","DMS 9625396  Butler    This research is concerned with the application of saddlepoint methods in four areas: (a) First they are used to construct intractable likelihoods in dynamic stochastic systems models so as to allow for statistical inference.  This leads to the computation of performance characteristics for these systems with applications in production management and systems reliability.  (b) Secondly, saddlepoint methods are used to approximate special functions arising in Statistics and applied Mathematics including Bessel and hypergeometric functions of scalar and matrix argument.  The matrix argument functions cannot always be accurately approximated and, when they can, the computational times needed are often prohibitive.  By contrast , the saddlepoint approximations are highly accurate and can always be computed in a couple seconds. (c) Saddlepoint approximations are developed for the multivariate cumulative distribution functions arising in sampling theory.  (d) Saddlepoint theory related to the generalized inverse Gaussian distributions is also investigated.    Concepts and models for stochastic systems and networks are now found in all areas of science.  Examples include various models for computer system networks, ecosystems, production management methods in manufacturing, and reliability testing.  The first part of this project develops methods that will allow for statistical inference with such models so as to assess various performance characteristics related to such systems.  These evaluations in turn allow for the description and prediction of outcomes for the system as well as for system construction and design.  Ultimately these model-based descriptions and predictions pertain to the real phenomena that underly the models.  The second major part of this proposal concerns the approximation of important special functions that arise in the use of statistical methods and applied mathematics.  Special functions were created by mathematicians because they repeatedly arise in the  solution of a wide range of important scientific problems. The special functions addressed here are called Bessel and hypergeometric functions and are perhaps the most important, widely encompassing and general of the special functions in Mathematics.  These functions historically have played a central role in the physical and mathematical sciences and are extremely difficult to compute.  The proposal suggests highly accurate approximations to these functions that can be computed in a fraction of the time that is currently needed for their approximation."
"9626339","Mathematical Sciences:  Models and Structures of            Multivariate Distribution","DMS","STATISTICS","07/01/1996","06/21/1996","A. Marshall","WA","Western Washington University","Standard Grant","Joseph M. Rosenblatt","06/30/2000","$60,101.00","","","516 High Street","Bellingham","WA","982259038","3606502884","MPS","1269","9146, 9187, EGCH, MANU","$0.00","DMS 9626339  Marshall     This research involves the study of life distributions from the  point of view of model selection.  One phase involves methods for  generating parameters in families.  Given a random variable with  distribution and survival functions, there are a number of ways  to impose parameters, in some instances by taking a function of  the random variable, and in some cases by taking a function of  the distribution or survival function.  The following are some  examples: (1) Location and scale parameters; (2) Frailty and  resilience parameters; (3) Power parameter; (4) Convolution  parameter (for special families such as the gamma family); (5)  Geometric-extreme stable parameter.  The investigators study  order relations for these parametric families.  For example, for  which parametric families are the hazard functions ordered; for  which families is there a stochastic or convex ordering.  The  introduction of parameters and their characteristics and their  orderings is fundamental to an insight concerning how to extend  the results to multivariate distributions.    Although the normal distribution plays a central role in many  applications and in statistical theory, it does not serve as a  model for lifetime data arising in industrial applications such  as manufacturing data, materials data, reliability data, nor does  it serve as a model for survival analysis or risk analysis in the  health or environmental sciences.  This research focuses on how  to generate non-normal parametric families that will provide an  insight into behavior patterns and prediction for such data.  In  particular, the methods being developed serve to upgrade risk and  reliability analyses in engineering applications.  A second  stage in this research is the development of multivariate  versions that take account of dependencies among measurements."
"9530492","Computer-aided Statistical Inference","DMS","STATISTICS","07/01/1996","03/25/1998","Rudolph Beran","CA","University of California-Berkeley","Continuing grant","Joseph M. Rosenblatt","06/30/2000","$171,000.00","","beran@wald.ucdavis.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, 1303, 9216, EGCH, HPCC, OTHR","$0.00","Beran          Modern statistical methods for large data sets rely on rotation of the   data in high dimensions (as in Fourier analysis, wavelet transforms, or   projection pursuit), on smoothing (as in nonparametric regression), on   shrinkage (as in Stein estimation or ridge regression), on variable selection   (as in linear regression), and on combinations of these ideas (such as   thresholding of wavelet or Fourier coefficients). Concurrently, statisticians   have introduced computer-aided techniques, such as cross-validation or the   bootstrap, for assessing the uncertainty in patterns recovered though data   analyses. This research project develops: (a) necessary and sufficient   conditions under which bootstrap distributions converge correctly plus   diagnostic methods for detecting bootstrap failure in data analyses; (b)   modulation estimators that recover a signal from noise by adaptively tapering    the rotated data plus confidence regions for the signal that are centered at   the modulation estimators; (c) nonparametric bootstrap confidence sets for all   pairwise rotational differences among the mean directions (or mean axes) of   several independent samples of directional (or axial) data.           The computer revolution in scientific and social measurement has created   large, complex data sets. In response, data analysts have devised computer-   assisted methods for recovering patterns from data. However, incompleteness of   the data as well as measurement errors induce possible errors in the   conclusions reached. The recent controversy about Census undercounts in the   cities is a prominent example. How much uncertainty is there in patterns   recovered through sophisticated data-analyses?  The statistical technique   called the bootstrap, which relies on fast computers, has grown since 1979   into the most widely applicable method for assessing uncertainties inherent in   data-analyses. Unfortunately, bootstrap methods, as currently used, can   sometimes give a misleading asse ssment of uncertainty. Part (a) of this   research project provides computer-intensive ways to detect and to correct   such bootstrap failure. This portion of the work contributes to the Federal   Strategic Area of high-performance computing. Part (b) of the project develops   uncertainty assessments for signals recovered from noisy measurements.   Electronic images recorded by a satellite camera are an instance of such   measurements. This portion of the work provides statistical methodology for   analyzing satellite data in the Federal Strategic Area of global change. Part   (c) of the project develops uncertainty assessments for analyses of   directional and axial data sets. Geophysical measurements in earthquake   studies, oil exploration, and studies of volcanic activity are examples of   such directional and axial data."
"9625777","Mathematical Sciences:  Topics in Nonparametric Analysis    and Model Building","DMS","STATISTICS, SIGNAL PROCESSING SYS PROGRAM","08/01/1996","04/01/1998","Kjell Doksum","CA","University of California-Berkeley","Continuing grant","Joseph M. Rosenblatt","07/31/2000","$216,000.00","","doksum@stat.wisc.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269, 4720","0000, OTHR","$0.00","DMS 9625777  Doksum                  This research addresses the question of how intuitive and   concise linear model concepts and techniques can be extended to   nonparametric settings.  Nonparametric counterparts of such commonly   used linear model ideas as regression  coefficients, correlation coefficients,   coefficient of determination,  and principal components are considered.    The research also studies nonparametric techniques for model diagnostics  that can be used for dimensionality reduction and to address the   question of adequacy of particular models.  Both asymptotic and finite sample   properties are studied, and the problem of developing reliable   data-based methods for smoothing parameter selection for functionals and   curves is addressed.   %%%       With the advent of computer data bases of unprecedented size and  complexity and with the dramatic increase in computer power, it has   become increasingly more desirable and possible to develop more flexible   models, concepts, and procedures that can be used to study relationships   between variables and to construct models without relying on rigid global   assumptions.  Much of  the recent work in statistics have addressed this   need for more general and flexible methods.  This research  further extends this work with a special focus on procedures that are   counterparts of many commonly used linear model concepts and that expose   important features in the data using intuitive and familiar ideas.  ***"
"9625384","Stochastic Modeling of Ion Channels","DMS","COMPUTATIONAL NEUROSCIENCE, STATISTICS","08/15/1996","08/26/1996","John Rice","CA","University of California-Berkeley","Standard Grant","K Crank","07/31/2000","$225,000.00","","rice@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1162, 1269","0000, 9183, BIOT, OTHR","$0.00","9625384  Rice and Fredkin  ABSTRACT    The investigators do research on a number of problems that arise  in modeling the stochastic behavior of ion channels and in the  statistical analysis of patch clamp recordings: (1)   investigations of maximum likelihood estimation of kinetic  parameters with particular attention to computational aspects and  the interpretation and assessment of the kinetic models;  (2)  investigation of problems which arise in the analysis of records  in which multiple channels are present;  (3) investigation of   the utility of models which are based on stochastic differential  equations in contrast to Markov models with a discrete state  space, which have typically been used in the field.  Ion channels  are large proteins that allow current in the form of ions to pass  across cell membranes.  These channels are the fundamental units  of current conduction in the neuromuscular system and the  interrelationships of a large variety of ion channels shape the  electrical signals of the system.  The study of ion channels is  useful not only for the purpose of deeper basic understanding of  the neuromuscular system, but also because ion channel function  is implicated in various neuromuscular diseases and because  pharmaceutical products, such as anesthetics, are targeted at ion  channels.  An experimental technique known as the ""single channel  patch clamp"" allows recordings to be made which provide important  information about these channels.  The apparently random  character of the recorded currents has led to the widespread use  of statistical models for the dynamics of the channels and to the  use of statistical techniques of data processing.  The very large  quantity of data produced by such recordings poses substantial  computational problems which must be resolved in order that  important information about the channels can be extracted  efficiently.  This interdisciplinary proposal is primarily  concerned with the development of effective statistical and  computational  methodology."
"9625774","Mathematical Sciences:  Time Series and Point Processes:    Networks and Wavelets","DMS","STATISTICS","07/01/1996","04/17/1998","David Brillinger","CA","University of California-Berkeley","Continuing grant","Joseph M. Rosenblatt","06/30/2000","$150,000.00","","brill@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","DMS 9625774  Brillinger  This project has two parts.  A first is to develop statistical techniques for dealing with graphical models whose vertex variates are time series or point processes.  Part of the motivation comes from problems in the neurosciences and in system sciences where one sets down box and arrow diagrams meant to provide the flow of information between interconnected units.  In the neuroscience case the units will be neurons, the data will be the firing times and models will developed and fit for the interconnections of single units or of groups of units.  Analyses will be carried out for experimental data including some collected from neurons distributed through the auditory system of the cat.  In various other substantive fields one is also interested in detecting the presence of connections and estimating the strength of those connections.  The second part of the project concerns the development of wavelet variants, for random processes, of some existing ordinary statistical techniques.  Wavelets are of use in eliciting signals of nonstationary character from time series and images for example.  Both parts of the project will require development of complex stochastic models and their properties, having in mind practical implementation.     A time series is a wiggly line meant to represent a phenomenon fluctuating in time, such as a seismogram, an electrocardiogram or an electroencephalogram.  A point process is a collection of times at which an event of interest occurs, such as an earthquake or a nerve cell firing or changes in mean level (for example of a river).  A network is a flow or wiring diagram of interacting entities such as nerve cells or cities or computers.  A focus of the research is with the case of time series or point processes recorded at each of the units of a network.  The time series and point processes are interacting and it is desired to learn the strengths and directions of those interactions.  Wavelet analysis is a novel method for elicitilg information  concerning phenomena that may be changing, from time series or image records.  The problem of detecting and estimating change arises often in problems of the environment and in particular of global warming.  It is intended to extend the wavelet technique to other types of evolving and distributed data such as point processes or categorical-valued processes."
"9626118","Mathematical Sciences:  Three Topics in Mathematical        Statistics","DMS","STATISTICS","07/01/1996","06/01/1998","Lawrence Brown","PA","University of Pennsylvania","Continuing grant","Joseph M. Rosenblatt","06/30/1999","$126,000.00","","lbrown@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, 9181, BIOT, OTHR","$0.00","DMS 9626118   Brown      This research involves three statistical topics. i)  There are  several useful formulations involving nonparametric function  estimation including nonparmetric regression, nonparametric  density estimation and the standard nonparametric signal  processing model.  Recently the investigator and others have  proved the asymptotic equivalence of these models in settings  involving observation of one dimensional response variables.  The  investigators propose to construct a formulation for asymptotic  equivalence to enable obtaining effective equivalence results   for multidimensional response variables. ii) One standard method  for constructing statistical tests in certain complex parametric  situations involves conditioning on an ancillary statistic and  constructing a test within the (simpler) conditional problem at  the desired level of statistical significance.  Unfortunately  this procedure entails some peculiar statistical decision  theoretic consequences.  These are investigated more fully and  alternatives are proposed to this standard methodology which are  nearly as simple to implement but avoid these undesirable  consequences. iii) Standard tests for bioequivalence are  inherently one - dimensional in that they look at only one  performance aspect at a time.  This research constructs  multidimensional bioequivalence procedures based in part on  principles contained in recent research by the investigator and   collaborators.    Statistical procedures are often classified as single parameter,  multiparameter, or nonparametric depending whether the research  looks respectively at one performance characteristic, several  related ones, or an unstructured continuum of them, such as a  response function over time of unspecified shape. i) An important  component of the modem biotechnology enterprise involves testing   whether newly developed replicas of existing technologies have  the same effect as the original.  For example, does a (cheaper)  generic drug work exactly t he same as the original prescription  drug it is intended to replace.  Standard statistical tests of  ""bioequivalence"" have been formulated to test this hypothesis  based on data gathered about the two technologies.  These  standard tests are inherently one-dimensional, and can examine  only one performance facet at a time.  This research involves the  construction of general statistical tests of bioequivalence which  can simultaneously examine several related performance aspects.  ii) Nonparametric function estimation lies at another extreme in  statistical methodology.  Such techniques involve little prior  structure and few apriori assumptions.  They are thus extremely  valuable in a variety of modern scientific enterprises involving  analysis of masses of complex data including research on the  environment and on global change.  One technical problem  impacting this area has been the existence of several apparently   different mathematical formulations for such problems.  This  research involves the creation of a unified theory for such  problems which will enable more efficient research and more  effective use of these methods. iii) A third aspect of this  research involves the notion of asymptotically ancillary  statistics.  These are a technical device invented in part to  simplify the analysis of complex multiparameter models.  Modern  data collection and computing have vastly increased the  importance of analyzing such models, but more familiarity with  them reveals certain peculiarities and deficiencies which result  from using this technical device.  This research proposes  alternate technical advice which is nearly as simple to use but  avoids these peculiarities."
"9626348","Topics in Nonparametric Analysis and Model Building","DMS","STATISTICS, SIGNAL PROCESSING SYS PROGRAM","07/15/1996","03/25/1998","Alexander Samarov","MA","Massachusetts Institute of Technology","Continuing grant","Joseph M. Rosenblatt","06/30/2000","$177,000.00","","samarov@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1269, 4720","0000, OTHR","$0.00","DMS 9626348  Samarov  This research addresses the question of how intuitive and concise  linear model concepts and techniques can be extended to  nonparametric settings.  Nonparametric counterparts of such  commonly used linear model ideas as regression  coefficients,  correlation coefficients, coefficient of determination, and  principal components are considered.  The research also studies  nonparametric techniques for model diagnostics that can be used  for dimensionality reduction and to address the question of  adequacy of particular models.  Both asymptotic and finite sample  properties are studied, and the problem of developing reliable  data-based methods for smoothing parameter selection for  functionals and curves is addressed.  With the advent of computer data bases of unprecedented size and  complexity and with the dramatic increase in computer power, it  as become increasingly more desirable and possible to develop  more flexible models, concepts, and procedures that can be used  to study relationships between variables and to construct models  without relying on rigid global assumptions.  Much of the recent  work in statistics have addressed this need for more general and  flexible methods.  This research further extends this work with a  special focus on procedures that are counterparts of many  commonly used linear model concepts and that expose important  features in the data using intuitive and familiar ideas."
"9625897","Evaluating Independence in Linear and Generalized Linear    Models","DMS","STATISTICS","07/01/1996","06/21/1996","Ronald Christensen","NM","University of New Mexico","Standard Grant","James E. Gentle","06/30/1998","$60,000.00","Edward Bedrick","fletcher@math.unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","MPS","1269","0000, OTHR","$0.00","DMS 96-25897  Christensen     The investigators study methods for evaluating the independence  assumption in standard normal theory linear models and in  generalized linear models.  The research develops both formal  tests and informal graphical procedures for evaluating  independence.  The methods are based on the formation of rational  subgroups of observations that were collected under similar  circumstances.  The formal tests for independence are based on  models that incorporate the subgroups into the linear structure.   For normal theory linear models the procedures can also be used  to provide tests for variance component models.  The researchers  study the properties of these tests.    Statistical models are widely used in the physical and social  sciences for prediction and for understanding the relationships  among factors in complex systems.  Statistical models are used in  environmental evaluation, to improve industrial processes, and to  develop chemical processes.  An important assumption made in most  statistical models is that observations are obtained  ``independently'' of one another.  This assumption has important  ramifications for the validity of statistical procedures.  Assuming independence, when it is not true, can lead to erroneous  conclusions.  Unfortunately, this assumption has traditionally  been very difficult to evalute.  The investigators develop new  methods for evaluating the validity of the independence  assumption and study the behavior of those methods."
"9625672","Mathematical Sciences:  Flowgraph and Saddlepoint Methods   for Statistics","DMS","STATISTICS","08/01/1996","07/18/1996","Aparna Huzurbazar","NM","University of New Mexico","Standard Grant","Joseph M. Rosenblatt","07/31/1999","$69,000.00","","aparna@stat.unm.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","MPS","1269","0000, OTHR","$0.00","DMS 9625672  Huzurbazar    This research involves the application of flowgraph and saddlepoint  methods to problems in statistics with particular emphasis on prediction  in stochastic networks. Stochastic network models are of current interest   in statistics and can be applied to study a variety of natural phenomena.  Consider the progression of diseases such as kidney failure, cancer,  or AIDS. Of interest is the prediction of a survival time for a patient.  Survival times can be thought of as first passage times from one  state to another in a stochastic network so that prediction of a  survival time for a patient involves analysis of a complex stochastic  network. This research is concerned with such prediction.  Methodology is developed for computation of Bayesian  predictive distributions for stochastic networks in situations  involving a multitude of covariates and heavily censored data.   These methods are used in conjunction with generalized linear models   and proportional hazards models, so that predictive distributions as well as    predictive hazards and predictive survival functions for   first passage times between any two states of a disease, or more generally,  stochastic networks, are computed.   %%%   This is a study of flowgraphs which extends beyond   the natural emphasis area of  survival analysis  into several diverse areas of  engineering systems.  Flowgraphs were originally developed in the engineering sciences  to design and analyze complex systems. For example, these systems could be   descriptions of a manufacturing process, the reliability of  an artificial organ, or the predicted time to completion of a  building project. The analysis of flowgraphs traditionally has been  hampered by computational difficulties. Current engineering methods   involve time-consuming computer simulations. The computational  aspects of this research are based on saddlepoint approximations.  Saddlepoint approximations are high performance computationally intensive   techniques that provid e fast and accurate approximations to these problems.   The results developed here are applicable in the areas of reliability, and   industrial, electrical, and systems engineering, in addition to survival  analysis.  ***"
"9625412","Curve Estimation Involving Time Series","DMS","STATISTICS","07/01/1996","06/21/1996","Sam Efromovich","NM","University of New Mexico","Standard Grant","Joseph M. Rosenblatt","06/30/1999","$55,000.00","","efrom@utdallas.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","MPS","1269","0000, OTHR","$0.00","DMS 9625412  Efromovich        This research on curve estimation focuses on optimal   adaptive nonparametric time series estimators that are: (i) asymptotically   efficient for different loss functions, (ii) well performing for the case   of small sample sizes in comparison with peer oracles based on underlying   curve, (iii) efficiently data-compressing for problems arising in   environmental problems like monitoring quality of water or the analysis of   global change via marine magnetic anomaly; (iv) robust to distribution of noise   and long covariance observations. The asymptotic analysis is based on the study of   local empirical processes, modern probabilistic results for mixing sequences   and sharp data-driven estimators of spectral density. Data-driven estimators   for the case of small sample sizes are explored both theoretically via   oracle inequalities and numerically via intensive Monte Carlo study.   %%%     The research involves the development of optimal methods for the    recovery of images that arise in different scientific problems including:   (i) secretion of  hormones such as insulin secretion where   no other methods have been successful so far due to noisy, long-dependent   and indirect data; (ii) monitoring quality of water based   on the analysis of large, correlated and sparse data sets that have undergone   efficient, computationally intensive data-compression;   (iii) timing of geological events, including plate motions and climatological   variations; (iv) testing and modeling properties of new materials.      ***"
"9625984","Curve Estimation Models for High-dimensional, Multivariate, and Discontinuous Data","DMS","STATISTICS","07/01/1996","01/17/1997","Hans-Georg Mueller","CA","University of California-Davis","Standard Grant","Joseph M. Rosenblatt","06/30/1999","$105,250.00","","hgmueller@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","0000, 9178, 9251, OTHR, SMET","$0.00","Mueller    In this research, statistical models which contain parametric, smooth and  discontinuous components are developed and applied to problems which  involve high-dimensional regression, samples of curves as observations, or  change-points and jump discontinuities. These models are motivated by  scientific problems including the longitudinal analysis of samples of  cohorts of subjects, the analysis of spatial incidence data describing the  spread of diseases, and the growth of children, where discontinuities may  occur. This also includes models which incorporate the influence of  covariates on a sample of response curves, and the construction of models  with discontinuous elements in the context of segmentation of DNA  sequences and of edge detection in image analysis. Methods for fitting the  models are devised, which typically involve complex iterative algorithms  using smoothing techniques, generalized linear models and estimating  equations as building blocks. The properties of these methods are analyzed  and they are applied to data pertaining to the scientific problems.     This research contributes to the solution of major scientific problems by  providing innovative statistical methods. One such problem is the  mortality of the older segments of a population and what one can say about  the aging process based on animal models, with potential impact on social  security and health care in the 21st century. Sophisticated methods are  needed in order to extract and combine the information on the observed  survival of the individuals from many cohorts. Another problem is to  describe and analyze the spatial spread of AIDS and of other epidemics as  time passes. A third problem is the segmentation of DNA sequences which  then may allow for association of the segments with biological functions. These  and related problems can be put into the framework of the new statistical  models which are developed in this research. The application of these  models allows to gain important new insight s into fundamental scientific  aspects of these complex problems."
"9634297","Mathematical Sciences:  Methods for Developing and          Evaluating Computer Models Used in Integrated Assessment","DMS","STATISTICS, GLOBAL CHANGE","10/01/1996","03/03/2000","Richard Berk","CA","University of California-Los Angeles","Standard Grant","K Crank","09/30/2000","$400,000.00","Robert Weiss, Richard Turco","berk@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269, 1577","1317, EGCH","$0.00","9634297  Berk    In this project, the investigators will develop methodological tools essential for computer   models used in integrated assessments. Three kinds of methodological contributions are   involved: 1) a flux coupler that links output from one model to input from another, 2)   methods to assess uncertainties in coupled computer models, and 3) statistical diagnostics   for coupled computer models. The flux coupler transforms model output in one set of   units, and for particular spatial and temporal scales, into model input in new units and   spatial temporal scales. The tools for assessing uncertainty capitalize on recent advances in   resampling procedures to represent sampling distributions for key model parameters and   outputs. The diagnostic techniques exploit new ways to link multiple inputs to multiple   outputs in a nonlinear fashion. As such, they provide a yardstick with which to judge the   performance of coupled computer models. The laboratory for developing and testing the   methodological tools is a coupled computer model linking local weather, local air quality,   and human outdoor water use for the Los Angeles Basin.    The interactions between humans and the larger physical and biological environment are   often simulated with large-scale computer models. In this project, the investigators focus   on how human water use affects local climate and air quality and then, in turn, is affected   by local climate and air quality. Outdoor water use, for instance, has impacts on local   temperature and the meteorological conditions that contribute to urban smog. But the   computer models used in such simulations need to be carefully evaluated. The key   products of this project are new tools for developing and assessing the computer models   used to understand human-environmental interactions. The new tools include: 1) a means   to allow computer models to more effectively transfer data between them, 2) methods that   allow the statistical uncertainties in the computer results  to be better described, and 3)   techniques for analyzing more thoroughly how well the computer models are using the   information provided to them. These tools all speak to the more general issue of   improving large-scale computer simulations in a wide variety of applications."
"9623884","Mathematical Sciences:  Bayesian and Nonparametric Methods  for Time Series Analysis with Environmental & Economic      Applications","DMS","STATISTICS","07/01/1996","05/08/1996","Bonnie Ray","NJ","New Jersey Institute of Technology","Standard Grant","Joseph M. Rosenblatt","06/30/2001","$201,533.00","","borayx@m.njit.edu","University Heights","Newark","NJ","071021982","9735965275","MPS","1269","0000, 1045, OTHR","$0.00","9623884    This project develops and implements new statistical methods for modeling complicated features of time series using computationally computationally intensive techniques. First the Principal Investigator developes applications of Markov Chain Monte Carlo methods to analyze multiple time series in a Bayesian framework. She shows how to incorporate features such as seasonal variation, random interventions, endogenous variable relationships, and long-range dependence in a Bayesian linear model for multivariate time series. The Bayesian framework allows for incorporation of prior information, such as knowledge about the covariance structure between series, and results in computationally feasible likelihood evaluations. The Bayesian model proposed can be used to describe some of the time series features observed in practice, but is restricted by its linear structure. The second area of research focuses on methods for modeling complicated nonlinear time series relationships in a non-Bayesian setup. The theoretical framework is based on recent advances in nonparametric function and density estimation in the regression setting. First, a test is developed for nonlinear periodic behavior and show how to model that behavior. Next simplified linear relationships among multiple nonlinear series using a nonparametric test applied to linear combinations of nonlinear series are explored. The simplified relationships can be used to improve forecasts of the individual series. Finally, full multivariate nonlinear models are developed using multivariate kernel density methods and local fitting of vector autoregressions. To help promote use of the new methods in practice, part of the effort is devoted to developing algorithms that are computationally feasible. The third project area focuses on educational innovations to improve statistical training and communication skills among engineers and applied statisticians. Improvement of these skills will help foster interactions with investigators in other discip lines.    The overall goal of the research is to develop methods that can be used to better understand complex relationships among real-world processes. Statistical tools are developed that can be used, for example, to investigate long-range dependent relationships between environmental series in order to assess global changes in the earth's atmosphere, or to analyze important nonlinear relationships among economic series. To help insure that future statistics practitioners have sufficient training to use such methods, educational innovations are developed, in parallel, aimed at improving data analysis, computing, and communication skills at both the undergraduate and graduate levels. By better educating students in the application of statistics to real-world problems, they will be better equipped to use statistical methods for understanding complex real-world dynamics and making intelligent policy decisions."
"9626187","Multivariate Nonparametric Methodology Studies","DMS","STATISTICS","06/15/1996","08/04/1998","David Scott","TX","William Marsh Rice University","Continuing grant","Joseph M. Rosenblatt","08/31/1999","$219,081.00","Dennis Cox","scottdw@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","9187, 9216, EGCH, HPCC","$0.00","DSM9616187  Scott    Nonparametric methodology is widely used in one and two dimensions, but not  in high dimensions.  This research focuses on the mid-range dimensions and  provides a deeper understanding of the implications of the curse of  dimensionality and related problems associated with massive data sets.  Particular emphasis has been given to multivariate regression and density  estimation problems, and closely related applications such as clustering  and ridges.  Anecdotal evidence has suggested a gap between the apparent  successes of nonparametric methodology in practice and the poor performance  predicted by theory.  We have examined new points of view, especially related  to locally adaptive estimation.  Higher quality estimation has often required  use of negative kernels, but our results have shown that equivalent gains  are possible in regions where the Hessian is indefinite, often in the tails  which dominate in higher dimensions.  In addition, we have developed a  class of locally adaptive but not higher order algorithms that work  better in practical problems and avoid problems of negativity.  We have  addressed problems arising from high dimensionality in several ways.  We have created algorithms for finding interesting subspaces from the density  estimation point of view.  Such subspaces are defined by maximal bias content,  sequentially peeling off low bias subspaces.  We have examined semiparametric  models for density estimation that can work better than ordinary nonparametric  algorithms, extending feasibility by several extra dimensions.  Visualization  is especially important when dealing with medium dimensional data and the  growing body of massive data sets.  One example of a new visualization tool  is provided by the density grand tour, which performs an ordinary grand tour  but displays a real-time view of a derived density estimate, the averaged  shifted histogram.  We have found that traversing ridges and contours is useful  to control or constrain viewing.  We h ave extended our density visualization  capabilities to regression surfaces and related problems in visual clustering  and visual discrimination applications.  Visualization is also important  for organizing complicated multiple testing problems is clustering, such as  our results in mode estimation and testing based on the mode tree.  We have  investigated a local testing algorithm for collapsing modes as the basis for  an improved clustering algorithm.  A natural extension has been demonstrated  for multiprocessor and parallel architectures for massive data sets.  A great challenge in mathematical sciences is provided by massive data sets.  At a recent National Research Council workshop, numerous scientists identified  critical statistical needs in their work:  alternatives to principal  components, specialized visualization tools for exploring massive data,  better clustering algorithms, and techniques for handling nonstationary data.  Results from our research directly impact three of these four critical  opportunities.  This program represents a comprehensive and long-term attack  on a host of important data analytic problems in multivariate estimation.  %%%  Statistical techniques that do not require formulae to be written down  explicitly are called nonparametric methods and include the well-known  histogram as a simple example.  Such techniques are widely used with data  in one and two dimensions, but not in higher dimensions where most of the  grand challenge problems are to be found.  This research focuses on  the mid-range dimensions where many serious theoreticians have  expressed concern that nonparametric methods may not work.  However, it is  well-known that many practicing scientists and engineers have been  successfully using nonparametric methods with data from signal processing,  image understanding, data mining, among a wide array of real problems.  This research is providing a deeper understanding of the implications of  the so-called curse of dimensionality and particular p roblems associated  with massive data sets.  Particular emphasis is given to problems in  multivariate regression and density estimation, as well as closely  related applications such as clustering and ridges.  We have obtained  a new understanding of how locally adaptive estimation should work  in overcoming the usual limitations of nonparametric methodology in  several dimensions.  For higher dimensional data, we have developed  algorithms for finding maximally interesting subspaces from the density  estimation point of view.  Such subspaces are defined by maximal bias  content and are constructed sequentially, peeling off low bias subspaces.  Beyond two dimensions, visualization is a critical task, especially as  related to the growing body of massive data sets.  One example of a success  is provided by our new density grand tour, which provides a new way  of looking at high dimensional data in real-time. We have extended our  density estimation visualization capabilities to regression surfaces.  Visualization is also very useful for examining data to detect the  presence of clusters.  Such clusters are critical for determining  the usefulness of data collected for proposes such as character  recognition, remote sensing crop identification, ground water pollution,  as well as many more specialized engineering and scientific applications.  Multiprocessor and parallel architectures versions of these algorithms  are particularly relevant in the massive data set situation.  A great challenge in mathematical sciences is provided by handling  massive data sets.  At a recent National Research Council workshop,  numerous scientists identified critical statistical needs in their work:  alternatives to principal components, specialized visualization tools  for exploring massive data, better clustering algorithms, and techniques  for handling nonstationary data.  Results from our research directly  impact three of these four critical opportunities.  This program represents  a comprehensive and long-term  attack on a host of important data analytic  problems in multivariate estimation.  Nonparametric methodology seems to work  well in the hands of experts, and this research is designed to not only aid  the expert but to facilitate the use of the methodology by a wider audience.  ***"
"9631351","Mathematical Sciences/GIG: Immersive Methods (Virtual Reality) for Exploratory Analysis","DMS","STATISTICS, ANALYSIS PROGRAM","09/01/1996","07/12/2000","Edward Wegman","VA","George Mason University","Standard Grant","Joseph M. Rosenblatt","08/31/2001","$361,174.00","","ewegman@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1269, 1281","0000, 1303, 9216, EGCH, HPCC, OTHR","$0.00","DMS 9631351 Wegman This research focuses on the development of a synthetic environment to be used as a tool for exploring data. The synthetic environment will be constructed from and involve the integration of a number of exploratory data analysis tools some of which have already been created, some of which will be created under the scope of this research. Topic areas being explored include 1) a fractal-based grand tour, 2) 1-dimensional grand tour for images and random fields, 3) visualization using two- and three- dimensional wavelets, 4) exploratory signal analysis, and 5) visualization of huge data sets. Mathematical issues being addressed include a new method for constructing grand tours so that all the tour spends equal time in all regions of the multidimensional space and a new methodology for integrating multivariate (multispectral) image data into a single image. Computational issues being addressed include the establishment of a new metaphor for data exploration involving the creation of a data world and portals through regions of it. The research uses three-dimensional models extensively. %%% This research focuses on the exploitation of virtual reality techniques for the analysis of statistical and stochastic data. The thesis of this research is that virtual reality offers a better technology for human-computer interaction as it relates to exploratory analysis than does the desktop metaphor. Virtual reality technology not only offers a much more compelling visual focus than does a standard computer screen, but also a more complete engagement of the investigator with the data world. The major thrust of this research is to combine scientific visualization, exploratory data analysis and virtual reality into a new technology for exploring data. The plan is to create a data world made of analysis modules that can be accessed directly simply by moving from one domain within the data world to another domain via port als. As one moves seamlessly from one domain to the next through a portal, the current data set will be ported into the appropriate analysis or representation technique. The data explorer will be able to interact with the data in the current representation, either directly and/or by means of icons. The main task of this project will be to create a total synthetic environment for the exploratory analysis of data. The impact of this work should be wide ranging. The interactive focus on high-dimensional data analysis has immediate impact on settings where large amounts of multiattribute data are collected. Settings such as biotechnology (e.g. human genome), NASA's EOS-DIS and similar projects (e.g. monitoring global change), manufacturing settings in which many parameters are monitored in both the design and production processes, and materials science where optimization involving a large number of parameters of materials mixtures are examples of data amenable to the type of analyses being facilitated with this research. ***"
"9625476","Mathematical Sciences: Estimation in Generalized Linear Mixed Models","DMS","STATISTICS","07/01/1996","06/21/1996","Charles McCulloch","NY","Cornell Univ - State: AWDS MADE PRIOR MAY 2010","Standard Grant","Joseph M. Rosenblatt","06/30/2000","$65,999.00","","chuck@biostat.ucsf.edu","373 Pine Tree Road","Ithica","NY","148502488","6072555014","MPS","1269","9197, EGCH","$0.00","DMS 9625476 McCulloch The generalized linear mixed model (GLMM) generalizes the standard linear model in three ways: accommodation of non-normally distributed responses, specification of a possibly nonlinear link between the mean of a response and the predictors, and allowance for some forms of correlation in the data. Unfortunately, standard techniques like maximum likelihood estimation are computationally difficult for GLMMs and hence a number of alternate approaches have been proposed. This research will develop two approaches to inference for GLMMs: 1) computationally-intensive simulation-based methods for maximum likelihood estimation, and 2) methods based on ""joint-maximization"" ideas. The joint maximization methods will be viewed as a set of generalized estimating equations for the purpose of theoretical evaluation and improvement. The new approaches will be evaluated and compared to extant methods. Generalized linear mixed models are an important and broadly applicable set of statistical models for the analysis of data. For example, they can be used to model repeated counts of animals through time at fixed sampling locations for the purpose of environmental assessment. They are applicable to data which are gathered in a wide variety of formats and are capable of modelling data exhibiting associations. Association in the above example would arise because repeated data values at a single location would be similar. Failure to incorporate such associations can lead to incorrect conclusions from the analysis of data. Unfortunately, the use of generalized linear mixed models has been limited due to computational difficulties and the lack of availability of well-tested parameter estimation methods which have known performance characteristics. This research will develop two approaches to the analysis of such data and evaluate their performance, both in absolute terms and in relation to extant methods."

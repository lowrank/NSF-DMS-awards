"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"2311297","Novel Analytical and Computational Approaches for Fusion and Analysis of Multi-Level and Multi-Scale Networks Data","DMS","STATISTICS","09/01/2023","06/14/2023","Ping Ma","GA","University of Georgia Research Foundation Inc","Standard Grant","Yulia Gel","08/31/2026","$245,404.00","Wenxuan Zhong","pingma@uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","068Z, 079Z, 1269, 9263","$0.00","COVID-19 has claimed nearly 6.6 million lives and made many prosperous nations with well-run healthcare systems weaker. One important lesson learned from this pandemic is that non-pharmaceutical public health interventions are critical to suppress the epidemic curve at the beginning of the epidemic breakout. Mild interventions with minimal impact on normal life that are still capable to effectively reduce the epidemic spread are highly desirable. Such interventions as, for example, social distancing and case isolation are very effective strategies to suppress the pandemic. However, in the U.S., such mitigation measures rely on individuals' self-reporting mechanisms, which are time-consuming to collect and error-prone. The current project aims to develop more accurate and computationally efficient statistical tools to enhance efficiency of mitigation measures at a broader front. This project offers multiple unique opportunities for students to participate in cutting-edge and interdisciplinary research at the interface of statistics and bio-surveillance.<br/><br/><br/>In this project, by analyzing mobility data, the investigators aim to develop a suite of analytical and computational approaches that enables the early detection of the epidemic outbreak and accurate identification of infected individuals. Compared to self-reporting mechanisms, mobility data contains non-continuous individualized information and can be easily obtained from the public domain. Both the contact and mobility data can be naturally represented as networks (graphs), where the individual node is a location or a person (or a group of people), and its edges (connections) correspond to measures of contact or mobility between the nodes. The project will develop a series of novel statistical and machine learning methods for reconstructing pseudo-transmission time, identifying the infected individuals, detecting potential connections related to transmission pathways and infectious individuals using large-scale mobility data, as well as hypothesis testing for the differences between networks under various interventions. The results of the project will be applicable to a wide range of bio-surveillance tasks and will contribute to the wellbeing of our society as a whole.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2327107","Conference: EnviBayes Workshop on Complex Environmental Data","DMS","STATISTICS","09/01/2023","06/30/2023","Andrea Kaplan","CO","Colorado State University","Standard Grant","Jun Zhu","08/31/2024","$20,000.00","Benjamin Shaby","akaplan@colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1269","7556","$0.00","This workshop titled ""EnviBayes Workshop on Complex Environmental Data"" will be held in Fort Collins, Colorado on September 18-20, 2023. Environmental and ecological data are often complex and diverse. This complexity may arise from intricate data-generating mechanisms, such as the presence of many interacting species in an ecological study, or from structural dependence, such as correlation in space and time of air pollution measurements.  As such, analysis of environmental and ecological datasets may require specialized statistical and computational tools to appropriately capture and account for the complexity. While the ecological and environmental statistics community is not large, it is easy to become siloed in each of the respective sub-communities. However, the advances in modeling approaches and tools for one type of dependence (and data) can often be usefully extended to another type of dependence and have a large impact on multiple application areas. It can therefore benefit statisticians working with different facets of environmental and ecological data to interact with each other to propagate knowledge among our sub-communities for the increased advancement of all disciplines. The workshop will have the following major aims: i) to provide sub-communities within environmental and ecological statistics with a venue to share cutting-edge advancements within their own networks to further spark deeper knowledge gains; ii) to transfer knowledge between sub-communities and foster collaboration.<br/><br/>The program of the EnviBayes Workshop on Complex Environmental Data includes: i) Environmental Health and Epidemiology; ii) Data from Remote Sensing; iii) Models for Spatial and Temporal Dependence; iv) Multivariate and Spatial Extremes; v) Using Climate Models to Project Future Extremes; vi) Models for Animal Movement and Abundance; vii) Statistical Inference Related to Complex Computer Models. This project will encourage the participation of graduate students, early-career researchers, and members of groups under-represented in statistics. More details about the workshop can be found at: https://statistics.colostate.edu/envibayes-workshop/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2325706","Conference: New Frontiers in Reliability and Risk Analysis","DMS","STATISTICS","10/01/2023","07/26/2023","Refik Soyer","DC","George Washington University","Standard Grant","Jun Zhu","09/30/2024","$21,285.00","Thomas Mazzuchi","soyer@gwu.edu","1918 F ST NW","WASHINGTON","DC","200520042","2029940728","MPS","1269","7556","$0.00","A group of multidisciplinary researchers from the George Washington University, Georgetown University, North Carolina State University, Trinity College, and City University of Hong Kong have developed a two-day conference in New Frontiers in Reliability and Risk Analysis to be held at the Science and Engineering Hall of the George Washington University on October 13-14, 2023. The conference will focus on modern statistical methodologies of Reliability and Risk Analysis, which have a broad appeal, with applications in a vast array of disciplines such as engineering, science, business, medicine, law, healthcare and the social sciences. The conference organizers encourage participation and attendance by early-career researchers with diverse backgrounds and researchers from underrepresented groups.<br/><br/>The conference will be emphasizing presentation of topics on novel application areas and modern theoretical developments such as Adversarial Risk Analysis, Fusion Learning in Survival Analysis, Big Data in Reliability, Financial Risk Analytics, Machine Learning in Risk Analysis, and Deep and Reinforcement Learning in Reliability and Risk Analysis, Block Chains and Reliability, and Bayesian Learning and Decision Making with Big Data. The conference will feature key plenary speakers. For more specific and up to date conference details, interested parties are referred to the conference website: statistics.columbian.gwu.edu/nds2023.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2327625","Collaborative Research: Conference: International Indian Statistical Association annual conference","DMS","STATISTICS","06/15/2023","06/16/2023","Soutir Bandyopadhyay","CO","Colorado School of Mines","Standard Grant","Jun Zhu","05/31/2024","$25,000.00","Debashis Mondal","sbandyopadhyay@mines.edu","1500 ILLINOIS ST","GOLDEN","CO","804011887","3032733000","MPS","1269","7556","$0.00","International Indian Statistical Association (IISA) 2023, a four-day international conference, will take place at Colorado School of Mines (Mines), Golden, Colorado from June 1 to June 4, 2023. The conference is an important event for researchers, students, and professionals in statistics, biostatistics, and data science. The conference provides a platform for emerging young researchers and students to share their theoretical and methodological research and applications in various scientific fields. It aims to promote the education, research, and application of statistics, probability, and data science while also fostering the exchange of information and scholarly activities among members. With a focus on diversity and inclusion, the conference provides numerous opportunities for women, minorities, and early-career researchers to participate in student paper competitions, speed sessions, short courses, plenary talks, invited sessions, and poster presentations. The conference also hosts a vibrant multi-stage student paper competition, highlighting the best research among early-career scholars.<br/><br/>From a technical perspective, IISA 2023 covers a wide range of topics from the fundamentals of statistical theory to computationally intensive methods and scientific application-oriented data modeling. It provides an opportunity for researchers and students to share their research in a variety of formats such as plenary talks, special invited talks, invited sessions, student paper competitions, posters, and panel discussions. The conference is an official annual meeting of the International Indian Statistical Association, and its main objective is to promote education, research, and application of statistics, probability, and data science, while also encouraging cooperation among members. With Mines and IISA as the primary organizers of the conference, the event promotes education and research in the Front Range area, fosters the exchange of information and scholarly activities among scientific institutions in the Front Range, Midwest, and across institutions in the United States. The conference website (https://www.intindstat.org/conference2023/index) provides up-to-date information and announcements regarding the conference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2312733","Conference: Quality and Productivity Research Conference - Statistics, Deep Learning, and the People Side of Process","DMS","STATISTICS","03/01/2023","02/28/2023","Jamison Kovach","TX","University of Houston","Standard Grant","Jun Zhu","02/29/2024","$22,012.00","Yaping Wang","jvkovach@uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1269","7556","$0.00","The 39th Quality and Productivity Research Conference (QPRC) will be hosted by the University of Houston (UH) in Houston, Texas from June 6-8, 2023. It is the main annual event for the Quality and Productivity Section of the American Statistical Association, and it will consider data science and statistics related topics though presentations, poster sessions, a technical tour, and a one-day short course. Because billions of data sets are collected, processed, and analyzed on a daily basis in virtually every environment known, with an increasing number of data-based decisions being made that have real-world consequences for individuals and society, the data science and statistics community must keep pace with the rapid growth and variety of collected data and provide up-to-date methodologies and guidance to every applied field that utilizes data. QPRC aims to support knowledge sharing regarding data science and statistics to enable researchers and practitioners to understand the impact of data-based systems and decisions, and avoid, or at least detect and mitigate, unintended adverse consequences. It will provide a unique opportunity for attendees to meet, exchange ideas and experiences, and form collaborations. Through participation in QPRC students will gain access to invaluable learning experiences and networking opportunities with other conference attendees.<br/> <br/>The conference theme is ?Statistics, Deep Learning & the People Side of Process,? and it will include 18 invited paper sessions, four to six contributed sessions, poster sessions, a technical tour, and a one-day short course (on June 5). QPRC has the potential to advance knowledge and understanding of topics related to data science and statistics by providing a unique opportunity for statisticians, data scientists, quantitative analysts, researchers, and practitioners to discuss the current progress made in computer-intensive fields such as machine learning, facial recognition, and so forth, and exchange novel ideas and experiences in working with modern big data. Hence, this conference has the potential to 1) disseminate new methods and data-driven approaches, the evaluation of previous findings, and the validation of theoretical approaches, 2) stimulate further investigations regarding the benefits of working with big, multidimensional data, both structured and unstructured, and 3) increase the awareness of the need to use big data ethically and to address the bias that may result from the automated collection and analysis of large datasets. In addition, QPRC has the potential to benefit society in several ways. First, it provides the opportunity for attendees to learn and reframe their understanding of concepts related to data science and statistics. Second, QPRC will promote the responsible deployment and interpretation of data science and statistical methods in a variety of applied areas. Third, to disseminate the knowledge from the conference to the broader community, QPRC presenters will be invited to submit their work for publication in a special issue of the Journal of Applied Stochastic Models in Business and Industry. Fourth, to broaden the participation of underrepresented groups (i.e., women, racial/ethnic minorities, etc.) in science, technology, engineering, and mathematics (STEM) disciplines, funding is requested to support graduate students especially those in underrepresented groups from U.S. institutions to participate in QPRC. The conference website is www.uh.edu/qprc2023.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2304646","Conference: Statistical Foundations of Data Science and their Applications","DMS","STATISTICS","02/01/2023","01/20/2023","Matias Cattaneo","NJ","Princeton University","Standard Grant","Jun Zhu","01/31/2024","$25,000.00","","cattaneo@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","7556","$0.00","This award supports a diverse and inclusive three-day conference titled ""Statistical Foundations of Data Science and their Applications"" to take place at Princeton University on May 8-10, 2023. Data science is a thriving broad discipline that combines various existing fields including classical and modern statistics, biostatistics, econometrics and machine learning, and has by now transformed the way that quantitative research is conducted in the social, behavioral and biomedical sciences, as well as in finance, industry and government more generally. The main goal of the conference is to bring together junior and senior scholars working on all aspects of foundational and applied data science, while also offering unique opportunities for mentoring junior and underrepresented scholars (e.g., underrepresented minorities, women, and persons with disabilities) across a broad range of disciplines. <br/><br/>While data science combines and potentiates the best of many scientific areas of study, it is regrettably not always the case that scholars working of those specific areas interact with each other in a synergistic way. Furthermore, for young scholars it is often hard to reach out outside their subfields, hampering their intellectual and professional development. These intellectual barriers sometimes reduce diversity and inclusion due to the socially inefficient intellectual silos present in different academic and professional communities. A key goal of the conference is to be highly interdisciplinary and open to new intellectual ideas and approaches, hoping to reach out to academia, industry and government. Another equally important and highly complementary key goal of the conference is to foster junior and underrepresented scholars by offering them specifically tailored activities to such goal, in addition to offering them opportunities to interact and network with many top data science scholars from around the world that will be in attendance. The website with details about the conference is https://orfe.princeton.edu/events/dsconf/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311108","New Directions in Bayesian Model Criticism","DMS","STATISTICS","09/01/2023","07/17/2023","David Blei","NY","Columbia University","Standard Grant","Yong Zeng","08/31/2026","$225,000.00","","david.blei@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","079Z","$0.00","This project will address the problem of Bayesian model criticism, which is crucial for the effective use of Bayesian statistics and probabilistic machine learning. Currently, the process of designing Bayesian models relies heavily on creativity and experience. This research will develop new statistical tools to evaluate the adequacy of Bayesian models, providing guidance for model design and revision. The project will focus on two innovative approaches: population predictive checks (population PCs) and the posterior predictive null (PPN). These methods combine Bayesian and frequentist ideas to enhance the robustness and rigor of Bayesian model checking. The research will contribute to the foundations of Bayesian statistics, foster connections between different statistical approaches, and advance the field of deep probabilistic models. This will also contribute to the research training of a graduate student who will be involved in the project.<br/><br/>Specifically, the research will develop two innovative approaches for Bayesian model criticism that will contribute to the field's technical advancements. The first approach focuses on population predictive checks (population PCs), which combine Bayesian and frequentist principles to provide population-based evaluation of Bayesian models. By leveraging the strengths of both paradigms, this research will develop novel methods that effectively assess the adequacy of Bayesian models, enabling researchers to gain insights into their behavior and performance for informed decisions on model design and revision. The second technical thread centers around the posterior predictive null (PPN), a novel type of model criticism that explores whether data generated from one proposed model can ""fool"" the model check of another model. By developing statistical tools to address this question, this research will assess the distinctiveness and Bayesian models, and give new directions for finding parsimonious solutions to data modeling. Through theoretical investigations, empirical evaluations, and real-world applications, including medical informatics and computational astrophysics, this research will demonstrate the efficacy of these innovations. The ultimate goal is to provide a comprehensive and practical workflow for building, evaluating, revising, and selecting modern Bayesian models. To ensure widespread access, the algorithms will be disseminated as open-source software, empowering statisticians, scientists, and probabilistic modelers to effectively employ these tools and advance the adoption of Bayesian statistics and probabilistic machine learning methodologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310288","Uniform inference on continuous treatment effects via artificial neural networks in digital health","DMS","STATISTICS","07/01/2023","06/13/2023","Shujie Ma","CA","University of California-Riverside","Standard Grant","Yong Zeng","06/30/2026","$150,000.00","","shujie.ma@ucr.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","MPS","1269","079Z","$0.00","This research project will provide a general formulation and a deep learning-powered toolbox for conducting causal analysis of continuous treatment effects in observational research.  Digital health innovations have paved the way for the collection of large-scale observational data. In practice, many empirical applications in healthcare programs involve continuous treatments. This project meets the immediate needs of practitioners seeking flexible and powerful statistical tools for conducting causal inference of continuous treatment effects based on large and heterogeneous digital health data. Students, especially from underrepresented groups, will be recruited to participate in the research. Easy-to-implement software packages will be developed and made publicly available. The research results will equip scientists and healthcare providers with principled analysis for making treatment recommendations, so as to improve patient care and reduce costs. Advanced digital technologies powered with a reliable deep learning toolbox will revolutionize healthcare analytics. The research will also promote collaborations with scientists from Medicine, Public Health, Engineering, and Social Sciences. In addition, the project will provide research training for graduate students.<br/><br/>This project will develop new statistical methodologies and the associated theories for conducting uniform causal inference of continuous treatment effects via deep learning.  It will pursue three specific research topics, and the developed methods will be used to solve a wide range of causal problems. Specifically, in the first topic, the project will develop a variety of neural network architectures to approximate the nuisance function for suitable data applications in digital health. In the second topic, the project will estimate the balancing weight using neural networks through generalized optimization, and construct simultaneous confidence bands for the dose-response curve for inference. In the third topic, the project will apply the proposed optimization procedure to the estimation of heterogeneous treatment effects, and to the longitudinal data setting. The research will provide a new perspective on estimating general continuous treatment effects using deep neural networks.  It will provide a powerful tool for causal analysis that combines the advantages of deep learning, direct covariate balancing, and generalized optimization.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2345215","Matching and Data Integration","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","07/01/2023","09/22/2023","Zongming Ma","CT","Yale University","Standard Grant","Yong Zeng","06/30/2025","$165,536.00","","zongming.ma@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269, 7454","068Z","$0.00","Demand for matching and integrating datasets arises in a wide range of application fields where the data collection process is distributed and cumulative, where data related to different aspects of a complex system must be collected separately through different protocols, and where anonymity is maintained in data communication. There is a great need to develop efficient algorithms and sound theoretical understanding for matching in these settings. Ideally, these developments should be based on concrete application scenarios, such as those arising in single-cell biology and privacy-aware social network analysis. The project will provide modeling, methods, theory, and software implementations for matching and data integration to researchers in the broader scientific community, including but not limited to cell biology, psychology, telecommunications engineering, and medicine. These methods will be especially attractive to medical researchers and cell biologists as they bear the potential of unleashing the full power of single-cell data these researchers have accumulated over time with substantial human resources and financial costs. This project will also make contributions to human resource development. The investigator will focus on improving diversity in statistics and data science research through active recruitment of students to work on the project.<br/><br/>This project will pursue three progressively more challenging goals. The first is to study the matching of two datasets with partially overlapping features, emphasizing the benefit of including non-overlapping features. The second is to develop methods for matching two datasets with disjoint feature sets. They jointly serve as preparatory steps toward the third goal: to develop theoretically and/or empirically justifiable pipelines for matching more than two datasets and integrating them into a single dataset to be used in downstream analyses. The developed methods and pipelines will be benchmarked and validated on real data through close collaborations with field experts and research labs at Stanford University, the University of Pennsylvania, and the University of North Carolina.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310419","Modeling Multivariate and Space-Time Processes: Foundations and Innovations","DMS","STATISTICS, EPSCoR Co-Funding","10/01/2023","07/11/2023","Pulong Ma","SC","Clemson University","Standard Grant","Jun Zhu","10/31/2023","$195,596.00","","plma@iastate.edu","201 SIKES HALL","CLEMSON","SC","296340001","8646562424","MPS","1269, 9150","","$0.00","Geophysical processes for temperature and pressure are often highly correlated and are evolving in space over time with complex structures. For instance, many atmospheric processes such as turbulent processes can exhibit long-range dependence with correlation decays slowly as distance increases. While existing covariance models are successful in describing the smoothness behavior of these processes, the correlation in these models often decays exponentially fast and hence is inadequate. The data resulting from many geophysical processes are often continuously indexed and exhibit complicated dependence structures in many disciplines, including geophysics, ecology, environmental and climate sciences, engineering, public health, economics, political sciences, and business science. This project will develop new multivariate and space-time covariance functions with their theoretical properties to characterize complex behaviors such as long-range dependence and asymmetry and develop robust estimation procedures for estimating smoothness behaviors and long-range dependence. The project will also develop and distribute user-friendly open-source software, facilitate its broad adoption for complex data analytical problems, and provide training opportunities for next-generation statisticians and data scientists. This project is jointly funded by the Statistics Program and the Established Program to Stimulate Competitive Research (EPSCoR).<br/> <br/>This project will develop theoretical foundations and statistical models for inferring multivariate and space-time processes with long-range dependence using a model-based framework. This framework integrates and extends powerful techniques arising in the literature on scale-mixture modeling and objective Bayes. A scale-mixture technique is used to construct new multivariate and space-time covariance functions and offers flexible properties including arbitrary smoothness, long-range dependence, and asymmetry. Theoretical foundation will be provided to study the practical usefulness of the resultant covariances in a principled and unified manner in terms of several properties such as origin/tail behaviors and screening effect and offer theoretical insights on prediction accuracy in both interpolative and extrapolative settings. Objective Bayes inference is used to enable robust parameter estimation for Gaussian processes under the confluent hypergeometric covariance function with the reference prior in which the smoothness and tail-decay parameters are allowed to be estimated. The developed statistical theory and inferential tools will provide new foundations for modeling multivariate and space-time processes in spatial statistics and related areas that use covariance models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2348163","Collaborative Research: Bayesian Residual Learning and Random Recursive Partitioning Methods for Gaussian Process Modeling","DMS","STATISTICS, MSPA-INTERDISCIPLINARY, CDS&E-MSS","06/15/2023","09/20/2023","Pulong Ma","IA","Iowa State University","Standard Grant","Yulia Gel","07/31/2025","$154,776.00","","plma@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269, 7454, 8069","1269, 1303, 5294, 9263","$0.00","Rare natural hazards (for example, storm surge and hurricanes) can cause loss of lives and devastating damage to society and the environment. For instance, Hurricane Katrina (2005) caused over 1,500 deaths and total estimated damages of $75 billion in the New Orleans area and along the Mississippi coast as a result of storm surge. Uncertainty quantification (UQ) has been used widely to understand, monitor, and predict these rare natural hazards.  The Gaussian process (GP) modeling framework is one of the most widely used tools to address such UQ applications and has been studied across several areas, including spatial statistics, design and analysis of computer experiments, and machine learning. With the advance of measurement technology and increasing computing power, large numbers of measurements and large-scale numerical simulations at increasing resolutions are routinely collected in modern applications and have given rise to several critical challenges in predicting real-world processes with associated uncertainty. While GP presents a promising route to carrying out UQ tasks for modern emerging applications such as coastal flood hazard studies, existing GP methods are inadequate in addressing several notable issues such as computational bottleneck due to big datasets and spatial heterogeneity due to complex structures in multi-dimensional domains. This project will develop new Bayesian GP methods to allow scalable computation and to capture spatial heterogeneity. The new methods, algorithms, theory, and software are expected to improve GP modeling for addressing data analytical issues across a wide range of fields, including physical science, engineering, medical science, public health, and business science. The project will develop and distribute user-friendly open-source software and provide interdisciplinary research training opportunities for undergraduate and graduate students.<br/><br/>This project aims to develop a new Bayesian multi-scale residual learning framework with strong theoretical support that allows scalable computation and spatial nonstationarity for GP modeling. This framework integrates and extends several powerful techniques respectively arising in the literature on GP and that on multi-scale modeling, including predictive process approximation, blockwise shrinkage, and random recursive partitioning on the domain. This framework decomposes the GP into a cascade of residual processes that characterize the underlying covariance structures at different resolutions and that can be spatially heterogeneous in a variety of ways. The new framework allows for adoption of blockwise shrinkage to infer the covariance of the residual processes and incorporates random partition priors to enable adaptivity to various spatial structures in multi-dimensional domains. New recursive algorithms inspired by wavelet shrinkage and state-space models will be developed to achieve linear computational complexity and linear storage complexity in terms of the number of observations. The resulting GP method will guarantee linear computational complexity in a serial computing environment and also be easily parallelizable. This Bayesian multi-scale residual learning method provides a new approach to addressing GP modeling issues among spatial statistics, design and analysis of computer experiments, machine learning, and nonparametric regression.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2348640","Change Point Detection for Data with Network Structure","DMS","STATISTICS","01/15/2023","09/25/2023","George Michailidis","CA","University of California-Los Angeles","Standard Grant","Yulia Gel","07/31/2025","$281,740.00","","gmichail@ucla.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244201","3107940102","MPS","1269","1269","$0.00","Detecting breaks and anomalies in a mechanism that drives the generation of data represents a critical task, due to numerous applications in high-impact areas including health, social, and engineering sciences. This project aims to advance the state of the art of change point analysis for big and complex data, by developing a simple to implement, yet powerful, scalable algorithmic framework, thus providing new tools to examine high-dimensional, long streams for events of interest. The potential application domains of this project include but not limited to occurrence of seizure in brain connectivity data sets, coordinated market and other systemic failures in economic and finance data, and identification of orchestrated malicious activities in computer network streams. The developed algorithms and methodology will be implemented in open-source software, while curated data sets will be made available to the community for use in change point analysis investigations. The project will offer multiple unique opportunities for interdisciplinary research training of the future generation of statisticians and for further enhancement of diversity in mathematical sciences.<br/><br/>To achieve the stated goals, the project (i) develops a unified detection framework for change points in complex statistical models for network and high dimensional time streams and (ii) provides a rigorous theoretical analysis of their accuracy in the form of consistency, finite sample bounds, and asymptotic distributions for the change points and other model parameters. The framework leverages a simple, easy to implement two-step strategy, wherein the first step one selects windows of the time series of appropriate length and using a standard exhaustive search strategy identifies at most a single change point in each of them. In the second step, a second search based on a global information criterion is employed to eliminate spurious change points. The strategy exhibits linear complexity in time (and thus matches the fastest available in the literature), yet is simple to implement and theoretically analyze, in particular for complex statistical models that exhibit network and low rank structure. Further, the following issues are rigorously addressed: (i) conditions of identifiability of the model parameters and the change points and (ii) probabilistic guarantees and uncertainty quantification for them in the presence of high dimensionality, network structure, temporal dependence, as well as dependence across data streams.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2348154","Modeling Multivariate and Space-Time Processes: Foundations and Innovations","DMS","STATISTICS, EPSCoR Co-Funding","10/01/2023","09/21/2023","Pulong Ma","IA","Iowa State University","Standard Grant","Jun Zhu","09/30/2026","$195,596.00","","plma@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269, 9150","","$0.00","Geophysical processes for temperature and pressure are often highly correlated and are evolving in space over time with complex structures. For instance, many atmospheric processes such as turbulent processes can exhibit long-range dependence with correlation decays slowly as distance increases. While existing covariance models are successful in describing the smoothness behavior of these processes, the correlation in these models often decays exponentially fast and hence is inadequate. The data resulting from many geophysical processes are often continuously indexed and exhibit complicated dependence structures in many disciplines, including geophysics, ecology, environmental and climate sciences, engineering, public health, economics, political sciences, and business science. This project will develop new multivariate and space-time covariance functions with their theoretical properties to characterize complex behaviors such as long-range dependence and asymmetry and develop robust estimation procedures for estimating smoothness behaviors and long-range dependence. The project will also develop and distribute user-friendly open-source software, facilitate its broad adoption for complex data analytical problems, and provide training opportunities for next-generation statisticians and data scientists. This project is jointly funded by the Statistics Program and the Established Program to Stimulate Competitive Research (EPSCoR).<br/> <br/>This project will develop theoretical foundations and statistical models for inferring multivariate and space-time processes with long-range dependence using a model-based framework. This framework integrates and extends powerful techniques arising in the literature on scale-mixture modeling and objective Bayes. A scale-mixture technique is used to construct new multivariate and space-time covariance functions and offers flexible properties including arbitrary smoothness, long-range dependence, and asymmetry. Theoretical foundation will be provided to study the practical usefulness of the resultant covariances in a principled and unified manner in terms of several properties such as origin/tail behaviors and screening effect and offer theoretical insights on prediction accuracy in both interpolative and extrapolative settings. Objective Bayes inference is used to enable robust parameter estimation for Gaussian processes under the confluent hypergeometric covariance function with the reference prior in which the smoothness and tail-decay parameters are allowed to be estimated. The developed statistical theory and inferential tools will provide new foundations for modeling multivariate and space-time processes in spatial statistics and related areas that use covariance models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311186","Collaborative Research: Design, Modeling and Active Learning of Quantitative-Sequence Experiments","DMS","STATISTICS","08/01/2023","06/15/2023","QIAN XIAO","GA","University of Georgia Research Foundation Inc","Standard Grant","Yong Zeng","07/31/2026","$213,036.00","Abhyuday Mandal","QX69137@UGA.EDU","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","079Z, 8038","$0.00","A new type of experiment concerning both quantitative and sequence (QS) factors has recently drawn great attention in science and engineering applications. In chemotherapy, to develop efficient drug combinations involving several drug components, researchers need to conduct experiments optimizing both the doses and the sequence orders of drug components. Such a problem raises new challenges for statisticians since the input space is semi-discrete and grows exponentially with the number of drugs. Researchers rely more than ever on statistical modeling and active learning to identify optimal settings given limited experimental resources. Additionally, QS experiments often have specific requirements. In the computer experiment for metal additive manufacturing processes, the output response is binary (success/failure), and it requires both interpolation and uncertainty quantification, which is an unsolved problem in the current literature. In this project, the investigators will provide systematic solutions to QS experiments, addressing challenges in design, modeling, uncertainty quantification, and active learning. The outcome of this project will help save experimental costs in applications involving QS factors. The applications to chemotherapy will help advance cancer research in the U.S., while the applications to manufacturing processes will enhance the industrial competitiveness of the U.S. Also, this project provides research training opportunities for graduate students. <br/><br/>Active learning in experiments, aka reinforcement learning under the broad context of machine learning, allocates runs in an adaptive manner, which is generally more efficient than one-shot experiments for optimizing the experimental settings. This project will establish new Gaussian process-based models for physical experiments with QS factors, based on which new active learning procedures will be developed. For analyzing computer experiments, a novel Hopfield process (HP) framework will be established as an accurate surrogate for interpolating binary (and categorical) outputs, which will facilitate uncertainty quantification and active learning. Optimal QS experimental designs will also be constructed by combing several Williams-transformed good lattice point sets, which possess desirable properties including space-filling, orthogonality, and paired balance. This research project will provide systematic solutions for various types of QS experiments that are of interest in scientific research and industrial applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2329296","Methods and Theory for Estimating Individual-Specific and Cell-Type-Specific Gene Networks","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","01/01/2023","05/16/2023","Emma Jingfei Zhang","GA","Emory University","Standard Grant","Yong Zeng","08/31/2025","$199,999.00","","jingfei.zhang@emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1269, 7454","068Z","$0.00","In the existing literature on biological network analyses, most approaches assume a common or stratified network structure across subjects. Consequently, they are not able to flexibly account for heterogeneity in individual-level networks. For example, the individual-level networks may differ due to the complex effects of genetic variants, sex, and varying compositions across biological samples. Characterizing such network heterogeneity presents an urgent need for new statistical methodology and theory. Motivated by gene co-expression analyses, this project aims to make substantial progress in network analysis with heterogeneity. The developed methods can impact a wide range of topics in human genetics and genomics, precision health, and medicine; they are also more broadly applicable to scientific fields such as neuroscience, finance, and social science. The PI will integrate research into education by training undergraduate and graduate students and developing special topics courses.<br/><br/>This project aims to provide novel and fundamental perspectives on the emerging challenges in estimating high-dimensional covariances with heterogeneity. The first part of the project breaks new ground on estimating individual-specific graphical models. The PI will develop a new graphical regression model that relates the conditional dependence structure to covariates of high dimensions. The second part addresses the challenge in inferring cell-type-specific gene networks from aggregated data with different compositions. The PI will develop a flexible framework that does not make specific assumptions on the distributions of expressions and consider a novel least squares estimation. The developed methods in this project have appealing features, including identifiability and interpretability, efficient computation, quantifiable statistical errors, and valid statistical inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2327635","Conference: SRCOS Summer Research Conference in Statistics and Biostatistics","DMS","STATISTICS","06/01/2023","09/21/2023","Katherine Thompson","KY","University of Kentucky Research Foundation","Standard Grant","Jun Zhu","05/31/2024","$50,000.00","Arnold Stromberg","katherine.thompson@uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","1269","7556, 9150","$0.00","The 58th Summer Research Conference (SRC) and Statistics Undergraduate Research Experience (SURE) will be held in Waco, Texas on June 11-14, 2023. The Southern Regional Council on Statistics (SRCOS) is a consortium of statistics and biostatistics programs from 45 universities in 16 states in the Southern region. The SRC is an annual conference sponsored by the SRCOS. The purpose of the SRC is to encourage the interchange and mutual understanding of current research ideas in statistics and biostatistics, and to provide motivation and direction to further research progress. The SRC will give new researchers an opportunity to participate in the meeting and to interact closely with leaders in the field in a manner not possible at larger meetings. In addition to the graduate student participation, the 58th SRC will also include the fourth annual Statistical Undergraduate Research Experience (SURE), a conference within a conference aimed to encourage the participation of undergraduate students from under-represented groups to pursue graduate education and career opportunities in STEM fields. SURE will include events specifically for undergraduate students and undergraduate mentors, such as a panel about career opportunities in statistics, a real data analytics workshop, and a speed-mentoring session with current statistics and biostatistics graduate students.<br/><br/>The SRC is particularly valuable for graduate students, isolated statisticians, and faculty from smaller regional schools in the southern region at drivable distances without the cost of travel to distant venues. Speakers will present formal research talks with adequate time allowed for clarification, amplification, and further informal discussions in small groups. Graduate students will attend and present their research in posters to be reviewed by more experienced researchers. Participation in SURE will encourage under-represented students to enter STEM fields, including statistics or biostatistics, and provide training to support this endeavor. The 58th SRC will strengthen the research of the statistics and biostatistics community as a whole and help bridge the gap for under-represented groups to pursue statistics or biostatistics, particularly in the sixteen states of the Southern Region. The SRCOS website can be found here: https://www.srcos.org; the 58th SRC website can be found here: https://www.srcos.org/conference; the SURE website can be found here: https://www.srcos.org/sure-2023.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2326893","Statistical Modeling and Inference for Network Data in Modern Applications","DMS","STATISTICS","01/01/2023","04/06/2023","Emma Jingfei Zhang","GA","Emory University","Continuing Grant","Yong Zeng","06/30/2024","$79,623.00","","jingfei.zhang@emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1269","","$0.00","In modern data science, networks have emerged as one of the most important and ubiquitous types of non-traditional data. Recently, data sets with a large number of independent network-valued samples have become increasingly available. In such data sets, a network serves as the basic data object, and they are commonly seen in neuroscience, genetic studies, microbiome studies, and social cognitive studies. Such types of data bring statistical challenges that cannot be adequately addressed by existing tools. This project seeks to provide foundational perspectives on the emerging inferential and computational challenges in modeling a population and populations of networks. The theory and methods developed here will allow us to characterize the network connectivity at the population-level, and to monitor how the subject-level connectivity changes as a function of subject characteristics. Quantifying such subject-level differences has become central in studying the human brain, genetics, and medicine in general. Motivated by applications in neuroscience, this research will be beneficial for a variety of fields that study brain development, aging, and disease diagnosis, progression and treatment. Integration of research and education will be achieved through training undergraduate and graduate students, and developing special topics graduate courses.<br/><br/><br/>This project aims to develop a new network response model framework, in which the networks are treated as responses and the network-level covariates as predictors. The framework developed in this project, under appropriate structural constraints, will preserve the intrinsic characteristics of networks, ensure model identifiability, facilitate scalable computation, and allow valid statistical inference. A variety of fundamental and critical computational and inferential challenges will be addressed under this framework, including model identifiability, efficient computation, quantifying computational and statistical errors, and debiased inference. Additionally, the investigator will develop two novel goodness-of-fit tests for a broad class of network models, including those considered in this project. Further, the investigator will investigate modeling with heterogeneity by developing a network mixed-effect model, and a framework for model-based network clustering. Developments in both directions are formulated to take into account the rich information from subject covariates. The theory to be developed under asymptotic regimes allows the network size, the number of network samples, and the model complexity (e.g., rank, sparsity, number of clusters) to increase at reasonable rates.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2237628","CAREER: Ensemble Kalman Methods and Bayesian Optimization in Inverse Problems and Data Assimilation","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","02/01/2023","01/26/2023","Daniel Sanz-Alonso","IL","University of Chicago","Continuing Grant","Stacey Levine","01/31/2028","$80,229.00","","sanzalonso@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269, 1271","062Z, 1045, 1303, 9263","$0.00","Blending complex predictive models with data is essential in many applications, including numerical weather forecasting, climate science, petroleum engineering, signal processing, and medical imaging. The challenges posed by the increasing complexity of forward models and dynamical systems in inverse problems and data assimilation can be mitigated by the development of computational methods that are derivative-free and require few model evaluations. This project is concerned with two important families of cost-efficient, derivative-free algorithms: ensemble Kalman methods and Bayesian optimization. Rigorous mathematical analyses will established which will contribute to the understanding of these algorithms, determining their potential and limitations in high-dimensional inverse problems and data assimilation. Methodological contributions will focus on the design of novel algorithms and computational frameworks to merge derivative-free optimization with machine learning. Numerical implementations of these new algorithms will be made publicly available. Beyond inverse problems and data assimilation, the principal investigator will also investigate the potential of ensemble Kalman methods and Bayesian optimization in large-scale scientific computing problems where gradients are unavailable or expensive to compute, and in data science applications where privacy is a concern. A central component of the project is the integration of education and research. The principal investigator will engage in the new Preceptor Program, a collaborative initiative to build data science curricula at minority-serving community colleges in Chicago. This program will create pathways for community college students from underrepresented groups to transfer to the University of Chicago. In addition, the investigator will complete two books aimed at graduate and upper-level undergraduate students that will incorporate topics drawn from this project.  Mentorship of graduate students and development of graduate-level courses is a core part of the project.<br/><br/>The project will consist of two interrelated research thrusts on ensemble Kalman methods and Bayesian optimization. Ensemble Kalman methods are popular algorithms in the geophysical  sciences, where they are often used with a small ensemble size to keep the number of model evaluations low. The first research thrust of the project will develop a new comprehensive non-asymptotic analysis of ensemble Kalman methods that rigorously explains when and why a small ensemble size may suffice. Previous analyses have focused instead on large ensemble asymptotics that cannot explain the practical success of these algorithms with a small ensemble size. Methodological contributions of this research thrust will be focused on deriving principled frameworks to blend ensemble Kalman methods and machine learning, as well as novel regularization techniques based on hierarchical formulations of inverse problems and data assimilation.  The proposed non-asymptotic theory will establish ensemble size requirements for these new methods. The second research thrust of the project will advance Bayesian optimization in graphical and manifold settings by developing new geometry-aware kernels, acquisition functions, and convergence guarantees. The PI will use tools from computational harmonic analysis to obtain approximation guarantees for stochastic processes on manifolds and tools from information theory to obtain regret bounds under mis-specified models. Finally, the investigator will explore synergistic ways to combine ensemble Kalman methods and Bayesian optimization, leveraging the strengths of both families of algorithms to mitigate their weaknesses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2238128","CAREER: Advances in Randomization Inference for Causal Effects: Heterogeneity, Sensitivity, and Complexity","DMS","STATISTICS","07/01/2023","06/30/2023","Xinran Li","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Yulia Gel","06/30/2028","$84,609.00","","xinranli@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","1045","$0.00","Understanding causal effects holds significant importance in various social, biomedical, and industrial studies, as it plays a vital role in decision making and policy formulation. This project aims to create innovative statistical methodologies that provide a more comprehensive understanding of causal effect heterogeneity, a more reliable assessment of the sensitivity of causal conclusions to unmeasured confounding in observational studies, and robust inference for modern complex experiments. The research has the potential to answer questions in such a diverse set of disciplines, as political science, education, and sociology. For instance, the project can help address inquiries regarding the proportion of individuals who benefit from a specific policy to any extent, in addition to the usual average treatment effects. The PI intends to disseminate the research outputs through publications, presentations, and the distribution of open-source software. Additionally, the educational and outreach activities will be systematically integrated to the  research agenda, aiming to enhance undergraduate education, spread causality knowledge to the broader audiences, and equip graduate students with the critical skills allowing them to become in-depth researchers and human-centered educators.<br/><br/>The Principal Investigator plans to develop new tools that provide a more comprehensive and robust understanding of causal effects in both randomized experiments and observational studies. These tools will be built upon or inspired by the randomization inference, which uses the randomization of treatment assignments as the reasoned basis. The project has three primary objectives. First, the PI will develop inference techniques for the distribution of individual causal effects, which is an important concern in practice, yet difficult to infer due to its unidentifiability from the observed data. Second, the project will deliver new sensitivity analyses that can accommodate extreme hidden confounding in observational studies, which can strengthen the causal conclusions. Third, the PI will develop robust inference methods for complex randomized experiments that go beyond simple randomization or involve peer influence. Finally, the project will provide new computationally efficient algorithms and will create publicly available R software packages that will facilitate the use of these new tools in applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2329879","Collaborative Research: Halfspace Depth for Object and Functional Data","DMS","STATISTICS","06/01/2023","07/25/2023","Xiongtao Dai","CA","University of California-Berkeley","Continuing Grant","Yong Zeng","06/30/2024","$122,184.00","","xdai@iastate.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","","$0.00","Complex data objects are increasingly being generated across science and engineering. Non-Euclidean data such as wind directions, neural connectivity networks, and phylogenetic trees draw practical interest, but are challenging to analyze due to their intrinsic constraints. Functional data such as trajectories and images also provide examples of another type of data of high complexity, which are observed on a continuous domain in time or space. In general, practitioners are interested in first exploring the data distributions before any modeling analysis. For instance, given a sample of growth trajectories of children, a first step is to identify typical versus extreme growth patterns, where the latter can be non-trivial to uncover. Also, when analyzing brain connectivity matrices, it is important to find unusual brain networks and differences between healthy and diseased populations. Data-driven methods robust to anomalies are essential in these settings since little is known about the data generating process, and outliers can affect the analysis. Due to the lack of a natural ordering in data objects, exploratory tools such as boxplot and quantile are unavailable for these types of data. The project will address the lack of techniques for exploring non-Euclidean and functional data. Principled statistics and visualization methods will be developed based on a novel way of ranking the observations. The project will also provide training for graduate and undergraduate students. <br/><br/>The central research theme is to develop exploratory data analysis tools for non-Euclidean and functional data objects. To overcome the absence of a canonical ordering for object data, the PIs will develop suitable data depth notions to quantify the centrality of data points with respect to the distribution. This will provide a center-outward ranking of the data that will be used as a building block for outlier detection methods, rank tests, and robust classifiers. Analogous to Tukey's halfspace depth for the multivariate Euclidean case, the new depth notions for object data are expected to be intuitive and robust, and have desirable properties well-grounded in theory. Specifically, the research project will investigate a depth notion for non-Euclidean objects; a data visualization and an outlier detection procedure for non-Euclidean data; halfspace depth notions for functional data, one based on theory and another one from an algorithmic perspective; and a depth notion for sparsely observed longitudinal data. Key challenges that will be addressed include a lack of vector space structure when dealing with non-Euclidean objects; the infinite dimensionality and degeneracy when defining depth notions for functional data; detecting outlying trajectories and images in shape and not just at any time point; and the sparsity and irregularity of observations in longitudinal data. Method and theory development will draw from metric geometry, functional data analysis, empirical process, and M-estimation. Software implementing a suite of depth-based methods will be made available to the public as an outcome of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311058","Collaborative Research: Advances in the Theory and Practice of Non-Euclidean Statistics","DMS","STATISTICS","09/15/2023","07/24/2023","Robert Paige","MO","Missouri University of Science and Technology","Continuing Grant","Yong Zeng","08/31/2025","$38,414.00","","paigero@mst.edu","300 W 12TH ST","ROLLA","MO","654096506","5733414134","MPS","1269","079Z","$0.00","Shape, image, and RNA data types have had an important impact in many areas of Medicine, Science, Technology, etc. These modern data types contain different kinds of information that typically do not belong to a Euclidean space. As such, the analysis of these data types depends upon the development of statistical methodology that is adapted to address the topology and geometry of non-Euclidean metric object spaces. The project considers geometric-topologically informed statistical methods for the analysis of data lying on such object spaces. The methods to develop can find applications for new kinds of image analyses that are more likely to detect important features, identify new measures of location for shape, image, and RNA data, and improve the quality of peripheral computer vision and 3D image data. The developed methods for RNA sequencing data may apply to the investigation of viral diseases and cancer at the genomic level. The project also provided research training opportunities for graduate students.<br/><br/>The project develops statistical parameters and their inference in object spaces that underlie projective shape data, digital image data, and RNA sequence analyses. The development relies upon two key features - nonlinearity and compactness. Information extracted from these data types is often represented as points on a stratified space. This project extends classical statistical methods to colored images via RGB correlations and 3D scene reconstructions and considers the processed images to be points on an object space, which is embeddable in a Hilbert space. Moreover, this project expands upon existing approaches with the aim of increasing the computational speed of algorithms required for real-world applications and introducing distribution-free methodologies for these data types.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310955","Knockoff Feature Selection Techniques for Robust Inference in Supervised and Unsupervised Learning","DMS","STATISTICS, MATHEMATICAL BIOLOGY","08/01/2023","07/18/2023","Yushu Shi","NY","Joan and Sanford I. Weill Medical College of Cornell University","Standard Grant","Yulia Gel","07/31/2026","$200,000.00","","yus4011@med.cornell.edu","1300 YORK AVE","NEW YORK","NY","100654805","6469628290","MPS","1269, 7334","7334","$0.00","This project aims to develop a new methodology for selecting key features among a large pool of potential variables that are predictive of the final outcomes. When applied to the biomedical field, these methods will enable the discovery of determinants of patient health, thus improving the prevention, treatment, and management of diseases. When used in fields such as engineering, psychology, sociology, economics, and environmental sciences, these methods can improve manufacturing processes, social programs that focus on diversity and equity, the care and management of mental health, and the preservation of the environment and natural resources. Additionally, the new methods will also help to generate high-quality synthetic data while maintaining the confidentiality of the original information, thereby spurring new scientific discoveries and providing a valuable educational tool. The project will offer a number of unique interdisciplinary training initiatives for the future cohorts of data scientists at the interface of statistics, machine learning, and biomedical sciences.<br/><br/>The research agenda is based on the 'knockoff method' for identifying key features predictive of the outcomes while maintaining false discovery control. The methods incorporate the microbiome phylogenetic structure in feature selection, accommodate missing values, incorporate multiple knockoffs to increase robustness, employ nonparametric Bayesian models for complex data structures, and introduce a new knockoff statistic based on conditional prediction function. The proposed statistics can be paired with state-of-the-art machine learning models to detect nonlinear relationships while accounting for feature correlation. Furthermore, by applying knockoff filtering with unsupervised learning models, this research can identify determinants of the feature space and provide insights into unsupervised clustering and learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310464","Covariate-adjusted Expected Shortfall under Data Heterogeneity","DMS","STATISTICS","09/01/2023","06/16/2023","Xuming He","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Jun Zhu","09/30/2023","$330,000.00","","xmhe@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","The expected shortfall of a random variable is the tail average below or above a given threshold specified by a quantile, and it becomes a natural and useful summary statistic when low- or high-valued outcomes are of primary interest, as is often the case in risk assessment and treatment effect detection. Given the emerging importance of the expected shortfall as a summary measure, the recent literature in financial econometrics, statistics and operations research has focused on the expected shortfall regression, which enables one to evaluate the tail differences after adjusting for the covariates or possible confounding factors. The project will study the estimation of covariate-adjusted expected shortfall, identify new approaches for estimation, and study the statistical properties for its adaptation to data heterogeneity. The proposed research will provide toolkit for data-driven and evidence-based analysis in diverse fields, including concussion research, health disparity research, and climate studies. The project will also contribute to the training of a new generation of statisticians and data scientists.<br/><br/>The project will develop a new approach to estimation of covariate-adjusted expected shortfall that is computationally feasible and flexible, adapts well to data heterogeneity, and allows effective statistical inference. The proposed approach is built on a characterization of the expected shortfall based on a quantile loss function, but without reliance on the quantile function itself. When the expected shortfall function takes a parametric form, the proposed approach will start with an initial estimator of the expected shortfall at possibly a sub-optimal rate of convergence and obtain a much better solution from convex optimization. The proposed method works under weak modeling assumptions and opens a new window of opportunities for better statistical inference for expected shortfall regression.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310487","Non-Gaussian Multivariate Processes for Renewable Energy and Finance","DMS","STATISTICS","09/01/2023","06/12/2023","William Kleiber","CO","University of Colorado at Boulder","Standard Grant","Jun Zhu","08/31/2026","$300,000.00","","william.kleiber@colorado.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1269","","$0.00","The electrical grid is experiencing challenges with the expanding introduction of distributed photovoltaic (PV) panels over neighborhood rooftops.  Such PV installations generate variable, uncertain and intermittent electricity supply due to microscale weather patterns and diurnal variation. Understanding the impacts of such an uncertain renewable energy resource requires high resolution space-time irradiance data; however, fine scale irradiance data are rare, and impacts assessments must therefore rely on simulated scenarios. In a parallel vein, cryptocurrencies are increasingly common investment areas for businesses and individual investors. However, the nascent state of the technology and price record leave a dearth of historical data. Thus, financial modeling and investment studies require simulated scenarios that consider the joint behavior of multiple cryptocurrencies simultaneously.  This project will develop new methodology to produce realistic, synthetic data sets that can be used in risk and impacts studies.  Moreover, this project will support education and training of students who will gain interdisciplinary research experience at the intersection of statistics, renewable energy science and finance.<br/><br/>This research will develop new modeling frameworks for multivariate non-Gaussian processes that exhibit intermittent jump-like behavior.  Such approaches will afford better understanding of the variability and intermittency of solar irradiances and the joint behavior in major cryptocurrency portfolios.  Space-time in situ pyranometer-based measurements of irradiances and a suite of popular modern cryptocurrency historical prices will be used to illustrate the new approaches.  The methods developed will directly benefit the fields of energy science, meteorology, finance and economics with applicability in many further disciplines including geography, ecology, environmental science and physics, among others.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2345035","Covariate-adjusted Expected Shortfall under Data Heterogeneity","DMS","STATISTICS","09/01/2023","08/31/2023","Xuming He","MO","Washington University","Standard Grant","Jun Zhu","08/31/2026","$330,000.00","","xmhe@umich.edu","ONE BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","1269","","$0.00","The expected shortfall of a random variable is the tail average below or above a given threshold specified by a quantile, and it becomes a natural and useful summary statistic when low- or high-valued outcomes are of primary interest, as is often the case in risk assessment and treatment effect detection. Given the emerging importance of the expected shortfall as a summary measure, the recent literature in financial econometrics, statistics and operations research has focused on the expected shortfall regression, which enables one to evaluate the tail differences after adjusting for the covariates or possible confounding factors. The project will study the estimation of covariate-adjusted expected shortfall, identify new approaches for estimation, and study the statistical properties for its adaptation to data heterogeneity. The proposed research will provide toolkit for data-driven and evidence-based analysis in diverse fields, including concussion research, health disparity research, and climate studies. The project will also contribute to the training of a new generation of statisticians and data scientists.<br/><br/>The project will develop a new approach to estimation of covariate-adjusted expected shortfall that is computationally feasible and flexible, adapts well to data heterogeneity, and allows effective statistical inference. The proposed approach is built on a characterization of the expected shortfall based on a quantile loss function, but without reliance on the quantile function itself. When the expected shortfall function takes a parametric form, the proposed approach will start with an initial estimator of the expected shortfall at possibly a sub-optimal rate of convergence and obtain a much better solution from convex optimization. The proposed method works under weak modeling assumptions and opens a new window of opportunities for better statistical inference for expected shortfall regression.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2247212","Conference: ICSA 2023 Applied Statistical Symposium","DMS","STATISTICS","06/01/2023","12/12/2022","Jian Kang","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Jun Zhu","05/31/2024","$20,000.00","Gongjun Xu","jiankang@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","7556","$0.00","This award supports the participation of graduate students, early-career researchers, and members of groups underrepresented in statistics in the International Chinese Statistical Association (ICSA) 2023 Applied Statistics Symposium, with the theme ?Statistical Learning and Reasoning from Data to Knowledge?, held at the University of Michigan, Ann Arbor, Michigan, from June 11 ? June 14, 2023. The event serves as the 32nd official annual meeting of ICSA. The main objective of the conference is to bring together both well-established and junior researchers, as well as students, from around the world who are actively pursuing theoretical and methodological research in statistics, biostatistics, data sciences and their applications in various allied fields. It provides the broader statistical community with a great opportunity to network and exchange ideas of recent developments in statistical theory, methodology, and applications.<br/> <br/>The format of the conference includes multiple plenary talks, invited sessions, student paper competitions, contributed posters and short courses. The organizers strive to enhance diversity of speakers and maintain a healthy inclusive atmosphere. In addition, a special panel discussion will be organized on mentoring and career development, especially geared toward women and researchers from groups underrepresented in Science, Technology, Engineering, and Math (STEM). For students, the conference will provide opportunities such as a student paper competition and contributed poster sessions. The website with details about the symposium is symposium2023.icsa.org<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2335936","Conference: Advancing Statistical Science for our Changing Climate","DMS","STATISTICS, LARS SPECIAL PROGRAMS","09/01/2023","08/18/2023","Katherine Ensor","VA","American Statistical Association","Standard Grant","Jun Zhu","08/31/2024","$99,590.00","Donna LaLonde","ensor@rice.edu","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269, 7790","1303, 4444, 5294, 5740, 7556, 7790","$0.00","The Statistical Science for our Changing Climate workshop series, which will be take place at the American Statistical Association (ASA) headquarters in Alexandria VA November 2023, February, 2024, and April 2024, will focus on moving statistical science forward to meet the challenges faced in the areas of climate and extreme events, climate and the impact on human and animal health, and climate and risk management both for the financial industry and the insurance industry. In the face of our rapidly changing climate, statistics plays a pivotal role in understanding and mitigating its profound impacts. As the frequency and intensity of extreme weather and weather-related events increase, reliable data analysis becomes crucial for identifying patterns, assessing risks, and formulating effective strategies. The complexity of climate systems necessitates the development of new statistical methodologies that can handle intricate datasets, account for uncertainties, and provide accurate predictions. The workshops will focus the statistics communities? energies on mitigating the multi-faceted impacts of a changing climate on our global structures.<br/><br/>Each workshop will take place over 2 days and will be organized into 4 modules. The first 3 modules will be focused on the critical statistical content and will begin with an overview talk presented by a previously identified lead investigator. Following the overview presentation, the workshop participants will gather in working groups to address the critical questions and incubate new ideas. The final module for each workshop will serve as a summary wrap-up and be open to external virtual participation. Lead investigators identified for each module will help develop key statistical science questions and take the lead on the authorship of the vision paper as well as research papers planned for each workshop.  In total, it is expected that nine scholarly research papers will be produced, including one overarching vision paper from each workshop. In each case, the vision paper will lay the foundation of what is needed in the discipline of statistical science to support a resilient society with respect to the domain of the workshop. Further, ASA will develop key white papers to communicate to its members and beyond opportunities in curriculum both undergraduate and graduate, as well as corporate collaborations, with an eye toward an ASA corporate fellows program for climate resilience. By combining expertise in statistical modeling, data analysis, and risk assessment with the insights of climate scientists, researchers can gain a comprehensive understanding of the complex dynamics driving climate change and contribute to the development of effective strategies, policies, and solutions that address the challenges posed by climate change on multiple fronts. For more information please see website:www.amstatclimate.org<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310668","Feature selection in several challenging directions","DMS","STATISTICS","09/01/2023","07/27/2023","Jiashun Jin","PA","Carnegie-Mellon University","Standard Grant","Jun Zhu","08/31/2026","$225,000.00","","jiashun@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","079Z","$0.00","Feature selection plays a crucial role in many statistical problems, such as cancer classification and analysis of text and network data. This project will study feature selection in several challenging, understudied directions. First, MDAStat is a recent large-scale data set on the publications of statisticians between 1971 and 2015, which provides a rich resource for research on network analysis and text analysis. The project will expand the scope of data set to 1971-2025 by collecting new data. Second, the project will develop a family of correlation metrics, which provide a better way to measure the nonlinear relationship between the response and predictive variables. The metrics provide more accurate feature selection results in many application problems in cancer and biomedical study. Last, the project will develop new approaches to extracting features from social networks and text documents and generate better results in applications such as analysis of health care data and author attribution (i.e., identifying the right authors of a possibly ancient text document). The research will generate new ideas and methods to address many challenging problems in modern statistical research, and will substantially increase the understanding of many problems in science and engineering, such as cancer and biomedical research, network analysis, text analysis, and natural language processing. <br/><br/>Feature selection is an important approach in high dimensional data analysis. The project will study feature selection in several challenging directions and will make contributions on the following topics. First, despite many studies on the rare/strong signal regime, the property of the lasso remains largely unknown in the more challenging rare/weak signal regime. The project will develop new techniques and use them to derive sharp rates of the Hamming selection errors of the lasso, especially for the rare/weak signal regime. Second, a challenging problem in feature selection is how to measure the nonlinear relationship between the response and predictive variables. The project will develop a family of nonlinear correlation metrics and use them to derive sharp phase transitions in nonlinear rare/weak models for cancer classification and cancer clustering. Third, despite that there are more than a handful of models for social networks, it remains unclear which model fits the best with real networks, partially because network goodness-of-fit is a challenging problem. The project will develop a novel goodness-of-fit approach and use it to identify the most appropriate models for social networks. Last, feature extraction and embedding with text documents and networks is a challenging problem. The project will develop novel approaches for feature extraction and embedding and using them for predicting future citation counts of a published paper and for author attribution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310974","Collaborative Research: Learning and forecasting high-dimensional extremes: sparsity, causality, privacy","DMS","STATISTICS, Secure &Trustworthy Cyberspace","08/15/2023","07/24/2023","Gennady Samorodnitsky","NY","Cornell University","Standard Grant","Jun Zhu","07/31/2026","$200,000.00","","gs18@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269, 8060","5294","$0.00","The principal goal of this research project is to learn how to forecast future extreme observations and to assess their impact. On an almost daily occurrence, the public is inundated with news accounts related to extreme observations arising from extraordinary climatic events from extended and severe droughts to extraordinary precipitation records, to record heat waves that have reached virtually every region of the US in one form or another. These extreme events appear unexpectedly, can be dangerous and occur in combinations that may or may not be coincidental. Do tropical storms become more deadly as global temperatures rise? Does extreme violence become more widespread as the economic conditions worsen? Questions of this type are studied by climate scientists and social scientists respectively, but statistical and probabilistic analysis of extreme values is an indispensable ingredient in any analysis. Modern statistical analysis of extremes is both blessed by the deluge of the amount of available data and cursed by this deluge. The available data are often high dimensional and contaminated. The necessity of quick forecast of future extremes and corresponding policy updates require online analysis of extremes. This research aims to evaluate causal impacts of various factors from a potentially large array of variables including changing environmental conditions, demographic movements within the US, changing landscapes, and changing economic conditions, on the frequency and magnitude of extreme events. From many variables, the hope is to produce methodology to extract the important features in the data that have a direct impact on describing and predicting extremes. This research also revolves around the notion of differential privacy and aims to develop tools for releasing global characteristics of a data set without revealing individual level information. The focus of this research will be related to developing differential privacy procedures that are tailored to extreme value characteristics of large data sets, which is challenging because extreme observations are precisely the ones that reveal the most individual information. <br/><br/>An overarching objective of this research project is to adapt modern statistical learning tools to the problem of forecasting extremes. Learning the structure of extremes presents difficult challenges due to both a limited number of extreme data and to the scarcity of extremal labels. One approach is to develop methods for detecting nonlinear sets of much smaller dimension that can provide an adequate description of extremes in high dimensions. A main thrust of this research is to develop powerful modern learning techniques (such as graph-based learning methods and kernel principal component analysis) that allow one to determine the extremal support from the data. A second main thrust of this research centers on the issue of causality in both small and large dimensional problems. In the most basic form, a set of variables X is said to be tail causal to a dependent vector Y if certain changes in X (sometimes themselves extreme but not always so) impact the tail behavior of Y. The potential outcomes framework for causality of extreme events will be a major focus in this proposal?s research agenda. A third main thrust of this research is about differential privacy in the context of extremes, which provides tools for releasing global characteristics of a data set without revealing individual level information. This is achieved by modifying the data before releasing it and, in particular, randomizing it, in such a way that the output of the procedure does not depend too much on any specific observation while still allowing for statistical inference for certain characteristics of the original data set.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311215","Collaborative Research: Multiple Hypothesis Testing on the Regression Analysis","DMS","STATISTICS","08/01/2023","07/21/2023","Xin Xing","VA","Virginia Polytechnic Institute and State University","Standard Grant","Jun Zhu","07/31/2026","$125,667.00","","xinxing@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1269","079Z, 8091","$0.00","This research project will develop new theories and methodologies for tackling fundamental issues related to false discovery rate (FDR) control under regression analysis. Novel statistical tools will be provided to analyze data from various scientific studies, such as brain imaging, genome-wide association studies, and atmospheric science. The development of statistical methods to analyze complex data will facilitate discoveries of key variables related to blood pressure, coronary heart disease, stroke, and other critical health issues. Furthermore, this research project will provide training opportunities for graduate students, who will acquire the skills to meet the growing demand for data scientists in industry and academia and software will be developed and made available on publicly accessible websites.  <br/><br/>Analyzing high-dimensional data using multiple testing under the general framework of regression analysis is a critical challenge in the era of big data. This project will develop a new framework for performing model-free multiple testing for regression analysis, as well as optimal multiple testing for high-dimensional regression model in terms of maximizing the power of detecting the true alternatives subject to the type I error rate control. This framework enables statistical inference for sufficient dimension reduction when the dimension diverges with respect to the sample size and expands the current literature on multiple testing to more complicated data structures where commonly assumed model assumptions are not valid. The framework could also be applied to draw inference for explainable neural networks. The research project not only aims to advance theories in multiple testing but also targets applications of the developed theories.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2327589","Conference: Synergies between Nonparametrics, Sequential Analysis and Modern Data Science","DMS","STATISTICS","09/01/2023","07/05/2023","Robert Keener","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Jun Zhu","08/31/2024","$15,000.00","","keener@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","7556","$0.00","The Department of Statistics at the University of Michigan will host the conference titled ?Synergies between Nonparametrics, Sequential Analysis and Modern Data Science? on the Ann Arbor campus from September 29 to 30, 2023.  The conference will feature non-parallel sessions with talks on current research in statistics and probability by distinguished speakers.  There will also be a poster session to give PhD students, postdocs, and junior professors an opportunity to present and receive feedback on their research.  Beyond the educational value from the presentations, the conference should serve to foster collaboration and discussion among the speakers, faculty, and students participating in the event.  Most of the funds from the NSF award will be used to support student participation in the conference, especially travel funds for non-local students.<br/><br/>Planned session topics for the conference include: sequential analysis to reinforcement learning; shape constraints and beyond; limit theorems for dependent data; modern data science; nonparametrics and selection biases; and astrostatistics in the 21st century.  For more information or to register to participate, please see the conference web-page: https://sites.lsa.umich.edu/woodroofememorial/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311164","A new parametric model, likelihood methods, and other advancements for multivariate extremes","DMS","STATISTICS","07/01/2023","06/16/2023","Daniel Cooley","CO","Colorado State University","Standard Grant","Jun Zhu","06/30/2026","$299,965.00","","Cooleyd@stat.colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1269","","$0.00","Understanding extremal dependence in high dimensions is essential for quantifying risk arising from a combination of multiple factors in a variety of disciplines including the environmental and climate sciences.  When dependence is described by covariance, many statistical methods exist to characterize and model structure for high-dimensional data.  Covariance, however, is a poor descriptor of a distribution's joint tail and methods specifically designed for extremes are necessary for accurate quantification of joint risk.  While theoretically-justified frameworks for describing extremal dependence are known, statistical methods for high dimensional extremes are very much needed by practitioners.  Building on the investigator's previous work, this project will present and develop the properties of a new multivariate distribution for extremes.  The distribution is characterized by a parameter matrix which summarizes pairwise tail dependencies like a covariance matrix, but which is linked to a theoretically-justified framework for extremes.  This distribution, coupled with tools in development by the investigator, will allow a practitioner to model and characterize risk for high dimensional data arising in finance, insurance, or meteorological applications. The project will also involve training a graduate student in extreme value analysis and collaboration with atmospheric scientists in government labs.<br/><br/>In more detail, recent work on transformed-linear models for extremes coupled with characterizing extremal dependence via the tail pairwise dependence matrix (TPDM) has built connections between extremes modeling and traditional linear statistics methods. Extremal analogues to principal component analysis, spatial autoregressive models, linear autoregressive moving average (ARMA) time series models, linear prediction, and partial correlation have been constructed. However, parameter estimation has thus far been somewhat ad-hoc, and has been based minimizing squared differences between the model's TPDM values and empirical estimates.  This project presents a new probability distribution, the transformed-linear T-distribution, which has the TPDM as a parameter.  As this distribution has a closed-form density, it makes likelihood estimation of the TPDM possible.  Additionally, this project will extend the investigator's recent linear time series work to build non-causal models, as the causal analogs to classical ARMA models show an asymmetry not seen in the data.  This project will also extend the recent partial tail correlation work to add causal direction to the graphical models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311250","Collaborative Research: Non-Parametric Inference of Temporal Data","DMS","STATISTICS","09/01/2023","06/16/2023","Hongyuan Cao","FL","Florida State University","Standard Grant","Jun Zhu","08/31/2026","$173,740.00","","hcao@fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","1303","$0.00","This project is driven by the need to address inquiries in diverse fields, including environmental sciences, epidemiology, and economics among others. The study of extreme weather events, such as tropical storms, requires meteorologists to determine whether more potent tropical storms occur more frequently than mid or low-level tropical storms over time. Epidemiologists studying the transmissibility and severity of COVID-19 utilize clinical laboratory data to evaluate the pattern of the trends. In investigating sea pollution levels, earth scientists gather data on mercury concentration in animals to determine whether there has been a rising trend in mercury concentration over the years. The primary objective of this research project is to enhance the methods used to tackle these questions and effectively communicate findings to the scientific community and the public. More informed decisions can be made based on the findings. This project also involves training and mentoring graduate students through their active involvement in the research. <br/><br/>The research team aims to develop innovative statistical methods to study temporally observed or time-indexed multi-sample data, which consist of measurements of different subjects made at different time points. Such data do not fall within the conventional univariate or high-dimensional time series since measurements at different time points may not have an inherent connection. The investigators and collaborators will develop a systematic asymptotic theory to address this challenge to estimate and infer temporally observed multi-sample data. They will establish consistency, asymptotic normality, and an extremal distribution theory for various associated statistics and study simultaneous confidence bands and change points analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310981","High-Dimensional Random Forests Learning, Inference, and Beyond","DMS","STATISTICS","08/15/2023","06/15/2023","Yingying Fan","CA","University of Southern California","Standard Grant","Yulia Gel","07/31/2026","$249,999.00","","fanyingy@usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","MPS","1269","1269","$0.00","Random Forests are one of the most popularly used computational methods for making predictions. The approach works by creating a group of decision-makers, like a team of experts, and then aggregates the individual predictions by these experts to form the final prediction. The great success of Random Forests has been verified by the superior performance when applied to many different types of data. Despite the tremendous success, Random Forests are still largely regarded as a Black-box method because of the limited theoretical understanding of it. The complicated nature of the algorithm and lack of theoretical understanding also make the results it produces less reproducible and hard to interpret. The project will theoretically study the properties of Random Forests to understand when the algorithm works, and more importantly, when the algorithm fails. Such studies can provide practitioners with more confidence and better guidance in applying Random Forests. The project will investigate how to improve the interpretability of Random Forests. Finally, with the understanding gained from these studies, the project will study how to improve the performance of the algorithm to make it even more useful for big data analysis. These research activities will offer numerous training initiatives for professional development of the next generation of statisticians and data scientists.<br/><br/>Recently, there has been made important progress in the analysis of random forest algorithms, for instance, proof of the polynomial consistency rate of the original version of Random Forests in the high dimensional setting, without making specific assumptions of the regression function and feature distribution. Yet, there are still many fundamentally important questions left unanswered. The overall objective of this project is to provide an in-depth understanding of complicated ensemble methods such as Random Forests, and provide improved, interpretable, and reproducible statistical estimation and inference results. The project will first study some important open questions about Random Forests, and then move to the statistical inference. In particular, recent studies have confirmed that Random Forests can adapt to sparse models. A natural question is how to undermine the underlying true sparsity structure. Furthermore, some preliminary results suggest that popular existing methods are biased when there exists feature collinearity. The project will develop valid feature importance measures and further investigate the calculation of p-values for evaluating conditional feature importance in the existence of feature collinearity. The project will also move beyond Random Forests and study the larger problem of the conditional independence test. Utilizing the insights gained from these theoretical studies, the project will further develop an improved ensemble learning method for better prediction, interpretability, and reproducibility in big data analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310450","Statistical Models and Methods for Complex Data in Metric Spaces","DMS","STATISTICS","07/01/2023","06/14/2023","Hans-Georg Mueller","CA","University of California-Davis","Standard Grant","Jun Zhu","06/30/2026","$335,849.00","","hgmueller@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","124Z","$0.00","Big data in the form of random objects are increasingly encountered across sciences and society. Adequate and principled approaches for their analysis are important to extract relevant information, to make predictions, and to assess whether there are differences between samples. However, the complexity of such data poses major challenges for statistical analysis and traditional methods cannot be applied.  In this project, these challenges will be addressed and overcome through the development of advanced nonparametric statistical methodology that can handle the complexity of object data.  Specific objects that will be studied include networks and distributions, with applications for brain imaging and genomics data, climate change data and data from other areas of current interest. For situations where random objects are repeatedly observed over time, such as repeatedly observed MRI brain scans for the same subject, quantifications of the underlying time dynamics will also be developed. The research will include methods for testing and estimation, theory, efficient computational implementation, and data applications, which are expected to lead to substantial new insights. The project will provide training for the next generation of data analysts and research statisticians and user-friendly code implementing the methods will be made available.  <br/><br/>The project will contribute to the foundations of the emerging field of metric statistics as a comprehensive framework for statistical methodology and theory for samples of random objects, which are random variables/data that take values in a metric space. Random objects encompass data in the form of distributions, networks, trees, covariance matrices and surfaces, and data on Riemannian manifolds such as spheres. The statistical analysis of such data is challenging as one cannot apply traditional statistical methods due to the absence of a vector space structure.  Specifically, for random objects that are situated in a geodesic space, a general class of regression models for transports from the barycenter to specific objects will be developed. In these models both predictors and responses are transports and specific examples are regression models for spherical data and distributions. The project will also include the study of regression models for multivariate distributions as responses, paired with Euclidean predictors, using slice-optimization in Wasserstein space. This approach will be illustrated with applications in climatology and life sciences data. Another goal is the development of transport processes in a geodesic space, constituting a new type of stochastic process. For such processes, anchor point representations for general types of random objects and latent Gaussian process representations for distributional objects in Wasserstein space will be obtained.  Additionally, a random effects model for Frechet regression will be developed, providing the first such model for longitudinal object data, with applications in brain imaging and distributional data analysis.  Throughout, the challenges resulting from the absence of Euclidean and algebraic structures will be addressed with empirical process theory and other tools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2309349","Conference: UCLA Synthetic Data Workshop","DMS","STATISTICS","04/01/2023","03/07/2023","Guang Cheng","CA","University of California-Los Angeles","Standard Grant","Jun Zhu","03/31/2024","$14,999.00","Xiaowu Dai","guangcheng@ucla.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244201","3107940102","MPS","1269","7556","$0.00","This award supports participation by experts from diverse mathematical disciplines and computer science, especially graduate students and other early-career researchers, in the upcoming UCLA Synthetic Data Workshop to be held at the University of California, Los Angeles, from April 13 to April 14, 2023. The goal of the workshop is to foster the collaboration of researchers in several areas connected to synthetic data and data privacy, including differential privacy, fairness, and adversarial robustness. The rationale for this activity is that synthetic data generation is a rapidly growing and highly disciplinary research area that draws much attention. For the development of algorithmic procedures for fraud deception and spam identification, as well as for the construction of AI-driven models in manufacturing and supply chain management, synthetic data has become a valuable resource. The goal of this workshop is to investigate scientific foundations that are spawned by these advancements and examine new strategies for solving open problems. The workshop will also have a substantial pedagogical component in the form of introductory talks that will cover background and recent exciting progress in its focus areas. These talks will be accessible to non-experts, including graduate students and junior researchers.<br/><br/>Synthetic data is especially useful when obtaining real-world data is either too costly or too risky. Recent results hint at a new and promising direction that practitioners may effectively train AI models by addressing edge scenarios and dangerous occurrences while using synthetic data. Despite numerous successful applications of synthetic data, its scientific foundation, e.g., the tradeoff among fidelity, utility, and privacy, is still missing. In addition, industrial standards for generating and utilizing synthetic data, as well as the privacy law concerning synthetic data, are yet to be established. This workshop will provide an environment for experts to exchange their ideas for open questions about synthetic data, such as whether or not privacy is lost when creating synthetic data, whether or not using synthetic data affects fairness, and how, at the most basic level, one should judge the quality and usefulness of synthetic data. The website for the workshop is https://ucla-synthetic-data.github.io/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310637","Experimental Design-based Weighted Sampling","DMS","STATISTICS","08/01/2023","06/13/2023","Roshan Joseph","GA","Georgia Tech Research Corporation","Standard Grant","Yong Zeng","07/31/2026","$250,000.00","","roshan@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","","$0.00","Random sampling is commonly used for inferring quantities of interest of a population. This project aims to develop a deterministic sampling technique that improves upon the existing sampling techniques by introducing weights for each sample. Building on the recent developments of data compression and subsampling algorithms, the new sampling technique has the potential to overcome the computational challenges faced by the existing techniques. The developed techniques are broad and can have applications in many fields of science and engineering, such as uncertainty quantification, Bayesian statistics, simulation, stochastic optimization, machine learning, and numerical analysis, to name a few. The PI will develop software packages to be distributed to the public for free to enable the widespread use of the proposed techniques. The sampling techniques will also be implemented in industries and tested in several engineering applications, which will provide a broad and immediate impact on society. The project also provides research training opportunities for graduate students. <br/><br/>The experimental design-based weighted sampling technique is developed by working on three broad classes of problems in statistics: (1) uncertainty quantification, where the probability density is known and fully specified, (2) Bayesian computation, where the probability density is known only up to a proportionality constant, and (3) data compression, where the probability distribution is unknown. The key idea of the proposed technique is to relax the restriction that the samples should follow the underlying distribution of the population. Instead, the samples are optimally generated to improve the estimation of the quantity of interest, and then weights are used for correcting the distributional mismatch. The resulting weighted sample can provide a more robust performance compared to the existing unweighted samples. The project develops new techniques such as optimally weighted quantizers, weighted minimum energy designs, adaptive integration methods using sequential designs, weighted twinning, and supervised data compression techniques using weights. Overall, this project will develop a suite of theoretically sound and computationally efficient techniques for weighted sampling that represents a significant advancement from the existing techniques that focus on generating unweighted samples from the target distribution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2239102","CAREER: Structure Learning and Forecasting of Large-Scale Time Series","DMS","STATISTICS","07/01/2023","01/11/2023","Sumanta Basu","NY","Cornell University","Continuing Grant","Jun Zhu","06/30/2028","$89,422.00","","sumbose@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","1045","$0.00","In many areas of modern biological and social sciences, researchers and practitioners seek to gain insight into the dynamics of a complex system using large-scale time series data sets. Examples include gene regulatory network reconstruction using time-course gene expression data sets, functional connectivity analysis of brain network architecture using neurophysiological signals, and monitoring systemic risk in the financial market using historical data on many firms' stock prices. The overarching goal of this project is to develop scalable statistical methods for learning such dynamic relationships using high-dimensional time series (HDTS) data sets, and provide a rigorous analysis of their properties. These methods, upon successful completion, are expected to aid data-driven testable hypothesis generation in systems biology, imaging-based biomarker search in computational neuroscience, and inform regulatory policy for financial risk management and monitoring.The research outcomes will be integrated into a number of education and outreach activities, including development of a modern data science curriculum with an accompanying online textbook as well as training of graduate and undergraduate students.<br/><br/>Existing algorithms for analyzing HDTS data sets rely primarily on using modern regularization in machine learning coupled with a squared error loss designed for independent data. This is in sharp contrast with the core modeling philosophy of classical time series, where temporal dependence among observations is explicitly encoded in the likelihood or loss function to increase the accuracy of structure learning and prediction. This project will narrow the gap by designing new algorithms where temporal dependence and regularization inform each other using dependence-aware machine learning methods. In particular, impulse response and quantile-specific graphical models in the time domain, adaptively regularized graphical models in the frequency domain, and random forests that explicitly incorporate temporal dependence in building regression trees, will be developed. These methods will be validated on real data sets from genomics, neuroscience and financial economics in consultation with domain experts. Results will be disseminated to public by publishing peer-reviewed articles in statistics, machine learning and other scientific journals. Software implementations of algorithms developed in this project will be made publicly available in the form of R packages.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2335846","Collaborative Research: Planning: FIRE-PLAN: Advancing Wildland Fire Analytics for Actuarial Applications and Beyond","DMS","STATISTICS, Human-Envi & Geographical Scis, HDBE-Humans, Disasters, and th, Cross-BIO Activities, Info Integration & Informatics","10/15/2023","08/09/2023","Yuzhou Chen","PA","Temple University","Standard Grant","Jun Zhu","09/30/2025","$129,949.00","","yuzhou.chen@temple.edu","1801 N BROAD ST","PHILADELPHIA","PA","191226003","2157077547","MPS","1269, 141Y, 1638, 7275, 7364","132Z","$0.00","The impacts of uncontrolled wildland fires range from the destruction of native vegetation to property damages to long-term health effects and losses of human lives. Increasing accuracy in projections of wildland fire activity, fire behavior, and wildland fire weather is the key toward developing more efficient fire control strategies and reducing the risks of wildfires. Recent studies have demonstrated that the tools of artificial intelligence (AI) can help in planning for upcoming prescribed burns by providing higher spatial and temporal fire weather forecasts and can also assist in developing more efficient strategies for wildfire risk mitigation. However, the modeling tools that are currently used to predict fire activity are largely subject to a number of temporal or spatial constraints. For instance, most deep learning (DL) approaches for wildfire risk analytics tend to be restricted in their capabilities to systematically capture the multidimensional information recorded at disparate spatio-temporal resolutions. Furthermore, such DL architectures are inherently static and do not explicitly account for complex dynamic phenomena, which is often the key behind the accurate assessment of wildfire driving factors. Finally, these models primarily rely on supervised learning approaches where a large number of task-specific labels (e.g., fire or no fire) are needed. To address these challenges in wildfire risk analytics, this project will leverage inherently interdisciplinary approaches at the interface of Earth system sciences, DL, computational topology, statistics, and actuarial sciences. <br/><br/>The project aims to introduce the concepts of topological data analysis (TDA) to wildfire predictive modeling, coupling them with such emerging AI machinery as time-aware graph neural networks. The resulting new methods are expected to better capture the shape patterns in the wildland fire processes with respect both to time and space and to assist in a more reliable statistical assessment of wildfire risks. The new high-fidelity predictive approaches will have the potential to deliver forecasts of fire behavior, fire activity, and fire weather at multiple spatial and temporal scales under scenarios of limited, noisy, or nonexistent labeled information. To enhance the utility of the research solutions in wildfire analytics, the researchers in this project will work in close collaboration with stakeholders, particularly, focusing on the insurance sector. The project will provide multiple interdisciplinary training opportunities at the nexus of wildfire sciences, AI, and mathematical sciences at all educational levels, from undergraduate students to practicing actuaries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310973","Collaborative Research: Learning and forecasting high-dimensional extremes: sparsity, causality, privacy","DMS","STATISTICS, Secure &Trustworthy Cyberspace","08/15/2023","07/24/2023","Richard Davis","NY","Columbia University","Standard Grant","Jun Zhu","07/31/2026","$250,015.00","Marco Avella Medina","rdavis@stat.columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269, 8060","5294","$0.00","The principal goal of this research project is to learn how to forecast future extreme observations and to assess their impact. On an almost daily occurrence, the public is inundated with news accounts related to extreme observations arising from extraordinary climatic events from extended and severe droughts to extraordinary precipitation records, to record heat waves that have reached virtually every region of the US in one form or another. These extreme events appear unexpectedly, can be dangerous and occur in combinations that may or may not be coincidental. Do tropical storms become more deadly as global temperatures rise? Does extreme violence become more widespread as the economic conditions worsen? Questions of this type are studied by climate scientists and social scientists respectively, but statistical and probabilistic analysis of extreme values is an indispensable ingredient in any analysis. Modern statistical analysis of extremes is both blessed by the deluge of the amount of available data and cursed by this deluge. The available data are often high dimensional and contaminated. The necessity of quick forecast of future extremes and corresponding policy updates require online analysis of extremes. This research aims to evaluate causal impacts of various factors from a potentially large array of variables including changing environmental conditions, demographic movements within the US, changing landscapes, and changing economic conditions, on the frequency and magnitude of extreme events. From many variables, the hope is to produce methodology to extract the important features in the data that have a direct impact on describing and predicting extremes. This research also revolves around the notion of differential privacy and aims to develop tools for releasing global characteristics of a data set without revealing individual level information. The focus of this research will be related to developing differential privacy procedures that are tailored to extreme value characteristics of large data sets, which is challenging because extreme observations are precisely the ones that reveal the most individual information. <br/><br/>An overarching objective of this research project is to adapt modern statistical learning tools to the problem of forecasting extremes. Learning the structure of extremes presents difficult challenges due to both a limited number of extreme data and to the scarcity of extremal labels. One approach is to develop methods for detecting nonlinear sets of much smaller dimension that can provide an adequate description of extremes in high dimensions. A main thrust of this research is to develop powerful modern learning techniques (such as graph-based learning methods and kernel principal component analysis) that allow one to determine the extremal support from the data. A second main thrust of this research centers on the issue of causality in both small and large dimensional problems. In the most basic form, a set of variables X is said to be tail causal to a dependent vector Y if certain changes in X (sometimes themselves extreme but not always so) impact the tail behavior of Y. The potential outcomes framework for causality of extreme events will be a major focus in this proposal?s research agenda. A third main thrust of this research is about differential privacy in the context of extremes, which provides tools for releasing global characteristics of a data set without revealing individual level information. This is achieved by modifying the data before releasing it and, in particular, randomizing it, in such a way that the output of the procedure does not depend too much on any specific observation while still allowing for statistical inference for certain characteristics of the original data set.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2328509","Conference: Ingram Olkin Forum at Carleton College","DMS","STATISTICS","07/15/2023","07/05/2023","Claire Kelling","MN","Carleton College","Standard Grant","Jun Zhu","06/30/2024","$20,000.00","Tarak Shah, Dean Knox, Gregory Lanzalotto","ckelling@carleton.edu","1 N COLLEGE ST","NORTHFIELD","MN","550574001","5072224303","MPS","1269","7556","$0.00","A forum on ?Statistical Challenges in the Analysis of Police Use of Force? will be hosted at Carleton College in Northfield, Minnesota on November 9-10, 2023.  Analysis of data regarding force used by police presents a number of statistical challenges that can, if not addressed carefully, lead to an incomplete or inaccurate understanding of the issues. This can in turn hamper or improperly redirect reform efforts, damage the credibility of advocacy campaigns, and alienate the communities most directly impacted by the use of force by police. This forum is part of a larger series of workshops hosted by the National Institute of Statistical Sciences (NISS) in a forum series titled ?Statistics Serving Society.? This series seeks to address urgent issues of statistical importance in society. Past forum topics have included algorithmic fairness, unplanned disruptions to clinical trials during COVID-19, and privacy-preserving methodologies. The Fall 2023 forum will result in collaborative research that is aimed at improving evidence-based policy on policing in urban and rural areas. Practitioners and community partners will further motivate the work being conducted and the resulting output of the forum. Engagement of both undergraduate and graduate students will be incorporated into the forum.<br/><br/>This in-person forum will highlight the unique approaches and perspectives developed by scholars working in this area, in order to build a deeper understanding of the issues and to improve the quality of evidence used to inform reform efforts. The range of obstacles and data limitations present in police use of force data make it a particularly challenging area to study, with issues including data quality and completeness, processing of unstructured data, privacy concerns, and statistical challenges in analyzing fairness. These issues are present in the study of other important social phenomena, and the knowledge gained through this exchange can improve research on related urgent social issues. This forum will also foster interdisciplinary collaboration between its proposed audience of statisticians, community members, activists, social scientists, policymakers, and law enforcement professionals. Such collaboration can lead to the development of innovative, data-driven approaches to address the complex issue of police use of force. Link: https://www.niss.org/events/iof-workshop-statistical-challenges-analysis-police-use-force<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310756","Developing Conjugate Models for Exact MCMC free Bayesian Inference with Application to High-Dimensional Spatio-Temporal Data","DMS","STATISTICS","09/01/2023","06/23/2023","Jonathan Bradley","FL","Florida State University","Standard Grant","Jun Zhu","08/31/2025","$227,251.00","","jrbradley@fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","1303, 8091","$0.00","The massive expansion in the production of data has led to natural computational challenges in uncertainty quantification. In particular, Bayesian methodology can account for sources of uncertainty, but requires techniques known to be computationally demanding. These difficulties are exacerbated when data are spatially and/or temporally correlated. The current solutions predominantly use either approximations or inefficient iterative methods such as Markov chain Monte Carlo (MCMC). This project resolves the computational challenges in uncertainty quantification with novel statistical methodology that does not require approximations and MCMC. Big data has impacted nearly every area of science, and as a result, methodological development and software for scalable, exact, MCMC free Bayesian methodology will have a substantial effect. Not only will the proposed methodology and software be an advancement in statistics, but it will be useful across a broad range of disciplines that deal with complex spatio-temporal processes such as neuroscience, climatology, demography, econometrics, ecology, meteorology, oceanography, and official statistics. The investigator will educate and train graduate students, and disseminate project findings through journal publications, public-use software, and conference presentations.<br/><br/>The objective of this project is to develop conjugate distribution theory for scalable Bayesian hierarchical models that create a larger framework for statisticians and subject matter scientist to perform MCMC free Bayesian inference without approximating the posterior distribution. In particular, this project will develop and extend the generalized conjugate multivariate (GCM) distribution, which allows one to simulate directly from the exact posterior distribution for a particular large class mixed effects models. This exact sample is referred to as Exact Posterior Regression (EPR). In Aim 1, the investigator will develop extensions of GCM and EPR to new settings including ordinal and nominal data, exact MCMC free inference for certain hyperparameters, and theoretical connections to existing statistical models.  Aim 2 involves extensions of EPR to multivariate spatio-temporal and multiscale spatial data, allowing one to leverage several sources of dependence to improve predictions and perform spatial change of support (COS) without the use of MCMC or approximate Bayesian methods. To achieve scalability, in Aim 3, the investigator will develop an exact Bayesian hierarchical model that repeatedly subsets the data in an informative manner that does not impose additional assumptions on the data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311249","Collaborative Research: Non-Parametric Inference of Temporal Data","DMS","STATISTICS","09/01/2023","06/16/2023","Wei Biao Wu","IL","University of Chicago","Standard Grant","Jun Zhu","08/31/2026","$252,937.00","","wbwu@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","1303","$0.00","This project is driven by the need to address inquiries in diverse fields, including environmental sciences, epidemiology, and economics among others. The study of extreme weather events, such as tropical storms, requires meteorologists to determine whether more potent tropical storms occur more frequently than mid or low-level tropical storms over time. Epidemiologists studying the transmissibility and severity of COVID-19 utilize clinical laboratory data to evaluate the pattern of the trends. In investigating sea pollution levels, earth scientists gather data on mercury concentration in animals to determine whether there has been a rising trend in mercury concentration over the years. The primary objective of this research project is to enhance the methods used to tackle these questions and effectively communicate findings to the scientific community and the public. More informed decisions can be made based on the findings. This project also involves training and mentoring graduate students through their active involvement in the research. <br/><br/>The research team aims to develop innovative statistical methods to study temporally observed or time-indexed multi-sample data, which consist of measurements of different subjects made at different time points. Such data do not fall within the conventional univariate or high-dimensional time series since measurements at different time points may not have an inherent connection. The investigators and collaborators will develop a systematic asymptotic theory to address this challenge to estimate and infer temporally observed multi-sample data. They will establish consistency, asymptotic normality, and an extremal distribution theory for various associated statistics and study simultaneous confidence bands and change points analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311251","Collaborative Research: Non-Parametric Inference of Temporal Data","DMS","STATISTICS","09/01/2023","06/16/2023","Likai Chen","MO","Washington University","Standard Grant","Jun Zhu","08/31/2026","$70,003.00","","likai.chen@wustl.edu","ONE BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","1269","1303","$0.00","This project is driven by the need to address inquiries in diverse fields, including environmental sciences, epidemiology, and economics among others. The study of extreme weather events, such as tropical storms, requires meteorologists to determine whether more potent tropical storms occur more frequently than mid or low-level tropical storms over time. Epidemiologists studying the transmissibility and severity of COVID-19 utilize clinical laboratory data to evaluate the pattern of the trends. In investigating sea pollution levels, earth scientists gather data on mercury concentration in animals to determine whether there has been a rising trend in mercury concentration over the years. The primary objective of this research project is to enhance the methods used to tackle these questions and effectively communicate findings to the scientific community and the public. More informed decisions can be made based on the findings. This project also involves training and mentoring graduate students through their active involvement in the research. <br/><br/>The research team aims to develop innovative statistical methods to study temporally observed or time-indexed multi-sample data, which consist of measurements of different subjects made at different time points. Such data do not fall within the conventional univariate or high-dimensional time series since measurements at different time points may not have an inherent connection. The investigators and collaborators will develop a systematic asymptotic theory to address this challenge to estimate and infer temporally observed multi-sample data. They will establish consistency, asymptotic normality, and an extremal distribution theory for various associated statistics and study simultaneous confidence bands and change points analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311178","Advances in High-dimensional Time Series Modeling and Its Interface with Deep Learning","DMS","STATISTICS","09/01/2023","06/12/2023","Yao Zheng","CT","University of Connecticut","Continuing Grant","Jun Zhu","08/31/2026","$55,603.00","","yao.zheng@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","079Z","$0.00","In time series, big data typically refer to high-dimensional time series (HDTS). Instead of analyzing individual series separately, the focus is on building a single framework to learn from time series of many interdependent variables simultaneously. Advances in statistical and machine learning methods for HDTS have attracted enormous attention from economists, financial analysts, engineers, and scientists in various fields. This project aims to develop new statistical models, inference tools and theory for HDTS while also exploring their interface with deep learning. The broader impact of this research on scientific communities and society will be promoted through interdisciplinary and collaborative research as well as efforts in course development and mentoring at different academic levels.<br/><br/>This project has three main objectives. The first objective is to develop new HDTS models that are more flexible, computationally scalable, and/or interpretable than existing ones, while filling the critical gap between finite- and infinite-order vector autoregressive frameworks in high dimensions. The second objective is to develop easy-to-implement inference tools, along with rigorous theory and efficient algorithms, that can address empirically important goals in HDTS analysis, such as Granger causality tests and dynamic factor inference, by blending state-of-the-art theory and techniques from HDTS modeling and tensor learning. The third objective is to explore and exploit the intrinsic connection between HDTS models and recurrent neural networks (RNN) to develop new algorithms and statistical theory for nonlinear deep learning-based HDTS models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2317533","Conference: ICSA 2023 China Conference","DMS","STATISTICS","06/01/2023","04/03/2023","Yichuan Zhao","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Jun Zhu","05/31/2024","$30,000.00","","yichuan@gsu.edu","58 EDGEWOOD AVE NE","ATLANTA","GA","303032921","4044133570","MPS","1269","7556","$0.00","The ICSA 2023 China Conference: Data Science with Applications to Big Data Analysis and AI will be held in Chengdu, China, on June 30 ? July 3, 2023. This award is used to provide travel support to the USA participants of this conference, who will deliver invited talks or give poster presentations. This conference is the annual conference of the International Chinese Statistical Association (ICSA), which provides statisticians and data scientists with an excellent opportunity to meet, interact, network and collaborations in the research. The main goal of the conference is to promote education, research in statistics and biostatistics, and provide a forum for leading experts, junior researchers, and practitioners to discuss big data analysis, machine learning, AI, and the integration with data science. The conference also provides a unique opportunity for the exchange of statistical research activities and development of data science between different countries. The financial support to graduate students, early-career researchers, and underrepresented researchers from the USA can help them to exchange ideas about new frontiers in statistics research with numerous outstanding researchers all over the world during the conference.<br/><br/>The conference maintains a very strong and comprehensive program, which includes two keynote speeches, over 160 invited talk sessions, junior researcher award session, and contributed poster presentations. The conference organizers strive to enhance the diversity of speakers and maintain the high quality of presentations. Moreover, the conference features a panel discussion on ""New Generation of Statisticians in Drug Development"", which is about the career development for junior statisticians. For students and junior researchers, the conference will provide other great opportunities such as junior researcher awards and poster sessions. The details about the conference can be found on the website https://maths.swjtu.edu.cn/english/ICSA_2023_China_Conference.htm.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310950","The Data Institute Conference","DMS","STATISTICS","02/01/2023","01/30/2023","James Wilson","CA","University of San Francisco","Standard Grant","Jun Zhu","01/31/2024","$15,000.00","","jdwilson4@usfca.edu","2130 FULTON ST","SAN FRANCISCO","CA","941171080","4154225203","MPS","1269","7556","$0.00","From March 12-14, 2023, the Data Institute at the University of San Francisco (USF) will host its third biennial Data Institute Conference at the downtown campus in San Francisco. The recent explosion of available data has fundamentally changed the way researchers across disciplines approach problems. Industry is experiencing a similar disruption as data driven decision-making is driving innovation toward new skills and roles in data science. While academic researchers have traditionally been the center of innovation and research, industry is pouring significant resources into developing solutions to many of these same problems. The Data Institute Conference will draw leaders from industry and academia to explore the latest theoretical advances and technological applications in data science. The conference aims to promote the next generation of cross-disciplinary research and offers educational and professional development opportunities to early-career data scientists from diverse backgrounds. The conference is designed to launch new research collaborations among academics and to build bridges between academia and advanced research groups from Bay Area industry and beyond. This conference will make an equally important impact to the greater scientific community with a focus on recruiting and increasing diversity in data science, with attendees who are not traditionally represented in STEM fields. <br/> <br/>Data science is a rapidly developing field with fluid boundaries but largely contains research areas from the statistical, mathematical, and computational sciences. This conference will bring together both junior and senior researchers from industry and academia to exchange ideas from recent advances to data science. There will be several invited and contributed talk tracks, including one in network analysis at which speakers will address many important questions on the estimation of graphical models, inference related to graphs, and applications of networks to neuroscience. Other tracks include practical issues and advances in experimentation and A/B testing; applications of machine learning to sports analytics; and precision healthcare. There will also be tracks devoted to the discussion of the societal impact of data science, including tracks on data privacy and artificial intelligence, the application of data science to public policy, and a panel discussion on data and algorithmic ethics. Most conferences in this space have attendees numbering in the thousands and are either very broad with dozens of parallel sessions or focused on a single topic in data science. Addressing the clear need of such a conference, the Data Institute conference will be of moderate size (250), with up to three parallel sessions at any given time to maximize interchanges between participants. For more information about the Data Institute Conference, please visit the following website: https://dsco.usfdatainstitute.org<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2241293","Conference: Third Annual Data Science Workshop: Emerging Data Science Methods for Complex Biomedical and Cyber Data","DMS","STATISTICS","02/01/2023","10/24/2022","Jie Chen","GA","AUGUSTA UNIVERSITY RESEARCH INSTITUTE, INC.","Standard Grant","Jun Zhu","01/31/2024","$19,089.00","Santu Ghosh","jiechen@augusta.edu","1120 15TH ST STE CJ3301","AUGUSTA","GA","309120004","7067212592","MPS","1269","7556","$0.00","This award provides support for the 3rd Annual Data Science Workshop entitled ?Emerging Data Science Methods for Complex Biomedical and Cyber Data? to be held on Augusta University (AU) Riverfront campus, March 16-17, 2023. With the increasing importance of the next generation of data, we strive to help students and early career researchers develop analytical thinking, statistical reasoning, communication skills, and creativity through this 3rd Data Science Workshop. The workshop will be in the form of specific research overviews and lectures provided by leading experts in the fields of statistical learning and data sciences. At the end of each day, a panel discussion will be held, led by an expert panelist and all the speakers of the day, for assimilating the various topics presented on the day and for addressing questions and comments from the audience. The Workshop participants will learn state-of-the-art forefront data science research methods used in academia, industry, and the government sector, facilitating them to be a more successful members of the future workforce in STEM fields.<br/><br/>This award supports the expert speakers and approximately 15 undergraduate and graduate students to attend the workshop. Graduate students will either present posters or give five-minute oral presentations on their respective research closely related to the theme of this workshop. The workshop aims to empower participants with emerging statistical methods that could allow them to address the complex data, arising from various applications and, in particular, from biosciences and cyber science. The topics of the workshop include deep learning, statistical machine learning, differential privacy, Bayesian data integration and cybersecurity data modeling, among others. Detailed information about this workshop is available at https://www.augusta.edu/mcg/dphs/workshop3/index.php<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2335847","Collaborative Research: Planning: FIRE-PLAN: Advancing Wildland Fire Analytics for Actuarial Applications and Beyond","DMS","STATISTICS, Human-Envi & Geographical Scis, HDBE-Humans, Disasters, and th, Cross-BIO Activities, Info Integration & Informatics","10/15/2023","08/09/2023","Tirtha Banerjee","CA","University of California-Irvine","Standard Grant","Jun Zhu","09/30/2025","$70,000.00","","tirthab@uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1269, 141Y, 1638, 7275, 7364","132Z","$0.00","The impacts of uncontrolled wildland fires range from the destruction of native vegetation to property damages to long-term health effects and losses of human lives. Increasing accuracy in projections in wildland fire activity, fire behavior, and wildland fire weather is the key toward developing more efficient fire control strategies and reducing the risks of wildfires. Recent studies have demonstrated that the tools of artificial intelligence (AI) can help in planning for upcoming prescribed burns by providing higher spatial and temporal fire weather forecasts and can also assist in developing more efficient strategies for wildfire risk mitigation. However, the modeling tools that are currently used to predict fire activity are largely subject to a number of temporal or spatial constraints. For instance, most deep learning (DL) approaches for wildfire risk analytics tend to be restricted in their capabilities to systematically capture the multidimensional information recorded at disparate spatio-temporal resolutions. Furthermore, such DL architectures are inherently static and do not explicitly account for complex dynamic phenomena, which is often the key behind the accurate assessment of wildfire driving factors. Finally, these models primarily rely on supervised learning approaches where a large number of task-specific labels (e.g., fire or no fire) are needed. To address these challenges in wildfire risk analytics, this project will leverage inherently interdisciplinary approaches at the interface of Earth system sciences, DL, computational topology, statistics, and actuarial sciences. <br/><br/>The project aims to introduce the concepts of topological data analysis (TDA) to wildfire predictive modeling, coupling them with such emerging AI machinery as time-aware graph neural networks. The resulting new methods are expected to better capture the shape patterns in the wildland fire processes with respect both to time and space and to assist in a more reliable statistical assessment of wildfire risks. The new high-fidelity predictive approaches will have the potential to deliver forecasts of fire behavior, fire activity, and fire weather at multiple spatial and temporal scales under scenarios of limited, noisy, or nonexistent labeled information. To enhance the utility of the research solutions in wildfire analytics, the researchers in this project will work in close collaboration with stakeholders, particularly, focusing on the insurance sector. The project will provide multiple interdisciplinary training opportunities at the nexus of wildfire sciences, AI, and mathematical sciences at all educational levels, from undergraduate students to practicing actuaries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311216","Collaborative Research: Multiple Hypothesis Testing on the Regression Analysis","DMS","STATISTICS","08/01/2023","07/21/2023","Zhigen Zhao","PA","Temple University","Standard Grant","Jun Zhu","07/31/2026","$197,007.00","","zhaozhg@temple.edu","1801 N BROAD ST","PHILADELPHIA","PA","191226003","2157077547","MPS","1269","079Z, 8091","$0.00","This research project will develop new theories and methodologies for tackling fundamental issues related to false discovery rate (FDR) control under regression analysis. Novel statistical tools will be provided to analyze data from various scientific studies, such as brain imaging, genome-wide association studies, and atmospheric science. The development of statistical methods to analyze complex data will facilitate discoveries of key variables related to blood pressure, coronary heart disease, stroke, and other critical health issues. Furthermore, this research project will provide training opportunities for graduate students, who will acquire the skills to meet the growing demand for data scientists in industry and academia and software will be developed and made available on publicly accessible websites.  <br/><br/>Analyzing high-dimensional data using multiple testing under the general framework of regression analysis is a critical challenge in the era of big data. This project will develop a new framework for performing model-free multiple testing for regression analysis, as well as optimal multiple testing for high-dimensional regression model in terms of maximizing the power of detecting the true alternatives subject to the type I error rate control. This framework enables statistical inference for sufficient dimension reduction when the dimension diverges with respect to the sample size and expands the current literature on multiple testing to more complicated data structures where commonly assumed model assumptions are not valid. The framework could also be applied to draw inference for explainable neural networks. The research project not only aims to advance theories in multiple testing but also targets applications of the developed theories.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310208","Fractional Ridge Regression","DMS","STATISTICS","09/01/2023","06/15/2023","Leonard Stefanski","NC","North Carolina State University","Standard Grant","Jun Zhu","08/31/2026","$320,000.00","","stefansk@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","","$0.00","Technological advances make it possible to collect enormous amounts of data. Implications for how businesses run (online retailing, precision manufacturing, social media), how science is conducted (environmental science, climate modeling, chemoinformatics, biotechnology, engineering), and how governments operate (health care, public safety, homeland security, national defense, agriculture production) are correspondingly enormous. For many uses of massive data sets, not all of the available information is relevant. For example, of the estimated 100,000 human genes, often only a handful are relevant to understanding a particular disease and developing a cure (the challenge is identifying the handful of relevant genes).  A key feature in many big-data explorations is the identification and deemphasis of superfluous information with the corresponding identification and accentuation of the most relevant information (separating the wheat from the chaff). Fractional Ridge Regression (FRR) is designed to improve both prediction and interpretability of statistical analyses of large data sets relative to statistical methods currently in use.  FRR improves the identification and extraction of relevant information from large data sets thereby improving the many areas of business, science, and government policy that rely on the analysis and understanding of large data sets. With nearly limitless applications, FRR research is ideal for engaging diverse statistics students in research projects.  Computing algorithms and statistical software will make FRR available to researchers in all disciplines, thereby multiplying its potential benefits to education and diversity in numerous areas of data science. The investigator will identify sub-projects for undergraduate and graduate students with attention to student recruitment from under-represented groups.<br/><br/>Fractional ridge regression joins ridge regression and the lasso in the statistician's regression modeling toolbox.  Ridge regression was introduced by Hoerl and Kennard in 1970 and twenty-six years later was followed by the introduction of the lasso by Tibshirani. The body of research ensuing from these seminal papers is staggering, and has contributed immensely to our understanding of shrinkage and selection methodology and to the practice of regression modeling in many areas of science. In some applications of regression modeling the goal is simply to achieve the best possible predictions of future response values. In other applications, interpretation is important as a way to guide understanding of the process under investigation. Ridge regression is very good at prediction, although it is often eclipsed by the lasso in terms of both prediction and interpretation because the lasso also allows for selection. Fractional ridge regression (FRR) improves both prediction (measured by mean square error) and interpretability (measured by variable selection specificity) relative to the lasso.  FRR accomplishes these twin goals via a unique and clever penalty function that adaptively downweighs only a data-driven subset of regression model coefficients (a fraction), while allowing for the complementary subset of regression coefficients to vary freely in order to obtain an optimal fitted model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310943","Graphical Modeling of High-Dimensional Functional Data: Separability Structures and Unified Methodology under General Observational Designs","DMS","STATISTICS","08/01/2023","06/14/2023","Alexander Petersen","UT","Brigham Young University","Standard Grant","Jun Zhu","07/31/2026","$178,859.00","","petersen@stat.byu.edu","A-153 ASB","PROVO","UT","846021128","8014223360","MPS","1269","8091","$0.00","The statistical methodologies outlined in this project are motivated by the need to analyze multi-subject neuroimaging data sets as well as longitudinal observations from biomedical studies, among a variety of other examples. In many instances, it is of primary importance to discover interactions and dependencies between components of the data that are collected over time. In the case of neuroimaging data sets, these dependencies represent areas of the brain that coordinate during a specific task or share common features of baseline activity when the brain is at rest.  This so-called functional brain connectome is known to be important biomarker for comparison across individuals or populations, provided that it can be reliably inferred from the data.  The size of such data sets is typically very large, leading to practical issues in computation as well as theoretical ones related to quantifying uncertainty in outputs produced by the statistical analysis.  The investigator will develop statistical methods, along with theoretical justifications and efficient computational packages, for estimating and interpreting functional connectivity networks and other large data sets of similar structure.  Through both research and instructional activities, the investigator will educate and train students at both the undergraduate and graduate levels in the development and use of statistical tools related to the project aims.<br/><br/>The data examples previously mentioned will be modeled as multivariate functional data (MFD), due to collection of multiple measurements at each time instant as well as the variability of these measurements across time. Most MFD methods, and the majority of existing computational tools for their analysis, simply apply univariate functional data methods to each component function separately, then combine the outputs for downstream analysis. Though simple, this approach ignores potentially valuable structures and properties that can be effectively harnessed in modeling and estimation. This is particularly the case for the graphical modeling of high-dimensional MFD that is the research focus of this project. The project aims to make foundational theoretical and algorithmic contributions to this nascent area of research by developing models and estimators that are flexible to different functional observation designs and manage the difficulties associated with the dual dimensionality problem of high-dimensional functional data, in which the large number of functions observed per subject is compounded with the intrinsically infinite dimension of each individual function.  Specifically, the investigator will develop novel tools for a regularized inverse correlation operator estimator, underlying separability structures of the MFD, and a historical functional graphical model. The products of the project will be validated mathematically by deriving relevant statistical properties of the estimators and empirically through the analysis of real data sets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311035","Modeling Complex Functional Data and Random Objects in Metric Spaces","DMS","STATISTICS","06/15/2023","06/14/2023","Yaqing Chen","NJ","Rutgers University New Brunswick","Continuing Grant","Jun Zhu","05/31/2026","$74,181.00","","yqchen@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","8091","$0.00","The rapid advancement of technology has led to a surge in complex data across various disciplines and society. This poses challenges for statistical analysis and is the driving force behind this research project. Examples of such complex data include brain connectivity correlation matrices, taxi trip networks, microbial compositions, and age-at-death distributions, for which algebraic operations such as sums and scalar multiplications are not well-defined and hence cannot be directly analyzed by conventional statistical tools that rely on algebraic operations to extract relevant information. This project aims to develop statistical methodology to address these data analytic needs, supported by theory and efficient computational implementations. This research project is anticipated to lead to substantial insights, including characterization of the co-evolution of brain regions of interest in early neurodevelopment, or comparison of mortality or income distributions across different countries. The methodology will also enable the detection of differences between groups of complex data, such as between brain connectivity networks during normal aging and pathological aging, and determine associations between different data objects, such as body mass compositions and physical activity intensity distributions. The project will also provide opportunities of statistical training and research for undergraduate and graduate students.<br/> <br/>This research project will develop statistical modeling and inference methods for functional data and random objects that take values in a metric space which by default does not possess vector space structures. The lack of linearity eliminates the applicability of existing methods that have been developed for Euclidean data and necessitates the development of novel tools for the analysis of such data. One focus of the research is modeling of sparse multivariate functional data. A factor analysis approach will be developed that lends itself to handling extreme temporal sparsity through nonparametric regression, which will be used to estimate the cross-sectional covariance matrices with the choice of a suitable metric. Another focus of the research is statistical modeling and inference for random objects in metric spaces. Principal component analysis (PCA) methods will be developed to model general object data using metric geometry. An application of the proposed object PCA to samples of random distributions in the Wasserstein space will be investigated, which benefits from the Wasserstein geometry potentially in conjunction with functional data analysis techniques. Inference methods will be devised for testing homogeneity and independence for samples of object data based on depth profiles, which uniquely characterize the law of random objects for a wide range of metric spaces. These developments will be accompanied by theoretical analysis and justification as well as scalable and stable algorithms that will be made into publicly available software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2318876","Conference: Midwest Machine Learning Symposium","DMS","STATISTICS","03/15/2023","03/03/2023","Mladen Kolar","IL","University of Chicago","Standard Grant","Jun Zhu","02/29/2024","$12,000.00","Mesrob Ohannessian, Yixin Wang","mkolar@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","7556","$0.00","This award supports the participation of graduate students and early-career researchers in the fourth Midwest Machine Learning Symposium (MMLS), to be held in Chicago, Illinois, from May 16-17, 2023. The main objective of MMLS is to convene regional machine learning researchers for stimulating discussions and debates, to foster cross-institutional collaboration, and to showcase the collective talent of machine learning researchers at all career stages. The symposium will continue to enhance the visibility of machine learning research at Midwestern institutions. Furthermore, the focus on junior researchers, both at the student and junior faculty level, aid in the educational and professional development of this demographic group.<br/><br/>The format of the conference includes multiple plenary talks, invited sessions, contributed posters (with student poster awards), and panel discussions. The organizers strive to maintain a good balance of demographic and geographic diversity among invited speakers and co-organizers, and will also widely advertise the symposium to encourage diversity among participants. The website with details about the symposium is http://midwest-ml.org/2023/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2329392","Conference: Summer Geometry Initiative","DMS","INFRASTRUCTURE PROGRAM, ALGEBRA,NUMBER THEORY,AND COM, GEOMETRIC ANALYSIS, APPLIED MATHEMATICS, FOUNDATIONS, STATISTICS, COMPUTATIONAL MATHEMATICS, ANALYSIS PROGRAM, Special Projects - CCF, Combinatorics","07/01/2023","06/23/2023","Justin Solomon","MA","Massachusetts Institute of Technology","Standard Grant","Tomek Bartoszynski","12/31/2023","$50,000.00","","jsolomon@mit.edu","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","MPS","1260, 1264, 1265, 1266, 1268, 1269, 1271, 1281, 2878, 7970","","$0.00","This award supports the 2023 offering of the Summer Geometry Initiative (SGI), to be held online July 10-August 18, 2023.  SGI is a six-week program supporting a cohort of undergraduates and early master's students in entering research in geometry processing.  In the first week of SGI, participants will attend hands-on tutorials introducing the theory and practice of geometry processing; no background or previous experience is necessary.  During the remaining weeks, participants will work in teams on research projects led by faculty and research scientists in this discipline, while attending talks and other sessions led by visiting researchers.  SGI aims to broaden the pool of students applying for PhD programs and employment related to geometry processing and applied mathematics more broadly; it has a long track record of increasing the size and diversity of the pool of new members of the community.<br/><br/>Geometry processing has a long history of breakthrough developments that have guided design of 3D tools for computer vision, additive manufacturing, scientific computing, and other disciplines.  Algorithms for geometry processing combine ideas from disciplines including differential geometry, topology, physical simulation, statistics, and optimization.  The tutorials and research in SGI bring SGI Fellows and other participants up-to-date with state-of-the-art ideas in this emerging discipline and introduce them to leaders in the field with whom they collaborate on exploratory research projects.<br/><br/>Website:  https://sgi.mit.edu/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310764","Theory and Methods for Modern Predictive Inference","DMS","STATISTICS","09/01/2023","08/23/2023","Jing Lei","PA","Carnegie-Mellon University","Standard Grant","Yong Zeng","08/31/2026","$240,000.00","","jinglei@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","079Z","$0.00","In statistics and machine learning, a central inferential task is to predict the values of future data given past observations. Prediction algorithms are widely used on a daily basis, such as spam detection, health risk evaluation, weather forecasting, economy, etc., making it one of the most fundamental and important classes of statistical inference tasks. In the past decade, the emergence of deep neural networks and powerful computers have made major progress in designing and implementing prediction algorithms, resulting in an explosive development of methods and applications. These new applications call for novel statistically principled methods with mathematical justifications to fully and correctly exploit the power of such new tools.  However, the unseen level of complexity in both the algorithms and datasets poses fundamental challenges to classical statistical and learning-theoretical frameworks that rely on simple problem structures.  Motivated by the algorithmic and computational advances in the modern data science era, in this research project, we plan to take on several new methodological challenges and fill theoretical gaps in statistical predictive inference, including simultaneous accuracy evaluation for a large collection of prediction models, and valid statistical inference using calibrated prediction. The project also provides research training opportunities for graduate students. <br/><br/>The research project consists of two parts. The first part will provide new insights into cross-validation, one of the most widely used methods for model and tuning parameter selection. The theoretical development will contribute to the understanding of the joint randomness of cross-validated risks, not only explaining the widely observed overfitting tendency of cross-validation but also pointing out potential solutions to correct it. This research work will further enrich and connect multiple areas of active research, including cross-validation, high-dimensional Gaussian comparison, model confidence set, and online learning. The second part aims at filling an important gap in the conformal prediction literature by developing a conformal-based hypothesis testing method beyond ex-changeability. We plan to explore connections between conformal prediction and classical topics such as Mann-Whitney rank-sum statistic, two-sample U-statistics, and semiparametric inference. The expected results will lead to new applications of conformal inference and create a new array of research problems across these related research topics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310578","Semiparametric Efficient and Robust Inference on High-Dimensional Data","DMS","STATISTICS","06/15/2023","06/13/2023","Alexander Giessing","WA","University of Washington","Standard Grant","Yulia Gel","05/31/2026","$175,000.00","","giessing@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","062Z","$0.00","Most data collected for scientific or industrial purposes suffer from missingness, lacking information about certain features that were lost at the sampling stage due to factors such as experimental design, non-compliance, or technical problems. This issue is particularly prevalent in high-dimensional data sets, where each observation comprises many features. Failing to effectively address the issue of missing and incomplete data results in inefficient and biased inference. Moreover, when traditional statistical methods, originally designed for low-dimensional incomplete data, are applied to high-dimensional data, they can yield misleading scientific discoveries. Therefore, there is an urgent need for innovative statistical methodologies specifically tailored to inference on incomplete data in high dimensions. For instance, in cancer research, next-generation sequencing technology allows for comprehensive genomic profiling. However, due to technical limitations and tumor heterogeneity, this profiling is typically incomplete. The ability to conduct valid inference after imputing incomplete profiles holds significant implications for advancing cancer treatment. This research project involves (1) developing methodology for inference on high-dimensional incomplete and missing data; (2) disseminating the resulting techniques to the statistics community via publications, seminars, and public release of software; (3) training PhD students in high-dimensional statistics and probability theory; and (4) increasing the exposure of high school students, undergraduates, and members of underrepresented groups to statistics and probability theory via introductory reading groups, conference presentations, and other outreach activities. The project will provide a broad range of mentoring, educational and professional development opportunities to train the next generation of statisticians and data scientists at various career levels.<br/><br/>The proposed research has two main objectives, namely (1) to develop a semiparametric efficient approach to inference after adjusting for incomplete data in high dimensions, and (2) to develop one- and two-sample bootstrap tests for high-dimensional hypotheses that retain correct size and power under incomplete data. Inference with incomplete data requires careful adjustments through inverse probability weighting or single/ multiple imputation. Any estimation error in these adjustments propagates into subsequent analyses. To address this challenge and achieve the first objective, the project will develop combined inference and adjustment procedures which treat imputation/ re-weighting not as a separate nuisance step, but as an integral part of the inference process. Specific solutions to several canonical incomplete data problems will be provided. For the second objective, the main challenge lies in designing a bootstrap procedure that accurately accounts for the variability of imputation/ re-weighting. To meet this objective, the project will develop a new parametric high-dimensional bootstrap procedure that can leverage such information. Different bootstrap tests for discrete/ categorical and continuous data will be provided.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311103","Finite multivariate density mixtures: applications and new approaches","DMS","STATISTICS","07/01/2023","06/16/2023","Michael Levine","IN","Purdue University","Standard Grant","Yulia Gel","06/30/2026","$160,000.00","","mlevins@stat.purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1269","1269","$0.00","Cluster analysis and classification of multivariate datasets are two of the most important tasks in modern statistical and data sciences. By leveraging copula-based density mixtures, this project will introduce new methods for clustering and classification of multivariate data which exhibit sophisticated dependence properties. The proposed methods will provide more reliable clustering solutions that could be in turn used, for example, for development of new therapeutic drugs and more precise quantification of gene interactions. This project will involve students, both at the undergraduate and graduate levels, to work on the computational aspects of this interdisciplinary research. The particular focus of the project will be on bolstering broader participation in statistical sciences and involvement of mentees from the underrepresented groups. Many of the students will be recruited through the National Alliance for Doctoral Studies in the Mathematical Sciences (commonly called simply ?Math Alliance?) that is headquartered at Purdue University.<br/><br/>Multivariate mixture models are widely applicable in many areas of statistics. They are particularly useful in clustering and classification of multivariate high-dimensional data. The majority of model-based clustering techniques for multivariate data are based on multivariate normal models and their direct generalizations. However, this approach is quite restrictive since, in most cases, it is difficult to model clusters of non-elliptical shapes. Even where it is possible, these models tend to be limited in their capabilities to cluster multivariate data of mixed types that include both continuous and discrete random variables. This project will develop an alternative to the existing model-based clustering methods for multivariate data. This alternative is based on using copula-based density mixtures which will allow for modeling a vast variety of dependence structures. Furthermore, the proposed solution will also allow for the principled clustering of datasets containing both continuous and discrete observations. The immediate outcome of this project will be the development of a family of computationally efficient algorithms. Such algorithms will provide reliable clustering of objects under the minimal assumptions. The proposed methodology will be widely applicable to such areas as clustering of genes and other biological entities in transcriptomics data as well as clustering of text documents, where the number of dimensions may be equal to the size of the vocabulary.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310679","Geometrization Approaches toward Understanding Deep Learning","DMS","STATISTICS","09/01/2023","06/15/2023","Weijie Su","PA","University of Pennsylvania","Standard Grant","Yulia Gel","08/31/2026","$275,000.00","","suw@wharton.upenn.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","1269","075Z, 1269","$0.00","The project will study the theoretical underpinnings of deep learning, a widely successful approach to various data-extensive applications. A comprehensive understanding of deep learning is crucial for the development of principled design and training of deep learning models, ultimately reducing computational burden and human costs. The research will span three foundational and complementary directions: understanding well-trained deep neural networks, examining deep learning training dynamics, and exploring data processing in interior layers of deep learning models. By bridging the gap between the complex training paradigms of modern neural networks and existing theories, the project will demystify these black-box models, making them more interpretable and efficient for a wide range of scientific and engineering applications. The project will offer multiple interdisciplinary opportunities for boosting the professional development of the next generation of statisticians and data scientists.<br/> <br/>The research activities will focus on three research projects to develop a comprehensive understanding of deep learning from a statistical and mathematical perspective. The first project will analyze symmetric geometries in the final stages of deep learning training, developing novel optimization techniques and statistical insights. The second project will provide a detailed understanding of the dynamics of modern deep learning training, integrating statistical and inferential ideas into the active field of deep learning dynamics. The third project will investigate how deep learning separates data according to class membership across all layers of the neural network, using techniques from random matrix theory, non-convex optimization, and learning theory. The successful completion of these research projects will result in a geometrization of deep learning methodologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311338","A statistical framework for the analysis of the evolution in shape and topological structure of random objects","DMS","TOPOLOGY, STATISTICS","07/01/2023","06/14/2023","Anne van Delft","NY","Columbia University","Standard Grant","Yong Zeng","06/30/2026","$329,639.00","Andrew Blumberg","av2972@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1267, 1269","068Z, 079Z","$0.00","Modern data sets often consist of sequential collections of point clouds that are samples from underlying objects with intrinsic geometry, such as curves, surfaces, or manifolds. Analyzing the dynamics of these time series of random objects requires qualitative inference methods that capture information on the geometric properties, i.e., the evolution of descriptors of the 'shape.' Analyzing shape is of paramount interest in many research areas such as genomics, climatology, neuroscience, and finance. In this project, we develop novel methodology and provide probabilistic and statistical foundations to model, analyze, and predict the evolution over time of geometric and topological features of data sets. The research will broaden the scope of the methodological interface between mathematics, computer science, statistics, and probability theory and will have direct applications to genomics and cell biology. We focus our theoretical work to support applications coming from two areas in genomics; cell differentiation in development and tumor evolution. This will be done in collaboration with the Herbert and Florence Irving Institute for cancer dynamics (IICD) at Columbia University. The research findings are also expected to influence model-building and data analysis techniques in geospatial data. Besides the theoretical contribution, we will provide software packages to make the inference methods available to a broad audience. The PIs further propose to design classes and produce expository notes from a cross-disciplinary perspective, and provide projects at the interface of mathematical statistics and topological data analysis for summer undergraduate mentoring.<br/><br/>Over the past few decades, there has been substantial interest in the area of geometric data analysis known as topological data analysis (TDA); this provides qualitative multiscale shape descriptors for point clouds. However, in order to draw reliable qualitative inferences on shape and topological features, it is crucial to account for the (evolving) spatial and temporal dependence present in the data. To address these questions, we take the point of view that the fundamental datum is a function, i.e., the observations are points in a function space. This perspective integrates statistical methodology and TDA in the context of functional time series (FTS). We provide novel methodology to model, analyze and predict data generated from nonstationary metric space-valued stochastic processes. Our framework establishes the statistical and probabilistic foundations for applying multiscale geometric descriptors to meaningfully capture their evolving geometric features as well as the investigation of topological invariants. This new methodology will allow practitioners to perform statistical inference to address important scientific questions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311034","Geometry Aware Exploratory Data Analysis and Inference Methods for Complex Data","DMS","STATISTICS","07/01/2023","06/13/2023","Paromita Dubey","CA","University of Southern California","Standard Grant","Yong Zeng","06/30/2026","$275,000.00","","paromita@usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","MPS","1269","079Z","$0.00","Complex big data appear routinely in the sciences and has become standard fare in contemporary data science. It is known to be difficult to analyze data that live in metric spaces, lacking fundamental vector space operations like addition and scalar multiplication and with no ordering between the data elements. Such data show up in the form of samples of histograms, networks, images, phylogenetic trees, and so on, and in multitudes of fields such as health monitoring, neuroscience, business and economics research, climate and environmental studies, evolutionary genetics, social sciences, and demography. Challenges are magnified when the observed complex data are dynamic, for example, when the data are time-varying or observed on other continuous domains. This project will push the frontiers in the state of the art of modern data analysis by creating a theoretically sound and user-friendly practical toolkit that will overcome these challenges for several important data analysis tasks. The new methods, being rooted only in pairwise distances between the data elements and tuning free by design, will immediately cater to the needs of scientists and engineers working with diverse representations of data, for example in longitudinal fMRI studies, online detection of the mutations in the virus phylogeny, understanding microbial diversity compositions, monitoring daily blood glucose distributions in electronic health analytics, time-varying gene-regulatory networks, understanding trends in social evolution and many more, offering practitioners a bundle of off-the-shelf tools to carry out exploratory analysis on the complex data before moving on to the downstream modeling tasks. The award will also support graduate students' training and offer research opportunities to undergraduates.<br/><br/>Model-free distance-based approaches drive the success of developing statistical methods oriented to complex non-Euclidean data with minimal requirements on the ambient data space or the data distribution. This research aims to expand the arsenal of methodology in object data analysis by developing new rigorously justified algorithms for common data analysis jobs and building inference procedures that lie at the heart of statistics and constitute the basis of what most scientists attempt to answer with data. To address the key challenge of the lack of a vector space structure in object data and the absence of ordering among the data elements, the new developments will be based on the concepts of depth profiles, which are the distributions of distances as dictated by the law of the data, and the transport ranks, that are center-outward ordering schemes for object data constructed using optimal transport maps between the depth profiles. Specific sub-projects will focus on rank-based object data clustering and classification, outlier detection, and mode-centric data analysis procedures. Inferential frameworks with rigorous theory will be designed for novel two-sample tests, independence tests, change point detection, and localization, all of which will be distance-based and easily implementable. Finally, the new tools will be broadened to include exploratory analysis and dimension reduction for time-varying object data, both when the observations are dense in time and the more challenging case when only sparse measurements in time are observed irregularly. Theory and methodology development will involve tools from the empirical process and U-process theory, M-estimation, and functional data analysis. Efficient and scalable software implementations together with codes for appealing visualizations, which are extremely challenging for object data, will be made freely available for practitioners.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2246815","Learning Complex Stochastic Systems","DMS","PROBABILITY, STATISTICS","08/15/2023","07/21/2023","Arnab Ganguly","LA","Louisiana State University","Standard Grant","Yong Zeng","07/31/2026","$200,000.00","","aganguly@lsu.edu","202 HIMES HALL","BATON ROUGE","LA","708030001","2255782760","MPS","1263, 1269","079Z, 9150","$0.00","Differential equations are often used to model temporal evolutions of a variety of systems. However, most realistic systems including those arising from biology, environmental science, engineering, physics, medicine and financial markets exhibit randomness in their behavior. Accurate analysis of such systems thus needs differential equations that can incorporate this randomness. Stochastic differential equations are powerful tools for this purpose. Understanding behaviors of these systems requires not just building mathematical models but integrating them with available data. This in turn requires various types of learning algorithms. It is important to judge the effectiveness of these algorithms by rigorous mathematical analysis, which is the primary objective of this project. The dynamics of these stochastic systems are however intricate with convoluted correlation structures, and there is a critical lack of mathematical results in the literature investigating learning methods for such complex data. The work done by the investigator will fill some of this gap by deriving mathematical results that will not only be able to answer if the algorithms become more accurate with data observed over longer periods of time but will be able to provide valuable insight on how to fine-tune the key parameters for optimal efficiency.  Building such data-driven stochastic models backed by rigorous mathematics enhances our understanding of complex systems across multiple domains and empowers informed decision-making in the presence of randomness.  The project will involve undergraduate and graduate students and will teach them valuable skills through a combination of theoretical knowledge, practical application, and hands-on experience with coding. It will enable them to excel in the digital age and adapt to the demands of an increasingly data-driven and technologically advanced world.  The results of the project will be disseminated through publications in well-known scientific journals and presentations at domestic and international conferences.<br/><br/>The project will study important learning problems for a broad class of stochastic differential equations (SDEs). These problems lie on the interface of stochastic analysis and statistical learning theory, and there is a paucity of theoretical results in probability, statistics and machine learning literature addressing them. The project is divided into three interconnected parts, each of which plays an important role in the other. Part I will address important problems on parametric inference including point estimation and testing of hypotheses. It will derive asymptotic results including law of large numbers, central limit theorems and large deviation principles for estimators of a finite dimensional parameter of a broad class of SDEs. Unlike some existing works in this direction which assume data to be in the form of a continuous trajectory, the investigator's work will consider the realistic case of availability of only discrete data points. Since asymptotic analysis requires the time horizon to go to infinity, the effect of time-gap (or discretization step) between the observations on the accuracy of these estimators over long time is not clear, and it is known that naive discretization of estimators based on a continuous trajectory of an SDE can lead to erroneous inference. The project will introduce appropriate scaling frameworks to quantify this effect and analyze the errors in different scaling regimes. Next, these results will be utilized to design tests for composite hypotheses-testing problems so that the probability of type I error decays rapidly and which are asymptotically uniformly powerful within a class of tests having similar level of type I error. Part II of the project concerns itself with the important topic of decision-making. Decision-making involves (constrained) minimization of suitable cost functions depending on model parameters.  Since these latter quantities are unknown, data-driven versions of such minimization problems are necessary in practice. In particular, it is necessary to construct suitable estimators of the cost functions so that decisions based on their minimization are close to the true decisions. The investigator will study a novel approach based on large deviation analysis and results of Part I which aims to guarantee that under appropriate conditions this can be achieved with a very high probability. Part III is devoted to nonparametric learning of SDEs. The last part falls in the realm of infinite-dimensional learning theory where the goal is to learn the entire driving functions of the SDE-based models as opposed to estimating finite-dimensional parameters. A rigorous computational framework combining Bayesian techniques with the theory of Reproducing Kernel Hilbert Space will be developed toward this end, and the theoretical properties of the resulting learning algorithms will also be studied.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310836","Robust Classification and uncertainty quantification for non-iid samples","DMS","STATISTICS","09/01/2023","07/24/2023","Leying Guan","CT","Yale University","Standard Grant","Yong Zeng","08/31/2026","$159,979.00","","leying.guan@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","079Z","$0.00","Although advancements in machine learning have significantly improved classification accuracy in various applications, recent research has expanded the focus beyond solely prediction accuracy. There is now an increased emphasis on the necessity for robust quantification of prediction uncertainty, self-awareness in handling abnormal samples, and the ability of classification models to generalize to minor or novel populations. Addressing challenges in these settings, where the traditional independent and identically distributed (IID) data generating assumption no longer holds, is crucial for effectively applying machine learning techniques in safety-critical and fairness-critical systems, such as medical diagnosis or policy making. The project will also contribute to the training of students through their involvement in the research.<br/> <br/>This project aims to advance robust methods for inferring class labels and quantifying uncertainty under complex non-IID settings by employing techniques from distributionally robust optimization, fairness learning, conformal prediction, and semi-supervised learning. The project will propose innovative classification strategies that exhibit improved worst-group performance across latent sub-populations and enhanced fairness with respect to potentially latent sensitive attributes. These strategies will be integrated with state-of-the-art machine learning techniques, including neural networks and gradient boosting, to develop robust, generalizable, and flexible machine learning algorithms. Additionally, the project will develop novel adaptive classification strategies and investigate their theoretical guarantees. These strategies will leverage both labeled training data and unlabeled test samples. Finally, the project aims to facilitate the in-depth application of the developed methods in safety-critical and fairness-critical systems, particularly in the domains of medical and immunological studies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311141","A shape-constrained approach for non-parametric variance estimation for Markov Chains","DMS","STATISTICS","09/01/2023","06/15/2023","Hyebin Song","PA","Pennsylvania State Univ University Park","Continuing Grant","Yong Zeng","08/31/2026","$82,440.00","Stephen Berg","hps5320@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","079Z","$0.00","Markov chain Monte Carlo (MCMC) methods have become one of the most important methods in modern statistics practice, as they provide straightforward computational approaches in a wide variety of statistical settings, such as Bayesian parameter estimation, uncertainty quantification for the estimated parameters, and model fitting while allowing uncertainties in model specifications. Despite widespread use, practical and theoretical difficulties remain for quantifying the uncertainty of estimates from MCMC simulations. This project will develop novel estimators for quantifying uncertainties in various MCMC sampling settings. In addition to making technical contributions, the project will result in the development of practical methods and open-source software packages that will enable practitioners to quantify uncertainty in MCMC estimates more accurately and make more efficient use of computational resources. This project integrates active research topics from multiple areas including statistical machine learning, MCMC, and nonparametric statistics, and therefore will provide an opportunity to train graduate students in these important areas of statistics.<br/> <br/>In this project, we combine ideas from the fields of MCMC sampling and shape-constrained estimation to propose novel non-parametric estimators for uncertainty quantification in MCMC sampling. In doing so, the investigators aim to advance various aspects of statistical inference related to variance estimation in Markov chains, and to improve understanding of shape-constrained estimators. Novel asymptotic variance estimators for Markov chain Monte Carlo (MCMC) based on shape-constrained inference will be developed, which will aid in uncertainty quantification for computer simulations based on MCMC. Additionally, the investigators will develop new technical tools to analyze non-parametric least squares estimators for functions with discrete supports with non-iid inputs. These findings will be used to establish the theoretical properties, including consistency, convergence rate, and bias-variance tradeoff characterizations, of the new estimators. New variance reduction methods will be developed for Monte Carlo methods, and efficient algorithms for computing shape-constrained estimators will be developed and implemented in open-source software packages.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311109","U-statistic Reduction with Accurate Risk Control","DMS","STATISTICS","07/01/2023","06/15/2023","Yuan Zhang","OH","Ohio State University","Standard Grant","Yong Zeng","06/30/2026","$199,999.00","","zhang.7824@osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269","079Z","$0.00","U-statistics are a class of statistics that play central roles in many modern statistical learning tools.  However, the heavy computational cost haunts their practical application.  Despite considerable research efforts since the 1970s towards computational reduction for U-statistics,  there exists little study on the important problem of accurate risk control in statistical inference for reduced U-statistics.  Also, how computational speed trades off with risk control accuracy remains uncharacterized.  This project will bridge this significant gap, providing the urgently needed infrastructual techniques that enable statisticians to securely scale up their U-statistic-based learning tools.  The results of this project will provide profound benefits to a wide spectrum of research areas and applications, including nonparametric statistics, machine learning, sociology, computer vision and biomedical sciences. The project will also provide research training for graduate students.<br/><br/>This project will study both classical ""noiseless"" U-statistics and network U-statistics, an important subset of noisy U-statistics.  The research will introduce innovative theoretical analysis techniques that lead to a sharp characterization of computational-statistical trade-offs and formulate new methods outperforming existing ones based on resampling and subsampling.  The project aims to establish a general framework for principled analysis of different popular U-statistic reduction schemes.  The research findings will be summarized into practical, step-by-step guides for easy implementation and tuning.  The PI's team will also develop and disseminate user-friendly software for public users.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311072","Non-parametric estimation under covariate shift: From fundamental bounds to efficient algorithms","DMS","STATISTICS","07/01/2023","06/13/2023","Martin Wainwright","MA","Massachusetts Institute of Technology","Standard Grant","Yong Zeng","06/30/2026","$330,000.00","","wainwrigwork@gmail.com","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","MPS","1269","079Z","$0.00","Machine learning (ML) methods for large-scale prediction have had dramatic impacts on various branches of science and engineering over the past decade.  However, these methods --- when they are used on data sets that differ from the training data --- are often reported to be unstable, or to fail in mysterious ways.  Failures of this type --- and our lack of understanding of their root causes --- pose a major roadblock to the adoption of modern ML methods in high-stake settings, where the cost of failure might be significant (e.g., self-driving cars, financial risk assessments, medical diagnoses).  The goal of this research project is to characterize the fundamental causes of such failures, and to develop new algorithms that mitigate these issues. The project will also integrate research and education through: (a) the involvement of both undergraduate and graduate students in the research and in the dissemination of research results; (b) the inclusion of the research results in courses at MIT and in the web-based course materials, which are accessed from other universities; and (c) short courses at summer schools and workshops. The project will also support mentoring graduate students and postdocs that are under-represented in the STEM fields, with continued professional support in their careers.<br/><br/>In more detail, the research focuses on the challenge of covariate shift, in which the distribution of the feature vectors used to train a large-scale prediction model differ from those on which it is evaluated.  An initial goal is to characterize fundamental limits and develop efficient algorithms for estimation under covariate shift, in the finite-sample non-asymptotic setting.  Armed with such an understanding, a follow-up goal is to develop computationally efficient procedures that achieve the fundamental limits along with theoretical understanding of their behavior.  The work in this project will leverage and build upon techniques and tools from non-parametric analysis, empirical process theory, concentration of measure, reproducing kernel Hilbert spaces, and information theory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311354","Scalable Bayesian regression: Analytical and numerical tools for efficient Bayesian analysis in the large data regime","DMS","STATISTICS","09/01/2023","06/13/2023","Andrew Gelman","NY","Columbia University","Standard Grant","Yong Zeng","08/31/2026","$299,908.00","Philip Greengard","gelman@stat.columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","075Z, 079Z","$0.00","Hierarchical regression has become a ubiquitous tool in statistics and data science. Applied researchers across the social and natural sciences, in fields such as epidemiology, political science, genomics, and many more, rely on hierarchical regression as an essential element of their data analysis toolbox. However, there are still major obstacles in practical and widespread use of hierarchical regression. The primary limitation is computational?learning from complex models with large amounts of data can require extensive compute time. With this research project, the investigators aim to supply the communities of applied statisticians and data scientists with a range of tools that open up more efficient user-friendly statistical modeling. This project provides research training opportunities for students.<br/><br/>The investigators will focus on the development of customized computational and analytical tools for statistical inference for hierarchical modeling. Popular tools for inference such as MCMC and variational methods rarely take advantage of friendly analytical structure in the model and they typically rely on many evaluations of the target density and its gradient. In this project, the investigators will exploit the analytical and numerical properties of the posterior density and capitalize on the oftentimes extensive friendly structure of models by building customized methods. This will involve the use of modern numerical linear algebra, approximation theory, and the tools of fast algorithms for the numerical solution of partial differential equations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311062","Nonparametric Testing: Efficiency and Distribution-freeness via Optimal Transportation","DMS","STATISTICS","07/01/2023","06/12/2023","Bodhisattva Sen","NY","Columbia University","Standard Grant","Yong Zeng","06/30/2026","$300,000.00","","bodhi@stat.columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","079Z","$0.00","Statistical hypothesis testing is the formal setup a statistician employs to decide between two competing hypotheses about the underlying data generating mechanism. Nonparametric methods have become increasingly popular in the theory and practice of statistics in recent times, primarily because of the greater flexibility they offer over parametric models. This research project investigates some problems in nonparametric hypothesis testing for multi-dimensional data. Modern computational capabilities, and the expanded data sets produced by modern scientific equipment have greatly increased the scope of such flexible statistical inference procedures. The investigator will develop a framework for ""distribution-free"" inference with multivariate data that generalizes many well-known and popular statistical ideas used for analyzing univariate data. On the collaborative front, the investigator will continue interdisciplinary research in astronomy. Further, some of these research problems will form the dissertation thesis of a current PhD student at Columbia. The investigator also plans to continue the tradition of mentoring undergraduate summer interns.<br/><br/>The main thrust of this research is to study distribution-free methods for multivariate and Hilbert space-valued data, based on the theory of optimal transport  -- a branch of mathematics that has received much attention lately in applied mathematics/probability/machine learning. These methods generalize the classical univariate rank-based methods to multivariate data. In the second part of the proposal, the investigator will study the asymptotic relative efficiency (ARE) of nonparametric tests and provide a characterization of ARE when the underlying test statistics converge weakly to an infinite mixtures of chi-square distributions, under the null hypothesis. This framework includes many interesting examples that arise in practice, including two-sample testing, independence testing, testing multivariate symmetry, inference on directional data, etc. The investigator will also develop a theoretical framework for estimating the ARE in this setting.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2239047","CAREER: Adaptive experiments towards learning treatment effect heterogeneity","DMS","STATISTICS","09/01/2023","12/20/2022","Jingshen Wang","CA","University of California-Berkeley","Continuing Grant","Yong Zeng","08/31/2028","$85,290.00","","jingshenwang@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","1045","$0.00","Understanding and characterizing differential and heterogeneous causal effects have become increasingly important in many scientific fields. For example, in precision health, identifying differential treatment effects serves as an essential step towards materializing the benefits of precision health, because it provides evidence regarding how individuals with specific characteristics respond to a given treatment either in efficacy or in adverse effects. In social science research, evaluations of the effectiveness of government programs or public policies across different individuals inform more effective policy-making. As reliably designed randomized experiments often provide evidence of the highest grade for verifying the effectiveness of a treatment or an intervention, this research project aims to develop three randomized experimental design strategies for better learning causal effect heterogeneity. These design strategies are broad and will be applicable in clinical trials, social experiments in biomedical sciences, public health sectors, and online controlled experiments in technological enterprises. Since the project will develop modern experimental design strategies and new statistical methods with many applications, this research project will provide opportunities for integrating research with teaching and training students across different stages. The project will impact STEM education through the training of undergraduate and graduate students and the recruitment of students from underrepresented groups into (bio)statistical fields. Activities to achieve these education-related goals include introductory reading groups, course developments, university undergraduate research programs, and outreach activities to underrepresented minorities.<br/><br/>This research project will develop three novel response adaptive experimental design strategies and theoretical insights toward learning treatment effect heterogeneity from a frequentist viewpoint. The first strategy will focus on designing randomized experiments to sequentially allocate experimental efforts so that subpopulations mostly harmed or benefited from a particular treatment can be efficiently identified. The second strategy will focus on learning treatment effect heterogeneity measured by the variability of the conditional causal effect variability. The learned heterogeneity allows the development of a new efficient covariate-adjusted response adaptive framework whose estimator may attain the best achievable efficiency. The third strategy aims to further materialize the benefit of treatment effect heterogeneity by designing randomized experiments to maximize participants' overall welfare. The research project is thus expected to open a new research connection between adaptive experiments and social welfare improvement.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310504","RUI: Predictive models with Incomplete and Fragmented Observations, and New Advances in Virtual Re-sampling for Big Data","DMS","STATISTICS","09/01/2023","06/29/2023","Majid Mojirsheibani","CA","The University Corporation, Northridge","Standard Grant","Yong Zeng","08/31/2026","$200,000.00","","majid.mojirsheibani@csun.edu","18111 NORDHOFF ST","NORTHRIDGE","CA","913300001","8186771403","MPS","1269","079Z, 9229","$0.00","A major focus of this project is on the development of new procedures to carry out statistical modeling, prediction, and inference in the presence of missing data.  Incomplete, missing, censored, and partially observed data are prevalent in many areas of medical sciences, engineering, economics and social sciences, which can in turn complicate the task of prediction and inference in data-driven decision-making processes. The investigator will study and explore the effectiveness of several new methods for handling missing values in complex data structures without imposing unrealistic or unnecessarily stringent conditions on the underlying mechanisms that cause the absence of information. Another major aim of this research project is to develop efficient data re-sampling methods to alleviate the formidable computational cost of computer-intensive statistical methods in big-data scenarios, where the data analyst must deal with, and sort through, massive amounts of data. The advent of such efficient methods is timely as the wave of ultra-large datasets has taken over many data-analytic initiatives in medicine, agriculture, and environmental protection. Additionally, this project embraces research experiences for graduate and undergraduate students, many of whom will then be persuaded to move on to further studies and research careers in STEM disciplines.<br/><br/>This research project deals with two broad classes of problems related to predictive models and inference. The first part focuses on selected topics in predictive models such as regression and classification for a number of nonstandard realistic setups. Specifically, the investigator will develop several local-averaging-type regression estimators in general metric spaces for incomplete and fractionally observed data with applications to statistical classification and the related problem of unsupervised machine learning. The aim is to carry out a rigorous study of the convergence properties of these estimators in various norms which is necessary for correct prediction and inference. In particular, this project will study and develop new exponential performance bounds for the Lp norms of the proposed estimators. The problem of bandwidth estimation for incomplete and fragmented functional data will also be studied; this is particularly important as the optimal bandwidth minimizing quantities such as the MISE or ISE is not necessarily optimal in classification. The second part of this research plan considers new objectives in virtual re-sampling as a method to reduce the formidable computational cost of big-data bootstrap in a number of important and challenging problems, while still retaining the benefits of bootstrap methodology. In particular, the investigator will develop virtual re-sampling strategies to (i) approximate the distribution of several refined higher criticism statistics for multiple testing problems in big-data scenarios, and (ii) to speed up the logarithmically slow rates of convergence of important functionals of density and regression estimators in two-sample problems such as those based on deconvolution density estimators and their sup-functionals for errors-in-variables models in big-data scenarios.  To achieve the objectives under (i) and (ii), the investigator will use adaptations of the methodologies used in the strong approximations of bootstrap empirical processes in the literature.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311307","Optimization of Markov Chain Monte Carlo Schemes with Spectral Gap Estimation","DMS","STATISTICS","09/01/2023","06/12/2023","Quan Zhou","TX","Texas A&M University","Continuing Grant","Yong Zeng","08/31/2026","$50,241.00","","quan@stat.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","079Z","$0.00","The enormous scale and increasing complexity of modern data make it computationally prohibitive to find exact solutions to many mathematical models used in data sciences. Sampling methods have emerged as an efficient alternative approach to this issue and are  widely used in various scientific fields, including biology, physics, finance, geoscience, and artificial intelligence. By generating a large number of random solutions and simulating different scenarios, sampling can reveal hidden patterns and relationships that may not be apparent from the data itself in a computationally efficient manner. However, for different data sets and scientific problems, the efficacy of a given sampling algorithm can vary significantly. One major goal of this project is to develop general methods for assessing sampling difficulty during the algorithm execution, which can be further used to adaptively tune the behavior of existing sampling algorithms to achieve optimal performance. For example, consider the problem of selecting genes associated with some complex disease using a modern genomic data set consisting of millions of genes. The research team aims to develop sampling algorithms that can quickly identify which genes are highly likely or unlikely to be associated with the disease and then allocate most computational resources to analyzing a small number of genes that may interact with each other and show intermediate evidence of association. This project will train students on both theoretical and interdisciplinary research and enable the investigator to develop and revamp courses related to sampling methodology.  <br/><br/>The Markov chain convergence theory provides both theoretical guarantees and practical guidance on the use of Markov chain Monte Carlo (MCMC) sampling, the main driving force behind Bayesian computation, and the past decade has witnessed a rapid development in the complexity analysis of MCMC methods for high-dimensional statistical models. However, estimation of the MCMC convergence rate from a single trajectory has received much less attention. The few existing methods are computationally expensive, making it difficult for practitioners to optimize the efficiency of MCMC schemes. This project aims to fill these gaps by developing novel theory and methodology for measuring, estimating and optimizing the convergence rates of various MCMC schemes, including classical Metropolis-Hastings algorithms and recently proposed importance tempering schemes.  New sampling algorithms will be developed aiming to overcome challenging multimodal target distributions that arise in statistics and other scientific fields. Additionally, a stochastic control approach will be explored to gain further theoretical insights into the optimal design of tempered MCMC schemes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311005","Bayes factor functions","DMS","STATISTICS","09/01/2023","06/08/2023","Valen Johnson","TX","Texas A&M University","Standard Grant","Yong Zeng","08/31/2026","$350,000.00","","vejohnson@tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","079Z","$0.00","Interpreting the evidence provided by experimental and observational studies is essential to the process of scientific discovery.  The most commonly used indirect measures of evidence are P-values.  Unfortunately, P-values do not directly reflect the probability that a new discovery has been made and are often mis-interpreted.  Bayes factors represent an informative alternative to P-values for reporting outcomes of hypothesis tests. They provide direct measures of the relative support that data provide to competing hypotheses and are able to quantify support for true null hypotheses.  The Bayes factor functions developed in this project will be defined from classical test statistics and will summarize evidence in support of scientific discoveries.  The resulting Bayes factor functions will provide clear summaries of the outcome from a single experiment, eliminate arbitrary P-value thresholds, and are ideal for combining evidence from replicated studies.  They will enhance both the reproducibility and replicability of scientific studies. The project will also provide research training for graduate students.<br/> <br/>The Bayes factor functions developed in this project will advance Bayesian testing theory and application in several ways.  These include defining Bayes factors directly from classical test statistics (which are readily available from standard statistical analyses);  modeling the distributions of these test statistics as functions of standardized effect sizes (the quantities of primary interest in scientific and observational studies); specifying scientific hypotheses so that there is a clear distinction between the null hypothesis of no scientific/treatment effect versus a new effect or benefit; and providing easily calculable expressions for the Bayes factors corresponding to given effect sizes. These innovations will allow scientists to easily assess statistical evidence arising from scientific studies conducted across a wide range of scientific disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310632","Statistical Inference for Optimal Transport","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, TPP-Tools for Particle Physics","08/15/2023","08/14/2023","Larry Wasserman","PA","Carnegie-Mellon University","Standard Grant","Yong Zeng","07/31/2026","$600,000.00","Sivaraman Balakrishnan, Mikael Kuusela","larry@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1253, 1269, 157Y","075Z, 079Z","$0.00","This research project concerns optimal transport, which is a mathematical method for transforming one probability distribution into another probability distribution.  Optimal transport has been used to transfer data from one scientific domain to another, thus enabling scientists to combine data from different sources. It has also been used to ensure that algorithms do not create unintended biases against demographic groups. The focus of this project is to develop rigorous statistical methods for optimal transport that permit precise assessment of uncertainty due to the fact that we only have access to finite datasets. The methods will be used with collaborators in particle physics to address data analysis problems that arise in data obtained for particle accelerators. Graduate students will be trained by including them in the research. Division of Physics provides cofunding for this award. <br/><br/>This project has three thrusts. The first is to develop statistical inference for transport maps.  The aim is to prove central limit theorems for these estimated maps and then use these theorems to construct confidence intervals. The investigators will also extend inferential ideas to robust versions of transport and to the Gromov-Wasserstein distance, which extends the idea of transport to measures on different spaces.  THey will then consider semiparametric theory (double robustness), higher-order inference, and optimal hypothesis testing.  The second thrust is the development of new transport maps.  By departing from the original definition, they can derive slightly less efficient maps that are easier to estimate and that still have good properties.  The third thrust is to apply the methods to the physical sciences.  This includes using optimal transport for estimating background distributions in particle physics, for simulator-based inference, to quantify the systematic uncertainty in particle unfolding, and to decorrelate signal classifiers from protected variables. They will also develop missing data transport for use when data from a signal region is not available.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2239234","CAREER: Statistical Inference in High Dimensions using Variational Approximations","DMS","STATISTICS","07/01/2023","12/28/2022","Subhabrata Sen","MA","Harvard University","Continuing Grant","Yong Zeng","06/30/2028","$86,479.00","","subhabratasen@fas.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269","079Z, 1045","$0.00","Modern data applications routinely involve massive datasets comprising a multitude of observations and features. To facilitate statistical learning in real time, there is an urgent need for principled and computationally efficient statistical methodology. Variational Inference methods have recently emerged as a popular choice in this context. The term ""Variational Inference"" refers to a general out-of-the-box strategy to develop statistical algorithms for a wide class of problems. For example, these algorithms are used as a sub-routine in text mining, generation of hyper-realistic artificial text and images, machine translation, etc. This approach is extremely attractive due to the computational efficiency of the proposed methods, and their superior practical performance. Despite these advantages, rigorous guarantees for these variational methods are still in a nascent state. This project will develop statistical guarantees for the validity of this approach in diverse settings. Subsequently, these new insights will be exploited to develop novel statistical methodology for modern data applications. The outcome of the proposed research will allow practitioners to deploy Variational Inference methods with confidence. In addition, the outcomes will add a new set of principled, computationally efficient methods to the statistician's toolkit. The PI will interweave his research and teaching throughout the research period and beyond. In particular, the PI will develop new undergraduate/graduate courses focusing on Variational Inference and mentor students (particularly those from under-represented backgrounds) with the aim of introducing them to opportunities in statistics and data science. The proposed research and educational activities will broaden participation in STEM generally, and encourage careers in statistics and data science.<br/><br/>This project will study statistical inference based on variational approximations focusing on three concrete thrusts: (i) Statistical inference based on the Naive Mean Field (NMF) approximation for regression models, (ii) NMF approximation beyond regression and (iii) Advanced Mean Field approximations. Under theme (i), the PI will develop empirical Bayes methodology for the high-dimensional linear model, and compare Bayesian variable selection algorithms using the NMF approximation. Theme (ii) will focus on the NMF approximation for Hidden Markov Random Fields and Bayesian Neural Networks. Finally, theme (iii) will focus on certain alternative mean-field approximations. Physicists conjecture that if the number of datapoints and features are both large and comparable, the NMF approximation is no longer accurate; instead, the Thouless-Anderson-Palmer (TAP) approximation, an advanced mean-field approximation, should facilitate Bayes optimal inference. The proposed research will establish this conjecture in the context of high-dimensional linear regression under a proportional asymptotic regime. The theoretical foundations of the proposed methodology will rest on disparate ideas originating in non-linear large deviations (studied in probability and combinatorics), spin glasses (studied in probability and statistical physics) and graphical models. In turn, these ideas will be combined with classical statistical ideas (e.g. nonparametric maximum likelihood) to develop computationally efficient methods for high-dimensional inference. This cross-pollination of ideas will generate independent follow up research directions in each domain.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310834","Simulation-based Inference through Random Features","DMS","STATISTICS","08/15/2023","07/26/2023","Cosma Shalizi","PA","Carnegie-Mellon University","Standard Grant","Yong Zeng","07/31/2026","$225,000.00","","cshalizi@cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","079Z","$0.00","Scientists use simulations to model all kinds of complex systems, from astronomy and ecology, through climatology and finance, to epidemiology and chemical engineering. Once scientists pin down all the parameters in a simulation model, it's easy to run it forward and see what it predicts. Going the other way, from outcomes back to parameters, is usually much harder, but it's the crucial step in fitting models to actual data from the real world, and so knowing which simulations are trustworthy. Good simulation modelers put lots of time and effort into working out what the crucial aspects or ""features"" of the data are for their models, and then adjusting their models so the simulation output matches those features of real data. This project aims to make such simulation-based inference nearly automatic. Bringing together ideas machine learning and nonlinear dynamics shows that simulation models can be fit by matching about two randomly-chosen features for every parameter. The project will make this into a practical and generic tool for simulation, by adapting the basic method to work with different types of data (time series, spatial, networks, etc.), writing software to automatically pick the features and adjust the model parameters, and developing statistical methods to quantify the uncertainty of the results. This can benefit every area of science and technology that uses simulations. The project will contribute to the training of STEM researchers through the involvement of a post-doctoral fellow and a graduate student.<br/><br/>Scientists increasingly express their ideas in generative models which produce fine-grained simulations of the processes they study. Traditional statistical approaches to parameter inference are ill-adapted to such models: just evaluating the likelihood function is usually computationally intractable, never mind optimizing it. Many techniques of simulation-based inference have developed in response, but these typically require picking multiple summary statistics or features, and tuning the generative model's parameters so that summaries calculated on simulations match those calculated on empirical data. Practitioners often spend considerable effort on carefully designing these summaries, aiming to minimize the loss of information from the full data. This project will free simulation-based inference from the need to design informative summaries, by instead using random functions of the data. This draws on two literatures not previously connected to simulation-based inference. One is work in machine learning over the last decade, which has highlighted the power of ""random features"", showing that predictions based on random functions of high-dimensional data are nearly as good as predictions based on optimal functions of the data. The other is now-classic work in nonlinear dynamics from the 1980s and 1990s, which suggests that 2d+1 random features should suffice to capture d underlying parameters. Bringing these ideas together with the well-established results on simulation-based inference will provide a simple, practical methodology of parameter estimation, uncertainty quantification and hypothesis testing, applicable to a wide range of modern simulation models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310788","New algorithms for Bayesian Computation","DMS","STATISTICS","09/01/2023","07/24/2023","Wing Hung Wong","CA","Stanford University","Standard Grant","Yong Zeng","08/31/2026","$225,000.00","","whwong@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","079Z","$0.00","Bayesian statistics is a highly principled approach to learn about unknown parameters and variables based on observed data. In this approach, existing knowledge about the unknown parameters is represented by the prior distribution. Once new data has been observed, then the prior distribution is updated by the Bayes formula to produce the posterior distribution which represents the updated knowledge about the parameter. Detailed information about the parameters of interest is usually obtained from the posterior distribution through computational inference methods such as Markov Chain Monte Carlo or Importance Sampling. However, these computational inference methods can become inefficient in some situations, such as when the likelihood function is too expensive to evaluate, or when the statistical model is given as a generative model without an explicit likelihood and can only be used to simulate the data.  The goal of this project is to develop approaches to Bayesian inference that remain computationally efficient in these situations. The results will enable wider use of Bayesian methods in many areas of science and technology. The project will also contribute to the training of graduate students through their involvement in the performance of the research.<br/><br/>Specifically, this project will create new computational tools to address two issues that are challenging for current algorithms, namely, how to sample from the posterior distribution in Hidden Markov Models with continuous variables, and how to design sequential methods for simulation from the posterior distribution even when the likelihood function is not available. Hidden Markov Models are widely used in the engineering and biological sciences, but currently algorithms for Bayesian inference in this model are available only if the variables involved are discrete variables. By creating efficient algorithms for the continuous case, the results of this project will enable engineers and computational biologists to apply these models to a much wider range of problems. The second goal of this project is to develop new tools for approximate Bayesian computation in models with intractable or unknown likelihood functions. These models can arise from many scientific areas such as phylogenetics and computer-based experiments. Currently there is only one available algorithm (the ABC algorithm) for Bayesian inference on this type of models. By developing an extension that can greatly improve the computational efficiency of this algorithm, the research in this project will benefit the aforementioned scientific areas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311292","Statistical Inferences under Monotonic Hazard Trend in Survival Analysis","DMS","STATISTICS","08/01/2023","06/30/2023","Chuan-Fa Tang","TX","University of Texas at Dallas","Continuing Grant","Yong Zeng","07/31/2026","$73,637.00","","chuan-fa.tang@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1269","068Z","$0.00","Correctly evaluating the increasing hazard rates under more risky environments or severe conditions is crucial for reducing modalities and failure rates. However, in many applications, the monotonic relationships between hazard rates and environments are misspecified or omitted and may further cause biased risk evaluation. This project will take the monotonic relationships seriously to obtain unbiased and efficient statistical inferences. The PI aims to develop distributional comparisons, parameter estimations, and hypothesis tests with data collected with ordered hazard rates. The proposed methods can be applied in broad areas such as biomedical, environmental, social, and physical studies. This project will also develop open-source software for a broader base of users. The PI will provide research opportunities for undergraduate and graduate students in modern statistics. In addition, graduate-level courses will be developed to help students explore and study shape-constrained statistical models and nonparametric regressions.<br/><br/>In this project, the PI focuses on statistical inferences under nonparametric hazard rate orderings and semi-parametric Cox-type proportional hazard models in survival analysis. When data were collected under hazard-ordered environments or treatments, the PI aims to test the equality of distributions, distinguish unequal distributions, and diagnose the hazard rate ordering assumption through nonparametric shape-constrained ordinal dominance curves. If hazard-related covariates were collected, the PI aims to study the partial linear Cox-type model with isotonic proportional hazards. This project will also examine the traditional Cox-type regression model by providing a goodness-of-fit test. The PI will explore both the theoretical and numerical performances of the proposed methods in the Cox-type regression models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311098","Dynamic treatment regimes  via smooth surrogate loss: theory, methods, and computational aspects","DMS","STATISTICS","09/01/2023","06/15/2023","Nilanjana Laha","TX","Texas A&M University","Continuing Grant","Yong Zeng","08/31/2026","$33,914.00","","nilanjanaaa.laha@gmail.com","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","079Z","$0.00","In complex diseases such as cancer, sepsis, or depression, patients often require treatments in multiple stages due to the dynamic nature of the disease. In such cases, physicians may need an algorithm or policy to switch to an alternative treatment option when necessary, in addition to prescribing the initial treatment.   Furthermore, since patients respond differently to treatments, physicians need to personalize the treatment policy based on each patient's specific needs and profile. This project aims to utilize modern, flexible machine learning techniques and existing patient data to identify the optimal treatment policy, known as the optimal dynamic treatment regime (DTR), in such time-varying situations. Ensuring scalability to real-world electronic health record data with a large sample size will be a key focus. This project will also develop and distribute user-friendly open-source software and provide research training experiences for graduate students.<br/><br/>Recent research has connected DTR policy learning to sequential classification problems, enabling the integration of machine learning techniques. However, currently, computationally efficient methods for solving the resulting classification problems are limited to specific cases, such as binary-treatment settings, and are prone to variance inflation. The first step of this project aims to demonstrate that the underlying sequential classification in any DTR policy learning problem can be provably solved via a smooth surrogate problem. This surrogate problem will be amenable to scalable machine-learning tools, such as stochastic gradient descent, facilitating fast implementation. In the second step, this project will combine the aforementioned machine-learning-based method with model-based techniques to construct a more stable hybrid estimator that is robust to potential model misspecifications. In addition, this project will comprehensively investigate the performance limits of the resulting methods by integrating classification theory, offline reinforcement learning, and the theory of nonparametric statistical inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310217","Fusion Pursuit for Pattern-Mixture Models with Application to Longitudinal Studies with Nonignorable Missing Data","DMS","STATISTICS","09/01/2023","06/14/2023","Lu Tang","PA","University of Pittsburgh","Continuing Grant","Yong Zeng","08/31/2026","$64,908.00","","lutang@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","068Z, 079Z","$0.00","Missing data is ubiquitous in scientific research, challenging the accuracy of statistical analyses, the results of which will ultimately generate knowledge and guide policy or decision making.  This project aims to develop a suite of new statistical tools to address the challenges in analyzing longitudinal studies with nonignorable missingness, such as informative dropout. The principal investigator will incorporate a machine learning approach termed fusion pursuit into the pattern-mixture modeling framework to achieve more efficient estimation and inference in longitudinal association analyses. The methods will find broad use in survey, medical, and policy research, and in other areas that involve longitudinal studies with a heavy presence of missing data. The project will also integrate research with the training of graduate students, developing trainees in the topics proposed through research involvement and teaching.<br/><br/>The project will extend the estimation and inference capabilities of pattern-mixture models in analyzing longitudinal data that are subject to missing not at random. Formulated in the framework of generalized estimating equations, this research develops a post-stratification fusion pursuit strategy to overcome over-stratification by missing-data patterns, which is the bottleneck of pattern-mixture models in analyzing large-scale data sets. The project will first develop a regularization approach to simultaneously collapse redundant missing-data pattern strata and estimate parameters of interest. To ensure valid statistical inference, the project will then develop a post-fusion inference approach to derive valid and generalizable confidence regions. Finally, the project will demonstrate the developed approaches in three situations of real-world longitudinal studies, including missing visits, missing covariates, and distributed data. The research project is expected to broaden the use cases of pattern-mixture models in analyzing longitudinal studies with missing data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311299","Data-driven selection of a convex loss function via shape-constrained estimation","DMS","STATISTICS","07/01/2023","06/13/2023","Min Xu","NJ","Rutgers University New Brunswick","Standard Grant","Yong Zeng","06/30/2026","$200,000.00","","mx76@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","079Z","$0.00","This research project focuses on the notion of loss functions, which is central to machine learning and statistics. Loss functions measure the difference between the output predicted by the model and the actual output, and they typically satisfy a property called convexity so that they can be easily optimized. Loss functions quantify how accurate a model is at describing the data and therefore, almost all predictive models are computed by learning model parameters which minimize a given loss function. Choosing a good loss function is vitally important; a good loss function not only improves our predictions, but also allows us to build tighter confidence intervals, and gives us greater robustness to outliers. Although there are general guidelines for choosing a suitable loss function, these guidelines are qualitative and imprecise; most people still default to a few standard choices such as the square error loss. The goal of this project is to develop methods to estimate an optimal convex loss function from the data at hand. We will design, implement, and test algorithms that practitioners can use to automatically obtain loss functions specifically optimized to their dataset, which will allow the practitioners to make better predictive models. Successful execution of this project will have far-reaching effects on standard practices in data science. This project will be deeply integrated with the planned educational components at both the undergraduate and graduate levels.<br/><br/>The first component of the project will look at linear regression and show that we can learn a data-driven convex loss function by approximating the unknown noise distribution with a log-concave density in a distributional distance known as the Fisher divergence. The proposed approach is computationally simple and, in settings where the noise is non-Gaussian, significantly improves upon the traditional squared error loss in estimation accuracy, inference quality, and robustness. The second component of the project will extend the idea to the setting of multi-task regression where the response is multivariate. The third component of the project will analyze the theoretical properties of score matching?the statistical method that underpins the first two components on convex loss estimation as well as being of fundamental importance in various other applications in statistical learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311252","Statistical Problems Through a New Perturbation Theory","DMS","PROBABILITY, STATISTICS, Combinatorics","09/01/2023","08/02/2023","Van Vu","CT","Yale University","Standard Grant","Yong Zeng","08/31/2026","$250,000.00","","van.vu@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1263, 1269, 7970","079Z","$0.00","One of the main goals of statistics and data science is to deduce information from a large set of data, usually given in the form of a large matrix. For instance, in most recommendation systems, the rows of the matrix represent customers, and the columns represent products, the entries are the (potential) rating of a customer for the corresponding product. Since data comes with noise, it is important to measure the impact of noise on the information we would like to deduce (in most cases, in terms of some matrix parameters). Given the popularity of the spectral method in data science, the problem has been studied by scientists across many fields for a century. However, in modern data science, it has been observed that very often, the data matrix has some structure, such as being low rank. The PI aims to develop a new theory with this restriction, which would improve many classical mathematical results, and at the same time, would lead to better estimates and faster algorithms for many real-life problems. The project also provides research training opportunities for graduate students. <br/><br/>Consider a matrix (which represents data). In practice, we often have access to a noise version of it, where each entry is added with some noise (or the whole matrix is added to a noise matrix). Classical perturbation theorems, such as Weyl or Davis-Kahan, provide us with estimates for the difference between key parameters of the original matrix (such as leading eigenvalues and eigenvectors) and their noisy counterparts. These results are sharp in worst cases analysis. However, in modern data science, data often has a low-rank structure, and noise is random. Under these assumptions, the PI has observed that one can provide much better bounds. Under this project, the PI will obtain optimal bounds under the above assumption, in various norms. The PI and his students will also attack several well-known algorithmic problems, such as matrix completion and Gaussian mixtures, using the spectral method combined with these new tools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2305631","CDS&E-MSS: Causal Induction in Sequential Decision Processes","DMS","STATISTICS, CDS&E-MSS","09/01/2023","08/02/2023","Qing Zhou","CA","University of California-Los Angeles","Continuing Grant","Yong Zeng","08/31/2026","$81,153.00","","zhou@stat.ucla.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244201","3107940102","MPS","1269, 8069","079Z, 9263","$0.00","Causal induction, i.e. learning the causal relations among a set of variables from data, is a fundamental problem in scientific research and engineering. An ideal approach to causal induction and inference is through experimental interventions. In many applications, scholars are often interested in maximizing certain outcome variables over a set of candidate experimental interventions, as in the so-called causal bandit problem. Motivated by a few pressing challenges faced by existing methods, namely unknown causality, data heterogeneity, and how to design adaptive experimental interventions, the PI will develop novel statistical methodology and theory for causal bandit and induction based on directed acyclic graphs, a class of popular graphical models for representing causality. Software packages will be released to provide efficient implementation of these methods and algorithms. The research will be integrated into education and research training at both undergraduate and graduate levels. Open-source software with detailed documentation will increase the visibility and the practical application of the research project. <br/><br/>This project investigates the causal induction problem under the framework of a sequential decision process. To handle unknown causality, the PI plans to develop a bandit algorithm with causal parent identification and a Bayesian backdoor bandit methodology that averages over parent set uncertainty. Both methods make efficient use of observational data to complement experimental data generated during sequential interventions. This will substantially generalize the causal bandit methodology and theoretical guarantees to the challenging setting under unknown causality. Borrowing the principle of optimism in the face of uncertainty, the PI further develops novel data-adaptive intervention procedures that directly target causal induction, to achieve active learning of the underlying causal graphical model. Since data generated under different experimental interventions do not follow an identical distribution, the PI will develop a new method to learn the interventional equivalence class on such heterogeneous experimental data. The PI will develop theoretical results to establish upper bounds on the cumulative regret in the causal bandit methods and the consistency of the structure learning algorithms. The sequential intervention design for active causal learning is adaptive to finite-size data and can be applied to a general underlying causal graph.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2238428","CAREER: Super-Quantile Based Methods for Analyzing Large-Scale Heterogenous Data","DMS","STATISTICS","06/01/2023","01/26/2023","Kean Ming Tan","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Yong Zeng","05/31/2028","$83,638.00","","keanming@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","1045","$0.00","Data-driven decision-making has become ubiquitous across many scientific disciplines and in every aspect of life.  As society relies more heavily on statistical modeling to make decisions, it becomes imperative for statistical methods to be able to model heterogenous data so that different decisions that will lead to better outcomes for different subgroups can be made.  This is appealing in the big data era as data-rich settings allow statistical models to be trained to be more flexible and robust to heterogeneous data. This project focuses on developing a new class of statistical methods for modeling large-scale heterogenous data based on the super-quantile (tail average), also referred to as the expected shortfall or conditional value-at-risk. The methods under development will have the potential to answer questions that cannot be directly answered previously using existing tools in different fields such as climate science, neuroscience, finance, and health disparity research.  The project will allow undergraduate and graduate students to work on cutting-edge methods for modeling data heterogeneity.  In addition, a graduate-level course on data heterogeneity will be developed.  The investigator will also engage in K-12 educational outreach in collaboration with the Center for Educational Outreach at the University of Michigan.<br/><br/><br/>With the abundance of data, there has been growing interest in modeling large-scale heterogeneous data for decision-making.  Quantile regression is one representative statistical tool for modeling heterogeneous data, but one limitation is that it focuses on a specific quantile level and may not be the best approach for answering scientific questions that involve aggregate information of the lower/upper tail of the distribution of interest.  The investigator studies the development of a new class of super-quantile-based tools to help practitioners answer such questions.  There are three aims in this project: (i) establish theoretical foundations and develop scalable computational algorithms for fitting super-quantile regression in the big-data regime, high-dimensional regime, and data with outliers; (ii) develop super-quantile regression methods in the presence of unmeasured confounders; (iii) develop a series of super-quantile-based methods for analyzing different data types. The investigator will create a comprehensive platform, including software and e-book tutorials, to encourage using super-quantile-based methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310769","Robustness and Optimality of Estimation and Testing","DMS","STATISTICS","10/01/2023","07/31/2023","Chao Gao","IL","University of Chicago","Standard Grant","Yong Zeng","09/30/2026","$160,000.00","","chaogao@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","","$0.00","This project is dedicated to tackling the challenge of data contamination in modern scientific studies through the advancement of robust estimation techniques. The primary objective is to develop innovative methods for robust high-dimensional estimation and robust nonparametric interpolation, enabling the identification of optimal procedures for different data sets with varying types of contamination. This research will yield invaluable insights into statistical inference within the realm of data science. Furthermore, its impact will extend beyond the field of statistics, reaching diverse disciplines such as genomics, biology, and social network analysis, where accurate analysis of complex data is of paramount importance. Moreover, this project places a strong emphasis on education and community outreach, fostering collaboration and inclusivity to unleash the full potential of big data for scientific discovery and understanding. The project also provides research training opportunities for graduate students. <br/><br/>This project addresses the challenge of achieving optimal robust statistical inference in both high-dimensional and nonparametric settings. It introduces novel statistical methods that satisfy the requirements of information-theoretic optimality, computational efficiency, and robustness to contamination. The research project covers various data contamination settings, including the classical Huber model and the modern Efron's model. In the high-dimensional setting, the project encompasses multiple comparisons, robust ranking, and robust group synchronization. In the nonparametric setting, the focus is on robust function interpolation. This project recognizes the existence of different types of contamination and aims to develop optimal robust procedures that can adapt to these diverse scenarios. Moreover, the project highlights the necessity for innovative methods to effectively handle contamination in large datasets and extract meaningful insights.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2306672","Applications of Algebraic Geometry to Multivariate Gaussian Models","DMS","STATISTICS","07/01/2023","06/16/2023","Aida Maraj","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Yulia Gel","06/30/2026","$56,286.00","","maraja@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","1269","$0.00","The present project aims to do an in depth analysis on the algebraic and geometric structure of two main types of Gaussian models that are commonly chosen in applications: colored Gaussian graphical (CGG) models and Brownian motion tree (BMT) models. CGG models are for modeling interactions among random variables, taking in consideration possible similar traits, and BMT models are Gaussian models for the evolution of continuous traits in mathematical phylogenetics. The investigator will then use this information to compute the complexity of the maximum likelihood estimate problem for each model, a key issue when analyzing data. Some elements of the project will involve undergraduate students majoring in STEM disciplines, especially those from underrepresented groups with limited educational resources.<br/>    <br/>The investigator will use algebra, geometry, combinatorics, and symbolic computations to better understand statistical models and make advancements on their maximum likelihood estimate (MLE) problem. For Gaussian models this starts by identifying Gaussian distributions with their covariance or concentration matrices and analyzing the polynomials vanishing on these matrices. The maximum likelihood degree (MLD) of a statistical model, which computes the complexity of finding the maximum likelihood estimate (MLE) of a statistical model for given data, relies on tools from algebra and geometry such as optimizing over an algebraic variety, intersection theory and polyhedral geometry. Specific questions that this project aims to answer are: (1) determine features in a phylogenetic tree that affect the maximum likelihood degree of its BMT model and connections to the algebraic degree of the vanishing ideal for the BMT model, (2) classify CGG models with toric structure; that is, with toric vanishing ideal or with vanishing ideal that turns toric after an appropriate linear change of variables, (3) find formulas for the maximum likelihood degree of CGG models and for the maximum likelihood estimate function of CGG models with MLD one. The investigator emphasizes CGG models with the algebraic structure of a toric variety because the bimonial equations of a toric statistical model can be used to fasten computations on the MLD of the model, produce Markov bases, contribute in hypothesis testing algorithms, and the polytope associated to the toric model is useful for studying the existence of MLE.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311306","Applications of stochastic analysis to statistical inference for stationary and non-stationary Gaussian processes","DMS","PROBABILITY, STATISTICS","09/01/2023","07/31/2023","Frederi Viens","TX","William Marsh Rice University","Standard Grant","Yong Zeng","08/31/2026","$250,000.00","Philip Ernst","viens@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1263, 1269","079Z","$0.00","Scientists and engineers want to extract information and develop an understanding of the natural world and human society by analyzing data to inform questions and decisions. The data often presents itself as the so-called time series, also known as stochastic processes, evolutions of measurable quantities over periods of time. The variations of these time series can be rather predictable from one period to the next, but less so over longer time intervals covering many periods. There are subtle differences in the nature of various types of these stochastic processes. For instance, the value of a financial stock or index, or the yearly global mean temperature, are buildups, accumulating stochastically over time. But daily returns on stocks or commodities futures, or the category (intensity) of successive Atlantic hurricanes, are of a different nature, typically showing a great deal of independence from one day or one event to the next, featuring a property of stationarity over time after adjusting for trends and seasonality. A critically important question is how some of these time series relate to each other. For instance, are global mean temperatures closely tied to Atlantic hurricane activity? Climate scientists would talk about significant attribution of the latter to the former if the relation is statistically significant. We have discovered that ordinary statistical tools work well to measure attribution when time series are largely stationary, but that the same tools can incorrectly point to a strong attribution when none actually exists, for time series, which are more accumulative. This incorrect attribution phenomenon, measured using a so-called correlation coefficient, occurs more frequently in scientific papers than one would hope. It is known as Yule's ""nonsense correlation"" in honor of the famed British statistician who first described the possibility empirically in 1926. Our work is the first to quantify exactly how this correlation can behave as a mathematical object, for accumulative time series, and for stationary time series. As a consequence of this award's work, we will provide scientists with demonstrably correct tools for correlations of time series, which will help them measure with great precision whether natural and societal phenomena, such as those described above, are statistically related, or whether they are more likely to be independent of each other. The project will also provide research training opportunities for graduate students. <br/> <br/> <br/>As is a well-accepted direction when developing tools for statistical inference, this award's work will study the properties of statistical tests which detect whether data streams are likely not to be independent. The objects of study are pairs of paths of times series or stochastic processes, and the empirical Pearson-type correlation statistic for any such pair. In particular, the work will apply to observational studies, rather than repeated experiments, since single time series are often the only type of data for any given environmental or economic variable. For stationary stochastic processes, we will derive precise estimates of the empirical correlation's fluctuations, by using calculations involving both exact distribution theory and normal approximations via stochastic analysis. These results will lead directly to proposing principled statistical methods for distinguishing between dependent and independent of pairs of stochastic processes. Next, we will investigate the realm of highly non-stationary paths, including random walks and Brownian motion, how the asymptotics for the empirical correlations deviate strongly from normality, and how to convert this information to the aforementioned application to distinguish between dependence and independence. Much of our work will draw on the distributional properties of classical variance and covariance objects for Gaussian vectors, as a technical aspect of stochastic analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310831","Toward Automated Uncertainty Quantification in Causal Inference","DMS","STATISTICS","07/01/2023","06/12/2023","Yixin Wang","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Yong Zeng","06/30/2026","$74,028.00","","yixinw@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","079Z","$0.00","When studying cause-and-effect relationships or making important decisions based on data, researchers and decision-makers often encounter uncertainties that can impact the reliability and trustworthiness of their conclusions. Understanding and quantifying these uncertainties is crucial for making informed choices, whether in scientific experiments, policy-making, or designing machine learning systems. To this end, the project aims to develop algorithms that can effectively address the challenge of uncertainty quantification in causal inference. In particular, many current approaches to quantifying uncertainty in causal relationships rely on sophisticated mathematical techniques that may not align well with real-world scenarios. This project seeks to change the situation by designing algorithms that are both theoretically sound and practically applicable across a wide range of situations. This project also provides research training opportunities for graduate students. <br/><br/>Technically, the project will focus on uncertainties in causal inference arising from two sources: uncertain causal graphs and limited informativeness of available data, both of which have significant implications for causal conclusions and downstream decision-making. To tackle these challenges, this project will develop algorithms that provide flexible and statistically valid uncertainty estimates, minimizing their dependence on specific causal problems. Leveraging recent advancements in algorithmic stability and private data analysis techniques, confidence intervals for causal estimates will be constructed, even when the causal graph is uncertain or learned from data. Additionally, these confidence intervals will be integrated with the available domain knowledge to further quantify the uncertainty arising from limited domain knowledge and the identification power of the data. Taken together, this project will facilitate the integration of different uncertainties, ultimately leading to more reliable and automated uncertainty quantification in causal inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310718","Game-theoretic statistics and safe anytime-valid inference","DMS","STATISTICS","08/01/2023","07/27/2023","Aaditya Ramdas","PA","Carnegie-Mellon University","Standard Grant","Yong Zeng","07/31/2026","$160,000.00","","aramdas@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","The majority of statistical inference ? a catchall phrase that refers to hypothesis testing, confidence intervals, prediction sets, and other forms of uncertainty quantification ? relies rather strongly on probabilistic modeling. However, reality does not always accord with the statistician?s models, especially when it involves non-random but yet uncertain events (like outcomes of sports games) or when the data source is not passive but may have an active role (for example, it may have an incentive not to be detected). The use of traditional probabilistic modeling may result in non-robust methodology, susceptible to being fooled by non-stochastic data. This project will develop broad foundations of a fundamentally different approach to statistical inference that was recently termed ?game-theoretic statistical inference.? The project also provides research training opportunities to graduate students. <br/><br/>Game-theoretic hypothesis testing is based on a broadly applicable principle of ?testing a hypothesis by betting against it? (or testing by betting for short). This is a project to develop a basic theory and methodology for nonparametric problems, where the assumption about the source of the data is minimized. Game-theoretic confidence sequences extend the aforementioned advances in testing to the setting of estimation. Since the data is not typically assumed as stochastic, the target of estimation must be carefully specified and could change with time. The project will develop the basic definitions and methodology for constructing such confidence sets and expound on nonparametric examples. Game-theoretic changepoint detection will directly build on the advances in the aforementioned two directions. Many classical change detection methods assume a parametric (and often i.i.d.) structure. The project will develop change detection methods that work in nonstationary and non-stochastic settings under nonparametric assumptions. This work has a few distinguishing points from classical statistical inference: (1) it is inherently sequential in nature, (2) it is often nonparametric and/or model-free, (3) it freely enables continuous monitoring and updating, and (4) it merges frequentist and Bayesian ideas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2331298","Conference: Mathematics and Machine Learning 2023","DMS","INFRASTRUCTURE PROGRAM, APPLIED MATHEMATICS, FOUNDATIONS, STATISTICS","09/01/2023","07/28/2023","Sergei Gukov","CA","California Institute of Technology","Standard Grant","Tomek Bartoszynski","02/29/2024","$40,000.00","","gukov@theory.caltech.edu","1200 E CALIFORNIA BLVD","PASADENA","CA","911250001","6263956219","MPS","1260, 1266, 1268, 1269","075Z, 079Z, 7556","$0.00","The conference ""Mathematics and Machine Learning 2023"" to be held at Caltech, Pasadena, CA on December 10-13, 2023 will bring together diverse groups of mathematicians and data scientists to discuss problems at the interface of these two disciplines. With the increasing role of artificial intelligence (AI) in everyday life and, more recently, in mathematical research, the time is ripe to start a regular annual meeting on this subject. Just like development of the World Wide Web started in the scientific community, the mathematical approach to machine learning can stimulate development of better AI algorithms and architecture whose future applications can extend far beyond 'pure' sciences. At the same time, advanced tools in machine learning start playing a major role in mathematical research, sometimes assisting expert mathematicians in making progress with some of the most difficult mathematical challenges. Although the interaction between the two communities seems natural, the opportunities are extremely limited and one of the main goals of the conference ""Mathematics and Machine Learning 2023"" is to fill this void.<br/><br/>The interactions between mathematics and machine learning (ML) can be roughly organized into three broad areas: the use of ML for very challenging cutting-edge problems in mathematics, the mathematical theory of ML, and the use of ML in automated proof assistants. All of these areas will be represented at the conference ""Mathematics and Machine Learning 2023.""  This award provides support to the speakers and to junior participants, with the priority given to underrepresented groups, Ph.D. students, and speakers/participants with no other sources of funding. Further information will become available on the conference website: https://mathml2023.caltech.edu/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311304","Trustworthy Reinforcement Learning: Inference, Reproducibility, and Adaptivity","DMS","Program Planning and Policy De, STATISTICS","08/01/2023","07/24/2023","Koulik Khamaru","NJ","Rutgers University New Brunswick","Standard Grant","Yulia Gel","07/31/2026","$175,000.00","","kk1241@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","066Y, 1269","","$0.00","In recent years, we have seen a surge of interest in the applications of statistics and machine learning to various problem domains in science and public policy, ranging from healthcare to education to engineering. This growth in the popularity of statistical methods comes with the new fundamental questions. First, most modern machine learning models, including deep learning and, more recently, deep reinforcement learning, are high-dimensional, thereby requiring development of novel statistical approaches for inference for such models. Furthermore, the datasets used in these models are often collected sequentially and, as a result, the data may not satisfy the assumptions of being independent and identically distributed. Examples include data for self-driving cars, online advertising, online recommendation systems, and personalized treatments. This project aims to develop novel statistical inference tools as well as computationally efficient approaches for reinforcement learning in independent non-identically distributed settings, with a particular focus on high-dimensional regimes. The project will offer numerous opportunities for interdisciplinary research training and professional development of the next generation of statisticians and data scientists.<br/><br/>The project consists of the two key thrusts at the interface of statistical sciences and reinforcement learning in high-dimensional settings.<br/>The first thrust is geared toward statistical inference for reinforcement learning under the high-dimensional scenarios where the data are independent and identically distributed. The focus of the second thrust is to design computationally efficient algorithms, specifically focusing on online and offline reinforcement learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310654","Doubly-robust variable selection in high dimensions","DMS","STATISTICS","09/01/2023","07/18/2023","Eugene Katsevich","PA","University of Pennsylvania","Standard Grant","Yulia Gel","08/31/2026","$225,000.00","","ekatsevi@wharton.upenn.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","1269","068Z, 7334","$0.00","The project will develop improved methodologies for the variable selection problem. The goal is to choose which explanatory variables are associated with an outcome variable of interest. Variable selection is a fundamental statistical problem that arises in countless application areas: Which genes influence the prevalence of a given human disease? Which demographic and socioeconomic variables influence a person?s income? Which electronic health record entries influence future medical costs? Especially when the number of potential explanatory variables is large, it is difficult to separate the important variables (the signal) from the irrelevant ones (the noise). The variable selection problem is also computationally challenging. Both of these issues hamper the progress of researchers in analyzing their data quickly and reliably. The project will address these important challenges by developing several methodological innovations and distributing the resulting improved methods in an open-source software package. The project will also provide multiple training opportunities to graduate students by involving them in the interdisciplinary research activities.<br/><br/>A wealth of research has gone into the high dimensional variable selection problem and the associated conditional independence (CI) testing problem, including model-X (MX) inference, debiased lasso inference, double regression CI testing, and semiparametric/causal inference. In the context of the CI testing problem (i.e. the problem of assessing whether a single variable is associated with the response given the others), the investigator has recently led an effort to break down the barriers between the distinct lines of existing work. Moving from the CI testing problem to the variable selection problem, the project will develop a methodology satisfying an exhaustive set of concrete statistical and computational requirements, all of which are not met by any existing methodology. First, the project will develop a procedure to construct null distributions for CI tests that are accurate in small samples (like the MX conditional randomization test) but require only a limited number of resamples (giving speed comparable to that of double regression approaches). Second, the project will develop a variable selection method that requires only a single machine learning step (like MX knockoffs) but is doubly robust (like double regression methods). Third, the project will develop and comprehensively evaluate Doubly Robust Variable Selection (DRVS), a best-of-all-worlds variable selection methodology incorporating the above two innovations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310664","Statistical Methods for Response Process Data","DMS","STATISTICS","08/01/2023","07/18/2023","Xueying Tang","AZ","University of Arizona","Continuing Grant","Yong Zeng","07/31/2026","$46,877.00","","xytang@math.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1269","","$0.00","The development of technology allows for the collection of diverse data but also poses challenges in statistical analysis. This research project aims to develop methods for analyzing response process data generated from recent computer-based educational assessments. Such data provide detailed information on test-takers behaviors that traditional item response data cannot capture. However, the complex format of the data and the diversity of human behaviors make it challenging to utilize the information systematically and efficiently. This project will develop innovative methods to understand and identify individual differences in learning and problem-solving. The resulting information will be valuable for designing individualized instruction or intervention strategies to support student success and advocate inclusiveness and equity in education. Additionally, user-friendly software will be developed for practitioners' use, and this project will provide research training opportunities for graduate and undergraduate students.<br/><br/>Response process data are an emerging type of data that tracks a respondent's interaction with computer-based items. This project aims to provide innovative, scalable, and interpretable statistical methods for utilizing rich information in response process data. Specifically, this project will focus on developing 1) a data-driven method for extracting features from process data, 2) a latent variable model for understanding how response process dynamics are driven by respondents' latent traits, and 3) a scalar-on-process regression model for describing statistical relationships between response process and other observed variables. Novel computational algorithms will be designed for statistical inference. The strong interpretability of the models will open the black box created by previous machine-learning-based approaches for process data, making it easier to validate the results and gain a deeper understanding of students' problem-solving behaviors. The outcomes of this project will enable educators to better evaluate students and design effective educational strategies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311059","Collaborative Research: Advances in the Theory and Practice of Non-Euclidean Statistics","DMS","STATISTICS","09/15/2023","07/24/2023","Victor Patrangenaru","FL","Florida State University","Continuing Grant","Yong Zeng","08/31/2025","$52,633.00","","vic@stat.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","079Z","$0.00","Shape, image, and RNA data types have had an important impact in many areas of Medicine, Science, Technology, etc. These modern data types contain different kinds of information that typically do not belong to a Euclidean space. As such, the analysis of these data types depends upon the development of statistical methodology that is adapted to address the topology and geometry of non-Euclidean metric object spaces. The project considers geometric-topologically informed statistical methods for the analysis of data lying on such object spaces. The methods to develop can find applications for new kinds of image analyses that are more likely to detect important features, identify new measures of location for shape, image, and RNA data, and improve the quality of peripheral computer vision and 3D image data. The developed methods for RNA sequencing data may apply to the investigation of viral diseases and cancer at the genomic level. The project also provided research training opportunities for graduate students.<br/><br/>The project develops statistical parameters and their inference in object spaces that underlie projective shape data, digital image data, and RNA sequence analyses. The development relies upon two key features - nonlinearity and compactness. Information extracted from these data types is often represented as points on a stratified space. This project extends classical statistical methods to colored images via RGB correlations and 3D scene reconstructions and considers the processed images to be points on an object space, which is embeddable in a Hilbert space. Moreover, this project expands upon existing approaches with the aim of increasing the computational speed of algorithms required for real-world applications and introducing distribution-free methodologies for these data types.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2308495","Principled phylogenomic analysis without gene tree estimation","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, MATHEMATICAL BIOLOGY","08/01/2023","07/19/2023","Sebastien Roch","WI","University of Wisconsin-Madison","Standard Grant","Amina Eladdadi","07/31/2026","$295,263.00","","roch@math.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1253, 1269, 7334","068Z","$0.00","This project aims to improve the estimation of species trees from genomic datasets. This estimation is challenging because different genomic regions evolve under processes that make their evolutionary histories (i.e., gene trees) discordant. This issue is exacerbated by widespread gene tree estimation errors in modern phylogenomic analyses. To address this challenge, this project's primary objective is to devise innovative mathematical, statistical, and computational techniques to analyze phylogenomic datasets without relying on gene tree estimation. This approach will produce more reliable species tree estimates in the presence of confounding processes. Species trees provide an evolutionary and comparative context in which many biological questions can be addressed. They play a vital role in understanding gene evolution, estimating divergence dates, detecting adaptation, studying trait evolution, etc. The developed methods will enhance the precision of biological discoveries based on species trees, advancing research that utilizes phylogenies. The project includes interdisciplinary research training for graduate students as well as the involvement of undergraduate students recruited through local initiatives. New course materials based on the proposed research will be developed for existing graduate courses and be made available through the PI?s website. The project will leverage connections to NSF-funded interdisciplinary institutes.<br/><br/>It is well established that different regions of a genome can evolve under different gene trees, due to processes such as incomplete lineage sorting, gene duplication and loss, and lateral gene transfer, complicating the estimation of species trees. Many methods that first estimate gene trees and then combine this information to estimate a species tree are known to have good theoretical guarantees, under the assumption that the true gene trees are known. That assumption is not satisfied in practice. Accounting theoretically for gene tree estimation error has proved challenging and few results are available. Building on prior work by the PI on the rigorous study of stochastic processes arising in this phylogenomic context, the proposed research will establish much-needed theoretical foundations for the analysis of multi-locus, multi-site datasets and the estimation of species trees without gene trees, including the development of novel estimators, the derivation of impossibility results and matching finite sample bounds, and the investigation of the effect of intra-locus recombination. This project will also enable the development of statistically rigorous, scalable algorithms. This interdisciplinary research will involve a close integration of applied probability, statistical theory, graph algorithms, and evolutionary biology.<br/><br/>This proposal is jointly funded by the Mathematical Biology and Statistics Programs at the Division of Mathematical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310881","Multiplex Generalized Dot Product Graph  networks: theory and  applications","DMS","STATISTICS","08/01/2023","06/29/2023","Marianna Pensky","FL","The University of Central Florida Board of Trustees","Standard Grant","Yulia Gel","07/31/2026","$250,000.00","","Marianna.Pensky@ucf.edu","4000 CENTRAL FLORIDA BLVD","ORLANDO","FL","328168005","4078230387","MPS","1269","1269, 8091","$0.00","Stochastic network models appear in a variety of applications, including genetics, proteomics, medical imaging, international relationships, brain science and many more. This research project examines collections of such networks, the so called multilayer network,  where each of the individual networks (layers) have the broadest possible organization and yet possess some common features that allow meaningful stochastic inference. Examples include brain networks of different individuals, protein interaction networks, and trade networks between countries in various commodities. The current project presents an integral effort of merging applications and theory and will develop techniques that will be applicable for solution of a variety of real-life problems involving graph-structured data. Results of this research will be beneficial for many domains of knowledge that rely on analysis of stochastic networks where layers belong to different groups: a) brain science research by providing tools for analysis of brain networks and their variations under various conditions; b) medical research and practice by providing model-based explanations on what makes brain networks associated with particular diseases different from normal; c) molecular biology by developing techniques for analyzing the enzymatic influences between proteins related to various functions; d) finance and international relations by analyzing world?s trade and financial networks corresponding to various modalities; e) social sciences by analyzing the similarities and the differences in communities related to different types of social connections. In addition, the project will provide ample opportunities for training through various educational activities, including mentoring Ph.D., M.S. and undergraduate students, teaching a Special Topics graduate courses, organizing interdisciplinary seminars, and promoting interdisciplinary research and diversity.  <br/><br/>In more detail, the project will study the multiplex network model where all layers have the same set of nodes, and all the edges between nodes are drawn within layers, which is true in the applications discussed above. The research will be built on the notion that  the matrices of probabilities of connections between nodes in layers of the network follow the most versatile Generalized Dot Product Graph (GDPG) model. GDPG includes all popular block network models as its particular cases. Although there have been some efforts to extend GDPGs to multilayer scenarios, the multilayer GDPG formulations have been limited to the very restrictive case where all networks are generated by the same invariant subspace. The latter is a direct extension of the SBM-equipped multiplex network where  communities persist in all layers. The deficiency of the above formulation is that it prevents finding partitions of layers of the network into groups according to some natural condition. Hence, it is imperative to advance the multiplex GDPG to the case where groups of layers are embedded into different subspaces. Finding those clusters of layers will allow to provide model-based assessments of the differences between networks corresponding to different conditions. In addition, GDPG will be further generalized to the multiplex Signed GDPG (SGDPG) network setting, which allows more flexible modeling of a variety of real life networks. The objective of the project is to provide various extensions to the multilayer GDPG models, to develop scalable algorithms and theoretical tools for their analysis, and to apply those findings to analysis of brain networks. Furthermore, it aims to supplement statistical procedures with precision guarantees via oracle inequalities and minimax studies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311064","Unravel machine learning blackboxes -- A general, effective and performance-guaranteed statistical framework for complex and irregular inference problems in data science","DMS","STATISTICS","07/01/2023","06/12/2023","Minge Xie","NJ","Rutgers University New Brunswick","Standard Grant","Yulia Gel","06/30/2026","$300,000.00","Jie Gao","mxie@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","079Z, 1269","$0.00","Despite tremendous growth in data analytics in the recent years, data scientists continue to confront a diverse set of emerging challenges, including the ?black-box? problems where machine learning methods might be empirically effective but tend to lack interpretability. These ?black-box? problems make it difficult to interpret the machine learning results and undermine the trust of the artificial intelligent outcomes, especially in health care domains. This research project will advance the foundations of interpretable data sciences and will develop new solutions for complex irregular statistical inference problems. The project will expand applications of statistics and uncertainty quantification in machine learning and data sciences. The research results will be integrated into course curricula to train the next generation of statisticians and data scientists. This project will pay a particular attention to advancing broader participation in statistical sciences at all educational levels and the research findings will be disseminated in various interdisciplinary venues to bolster knowledge synthesis among different domains. <br/><br/>In more detail, this project will provide novel mathematical and computational developments to tackle irregular inference problems and will unravel the black-boxes in several machine learning models in terms of their interpretability. Here, ?irregular inference problems? refers to highly complex problems for which the existing regular statistical inference conditions and large sample theories do not apply. The research agenda will focus specifically on providing valid and performance-guaranteed statistical inference for problems concerning discrete (numerical) or non-numerical parameters, and problems involving non-traditional data (e.g., non-numerical data), tailoring the study to the three popular machine learning models: random forests, deep neural networks, and graphical networks. The research initiative includes four subprojects: (i) uncertainty quantification of the tree learning methods and random forests; (ii) performance-guaranteed architecture discovery of deep neural network models; (iii) statistical inference for generative graphical networks; and (iv) theoretical developments for solving irregular inference problems. The results of the projects are expected to improve interpretability of a broader class of machine learning tools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311127","Taming distribution shifts in statistical learning","DMS","STATISTICS","07/01/2023","06/15/2023","Cong Ma","IL","University of Chicago","Standard Grant","Yulia Gel","06/30/2026","$275,000.00","","congma2015@gmail.com","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","079Z","$0.00","In recent years, machine learning has achieved remarkable success in various benchmark tasks, including image classification, question answering, and speech recognition. However, these models tend to exhibit reduced performance when applied to test data that differs from the data they were trained on. This scenario, also known as distribution shift, is a common issue in fields such as healthcare, criminal justice, and robotics. For example, a classification model trained on medical images from one hospital may not perform well on images collected from another hospital due to differences in medical equipment, scanning protocols, and subject populations. This project aims to address the challenges of distribution shift in learning from data by developing principled methods and theories. Additionally, it seeks to train the next generation of data scientists who can handle learning under distribution shifts.<br/><br/>The project will focus on three important problems involving distribution shifts: nonparametric regression under covariate shifts, off-policy evaluation, and offline reinforcement learning. To advance our understanding of these challenging problems, the project will (1) investigate more appropriate assumptions on distribution shifts that allow for efficient knowledge transfer between different distributions while also being practical; (2) characterize the information-theoretic limitations of learning under distribution shifts, determining when learning is possible and when it is not; and (3) develop efficient and adaptive learning methods with optimal finite-sample guarantees that adjust to the unknown amount of distribution shift. Throughout this project, new tools will be developed in high-dimensional statistics, information theory, and sequential decision making, and numerous opportunities will be provided to enhance interdisciplinary graduate and undergraduate research training.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311399","Making Use of the Curse of Dimensionality in Modern Data Analysis","DMS","STATISTICS","09/01/2023","06/14/2023","Hao Chen","CA","University of California-Davis","Standard Grant","Yulia Gel","08/31/2026","$275,000.00","","hxchen@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","1269","$0.00","This research project delves into cutting-edge data analysis, frequently dealing with high-dimensional or non-Euclidean data, such as sensor readings, genomic information, imagery, and network datasets.  Such data is commonly encountered across diverse disciplines, including biology, social science, computer science, and astronomy.  A major challenge in analyzing this data is the curse of dimensionality, which causes traditional tools to degrade rapidly as the number of dimensions or features grows.  While previous attempts have focused on reducing the dimensionality of the data or through regularization techniques, these methods often exhibit limitations.  In contrast, this project adopts an innovative strategy by harnessing the patterns that emerge as a result of the curse of dimensionality to bolster data analysis.  The project aims to provide valuable tools for data analysis and explore the role of statistics in the era of big data.  The tools developed within this project will be made available as open-source software packages with thorough documentation, enhancing collaboration between the statistics community and researchers from various scientific fields and making data analysis procedures more transparent.  The project also includes training and educational components for undergraduate and graduate students, equipping them with interdisciplinary data analysis skills that will be invaluable for the next generation of researchers.<br/><br/>This project seeks to develop innovative methodologies and foundational theories for crucial data analysis tasks involving high-dimensional and non-Euclidean data.  Specifically, the project will create a pioneering high-dimensional classification framework that leverages interpoint distance ranks and takes into account the curse of dimensionality, resulting in significantly reduced misclassification rates compared to existing methods across various settings.  Moreover, the project will establish a unified community detection framework capable of identifying all three community structures -- assortative, disassortative, and core-periphery ? without prior knowledge of which community structure the network is.  By linking these distinct structures to high-dimensional data behaviors, where the core-periphery structure naturally emerges due to the curse of dimensionality, the unified framework exhibits superior performance in numerical studies on simulated and real datasets across all three mixing patterns, whereas existing methods struggle in at least one of these patterns.  Lastly, the project will develop an innovative high-dimensional clustering framework that employs the patterns of curse of dimensionality to reduce misclustering rates.  These methodological and theoretical advancements will enhance the understanding of modern, complex data from diverse fields, promoting the comprehension of significant scientific issues in these areas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311024","Robust and scalable algorithms for learning hidden structures in sparse network data with the aid of side information","DMS","STATISTICS","08/15/2023","06/15/2023","Adel Javanmard","CA","University of Southern California","Standard Grant","Yulia Gel","07/31/2026","$275,000.00","","ajavanma@usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","MPS","1269","068Z, 1269","$0.00","Network data is becoming increasingly relevant in various areas, including social sciences, biology, computer science, and engineering. In the social sciences, network data is used to study social interactions and relationships, such as friendship networks, political affiliations, and knowledge transfer networks. In biology, network data is used to model and analyze biological systems, such as gene regulation networks, protein-protein interaction networks, and food webs. Learning the hidden structures within networks, in particular detecting and modeling community structures, is of paramount importance. This process not only enhances the interpretability of data but also enables data compression, manages data heterogeneity by detecting latent subpopulations and fitting appropriate models to each, and addresses the issue of missing labels.  Despite a plethora of clustering algorithms, current approaches often suffer from scalability and robustness issues, limiting their effectiveness in real-world applications. Furthermore, as data sharing becomes more prevalent, there is often a wealth of contextual information available about the nodes in a network, such as demographic information or browsing history for users on an online platform, that can be effectively combined with network data to greatly enhance the effectiveness of clustering procedures. To tackle these limitations and advance the field, this project will develop robust and scalable inferential network methods, which adapt to the heterogeneity of node degrees and allows to combine nodewise side information with pairwise interaction data for a more effective analysis. This project will support education in statistical and machine learning research by providing training opportunities for graduate students, from diverse backgrounds, to participate in cutting-edge research. It also benefits society by providing tools for understanding and managing complex network systems.<br/><br/>This research consists of three interrelated parts, which work in concert to provide a unifying framework for learning latent structures in sparse network data. In the first part, we devise clustering algorithms based on semidefinite programming which allows us to combine the high-dimensional contextual information on the nodes with the interaction graph. In the second part, we will develop methods to improve the robustness of the inferential algorithms to adversarial perturbations in the nodewise contextual data or the interaction graph. The third part builds on the algorithms devised in the previous two parts and provides low-computation and memory-efficient implementation of these algorithms that can scale to large-scale network data. In addition, the project will investigate the potential uses of this project across diverse domains, utilizing the resulting clustering algorithms and optimization-statistical tools.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2310942","Semiparametric Techniques for Data Exploitation across Heterogeneous Populations","DMS","STATISTICS","07/01/2023","06/14/2023","Jiwei Zhao","WI","University of Wisconsin-Madison","Standard Grant","Yulia Gel","06/30/2026","$224,999.00","","jiwei.zhao@wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","1269, 8038, 8091","$0.00","In various fields, from clinical medicine to policy research, researchers often have access to data and information from multiple populations that are relevant but different. For example, in biomedical studies that use electronic health records, relying solely on labeled data for analysis may be inefficient due to small sample sizes resulting from various resource constraints. In a clinical trial setting, physicians may need to interpret evidence from a randomized controlled trial consisting of patients whose demographics and other historical characteristics are quite different from their own patients. Similarly, researchers studying a pneumonia outbreak during the flu season may find a predictive model developed during the non-flu season to be relevant and useful. In all of these scenarios, it is crucial to develop methods that can appropriately incorporate information from one population into statistical analyses for another. This project will develop a suite of statistically sound methods that can effectively integrate external data into primary studies. The research product has the potential to be applied to various fields, such as Alzheimer's disease, mental health disorders, cancer, and pain research. The project also contains active mentoring plans at both disciplinary and interdisciplinary levels, benefiting local high school students, undergraduates, master's and PhD students, as well as biomedical investigators.<br/><br/>In this project, the unique combination of semiparametric statistics, robust statistical methods, statistical learning techniques, missing data analysis, and high-dimensional data analysis will be leveraged to develop a suite of statistically sound methods for incorporating external data into primary studies. The new methods have minimal model assumptions: they are either developed under an assumption lean framework or allow for misspecification of more than one nuisance model in the procedure. Compared to naive methods that do not incorporate external data, the new methods are guaranteed to increase estimation efficiency, improve statistical power, and enhance scientific discovery. Moreover, they achieve maximum efficiency gains when the nuisance models are correctly specified.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311187","Collaborative Research: Design, Modeling and Active Learning of Quantitative-Sequence Experiments","DMS","STATISTICS","08/01/2023","06/15/2023","Xinwei Deng","VA","Virginia Polytechnic Institute and State University","Standard Grant","Yong Zeng","07/31/2026","$181,968.00","","xdeng@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1269","079Z, 8038","$0.00","A new type of experiment concerning both quantitative and sequence (QS) factors has recently drawn great attention in science and engineering applications. In chemotherapy, to develop efficient drug combinations involving several drug components, researchers need to conduct experiments optimizing both the doses and the sequence orders of drug components. Such a problem raises new challenges for statisticians since the input space is semi-discrete and grows exponentially with the number of drugs. Researchers rely more than ever on statistical modeling and active learning to identify optimal settings given limited experimental resources. Additionally, QS experiments often have specific requirements. In the computer experiment for metal additive manufacturing processes, the output response is binary (success/failure), and it requires both interpolation and uncertainty quantification, which is an unsolved problem in the current literature. In this project, the investigators will provide systematic solutions to QS experiments, addressing challenges in design, modeling, uncertainty quantification, and active learning. The outcome of this project will help save experimental costs in applications involving QS factors. The applications to chemotherapy will help advance cancer research in the U.S., while the applications to manufacturing processes will enhance the industrial competitiveness of the U.S. Also, this project provides research training opportunities for graduate students. <br/><br/>Active learning in experiments, aka reinforcement learning under the broad context of machine learning, allocates runs in an adaptive manner, which is generally more efficient than one-shot experiments for optimizing the experimental settings. This project will establish new Gaussian process-based models for physical experiments with QS factors, based on which new active learning procedures will be developed. For analyzing computer experiments, a novel Hopfield process (HP) framework will be established as an accurate surrogate for interpolating binary (and categorical) outputs, which will facilitate uncertainty quantification and active learning. Optimal QS experimental designs will also be constructed by combing several Williams-transformed good lattice point sets, which possess desirable properties including space-filling, orthogonality, and paired balance. This research project will provide systematic solutions for various types of QS experiments that are of interest in scientific research and industrial applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2311291","Safe and Robust Causal Inference for High-Dimensional Complex Data","DMS","STATISTICS","08/01/2023","06/13/2023","Yang Ning","NY","Cornell University","Standard Grant","Yong Zeng","07/31/2026","$160,000.00","","yn265@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","","$0.00","Understanding causal effects among multivariate variables is central to many empirical and experimental types of research. In an era where data is vast, statisticians are faced with the challenge of drawing causal inferences from massive data. For example, the analysis of data can be complicated by the presence of potentially confounding variables, heterogeneity, and temporal dependence within populations. This poses tremendous computational and statistical challenges to existing causal inference methods. Driven by the availability of modern datasets, the research project aims to develop cutting-edge machine-learning methods to address the theoretical, methodological, and computational challenges of drawing causal inferences from massive data. The development of the proposed research would not only push the frontier of causal inference theory but also benefit a broad range of researchers in a variety of areas, including medicine, epidemiology, computer science, and social science. This project also provides research training opportunities for graduate students. <br/> <br/>The goal of the project is to develop a novel high-dimensional causal inference framework that is influence-function doubly-robust and safe relative to a class of base estimators. Specific projects include proposing a covariate balancing methodology coupled with modern machine learning techniques for efficient causal inference with high-dimensional data, optimally distributed covariate balancing for massive heterogeneous data, and a sequential covariate balancing approach for marginal structural models in longitudinal data. A common theme throughout is the use of the covariate balancing methodology, which comes from the ``propensity score tautology,? the estimated propensity score is appropriate if it balances the covariates.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2236854","CAREER: Data-Centric Evolutionary Contagion Models with Parallel and Quantum Parallel Computing","DMS","STATISTICS, MATHEMATICAL BIOLOGY","07/01/2023","01/24/2023","Andrew Holbrook","CA","University of California-Los Angeles","Continuing Grant","Zhilan Feng","06/30/2028","$104,906.00","","aholbroo@g.ucla.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244201","3107940102","MPS","1269, 7334","079Z, 1045","$0.00","Global viral epidemics produce vast amounts of high-dimensional data indexed by the location and time each virus is observed. Scientists, businesses, governments and independent organizations want to learn from this data so they can understand basic biological mechanisms, invest capital, allocate aid and design coherent policy in a changing world. Evolutionary contagion (Evo-Con) models seek to identify and predict viral variants with heightened rates of spread by jointly modeling viral contagion and evolution. Analyzing spatial patterns of viral contagion is an area of immense scientific interest, but the task requires accounting for the nature of transportation networks that shape the global economy. As a result, big data applications become computationally intensive and benefit from high-performance computing. The project advances knowledge and utility of Evo-Con models in the context of massive amounts of complex, dynamic and geographically distributed data. In lockstep with these developments, the investigator will develop ""Statistical Learning Goes Viral"", a free-access MOOC (Massive Open Online Course), with concomitant free-access expository textbook and further expand his efforts building bridges between UCLA and key historically black colleges and universities.<br/><br/>The investigator will combine theory, methods and computing in a way that facilitates high-impact data analysis and easy measurement of success. The PI will (1) develop a class of nonlinear and multivariate phylogenetic Hawkes processes that use autoregressive neural networks to maintain both flexibility and scalability. Nonlinear phylogenetic Hawkes process development will capitalize on experience building heavily hierarchical models for joint data, but a pragmatic approach will depart from previous Bayesian implementations to enhance scalability and prediction; and (2) adapt these stochastic process models to complex transportation patterns in a way that responds to spatial data precision by combining nonlinear dimension reduction with convolutional neural networks (CNN). When travel networks explicitly present themselves, the PI will leverage experience building spatial models that account for network structures by incorporating nonlinearities through graph CNNs. More radical within phylogeography, the PI will eschew explicit network representations with the help of spherical CNNs that build implicit representations of global spatial dependencies to model viral contagion with increased flexibility. By hierarchically combining (1) and (2) within the same factor graph, the joint data neural model will fully integrate spatial, genomic, and temporal data. Finally, the investigator will (3) construct high-performance computing techniques that leverage conventional and quantum resources to fit complex, multimodal and high-dimensional model geometries. Computational developments will go well beyond track-record inventions of parallelized Markov chain Monte Carlo algorithms that use graphics processing units and quantum computers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2237322","CAREER: Towards Tight Guarantees of Markov Chain Sampling Algorithms in High Dimensional Statistical Inference","DMS","STATISTICS","07/01/2023","01/20/2023","Yuansi Chen","NC","Duke University","Continuing Grant","Yulia Gel","06/30/2028","$84,817.00","","yuansi.chen@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","025Z, 1045, 1269","$0.00","Drawing samples from a distribution is a core computational challenge in fields such as Bayesian statistics, machine learning, statistical physics, and many other areas involving stochastic models. Among all methods, Markov Chain Monte Carlo (MCMC) algorithms stand out as the most widely used class of sampling algorithms with a broad range of applications, notably in high dimensional Bayesian inference. While MCMC algorithms have been proposed, studied, and implemented since the foundational work of Metropolis et al. in 1953, many convergence properties of algorithms used in practice are not well understood. Practitioners in Bayesian statistics are often faced with a series of key challenges to be addressed rigorously: the choice of algorithm hyper-parameters, the estimated computational cost and the choice of the best algorithm, etc. This project focuses on developing theoretical guarantees of MCMC sampling algorithms that arise in large-scale Bayesian statistical inference problems.<br/>The project will also offer numerous interdisciplinary research training, outreach and mentoring opportunities for the next generation of statisticians and data scientists at all levels, from undergraduate to doctoral students.<br/><br/>This project will address three specific research problems centered around MCMC algorithms in high dimensional inference. First, the project intends to rigorously rank the efficiency of MCMC algorithms for sampling log-concave distributions and to provide succinct non-asymptotic mini-max analysis of mixing time. Log-concave distributions in sampling are as important as convex functions in optimization, and one cannot expect to build a foundational theory basis without determining the fundamental limits of sampling algorithms on log-concave distributions. Widely-used algorithms such as Hamiltonian Monte Carlo, Gibbs sampling and hit-and-run will be studied rigorously. Second, as concentration inequalities constitute an essential component in understanding the efficiency of MCMC sampling algorithms, the project will develop a fine-grained understanding of concentration of high dimensional log-concave distributions via new technical tools such as stochastic localization. Finally, the project will unify the existing theoretical tools for studying discrete-state and continuous-state sampling algorithms through localization schemes. The proposed research aims to advance the field with a comprehensive understanding of MCMC sampling algorithms and their optimal settings in both discrete and continuous cases. The project will provide a wide range of interdisciplinary initiatives to enhance professional development of undergraduate and graduate students in statistical sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2238656","CAREER: New Challenges in Statistical Genetics: Mendelian Randomization, Integrated Omics and General Methodology","DMS","STATISTICS","05/01/2023","02/16/2023","Jingshu Wang","IL","University of Chicago","Continuing Grant","Yong Zeng","04/30/2028","$58,527.00","","jingshuw@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","079Z, 1045","$0.00","With the rapid development of genetic technologies and the continuing collection of large-scale biobanks, scientists are provided with unprecedented opportunities to predict, prevent, and treat common diseases in a personalized and efficient way. In the meantime, analyzing such data presents many new challenges as 1) data come from multiple sources and can suffer from various biases and confounding; 2) scientific questions need an understanding of not only associations but also causal relationships among different risk factors and diseases. This project will address a range of statistical challenges in performing the integration of different omics data types to elucidate potential genetic changes that lead to disease development or relate to the discovery of treatment targets. The project will bridge statistics, machine learning, genetics, and medical research from an analytical perspective. In addition to helping young generations develop independent thinking, the educational activities will help them develop the ability to form objective opinions on social events and to analyze data to form an unbiased judgment on news stories. The PI will develop software and share research results on social media that can be useful to scientists and clinicians, doctors, and industrial professionals. This project also supports graduate students in the research. <br/><br/>In the project, the PI plans to develop three aspects of research for modern statistical genetics. For Mendelian Randomization, which uses genetic mutations as natural experiments to understand risk factors for disease progression, the PI will focus specifically on adjusting for confounding and evaluating the temporal causal effects of clinical risk factors with new frameworks. For analyzing gene regulation with single-cell multi-omics data, the PI will work on a widespread regulation mechanism called alternative polyadenylation, building new statistical models to understand its functional roles with data from new technologies such as spatial transcriptomics and single-cell CRISPR screens. Furthermore, the PI will also investigate new statistical ideas motivated by recent methodological developments in genetics that can help to solve general problems in hypotheses testing and Bayesian inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232547","Conference: Advances in Statistical and Computational Methods for Analysis of Biomedical, Genetic, and Omics Data","DMS","STATISTICS","01/01/2023","08/08/2022","Swati Biswas","TX","University of Texas at Dallas","Standard Grant","Yong Zeng","12/31/2023","$31,650.00","Pankaj Choudhary","Swati.Biswas@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1269","7556","$0.00","A three-day international conference titled ""Advances in Statistical and Computational Methods for Analysis of Biomedical, Genetic, and Omics Data"" will be held at the University of Texas at Dallas from March 17-19, 2023. The conference will bring together diverse groups of students, junior researchers, and leading researchers from academia, industry, and government in a collaborative setting to discuss cutting-edge statistical and computational methods for health-related research. The scientific program will include plenary and invited talks, and a poster session. In addition to its core mission of contributing towards the advancement of science, this conference will provide opportunities for students and young researchers to engage in robust interaction and networking with senior researchers. Thus, the conference will also contribute toward building a diverse human capital skilled in statistics and data science for health-related research.<br/><br/>The theme of the conference is timely, especially in today's era of big data. Massive amounts of a variety of biological and medical data are now available and continue to be collected. Information gleaned from them can power breakthroughs leading to advances in not only personalized medicine but also innovative drug designs and evidence-based medical decision support systems that can potentially revolutionize healthcare. However, a key challenge is the development of new statistical and computational methods that will allow researchers to dive deep into such data and derive actionable information from them. The conference will contribute toward fostering the development of newer and better tools for filtering useful information from large, complex, and noisy datasets, helping pave the way for future advances leading to improved healthcare. More details about the conference can be found at https://sites.google.com/view/abgod2023/home.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2239448","CAREER: Statistical Learning with Recursive Partitioning: Algorithms, Accuracy, and Applications","DMS","STATISTICS","06/01/2023","12/28/2022","Jason Klusowski","NJ","Princeton University","Continuing Grant","Yulia Gel","05/31/2028","$84,472.00","","jason.klusowski@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","1045","$0.00","As data-driven technologies continue to be adopted and deployed in high-stakes decision-making environments, the need for fast, interpretable algorithms has never been more important. As one such candidate, it has become increasingly common to use decision trees, a hierarchically organized data structure, for building a predictive or causal model. This trend is spurred by the appealing connection between decision trees and rule-based decision-making, particularly in clinical, legal, or business contexts, as the tree structure mimics the sequential way a human user may think and reason, thereby facilitating human-machine interaction. To make them fast to compute, decision trees are popularly constructed with an algorithm called recursive partitioning, in which the decision nodes of the tree are learned from the data in a greedy, top-down manner. The overarching goal of this project is to develop a precise understanding of the strengths and limitations of decision trees based on recursive partitioning, and, in doing so, gain insights on how to improve their performance in practice. In addition to this impact, high-school, undergraduate, and graduate research assistants will be vertically integrated and benefit both academically and professionally. Innovative curricula, workshops, and data and methods competitions involving students, academics, and industry professionals will facilitate outreach and encourage participation from a broad audience. <br/><br/>This proposal aims to provide a comprehensive study of the statistical properties of greedy recursive partitioning algorithms for training decision trees, as is demonstrated in two fundamental contexts. The first thrust of the project will develop a theoretical framework for the analysis of oblique decision trees, where, in contrast to conventional axis-aligned splits involving only a single covariate, the splits at each decision node occur at linear combinations of the covariates. While this methodology has garnered significant attention from the computer science and optimization communities since the mid-80s, the advantages they offer over their axis-aligned counterparts remain only empirically justified, and explanations for their success are largely based on heuristics. Filling this long-standing gap between theory and practice, the PI will investigate how oblique regression trees, constructed by recursively minimizing squared error, can adapt to a rich class of regression models consisting of linear combinations of ridge functions. This provides a quantitative baseline for a statistician to compare and contrast decision trees with other less interpretable methods, such as projection pursuit regression and neural networks, that target similar model forms. Crucially, to address the combinatorial complexity of finding the optimal splitting hyperplane at each decision node, the PI?s framework can accommodate many existing computational tools in the literature. A major component of the research is derived from connections between recursive partitioning and sequential greedy approximation algorithms for convex optimization problems (e.g., orthogonal greedy algorithms).  The second thrust focuses on the delicate pointwise properties of axis-aligned recursive partitioning, with implications for heterogeneous causal effect estimation, where accurate pointwise estimates over the entire support of the covariates are essential for valid inference (e.g., testing hypotheses and constructing confidence intervals). Motivated by simple setting where decision trees provably fail to achieve optimal performance, the PI will investigate how the signal-to-noise ratio affects the quality of pointwise estimation. While the focus is on causal effect estimation directly using decision trees, the PI will also investigate implications for multi-step semi-parametric settings, where preliminary unknown functions (e.g., propensity scores) are estimated with machine learning tools, as well as conditional quantile regression, both of which require estimators with high pointwise accuracy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."

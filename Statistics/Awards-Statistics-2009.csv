"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"0907678","Quasi-Likelihood of Models:  Modified Profile Likelihood for Model Selection","DMS","STATISTICS","07/01/2009","06/30/2009","Heping He","KS","University of Kansas Center for Research Inc","Standard Grant","Gabor Szekely","06/30/2012","$120,000.00","","hhe@math.ku.edu","2385 IRVING HILL RD","LAWRENCE","KS","660457552","7858643441","MPS","1269","0000, 6890, 9150, OTHR","$120,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).  <br/><br/>Most of statistical inferences are  based on statistical models for data, so model selection plays a fundamental role in statistical inferences. There is huge amount of literature to develop model selection theory and methodologies such as AIC, BIC, bootstrap criteria, cross-validation criteria and so on. However, model selection is still an ``unsolved'' problem in the sense that there are no magic procedures to get the best model. The goal of this proposal is to develop quasi-likelihood functions of candidate models as a very accurate and natural model selection criterion. Note that the quasi-likelihood functions here are functions of models themselves instead of parameters in the models. Motivated by the modified profile likelihoods (MPLs), the investigator treats those parameters in each candidate model as nuisance parameters, and the models themselves as the values of the ``parameter'' of interest,  to develop the quasi-likelihood functions of candidate models. The selected model is then the one maximizing the quasi-likelihood of models. Some simulations have shown that the proposed MPL works very well for the selection of error probability laws in location-scale models. The MPL of models has also been obtained for composite transformation models. The investigator will then develop the quasi-likelihood function of models to select variables and error probability laws in regressions and study its theoretical properties. The investigator will also develop the quasi-likelihood of models in exponential family, study its theoretical properties justifying its good performances expected in simulations and applications, and explore to apply it to regular models. Other than these, the investigator may go further to develop the quasi-likelihood to select the number of change points in the change point problems, and to select the order of AR or ARMA time series models. The investigator will also compare the proposed quasi-likelihood function with AIC, BIC, and so on to see its advantages. The investigator may study the other model selection problems in statistics and the other disciplines and carry out some practical and important applications.<br/><br/>Model selection is one of the fundamental tasks of scientific inquiry. The proposed quasi-likelihood of models provides a novel, very natural, universal and extraordinarily good way to select models. This novel model selection criterion would be a significant progress in solving the ``unsolved'' model selection problems in statistics and the other disciplines. Since model selection problems exist arguably in almost every discipline, the proposed quasi-likelihood of models can be broadly used in various disciplines such as statistics, signal processing, econometrics, medicine, biology, computer sciences, communication, engineering, physics and even quantitative chemistry. The investigator will collaborate with the other disciplines to solve some of their real problems."
"0906341","Studies in Measurement Error Problems","DMS","STATISTICS","07/01/2009","07/02/2009","YANYUAN MA","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","06/30/2012","$165,046.00","","yzm63@psu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, 6890, OTHR","$165,046.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The principal investigator (P.I.) will study three topics in measurement error models, develop relevant methodologies and analyze their corresponding properties and performances. <br/><br/>The first topic concerns functional models in the situation when the main model contains unspecified error, hence is a semiparametric model by itself. The P.I. will study the structure and interaction of the two nonparametric components, the unknown error distribution and the unknown latent variable distribution, and propose an operation to best treat each of them. The general approach is geometry based. The resulting estimating procedure possesses robustness to model misspecification in both components, and allows to achieve optimal estimation efficiency. The asymptotic consistency and normality will be demonstrated both theoretically and in numerical examples.<br/>The second topic concerns the model goodness-of-fit test in measurement error models. The P.I. will propose a pseudo-score type methodology.<br/>She will demonstrate that the new testing procedure is feasible in accommodating the computational issues specific in such models, and has the desired consistency and power property. In addition, the optimal power property associated with the usual profiling estimation procedure can be equivalently achieved via projection. She will also study the relation between the Wald test and the pseudo-score test and demonstrate their equivalence in a wider range than previously known in literature.<br/>The third topic concerns the small sample performance in measurement error models. Existing literature has indicated that the first order asymptotics in measurement error models often require very large sample size to show its relevancy. The P.I. will tackle this problem using a saddle point approximation technique, hence achieving a higher order approximation than the classical first order theory. Because the functional measurement error model is semiparametric, yet existing saddle point approximation theory is developed and heavily relies on parametric model assumption, the P.I. will develop and study new methodology in this area.<br/><br/>The series of projects in this proposal will resolve some of the most fundamental issues in their most general form in measurement error models. Since errors in measurements widely present in almost all scientific fields, including health and medicine, environment and atmospheric science, finance and economics, material and chemical sciences, the new methodologies will generate wide interest and have important application in these fields. They will also provoke further studies and development in related semiparametric problems and computing methods in statistical sciences itself."
"0906252","Generalized Linear Models for Large Correlation Matrices Via Partial Autocorrelations","DMS","STATISTICS","07/15/2009","07/07/2009","Mohsen Pourahmadi","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","06/30/2012","$195,000.00","","pourahm@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, 6890, OTHR","$195,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>This research will focus on developing statistical models for large correlation matrices in the spirit of the generalized linear models using the partial correlation as the new unconstrained parameters. In particular, computationally efficient procedures will be developed for simulating random correlation matrices which are of great interest in simulation testing of new statistical methods and data mining algorithms, digital signal processing, and working correlation matrices in the analysis of longitudinal data. Large correlation matrices arise quite often in business and economics, epidemiology, environmental monitoring, biotechnology and spectroscopy where modern technological innovations have made it possible to collect massive amount of data with relatively low cost. The three major difficulties in modeling and simulating correlation matrices are (i) the positive-definiteness constraint, (ii) the high-dimensionality and (iii) the additional constraint that its diagonal entries must equal to one. While the Cholesky decomposition and other techniques can handle (i) and (ii), they are unable to handle (iii). The proposed research intends to reparameterize a correlation matrix in an unconstrained and statistically interpretable manner using the basic concept of partial correlation.  Consequently, sparse and flexible statistical models, data analytic and graphical tools for correlation matrices will be developed in analogy with those commonly used in regression and time series analysis. The methods and tools to be employed include: the theory of generalized linear models, time series analysis, numerical linear algebra, theory of orthogonal polynomials and the Monte Carlo methods.  <br/><br/>The proposed work has the potential of elevating the basic concept of partial autocorrelation as a bona fide tool for modeling standard multivariate data in a manner similar to its well-established role in time series analysis, signal processing, the theory of orthogonal polynomials and graphical models. It has the added feature of connecting these apparently disparate areas, which brings out the interdisciplinary nature of the work. The focus on high-dimensional data analysis has immediate impacts on settings where large amounts of multivariate data are collected. Important examples of such settings are financial markets, environmental monitoring and global change, biotechnology and manufacturing. Graduate students will be involved in various phases of the project, the results will be incorporated in courses and presented in seminars and workshops accessible to researchers outside the field of statistics."
"0902303","Conference on Statistical Methods for Complex Data","DMS","STATISTICS","03/01/2009","03/09/2009","Jianhua Huang","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","02/28/2010","$10,000.00","Xihong Lin","jianhua@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, 7556, OTHR","$0.00","A conference on the current statistical issues that involve large and complex data will be held at the Texas A&M University in College Station, Texas, on March 13th and 14th, 2009. The conference will focus on nonparametric and semiparametric methods, methods to analyze functional data, measurement error and inverse problems, and statistical application in biology, genetics and population sciences. The conference will provide statisticians and scientists a venue for stimulating interdisciplinary interactions. It will address emerging statistical issues and help shape future directions of methodological research on complex data. Funds from NSF will be primarily used to defray the travel costs of young researchers and graduate students.<br/><br/>The purpose of this conference is to bring together top and junior researchers to define and expand the research frontiers of methodological research on complex data. The workshop provides a focal venue for top and junior researchers to gather, interact and present their new research findings, to discuss and outline emerging problems in their fields, and to lay the groundwork for fruitful future collaborations. Moreover, it will provide an overview for graduate students who are interested in seeking for an interesting area to work. The workshop focuses on several topics crucial in modeling and analyzing complex data. It will be very helpful for advancing statistical research that addresses problems of high societal impact."
"0905730","""Nonparametric Estimation with Applications to Large and Complex Survey Data""","DMS","STATISTICS","07/01/2009","07/02/2009","Lily Wang","GA","University of Georgia Research Foundation Inc","Standard Grant","Gabor Szekely","06/30/2013","$100,200.00","","lwang41@gmu.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, 6890, OTHR","$100,200.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>One difficulty facing today's survey statisticians is the increasingly complex structures of surveys. The U.S. is very well provided with various sorts of longitudinal surveys which have considerable advantages over widely used cross-sectional data for capturing dynamic demographic relationships. It is desirable to make inferences from these complex surveys as model-free as they can be. Nonparametric statistics is a flexible and promising tool that properly reflects complex design structures. However, the simultaneous consideration of detection of survey errors with high dimensionality, smoothing and the additional complexity emerging from complex correlation structures presents great challenges in nonparametric survey analysis. The investigator works on novel nonparametric model-assisted methods for large and complex surveys, including longitudinal surveys, via incorporation of ""cheap"" auxiliary information. The current project includes (1) developing finer and more intelligent nonparametric tools for survey sampling; (2) investigating nonparametric survey methodology in the presence of nonsampling errors, such as nonresponse and measurement errors;<br/>(3) exploring new procedures and novel theory in longitudinal survey analysis.<br/><br/>The field of survey research is undergoing profound and rapid changes brought on by larger societal, technological, and theoretical developments. With large complex surveys in many research areas becoming increasingly available for public use, the theory and practice in this proposal can serve as an important tool for survey practitioners, (bio)statisticians, epidemiologists, economists, sociologists, and other researchers.<br/>The proposed methodologies will significantly enrich the techniques of longitudinal survey modeling and broaden the traditional understanding of survey sampling. The proposed research will also strengthen the U.S. federal statistical system by providing survey researchers from several federal agencies (including Census Bureau, the National Center for Health Statistics, the Bureau of Justice Statistics, and the Bureau of Labor Statistics) modern and advanced methods in survey methodology.<br/>"
"0905731","""G-SELC: A New Global Optimization Technique Using Genetic Algorithms, Tabu Search and Gaussian Processes""","DMS","STATISTICS","07/01/2009","06/24/2009","Abhyuday Mandal","GA","University of Georgia Research Foundation Inc","Standard Grant","Gabor Szekely","06/30/2013","$100,000.00","","amandal@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, 6890, OTHR","$100,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>In this work, the investigator develops a new global optimization technique, which is primarily motivated by applications in drug discovery. Although identification of useful compounds is critical to improving efficiency in drug discovery, pharmaceutical industries generally adopt ad hoc approaches to identify promising compounds. The proposed research aims to develop an efficient technique named G-SELC, which expedites this process. To this end, the investigator develops a global optimization procedure using local search techniques such as Genetic Algorithms and Tabu Search combined with statistical modeling involving Gaussian processes. This research is also extended to categorical variables in Gaussian process. In addition, the investigator develops efficient numerical techniques to reduce computational burden for this batch sequential optimization problem.<br/><br/>Identifying promising compounds from a vast collection of feasible compounds is an important and yet challenging problem in pharmaceutical industry. The proposed research helps reduce the expenditure at the early stages of drug discovery, thus creating significant economic and social benefits. This optimization technique is also used to identify optimal solutions in many other scientific research problems such as computer experiments, functional magnetic resonance imaging and nanotechnology. The investigator shows that in some applications, categorical variables can be treated as continuous. This simplifies the computation in Gaussian process modeling significantly. It has far-reaching consequences not only in drug discovery, but also in complex computer modeling where Gaussian process modeling is used extensively which includes modeling air quality, calibration of computational models of cerebral blood flow, predicting climate and weather, statistical mechanics of granular flow, terrestrial models, dynamics of infectious diseases and so on.<br/>"
"0907170","Statistical Methods for Complex Functional Data","DMS","STATISTICS","07/15/2009","07/10/2009","Lan Zhou","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","06/30/2013","$262,106.00","Jianhua Huang","lzhou@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, 6890, OTHR","$262,106.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>To meet the rising need for analyzing functional data with complex structures, the investigators develop innovative statistical methods under three broad categories: (1) Multi-level functional data analysis. New methods for multi-level principal components analysis, multi-level clustering and classification will be developed. New methods will also be developed for fitting functional mixed effects models --- a very flexible class of models for functional data. (2) Correlated functional data. New methods will be developed for modeling time series of curves and spatially correlated curves. (3) Two-way functional data. A new principal components analysis is developed for two-way functional data, where both index domains of the data matrix are structured. In these projects, the structures of the functional data vary from case to case, but the common challenge is to deal with the covariance kernel of a random functional object. The main strategy is to reduce dimension through functional principal components. In addition, an alternative regularization strategy is also investigated based on shrinkage to simple structures. Penalized splines are used for estimating the principal components functions. By looking into functional data with complex structures, the research has significant potential to advance the knowledge of statistics.<br/><br/>Functional data are data that can be represented as a collection of curves or functions. Examples of functional data include, but not limited to, a patient's vital signs over time, a digitized image, geographical data and demographic data. As automated measuring systems make data collection easy, functional data become more prevalent with increasingly complex structure. Our research is motivated by analyzing functional data arising from studies on colon physiology and colon cancer, studies of US ethnic diversity dynamics and business operational management. The statistical methodology innovations proposed are widely applicable in various fields that involve functional data, such as environment and global change, health and medicine, etc. The success of the proposed research will benefit people with deeper understanding of functional data."
"0904177","Development of Composite Likelihood Method in High-Dimensional Correlated Data Analysis: Estimation, Inference and Model Selection","DMS","STATISTICS","08/01/2009","07/23/2009","Peter Song","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","07/31/2012","$149,845.00","","pxsong@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, 6890, OTHR","$149,845.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111&#8208;5).<br/><br/>Recent advances in computing and measurement technologies have given subject-matter scientists opportunities to develop large scale experiments and ambitious information collection schemes that have led to various types of high-dimensional correlated data. This proposal focuses on the development of statistical theory and methods of composite likelihood for analyzing such high-dimensional correlated data. In particular, the principle investigator plans to achieve three research goals: To develop a new algorithm analogous to the EM algorithm in the context of composite likelihood theory for the analysis of incomplete high-dimensional data; to develop a new model selection criterion analogous to the Bayesian Information Criterion for the scenario where the number of model parameters may increase in the sample size; and to develop a new procedure to evaluate and boost the efficiency loss due to dimension reduction in the composite likelihood methodology.<br/><br/>All the developed methods will be applied to the analysis of data from practical studies to facilitate the understanding of subject-matter sciences and ultimately to improve human knowledge and quality of life. The principle investigator has close connections with researchers in other fields such as Biology, Computer Science, Epidemiology, Health and Medical Sciences at University of Michigan. He has been working closely with these scientists who will serve as local users of the methodologies and provide valuable feedback. The project is also devoted to substantial educational initiatives that will involve undergraduate and graduate students and expose them to state-of-the-art research in various interdisciplinary topics related to the proposed research. These include new courses, short courses at major conferences, summer workshops, mentoring, and software development. These and other dissemination activities will increase awareness of modern powerful methods for data analysis among scientists from other fields."
"0907014","Efficient estimation in semiparametric regression with possibly incomplete data","DMS","STATISTICS","07/01/2009","06/30/2009","Ursula Mueller-Harknett","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","08/31/2012","$113,928.00","","uschi@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, 6890, OTHR","$113,928.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>In the context of missing data and semiparametric regression models (i.e., models with both finite dimensional and infinite-dimensional parameters), little work has been done on efficient estimation and still less on estimating general functionals. Most studies limit their attention to estimating the mean response. In contrast, this research project studies estimation of arbitrary expectations involving response and covariables. The investigator will also address estimating densities and distribution functions. The focus is on efficient estimation in semiparametric regression with responses missing at random. The analysis of semiparametric models is an important topic with practical, real-world implications: in applications there is typically some information about the structure of the data available, but not sufficient to specify an appropriate parametric model; semiparametric methods make optimal use of that information. However, even simple (widespread) semiparametric models, such as the partly linear model, are not yet fully understood. This research will further our understanding. Most of the anticipated results will also apply to cases where data are complete. The first research strand has the goal of deriving efficient estimators of expectations of covariates and the response variable in semiparametric regression. A second strand focuses on estimation of the response density in the nonlinear regression model. The investigator intends to show that, for certain classes of well-behaved regression functions, the response density can be estimated with a root n rate and, moreover, efficiently. It is not anticipated that it will always be possible to estimate the density with the parametric rate root n: limitations and possible alternative approaches will be investigated. The key methodological innovation in these two strands is the combination of full imputation, efficiency and empirical likelihood ideas. The third strand considers estimation of the error distribution function in nonparametric regression with missing responses.<br/><br/>Many scientific investigations depend upon statistical analysis to draw conclusions. In many cases, however, incomplete data present a challenge to the accuracy of those conclusions. This applies in many fields, including epidemiology, pharmaceutical research and social/behavioral investigations involving the analysis of survey data. The results of this research project will enable data sets with missing values to be treated more efficiently and improve the accuracy of statistical conclusions about the data. Despite significant recent progress, inefficient methods remain in frequent use. Examples include listwise deletion of cases, and imputation methods which do not use all the available information about the data. Deleting or disregarding unique or scarce data is clearly not a desirable option. Efficient analysis will make use of all available information about the structure of the data, leading to unbiased, least-dispersed estimation methods: in other words, greater accuracy."
"0906532","Nonstationary spatial-temporal covariance models for multivariate processes on a globe","DMS","STATISTICS","08/01/2009","08/02/2009","Mikyoung Jun","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","07/31/2012","$75,000.00","","mjun@central.uh.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, 6890, OTHR","$75,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>This project studies classes of spatial-temporal covariance functions for nonstationary, multivariate processes on a globe.  Many processes in geophysical and environmental problems these days take large portion of the Earth as their spatial domain and exhibit strong spatial nonstationarity (particularly with respect to latitude). Moreover, it is common to have multiple variables of interest, such as relationship between precipitation and temperature. However, there are not many spatial-temporal covariance functions that can deal with nonstationary processes on a globe and there are almost none developed so far for multivariate problems. In this regard, the investigator develops a flexible class of spatial-temporal covariance functions for univariate as well as multivariate processes on a globe. The idea of applying differential operators with respect to latitude, longitude, and time to an isotropic process on a globe is explored. The coefficients of the operators, varying over latitude, allow flexible nonstationary covariance models for univariate process. It also helps to create a rich class of cross covariance models suitable for real physical processes. The ultimate goal of this project is to build a joint statistical model for multiple climate model outputs. It has been demonstrated that these numerical climate models have correlated errors and it is critical to have flexible cross covariance models to accurately model the dependence structure among different climate model errors. There are several interesting computational issues that arise from this application. In particular, the investigator studies algorithms for fast computation of inverse of covariance matrix with special structures, covariance tapering, likelihood approximation, and missing data imputation method.<br/><br/>  <br/>This study is motivated by the scientific problem of evaluation and integration of multiple climate model outputs. Under the coordination of the Intergovernmental Panel on Climate Change (IPCC), various organizations over the world are developing numerical climate models and the cost to develop and run these models are enormous. However, it is common to simply take averages of these models and assume they are independent. In addition to the contribution to the field of statistics, the proposed study will provide a useful tool for climate scientists for inter-comparison of multiple climate models. Moreover, it will help climate scientists to improve their understanding of past, current, and future climate by accurately integrating multiple climate models beyond simple averages and allow them to achieve more precise uncertainty in their predictions. The results of the proposed research will be formed as multidisciplinary courses for students of both statistics and atmospheric scientists. The investigator anticipates that this type of effort will boost future collaboration between statisticians and atmospheric scientists.  <br/><br/><br/>"
"0836057","Support of the Committee on Applied and Theoretical Statistics","DMS","STATISTICS","09/01/2009","08/27/2009","Michelle Schwalbe","DC","National Academy of Sciences","Standard Grant","Gabor Szekely","08/31/2011","$90,000.00","","mschwalbe@nas.edu","2101 CONSTITUTION AVE NW","WASHINGTON","DC","204180007","2023342254","MPS","1269","0000, OTHR","$0.00","The National Academies' Committee on Applied and Theoretical Statistics(CATS) is a primary interface between the research enterprise and federal agencies that rely on the statistical sciences.  CATS organizes workshops and studies that provide objective and authoritative advice on how best to apply the tools of statistics, information analysis, and uncertainty management to practical problems of national importance.  In so doing, CATS strengthens the policymaking process; increases the visibility of, and appreciation for, the statistical sciences; and identifies growth areas for the discipline.<br/><br/>In recent years, CATS helped initiate and steer a major Academies evaluation of the nation's forensic science enterprise, which led to a 2009 report that raised a large number of concerns and recommended a comprehensive set of federal actions.  Late in 2008, CATS co-sponsored a cross-disciplinary workshop to help reduce the uncertainties associated with remotely sensed climate data; a report on that workshop is forthcoming.  In August of 2009, CATS is co-sponsoring a workshop on technical and policy challenges facing large-scale integration of scientific data, which will also lead to an Academies report.  CATS is currently working to initiate studies to assist the National Security Agency and the National Nuclear Security Administration in meeting their goals.<br/>"
"0906661","New Developments in Estimation, Selection and Applications for Mixed Models","DMS","STATISTICS","07/01/2009","12/08/2010","Tao Huang","VA","University of Virginia Main Campus","Standard Grant","Gabor Szekely","06/30/2012","$116,497.00","","th8e@virginia.edu","1001 EMMET ST N","CHARLOTTESVILLE","VA","229034833","4349244270","MPS","1269","0000, 6890, OTHR","$116,497.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).  <br/><br/>Correlated data are seen in diverse fields of sciences and humanities, ranging from computational biology and geology to health and social studies. However, modern correlated data frequently present additional complications such as high-dimensionality, nonlinearity and nongaussianity, for which more complex models are needed.  Often times, these models require a large number of parameters, and the number of the parameters can increase with the sample size and can even be greater than the sample size.  These pose significant challenges on both theoretical and computational fronts, as it is difficult to directly apply traditional likelihood techniques.  This proposal aims to develop innovative statistical procedures and efficient computing algorithms for analyzing correlated data with complicated features using mixed models. The investigator focuses on three classes of mixed models: linear mixed models, nonlinear mixed models and generalized mixed models, and extends the concept of partial consistency and the nonconcave penalized least squares method to address several challenging issues in mixed model estimation and selection.  In particular, the investigator 1) explores the concept of partial consistency in linear mixed models and develops a simple yet robust two-step estimation method; 2) develops penalized least squares methods to select fixed effects as well as the covariance and precision matrices of random effects; 3) formulates the nonlinear mixed model estimation and testing problems as model selection problems and develops a group selection method for nonlinear mixed models; 4) extends the proposed penalized least squares method to ultra-high dimensional variable selection; and 5) generalizes the proposed two-step estimation method and penalized least squares method to generalized linear mixed models. <br/><br/>The research findings of this proposal will greatly broaden the applications of mixed models, especially in jointly modeling different types of data.  For example, one can jointly model clinical and genomics data in a unified way where genomics data such as gene expressions are treated as random effects and explain the heterogeneity among groups while clinical data such as age, gender and blood pressure are treated as fixed effects and are of primary interest.  Such a joint modeling approach allows one to account for the correlation among genes and to study genes or genetic pathways in a system way rather than traditional gene-by-gene way.  Moreover, the research findings of this proposal will also shred light on analyzing high-dimensional and massive data.  For example, by extending the concept of partial consistency to mixed models, one will have better understandings as how to explore the unique structure of high-dimensional data in order to extract valuable information to produce consistent, efficient and robust estimates for some parameters. In addition, the proposed methodologies will be introduced to researchers in other areas through interdisciplinary collaboration work, and will also be integrated into the investigator's educational activities by developing graduate and undergraduate curriculums and by training graduate students.  Open source R and Matlab codes implementing the proposed methodologies will be made available to general public."
"0907297","Adaptive Designs and Sequential Monitoring","DMS","STATISTICS","09/01/2009","08/25/2009","Feifang Hu","VA","University of Virginia Main Campus","Standard Grant","Gabor Szekely","08/31/2013","$130,001.00","","feifang@gwu.edu","1001 EMMET ST N","CHARLOTTESVILLE","VA","229034833","4349244270","MPS","1269","0000, OTHR","$0.00","Clinical trials are complicated and usually involve multiple objectives such as controlling type I error, increasing power of detecting treatment difference, assigning more patients to better treatment, and more. In literature, both adaptive designs (by changing design procedure sequentially) and sequential monitoring (by changing analysis procedure sequentially) have been proposed to achieve these objectives to some degree. In this project, the investigator combines these two sequential procedures and studies the advantages of sequential monitoring response-adaptive randomized clinical trial. The investigator first derives the asymptotic distribution of the sequential test statistics of the combined procedure. Based on the asymptotic properties, the investigator selects appropriate boundaries for the combined procedure to achieve multiple objectives. Further, he investigates the implementation of the combined procedure to some real clinical trials.<br/><br/>Clinical trials usually involve multiple competing objectives such as detecting clinical difference among treatments, minimizing total cost and protecting more people from possibly inferior treatments. Adaptive designs dynamically use sequentially accruing data in decisions for collecting future data in order to achieve these objectives. In conducting clinical trials, sequential monitoring is a standard technique to balance the ethical and financial advantages of stopping a trial early against the risk of an incorrect conclusion. This proposal is concerned with combining adaptive designs and sequential monitoring. The investigator will first investigate sequential monitoring of a response-adaptive randomized clinical trial and will then study the advantages of this combined procedure. Upon completion of this project, one can apply sequential monitoring to an adaptive randomized clinical trial to achieve multiple objectives. The research projects will produce advanced statistical tools for analyzing data which is  sequentially collected. These tools may be applied in many fields including drug development, medical studies, industrial experiments, economics and finance."
"0905400","Topics in Nonlinear and Functional Time Series","DMS","STATISTICS","09/15/2009","09/13/2009","Lajos Horvath","UT","University of Utah","Standard Grant","Gabor Szekely","08/31/2013","$250,000.00","Alexander Aue","horvath@math.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","MPS","1269","0000, OTHR","$0.00","The investigators examine statistical inference techniques for nonlinear and functional time series models that work in both stationary and nonstationary regimes, thereby providing unified procedures that help advance statistical theory. Since determining whether a given set of (functional) observations is stationary can be a vexing problem, the proposed research also helps to lighten the statistical analysis for practitioners. Moreover, the investigators develop a new framework to deal with dependent Hilbert space-valued random functions which is both mathematically challenging and important for statistical applications in various areas such as finance, econometrics, astronomy, geophysics, climatology and genetics. This research is based on delicately fusing elements of statistical and probability theory with time series and functional data analysis.<br/><br/>The investigators' research is aimed at providing flexible statistical tools for practitioners that are less sensitive to underlying model assumptions and time dependent changes in environment. In view of the economic crisis in the Fall of 2008, this seems to be of particular importance for the analysis of financial data, but may also prove relevant in other scientific fields such as climatology. To enhance our understanding of these complex scientific questions, the investigators' research will provide novel data-analytic tools for practitioners by advancing statistical theory."
"0906665","Feature and Structure Identification and Variable Selection for Functional, Longitudinal and Cross-sectional Data","DMS","STATISTICS","06/15/2009","06/05/2009","Jianhui Zhou","VA","University of Virginia Main Campus","Standard Grant","Gabor Szekely","05/31/2012","$109,650.00","","jz9p@virginia.edu","1001 EMMET ST N","CHARLOTTESVILLE","VA","229034833","4349244270","MPS","1269","0000, 6890, OTHR","$109,650.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>In the functional linear regression model, the investigator proposes a feature identification procedure to identify the null intervals of the functional coefficients and to estimate the functional coefficients on the non-null intervals. This procedure can be considered as variable selection and dimension reduction in the functional setting. In the generalized linear model for longitudinal data, the investigator proposes a structure identification procedure to select the correlation structure for clustered data. This procedure does not require a likelihood function, is not restricted by cluster size while most existing methods for longitudinal data suffer from large cluster size, and can be extended to spatial statistics and gene networks. The proposed estimators for identifying features and structures are expected to enjoy the Oracle property. To account for heterogeneity in cross-sectional data, the investigator proposes the varying-coefficient index models, including most varying-coefficient models in the current literature as special cases. Variable selection is proposed in this general setting by utilizing B-spline approximations to the nonparametric components and adopting a conceptually simple optimization of canonical correlation. The proposal is expected to inherit nice properties from dimension reduction techniques.<br/><br/>The investigator's proposal is motivated by an aging study, and has the extension to geosciences, health study, and bioinformatics for addressing important scientific questions. The investigator's work aims to broaden the applicability of functional regression models and varying-coefficient index models, and to enable better handling of time-dependent regression analysis in a variety of applications. The proposed work is among the first in methodology development with a solid asymptotic theory to address the investigated problems. The investigator has the plan to provide free software packages to academic and industrial users of the proposed procedures. The support of the proposed research helps the investigator in the training of the students at the University of Virginia, where few senior faculty members are available to supervise a larger number of graduate students, including female and minority students."
"0906616","Structured classification and regression","DMS","STATISTICS","07/01/2009","06/16/2009","Xiaotong Shen","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","06/30/2012","$355,221.00","","xshen@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, 6890, OTHR","$355,221.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>The proposed project aims to develop new statistical theory and methodology for high-dimensional structured data. The project is inspired by challenging problems that arise in two important biological applications: disease gene identification and gene function discovery, where one central issue is how to utilize problem structure effectively to deal with high statistical uncertainty in a discovery process. The project consists of two major components: subnetwork analysis and structured learning. With regard to subnetwork analysis, the PI and his collaborators will develop new techniques for extracting a certain low-dimensional subnetwork structure, where a network is described by a directed or an undirected graph. With regard to structured learning, the PI and his collaborators will develop new large margin techniques for partial multi-label hierarchical classification, with particular effort focused on accurate prediction under hierarchical constraints and various hierarchical loss functions. The goal is to achieve a substantial improvement on predictive accuracy over the current best techniques. In addition, computational tools will be developed to target real problems and to provide optimal or near-optimal solutions. <br/><br/>   The proposed project will address fundamentally important issues in structured data analysis. It will generate research interest for studying emerging problems, and will promote collaborations between statisticians and scientists from other fields such as computer science and biomedical science. The research program will have an impact in several areas of research, particularly in document management and exploration, automatic machine processing, biomedical research, and social science. The educational program will integrate teaching with research to get students exposed to state-of-the-art research, and to create an interdisciplinary learning environment for training and learning."
"0846068","CAREER:  New Statistical Methodology and Theory for Mining High-Dimensional Data","DMS","STATISTICS","08/01/2009","05/15/2013","Hui Zou","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","07/31/2015","$400,000.00","","hzou@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, 1045, 1187, OTHR","$0.00","The area of high-dimensional modeling is developing rapidly. This research aims to push these developments forward to meet new challenges arising in different fields. In particular, the investigator studies (a). new statistical methodology and theory for mapping high-dimensional datasets onto a space with much-lower dimensions while assuring minimum distortion; (b). efficient and robust variable selection in semiparametic models; (c). a novel regularization approach to nonparametric model selection and estimation.<br/><br/><br/>Modern computing power and scientific innovations allow scientists to easily collect high-dimensional data in various disciplines. Analysis of high-dimensional data poses many challenges and offers great opportunities to statisticians. The availability of high-dimensional data has reshaped statistical modeling. This proposal focuses on new statistical methodology and theory for knowledge discovery and information retrieval from high-dimensional data. The investigator plans to develop User-friendly computer programs for public use. The research will make significant contributions to areas outside statistics as well, including biology, computer science, biomedical engineering, medical informatics, economics, and so on. The integrated educational program includes substantial initiatives that will involve undergraduate and graduate students and expose them to state-of-the-art research in the topics related to the proposal. These include new courses, workshops and mentoring. The research results will be integrated into K-12 education and be applied to industrial research.<br/><br/><br/>"
"0905763","Analysis of Functional Time Series","DMS","STATISTICS","09/15/2009","09/13/2009","Rong Chen","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","08/31/2014","$149,884.00","","rongchen@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","In this project the investigator develops new models, methods and associated theory under a general framework of `Functional Time Series Analysis'. Analyzing time series in a functional framework is increasingly practical and is gaining importance rapidly as more and more applications involving such data sets. There are four research projects, in the board directions of functional time series driven by dynamic processes, distributional time series driven by dynamic processes, functional ARMA models, and functional regression models with functional time series errors. They are closely related but with different focuses. The combination of the projects builds a comprehensive framework for functional time series analysis. For each project, statistical properties of the underlying models, statistical inference and predictions under these models, and the theoretical properties of the inference and prediction methods are studied. Several special and important applications are studied.<br/><br/>Functional time series analysis can be viewed as a marriage between the traditional time series analysis and the field of functional data analysis of independent functional observations. Time series analysis is mainly interested in the dependent structure of the observations over time, the understanding the dynamic nature of the underlying process and accurate predictions of the future. Modern data collection capability has lead to broader definition of `data' and more and more observations are in the form of functions, images, and distributions. The intersection between time series analysis and functional data analysis has not been systematically explored. In this project, the investigator develops a general framework of functional time series analysis that is amenable to statistical thinking and the analysis of real problems.  This project paves the way for developing a completely new research area in statistics. It has broad impact in advancing our capabilities of statistical data analysis. It aims to produce advanced statistical tools for analyzing functional time series that are encountered in many important application fields including economics and finance, environmental studies, medical and neuroscience, ecology and meteorology. The project also actively engages in activities related to education and research training of graduate and undergraduate students, especially attracting minority and women students into the field of statistics and statistical applications.<br/>"
"0906420","Statistical Methods and Theory in Some High-Dimensional Problems","DMS","STATISTICS","09/01/2009","08/26/2009","Cun-Hui Zhang","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","08/31/2013","$221,627.00","","czhang@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","The research project will focus on developing practical methods, efficient algorithms and solid theory for the selection of important features, estimation of unknown parameters and prediction of responses with high-dimensional data, especially in the case where the number of features is much larger than the number of samples. It will further develop recently proposed methodologies and algorithms for feature selection in linear regression, extend them to more general high-dimensional statistical models, investigate their consistency and optimality properties in selection and estimation. The methodologies developed in the project will be directly relevant to many applications. The project will specifically investigate applications in two important areas. The first one is signal processing, including efficient sampling, representation, transmission and recovery of data objects. The second one is communications networks, including detection and estimation of significant patterns in volume and changes in data streams.  <br/><br/><br/>High-dimensional data is an area of intense current interest in statistical research and practice due to the rapid development of information technologies and their applications to modern scientific experiments. Important fields with an abundance of high-dimensional data include bioinformatics, signal processing, neural imaging, communications networks and more. In many suchscientific and engineering applications, the size of the problem is measured by the number of features: genetic components in bioinformatics, brain regions or voxels in neural imaging, or computers and routers in theInternet. A main challenge in high-dimensional data is that the size of the problem is often much larger than the size of the data to be used. The project is motivated and will be directly applicable to signal processing and monitoring communications networks. Due to mathematical and statistical commonalities of problems involving high-dimensional data, the project will also be directly applicable to bioinformatics, neural imaging and many more disciplines where modern information technologies prosper. Furthermore, the project will have significant educational impact."
"0906773","Robust Multivariate Statistics: Beyond Ellipticity and Affine Equivariance","DMS","STATISTICS","06/01/2009","05/15/2009","David Tyler","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","05/31/2013","$222,207.00","","david.tyler@rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","The concepts of affine equivariance and elliptically symmetric distributions have played a central role in the development of robust multivariate statistical methods over the past 30 years. Statistical methods which are robust over the class of elliptical distributions are more widely applicable than methods based solely on the multivariate normal distribution. The class of elliptical distributions, though, represents only a small class of  multivariate models. Important cases which do not fall within this class are mixture models and independent components models. Affine equivariance is a useful property since methods possessing it behave equally well over different covariance structures. However, there are many cases where interest lies in certain type of covariance structures, e.g. factor analysis models. The investigator's research goals are thus two-fold. First, the investigator is to further develop and study ""invariant coordinate selection."" This is a new multivariate method, recently introduced by the investigator, which is well suited for exploring non-elliptical models. In particular, it can be used to uncover Fisher's linear discriminant subspace for mixture models when the group identifications are unknown, and can be use to uncover the independent components in independent components models. Second, the investigator is to study the properties of certain non-affine equivariant methods, such as orthogonal equivariant M-estimates. The primary goal here is to achieve a better understanding of the type of covariance structures for which such methods may or may not be advantageous.<br/><br/>The need to analyze multivariate data arises in many diverse disciplines, such as computer science, psychology, meteorology, sociology, biology, econometrics and engineering. The primary interest in such data typically is not with an understanding of each variable separately, but rather with the interrelationships among the variables or with unmeasurable ""latent"" variables. Many common methods employed in these areas are based upon the multivariate normal model, which are now well known to perform poorly if the normal model does not hold. In particular, only a few errors in the data or a slight deviation in the model can highly influence the interpretation of an experiment or a data set, sometimes with disastrous consequences. This is particularly problematic with high dimensional data, i.e. data consisting of many variables, since bad data points or deviations from the model can be difficult to detect whenever they are associated not with just one variable but with a number of the variables. Thus, multivariate methods which are not greatly affected by such problems are crucial to a proper analysis of such data. The investigator anticipates that the intended research will have an important impact not only on steering the direction of research within robust statistics, but also on the methodology used within the many disciplines that routinely deal with multivariate data."
"0906569","Mathematical Methods for Approximately Exact Statistical Inference","DMS","STATISTICS","08/15/2009","08/07/2009","John Kolassa","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","08/31/2012","$122,630.00","","kolassa@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","This proposed research applies a variety of mathematical techniques, including multivariate complex analysis and combinatorics, to open questions concerning inference from small samples.  This inference includes standard frequentist techniques including the calculation of p-values and confidence intervals.  The following aims are undertaken: 1. The approximation of DiCiccio and Martin (1993), conditional p-values using an approximate equivalence relation between a Bayesian credible region and a frequentist critical region, are improved, by determining an optimal or near-optimal set of initial conditions for the partial differential equation that is involved in the approximation. 2. Exact-enumeration techniques are applied to conditional inference in Cox regression.  3.  An asymptotic approximation are constructed for the conditional distribution of a likelihood ratio statistic, under the regularity conditions far weaker then generally present in the existent literature.<br/><br/>Experimentalists routinely ask how well their data fits a hypothesis about the mechanism generating their data; the truth or falsehood of this hypothesis routinely is of importance to society as a whole.  For example, in a medical clinical trial, one might investigate how closely data conform to a hypothesis that a new drug is equivalent at treating a particular disease to a standard drug, and in an engineering study, one might investigate how closely the data conform to a hypothesis that a part designed in a new way lasts no longer than a part designed in a conventional way.  Disproving such a hypothesis often leads to adopting an alternative hypothesis that a new drug or part design actually represents an improvement.  Disproving such a hypothesis involves a probabilistic proof by contradiction, in which investigators calculate the probability of observing data representing evidence at least as strong against the initial hypothesis, and reject this hypothesis if this probability is small.  For example, investigators interested in a new design for a low-emission vehicle might compare a new battery design to an old design, in an experiment in which batteries of both types are installed in vehicles and tested under a variety of conditions. In situations like this, the method for quantifying evidence against the hypothesis of equal reliability of both batteries is well established. This research helps to attach probabilities to the various possible outcomes of the experiment under various assumptions about the relative reliabilities of the batteries, accounting for variation in experimental conditions and for various non-battery reasons for vehicle failure. Similar questions arise in medicine, finance, the social sciences, and many other fields of interest."
"0905753","Design and Analysis of Complex Experiments: Branching Factors and Functional Responses","DMS","STATISTICS","08/01/2009","07/31/2009","Ying Hung","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","07/31/2013","$128,096.00","","yhung@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, 6890, OTHR","$128,096.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>Statistical design and analysis of experiments is an effective and commonly used tool in scientific discoveries. The rapid growth in technology and computing power has made available many complex experiments, such as those with branching factors and functional responses. It also poses many new challenges. The primary objective of this proposal is to develop a set of novel and efficient statistical methods to tackle the emerging challenges and thus accelerate discoveries in many disciplines that use experimental investigation. The research plan consists of two parts. The first part of the research focuses on design and analysis of experiments with branching and nested factors. In many complex experiments, some of the factors exist only within the level of another factor. Such factors are often called nested factors. A factor within which other factors are nested is called a branching factor. Design and analysis of experiments with branching and nested factors are crucial in many complex systems and have not received much attention in the literature. In the first part of this proposal, new classes of designs, theory, combinatorial and algorithmic construction strategies, and structured modeling are proposed that can take into account the branching and nested structure in a complex experiment and identify important factors effectively. The second part of the research focuses on the analysis of computer experiments with functional responses. Physical experiments can be expensive and time-consuming; thus, computer experiments have been widely used as economical alternatives. Many computer experiment responses are collected in a functional form. However, literature on modeling computer experiments with functional responses remains scarce as most of the existing modeling techniques focus on single outputs. Although there are some dimension reduction techniques for functional responses, they do not account for an important feature, the deterministic outputs, of computer experiments. To address this issue, a sequential technique is proposed, which provides an interpolating model. It also incorporates a novel iterative procedure and thus enjoys great computational efficiency.<br/><br/>The new class of designs, design theory, combinatorial and algorithmic construction methods, and structured models proposed in this research appears to be the first systematic investigation of experiments with branching and nested factors. They can open new avenues for studying problems that energize both theoretical and applied research. The proposed sequential modeling technique for computer experiments with functional responses takes into account the special features in computer experiments and enjoys great computational efficiency. It is an innovative concept which can lead to new research in functional data analysis. Both methods are readily applicable to a variety of scientific fields, such as electronic packaging, biomechanical engineering design, wildfire control, and influenza modeling."
"0906784","Regularization Methods in High Dimensions with Applications to Functional Data Analysis, Mixed Effects Models and Classification","DMS","STATISTICS","08/01/2009","06/09/2011","Yingying Fan","CA","University of Southern California","Continuing Grant","Gabor Szekely","07/31/2012","$200,826.00","Gareth James","fanyingy@usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","MPS","1269","0000, OTHR","$0.00","Historically statistics has dealt with the problem of extracting as much information as possible from a small data set. However, over the last decade, because of technological advances in various fields such as image processing, computational biology, climatology, economics and finance, one of the most important active research topics in statistics now involves dealing with data sets with enormous numbers of predictors. Such large scale problems may be abstracted as statistical regression and classification problems with the number of explanatory variables much larger than the number of observations. In these situations some form of regularization is essential. The investigators study a general class of penalty functions and the theoretical properties of the resulting regularization methods in regression and classification settings. In addition, two specific penalty functions that each motivate a different methodology are developed. The theoretical and empirical properties of these methods in the most common linear regression setting are investigated. Finally, the investigators study extending the methodologies to areas that are less well explored in the high dimensional setting, namely, mixed effects models, functional linear regression, and classification problems.<br/><br/>The proposed research is expected to have a broad impact on the practice and education, both of statistics, as well as on fields outside statistics. The common theme underlying this entire proposal is that of developing general regularization penalties and related methodologies for high dimensional problems. The investigators together have direct connections in many fields outside statistics such as Computational Biology, Finance, Marketing, Machine Learning, and Econometrics. The investigators will systematically develop software to implement the proposed methods through free software packages, like R, and then make them readily available and publicize them in all these fields. High dimensional data are becoming increasingly common, so the developed methodologies and software will be widely utilized. The research will also contribute to the training and development of future data analysts (including both statisticians and researchers outside statistics who analyze data)."
"0906588","Nonparametric likelihood for dependent data","DMS","STATISTICS","07/01/2009","06/22/2009","Daniel Nordman","IA","Iowa State University","Standard Grant","Gabor Szekely","06/30/2013","$169,982.00","","dnordman@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","0000, 6890, OTHR","$169,982.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The investigator seeks to develop effective nonparametric likelihood and resampling methods for dependent or correlated data structures.  The project particularly targets the development of empirical likelihood methodology for spatial data under nonstandard sampling designs, the investigation of optimal implementation of the bootstrap in non-stationary temporal and spatial settings, and the creation of novel resampling methods through data-transformations aimed at weakening data correlation.  This work intends to produce efficient and accurate statistical methodology for several correlated data structures, which are applicable without stringent assumptions on the underlying data-generating process.<br/><br/>Scientific investigations commonly rely on the statistical analysis of data, which are often correlated in a complex manner.  Current statistical metholodogy depends heavily on selecting probability models to accurately represent the underlying correlation in data.  However, such model selection can be difficult in practice and any inference drawn from a mistaken model may be misleading.  This research targets developing alternative, model-free tools for valid statistical inference with correlated data, which are not susceptible to errors in model choice and can also help advance data inference in scientific areas such as Environmetrics, Economics, Geology, Astronomy, etc.,  that naturally encounter correlated data.<br/>"
"0904181","Collaborative Research: Novel methods for pharmacogenomic data analysis using gene clusters","DMS","STATISTICS","08/15/2009","08/19/2009","Shuangge Ma","CT","Yale University","Standard Grant","Gabor Szekely","07/31/2013","$100,000.00","","shuangge.ma@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>Numerous pharmacogenomic studies have been conducted using microarrays to survey the whole genome and detect disease-associated genes. Genes have the inherent clustering structure. The goal of this study is to develop a systematic framework using principal component analysis (PCA) based methods to detect gene clusters differentially expressed and/or with joint predictive power. More specifically, the investigators will (1) develop novel methodology to detect gene clusters marginally differentially expressed; (2) develop penalization methodology to detect gene clusters with joint predictive power for the disease clinical outcomes of interest; and (3) conduct extensive numerical studies, and develop publicly available software. This study will greatly advance our understanding of the ?large p, small n? statistics as well as human genomics. Methodologies developed in this study can be applied in other areas including image processing, immunology, molecular dynamics, small-angle scattering, and information retrieval.<br/><br/>Identification of genomic markers from analysis of pharmacogenomic data is a key step in understanding human genomics and personalized medicine. The proposed study has been motivated by the urgent need to overcome drawbacks of existing methods. It will feature novel statistical methods, rigorous theoretical development, extensive numerical studies, development of public software, and a direct impact on practical studies. The proposed study will enrich the family of high dimensional methodologies in general. In addition, analysis of breast cancer, colon cancer, and lymphoma microarray data will lead to a deeper understanding of the genomic mechanisms underlying those cancers. From educational and social prospective, the proposed study will foster more intensive collaborations among investigators from different institutions and background. It will promote teaching, training and learning at Yale University and at the University of North Carolina. Moreover, the investigators will attend statistical and genomic conferences and give presentations, which may promote interdisciplinary research among scientists from diverse fields.<br/><br/>"
"0906864","Statistical methods for space-time processes, time-frequency methodologies, and applications","DMS","STATISTICS","08/15/2009","08/08/2009","Peter Craigmile","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","Gabor Szekely","08/31/2012","$120,000.00","","pfc@stat.osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","This project considers space-time models and time-frequency methods for the analysis of space-time data observed continuously (and often sparsely) in space, but discretely (and regular) in time.  This research extends the spatially-dependent filtering approach used to define space-time processes to include space-time long memory processes, non-Gaussian processes, and non-linear processes. Since methods need to be developed for these types of statistical models that are efficient to use, part of this research focuses on statistical inference.  A secondary study involves spectral and wavelet methods for the analysis of space-time processes.  A spectral analysis is used to explore features of a statistical process in the frequency domain in terms of a linear combination of complex exponentials (sinusoids).  A wavelet analysis provides a space/time-scale (approximately a space/time-frequency) decomposition of a statistical process in terms of averages and changes of averages over different temporal or spatial scales.  Developing methods of<br/>spectral- and wavelet-based exploratory data analysis and inference are of key interest.<br/><br/>There is a growing need in many scientific areas to be able to understand phenomena that vary jointly across space and in time. Statistical methods are required in practice because these phenomena are observed in the presence of uncertainty.  For example, Paleooclimatology (the history or ""archaeology"" of climate) involves obtaining surrogate measures for climatic variables over space that are valid over long time scales.  Important scientific questions can be answered by relating data obtained from paleoclimatology to drivers of climate variability.  The use of space-time statistical models and spectral and wavelet-based space-time analyses can inform how different temporal scales affect the climate relationships observed, and to understand how these relationships vary spatially.  This research is directly applicable to other scientific areas, and results will be communicated via peer-reviewed articles in subject-matter as well as statistical areas.  A diverse cross-section of students (statistical and non-statistical) will be mentored in methods of time series analysis and spatial statistics (via supervision and teaching).<br/><br/>"
"0907522","Collaborative Research: New MCMC-Enabled Bayesian Methods for Complex Data and Computer Models Applied in Astronomy","DMS","STATISTICS","07/15/2009","08/24/2011","Yaming Yu","CA","University of California-Irvine","Standard Grant","Gabor Szekely","06/30/2013","$473,029.00","Yaming Yu","yamingy@uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1269","0000, 6890, OTHR","$473,029.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/><br/>The California-Boston AstroStatistics Collaboration is developing a new model-based strategy for statistical inference that embeds computer models into multilevel models that explicitly account for complexities of both astronomical sources and the data generation mechanisms inherent in new high-tech telescopes. The resulting highly structured models must be fully utilized in order to learn about the underlying astronomical and physical processes. This strategy requires state-of-the-art scientific computation, advanced methods for statistical inference, and careful model checking procedures. The Collaboration has a track record using these methods to solve outstanding data-analytic problems in astronomy. In addition, the PIs (van Dyk, Meng, and Yu) have substantial research experience in developing the methods that the Collaboration will extend, employ, and publicize: inferential and efficient computational methods under highly-structured models that involve multiple levels of latent variables and incomplete data. Such models are ideally suited to account for the many physical and instrumental filters of the data generation mechanism in high-energy astrophysics. The five astronomers (Chiang, Connors, Kashyap, Kelly, and Siemiginowska) all have expertise on the instrumentation and science of high-energy and/or optical astronomy, and, all have collaborated with statisticians in efforts to develop appropriate methods to address scientific questions. The collaboration specifically aims to develop a mixture of parametrized and flexible multi-scale models that can be combined with complex computer-models to describe spectral, spatial, and timing data, either marginally or jointly. The models are developed in a fully Bayesian framework that allows us to incorporate external information, provide coherent estimates of uncertainty, and calibrate statistical comparisons of proposed underlying physical models. These methods require the Collaboration to develop new sophisticated statistical computing techniques for Monte Carlo exploration of complex and often multi-modal posterior distributions.<br/><br/>In recent years, technological advances have dramatically increased the quality and quantity of data available to astronomers. Newly launched or soon-to-be launched space-based telescopes are tailored to data-collection challenges associated with specific scientific goals. These instruments provide massive new surveys resulting in new catalogs containing terabytes of data, high resolution spectrography and imaging across the electromagnetic spectrum, and incredibly detailed movies of dynamic and explosive processes in the solar atmosphere. The spectrum of new instruments is helping scientists make impressive strides in our understanding of the physical universe, but at the same time generating massive data analysis challenges for scientists who study the resulting data. The complexity of the instruments, the complexity of the astronomical sources, and the complexity of the scientific questions leads to many subtle inference problem that require sophisticated statistical tools. For example, data are partially missing, are subject to varying measurement errors, and are contaminated with irrelevant artifacts. Scientists wish to draw conclusions as to the physical environment and structure of the source, the processes and laws which govern the birth and death of planets, stars, and galaxies, and ultimately the structure and evolution of the universe. Sophisticated astrophysics-based computer-models are used along with complex mathematical models to predict the data observed from astronomical sources and populations of sources. The California-Boston AstroStatistics Collaboration aims to tackle outstanding statistical problems generated in astrophysics by establishing frameworks for the analysis of complex data using state-of-the-art statistical, astronomical, and computer models. In so doing the Collaboration will not only develop new methods for astronomy but will also use these problems as a spring board in the development of new general statistical methods, especially in signal processing, multilevel modeling, computer modeling, and computational statistics."
"0906929","Information-Theoretical Methods in Statistics","DMS","STATISTICS","08/01/2009","07/26/2009","Zsolt Talata","KS","University of Kansas Center for Research Inc","Standard Grant","Gabor Szekely","07/31/2013","$110,000.00","","talata@math.ku.edu","2385 IRVING HILL RD","LAWRENCE","KS","660457552","7858643441","MPS","1269","0000, 9150, OTHR","$0.00","The investigator studies statistical model selection problems using information criteria approach. The research concentrates on two related problems: statistical estimation of context trees and statistical estimation of basic neighborhoods of Markov random fields. Statistical estimation of context trees means estimating the memory structure of a discrete-time and finitely-valued stochastic process from a realization of finite length. As the memory length may depend on the actual past, the memory structure can be represented by a tree graph. In the literature, the memory length is usually assumed to be bounded. Then the process is a Markov chain, but the context tree model provides a more efficient description of the memory structure. The research focuses on dropping the finite memory assumption, therefore arbitrary stationary ergodic processes are considered. Model selection methods using information criteria are studied to estimate context trees, using information theoretical, statistical and probability tools. The notion of strong consistency is generalized and new statistical aspects are considered. Markov random fields can be regarded as generalizations of Markov chains to higher dimensional index sets. Statistical estimation of basic neighborhoods of Markov random fields means estimating the interaction structure of finitely valued random variables on an integer lattice, from a realization in a finite region. The basic neighborhood is the smallest region around a site that affects the distribution at the site. The research focuses on modified information criteria to develop consistent estimators of the basic neighborhood. In both of the investigated areas, computational complexity aspects and applications are also considered.<br/><br/>Context tree models are used in statistics, information theory, bioinformatics and various other disciplines. Specific areas of applications include lossless data compression, universal prediction of individual sequences and genetics to model DNA and protein sequences. Markov random fields are special Gibbs fields, therefore they provide essential models in statistical physics for modeling interactive particle systems. They are also used in several other fields, including image processing and pattern recognition. The project aims at two directions. On one hand, it aims to achieve progress in the theory of model selection methods using information criteria, in particular, in context tree estimation and in estimation of basic neighborhoods of Markov random fields. On the other hand, it aims to bring the theoretical achievements to the fields of applications. The research includes collaborative work with researchers in the area, and it also involves the potential of broadening the applications of the results in collaboration with researchers from the areas of applications, namely, from bioinformatics and engineering. A goal is to involve graduate students to the research."
"0906631","Adaptive Markov Chain Monte Carlo methods","DMS","STATISTICS","08/01/2009","07/31/2009","Yves Atchade","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","07/31/2012","$99,556.00","","atchade@bu.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, 6890, OTHR","$99,556.00","This award is funded under the American Recovery and Reinvestment Act of<br/>2009 (Public Law 111-5).<br/><br/><br/>Markov Chain Monte Carlo (MCMC) is a flexible computational technique that<br/>has proven very useful to many scientific disciplines and is the backbone<br/>of current implementations of Bayesian inference. Recent research<br/>developments suggest that the use of adaptive methods can make Monte Carlo<br/>algorithms considerably more effective. This research proposal has two<br/>major components. The first part will contribute to the development of a<br/>limit theory for adaptive MCMC algorithms.  More Specifically, the PI will<br/>develop a resolvent-based martingale approximation technique to investigate<br/>the central limit theorem and the asymptotic variance estimation for<br/>various adaptive MCMC algorithms. The second part of this research activity<br/>will develop a new MCMC algorithm for the Bayesian analysis of statistical<br/>models with intractable normalizing constants, a topic that currently poses<br/>major computational challenges. This part of the research is driven by the<br/>protein design problem in computational biology but the same problem also<br/>frequently occurs in many other statistical models including Markov random<br/>fields, Markov point processes.<br/><br/><br/>Markov Chain Monte Carlo is a well-established Monte Carlo technique for<br/>sampling probability distributions. The method is used widely to solve<br/>substantive problems in many areas of applications. It is especially useful<br/>in situations with high-dimensional data, which is increasingly common in<br/>science and engineering research as well as applications. Thus, the<br/>research developments from this project can have a significant impact on<br/>methods for doing statistical inference in these areas. Through its<br/>theoretical component, this research project will advance the general<br/>understanding and strengthen the use of adaptive Monte Carlo methods in<br/>practice. In the course of developing the asymptotic theory, the PI will<br/>develop original extensions to some well-established probabilistic tools.<br/>The research is also proposing a new algorithm to tackle one of the most<br/>challenging current problem in Monte Carlo simulation; sampling from the<br/>posterior distribution of statistical models with intractable normalizing<br/>constants."
"0855596","The First Institute of Mathematical Statistics Asia Pacific Rim Meetings","DMS","STATISTICS","04/01/2009","03/20/2009","Runze Li","PA","Pennsylvania State Univ University Park","Standard Grant","Gabor Szekely","03/31/2010","$8,000.00","","rzli@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, 7556, OTHR","$0.00","The Principal Investigator of this project is the scientific committee co-chair for the inauguration meeting of a new Institute of Mathematical Statistics Asia Pacific Rim meeting (IMS-APRM) series. This international conference covers all of the most active research topics in statistics and probability. The focuses of this conference are emerging challenging statistic and probability problems, and data-analytic techniques to address scientific questions of disciplines such as computational biology, financial engineering and industrial engineering, medical research and social sciences. A distinguished feature of this international conference is that it addresses of emerging statistical and probability problems that arise from various scientific fields. It will be extremely useful to shape the future directions of statistical research. The aim of this project is to seek funding for junior researchers and graduate students in USA to participate this conference.<br/><br/>The goal of this conference series is to bring together top and junior researchers and foster new collaboration among researchers in Asia and Pacific Rim. The conference provides a focal venue for the senior and junior researchers in both statistics and probability to gather, interact and present their new research findings, to discuss and outline emerging problems in their fields, and lay the ground work for fruitful future collaborations, in particular, the collaborations among the young generations.  This conference is designated to serves advanced graduate students and young researchers looking for new topics to work on, and experienced researchers who hope to gain an overview of contemporary developments in statistics and probability. In selection of the invited speakers, special attention was paid to strike a subtle balance between gender, ethnicity as well as other under-presented groups. This conference focuses on various challenging problems in the frontier of statistics and probability. It is very helpful for shaping the future directions of statistical research that addresses problems of high societal impact."
"0905772","Proportional Hazards Model for Various Types of Censored Survival Data with Longitudinal Covariates","DMS","STATISTICS","09/01/2009","08/26/2009","Jian-Jian Ren","FL","The University of Central Florida Board of Trustees","Standard Grant","Gabor Szekely","03/31/2012","$180,000.00","","jjren@umd.edu","4000 CENTRAL FLORIDA BLVD","ORLANDO","FL","328168005","4078230387","MPS","1269","0000, OTHR","$0.00","The interrelationships between time-to-event (survival time) variable and longitudinal covariates is often the primary research interest in medical and epidemiological studies. Due to the challenges encountered in some important clinical trials on AIDS and cancer research, recently the statisticians started modeling survival data and longitudinal data jointly via Cox's proportional hazards model. Such a joint modeling procedure or methodology has broad applications in many scientific research fields, but it is a considerably difficult problem due to censoring on the survival time and that the covariate process is only observed at some given time points. Up to now, statistical methods on this topic have not been fully or well developed, while the importance and needs for developing these methods have become more evident when the proposer and her collaborators recently encountered some more complicated problems which have not been studied in statistical literature; see examples listed below. Specifically, there have not been any modeling procedures that directly study the relationship between survival time and within-subject historic patterns of change in longitudinal covariates, nor have there been any works on joint modeling doubly censored or interval censored survival data together with (intensive or multi-phase intensive) longitudinal covariates, which is far more challenging than right censored data problem. In fact, there have been no published works on the Cox model with doubly censored data, not even for the case with time-independent covariates. In this research, asymptotic methods and simulations will be mainly used in the studies, and the issues under consideration include: (a) derivation of the empirical likelihood based MLE for the Cox model with longitudinal covariates for right censored, doubly censored and interval censored survival data, respectively; (b) computation algorithms for the MLE; (c) asymptotic properties of the MLE; (d) Wilk's theorem for the MLE; (e) goodness-of-fit tests for the Cox model; (f) comparison with alternative methods. At least two Ph.D. students of the proposer will be involved in and benefit from the proposed research.<br/> <br/>The new statistical methodology to be developed in this project has direct impact to medical research, epidemiology, social and behavioral sciences, etc. For instance, the data examples which we have encountered and motivate the research of this project include the following problems on joint modeling survival time and longitudinal covariates. In a prostate cancer study on mice, part of  the research focus is joint modeling interval censored survival time and longitudinal covariates. In a smoking cessation study, the research focus is joint modeling right censored survival time and intensive longitudinal covariates. In a recent study of child development, the research focus is joint modeling doubly censored survival time and multi-phase intensive longitudinal covariates."
"0906429","North American Meeting of New Researchers in Statistics and Probability","DMS","STATISTICS","03/01/2009","03/06/2009","Tracy Bergemann","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","02/28/2010","$24,300.00","","berge319@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, 7556, OTHR","$0.00","This proposal describes plans for the Twelfth Meeting of New Researchers in Statistics and Probability, a conference series sponsored by the Institute of Mathematical Statistics, to be organized by and held for junior researchers.  The primary objective is to provide a much needed venue for interaction among new researchers.  In contrast with large meetings, this conference will be restricted in size, with a target of 85 participants.  Sessions will be followed by panel discussions and breaks to facilitate<br/>interactions.<br/><br/>The proposed conference will take place July 28 -- July 31,  2009 at the Johns Hopkins University, Baltimore, and the National Cancer Institute, to immediately precede the annual Joint Statistical Meetings in Washington DC. Housing, meals, and conference facilities will be provided on campus.<br/><br/>Participants will be statisticians and probabilists that have received their Ph.D. within the past five years or are expecting to receive their degree within the next year.  Each participant will present their research.  Topics will range across a variety of areas in statistics from theory and methods to applications.  Senior speakers will discuss topics of particular interest to new researchers.  Panel discussions during the conference will cover the topics of journal publications, funding, career development, and mentoring.<br/><br/>The intellectual merit of this proposal is based on its human resources development.  The professional development of new researchers is stimulated by promoting their interaction and creating networks of colleagues. Participants present their work in a smaller, more controlled conference environment, maximizing their intellectual interaction and growth.  Women,<br/>minorities, and the disabled are explicitly encouraged to attend, and we have an established track record of attracting participants from these categories.<br/>"
"0854975","FRG: Collaborative Research: Statistical Inference for High-Dimensional Data: Theory, Methodology and Applications","DMS","STATISTICS","08/01/2009","03/18/2013","Huibin Zhou","CT","Yale University","Continuing Grant","Gabor Szekely","07/31/2013","$330,000.00","","huibin.zhou@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","0000, 1616, OTHR","$0.00","The analysis of high-dimensional data sets now commonly arising in scientific investigations poses many statistical challenges not present in smaller scale studies. Extracting information with precision from such data is becoming ever more important. This FRG proposal is the PIs' unified effort to respond to the pressing scientific needs. Specifically, The goals are to develop a comprehensive theoretical framework and general methodologies for estimating a large covariance matrix and its functionals and for functional data regression where the predictors and/or the responses involve functional measurements, and to address a wide range of important applications in biomedical studies. <br/><br/>The statistical and scientific objectives outlined in this proposal are at the intellectual center of a rapidly growing field in statistics and biostatistics. The new technical tools, inference procedures, and computing algorithms for analyzing high-dimensional data will greatly facilitate scientific investigations in a wide range of disciplines, These fields include astronomy, biology, chemistry, bioinformatics, and particularly in medicine. The proposed efficient analytical procedures hold great potential in deriving more accurate prediction rules for clinical outcomes based on new biological and genetic markers and thus may lead to a better understanding of disease processes. Research results from this proposal will be disseminated through the workshops and seminar series such that the methods would be publicly available to researchers in other disciplines. Software tools developed will be made freely and publicly available as open source code. The proposed project will also bring high-quality training to students and postdoctoral researchers."
"0845368","CAREER: New Statistical Methods for Massive Spatial, Temporal and Spatial-Temporal Processes","DMS","STATISTICS","07/01/2009","03/18/2014","Yongtao Guan","CT","Yale University","Standard Grant","Gabor Szekely","06/30/2014","$400,000.00","","yguan@miami.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","0000, 1045, 1187, 6890, OTHR","$400,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>Dimension reduction plays an essential role in reducing the complexity of data so that the most useful information in data can be successfully extracted. Most existing dimension reduction methods are developed under the assumption that the data are independent. Consequently, they may be inefficient and sometimes even inappropriate for analyzing spatial/temporal data which are often naturally correlated. The proposed research intends to fill in this gap by developing inverse regression based dimension reduction methods for data arising from three different types of spatial/temporal processes: spatial point processes, recurrent event processes and quantitative spatial processes. Specific goals of the project include 1) developing general frameworks and methods for conducting dimension reduction for both univariate and multivariate spatial point processes and 2) generalizing these methods to the cases of recurrent event processes and quantitative spatial processes. Special attentions will be given when the dimension of the response is also high. In addition, the PI will also develop computationally efficient analytical tools such as second-order analysis for the modeling of massive recurrent event process data.<br/><br/>With the fast development of modern data collection technologies, especially with the increased availability of more accurate Global Positioning System and Geographical Information System, large-scale spatial, temporal and spatial-temporal data have become rapidly available in recent years. Many of these data are massive and highly complex in nature, posing unprecedented challenges to data analysis. The proposed research will develop efficient statistical tools that can be used to analyze such data. The PI will collaborate closely with field scientists from various disciplines to apply these tools to solve real-life problems that have motivated this research. Specific goals of these collaborations includes, but are not limited to, 1) improving the understanding of tropical forestry diversity, 2) better assessing the health effects of air pollution on asthmatic children and 3) providing more accurate spatial predictions of US watershed characteristics such as discharges and fluxes. Key educational components of the project include providing interdisciplinary statistical trainings to students especially minority students at both the graduate and undergraduate levels and helping three local high schools improve their AP Statistics teaching."
"0904125","Development of Innovative Statistical Tools in Design of Scientific Experiments and Surveillance Studies","DMS","STATISTICS","08/01/2009","07/25/2012","Sam Hedayat","IL","University of Illinois at Chicago","Standard Grant","Gabor Szekely","07/31/2014","$304,997.00","","hedayat@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","0000, 9179, OTHR","$0.00","Providing flexibility to an experimenter in ensuring efficient and cost-effective data collection methods in pharmaceutical, biotechnological, agricultural and engineering studies have thrown up unprecedented challenges to the design theorists. The investigator develops innovative design-based techniques, in the classical sense as well as in the context of factorial experiments and crossover designs with immediate applications in such areas as biopharmaceutical trials, drug abuse liability trials, and evaluation of new drugs for unwanted effects on electrical properties of the heart, which includes QT/QTc trials in safety assessment. The issues of timely detection and prevention of various types of adverse health events such as drug toxicity, are gaining importance in surveillance related to public health and phase IV clinical trials. The investigator also explores and develops relevant innovative surveillance strategies and examine their relative performance with respect to several well-known optimality criteria and with reference to the role of statistical indicators [such as false alarm rate, delay time for true alarm, in-control average run length]. The investigator examines strategies for determining how frequently and how efficiently the adverse event rate should be monitored in the sense of timely detection and intervention, incurring minimal cost.<br/><br/>The project develops innovative statistical theory and related computational methodologies in resolving some critical design and surveillance issues, not adequately addressed hitherto, covering such areas as pharmaceutical, biotechnological, agricultural and engineering studies. Researchers learn about cutting edge techniques to deal with some real life problems such as(i) the effects of environmental factors on quality of drug substance or shelf life for drug product or storage conditions, (ii) surveillance issues in medical sciences and public health [complicated cases of pregnancies, pandemic influenza, West Nile virus, severe acute respiratory syndrome [SARS], emission of radiation from hazardous pollutants in air / surface / water] and in drug toxicity study in Phase IV clinical trials. Practitioners utilize the findings for more insightful experimentation and data collection techniques in these fields. Advanced Undergraduate and Graduate students are trained in the statistical methods."
"0907371","Conference on Modeling High Frequency Data in Finance; Summer 2009; Hoboken, NJ","DMS","PROBABILITY, STATISTICS","03/15/2009","03/03/2009","Ionut Florescu","NJ","Stevens Institute of Technology","Standard Grant","Tomek Bartoszynski","02/28/2010","$15,000.00","Khaldoun Khashanah, Maria Mariani","ifloresc@stevens.edu","1 CASTLEPOINT ON HUDSON","HOBOKEN","NJ","07030","2012168762","MPS","1263, 1269","0000, 7556, OTHR","$0.00","This award provides funds to support a joint workshop (Stevens Institute of Technology and New Mexico State University) in high frequency data modeling. The main purpose of this workshop is a synergy between Econophysics and high frequency data modeling. Specifically, the focus of the conference is on models for high frequency data and in particular applications of statistics and statistical mechanics to this modeling problem. Furthermore, we are interested in complex systems and system of systems as applying to this problem.<br/>The scientific motivation for the conference arises from the fact that organizing a conference in the trade capital of the world has the potential of bringing together the best mathematicians, practitioners and regulators to help develop and better the modeling aspect of the marketplace.<br/>The main objective of this meeting is to expose today's economic and modeling problems to mathematicians and current graduate students in the hope that this will improve the quality of the research problems studied at the moment of the conference."
"0906545","Bayesian Hierarchical Methods for Modeling Chromosomal Spatial Correlation","DMS","STATISTICS","07/01/2009","06/29/2009","Xinlei Wang","TX","Southern Methodist University","Standard Grant","Gabor Szekely","06/30/2012","$75,000.00","","swang@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1269","0000, 6890, OTHR","$75,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>Recently, many genomic studies have shown that significant chromosomal spatial correlation exists in gene expression of many organisms. Ignoring such correlation in statistical modeling can greatly reduce the efficiency of estimation and the power of statistical inference. In this project, the investigators develop a set of new Bayesian hierarchical models to account for chromosomal spatial correlation in the following three areas of methodologies and applications: (1) incorporating the spatial correlation associated with linear chromosome structures into the analysis of gene expression data; (2) quantifying and inferring chromosome folding structures in vivo using gene expression data; (3) probing the global three dimensional structure of a chromosome in vivo. <br/><br/>This study provides statistical tools to explore three dimensional (3D) chromosome structures and directly addresses the important biological question that how the 3D structures facilitate the coordination in gene transcription. Not only can it generate new biological insights about spatial configuration of chromosomes, but also it can greatly improve the understanding of the relationship between the function and structure of chromosomes in living organisms. It also has potential clinical significance; for example, it provides tools to formally identify and compare chromosomal spatial patterns in gene expression from tumor and normal samples, which can discover subtle but coordinated changes in the tumor genome and lead to clinical insights on the underlying regulatory mechanism of cancer.  Besides scientific novelty and significance, this research fosters intensive collaboration among researchers from various fields including statistics, computational biology, experimental biology and clinical cancer research, and promotes intellectual interactions among participating institutions.<br/><br/>"
"0906398","Computing Environments for Stastistics","DMS","STATISTICS","08/01/2009","05/09/2011","Luke-jon Tierney","IA","University of Iowa","Continuing Grant","Gabor Szekely","07/31/2013","$299,669.00","","luke@stat.uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1269","0000, OTHR","$0.00","<br/>The investigator explores and develops new principles for the design<br/>of statistical software to take advantage of modern computing power.<br/>Particular emphasis is placed on exploring the effective use of<br/>parallel computing, compilation, and code analysis for statistical<br/>languages.  Pilot implementations are incorporated in open source<br/>statistical software systems.<br/><br/>Effective statistical methodology made available through statistical<br/>software is critical to the ability of researchers to make maximal use<br/>of experimental and observational data, and for the ability of<br/>instructors to teach good research practices.  The design principles<br/>developed by this research lead to software that improves the ability<br/>of researchers, instructors, and other users of statistical<br/>methodology to apply this methodology more effectively in scientific<br/>research and teaching and to take full advantage of modern<br/>high-performance computational resources.  These principles also lead<br/>to software frameworks that can be used to more rapidly deliver new<br/>statistical methodology to end users.  Applications in a range of<br/>areas serve as testbeds for methods and principles developed in this<br/>research.<br/>"
"0906400","Travel Support for the Directions in Statistical Computing (DSC) 2009 Meeting; Copenhagen, Denmark; Summer 2009","DMS","STATISTICS","03/01/2009","02/17/2009","Luke-jon Tierney","IA","University of Iowa","Standard Grant","Gabor Szekely","02/28/2011","$8,000.00","","luke@stat.uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1269","0000, 7556, OTHR","$0.00","DSC 2009, an international workshop on Directions in Statistical<br/>Computing, will be held July 13--14, 2009 at the University of<br/>Copenhagen, Denmark.  This is the sixth in a series of meetings;<br/>previous meeting were held in Vienna, Austria (1999, 2001, and 2003),<br/>Seattle, WA (2005), and Auckland, New Zealand (2007). The workshop<br/>will focus on, but is not limited to, open source statistical<br/>computing and aims to provide a platform for exchanging ideas about<br/>developments in statistical computing.  UseR! 2009 will be held July<br/>8--10 in Rennes, France, and is the fifth in a series of conferences<br/>the use and development statistical methods and software related to<br/>the R system.  This proposal is for funding exclusively to provide<br/>support to assist with travel expenses for junior investigators from<br/>US institutions to attend this meeting. Work of young researchers is<br/>often among the most novel, yet often these researchers lack the<br/>travel funds necessary to attend such a conference because they have<br/>not yet established themselves sufficiently to attract external travel<br/>and other funding for their work.<br/><br/>Statistics is playing an increasingly important role in<br/>interdisciplinary research, and statistical computing is essential as<br/>the means for making new methodology available to researchers and<br/>allowing the best statistical practices to be applied effectively.<br/>Scholarly conferences on statistical computing are an essential<br/>framework for the exchange of ideas and for encouraging the<br/>exploration of new directions in the design of statistical software.<br/>International conferences are particularly valuable as they allow the<br/>exchange of ideas among researchers from around the world who would<br/>not ordinarily come into direct contact.  The benefit to young<br/>researchers of a first exposure to the international research<br/>community is particularly important.  International meetings also<br/>provide an opportunity to meet with outstanding international graduate<br/>students and encourage them to consider opportunities for further<br/>study in the United States.<br/>"
"0906421","Stability, Inference, and Weighting in Model Selection","DMS","STATISTICS","09/01/2009","09/17/2012","Dennis Boos","NC","North Carolina State University","Standard Grant","Gabor Szekely","08/31/2013","$200,000.00","Leonard Stefanski","boos@stat.ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","0000, OTHR","$0.00","The first of three components to the proposed research exploits the form of weighted least squares estimators to develop new approaches to regression diagnostics, robust estimation, and variable selection.  The second component addresses the problem of frequentist inference after model averaging, and ""bagging"" in particular. A new ""parallel bootstrap"" resampling approach is proposed for finding variance estimators with total computational effort about twice that of the original bagging step. The third component addresses the problem of determining when model averaging is beneficial.  Model averaging often improves prediction over a base procedure, but not always. Because the improvement in prediction is achieved at the expense of interpretability, it is important to know when model averaging is advantageous and when it is not. Thus a model-stability index that is applicable to general regression models will be developed and studied.<br/><br/>The proposed research addresses well-recognized problems that arise when estimating the relationship between a dependent response variable Y and several predictor variables, e.g., Y = loan failure and X1 = household income, X2 = number of dependents, etc.  The methods and insights developed by this research will be very useful in applications of regression modeling and thus will facilitate research in numerous other disciplines including basic science research and applied research that directly affects quality of life. Results will be disseminated via web and journal publications, conferences, and seminars. The project will also significantly impact human resource development in the guise of education and training of graduate students, some of whom who are likely to come from under-represented groups in light of the NC State Statistics Department's commitment to a diverse graduate program.<br/><br/><br/><br/><br/><br/><br/>"
"0906466","Likelihood Inference in Models with a High-Dimensional Nuisance Parameter","DMS","STATISTICS","09/15/2009","09/15/2009","Thomas Severini","IL","Northwestern University","Standard Grant","Gabor Szekely","08/31/2012","$178,974.00","","severini@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1269","0000, OTHR","$0.00","Likelihood methods, such as maximum likelihood estimation and likelihood ratio tests, play an important role in statistical theory and methodology.  There is a large body of work showing that, under relatively weak conditions,  likelihood-based methods of inference are optimal in large samples.  Such results are based on asymptotic theory in which the dimension of the parameter remains fixed as the sample size increases indefinitely. However, the conclusions based on such a large-sample theory may not be valid for models in which the dimension of the parameter is large relative to the sample size.  Thus, for many models used in practice, standard methods of likelihood-based inference may not perform well.  The goal of this research is to study and develop likelihood-based methods of inference in models in which the dimension of the nuisance parameter is large relative to the sample size. The research will focus on three broad areas: the development of higher-order asymptotic approximations to the distribution of the likelihood-based statistics in models with stratum nuisance parameters,  the development of a small-dispersion asymptotic theory for models with stratum nuisance parameters, and the development of methods of inference in models with an unknown function.  The research will consider the theoretical properties of likelihood-based methods of inference as well as the development of new statistical methodology based on those results.<br/><br/>Statistical methods are used in a wide range of fields.  In particular, likelihood-based methods have been used in in applications ranging from the reliability of computer software to the analysis of genetic data.  Much of current statistical theory is restricted to relatively simple models, in which the available data is large relative to the number of unknown parameters in the model.  However, in complex models, it may be necessary to estimate a large number of parameters based on relatively little data.  This research will develop statistical theory and methodology for this type of model and these results will lead to improved statistical methods that will be useful in many areas of application.  The research will  also further our understanding of the properties of statistical models and, hence, will be useful in the training of researchers in statistics and related fields."
"0906056","Monte Carlo and Quasi-Monte Carlo Methods for Statistics","DMS","STATISTICS, Info Integration & Informatics","09/15/2009","07/19/2012","Art Owen","CA","Stanford University","Continuing Grant","Gabor Szekely","08/31/2014","$659,759.00","","owen@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269, 7364","0000, OTHR","$0.00","This project has two major components.  The first is an investigation of visualization and analysis methods for data sets in high dimensions, with a focus on categorical variables whose number of unique levels is comparable to the total sample size. Examples of such variables include search query strings, ISBNs, song titles, author names, URLs, genotypes, environments, and customer ID numbers.  The visualization methods are designed to show broad trends and to highlight anomalies. The inferential methods are of the sample reuse type: the bootstrap and cross-validation. New methods are necessary here because the data sets have complicated interlocking patterns that invalidate any IID sampling assumptions. The second component is better statistical inference by improving on their numerical methods.  This includes calibration of empirical likelihood methods to get better coverage and to extend confidence regions for the mean beyond the convex hull  of the data points.  It also includes the embedding of quasi-Monte Carlo sampling methods into Markov chain Monte Carlo algorithms to combine the accuracy of the former and the wide applicability of the latter.<br/><br/><br/>Exploratory data analysis of categorical variables is useful to see broad patterns including small groups of customers that have similar tastes for a small list of songs or books or movies.  It is also useful to identify anomalies that may indicate abusive behavior, including cyber-attack, and what is commonly called spam in the online context. One of the original motivations for the sample reuse methods is in crop science.  In some of those problems, a large number of plant varieties (genotypes) are grown under many different environmental conditions.  A statistical model is used to determine which varieties to use in each environment.  Earlier statistical methods were based on assumptions that don't fit this setting and they often did not select the best model.  New methods from this project may therefore be used to select better models which then result in increased production of food and fiber. The empirical likelihood work is basic research aimed at removing unnecessary mathematical assumptions from statistical models in order to widen their applicability. The Monte Carlo sampling component of the project is basic research on a computational technique used extensively in physics as well Bayesian statistical inference.<br/>"
"0906812","High dimensional data: new phenomena and theory in modeling and approximation","DMS","STATISTICS","07/01/2009","02/19/2013","Iain Johnstone","CA","Stanford University","Standard Grant","Gabor Szekely","09/30/2013","$1,205,685.00","David Donoho","imj@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, 6890, OTHR","$1,205,685.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The project studies high dimensional settings exemplified by Linear Regression model selection when there are more potential predictors than observations, and Linear Discriminant Analysis when there are more available features than observations, in both cases assuming that only a small unknown fraction are relevant.  A two-dimensional phase diagram indexes the ratio of number of variables to number of observations as well as a measure of the fraction of relevant variables.  In one region of this diagram, the analysis task can be completed successfully, elsewhere it fails utterly. The investigators propose a four-pronged effort on dimensionality reduction: (1) Phenomenology of Phase Transitions in High-Dimensional Data Analysis: Make structured large-scale computational studies to investigate several such transitions in depth and expose empirical regularities for theoreticians to study.  (2) Theoretical Statistics Supporting High-Dimensional Data Analysis: Proposers have developed `at the physicist's level of rigor' a derivation showing roughly that regression model selection must fail for *any* algorithm above a certain boundary in the phase diagram.  A rigorous proof, planned for this project, will involve the interplay of classical statistical decision theory, random matrix theory, and statistical physics heuristics.  (3) High Dimensional Convex Geometry: A surprising but revealing relationship exists between several of the phase transitions of high-dimensional data analysis and certain key phenomena in high-dimensional convex geometry. The project will further explore these phase transitions and connections.  (4) Inference with Large Random Matrices: A random matrix theory perspective leads to useful new questions and results in classical multivariate analysis that will be pursued in this proposal, with useful connections with the phase transition work expected.  For example, the project will systematically study the distribution of the largest root statistic at ``contiguous'' alternatives in a variety of the standard statistical settings of multivariate analysis, and address related questions such as tail inequalities for the double Wishart model. <br/><br/>Scientific practice in fields ranging from computational biology to image understanding generates ever more datasets in which massive numbers of features are measured per observational unit.  The resulting high-dimensional datasets are often mined for features and associations. In many cases, there are at least as many features as observations.  It has lately become clear that data analysis in this setting offers deep new phenomena of real importance to applications. Two examples -- of many -- include: Linear Regression model selection when there are more potential predictors than observations, but only a small fraction of these are relevant (and which ones aren't known), and Linear Discriminant Analysis when there are more available features than observations, but again only a small unknown fraction are relevant.  In such cases there is a `breakdown' phenomenon, described by a 'phase diagram': a precise relationship between the number of relevant features and the number of observations at which certain procedures for learning from data become impossible.  The new results to be developed about this phenomenon by this project will provide practitioners of high-dimensional data analysis with an improved understanding of the sharp limits to data mining, as well as forging new links between statistical theory and fields like high-dimensional convex geometry and statistical physics."
"0906023","Statistical Aggregation in Massive Data Environments","DMS","STATISTICS","07/15/2009","07/15/2009","Nan Lin","MO","Washington University","Standard Grant","Gabor Szekely","06/30/2012","$119,934.00","","nlin@wustl.edu","ONE BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","1269","0000, OTHR","$0.00","Enormous amount of data are now being generated in many areas. Direct applications of existing statistical methods do not satisfy the computational need for performing on-line analytical processing (OLAP) on such massive data. Computer scientists have developed a data warehouse environment called data cube to reduce computational cost by compressing subsets given by some partitioning variables. Analysis of any subset can then possibly be achieved by aggregating the compressed data, and the computational cost becomes low because of no need to access the raw data. For complicated analyses, it is challenging to find proper compression and aggregation schemes and to study the statistical property of the aggregated analysis. Similar issues exist in another massive data environment, data stream. Existing development in these areas either aims to achieve lossless analysis, which has achieved very limited successes only for simple calculations, or provide no theoretical evaluation for the analysis from aggregation. The purpose of this proposed research is to develop statistically sound compression and aggregation methods for advanced statistical analysis of data cubes and data streams, use the above compression-then-aggregation strategy to improve computational efficiency of some statistical analysis, and develop the associated asymptotic theory.<br/><br/>Massive data sets are common nowadays, and many traditional statistical techniques become inapplicable due to high computational costs. In this proposal, the investigator will extend the current data cube techniques to support more complicated OLAP of massive data sets by studying the statistical properties of the desired analysis. This interdisciplinary project will result in significant contributions to data warehousing, OLAP technology, and statistical computing. It will bring great impacts to important applications in large-scale medical studies, national and homeland security, stream data mining, high-performance computing, and information technology. This project's findings will be broadly disseminated to the academic community and industry through scholarly publications and conferences. We will also use the new findings as new course materials in education and training of information analysts and university students."
"0906593","Estimation, Detection and Control of Multiple Change-point Stochastic Systems with Applications to Economics, Engineering, Biology and Climate Science","DMS","STATISTICS","09/01/2009","08/31/2009","Haipeng Xing","NY","SUNY at Stony Brook","Standard Grant","Gabor Szekely","08/31/2012","$110,000.00","","haipeng.xing@stonybrook.edu","W5510 FRANKS MELVILLE MEMORIAL L","STONY BROOK","NY","117940001","6316329949","MPS","1269","0000, OTHR","$0.00","The problems of parameter estimation, abrupt change detection, and system control in stochastic systems with multiple change-points arise in financial econometrics, industrial quality control, gene mapping and abrupt climate change studies. An important ingredient in the solution to these problems is efficient estimation of piecewise constant parameters with unknown multiple change-points. In the proposed research, the investigator will develop a unified hidden Markov filtering approach to parameter estimation, and bounded complexity approximation algorithms that can be implemented via parallel recursions. In particular, four types of problems from different fields are investigated in the study. The first develops estimation and forecasting procedures for an econometric time series model, in which a generalized autoregressive conditional heteroskedasticity process for conditional variances is intertwined with piecewise constant unconditional variances for which the change-points are unknown. The second studies detection of parameter changes with unknown pre- and post-change distributions in stochastic systems. The third investigates segmentation of piecewise constant signals in sequence data and discusses its applications to the analysis of genomic copy number variation. The fourth, arising in abrupt climate change, develops estimation theory and algorithms for multiple structural changes in the celebrated Lorenz system, which is chaotic. This system provides a highly simplified description of atmospheric circulation and has served as a testbed for methods of parameter estimation in climate studies. The investigator will show how these challenging problems from different areas are unified and solved by the hidden Markov filtering approach to multiple change-points.<br/><br/>Abrupt or gradual parameters changes in complex stochastic systems are often encountered in various scientific and engineering practices including economics, biology and climate studies. While systems with gradually changing parameters have received much attention historically, recent advances in science and engineering show the growing importance of stochastic systems with abrupt, but unknown parameter changes. In economic studies, the authorities and industrial practitioners are interested in dating turning points of economic activities. In current genomic research, DNA copy number variations (i.e., gains or losses of specific chromosomal segments) are key genetic events in the development and progression of numerous diseases including cancer, HIV acquisition, and Alzheimer and Parkinson's disease, and an important step in studying these genetic events is to identify the regions of variations. In climate studies, abrupt changes of Earth's climate have attracted increasing attention from researchers and policy makers due to their projected severe impact on regional and global ecological and economic systems, and new statistical techniques are needed for nonstationary climate or climate-related variables due to the limited values of current statistical tools with a gradually changing assumption. However, the effort of solving these problems is hampered by the statistical and computational complexity caused by unknown abrupt changes. The proposed research presents a unified statistical theory to model and estimate such changes in complex systems, and the applications should shed new light on efforts at tackling these challenging problems. In particular, this study allows us to model abrupt changes of climate systems at the global level by incorporating current state-of-the-art climate modeling techniques with statistical analysis. Furthermore, the application of the proposed research to climate sciences links the statistical theory with high-performance computation, and hence opens up new horizons to statistical methodologies.<br/><br/>"
"0906790","Nonparametric Curve Estimation in the Presence of Nuisance Functions","DMS","STATISTICS","08/01/2009","04/17/2014","Sam Efromovich","TX","University of Texas at Dallas","Continuing Grant","Gabor Szekely","07/31/2015","$345,000.00","","efrom@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1269","0000, OTHR","$0.00","The proposal focuses on developing statistical methodology, theory and methods of adaptive nonparametric curve estimation in the presence of nuisance functions. It is motivated by biological and medical applications. The investigator studies four main classes of considered problems. Each is classified by how an estimator can match the performance of an oracle knowing underlying nuisance functions. They are: (a) Estimator can match oracle. An example is a regression problem with a smooth design density being nuisance function. (b) Estimator can match oracle if complementary observations are available. Examples are deconvolution and regression with measurement errors in predictors where complementary observations are needed to estimate distribution of measurement error. (c) Estimator cannot match oracle. An example is estimation of the density of regression error when a nuisance regression function is not sufficiently smooth. (d) A mixture of the above-formulated settings. It is proposed to develop a general theory of adaptive estimation for the aforementioned classes of statistical problems with particular applications to:  missing, stratified and censored data, hidden components, mixed multivariate models involving continuous and nominal or ordinal categorical variables, and time series. Theoretical results are tested and applied to the analysis of ChIP-on-chip microarrays and ultra-fast fMRI.<br/><br/>The primary focus of the research is to create adaptive statistical procedures which can work in the presence of nuisance functions. This research is motivated by and tested on well-understood applications in the statistical analysis of: (i) ChIP-on-chip microarrays used to find regulatory protein binding sites in a bacterial genome. Interactions between protein and DNA are fundamental to life. They facilitate and mediate gene expression, DNA replication and repair. The proposed statistical analysis of ChIP-on-chip microarrays points on exact location of protein-DNA binding sites. Because the statistical analysis does not require measuring of nuisance functions, it makes microarray experiments cheaper, faster and more accurate. (ii) Ultra-fast functional magnet resonance images, which help in understanding aging and brain diseases such as Alzheimer's and Parkinson's Diseases. Ultra-fast fMRI is an exciting new technology for studying brain functions with the temporal resolution of 50 milliseconds. This resolution sheds light on both neurons and physiological activities in the brain. Proposed statistical analysis, which is robust to nuisance functions, can denoise emodynamic responses, study cognitive functions like memory, speech and emotion, and create a map of physiological activities of the human brain."
"0906734","Bayesian Mixture Models: Unified Theoretical Frameworks and MCMC Methods","DMS","STATISTICS","07/01/2009","06/29/2009","Subharup Guha","MO","University of Missouri-Columbia","Standard Grant","Gabor Szekely","06/30/2013","$150,000.00","","s.guha@ufl.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269","0000, 6890, OTHR","$150,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5)<br/><br/>Different kinds of Bayesian mixture models, such as finite mixtures, finite and infinite hidden Markov models, Dirichlet process models and analysis of densities models, and their methods of inference have traditionally been developed from isolated perspectives.  There have been few attempts to view these models from a common standpoint. For the massive data sets increasingly encountered in real world studies, the inferential techniques for these models often necessitate heavy computational burdens that preclude fully Bayesian solutions. The proposed research will (i) achieve a unification by formulating general classes of mixture models whose special cases are many common mixture models, (ii) explore from a common standpoint important theoretical properties of generalized mixture models, such as posterior consistency, and discover asymptotics that form the basis of broadly applicable and cost-effective inferential strategies, (iii) develop efficient Markov chain Monte Carlo techniques for fitting generalized mixture  models to large datasets, (iv) develop user-friendly statistical software that implement these methods with the goal of disseminating them to researchers, and (v) apply the proposed methods to analyze publicly available, high-throughput Comparative Genomic Hybridization (CGH) data on various kinds of cancer.<br/><br/>Bayesian mixture models are ubiquitous in statistical applications because of their ability to capture real-world complexities through relatively simple constructions. These models have found application in such diverse areas as computer science, epidemiology, economics, finance, forestry, genetics, and marketing. The range of applications of this rapidly developing area has exploded in the last decade. Through its theoretical and methodological components, this research will establish key characteristics shared by disparate classes of mixture models. It will enable the utilization of mixture models in applications where it is currently difficult, if not impossible, to fit the models due to sheer volume of data. The investigator will ensure effective dissemination of the research through open-access software developments, publication in leading journals, application of the proposed methods to the analysis of microarray-based cancer data, and presentation of the results in statistical and subject-matter conferences."
"0907064","Bayesian Optimal Sequential Design for Random Function Estimation","DMS","STATISTICS","07/15/2009","07/02/2009","Marco Ferreira","MO","University of Missouri-Columbia","Standard Grant","Gabor Szekely","06/30/2013","$130,000.00","","marf@vt.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269","0000, 6890, OTHR","$130,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>In this project, the investigator develops computational methods for Bayesian optimal sequential design for the estimation of random functions. Random function estimation, either in the context of Bayesian nonparametric regression or in the analysis of spatial/spatio-temporal processes, has become an ubiquitous tool in most areas of science. While methods for the estimation of random functions are reasonably well developed, optimal design for such problems is in its infancy. This proposal presents a research program that develops a novel computational framework for Bayesian optimal sequential design for random function estimation. This computational framework is based on evolutionary Markov chain Monte Carlo (EMCMC), which combines ideas of genetic or evolutionary algorithms with the power of Markov chain Monte Carlo. This framework is able to consider general models for the observations, such as generalized linear models and scale mixtures of normals. In addition, this methodology easily accommodates multiple covariates and general priors on the space of regression functions based on basis functions such as splines and Gaussian kernels. Finally, this framework allows optimality criteria with general utility functions that may include competing objectives, such as for example minimization of costs, minimization of the distance between true and estimated functions, and minimization of the prediction error.<br/><br/>Estimation of random functions arises in many application areas, such as for example environmental science, epidemiology, climatology, and engineering.<br/>An important example in engineering is the statistical approximation of computer model output, e.g., approximation of fluid flow simulators and rocket booster simulators. Usually, scientists want to run such simulators for many different experimental conditions. However, typically each run of a computer model is extremely expensive and time consuming. An effective way to deal with these resource constraints is to run the simulator for a relatively small number of experimental conditions and to fit a statistical nonparametric model to approximate the output of the simulator. The proposed computational methods allow Bayesian optimal choice of experimental conditions in a sequential fashion, that is, the next experimental conditions are chosen based on what has been learned from the previous experimental conditions. Thus, the proposed methodology for sequential choice of design points will result in huge improvements in cost and efficiency. Another important application of the proposed Bayesian methodology is in dynamic monitoring of spatio-temporal environmental processes. The statistical design problem is to decide where to locate the several stations of a monitoring network. In case some of the monitoring stations are mobile, the proposed methodology leads to an optimally adaptive monitoring network which keeps costs under control while maximizes learning."
"0906392","Exploring and detecting complex multivariate dependencies through sparse graphical models","DMS","STATISTICS","07/15/2009","07/10/2009","Balakanapathy Rajaratnam","CA","Stanford University","Standard Grant","Gabor Szekely","06/30/2011","$103,764.00","","brajaratnam01@gmail.com","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, 6890, OTHR","$103,764.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>The covariance parameter is the natural parameter of interest when exploring complex relationships between many variables in parametric models. Current methodology on high dimensional covariance estimation has focused on regularizing or putting zeros in the covariance matrix or its inverse using methods based on the lasso. Though very useful, these methods do not address some of the glaring gaps in the literature. First it is well known that lasso and similar penalization methods yield sparse models and estimators - yet a formal undertaking of the spectral properties of regularized covariance estimators or those of random matrices that arise naturally in graphical models is not available in the literature. This gap in the literature will be addressed. Second, an important class of models that have recently received much attention are the so-called covariance graph models. These models encode marginal independences in multivariate distributions and thus can yield more parsimonious representations. A comprehensive framework for Bayesian inference and model selection for this class of models is not available. This important class of problems is investigated in this project. One of the original justifications for the need for covariance regularized estimation is that the covariance matrix features in the mean estimation problem, and when constructing confidence intervals for the mean (for instance in MANOVA), or in regression yet there is relatively very little work in the area of covariance regularization required for the specific needs of regression. A generalized framework which investigates the merits of using the covariance matrix of the explanatory variables for regression purposes is undertaken, thereby providing insights into obtaining better estimators for regression coefficients than those suggested by standard methods. <br/><br/>In recent years, the availability of high-throughput data from genomic, finance, environmental, marketing (among other) applications has created an urgent need for methodology and tools for analyzing high-dimensional data. Making sense of all the many complex relationships that are in the data, formulating correct models and developing inferential procedures is one of the major challenges facing statisticians today, and also those working in applied fields. This project proposes to tackle some of the pressing questions that arise when exploring multivariate dependencies in high dimensions. As a concrete application, the methodology developed in this project will be used to understand the interconnectedness of genes in cancer studies and cardiovascular medicine, while maintaining the statistical rigor and ease of interpretability of previously developed methods. Hence a project of this nature will have widespread applications, as understanding relationships between many variables or players is an endeavor that is common to many scientific disciplines. The proposed work, though rooted in the principles of statistics, is interdisciplinary, and involves collaborations with biomedical scientists, engineers and the environmental scientists."
"0906044","Monte Carlo and reconfigurable computing in Bayesian inference","DMS","STATISTICS","08/01/2009","09/20/2011","Wing Hung Wong","CA","Stanford University","Continuing Grant","Gabor Szekely","07/31/2014","$1,012,007.00","","whwong@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","The primary aim of this project is to develop the concepts, methods and algorithms to extract information about the nature of a joint distribution in high dimensional space from samples generated by Monte Carlo algorithms. The approach will be based on the equi-energy sampling approach recently developed by the principle investigator's group under prior NSF support. The secondary aim of this project is to implement Monte Carlo sampling methods based on radically new hardware and computation model, such as those based on field programmable gate arrays. Some of the Monte Carlo methods developed in the primary aim will be implemented using these new architectures, in order to enhance our ability to solve hard inference problem by Monte Carlo computation. This project will provide interdisciplinary training of next generation scientists working at the interface of statistics, computation and biology.<br/><br/>By developing methods to study the shape, topology and entropy for the posterior distribution, this research will provide fundamental tools for Bayesian inference. As such, it will have impact on numerous application areas ranging from computational biology to economic analysis. Beyond Bayesian statistics, this research will also have impact on other scientific areas that utilize Monte Carlo sampling to study a distribution, e.g. in equilibrium statistical physics where one is interested in understanding the energy landscape associated with a Boltzmann distribution.  This project will provide interdisciplinary training of next generation scientists working at the interface of statistics, computation and biology."
"0940077","Statistical Methods from Spectral Analysis with Markov Chains","DMS","STATISTICS","03/01/2009","05/20/2009","Julia Salzman","CA","Stanford University","Standard Grant","Gabor Szekely","07/31/2012","$113,883.00","","julia.salzman@gmail.com","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","Modern science, especially biochemistry, has become dependent on numerical analysis of large amounts of data generated in most every experiment.  Scientific advancement in biology and in understanding disease pathogenesis will likely depend on the analysis of the huge corpus of biomolecular data (eg. microarray, RNA and DNA sequence data). This advancement is linked to the field's ability to continue developing statistical methodologies capable of identifying a robust ``signal'' which can be reproducibly identified in multiple experiments all of which generate noisy data.  The PI has shown how the theoretical framework of spectral analysis with Markov chains unifies several statistical methods for identifying structure in data that is observed with noise: discrete Fourier analysis, correspondence analysis, principle components analysis, as well as spectral clustering.  This unifying framework also provides insight into, and  generalization of, the more traditional methods listed above. Therefore, the PI's proposed research has two major directions.  In one direction, it will continue basic methodological development of exploratory data analysis with a focus on methods capable of identifying biological signals observed in noisy experimental conditions.  In another, it will focus on rigorous statistical analysis of this methodology which is in wide use in statistics, computer science and bioinformatics.<br/><br/>Statistical methods developed here will be particularly aimed at the study of cellular regulation of gene and protein expression.  These cellular mechanisms have wide ranging importance in understanding human disease including cancer and infectious disease.  The data analytic methods developed under this grant will be implemented and made publicly available through Bioconductor, a package in R.  The broad goal of this proposal is to work towards providing a methodological unification of methods in statistics, biology and computer science to biomolecular data.  Thus, it falls roughly into the field of bioinformatics."
"0906801","Whole brain inference and prediction in neuroimaging","DMS","STATISTICS","09/15/2009","09/04/2009","Jonathan Taylor","CA","Stanford University","Standard Grant","Gabor Szekely","08/31/2012","$200,000.00","","jonathan.taylor@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","The investigators in this proposal study two different aspects of noise in neuroimaging data: structure of non-Gaussian random fields and their applications to neuroimaging; along with interpretable predictive modelling of fMRI. The structure of non-Gaussian random fields is expected to shed light on how useful Random Field Theory (RFT), presently used for controlling Family Wise Error Rate (FWER) in neuroimaging studies, can be expected to be for truly non-Gaussian fields. The predictive models proposed by the investigators place much emphasis on interpretability and will allow comparison with the usual approaches based on detecting correlation between experimental stimuli and neuroimaging data while controlling FWER or False Discovery Rate (FDR).<br/><br/>This project is motivated by the need for flexible and valid statistical procedures to interpret neuroimaging data  and to confirm neuroscientific hypotheses derived from previous work. Examples of such neuroimaging data can be found in fields like social neuroscience, which seeks to understand and model human social behaviour based on the activation and interactions of various regions of the human brain; neuroeconomics, which seeks to interpret and model some of the decision-making processes of humans based on models of the human brain; and disease models such as schizophrenia in which the goal is to understand how the brains of schizophrenic patients differ in anatomy and function from those of non-schizophrenic patients. Virtually all neuroimaging data is what is known as ""high-throughput"" which means that huge amounts of data are recorded, typically for only a small group of individuals. Statistical tools are needed to produce consistent and reproducible results from such data. The investigators in this proposal will develop tools that can be used to confirm existing hypotheses about neuroimaging data, as well as generate hypotheses via interpretable predictive models of decision making processes."
"0906394","New Change-point Problems in Genomic Profiling","DMS","STATISTICS","09/01/2009","09/04/2009","Nancy Zhang","CA","Stanford University","Standard Grant","Leland Jameson","08/31/2012","$100,000.00","","nzh@wharton.upenn.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","DNA copy number data, which measures gains and losses of segments of genomes, is an important data type for understanding genetic variation and for clinical research. The analysis of DNA copy number data motivates new statistical problems, especially in the areas of change-point detection and high dimensional data analysis. This proposal identies these problems, formulates statistical models, and proposes methods for their solution.  The topics covered include model selection for irregular high dimensional models, simultaneous change-point detection in a large number of aligned sequences, and segmentation of partially observed sequences.   These developments in statistical methodology are a direct response to the current analysis needs at the Stanford Genome Technology Center and in the Cancer Genome Atlas Project, and open source software will be made available to these and broader communities.<br/><br/><br/><br/>Cancer and other genetic diseases are no stranger to genome scientists: high-throughput technologies and statistical analyses have always promised to provide a systems level?s view of disease inheritance and progression.  In recent years, new concurrent advances in genomics and statistics, including more efficient high throughput data-collection methods, larger patient sample sets, the atmosphere of more open collaboration, and greater sophistication in study design and data analysis have positioned us to make major new advances in studying genetic disease.  Despite this promise, there is still much waiting to be done.  In particular, statistical methods for the analysis of genome-wide profiling data lacks the sophistication to deal with the many issues that arises in modern data collection schemes.  These issues include high dimensionality, missing observations and simultaneous inference in a large number of patient samples.    In this proposal, the investigator and her colleagues formulate these new problems and put forth models with practical solutions.  These developments in statistical methodology are a direct response to the current analysis needs at the Stanford Genome Technology Center and in the Cancer Genome Atlas Project, and open source software will be made available to these and broader communities.<br/>  <br/>"
"0832581","Travel Support for the 57th Session of the International Statistical Institute, August 2009, Durban, South Africa","DMS","STATISTICS","01/01/2009","08/18/2008","Ronald Wasserstein","VA","American Statistical Association","Standard Grant","Gabor Szekely","12/31/2009","$22,788.00","","ron@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","0000, 7556, OTHR","$0.00","The American Statistical Association (ASA) will use this NSF award as a travel grant for United States participants to attend the 57th Session of the International Statistical Institute (ISI) in Durban, South Africa from August 16-22, 2009. The ISI meeting includes the meetings of the Bernoulli Society, International Association for Official Statistics (IAOS), International Association for Statistical Computing (IASC), International Association of Survey Statisticians (IASS), and the International Association for Statistical Education (IASE). Thus it is an umbrella meeting with a wide range sessions with applications of interest for thousands of statisticians. The travel grants will provide partial support to defray transportation costs for individuals selected from institutions and non-profit associations. An emphasis of the award is to encourage and provide the opportunity for younger statisticians, as well as statisticians from disadvantaged groups, to participate in the meeting. Participants will be notified of the availability of the travel grant through Amstat News, a membership publication of the ASA, and on the ASA Home Page on the Internet. Notices will be provided to university and college departments to encourage younger statisticians to apply for the travel grant. A review and selection committee will be established to review the applications and select grantees. The committee will be comprised of four or more ASA members and will convene at the ASA office in Alexandria, Virginia. Special consideration will be given to statisticians who have recently received their Ph.D.'s and to women and minorities. <br/>"
"0939609","Case Studies in Bayesian Statistics and Machine Learning Workshop Conference Travel; October 2009, Pittsburgh, PA","DMS","STATISTICS","08/15/2009","09/01/2009","Joseph Kadane","PA","Carnegie-Mellon University","Standard Grant","David Stoffer","07/31/2010","$0.00","","kadane@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, 7556, OTHR","$0.00","The First Workshop in Bayesian Statistics and Machine Learning, to be held at Carnegie Mellon University, October 15-17, 2009, will hear extended discussion of the following case studies:  ""Decision theoretic Bayesian nonparametric inference for the molecular characterization and stratification of colorectal cancer using genome-wide arrays"", by Christopher C. Holmes, Christopher Yau, Ian Tomlinson and Jean-Baptiste Cazier; ""Calibrating the Universe: A Bayesian Uncertainty Analysis of Galaxy Simulation"" by Ian Vernon, Richard Bower and Michael Goldstein; ""Rigorous Error Analysis for Small Angle Neutron Scattering Datasets Using Bayesian Inference"", by Chip Hogg, Jay Kadane, Jong Soo Lee, and Sara Majetich. A new investigator's session, a short-course introduction to Bayesian analysis and machine learning, a session on how to  get a grant, and a mixer, are all aimed at attracting students and recent graduates.<br/>    <br/>   This grant supports travel and local expenses for speakers and young investigators to attend the  First Workshop in Bayesian Statistics and Machine Learning, to be held at Carnegie Mellon University, October 15-17, 2009. The fields of Bayesian statistics and machine learning are starting to grow together; we hope that the workshop will accelerate this process by focusing attention on serious scientific applications of the methods."
"0907439","Collaborative Research: Models for Network Evolution: A Study of Growth and Structure in the Wikipedia","DMS","STATISTICS","09/01/2009","08/21/2009","David Banks","NC","Duke University","Standard Grant","Gabor Szekely","08/31/2011","$90,360.00","","banks@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, OTHR","$0.00","<br/>The project will develop new statistical models for network growth and change, and apply these to study the evolution of the Wikipedia.  The research builds on latent factor models for social networks and recent advances in variable selection and cluster analysis in high dimensions.  Using information on the text in Wikipedia entries and its current connectivity structure, the research will estimate where new entries will appear and characterize the local graph structures in different regions of the hyperlinked data set.   Although the models are tuned to the Wikipedia, the methodology has general relevance to the study of complex networks.<br/><br/>The Wikipedia is a unique mirror of human knowledge  It has grown quickly, and this growth continues.  From the standpoint of understanding how humans organize information, it is important to identify the ""holes"" in the Wikipedia, where new entries will arise.  Similarly, one wants to know whether information on, say, Henry VIII is organized in the same way as information on Homotopy Theory.    Both kinds of  questions can be  analyzed statistically, using publicly available version control data that has been archived to help discover Wikipedia vandalism.  The research has direct impact on the study of the structure of human knowledge, and indirect impact on the study of change in complex networks."
"0906449","Association Analysis of Multivariate Competing Risks Data","DMS","STATISTICS","07/01/2009","07/02/2009","Yu Cheng","PA","University of Pittsburgh","Standard Grant","Gabor Szekely","06/30/2012","$196,354.00","","yucheng@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","0000, 6890, OTHR","$196,354.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).  In this proposal, the investigator describes three projects on association analysis of multivariate competing risks data which arise frequently in genetic family studies, demography and other areas.  Often one is interested in familial association of the onset time of a certain event, with the presence of competing events which may dependently censor the occurrence of the target event.  The usual association methods for multivariate survival data assuming the censoring by competing events independent of the target event may produce biased results.  In addition, the marginal distributions of the event of interest are not identifiable.  Hence the proposed association analysis for multivariate competing risks data focuses on two important quantities in competing risks literature: cause-specific hazard (CSH) and cumulative incidence functions (CIFs).  The investigator develops a series of association analyses of multivariate competing risks data which account for the dependence censoring by competing events appropriately.  The first project is related to two equivalent association measures of multivariate competing risks data which are CSH ratios and estimated nonparametrically without smoothing.  In the second project, the investigator expands the application of frailty models to association analysis of multivariate competing risks data through an improper random variable and expresses the bivariate CIF in terms of its marginals and an association parameter.  To incorporate covariates, in the third project, the investigator develops parametric regression models to investigate covariate effects on marginal CIFs and the indirect effects of covariates on the association analysis.  These association methods cover many existing approaches for bivariate data as special cases. <br/> <br/>The proposal concentrates on modeling familial association in a target event with the presence of competing events where the standard methods may produce biased results.  For example, in a large dementia study, family clustering in dementia onset is of interest where the competing event death may preclude the occurrence of dementia.  The application of this research to the dementia and other studies in health and medicine is expected to generate novel insights on the association in a target event among members of a cluster, which help individuals and practitioners perceive the familial risks more accurately.  The enhanced understanding may lead to better prevention and intervention in the target population who are at elevated risks. The methods can be used in many other applications such as demographic studies of human mortality, extremes in financial assets and returns, genetic evaluation of sires for longevity of dairy cows and annuity valuation with dependent mortality in insurance."
"0907655","Data Depth for Nonparametric Multivariate Analysis: Goodness-of-Fit Tests Based on Spacings, Classification, and A Coherent Framework for Data Depth","DMS","STATISTICS","07/01/2009","07/04/2009","Jun Li","CA","University of California-Riverside","Standard Grant","Gabor Szekely","09/30/2013","$119,736.00","","jun.li@ucr.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","MPS","1269","0000, 6890, OTHR","$119,736.00","This award is funded under the American Recovery and Reinvestment Act of 2009 Public Law 111-5). <br/><br/>Advanced computing and data acquisition technologies have made possible the gathering of large multivariate data sets in many fields. Efficient multivariate statistical analysis tools for such data sets are highly sought after. Among the existing multivariate analysis approaches, the one based on the data depth has received most attention recently, due to its highly desirable nonparametric nature. Expanding further along the theme of data depth, this proposal outlines three new research projects in nonparametric multivariate inference, namely: (1) to develop a new class of multivariate goodness-of-fit tests based on multivariate spacings; (2) to introduce a novel nonparametric classification algorithm using the so-called DD-plots; (3) to extend the general framework for all notions of data depth, and to develop new data depths which are suitable for analyzing data drawn from non-continuous distributions. <br/><br/>The proposal addresses important problems in theoretical multivariate statistics, which have a wide range of applications in practice. All three research directions are highly competitive, since any new development in these directions will significantly advance multivariate statistical methodology as a whole. The proposed research also aims to bring forth many efficient statistical inference procedures with immediate applicability in many domains such as medicine, biology, psychology, just to name a few. Motivating examples in ecology and environmental sciences are elaborated in the proposal. They will also be illustrated fully in future publications, which should help foster more interdisciplinary interaction between statistics and other fields."
"0925275","From Probability to Statistics and Back: High Dimensional Models and Processes Conference; Seattle, WA; Summer 2010","DMS","STATISTICS","09/01/2009","04/21/2009","Florentina Bunea","FL","Florida State University","Standard Grant","Grace Yang","10/31/2010","$22,000.00","","fb238@cornell.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","0000, 7556, OTHR","$0.00","<br/>From Probability to Statistics and Back: <br/>High-Dimensional Models and Processes Conference <br/><br/>The investigator proposes to organize a conference at the interface of <br/>probability, statistics and biostatistics, and machine learning. The meeting <br/>aims to identify the new frontiers that can emerge from the interaction of <br/>these fields. The conference will provide  an open forum for discussing <br/>strategies for creating a workforce for the 21s century that will continue <br/>to do fundamental research, which is a key element for the successful <br/>development of the society as a whole.<br/><br/>The themes that will be brought together render uniqueness to this <br/>conference: empirical processes theory, a field of probability theory; <br/>fundamental areas in statistics and biostatistics such as the <br/>theory of nonparametric and semiparametric models <br/>and the theory of shape-constrained estimation; and learning theory. The <br/>meeting will promote immediate dissemination of the most cutting-edge <br/>theoretical results in these fields.<br/>"
"0906424","Optimal Sequential Allocation in Dynamic Environments","DMS","STATISTICS","07/15/2009","07/04/2009","Philippe Rigollet","NJ","Princeton University","Standard Grant","Gabor Szekely","06/30/2013","$110,000.00","","rigollet@math.mit.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","0000, OTHR","$0.00","Bandit problems have been studied in many different contexts and variations. The vast majority of work focuses on a static environment, in which, at each time, the probability distribution of the reward yielded by each action remains unchanged. This static model may clearly fail to produce decision strategies that are optimal in a dynamically changing environment. Despite their sounding relevance in practical applications, such environments have received sporadic attention in the statistical community so far. The contribution of the proposed research to the current state of knowledge will consist in proposing models for new dynamic environments that are motivated by a significant class of applications, designing policies that adapt to dynamic environments, analyzing the performance of these policies and assessing  optimality from a finite time (non asymptotic) point of view.<br/><br/>Sequential allocation in dynamic environments is a problem that arises at the intersection of nonparametric statistics, machine learning and operations research. This project involves techniques from these fields and points out fundamental bridges between the extant results to form a more unified theory of the subject. This theory will then serve as a basis for producing computationally efficient allocation policies with potential applications in clinical trials, drug discovery and real time web page content optimization.<br/>"
"0940365","Theil-Sen Estimators in Semiparametric Mixed Models","DMS","STATISTICS","03/31/2009","06/08/2009","Hanxiang Peng","IN","Indiana University","Standard Grant","Gabor Szekely","08/31/2009","$55,458.00","","hpeng@math.iupui.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","MPS","1269","0000, 9150, 9178, 9251, OTHR","$0.00","<br/>The investigators propose to construct the robust Theil-Sen estimators (TSE's) in general regression models based on multivariate medians, to study their theoretical behaviors, and to explore their practical applications.  The proposed TSE's are given for different regression models based on different multivariate medians. These models include multivariate linear regression, nonparametric regression, semiparametric regression, mixed and additive regression, penalized spline regression, local polynomial regression, wavelet-based smoothers, kriging, etc., while the multivariate medians include in particular those based on different depth functions such as the half space depth, the projection depth, the simplicial depth, the spatial depth, etc. The theory of depth functions can be viewed in part as a multivariate generalization of the univariate rank theory. Depths induce an ordering of all points from a center outward in a high dimensional space because of the lack of the linear ordering in the high dimension. The investigators specifically propose to: 1) Generalize depth functions of vector to matrix argument; 2) Investigate the uniqueness, robustness,  consistency,  and asymptotic normality; 3) Compare asymptotic relative efficiency of the proposed  TSE's with other common estimators(e.g. least squares estimators), compare different multivariate-median-based TSE's,  and compare with other robust estimators;  4) Calculate the complexities, implement algorithms and provide codes that can be accessed by other potential users; 5) Conduct statistical inference, perform simulations, and apply to real applications.  <br/><br/>The Theil-Sen estimator is an estimator of the slope parameter in a simple linear regression model. It is robust to outliers, easy to compute, competitive to the least squares estimator, and has an intuitive geometric interpretation. Despite its many good properties, the TSE is vastly under-utilized because it is developed for a simple linear regression. In recent years semiparametric and nonparametric models have become a popular choice in statistical modeling. They now play an increasing important role in many areas of statistics since they are more realistic and flexible than parametric models. The proposed research will extend the robust TSE's to various useful semiparametric and non-parametric regression models, and will accordingly advance the theory of robust estimation in semiparametric models and provide more robust ways of analyzing data from many applications.  Just like regression analysis which is so popularly used in almost every area of science, these proposed Theil-Sen estimators have wide applications, for example, in astronomy; in remote sensing; in geosciences(e.g. detecting trends of extreme rainfall series); in environmental sciences (e.g. trend analysis for ambient water quality such as detecting seasonal patterns, changes in rainfall, etc so as to assess the relationships among different factors;  to help set water quality guidelines for impacted streams; etc.); in pattern recognitions (e.g. detection of road segments in noisy aerial images), in social sciences;  and so forth.  The proposed research will also involve training of graduate students for future researchers in statistics as well as providing selected undergraduate students with research experience.  <br/>"
"0928262","A NISS/ASA Writing Workshop for New Researchers, August 2009; Washington, DC","DMS","STATISTICS","06/01/2009","05/11/2009","Keith Crank","VA","American Statistical Association","Standard Grant","Gabor Szekely","05/31/2010","$19,998.00","Nell Sedransk","kcrank@gmu.edu","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","0000, 7556, OTHR","$0.00","This award will support a workshop on effective technical writing for new researchers in the statistical sciences who seek to publish their research or to present their research plans in the form of grant proposals for federal funding. Researchers, especially new researchers, often have difficulty disseminating their research results not because of the quality of the research but rather because of inappropriate choices of publication venues for the particular research and/or because of poor presentation of technical material to the chosen audience. The National Institute of Statistical Sciences and the American Statistical Association will manage the Workshop.<br/><br/>This workshop will open with tutorial sessions on the organization of material for a technical article or grant application, on technical writing techniques and on the specific missions and audiences of key journals in the statistical sciences. Then each participating new researcher will work individually with an experienced journal editor as mentor to address these issues on an individualized basis for draft of the new researcher's work in progress. Revisions following this guidance will be critiqued by the mentor to assure that the new researcher's implementation of writing techniques has been successful before the article or the grant proposal is submitted for review. <br/><br/>"
"0854903","Collaborative Research: Tree Structured Object Oriented Data Analysis","DMS","STATISTICS","09/15/2009","09/18/2009","Haonan Wang","CO","Colorado State University","Standard Grant","Gabor Szekely","08/31/2012","$50,380.00","","wanghn@stat.colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1269","0000, OTHR","$0.00","This proposal is for research on finding ways of developing population level understanding of, and insights about, a collection of tree structured objects. I.e. the goal is the analysis of the variation, including variation in branching structure, in a population of data points that are trees. While this goal is statistical in nature, it is very far beyond the reach of existing statistical methods. Thus an entire new area of statistical research is opened up by the work proposed here. This work is driven by a particular example data set of human brain artery trees, collected by a neuro-surgeon collaborator, who has, and will continue to, inform the research directions chosen, and the steps taken. While this motivating example is vasculature of the human brain, there are many other contexts which will be impacted by the new methods developed here, discussed below. The closest statistical area to the proposed research is the currently active area of Functional Data Analysis, in which the atoms of the statistical analysis are curves (instead of the more typical numbers or vectors). This abstract concept was extended to Object Oriented Data Analysis (OODA), by Wang and Marron (2007), where the atoms become more complicated objects of various types, including tree structured objects. OODA presents a number of major new data analytic challenges. Addressing these challenges will require the development of totally new types of statistical methods. Even simple statistical concepts, such as the population mean, are not straightforward to develop. Deeper properties, in particular the quantitation of variation about the mean, are far more challenging. The results of Wang and Marron (2007) were a first pass at formulating statistical concepts in terms of optimization problems. A major limitation was that it was unclear how to compute useful solutions of these for realistic data sets. Aydin et al (2008) achieved a major breakthrough in this direction by inventing linear time solutions to some of these apparently intractable optimization problems, which brings practical OODA of the artery tree data set within reach of modern computational facilities. The proposed work is on much deeper analyses, which requires the invention of powerful new approaches to understanding variation. In particular, the current topology-only analyses will be extended to full nodal attribute data types that will enable simultaneous study of other types of variation as well (e.g. in branch thickness and location), the entirely new area of discrimination for populations of tree structured objects will be explored, and innovations in the visualization of complex tree data objects will be made. These deeper analyses are expected to yield deep new anatomical results, involving symmetry and dependence on covariates such as age, that are unavailable from the simple summaries currently being used to analyze tree data. <br/><br/>The driving data set for this research is a collection of over 100 patients? magnetic resonance angiographic (MRA) brain artery trees. This proposed project will develop new statistical methods for extracting useful information from this collection of trees. Major new population-level insights on human brain anatomy will be targeted. The big picture goal of this research is the development of methods for characterizing normal brain artery structure. As well as being an important scientific anatomical goal in itself, this has potential for major medical applications. For example, this work provides the potential for a vascular-based diagnosis of brain tumors (which have aberrant arterial trees). Arterial tree analysis should improve the success of current cancer treatments through earlier diagnosis than is available using current techniques. Future medical applications are expected to extend well beyond the driving problem of human brain arteries to many other types of widely-studied anatomical structures, such as airways in the lung, the nervous system and various types of collection duct systems. In addition, this body of work is expected to drive new ideas in many other areas which naturally encounter trees as data objects, such as text mining (where a standard technique is representation of grammatical structures as trees), phylogenetic trees in genetics, and the analysis of social and computer networks. Finally this work is expected to have an impact on mathematics by stimulating the development of new ideas there, e.g. in optimization and graph theory."
"0907632","Inference in high-dimension: statistics, computation and information theory","DMS","STATISTICS","09/01/2009","08/22/2011","Bin Yu","CA","University of California-Berkeley","Continuing Grant","Gabor Szekely","08/31/2012","$240,000.00","Martin Wainwright","binyu@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","This research proposal consists of two related research thrusts, all centered around the common goal of an integrated treatment of statistical and computational issues.  The first research thrust concerns various issues associated with the use of structured regularization methods in high-dimensional inference.  Such types of regularization are natural in different settings, including estimation of structured covariance matrices, graphical model selection, and hierarchical data modeling.  The researchers propose to provide sharp characterizations of when structured regularization, with its typically higher computational costs, is guaranteed to yield improvements in statistical efficiency, or conversely, when structured regularization might impair statistical efficiency.  The second research thrust addresses the role of statistical stability in optimization, and the development of new methodology for choosing path length parameters in iterative algorithms.<br/><br/>Statistical inference problems of a high-dimensional nature---meaning where the number of observations n is similar to or even smaller than the number of parameters p---are ubiquitous throughout various areas of science and engineering, among them genomics, neuroscience, remote sensing, natural language processing, data compression, financial time series, and statistical signal processing.  As a concrete instances, consider the problem of estimating the structure of a social network consisting of a large number of individuals (p could be 10,000 or larger) based on a relatively small number of snapshots (n could be 100 to 1,000).  The overarching theme of the proposal is the development of new methodology and theory for high-dimensional data. Given the ubiquity of such data, such developments have the potential to impact a variety of fields making use of statistical modeling and tools, among them information technology (IT), neuroscience, remote sensing, and data compression.  Moreover, the proposal is inter-disciplinary in nature, and so has the potential to strengthen bridges between statisticians and researchers in other departments (e.g., computer science, electrical and civil engineering) also working on IT applications.  Via this type of intellectual unification, the proposed research is likely to have broader impact---much beyond any specific technical contributions---in terms of bridging different research communities, and providing broad training to graduate students and postdoctoral researchers."
"0906497","General Semiparametric Inference via Bootstrap Sampling","DMS","STATISTICS","07/01/2009","07/04/2009","Guang Cheng","IN","Purdue University","Standard Grant","Gabor Szekely","06/30/2012","$100,003.00","","guangcheng@ucla.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1269","0000, 6890, OTHR","$100,003.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>The research objectives of this project are first to prove the theoretical validity of the bootstrap method as a general inferential tool for the semiparametric models, and then invent a computationally attractive bootstrap inference procedure, called k-step bootstrap.  Semiparametric modelling has provided an excellent framework for the modern complex data due to its flexibility to model some features of the data parametrically but without assuming anything for the other features. The bootstrap is the most popular data-resampling method used in statistical analysis, and has recently been applied to the semiparametric models arising from a wide variety of contexts. Therefore, the systematic theoretical studies on the bootstrap inferences for the semiparametric models are fundamentally important. In practice, the computational cost of the bootstrap inference procedure is particularly high for the semiparametric models. Thus, the investigator proposes an approximate bootstrap method, i.e. k-step bootstrap, and will show that this novel approach results in huge computational savings but without sacrificing any degree of inference accuracy. In addition, the investigator will develop a set of asymptotic results to elucidate the asymptotic structure of the semiparametric M-estimation, which is crucial for the future theoretical research. M-estimation refers to a general method of estimation including the maximum likelihood estimation as a special case.<br/><br/>The primary impact of the proposed work is to lay solid theoretical foundation for the general semiparametric inferences via bootstrap sampling. In addition, the proposed k-step bootstrap approach is practically beneficial in several regards. For instance, the scientists who bootstrap a large data set will benefit, as the minimal computational cost needed in the k-step bootstrap to achieve the satisfactory inference accuracy will be precisely analyzed. However, the broader impacts of the proposed activities are multiple. For instance, a key aspect of this project is the integration of research and teaching, which will be achieved by proposing specific projects for students during the teaching of classes on semiparametric inferences and bootstrap computation. This pedagogical method also facilitates the participation of underrepresented groups of students.<br/>"
"0906639","Recovery of Functions via Moments: Hausdorff Case","DMS","STATISTICS","08/01/2009","07/31/2009","Robert Mnatsakanov","WV","West Virginia University Research Corporation","Standard Grant","Gabor Szekely","07/31/2012","$119,959.00","","rmmnatsakanov@mail.wvu.edu","886 CHESTNUT RIDGE ROAD","MORGANTOWN","WV","265052742","3042933998","MPS","1269","0000, 6890, 9150, OTHR","$119,959.00","This award is funded under the American Recovery and Reinvestment  Act of 2009 (Public Law 111-5). The intellectual merit  of this proposal is connected to  the problem of recovering  a multivariate function from its assigned moments and the problem of density estimation for high-dimensional data. The problem of recovering a function from its moments is a special case of the classical moment problem, which concentrates mainly on the questions of the existence and uniqueness of a function with specified moments. The importance of the probabilistic moment problem (Hamburger, Stieltjes, and Hausdorff) can be explained by its application in many statistical inverse problems. For example, in tomography, the moments of an object (a function) are uniquely defined by  the x-rays (projections) of the object being imaged. Many inversion formulas are derived by inverting the moment generating function and the Laplace transform. However, there are only a few  approaches for recovering functions via moments. This can be explained by the unstable behavior of the current methods (e.g., the  Maximum Entropy method applied even in the one-dimensional case) when the higher order moments are involved. The investigator develops a new approach, which yields a stable procedure for recovering functions  within the context of the  multivariate Hausdorff moment problem. Apart from being an alternative to the traditional estimation technique, this approach is applicable in situations where other methods can not be applied. For example, one cannot use a traditional method, e.g., kernel smoothing, when the observed  data are the moments.  The results obtained within this project will have broad impacts  not only in the  multidimensional  Hausdorff moment problem, in the theory of non-parametric  estimation in indirect models (deconvolution and demixing), and in entropy estimation of high-dimensional macromolecules, but also in numerous applications in areas of critical importance, such as image analysis, computed tomography, molecular physics, and homeland security. In particular, in computed tomography, when only a few projections are available, the problem  of image reconstruction becomes ill-posed, and  hence, perfect reconstruction is impossible. <br/><br/>The investigator shows that proposed approach provides a uniform approximation rate, which is an important issue in approximation theory. Besides, in many statistical inverse problems, e.g., those based on convolutions, mixtures, multiplicative censoring, and right-censoring, the moments of the unobserved distribution of actual interest can be easily estimated from the transformed moments of the observed distributions. In all such models,  one can recover a function analytically from its moments by means of proposed technique. In the area of homeland security, the iris classification problem represents another field, where moment-recovered constructions will have an impact."
"0906300","Connecting Markov Random Fields with Geostatistical Models","DMS","STATISTICS","08/01/2009","07/23/2009","Debashis Mondal","IL","University of Chicago","Standard Grant","Gabor Szekely","07/31/2012","$150,000.00","","mondal@wustl.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, 6890, OTHR","$150,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>Recently derived connections between the intrinsic autoregressions and the de Wijs process open up the possibility to further explore and exploit connections between Gaussian Markov random fields and the continuum models in geostatistics to facilitate the statistical analysis of spatial data. In the analysis of areal data, Markov random fields often draw criticism by assuming a dependence structure based on sharing boundaries between two regions (counties, districts or states) and ignoring actual area information. In contrast, geostatistical models can incorporate actual area information by considering integrals over regions, but are faced with computational challenges for large data. This project develops a wide range of new methodologies to fit geostatistical models on large data sets by using Markov random fields. The investigator's work provides new theoretical results for scaling limits of stationary and intrinsic processes and new insight into construction of space-time processes. The investigator applies these methodologies to study the Fairfield-Smith law of fertility on large agricultural fields, obtain the pycnophylactic map of Seattle population, investigate the risk of various cancers in the state of Illinois and interpolate wind vectors.<br/><br/><br/>Recent decades have seen much progress and interest in studying spatial variability with applications in image analysis, agriculture, epidemiology, geology and other areas of environmental science. Two approaches to these studies have emerged, one based on Markov random fields, the other on geostatistics. In this project, the investigator combines the ideas of both Markov random fields and geostatistics to obtain a range of new methodologies to analyze spatial data. The investigator's theoretical work connects the mathematics that originated in the studies of gold mines in South Africa in the nineteen fifties to quantum physics and to recent theory of spatial statistics. The practical effect of investigator's approach is enormous. For example, in studies of geographic variations of risk of cancers, this new methodology overcomes past computational challenges to include detailed area and population information in statistical models and thus provides new directions in revealing differences in risk across geographic regions, indicating areas of high relative risk that may require screening and intervention and detecting effects of socio-economic conditions and exposure to environmental hazards."
"0960590","3rd Midwest Statistics Research Colloquium","DMS","STATISTICS","12/01/2009","12/01/2009","Michael Stein","IL","University of Chicago","Standard Grant","Gabor Szekely","11/30/2010","$12,000.00","","stein@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, 7556, OTHR","$0.00","The Third Midwest Statistics Research Colloquium will be held at the University of Chicago March 26 and 27, 2010.  The conference will consist of three sessions of two talks each, an additional two sessions of talks by graduate students, and an evening poster session.  Speakers who have already accepted invitations are Yuguo Chen, Shuva Gupta, Chris  Hans, Sunduz Keles, and Susan Murphy.  Funds from NSF will be used to defray the travel and lodging costs of speakers and of graduate students and other new researchers.  It is our intention that the Midwest Statistics Research Colloquium becomes an annual event serving to disseminate the best research being done in the region and to support intellectual exchanges in the community.<br/><br/>The Midwest possesses a large number of strong statistics departments.  To encourage interactions between these departments and the broader statistical research community in the region, a number of these departments sponsored the First Statistics Research Colloquium, held at the University of Chicago in March 2008.  This was followed by a second meeting, also held in Chicago, in March 2009.  The investigator, together with the organizing committee, seeks to follow up the success of these meetings with a third meeting in March, 2010.  The day and a half meeting includes talks on a wide array of topics in theoretical and applied statistics and a poster session for graduate students and other new researchers.  The regional nature of the meeting will allow many graduate students to carpool to the meeting, thus allowing many students to attend at modest cost.  In addition, we will be encouraging direct interactions among the graduate students to get them started building the personal and intellectual relationships among their peers that will nourish them throughout their careers.<br/>"
"0852523","Second Midwest Statistics Research Colloquium","DMS","STATISTICS","03/01/2009","12/08/2008","Michael Stein","IL","University of Chicago","Standard Grant","Gabor Szekely","08/31/2009","$10,000.00","","stein@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, 7556, OTHR","$0.00","The Second Midwest Statistics Research Colloquium will be held at the University of Chicago March 27 and 28, 2009.  The conference will consist of four sessions of two talks each and an evening poster session.  Speakers who have already accepted invitations are Peter McCullagh, Beth Andrews, Charles Geyer, Keith Worsley and Ed Ionides.  Funds from NSF will be used to defray the travel costs of speakers and of graduate students and other new researchers.  It is our intention that the Midwest Statistics Research Colloquium becomes an annual event serving to disseminate the best research being done in the region and to support intellectual exchanges in the community.<br/><br/>The Midwest possesses a large number of strong statistics departments.  To encourage interactions between these departments and the broader statistical research community in the region, a number of these departments sponsored the First Midwest Statistics Research Colloquium, held at the University of Chicago in March 2008.  The investigator, together with the organizing committee, seeks to follow up with a second meeting in March, 2009.  The day and a half meeting includes talks on a wide array of topics in theoretical and applied statistics and a poster session for graduate students and other new researchers.  The regional nature of the meeting will allow many graduate students to carpool to the meeting, thus allowing many students to attend at modest cost.  In addition, we will be encouraging direct interactions among the graduate students to get them started building the personal and intellectual relationships among their peers that will nourish them throughout their careers.<br/><br/><br/>"
"0906568","Quantile regression with mismeasured or missing covariates","DMS","STATISTICS","07/01/2009","07/04/2009","Ying Wei","NY","Columbia University","Standard Grant","Gabor Szekely","06/30/2013","$130,000.00","","yw2148@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","0000, 6890, OTHR","$130,000.00","<br/>This award is funded under the American Recovery and Reinvestment Act of 2009 Public Law 111-5). <br/><br/>Quantile regression (Koenker and Bassett, 1978) has emerged as an important statistical methodology, and has been used in a wide range of applications including economics, biology, ecology and finance. Very often a data set is not perfectly obtained. Some variables may be measured with error, while others may contain missing observations.  Ignoring measurement errors or missing observations could lead to substantial bias in estimation. For this reason, how to handle measurement errors and missing data has generated a large number of literatures. Unfortunately, most of the existing methods rely on a parametric likelihood form, and hence cannot be applied to quantile regression directly.  This proposal targets at developing methods and theories for obtaining unbiased quantile estimates even in the presence of measurement errors and/or missing observations.  The specific proposed research activities under this project include the following four aspects. (1) Develop estimation methods for linear quantile models allowing the existence of measurement errors, and investigate the asymptotic properties for the resulting estimator. (2) Extend the estimation method for linear quantile model to semiparametric models, which brings more flexibility and hence facilities a wider range of applications. (3) Develop related inference and model adequacy assessment tools. (4) Extend the proposed methods in 1 - 3 to address missing data problems in conditional quantile models, including estimation, inference and model assessment. The statistical methods to be employed for this proposal cover quantile regression, methods and theories for measurement errors and missing data problems, nonparametric and semi-parametric modeling, goodness-of-fit tests, bootstrapping methods and robust statistics.<br/><br/>The proposed research will lead to more accurate inference and more comprehensive qualifications in various research applications in epidemiology, HIV research, genetics, cancer research and environmental science, as measurement errors and missing data commonly exist in those applications. The methodologies to be developed by the investigators are of general interest to statistical research. The proposed research will be widely disseminated through publications, presentations in domestic and international conferences, and collaborations with clinical and public health researchers."
"0906919","Nonparametric Methods for Jump Processes Under Microstructure Noise","DMS","STATISTICS","07/15/2009","07/07/2009","Jose Figueroa-Lopez","IN","Purdue University","Standard Grant","Gabor Szekely","06/30/2012","$107,250.00","","figueroa@math.wustl.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1269","0000, 6890, OTHR","$107,250.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>The investigator studies nonparametric methods for continuous-time jump processes that are contaminated by a background noise, or are affected by random clocks. In financial markets, for instance, the background noise is a byproduct of the way trading takes place, while a random clock could model non-synchronous trading effects or a cumulative measure of economic activity. The proposed research develops methodologies to quantify and mitigate the effects of the nuisance components by determining appropriate sampling frequencies, bias correction tools, and data-driven model selection criteria. The focus of the work is on drawing inferences for the jump component of the process. Three concrete research directions are put forward:  (1) Adaptive nonparametric methods for the infinite-dimensional parameter controlling the jump dynamics, (2) Incorporation of the market microstructure of asset prices into the model and the statistical methodology, and (3) Extensions to more versatile models with jumps such as time-changed Levy or additive processes.<br/><br/>In recent years increasingly complex probabilistic models have been developed in a quest to incorporate the real nature of the phenomenon under consideration. Considerably less effort has been devoted to a systematic study of the effects that departures from the presumed model have in the statistical estimation of the underlying parameters driving the phenomenon.  However, without an appropriate statistical methodology, even the most sophisticated paradigm produces only limited practical impact in industry and across other fields. The proposed work is expected to significantly advance the theory of mathematical finance by targeting the aforementioned critical implementation issues.  Furthermore, in light of the ongoing trend by the financial industry  to adopt risk-adverse models that incorporate ""bubles"" and potential crashes, the present research is expected to foster opportunities of collaboration between academia and industry. An empirical assessment of the potential sudden price shifts of a commodity is critical to develop appropriate risk-management and investment strategies. Other outreach objectives include extensions to spatio-temporal discontinuous processes which are increasingly in demand in fields such as in environmental sciences."
"0854908","Collaborative Research: Tree Structured Object Oriented Data Analysis","DMS","STATISTICS","09/15/2009","09/18/2009","James Marron","NC","University of North Carolina at Chapel Hill","Standard Grant","Gabor Szekely","08/31/2012","$149,995.00","Gabor Pataki","marron@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","0000, OTHR","$0.00","This proposal is for research on finding ways of developing population level understanding of, and insights about, a collection of tree structured objects.  I.e. the goal is the analysis of the variation, including variation in branching structure, in a population of data points that are trees.  While this goal is statistical in nature, it is very far beyond the reach of existing statistical methods.  Thus an entire new area of statistical research is opened up by the work proposed here.  This work is driven by a particular example data set of human brain artery trees, collected by a neuro-surgeon collaborator, who has, and will continue to, inform the research directions chosen, and the steps taken.  While this motivating example is vasculature of the human brain, there are many other contexts which will be impacted by the new methods developed here, discussed below.  The closest statistical area to the proposed research is the currently active area of Functional Data Analysis, in which the atoms of the statistical analysis are curves (instead of the more typical numbers or vectors).  This abstract concept was extended to Object Oriented Data Analysis (OODA), by Wang and Marron (2007), where the atoms become more complicated objects of various types, including tree structured objects.  OODA presents a number of major new data analytic challenges.  Addressing these challenges will require the development of totally new types of statistical methods.  Even simple statistical concepts, such as the population mean, are not straightforward to develop.  Deeper properties, in particular the quantitation of variation about the mean, are far more challenging.  The results of Wang and Marron (2007) were a first pass at formulating statistical concepts in terms of optimization problems.  A major limitation was that it was unclear how to compute useful solutions of these for realistic data sets. Aydin et al (2008) achieved a major breakthrough in this direction by inventing linear time solutions to some of these apparently intractable optimization problems, which brings practical OODA of the artery tree data set within reach of modern computational facilities.  The proposed work is on much deeper analyses, which requires the invention of powerful new approaches to understanding variation.  In particular, the current topology-only analyses will be extended to full nodal attribute data types that will enable simultaneous study of other types of variation as well (e.g. in branch thickness and location), the entirely new area of discrimination for populations of tree structured objects will be explored, and innovations in the visualization of complex tree data objects will be made.  These deeper analyses are expected to yield deep new anatomical results, involving symmetry and dependence on covariates such as age, that are unavailable from the simple summaries currently being used to analyze tree data.<br/><br/>The driving data set for this research is a collection of over 100 patients? magnetic resonance angiographic (MRA) brain artery trees.  This proposed project will develop new statistical methods for extracting useful information from this collection of trees.  Major new population-level insights on human brain anatomy will be targeted.  The big picture goal of this research is the development of methods for characterizing normal brain artery structure.  As well as being an important scientific anatomical goal in itself, this has potential for major medical applications.  For example, this work provides the potential for a vascular-based diagnosis of brain tumors (which have aberrant arterial trees).  Arterial tree analysis should improve the success of current cancer treatments through earlier diagnosis than is available using current techniques.  Future medical applications are expected to extend well beyond the driving problem of human brain arteries to many other types of widely-studied anatomical structures, such as airways in the lung, the nervous system and various types of collection duct systems.  In addition, this body of work is expected to drive new ideas in many other areas which naturally encounter trees as data objects, such as text mining (where a standard technique is representation of grammatical structures as trees), phylogenetic trees in genetics, and the analysis of social and computer networks.  Finally this work is expected to have an impact on mathematics by stimulating the development of new ideas there, e.g. in optimization and graph theory.<br/>"
"0907177","Significance Based Procedures for Mining and Prediction of Large Data Sets","DMS","STATISTICS","09/01/2009","07/26/2009","Andrew Nobel","NC","University of North Carolina at Chapel Hill","Standard Grant","Gabor Szekely","08/31/2013","$210,001.00","","nobel@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","0000, OTHR","$0.00","Exploratory methods play a critical role in the understanding of large data sets, regardless of their origin, and are typically the first step in their analysis. The investigator is studying the development and use of exploratory, data-mining methods that identify patterns or regularities in high-dimensional data. The specific focus of his research is the problem of identifying sample-variable associations in large data sets that may arise from multiple measurement technologies. In the typical case where the data from an experiment are represented in the form of a rectangular matrix, sample-variable associations correspond to distinguished submatrices of the data matrix. The investigator is developing a statistically principled, significance-based approach to the problem of finding large average submatrices of a data matrix, using a simple iterative algorithm. The algorithm is applicable to real-valued and categorical data matrices. In addition to the basic method, the investigator is developing several extensions, including data-driven null models that incorporate dependence between variables, data arising from the simultaneous application of multiple measurement technologies, and application of the basic method to prediction problems such as classification, regression and survival analysis. In addition, the investigator is developing basic theory to support the use of the algorithm, and to assess the structure of data matrices under the different null models. The development and application of the methods is being carried out in close collaboration with several groups of biomedical researchers. In particular, the new data mining methodology is being incorporated into software that is used by collaborating scientists to identify and assess significant sample-variable associations in ongoing experiments involving breast, brain and lung cancer.<br/><br/>Large data sets are now common in many experimental areas of science, and in particular gene-level studies of human diseases such as cancer. In such studies it is not unusual to encounter experiments containing from hundreds to thousands of samples, and tens of thousands to millions of measurements on each sample. Large data sets are part of a trend away from traditional hypothesis-driven scientific research towards data-driven research, in which researchers explore large data sets for patterns or regularities that, in conjunction with subject matter expertise, yield hypotheses that can be tested by more traditional means. The investigator is studying an exploratory method that identifies statistically significant associations between samples and variables in large data sets, associations that can yield testable scientific hypotheses.  The methods being developed by the investigator are computationally efficient, and are based on established statistical principles, in particular the notion of statistical significance.  The investigator is also studying ways in which the basic exploratory method can be applied to data arising from multiple measurement technologies, and application of the basic method to statistical problems such as classification and survival analysis.  These activities are being carried out as part of a collaborative research program involving the sustained interactions of faculty and students from the statistical, biological, and medical sciences. The exploratory method developed by the investigator is being integrated into the basic exploratory tools of the collaborating scientists, and is a component in the analysis of several new, previously unanalyzed, data sets."
"0904184","Collaborative Research: Novel methods for pharmacogenomic data analysis using gene clusters","DMS","STATISTICS","08/15/2009","08/19/2009","Michael Kosorok","NC","University of North Carolina at Chapel Hill","Standard Grant","Gabor Szekely","07/31/2012","$100,000.00","","kosorok@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","0000, OTHR","$0.00","<br/>Numerous pharmacogenomic studies have been conducted using microarrays to survey the whole genome and detect disease-associated genes. Genes have the inherent clustering structure. The goal of this study is to develop a systematic framework using principal component analysis (PCA) based methods to detect gene clusters differentially expressed and/or with joint predictive power. More specifically, the investigators will (1) develop novel methodology to detect gene clusters marginally differentially expressed; (2) develop penalization methodology to detect gene clusters with joint predictive power for the disease clinical outcomes of interest; and (3) conduct extensive numerical studies, and develop publicly available software. This study will greatly advance our understanding of the ?large p, small n? statistics as well as human genomics. Methodologies developed in this study can be applied in other areas including image processing, immunology, molecular dynamics, small-angle scattering, and information retrieval.<br/><br/>Identification of genomic markers from analysis of pharmacogenomic data is a key step in understanding human genomics and personalized medicine. The proposed study has been motivated by the urgent need to overcome drawbacks of existing methods. It will feature novel statistical methods, rigorous theoretical development, extensive numerical studies, development of public software, and a direct impact on practical studies. The proposed study will enrich the family of high dimensional methodologies in general. In addition, analysis of breast cancer, colon cancer, and lymphoma microarray data will lead to a deeper understanding of the genomic mechanisms underlying those cancers. From educational and social prospective, the proposed study will foster more intensive collaborations among investigators from different institutions and background. It will promote teaching, training and learning at Yale University and at the University of North Carolina. Moreover, the investigators will attend statistical and genomic conferences and give presentations, which may promote interdisciplinary research among scientists from diverse fields."
"0906660","Model selection and efficient learning  for high dimensional clustered data","DMS","STATISTICS","09/01/2009","08/07/2009","Annie Qu","IL","University of Illinois at Urbana-Champaign","Standard Grant","Gabor Szekely","08/31/2013","$210,144.00","","aqu2@uci.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","0000, OTHR","$0.00","This research project is aimed at developing statistical theory and practical methodology for complex high-dimensional clustered data where the number of variables is larger than the sample size. This problem is especially important and relevant in microarray data where there are thousands of genes involved. The focus of this research will be to show how to efficiently and accurately extract information from a large quantity of often noisy information consisting of high-dimensional data, so as to identify and select significant variables of scientific interest. The PI and her collaborators will develop estimation procedures, statistical inference functions, model selection and classification procedures by incorporating correlation into the models. The specific goals for this research plan are: (1) To propose flexible estimation procedures for the link function and the marginal variance function when their forms are unknown in the generalized linear models; (2) To develop semiparametric classification for time-course gene expression data;  (3) To propose model selection criteria for choosing informative correlation structures; (4) To develop efficient and consistent model selection procedures for generalized additive models where the likelihood is unspecified; (5) To develop a sufficient dimension reduction method for correlated data and retain the full regression information without imposing parametric models.<br/><br/>The research project will help to tackle fundamental questions in statistical science and will stimulate interest  from a large group of scientists in the fields of longitudinal and cluster data analysis. It will also enhance the development of, and makes connections between, theory and method in statistics, biostatistics and computer science. This research will have significant impact and many applications in  biomedical studies, genome research, econometrics, environmental studies, oceanography, social science and public health where correlated data often arise.  The PI will integrate the proposed research areas substantially into educational activities through the development of new university courses, and through presenting short courses at major statistical meetings. The research will advance undergraduate and graduate students' learning and training for handling high-dimensional correlated data."
"0906880","Complexity Regularization in Statistical Learning Theory","DMS","STATISTICS","08/01/2009","05/06/2009","Vladimir Koltchinskii","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","07/31/2013","$217,989.00","","vlad@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","0000, OTHR","$0.00","Vladimir Koltchinskii studies two important classes of problems in High-Dimensional Statistics and Machine Learning: sparse recovery and manifold learning. In both cases, the focus is on the problems in which penalized empirical risk minimization with convex loss functions and convex complexity penalties is used to define statistical estimators of target functions and in which the geometric nature of the problem plays an important role. One of the goals is to extend the theory of sparse recovery that emerged in Harmonic Analysis, Signal Processing and Statistics beyond the usual framework of finite dictionaries to include a variety of problems that are of importance in Machine Learning (in particular, in kernel machines methods and ensemble methods). Specifically, the aim is to develop a theory of sparse recovery based on penalized empirical risk minimization in large ensembles of kernel machines and in linear spans and convex hulls of infinite dictionaries. Another goal is to develop a mathematical theory of several manifold learning methods introduced in the recent years. This includes methods of statistical estimation of partial differential operators associated with a manifold, such as Laplace-Beltrami operator, based on the data sampled from this manifold. These operators are used to develop an ``approximate version'' of harmonic analysis for functions on the manifold that is of importance in nonparametric function estimation. In particular, the research focuses on the analysis of regularized estimators of eigenvalues and eigenfunctions of these operators and on the development of error bounds for complexity regularized estimators in learning problems for manifold data.<br/><br/>The project is closely related to several lines of research in Mathematics, Statistics and Computer Science. Better understanding of subtle geometric nature of complex, high-dimensional data sets and taking it into account in the development of statistical inference for high-dimensional data are very important challenges in Statistics and Machine Learning. Sparse recovery and manifold learning are among the most important developments in these areas where the methods of Asymptotic Geometric Analysis, High-Dimensional Probability and Differential Geometry are used to study a number of challenging statistical problems. This leads to new mathematical tools and new statistical methods with potential applications in a variety of areas where the approach based on Machine Learning is crucial, such as Brain Imaging, Bioinformatics, Data and Visual Analytics. The research also benefits education by providing training opportunities for graduate students and it facilitates exchanges and collaborations between Mathematics, Statistics and Computer Science. <br/>"
"0968714","Generalized Fiducial Inference for Modern Statistical Problems","DMS","STATISTICS","06/03/2009","11/05/2009","Jan Hannig","NC","University of North Carolina at Chapel Hill","Continuing Grant","Gabor Szekely","07/31/2011","$170,900.00","","jan.hannig@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","0000, OTHR","$0.00","In this proposal the investigators revisit Fisher's controversial fiducial argument with a modern set of questions in mind.  This is motivated by the success of generalized inference as introduced by Tsui & Weerahandi (1989), which in fact leads to the same results as Fisher's fiducial inference (Hannig, Iyer & Patterson, 2006).  The investigators do not attempt to derive a new ``paradox free theory of fiducial inference''.  Instead, with minimal assumptions, the investigators present a new simple fiducial recipe that can be applied to conduct statistical inference via the construction of generalized fiducial distributions.  This recipe is inspired by the concept of generalized pivotal quantity and is designed to be fairly easily applicable in many practical applications. It can be applied regardless of the dimension of the parameter space (i.e., including nonparametric problems), and it often leads to statistical procedures that are asymptotically exact and, more importantly, possess very good approximate small sample properties. The investigators propose to investigate theoretical properties of generalized fiducial distributions for statistical problems and apply their findings to various problems of broader interest.<br/> <br/> <br/>Systematic study of properties of generalized fiducial inference will increase our understanding of foundations of statistics and will give statisticians an additional tool to use when dealing with problems they encounter in practice.  More directly, successful solution of the proposed applied problems will immediately bear fruit in the application areas, e.g., pharmaceutical statistics and metrology. For instance, the U.S. Food and Drug Administration (FDA) guidance document spells out analysis procedures for demonstration of equivalence of two or more drug formulations. The investigators aim to show that the fiducial approach will lead to more efficient procedures, which will result in cost and time savings, an important issue for the drug industry. In metrology, the International Bureau of Weights and Measures (BIPM) in conjunction with the International Organization for Standardization (ISO), has published a ``Guide to Expression of Uncertainty in Measurements"" (GUM) which spells out the procedures to be followed by national metrological institutes such as NIST in the US, NPL in UK, and PTB in Germany. A problem that is unique to metrology is that every measurement is subject to unknown and unknowable systematic errors that are often larger than random errors. The only way to quantify these unknowable systematic errors is via specification of subjective distributions for them. The GUM specifies how to combine data-based estimates of standard deviations for some error components in the calculations and subjective estimates of uncertainty for other error components. The investigators aim to demonstrate that the fiducial method provides a natural approach for accomplishing this. Such results are likely to influence the metrology community in modifying and improving their current procedures."
"0906073","Statistical Inference of Models with Time-Varying Parameters","DMS","STATISTICS","09/01/2009","05/11/2011","Wei Biao Wu","IL","University of Chicago","Continuing Grant","Gabor Szekely","08/31/2014","$244,300.00","","wbwu@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00","The investigator studies estimation, testing and construction of simultaneous confidence regions for time-varying parameters in models with non-stationary and dependent errors. The simultaneous confidence regions can be used to test patterns of associations between covariates and responses. The investigator also develops new tools for asymptotic analysis of non-stationary time series.<br/><br/>During the last decade, models with time-varying data generating mechanisms have gained substantial attention in various fields including economics, finance, engineering, sociology, medical science, environmental science among others. Results from the proposal are useful for understanding the dynamic association between explanatory variables and responses for temporally observed data in which the underlying physical mechanism change with respect to time.<br/>"
"0906765","Space and Space-Time Models for Large Datasets","DMS","STATISTICS","09/01/2009","08/31/2009","Bruno Sanso","CA","University of California-Santa Cruz","Standard Grant","Gabor Szekely","08/31/2013","$175,000.00","","bruno@soe.ucsc.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1269","0000, OTHR","$0.00","In this project the investigator focuses on linear representations of<br/>Gaussian random fields. He considers models that either avoid explicit<br/>computation of the covariance matrix or do so for relatively small<br/>dimensions. The correlation functions of the considered processes are<br/>very general, avoiding particular symmetries or stationarity. The PI<br/>considers models that work in domains of general dimension and, in<br/>particular, on the sphere, for gridded and non-gridded data. The PI uses<br/>hierarchical methods of inference to account for measurement errors and<br/>different sources of information. He develops statistically rigorous<br/>procedures to summarize the information of dynamically evolving random<br/>fields in a reduced number of time series and designs fast Monte Carlo<br/>methods that take advantage of parallel architectures.<br/><br/>The investigator studies statistical models for spatial and<br/>spatio-temporal processes observed at a large number of locations and<br/>time steps. The PI's research addresses the need for increasingly<br/>sophisticated models that can deal with different sources of<br/>information, include expert opinion and handle effectively several<br/>sources of uncertainty. While operating on large datasets, the PI's<br/>models consider time evolving dynamics and spatial heterogeneity.<br/>This allows for the analysis of phenomena on global scales, making good<br/>use of state of the art inferential and computational methods. An<br/>example of a problem where unified inferences from a variety of data<br/>sources is needed is the prediction of future climate from different<br/>climate models. Climate change prediction currently has a large societal<br/>impact. This research provides tools to enhance our quantitative<br/>understanding of the uncertainties  involved in such predictions. This<br/>will improve the ability of decision makers to make quality policy<br/>decisions."
"0906592","Generalized Linear Models","DMS","STATISTICS","08/01/2009","05/06/2011","Peter McCullagh","IL","University of Chicago","Continuing Grant","Gabor Szekely","07/31/2013","$200,000.00","","pmcc@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00","This proposal focuses on mathematical models for the selection of experimental or observational units in sample surveys, traffic studies, ecological surveys and clinical trials. The PI studies the concept of auto-generation of units, a marked point process for the occurrence of events, interpreted as observational units, the mark incorporating all relevant quantitative information. This is intended as a probabilistic model for situations such as marketing studies and animal behaviour surveys, where each sampling unit is a event having a random type or mark, including both covariate and response. Since the number of units and the covariate configuration are both random, care must be taken in calculating the response distribution because different schemes for sampling the point process yield different sampling distributions. The sampling scheme may be biased in the sense that it favours larger values of the response. Less obvious biases may occur in samples drawn from binary random-effects models. For example, there may be interference from the covariate value for other events. Moreover, the response distribution for a unit taken by quota sample from a fixed covariate stratum need not be the same as the conditional distribution for an autogenerated unit having the same covariate value.<br/><br/>In the traditional statistical formulation taken from field trials, the set of units is regarded as fixed in advance, and treatment is assigned at random to those fixed units. In survey work, the sample units are selected from the population by an objective randomization mechanism independent of the response.<br/>By contrast, in animal behaviour studies, each behaviour event is a unit, and neither the number nor the configuration of units is pre-specified. In clinical trials, the patients are not fixed in advance, nor are they selected by random sampling. Instead, they are volunteers, to some extent self-selected, and frequently subject to pre-screening to improve compliance rates. The aim of this work is to develop probabilistic schemes tailored to biased-sampling schemes of this sort, where the response distribution among sampled units may be different from the response distribution in the broader population."
"0905777","Efficient Analysis of Competing Risks Models with Missing Data","DMS","STATISTICS","09/01/2009","09/04/2009","Yanqing Sun","NC","University of North Carolina at Charlotte","Standard Grant","Gabor Szekely","08/31/2013","$120,000.00","","yasun@uncc.edu","9201 UNIVERSITY CITY BLVD","CHARLOTTE","NC","282230001","7046871888","MPS","1269","0000, OTHR","$0.00","In the HIV vaccine efficacy study, a very high percentage of marks of interest may be missing and the problem is attributed to the evolving nature of the HIV viruses. This proposal proposes some efficient statistical methods for dealing with missing marks under competing risks models. The investigator will investigate the mark-specific proportional hazards model and the mark-specific Cox model with time-varying effects. The mark-specific vaccine efficacies can be expressed in terms of one of the regression functions under the proposed models. To evaluate mark-specific vaccine effects and its dependence on the mark, the investigator studies the mark-specific proportional hazards model and the mark-specific Cox model with time-varying effects that hold at each level of the mark variable. There is a built-in structure between the mark, failure times and covariates. Arbitrary modeling of the conditional distributions of the mark variable given theauxiliary variables, as did in the existing literature, may run into conflicts with the underlying models and result in inconsistency. The investigator proposes a two-stage approach to achieve more efficient estimation procedures. The inverse probability weighted complete-case estimators are derived in the first stage. The two-stage efficient estimation procedure is obtained using the idea of the augmented inverse probability weighted complete-case method and based on the first stage estimators. The statistical procedures will be developed to more effectively evaluate HIV vaccine efficacies. The problems of missing marks in competing risks models are not unique to the HIV vaccine efficacy trials. The analysis of other statistical models using competing risks data with missing marks will also be studied. The proposed methods will be justified theoretically, evaluated in simulations and applied to analyze the HIV vaccine efficacy trials.<br/><br/><br/><br/>An objective of randomized placebo-controlled preventive HIV vaccine efficacy trials is to assess the relationship between the vaccine effect to prevent infection and the genetic distance of the exposing HIV to the HIV strain(s) represented in the vaccine construct. The investigator proposes some efficient statistical methods to evaluate the HIV vaccine efficacies when a high percentage of the genetic distances (or marks) may be missing due to the evolving nature of the HIV viruses. The missing marks can also occur in competing risks data from other medical studies. The investigator proposes to study the vaccine efficacies under the mark-specific proportional hazards model and the mark-specific Cox model with time-varying effects. These models have clear biological interpretations for studying HIV vaccine efficacies. Other statistical models using competing risks data with missing marks will also be studied. The proposed research would provide critical statistical tools needed for developing more effective vaccines and enrich a collection of statistical tools which have important impact on the risk analysis of competing risks data."
"0907466","Statistical Inference for Censored Preference Data","DMS","STATISTICS","08/01/2009","08/02/2011","Guy Lebanon","GA","Georgia Tech Research Corporation","Continuing Grant","Gabor Szekely","07/31/2012","$175,881.00","Yajun Mei","lebanon@cc.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","0000, OTHR","$0.00","Ranked data arises from m raters ordering by some mechanism n items to express their preferences for the item. Such data can represent election voting, psychological and medical surveys, book and movie recommendation, and web-site ranking system such as search engines. In this proposal the investigators develop the theory and methodology of statistical inference in the case where n and m tend to infinity, and each rater provides an increasingly censored or partial preference information. Under this scenario, they demonstrate how to obtain consistent non-parametric estimators and develop efficient computational procedures for their use. Another aspect that is examined is visualizing preference data by embedding it in a low dimensional space, and designing appropriate surveys for preference data.<br/><br/>The methodology and theory developed in this proposal should help build superior recommendations systems which are becoming increasingly popular in today's online businesses. Such systems build a customized list of recommended items based on the user's past preferences. The proposal also develops visualization techniques for such data which should increase the ability of businesses to analyze customer survey data. In the past such techniques have been either ad-hoc and lacking statistical interpretation, or computationally prohibitive. This proposal aims at developing useful tools for preference data that are both statistically interpretable and computationally efficient, in a realistic large data setting.<br/>"
"0906813","New Directions in Functional Data Analysis","DMS","STATISTICS, COFFES","09/01/2009","06/19/2013","Jane-Ling Wang","CA","University of California-Davis","Continuing Grant","Gabor Szekely","07/31/2015","$399,627.00","","janelwang@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269, 7552","0000, OTHR","$0.00","Functional data analysis is an emerging area in statistics that deals with a sample of random functions.  In practice, measurements of these random functions are often taken intermittently at discrete time points and may be subject to random noise. This results in two scenarios, one with complete or dense recordings, termed 'functional data', and another with sparse measurements at discrete time points, termed 'longitudinal data' because such data are typical for longitudinal studies. Statistical analysis for these two types of data and the respective theory differ substantially.  The investigator advocates that it is not necessary to develop methodology for functional and longitudinal data separately as is common practice. Instead, a unified approach that handles both data structures on a single platform will be developed. The approach is rooted in the dimension reduction approach of principal component analysis, which has been extended to functional/longitudinal data and termed 'functional principal component analysis (FPCA)'. Existing FPCA approaches assume that data are from a single population, thus do not take advantage of available information on covariates, which can be either time-independent or time-dependent. The first theme/aim of the proposal is to fill this gap in the literature by adjusting FPCA for covariate information, for both functional and longitudinal data. A recent FPCA approach developed by the PI and colleagues facilitates this extension, which is coupled with nonparametric and semiparametric approaches. A second theme of the proposal is to apply the functional methodology in Aim 1 to unstructured (non-functional) high dimensional data by reordering and then 'stringing' them into data which can then be interpreted as functional data. 'Stringing' can be accomplished through multidimensional scaling. If sufficiently strong correlations exist among the variables of the unstructured data, an ordering can be found in which neighboring variables are highly correlated. This approach turns the curse of high-dimensional data into a blessing, as functional data analysis inherently takes advantage of the adjacency of high dimensional densely recorded data for each subject. The overarching goal of this project is to develop a cohesive framework for several types of high dimensional data through a combination of FDA and Stringing approaches. The two themes are thus tightly connected and further explored in Aim 3, to develop and disseminate software for these methods. <br/><br/>High dimensional data are common nowadays in many disciplines. This proposal focuses on two types of high-dimensional data: functional/longitudinal data and unstructured high-dimensional data. The combined approaches of Aims 1 and 2 can handle a large variety of high-dimensional data. The proposed research is motivated by real world problems, such as those from the Baltimore Longitudinal Study or in gene expression studies listed in the databases of the National Center for Biotechnology Information. A significant portion of the problems originates from ongoing collaborations of the PI with biologists and demographers and aims to identify:  (1) key biological and behavioral factors that contribute to longevity, (2) key risk factors to diseases, and (3) genes that are related to patient survivals. The new approaches will help to shed light on important issues in many fields by overcoming the challenges with high dimensional data."
"0907484","Density-Preserving Maps","DMS","STATISTICS","09/01/2009","08/11/2011","Alexander Gray","GA","Georgia Tech Research Corporation","Continuing Grant","Gabor Szekely","08/31/2012","$180,000.00","","agray@cc.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","0000, OTHR","$0.00","This project will investigate two aspects of high-dimensional statistics.  First, it develops a new alternative paradigm for nonlinear dimension reduction (often called manifold learning) in which, instead of preserving local distances in the original space as done by existing approaches, the approach preserves the densities in the original space.  The motivation is twofold: Using results from Riemannian geometry, the investigators have shown that is not possible in general to preserve distances, and that it is always possible to preserve densities; in addition, because perhaps the common scientific use of nonlinear dimension reduction methods is to visualize clusters and outliers, which are arguably best formally described in terms of densities, it can be argued that this approach directly preserves the actual information of interest.  This is achieved by means of novel formulations resulting in least-squares problems, as shown in preliminary work, or convex optimization problems to be developed.  Second, the project develops theory and methodology for nonparametrically estimating the densities of points lying on a submanifold, which is needed as the first step in the overall approach.  This includes asymptotic results which are dependent on the dimension of the submanifold rather than that of the ambient space, as current exist.  This provides contrast to the popular conclusion that nonparametric estimation in high dimensional spaces is simply intractable.  Theoretical, methodological, and experimental development will be performed.<br/><br/>Very high-dimensional data, such as text documents, images, or astronomical spectra as typically encoded, have become increasingly important and prevalent, while statistical theory and methods have only recently attacked such problems with full vigor.  Such data are critical for homeland security, medicine, remote sensing of the environment, e-commerce, and a host of other domains.  The intellectual merit of the work is the introduction of a new way of formulating and analyzing two fundamental statistical operations on such data, called dimension reduction and density estimation.  Each of these could open the door to new avenues in the much-needed area of very high-dimensional statistics.  The broader impact of the work is the transformative ability of analysts to reliably identify outliers and clusters in high-dimensional data -- for example such a tool could help astronomers identify new types of astrophysical objects.  The work will be distributed as part of a well-distributed state-of-the-art toolbox of statistical methods to maximize impact across many areas of data analysis."
"0846234","CAREER: Sparse Modeling and Estimation with High-dimensional Data","DMS","STATISTICS","07/15/2009","06/05/2012","Ming Yuan","GA","Georgia Tech Research Corporation","Continuing Grant","Gabor Szekely","06/30/2013","$311,943.00","","ming.yuan@columbia.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","0000, 1045, 1187, OTHR","$0.00","With the recent advances in science and technology, high dimensional data are becoming a commonplace in diverse fields. The goal of this proposed research is to develop methods and theory for several basic classes of statistical problems associated with this type of data. Among the central questions are the nature of sparsity in different contexts, and how it determines our ability or inability to deal with high dimensional data. The investigator studies a reproducing kernel Hilbert space based framework to exploit sparsity for general predictive problems. The framework underpins the connections among various popular methods that encourage sparsity, and provides an opportunity to study them in a unified fashion, which in turn will foster the development of improved methods and algorithms. The investigator will also consider the problem of covariance matrix estimation and selection. The research concentrates on understanding the nature of and connection among various notions of sparsity for large covariance matrix, and their relationship with Gaussian graphical models.<br/><br/>From the world's most powerful telescopes to the finest atomic force microscopes, from the flourishing financial market to the fast-growing World-Wide Web, high dimensional and massive data are being produced at an astonishing rate. Immediate access to copious amount of interesting and important information presents unprecedented opportunities, but also creates unique challenges, to mathematicians in general and statisticians in particular. Development of statistical theory to understand the nature of their fundamental characteristics, and methodology to address the associated issues, including those discussed in this proposal, will advance our intellectual exploration and knowledge, and undoubtedly benefit a multitude of scientific and technological fields -- genomics, medical imaging, communication networks, and finance are just a few well known examples.<br/>"
"0906720","Statistical Methods for Hybrid Optimization","DMS","STATISTICS","09/01/2009","05/23/2011","Herbert Lee","CA","University of California-Santa Cruz","Continuing Grant","Gabor Szekely","08/31/2013","$215,000.00","","herbie@ams.ucsc.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1269","0000, OTHR","$0.00","This project combines local numerical optimization routines from applied mathematics with flexible global models from statistics to create efficient hybrid optimization routines.  These routines can address difficult optimization problems involving multi-dimensional and multi-modal functions.  We focus on pattern search as our local routine and treed Gaussian processes as our statistical emulator.  We develop an implementation in an asynchronous parallel computing environment, and allow for both known and hidden constraints.  We also explore optimization of stochastic functions and optimization under uncertainty, as well as robust optimization.<br/><br/>This project solves difficult optimization problems by combining tools from the fields of applied mathematics and statistics.  By intertwining efficient local numerical routines with global statistical emulators, we develop efficient and robust algorithms for maximizing or minimizing functions.  This inherently inter-disciplinary work has immediate applications in a wide range of fields, and we demonstrate its effectiveness on problems from electrical engineering and hydrology.  <br/><br/>"
"0907185","Collaborative Research:  New MCMC-enabled Bayesian Methods for Complex Data and Computer Models Applied in Astronomy","DMS","STATISTICS","07/15/2009","07/19/2009","Xiao-Li Meng","MA","Harvard University","Standard Grant","Gabor Szekely","06/30/2013","$378,426.00","","meng@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","0000, 6890, OTHR","$378,426.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The California-Boston AstroStatistics Collaboration is developing a new model-based strategy for statistical inference that embeds computer models into multilevel models that explicitly account for complexities of both astronomical sources and the data generation mechanisms inherent in new high-tech telescopes. The resulting highly structured models must be fully utilized in order to learn about the underlying astronomical and physical processes. This strategy requires state-of-the-art scientific computation, advanced methods for statistical inference, and careful model checking procedures. The Collaboration has a track record using these methods to solve outstanding data-analytic problems in astronomy. In addition, the PIs (van Dyk, Meng, and Yu) have substantial research experience in developing the methods that the Collaboration will extend, employ, and publicize: inferential and efficient computational methods under highly-structured models that involve multiple levels of latent variables and incomplete data. Such models are ideally suited to account for the many physical and instrumental filters of the data generation mechanism in high-energy astrophysics. The five astronomers (Chiang, Connors, Kashyap, Kelly, and Siemiginowska) all have expertise on the instrumentation and science of high-energy and/or optical astronomy, and, all have collaborated with statisticians in efforts to develop appropriate methods to address scientific questions. The collaboration specifically aims to develop a mixture of parametrized and flexible multi-scale models that can be combined with complex computer-models to describe spectral, spatial, and timing data, either marginally or jointly. The models are developed in a fully Bayesian framework that allows us to incorporate external information, provide coherent estimates of uncertainty, and calibrate statistical comparisons of proposed underlying physical models. These methods require the Collaboration to develop new sophisticated statistical computing techniques for Monte Carlo exploration of complex and often multi-modal posterior distributions.<br/><br/>In recent years, technological advances have dramatically increased the quality and quantity of data available to astronomers. Newly launched or soon-to-be launched space-based telescopes are tailored to data-collection challenges associated with specific scientific goals. These instruments provide massive new surveys resulting in new catalogs containing terabytes of data, high resolution spectrography and imaging across the electromagnetic spectrum, and incredibly detailed movies of dynamic and explosive processes in the solar atmosphere. The spectrum of new instruments is helping scientists make impressive strides in our understanding of the physical universe, but at the same time generating massive data analysis challenges for scientists who study the resulting data. The complexity of the instruments, the complexity of the astronomical sources, and the complexity of the scientific questions leads to many subtle inference problem that require sophisticated statistical tools. For example, data are partially missing, are subject to varying measurement errors, and are contaminated with irrelevant artifacts. Scientists wish to draw conclusions as to the physical environment and structure of the source, the processes and laws which govern the birth and death of planets, stars, and galaxies, and ultimately the structure and evolution of the universe. Sophisticated astrophysics-based computer-models are used along with complex mathematical models to predict the data observed from astronomical sources and populations of sources. The California-Boston AstroStatistics Collaboration aims to tackle outstanding statistical problems generated in astrophysics by establishing frameworks for the analysis of complex data using state-of-the-art statistical, astronomical, and computer models. In so doing the Collaboration will not only develop new methods for astronomy but will also use these problems as a spring board in the development of new general statistical methods, especially in signal processing, multilevel modeling, computer modeling, and computational statistics."
"0854973","FRG: Collaborative Research: Statistical Inference for High-Dimensional Data: Theory, Methodology and Applications","DMS","STATISTICS","08/01/2009","06/13/2011","T. Tony Cai","PA","University of Pennsylvania","Continuing Grant","Gabor Szekely","07/31/2013","$851,270.00","","tcai@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, 1616, OTHR","$0.00","The analysis of high-dimensional data sets now commonly arising in scientific investigations poses many statistical challenges not present in smaller scale studies. Extracting information with precision from such data is becoming ever more important. This FRG proposal is the PIs' unified effort to respond to the pressing scientific needs. Specifically, The goals are to develop a comprehensive theoretical framework and general methodologies for estimating a large covariance matrix and its functionals and for functional data regression where the predictors and/or the responses involve functional measurements, and to address a wide range of important applications in biomedical studies.  <br/><br/>The statistical and scientific objectives outlined in this proposal are at the intellectual center of a rapidly growing field in statistics and biostatistics. The new technical tools, inference procedures, and computing algorithms for analyzing high-dimensional data will greatly facilitate scientific investigations in a wide range of disciplines, These fields include astronomy, biology, chemistry, bioinformatics, and particularly in medicine. The proposed efficient analytical procedures  hold great potential in deriving more accurate prediction rules for clinical outcomes based  on new biological and genetic markers and thus may lead to a better understanding of disease processes. Research results from this proposal will be disseminated through the workshops and seminar series such that the methods would be publicly available to researchers in other disciplines. Software tools developed will be made freely and publicly available as open source code. The proposed project will also bring high-quality training to students and postdoctoral researchers."
"0905570","Periodic Stochastic Processes","DMS","STATISTICS","09/01/2009","08/31/2009","Robert Lund","SC","Clemson University","Standard Grant","Gabor Szekely","08/31/2012","$99,999.00","","rolund@ucsc.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","MPS","1269","0000, 9150, OTHR","$0.00","The investigator will study several stochastic process problems in<br/>settings where the observations have a periodic structure.  These problems<br/>include 1) assessing whether or not two periodically stationary time<br/>series have the same dynamics, 2) the development of models for count<br/>series in a periodic environment, and 3) assessing trends in periodic<br/>storage processes.  To do this, the investigator will investigate periodic<br/>versions of some classical Markov chain, renewal process, and time series<br/>models.  Parameter estimation procedures for the developed models,<br/>including issues of parsimony, will be studied.<br/><br/>Most processes observed in everyday life --- tides, temperatures, gasoline<br/>prices --- have a seasonal (periodic) component.  This research will<br/>develop rigorous statistical procedures to analyze several types of<br/>periodic data.  The applications are aimed at answering several climate<br/>change questions including 1) are Atlantic Basin hurricane counts<br/>increasing (periodic series of counts)?, 2) are daily snow depth series in<br/>the United States showing change (periodic storage processes)?, and 3) can<br/>the climate record from two neighboring towns be declared the same or are<br/>the two localities different in some aspect (equivalence of periodically<br/>stationary time series dynamics)?<br/><br/><br/>"
"0906795","Inference for high-dimensional and multivariate data","DMS","STATISTICS","09/01/2009","07/27/2011","Peter Hall","CA","University of California-Davis","Continuing Grant","Gabor Szekely","08/31/2013","$199,376.00","","pghall@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","0000, OTHR","$0.00","The research program will develop and explore new methods, both parametric and nonparametric, for solving a variety of problems in multivariate and high-dimensional statistics.  In particular it will study models for the stochastic fluctuations of rankings and for assessing the reliability of rankings.  It will develop nonlinear methods for variable selection, and for quantifying the association among variables, in very high-dimensional problems.  And it will take up in detail the problem of defining and accessing, in an empirical way, the notions of probability density and mode for functional data.  <br/><br/><br/>Problems involving rankings arise in a very wide variety of contexts.  Examples include the ranking of universities or other institutions on the basis of measures their performance.  However, relatively little is known about the reliability of rankings, which after all are based on empirical evidence which is subject to error.  The research program will study this matter in depth.  Methods for variable selection are required in an increasingly wide range of practical problems, for example the selection of genes in terms of their apparent influence on disease and mortality.  The research program will focus on this problem in relatively complex cases, where the relationship between the variables and health issues is unusually complex.  Functional data, for example where the data are recorded in the form of curves rather than numbers, are encountered in fields as diverse as assessing the protein content of wheat to exploring ways in which the characteristics of climate change over time.  However, the extent to which conventional statistical notions, such the concept of the `most likely data value,' are valid for such data is not well understood.  The research program will remove this obscurity."
"0907622","Dimensionality reduction methods in multivariate time series","DMS","STATISTICS","09/01/2009","07/27/2011","Prabir Burman","CA","University of California-Davis","Continuing Grant","Gabor Szekely","08/31/2014","$174,981.00","","burman@wald.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","0000, OTHR","$0.00","Multivariate time series is an active area of research in many academic disciplines such as Economics, Environmental Science, Finance, and Statistics among others. Since many observed series are not low dimensional, traditional methods lead to models with poor performance because too many parameters are estimated. Statistical methods in this proposal deal with these issues. Procedures proposed here bypass the generalized eigenvalue problems associated with traditional methods for dimensionality reduction which create serious difficulties in the theory and practice of data analysis. Simultaneous parsimonious modeling of signal and noise is addressed. An entirely new method based on exponential prediction is outlined in this proposal.<br/><br/>There is substantial interest among researchers in building parsimonious models and in developing effective methods for prediction. This proposal has been motivated in part by a need to analyze air pollution and macroeconomic data. In the study of air pollution, it is important to model multivariate time series consisting of pollutants, particulates and meteorological variables, and their impact on cardiovascular and respiratory mortality. Information obtained from such analyses has implications for policy on public health. The results of this research will be used to analyze air pollution and macroeconomic data. More generally, this proposal addresses issues in analysis of high dimensional time series that have wide applications. The educational aspect of this proposal includes monitoring and teaching graduate students the theory and practice of multivariate time series.<br/>"
"0906858","Two Problems in Statistical Inference","DMS","STATISTICS","09/01/2009","09/04/2009","Weizhen Wang","OH","Wright State University","Standard Grant","Gabor Szekely","08/31/2012","$102,811.00","","weizhen.wang@wright.edu","3640 Colonel Glenn Highway","Dayton","OH","454350001","9377752425","MPS","1269","0000, OTHR","$0.00","When making inferences about parameters using the confidence interval (CI) or the hypothesis test (HT), typically, the CI provides more information about the parameter, but is hard to construct; while the HT has a relatively easy construction,  but does not provide precise information about the location of parameter as the CI does. The principle investigator (PI) tries to resolve these two problems (at least to a certain degree) by i) providing a construction method of the CI based on coverage probability, and ii) a generalization for the HT. There have been many efforts to derive a CI since it was first proposed by Laplace in 1814. There are five methods for the CI construction: pivotal quantities, inversion of tests, guarantee intervals, Bayesian method and invariance. But none of these is based on the analysis of coverage probability, which, however, is all one needs in the definition of CI. The development of such a method is one goal of the proposal. In fact, by focusing on the coverage probability, optimal CIs, including the smallest CI (a subset of any other CI), can be constructed within a certain class of intervals, and the smallest interval automatically minimizes the expected length and the false coverage probability. The PI will construct the smallest or admissible CI's using the criterion of set inclusion introduced by PI in 2006 under different scenarios. The traditional HT only deals with a two-choice problem. However, most applications involve a multiple-choice problem. In the second part of the proposal, the PI will generalize the HT procedure so that one is able to make a choice among more than two mutually exclusive claims. This can be done by first partitioning the basic alternative into multiple claims and partitioning the sample space correspondingly, then using the observed data to decide which claim is tested as the alternative, and finally conducting a traditional test for the selected claim. This new procedure provides flexibility to solve any multiple-choice problem. Various applications will be addressed, including traditional problems, such as analysis of variance, model selection, detecting small shifts in quality control, and some open problems, including the detection of active effects in nonorthogonal saturated designs. In short, almost all testing problems, except for those with a one-sided alternative, can be reconsidered using the new procedure, and different, more efficient results are expected.<br/><br/> A parameter is a certain quantity that describes the entire distribution of a population of interest, and inference about the parameter is one of the fundamental problems in Statistics. A simple but very useful example is to estimate the proportion (the parameter) of all patients (the population) who show improvement after taking a certain drug. As two major statistical inference tools for a parameter, the confidence interval (CI) addresses the ""what"" type of question and the hypothesis test (HT) answers the ""yes"" or ""no'' type of question. In spite of the tremendous progress in statistical theory and applications in recent years, the foundation of Statistics is not as solid as it should be. Some basic problems, including the comparison of two proportions, still do not have an ideal solution.  However, a fine solution for this problem would be very helpful to establish the superiority of a newly developed drug over the control more securely and more efficiently. As another case, a high dose of a drug typically has a severe side effect. So identifying the minimum dose level of a drug that is effective is an important issue for patients. This involves the comparison of several proportions for different dose levels with a common proportion of the control group. The main task of Statistics is to make estimations, predictions and decisions with measured precision and/or high probability of being correct based on the observed data. The ongoing research is an attempt to improve the understanding of Statistics from the root, and will lead to better or optimal solutions for the two problems mentioned above as direct applications.  More specifically, short confidence intervals will be constructed based on coverage probability, and the newly proposed testing procedure will be able to handle the multiple-choice problem."
"0906597","Bootstrap and Threshold Models in Non-standard Problems","DMS","STATISTICS","07/01/2009","06/14/2009","Bodhisattva Sen","NY","Columbia University","Standard Grant","Gabor Szekely","06/30/2012","$100,077.00","","bodhi@stat.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","0000, 6890, OTHR","$100,077.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>This proposed research deals with methodological and inferential strategies in some non-standard problems that arise in certain non-parametric scenarios. The ""non-standard"" problems include situations exhibiting non-standard asymptotics -- where estimators converge at rates different from the usual square-root-n rate and/or have non-normal limit distributions. In this proposal, the investigator studies three core directions of statistical research. These are: (A) (In)-consistency of different resampling methods in ""non-standard"" problems, (B) Estimation and inference with shape restricted functions (where knowledge on the shape of the function, like monotonicity/convexity, is incorporated in estimation), and (C) Estimation of an appropriate ""threshold"" in the domain of a function where sharp and potentially substantial changes (""regime changes"") occur. There is an inherent lack of ""smoothness"" in these problems (sometimes called the ""sharp-edge effect"") that manifests in the non-standard rates of convergence and the non-normal limit distributions. Statistical inference in these ""non-standard"" problems is difficult as the asymptotic distribution theory is complex (and in some cases unknown) with complicated limit distributions, containing nuisance parameters. Bootstrap methods are a natural alternative and are generally reliable in ""regular"" square-root-n convergence problems. Although there has been extensive activity in the last two/three decades in understanding the behavior of bootstrap in different ""regular"" scenarios, there has not been much work in such ""non-standard"" problems, justifying the research projects undertaken in the proposal. <br/><br/>The study of the problems has been greatly stimulated by an astronomy collaboration investigating the dark matter content and distribution in dwarf spheroidal (dSph) galaxies. Recent estimates show that the universe consists of about 96% dark matter and dark energy, though very little is known about them as yet. The dSph galaxies occupy a special position in this study -- they are supposed to be the smallest systems containing dark matter, and hence the study of these galaxies is of considerable importance in understanding the structure of the universe. The proposed research will also have diverse other applications, ranging from disciplines in public health like biomedical studies and epidemiology to aspects of the social sciences, especially economics. This is because ""non-standard"" problems arise naturally in the analysis of productions of firms/companies (economics), the study of the risk of succumbing to illness or infection with age (biomedical research), in the investigation of ""sensitive"" time periods (affecting health) in the early development of infants (epidemiology), and so on. The frontiers of the proposed research can be extended through incorporation in the Ph.D. level curriculum. Such interdisciplinary research will open up avenues of investigation in other realted areas of universal interest."
"0905561","Development of Statistical Methods for High-dimensional and Complex Data","DMS","STATISTICS","07/01/2009","02/20/2013","Yichao Wu","NC","North Carolina State University","Standard Grant","Gabor Szekely","06/30/2013","$120,000.00","Yichao Wu","yichaowu@uic.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","0000, OTHR","$0.00","As technology advances, scientists are challenged by more and more high-dimensional and complex data. For example, genetic data from microarray experiments are very large in size and new techniques are needed to identify specific genes for various diseases. Longitudinal data on various variables over time on millions of individuals produce interesting challenges. Such data call for new statistical techniques. Some of the challenges in high-dimensional data include variable selection from a large group of variables. Some of the existing methods also suffer from a high false discovery rate. In addition, in quantile regression methods, an odd phenomenon of quantile crossing needs to be addressed. Finally, spatial and longitudinal studies require special efficient methods for estimating the covariance patterns. Motivated by different features of high-dimensional or complex data, the PI develops several methods. In this grant application, the PI proposes: 1. new techniques for variable selection for high-dimensional data and new methods to reduce the false discovery rate; 2. new techniques to handle the phenomenon of quantile crossing with application to class probability estimation; 3. new methods to estimate covariance structure for spatial data and longitudinal data; and 4. parametrically guided nonparametric estimation for the quasi-likelihood method. The proposed methods will be studied theoretically for their asymptotic behavior and compared with some of the existing methods both theoretically and through simulations.<br/><br/>High-dimensional variable selection techniques are called for by many scientists to efficiently analyze large-scale complex financial, environmental, and biomedical data such as gene expression, proteomics and metabolomics, or brain imaging data. These types of data require techniques to identify important features. To achieve this goal, the PI proposes a screening method to select appropriate statistical models. This screening method can be applied to biomedical data to locate important genes responsible for diseases of interest such as breast cancer and leukemia. Spatial and longitudinal data are sparsely and irregularly observed also in environmental and clinical studies. For such data, many efforts have been devoted to studying their covariance structures. In this proposal, the PI proposes a flexible convolution-based method to estimate covariance structures nonparametrically. This method can be applied to many environmental data such as precipitation and wind to improve our understanding of environmental changes including the well-known ""climate change"" issue. This research has many societal applications. In addition, the PI takes advantage of the mentoring program in the department to work with US doctoral students, especially women and minorities. The PI also works with undergraduates from NSF-CSUMS program in the department as it is important to train computationally strong critical thinkers for the future."
"0907009","Collaborative Research: Models for Network Evolution:  A Study of Growth and Structure in the Wikipedia","DMS","STATISTICS","09/01/2009","08/21/2009","Edoardo Airoldi","MA","Harvard University","Standard Grant","Gabor Szekely","08/31/2011","$59,485.00","","airoldi@temple.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","0000, OTHR","$0.00","The project will develop new statistical models for network growth and change, and apply these to study the evolution of the Wikipedia.  The research builds on latent factor models for social networks and recent advances in variable selection and cluster analysis in high dimensions.  Using information on the text in Wikipedia entries and its current connectivity structure, the research will estimate where new entries will appear and characterize the local graph structures in different regions of the hyperlinked data set.   Although the models are tuned to the Wikipedia, the methodology has general relevance to the study of complex networks.<br/><br/>The Wikipedia is a unique mirror of human knowledge  It has grown quickly, and this growth continues.  From the standpoint of understanding how humans organize information, it is important to identify the ""holes"" in the Wikipedia, where new entries will arise.  Similarly, one wants to know whether information on, say, Henry VIII is organized in the same way as information on Homotopy Theory.    Both kinds of  questions can be  analyzed statistically, using publicly available version control data that has been archived to help discover Wikipedia vandalism.  The research has direct impact on the study of the structure of human knowledge, and indirect impact on the study of change in complex networks."
"0904548","Statistical Models of Internet Traffic and Queueing Analysis","DMS","STATISTICS","08/01/2009","08/02/2009","Bowei Xi","IN","Purdue University","Standard Grant","Gabor Szekely","07/31/2011","$54,900.00","","xbw@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","0000, 6890, OTHR","$54,900.00","<br/>This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>The investigator will construct different statistical models for the general Internet traffic and traffic from one extremely time-sensitive application, Voice over Internet Protocol (VoIP), due to the different traffic generation mechanism: A VoIP call creates a unique ON-OFF traffic process and the superposed VoIP traffic is generated following a call arrival process. On the other hand the general Internet traffic is a superposed packet stream where individual connection can be treated as a marked point process. The statistical models for both types of traffic will reflect the effect of superposition of multiple connections. The number of connections will be an important parameter in all the models under consideration. Stochastic processes theory will be extensively used to create statistical models of the non-linear long range dependent Internet traffic. The investigator will conduct a queueing study for VoIP, using the proposed VoIP traffic model as the arrival process at a queue. The queueing study will not be limited to the tail probabilities, and will provide results for the entire output process to better understand the effect of superposition on queueing behavior.        <br/><br/>Statistical models of Internet traffic are critical for computer network design and capacity planning purpose. Parsimonious models which accurately capture the Internet traffic properties will assist network engineers to design future networks with the ability to accommodate user traffic growing quickly beyond the scope of what is being observed today. The models will have simple structure that provides intuition and formulas for the traffic statistical properties, and serve as a tractable basis for further mathematical study of the Internet traffic. Because Quality of Service (QoS) criteria for the general Internet traffic can be a little relaxed while VoIP must comply with very strict QoS standards, the queueing study for VoIP will demonstrate how much VoIP traffic a network can support while maintaining the QoS standards. The proposed research requires knowledge in both statistical theory and computation.  Students directly involved in the project will gain experience of statistical computing, and learn the knowledge of stochastic processes and queueing theory."
"0926664","Sixth International Conference on Extreme Value Analysis","DMS","PROBABILITY, STATISTICS","05/01/2009","04/27/2009","Richard Davis","NY","Columbia University","Standard Grant","Gabor Szekely","04/30/2010","$20,000.00","","rdavis@stat.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1263, 1269","0000, 7556, OTHR","$0.00","This grant will used to support the Sixth International Conference On Extreme Value Analysis, to be held in June of 2009 at Colorado State University, Fort Collins, Colorado.  While extreme value theory has always played an important role in probability and statistics, recent events such as the economic meltdown of the international banking system and the heightened threats of climate change  have underscored the pressing need to develop and expand methodologies in extremes to tackle these new and critical challenges.  Many of the first applications of extremes were tied to hydrology.  Engineers were interested in designing dams that would cope with maximum river flow.  This led naturally to the concept of a one hundred year flood.  Analogues of the concept of the 100 year flood have spread to atmospheric science, where one may be interested in the 100 year rainfall, and to finance where the 100 year flood is replaced by the 10 day or 1 year VaR (value at risk).  <br/><br/> <br/><br/>In the last ten years there has been a rapid development of new theory and methodology in extreme value theory.  This has been motivated by applications in climate and atmospheric science; geosciences;  hydrology; finance, economics, and insurance; and telecommunications and stochastic networks.  Recent events such as the economic meltdown of the international banking system and the heightened threats of climate change  have underscored the pressing need to develop and expand methodologies in extremes to tackle these new and critical challenges.  The ? Sixth International Conference On Extreme Value Analysis? will bring together a wide range of researchers, practitioners, and graduate students whose work is related to the analysis of extreme values.  All aspects of risk and extreme value analysis and their applications will be included with participants  from a variety of disciplines in the physical sciences, engineering and economics.  There will, however, be  greater focus on more pressing needs in atmospheric science and finance."
"0906818","A New Paradigm for Multiple Correlated Outputs Given Dissimilarity and Other Information From Multiple Sources","DMS","STATISTICS","07/01/2009","02/22/2012","Grace Wahba","WI","University of Wisconsin-Madison","Standard Grant","Gabor Szekely","09/30/2013","$582,405.00","","wahba@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","0000, 6890, OTHR","$582,405.00","<br/>This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/>The objective of this research is to develop novel/improved methods for statistical model building and risk factor estimation when the training set has complex continuous and discrete attribute variables and including, but not limited to extremely long attribute vectors of which only a small set of interacting variables are believed to be relevant, but it is not known which. In addition, noisy relationship or dissimilarity information may be known between sufficiently many pairs of training set members, along with multiple correlated Bernoulli outcomes. In this context the objective is to model relations between the attribute and dissimilarity information and the multiple Bernoulli outcomes, as well as their correlations and conditional relationships. In particular the work will be concerned with issues related to developing tuning procedures for the multiple correlated Bernoulli case with heterogenous input data and penalty functionals, variable selection problems in this complex setup and the development of novel/improved computational tools to handle large data sets with complex optimization criteria.<br/><br/>With the availability of huge  amounts of data and high speed computing, modern statistical model building and data mining tools are doing impressive things to extract information in just about every scientific field from Astronomy to Zoology, not to mention problems in marketing, government, defense, health and the economy. Data sets in many areas of interest contain deeply embedded information relating to the risks of various outcomes, given complex inputs or observables.<br/>However the generation of huge, complex data sets in many fields of endeavor is beginning to outstrip the tools available for analyzing them. <br/>A new paradigm is proposed for development, which builds on previous results and which has as core the ability to deal with complex heterogenous data structures, to understand relationships between complex multivariate input information and complex multivariate correlated responses.<br/>The proposed work, when completed and disseminated, will provide a set of important and useful tools for new/improved statistical model building/data mining relating observational data in complex input structures to complex multiple correlated outcomes.<br/>"
"0854970","FRG: Collaborative Research: Statistical Inference for High-Dimensional Data: Theory, Methodology and Applications","DMS","STATISTICS","08/01/2009","09/23/2010","Tianxi Cai","MA","Harvard University","Continuing Grant","Gabor Szekely","07/31/2013","$142,987.00","","tcai@hsph.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","0000, 1616, OTHR","$0.00","The analysis of high-dimensional data sets now commonly arising in scientific investigations poses many statistical challenges not present in smaller scale studies. Extracting information with precision from such data is becoming ever more important. This FRG proposal is the PIs' unified effort to respond to the pressing scientific needs. Specifically, The goals are to develop a comprehensive theoretical framework and general methodologies for estimating a large covariance matrix and its functionals and for functional data regression where the predictors and/or the responses involve functional measurements, and to address a wide range of important applications in biomedical studies. <br/><br/>The statistical and scientific objectives outlined in this proposal are at the intellectual center of a rapidly growing field in statistics and biostatistics. The new technical tools, inference procedures, and computing algorithms for analyzing high-dimensional data will greatly facilitate scientific investigations in a wide range of disciplines, These fields include astronomy, biology, chemistry, bioinformatics, and particularly in medicine. The proposed efficient analytical procedures hold great potential in deriving more accurate prediction rules for clinical outcomes based on new biological and genetic markers and thus may lead to a better understanding of disease processes. Research results from this proposal will be disseminated through the workshops and seminar series such that the methods would be publicly available to researchers in other disciplines. Software tools developed will be made freely and publicly available as open source code. The proposed project will also bring high-quality training to students and postdoctoral researchers."
"0847647","CAREER: Random matrices and High-dimensional statistics","DMS","PROBABILITY, STATISTICS","08/01/2009","07/30/2013","Noureddine El Karoui","CA","University of California-Berkeley","Continuing Grant","Gabor Szekely","07/31/2015","$400,001.00","","nkaroui@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1263, 1269","0000, 1045, 1187, OTHR","$0.00","This research program is focused on the development of data analysis methods and of a theoretical framework for the new paradigm of high-dimensional statistical problems. The theoretical problems are concerned with spectral properties of large dimensional random matrices. More precisely, four of the main objectives of the program are: 1) further develop new covariance estimation methods;  2) further our understanding of the spectral properties of relevant large random matrices; 3) find and contribute to areas of application where this high-dimensional statistics framework is relevant; 4) train graduate students in high-dimensional statistics and make undergraduate students at least aware of possible pitfalls of classical methods and of better alternatives when available. More specifically, statisticians are now often faced with ""n by p"" data matrices X, for which p, the number of variables recorded per observations, is of the same order of magnitude as n, the number of recorded observations, and p and n are both large. The sample   covariance matrix computed from this data is of great importance to a number of applications, as it underlies widely used methods like principal components analysis. However, the theoretical results which underlie the method, classically developed in the ""small p and large n"" setting, fail to apply in the ""large n and large p"" setting just described. Hence, a thorough study of sample covariance matrices in this setting is needed. Eigenvalues of such large dimensional matrices are of particular interest. The investigator plans to launch a multi-pronged effort to get at various kinds of properties of these objects: for instance, he plans to develop theoretical results that will allow inferential work to be done from computation of extreme eigenvalues of sample covariance matrices, develop new methods of estimation of the whole covariance matrix, and also work on the impact of naively plugging-in the sample covariance matrix as a proxy for the population covariance in certain optimization problems which depend on this latter parameter. An effort will be made to try and apply this theoretical work to real-world problems, both to raise awareness in applied communities about the pitfalls associated with high-dimensional covariance matrices, and to shape the models that will be studied to be of most relevance to applied researchers.<br/><br/>Technological progress allows us to store and use massive amounts of data about many aspects of our daily lives. An interesting problem is to use the data to understand how certain traits depend on each other. In the stock market, we might be interested in how the behavior of one stock affects the behavior of another stock; understanding all these interrelationships leads to having a measure of the risk taken by investing in portfolios that use the corresponding stocks. Statisticians have a number of tools to deal with all these interrelationships. We can discover ways to look at the data so that, even if all interrelationships are small or weak, so each trait ""should"" not help us learn too much about any other trait, we might still find combinations of the traits that carry enormous amounts of information. We also know what typical values for these combinations are, so we might be able to detect unusual features in the data set by looking at it the right way. Those statistical techniques have very wide applications in various fields of science, ranging from climatology to genetics, image recognition, finance etc... Thousands of research papers are published each year that use these techniques. However, the theory that underlies these statistical techniques was created in an era where massive datasets just did not exist. This research project is focusing on theories and their applications that are better suited to handle our current massive datasets. The applications should allow us to see structure where the classical tools fail to see any and tell us when there is no structure when the classical tools tell us there is. We also have increasing evidence that our standard tools give us often very inaccurate results about our standard measures of risk or amount of information carried in combination of traits. It seems that risks might be underestimated and amount of information might be overestimated. Part of this research program will be dedicated to measuring how inaccurate the classical results are for large datasets, how much practical predictions are affected, and how a more relevant theory can be used for correcting these inaccuracies.<br/>"
"0906551","Empirical likelihood with infinitely many constraints","DMS","STATISTICS","09/01/2009","08/26/2009","Anton Schick","NY","SUNY at Binghamton","Standard Grant","Gabor Szekely","08/31/2012","$130,000.00","","anton@math.binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","MPS","1269","0000, OTHR","$0.00","A powerful method to do inference in models with finitely many parametric constraints is the empirical likelihood approach.<br/>This nonparametric maximization method was originally introduced by Owen in order to construct confidence regions for the underlying parameter.<br/>In the mean time the empirical likelihood method has been shown to also result in efficient estimation and testing. Needed are generalizations of this method that allow for semiparametric constraints and allow for infinitely many (parametric or semiparametric) constraints.<br/>The investigator extends the scope of the empirical likelihood into these two directions. This research advances the theory of estimation in semiparametric models and provides new methods to efficiently analyze data in a wide array of concrete problems. In the process technical problems of independent interest such a central limit theorems for quadratic forms with increasing dimensions are solved.<br/><br/>Semiparametric models are widespread in many fields that use statistics.<br/>Although this research is theoretical in nature, it has a strong practical impact by providing more effective inference methods for all those fields.<br/>For example, results on time series have applications in economic forecasting and in mathematical finance; results on bivariate models have applications in actuarial sciences and in medical research. In medical research bivariate data naturally arise as pre- and post-treatment measurements.<br/>The research will provide ample opportunities to prepare graduate students for careers in both industry and academics."
"0907491","Multivariate Nonparametric Methodology Studies","DMS","STATISTICS","08/15/2009","08/06/2009","David Scott","TX","William Marsh Rice University","Standard Grant","Gabor Szekely","07/31/2014","$100,000.00","Dennis Cox","scottdw@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","0000, OTHR","$0.00","The demands on statistical methodology have grown relentlessly as new technologies for data collection appear.  Many of the resulting datasets are unusual by statistical standards: massive and highly nonlinear with contamination.  Sometimes the data come from a mechanism which is only partially known. The tasks of estimation, testing, functional testing, pattern discovery, feature extraction, visualization, and comparison require the statistician look at each problem anew. Nonparametric methodology, which has been widely used in one and two dimensions, is also appropriate in these higher dimensions. Particular emphasis will be given to multivariate regression and density estimation problems, and closely related applications such as clustering, mixture estimation, pattern recognition, robust estimation, and dimension reduction.  The statistician's view of the scientific method is as a continuously improving process of model building, data collection, estimation, criticism, and refinement. However, many practicing statisticians are stymied by an inability to repair poorly fitting models. Of particular interest in this research are methods which provide critical diagnostic information as part of the model estimation task.  A focus of this research is a particular minimum-distance data-based parametric estimation algorithm, which has been investigated for its robustness properties. The algorithm can be applied to mixture models and spline fitting. An incomplete density model may be fitted, a unique capability that will be explored fully in the context of regression, image processing, clustering, outlier detection, and density estimation.  Nonparametric methodology for functional data analysis will be devised and tested with novel data sources.<br/>Other applications include adaptive wavelet thresholding, solution of the mixture of regression problems, and application to models which apply to only a subset of the data.<br/><br/>Research in data analysis and statistical modeling provides intellectual challenges with deep applications in almost every field of natural and social sciences and engineering. The field of nonparametric statistics has made a significant contribution to the success of science with algorithms that are hidden but critical even in the inner workings of cell phones.  At a recent National Research Council workshop, numerous scientists identified critical statistical needs in their work with massive data sets: new dimension reduction algorithms, specialized visualization tools for exploring massive data, better clustering algorithms, and techniques for handling nonstationary data.  Results from this proposed research directly impact three of these four critical opportunities.<br/>This program represents a comprehensive and long-term attack on a host of important data analytic problems in multivariate estimation.<br/>Graduate training is significant component of this project.  The results will be of long-term theoretical interest and will provide short-term solutions to real-world problems."
"0905656","Inference using Shape-Restricted Regression Splines","DMS","STATISTICS","07/01/2009","07/02/2009","Mary Meyer","CO","Colorado State University","Standard Grant","Gabor Szekely","06/30/2013","$180,000.00","","mary.meyer@colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1269","0000, 6890, OTHR","$180,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).  The PI develops methods in function estimation and inference, using shape-restricted regression splines.   The work includes three broad areas in estimation and inference.  First, generalized multiple regression models is investigated, where the mean response function is assumed to be smooth and have a shape restriction such as monotone or convex.  Second, a new maximum-likelihood method for smoothed unimodal density estimation is developed, that allows for heavy tails such as in the Pareto family of densities.  An application is a new robust regression method that estimates the error density estimation non-parametrically, simultaneously with the regression function.  Finally, the proportional hazards model is developed, where the hazard function is assumed to be smooth and have a shape restriction such as monotonicity or convexity.  <br/><br/><br/>Many problems in data analysis involve estimation of a function.  Standard methods require the specification of the function up to a few parameters, but the a priori knowledge about the function is often vague and qualitative.  For example, the researcher might know that a growth curve is smooth, increasing, and concave.  The expected number of nesting sites at a lake might be a decreasing function of some pollution measure.   Perhaps a hazard rate function is known to be increasing and convex, as in modeling wear-out of a mechanical part, or bath-tub shaped, as in modeling organ transplant failures.     Nonparametric methods in function estimation are appealing because they require minimal assumptions, but development of practical inference methods is more difficult.   Many methods that assume only smoothness of the function are sensitive to user-defined choices of the smoothing parameters such as bandwidth, number of knots, or penalty parameter, and the user can not rely on inference results that change with these choices.  However, when the researcher can also assume a shape such as increasing or convex, the fits to the data become more robust, for the simple reason that the ``wiggling'' associated with over-fitting is obviated.   The PI develops inference methods in three important areas: first, generalized regression models, such as when the response is a count or binary.  Second, a new method for robust regression is developed, where the error density is assumed to be unimodal and symmetric, to allow for either heavy-tailed or thin-tailed errors.  Finally, the proportional hazards model is developed, under shape and smoothness assumptions for the hazard function.  These models are often used in medical studies to compare treatments while accounting for possible mitigating factors, and in industry to model mechanical systems.    All three of these research projects result  in basic data-analysis tools that can be used in virtually any area of science.  <br/>"
"0905315","Models for Extremes on a Spatial Lattice","DMS","STATISTICS","07/01/2009","06/25/2009","Daniel Cooley","CO","Colorado State University","Standard Grant","Gabor Szekely","06/30/2013","$169,990.00","","Cooleyd@stat.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1269","0000, 6890, OTHR","$169,990.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>The investigator and his colleagues propose and study a model which can be used to characterize extremes (specifically threshold exceedances) on a regular spatial lattice.  A spatial model for threshold exceedance data needs to handle situations where data values exceed the threshold only in limited areas of the study region.  The proposed model accomplishes this by creating an overall model composed of many smaller models.  The spatial domain is covered by a number of small and overlapping subregions and data on these subregions is modeled with parametric multivariate extreme value models of low dimension.  The investigators show that these subregion models can be combined in such a way that the angular measure of the overall model meets the requirements of an extreme value distribution.  The idea of constructing an overall model from models of lower dimension is inspired by lattice models from spatial statistics, and the model's foundation is based on ideas from traditional extreme value theory.<br/><br/>Although they occur infrequently, understanding the nature of extreme events is important because of their significant human and economic impact.  Recently, there has been much interest in spatial modeling of extremes, particularly in the context of modeling geophysical data such as precipitation.  Despite the interest in this area, there still exists a need for extremes models which can describe the dependence in spatial data which are recorded at many locations.  The goal of this proposal is to develop and study a model for threshold exceedance data that is recorded on a spatial lattice.  While the research in this proposal is motivated by spatial, geophysical data, the methodologies could be utilized in extremes applications well beyond climate science. Because multivariate extremes models for high dimensions are lacking, advancements in this area are significant and are likely to be extended and adapted."
"0907562","Collaborative Research: Bayesian Hierarchical Methods for Modeling Chromosomal Spatial Correlation","DMS","STATISTICS","07/01/2009","06/29/2009","Guanghua Xiao","TX","University of Texas Southwestern Medical Center","Standard Grant","Gabor Szekely","06/30/2012","$74,954.00","","Guanghua.Xiao@UTSouthwestern.edu","5323 HARRY HINES BLVD.","Dallas","TX","753909020","2146484494","MPS","1269","0000, 6890, OTHR","$74,954.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>Recently, many genomic studies have shown that significant chromosomal spatial correlation exists in gene expression of many organisms. Ignoring such correlation in statistical modeling can greatly reduce the efficiency of estimation and the power of statistical inference. In this project, the investigators develop a set of new Bayesian hierarchical models to account for chromosomal spatial correlation in the following three areas of methodologies and applications: (1) incorporating the spatial correlation associated with linear chromosome structures into the analysis of gene expression data; (2) quantifying and inferring chromosome folding structures in vivo using gene expression data; (3) probing the global three dimensional structure of a chromosome in vivo. <br/><br/>This study provides statistical tools to explore three dimensional (3D) chromosome structures and directly addresses the important biological question that how the 3D structures facilitate the coordination in gene transcription. Not only can it generate new biological insights about spatial configuration of chromosomes, but also it can greatly improve the understanding of the relationship between the function and structure of chromosomes in living organisms. It also has potential clinical significance; for example, it provides tools to formally identify and compare chromosomal spatial patterns in gene expression from tumor and normal samples, which can discover subtle but coordinated changes in the tumor genome and lead to clinical insights on the underlying regulatory mechanism of cancer.  Besides scientific novelty and significance, this research fosters intensive collaboration among researchers from various fields including statistics, computational biology, experimental biology and clinical cancer research, and promotes intellectual interactions among participating institutions."
"0906739","Robust Frontier and Boundary Estimation: Theory and Application","DMS","STATISTICS","07/01/2009","06/23/2009","Lan Xue","OR","Oregon State University","Standard Grant","Gabor Szekely","06/30/2012","$32,873.00","","xuel@stat.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","MPS","1269","0000, 6890, OTHR","$32,873.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>This research project aims to develop robust estimation methods for various non- and semiparametric frontier models using the efficient polynomial spline quantile smoothing method. More specifically, the objectives of the proposed research include: to propose an easy-to-use robust estimation procedure for non- and semi-parametric frontier models; to develop an innovative penalized polynomial spline quantile regression for variable selection and efficient estimation of non- and semi-parametric frontier models; to study the asymptotic properties and develop efficient numerical algorithms of the proposed methods. The P.I. also plans to investigate polynomial spline estimations of flexible semiparametric ARCH models and formulate a nonparametric likelihood ratio test based on polynomial spline estimation to make inferences of the coefficient functions in the semiparametric ARCH model.<br/><br/><br/>The proposed research generates new methods and theories of curve fitting. The results of the proposed research are very useful for researchers in a wide range of fields. For example, it can be used to enhance the understanding of the impact of the macro economic variables on the stock prices and benefit investment banks to help improve risk management. Furthermore, the proposed research will be incorporated into teaching activities through development of a graduate course on nonparametric methods and time series analysis, which promotes involvement of students in the research of current sciences."
"0906808","Statistical inference when both the model and/or data dimension is large","DMS","STATISTICS","09/01/2009","08/24/2009","Peter Bickel","CA","University of California-Berkeley","Standard Grant","Gabor Szekely","08/31/2013","$519,903.00","","bickel@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","The investigator is studying inference for data which is both high dimensional and complex.The main topics investigated are:<br/>I. Identifying graphical models<br/>II.Ascribing explanatory power to variables<br/>III.Frequentist behaviour of nonparametric Bayes procedures<br/>IV.Particle filters<br/>The framework is non and semiparametric and the results  will be asymptotic.But the qualitative insights gained have led to the discovery of new methods by the investigator and should do so again.<br/><br/>A preeminent feature of 21st century data in almost all fields is their complexity compared to he number of replicates.Images can be viewed as vectors with dimensions in the thousands,climate models produce vectors giving  predicted values at tens of thousands of locations ,genomes are 3 billion basepairs long.Accompanying this type of data is  a dearth of models for their generation.What theory there is for such situations tells us that we should be unable to do anything without impossibly large numbers of replicates.Yet are coping,we believe because ,if we consider predictions the models are ""sparse""(Most factors are irrelevant) or data are ""sparse"" (The factors which do matter are highly dependent and  can in fact be represented much more compactly than is apparent.) The investigator is studying these underlying ideas and developing models which apply in contexts including  climate modeling, genomics, and astronomy.<br/>"
"0924257","Seventh International Workshop on Objective Bayesian Methodology; Philadelphia, PA","DMS","STATISTICS","07/01/2009","06/19/2009","Lawrence Brown","PA","University of Pennsylvania","Standard Grant","Gabor Szekely","06/30/2010","$20,000.00","","lbrown@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, 7556, OTHR","$0.00","The investigator will coordinate a five-day Objective Bayes workshop to be held at the Wharton School of the University of Pennsylvania. This workshop will bring together leading researchers from around the world who are active in the area of objective prior methodology. It may be viewed as a continuation of previous international meetings convened over the past 12 years devoted to Objective Bayes theory and methodology. The main objectives of the meeting are to facilitate the exchange of recent research developments within this scientific community, to provide opportunities for new researchers and underrepresented groups to be knowledgeable and active in this important area of research, and to establish new collaborations that will channel efforts into pending problems and open new directions for investigation. Objective Bayesian methodology is, for the most part, oriented towards the development of prior distributions that can be used automatically, i.e., that do not require subjective input other than the specific probabilistic model chosen to describe the data. Accompanying the development of forms particularly suitable for particular types of applications is the study of computational techniques for their implementation and evaluation of both the theoretical and concrete implications of their use. Many of the topics proposed for the workshop reflect an emphasis on objective Bayesian methodology for general classes of practical applications such as spatial-temporal models, hierarchical random-effects models, multiple comparisons and goodness -of-fit. In the direction of specific applications there will be sessions on Bayesian applications in astrophysics and on Bayesian applications in Business and Marketing research. In a more theoretical direction, there will be sessions on the foundations of objective Bayes analysis, including a special retrospective examination of Harold Jeffreys' seminal contributions to the area, and on the unification of Bayesian and frequentist statistical methods. The principal behind these so-called objective Bayes methods also has a long history dating at least back to work of Laplace almost 200 years ago. But two much more recent developments have brought emphasis and importance to the use of these methods. One is the rapidly increasing massiveness and complexity of modern statistical data. This has put extreme and often unmanageable strains on the theory and implementation of conventional non-Bayesian methods. (At the same time such complexity almost completely denies any reasonable possibility of formulating reasonable subjective prior opinions needed for ordinary Bayesian analyses.) The other parallel factor is the development within the past two decades of a suite of mathematical and computational strategies for successfully computing appropriately structured objective Bayesian procedures in highly complex situations. Within this time span an expanding community of scholars has been investigating these objective Bayes models and methods. The current conference proposal is to organize a workshop devoted to the investigation of objective Bayes methods."
"0951689","Workshop on Categorical Data Analysis","DMS","STATISTICS","12/01/2009","12/01/2009","Linda Young","FL","University of Florida","Standard Grant","Gabor Szekely","11/30/2010","$9,880.00","Alan Agresti, George Casella, Michael Daniels","LJYoung@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","0000, 7556, OTHR","$0.00","Categorical data are collected in numerous applications. As computing resources and power have become widely available, a comprehensive set of categorical data methods has begun to emerge.  Yet, important open research questions remain. As increasingly large categorical data sets are collected, challenges of analyzing high dimensional data become more common. Assessing the correlation structure and properly modeling it are more problematic for categorical data than for normally distributed data. Similarly, when missing data are non-ignorable, generalized estimating equations usually result in biased estimators. Mixed models and smoothing techniques for categorical data are important areas of research. Bayesian methods have become popular for model averaging and model selection procedures. An area of particular interest now is the development of Bayesian diagnostics (e.g., residuals and posterior predictive probabilities) that are a by-product of fitting a model. In this workshop, research leaders and young researchers in the field of categorical methodology will be brought together. Eleven leaders will present lectures at the cutting-edge of research. Young researchers will have the opportunity to present posters show-casing their research and to visit extensively with seasoned researchers.<br/><br/>Numerous applications give rise to categorical data, e.g. sex, race, and education level. Many of the traditional statistical methods assume that the data are from a normal distribution and are not appropriate for categorical data. With the increased availability and power of computational resources, a comprehensive set of statistical methods for categorical has been developed. However, additional statistical methods are needed. In genetics, large numbers of categorical responses are collected from each individual, leading to complex analytical questions. In many studies, some data are missing. This could arise because a person drops out of a study, refuses to respond, etc. Sometimes the missing data occur randomly, and the missingness may be ignored. At other times, the missing data is not at random, such as when a treatment makes people feel badly and they drop out. Such missingness cannot be ignored. These are but two research areas to be explored during the workshop. The workshop will provide an excellent opportunity to discuss the many recent significant developments in categorical methodology and to identify important problems and new research directions.  This workshop will identify the methods that seem to work best in areas of application driven by new technology. <br/>"
"0906482","Quantile Regression for Multivariate Time Series Models with Functional Coefficients","DMS","STATISTICS","07/15/2009","07/08/2009","Jiancheng Jiang","NC","University of North Carolina at Charlotte","Standard Grant","Gabor Szekely","06/30/2012","$99,999.00","","jjiang1@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","MPS","1269","0000, 6890, OTHR","$99,999.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5)<br/><br/>Quantile regression receives increasing attention in econometrics and statistics for its advantages over mean regression. For multivariate nonlinear time series, there is little solid mathematical theory on quantile regression in the literature, although much work has been contributed using the maximum likelihood or least squares estimation. In this research project the investigator develops spatial quantile regression modeling theory of multivariate nonlinear time series data with multivariate exogeneous variables. Several multivariate functional-coefficient models and associated estimation methods are proposed. From a theoretical perspective, the investigator and his colleagues study asymptotic properties of the estimators, variable selection, and parametric and nonparametric hypothesis testing for the proposed models, based on the global/local spatial quantile regression. The novel modeling approaches open a prosperous avenue of research in the multivariate nonlinear realm and are expected to stimulate others to address a number of problems which remain beyond the reach of existing models and techniques. The computational method for implementation of the proposed methodology is also considered.  <br/><br/>In financial markets, multiple time series are usually related. For example, the yields of three-month, six-month and twelve-month Treasury bills are highly related and exhibit co-movement.  For such multivariate time series data, one should use multivariate models. Although univariate models for each time series may be employed, they are not able to capture the relationship among different time series and may not be efficient. Since nonlinear features widely exist in economic data, it is important to develop some multivariate nonlinear modeling techniques. The investigator proposes flexible multivariate nonlinear models and introduces cutting edge techniques to refine the models and to achieve robustness and efficiency of estimation.  This is very important because it relaxes restrictive assumptions frequently used in statistical and economic research and hence enables us to achieve more accurate and realistic results. The proposed variable selection method is important because economic data often include many variables. Which variables should be chosen for the problems of interest? Decisions in variable selection are often arbitrary. The research will provide elegant methods to identify those relevant variables and enable investigators to make reliable decisions.  The proposed hypothesis testing methods are also important because they allow one to refine the models. After fitting a model, a relationship between variables is discovered. Is this discovery true in the real situations? With the aid of the proposed hypothesis testing methods, the question can be correctly answered with high probability, and hence the rate of error in discovery can be reduced. <br/>"
"0907017","Methods for High-Dimensional and Functional Data, with Applications to Mapping Human Brain Networks","DMS","STATISTICS","08/01/2009","07/26/2009","Philip Reiss","NY","New York University Medical Center","Standard Grant","Gabor J. Szekely","07/31/2012","$127,537.00","","phil.reiss@nyumc.org","One Park Avenue, 6th FL","New York","NY","100165800","2122638822","MPS","1269","0000, OTHR","$0.00","The application proposes a set of methods for analyzing high-dimensional and functional data.  Although motivated by problems in mapping the human brain and its network structure, the methodological innovations proposed are much more widely applicable.  The main objectives are to develop and disseminate novel methodology in the following three areas.  (1) Linear and generalized linear regression of scalar outcomes on image predictors: There has been considerable work in the functional data analysis literature on regressing scalars on one-dimensional functional predictors.  The investigator has pioneered an approach well suited to the more challenging problem of high-resolution image predictors, but much further work is needed in this area.  (2) Inferring networks from large covariance and correlation matrices: This includes multiple time series and time-frequency methods to infer connections among brain regions from functional neuroimaging data, and techniques for two-sample testing and discrimination with high-dimensional data.  (3) Analyzing complex outcomes based on the distances among outcomes: This includes distance-based reliability assessment and permutation tests that use distances to infer differences among groups.<br/><br/>In their quest to understand human brain function and mental health disorders, researchers in neuroscience and psychiatry are increasingly collecting large and complex data sets.  Such data sets may include large numbers of variables per subject, or even sequences of brain maps for each subject, as are produced by functional magnetic resonance imaging.  Novel statistical techniques play a central role in deriving scientifically useful information from these complex data sets, which may ultimately aid in the diagnosis and treatment of psychiatric disorders.  The methods developed in this research have potential applications in other scientific domains, including ecology and genetics. <br/><br/>"
"0907708","Analysis of Neuronal Spike Trains using Prototype Point Processes","DMS","STATISTICS","09/01/2009","08/19/2009","Frederic Schoenberg","CA","University of California-Los Angeles","Standard Grant","Gabor J. Szekely","08/31/2012","$100,001.00","","frederic@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","0000, OTHR","$0.00","The investigator develops tools for the summary and description of datasets involving catalogs of multiple realizations of point processes, using non-parametric techniques involving point process prototypes. Particular attention is paid to the case where each realization is a list of points in space and time, and a major characteristic of this proposed research involves the extension of prototypes and analyses based on metrics such as spike-time distance from one-dimensional point processes to the case of repeated realizations of point processes in higher dimensions. For such datasets, prototypes represent a useful summary of the behavior of the typical realization of the point process, and as such can be used to compare realizations from distinct classes of point processes. The methods investigated are entirely non-parametric, relying on minimal assumptons about the point processes being studied. The investigators apply their techniques to the neuroscience problem of summarizing and describing patterns in neuronal cell firings in conditioned and unconditioned subjects responding to various stimuli and to the description of typical spatial-temporal incidence of wildfires within seasons or years, as well as the characterization of other point process datasets such as earthquake catalogs.<br/>        The methods developed in this proposal are useful for detecting and summarizing patterns in data from a wide variety of applications where one records repeated observations of phenomena that seem to occur at random times and locations. Examples include the firings of neurons within the brain, global earthquakes and their aftershocks, and the locations and times of wildfire ignitions in Southern California. Prototype methods are incredibly useful for the description of such catalogs and, in the context of earthquakes and wildfires, can aid urban planners and emergency response personnel, structural engineers and those interested in insurance in describing what to expect in a typical realization. In the neuroscience setting, this project contributes to the understanding of the brain and its properties, as the methods explored here help to characterize the typical neuronal firing patterns among subjects receiving different stimuli or different forms of conditioning.<br/>"
"0907070","High-Dimensional Predictive Density Estimation","DMS","STATISTICS","07/15/2009","07/04/2009","Xinyi Xu","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","Gabor J. Szekely","06/30/2013","$113,851.00","","xinyi@stat.osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","This research concerns the development of a new methodology for predictive analysis, which extracts information from historical and current data to predict future trends and behavior patterns. The Bayesian approach is appealing for this problem because it provides a complete predictive density that assigns probabilities to every possible outcome, and it naturally incorporates the uncertainty inherent in the parameter estimation and model selection processes.  The prior specification for Bayesian predictive procedures, however, becomes challenging as the number of potential predictors grows, an all too common problem as massive data sets are increasingly prevalent in many scientific areas.  In this project, the PI investigates the use of Bayesian techniques for predictive density estimation and their frequentist properties, and then exploits those properties to construct new families of priors that have desirable risk properties, adapt to unknown data structures and also permit tractable computation for large and complex data sets. These priors are ""minimally informative"" in the sense that they allow input of subjective information through the choice of prior center, yet utilize this information in a very robust fashion.  The resulting predictive estimators effectively combine information from different dimensions and therefore improve overall prediction performance. Applications in financial and social problems will be developed using the new methodology.<br/><br/>Extracting information from massive data sets and exploiting it to make predictions of future uncertain events are fundamental problems in both statistics and the sciences.  The proposed research provides not only powerful theoretical tools, but also easily-implementable computing strategies for predictive analysis.  It can help researchers in various fields to better identify risks and opportunities, and thus to optimize their decision making. The methodological developments are motivated by a portfolio allocation problem in finance and a missing data imputation problem in the social sciences. The proposed procedures are applicable to many other scientific and technical areas, such as genomics, climatology, medical sciences and public health, where large data sets are collected and accurate predictive analysis is desirable.  For example, in health care service studies, predicting people's future health care costs is an important topic given a high concentration of health care expenditures among a relatively small percentage of the population.  Using the proposed methods, one may better exploit information in vast medical and insurance databases to identify the individuals with high health risks and to predict their future medical costs.  To facilitate the use of these new methods, the PI will implement the procedures and algorithms in R or Matlab, and make this software available to the public along with the associated research reports."
"0907710","Statistical Inferences for Deterministic Dynamic Models Containing both Constant and Time-varying Parameters with Applications to Infectious Diseases","DMS","STATISTICS","07/01/2009","06/05/2009","Jianwei Chen","CA","San Diego State University Foundation","Standard Grant","Gabor J. Szekely","06/30/2013","$89,472.00","","jchen@mail.sdsu.edu","5250 Campanile Drive","San Diego","CA","921822190","6195945731","MPS","1269","0000, 6890, OTHR","$89,472.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>Deterministic dynamic models have become very popular in modeling human immunodeficiency virus (HIV) dynamics, pharmacokinetic/pharmacodynamic analysis, tumor cell kinetics, and genetic networks. Current statistical methods for estimating unknown dynamic parameters in deterministic dynamic models from noisy data require intensive computation. This research develops new and efficient statistical estimation, inference methods and computational algorithms for deterministic dynamic models containing both constant and time-varying parameters. Three estimation procedures including kernel smoothing, discretization and spline methods are being investigated to solve challenging statistical problems in deterministic dynamic models. The investigator is focusing on the following three aims: (i) methodological and theoretical development of the semiparametric and nonparametric approach to the time-varying coefficient partially linear dynamic model and the time-varying coefficient dynamic model; (ii) modeling of HIV/Cell dynamics via these estimation techniques; (iii) efficient semiparametric and nonparametric estimation and algorithms in the partially linear dynamic system and the time-varying coefficient dynamic system. Asymptotic theory and simulation studies are being implemented to investigate the properties of the proposed methods, hypothesis testing procedures and adaptive bandwidth selection. The new procedures are being applied to AIDS clinical data. In addition to developing a number of innovative semiparametric and nonparametric techniques and useful deterministic dynamic models, this research also provides new insights into nonparametric inference. The new techniques being developed are statistically interesting beyond their direct applications to infectious diseases and will have significant impact on statistical thinking, methodological development, and theoretical studies.<br/>          <br/>Modeling dynamic systems for infectious diseases is critical for understanding pathogenesis of infection and providing guidance in the development of treatment strategies. It is of concern not only to researchers but also to decision makers developing plans for public health. The investigator's research provides valuable modeling diagnostic tools for biomedical researchers and practitioners to analyze and interpret clinical data with improved accuracy via dynamic models. The new statistical dynamic models and estimation techniques being developed will also be applicable to problems in engineering and for econometrics. The educational component of this research expands opportunities for students to learn about modern statistical modeling techniques and their applications in infectious diseases."
"0904165","Conference on Nonparametric Statistics and Statistical Learning; Spring 2010; Columbus, OH","DMS","STATISTICS","09/01/2009","03/09/2009","Joseph Verducci","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","Gabor J. Szekely","12/31/2010","$15,000.00","","verducci.1@osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, 7556, OTHR","$0.00","An international conference on Nonparametric Statistics and Statistical Learning is scheduled for May 19-22, 2010 at the Ohio State University. The conference is cosponsored by the American Statistical Association (ASA) Section on Nonparametric Statistics and the newly formed ASA Section on Statistical Learning & Data Mining.  This is the first and very timely effort to explore the synergy of these two areas in a single conference. The conference features six plenary talks by internationally prominent researchers whose work relates closely to both fields. Sixteen invited breakout sessions, each with three talks, cover additional topics with potential interest to both fields. These include Robustness of Statistical Learning Methods, Implications of Data Reduction by Data Depth, Nonparametric Bayesian Methods and Model Selection, Rank Based Methods for High Dimensional Problems, Designs for Variable Screening, Ranked Set Sampling and the Collapse of Importance Sampling in Very Large Scale Systems. There are also eight contributed paper sessions and two contributed poster sessions where junior investigators and graduate students are expected to participate.  Nonparametrics and Statistical Learning share key foundational structures. Both disciplines avoid unrealistic assumptions about underlying distributions or models in scientific studies; both allow for complex association among variables; and both address problems in data summarization, discovery, classification and prediction. The advent of powerful computers with accompanying massive data sets brings both disciplines to the forefront of statistical theory and practice. The goal of the proposed conference is to present some of the most important recent advances in these fields and to discuss future research directions. A major part of the conference focuses on bringing statistical research leaders together with students, postdoctoral fellows, and young academics in a stimulating environment. The funding from the NSF supports attendance of graduate students and junior researchers in American universities to present either a talk or a poster. The conference is expected to accelerate interactions and collaborations among researchers in the important areas of nonparametric statistics and statistical learning, and thereby lead to the development of new and more effective methods of modeling and inference.  <br/><br/>Statistical learning, with its roots in nonparametric statistics, pervades virtually all aspects of modern life.  From automatic interpretation of hand-written numbers to sort mail by zip code, to identification of which genes to target and which chemical features might best be incorporated into new medicines, to real time analysis of satellite images in predicting hurricane paths, to anywhere else there is massive data with many sources of variation, statistical learning has become the main strategy for putting the data to good use.  Additional applications include optimal allocations of credit, selection of product features to emphasize in different markets, and evaluation of concordant sources when determining the need to raise threat levels to Homeland Security. The conference features a session on national statistics with key speakers from the Bureau of Labor Statistics, the Census Bureau, and the defense industry, and Application sessions with speakers from the National Center for Atmospheric Research, the software industry, and other private sectors.  Interactions among academic, governmental, and industrial statisticians in the field should strengthen both research and practice."
"0904253","Topics in the Theory of Randomization II","DMS","STATISTICS","08/01/2009","07/23/2009","William Rosenberger","VA","George Mason University","Standard Grant","Gabor J. Szekely","07/31/2012","$134,000.00","","wrosenbe@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1269","0000, 6890, OTHR","$134,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5).<br/><br/>This project deals with several open topics in the theory of randomization. While the investigator''s previous NSF grant focused on asymptotic theory, this project deals with exact and Monte Carlo techniques, and proposes to solve problems regarding the properties of bias, balance, and inference with respect to randomized clinical trials. In particular, exact distributional properties of Efron's biased coin design will allow for a detailed inspection of selection and accidental biases, the exact balancing properties of the design, and exact computation of the variance of the randomization test. Monte Carlo techniques are proposed to deal with virtually every type of inference required from randomized clinical trials, including longitudinal outcomes, incorporating covariates either through covariate-adaptive randomization or through covariate-adjusted regression models, and sequential analysis of treatment effects. Under this complete theoretical and computational template, the practitioner will be able to determine appropriate Monte Carlo sample sizes and appropriate methods of determining power and preservation of type I error rates. Finally, the investigator will compare<br/>exact, Monte Carlo, and asymptotic procedures where appropriate.<br/><br/>This project will impact clinical medicine, and has the potential to change the way clinical trials are analyzed. The results will provide clinicians with the appropriate way to fully incorporate the design (randomization procedure) into the analysis. It will impact clinical trials practice by providing analytic tools to evaluate the properties of the trial design, which will allow the clinical trials personnel to choose the most appropriate designs that will minimize bias and variability of the trial results."
"0907241","Adaptive Designs","DMS","STATISTICS","07/01/2009","07/04/2009","Jay Bartroff","CA","University of Southern California","Standard Grant","Gabor J. Szekely","06/30/2013","$130,000.00","","bartroff@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","1269","0000, 6890, OTHR","$130,000.00","This award is funded under the American Recovery and Reinvestment Act of 2009 (Public Law 111-5). <br/><br/>The investigator will develop designs for statistical experiments that adapt over time to incoming data -- so-called ""adaptive designs"" -- and plans for analyzing data coming out of such experiments for two general classes of problems: (I) optimal parameter estimation, control, and design in multiperiod regression problems with nonlinear models (e.g., generalized linear models), and (II) time-sequential tests of multiple hypotheses. In Part I, recent computational advances known as approximate dynamic programming will be harnessed that hold promise for developing optimal or nearly-optimal estimation and control procedures in nonlinear regression models. In Part II, recent methodological advances will be used and extended to develop a unified approach to testing multiple hypotheses over time or in stages in a statistically optimal way, with either strong (FWER) or weak (FDR) error control. For both parts, the performance of the resulting procedures will be studied analytically, assessed through extensive numerical simulations, and applied to real data. Applications in economics, DNA microarray data, psychometric testing, biomedical trials, and engineering control problems will be addressed, and practical algorithms will be developed for real, on-line implementation in these areas.<br/><br/>Many statistical challenges of great societal importance require adapting one's actions quickly and intelligently as new information arrives over time. Some of these areas include solar energy, robotics, automobile emissions, homeland security, and health care. In this project the investigator will develop the statistical procedures and algorithms that underlie some of the most challenging problems in these areas. Part I of this project concerns regression problems, where the observer can influence the settings under which statistical information is generated, and how to design and change these settings over time in order to most efficiently learn about unknown parameters and control the effects of the chosen settings. Recent computational advances known as approximate dynamic programming hold the potential to solve previously intractable problems of this form. Part II concerns how to most efficiently combine statistical information from many disparate sources over time in order to reach justified conclusions, and how to avoid the multiple testing fallacy of false discoveries. The theory underlying these problems will be studied to develop data analysis and computational algorithms for real, on-line implementation, and software packages will be developed to facilitate applications."

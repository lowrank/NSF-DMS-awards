"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1811294","Collaborative Research: Theory and Methods for Highly Multivariate Spatial Processes with Applications to Climate Data Science","DMS","STATISTICS","08/01/2018","05/11/2018","William Kleiber","CO","University of Colorado at Boulder","Standard Grant","Gabor Szekely","07/31/2021","$92,717.00","","william.kleiber@colorado.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1269","1269","$0.00","Geophysical, environmental and ecological datasets often include many variables observed over a set of irregular geographical locations. While spatial datasets are increasing in size, they are also increasing in complexity with many variables being simultaneously observed, recorded, modeled or derived. Current methods in spatial statistics are unable to cope with such highly multivariate datasets; this research addresses this gap in statistical science, aiming to establish a new framework for multivariate spatial models. The testbed for the new framework is in the field of climate data science. Understanding of the Earth system relies on coupled physical models that represent the dynamic evolution of the atmosphere, ocean, land use, rivers, glaciers and other processes. These models have led to vast amounts of climate model data that severely constrain storage resources. Moreover, statistical emulators are increasingly common and desirable alternatives to running complex physical models directly. Development and validation of compression and emulation algorithms require understanding and maintaining complex dependencies between physical variables, but current tools are univariate or pairwise-based. This research will provide statistical guidance for climate data science applications.<br/><br/>This project focuses on a modeling framework for multivariate spatial processes, and relies on new theory incorporating graphical models in multiscale multivariate spatial process representations.  Moreover, many multivariate datasets exhibit non-Gaussian behavior.  A companion thrust of this work is in introducing and exploring empirical likelihood techniques for large multivariate spatial processes.  Finally, the proposed models and estimation frameworks will be applied to a climate dataset from the Community Atmosphere Model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1822820","Conference on Stochastic Weather Generators","DMS","STATISTICS, Climate & Large-Scale Dynamics","09/15/2018","07/16/2018","William Kleiber","CO","University of Colorado at Boulder","Standard Grant","Gabor Szekely","08/31/2019","$30,000.00","","william.kleiber@colorado.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1269, 5740","4444, 7556","$0.00","The Conference on Stochastic Weather Generators (SWGEN 2018) will be held in Boulder, Colorado on October 2-4, 2018.  The conference will be held at the National Center for Atmospheric Research.  Stochastic weather generators (SWGs) are mathematical or statistical algorithms whose simulated values capture the statistical distribution of weather or climate variables of interest. Weather and climate-related phenomena can have substantial impacts on human safety, for example flooding, heat waves and severe storms.  They also have significant societal and economic impacts: seasonal water resource planning, crop yield studies and weather volatility risk assessments all require high resolution simulations of relevant weather quantities such as temperature, precipitation, wind speed and solar insolation. Historically, SWGs were developed by domain scientists for applied projects such as statistical downscaling of climate models. Recently the applied mathematics and statistical science communities have taken increasing interest in their formal development, which feed back into novel modeling, estimation and simulation techniques that then warrant theoretical study. SWGEN 2018 serves as the first conference devoted to SWGs in the United States, and is intended to introduce a wide audience of mathematical, statistical and domain scientists to the current state of the field, disseminate novel techniques for precipitation, temperature, wind speed and solar radiation modeling, to forge new collaborative opportunities and to identify areas of future research.<br/><br/>The conference will consist of keynote lectures, a poster session and research talks featuring topics of major interest in mathematical and statistical communities for weather simulation including 1) modeling techniques for renewable energy applications such as wind and solar irradiance simulation, 2) modeling and simulation of complex nonstationary and inhomogeneous processes that evolve over space and time, and 3) large-scale spatiotemporally-coherent simulation methods. Generating realistic spatiotemporal weather processes requires novel techniques from areas such as spatial and computational statistics. These methods dovetail with current interests in the statistical community involving big data modeling, estimation and simulation techniques. Other topics will be considered, including nonlinear processes, high frequency data, multi-scale models, extremes, analog or resampling techniques, and non-Gaussian processes with applications in hydrology, agriculture, air quality, insurance and environmental engineering. Links between topics will be identified and discussed to better understand the role of mathematics and statistics in this field.<br/><br/>The conference website is at: https://www2.cisl.ucar.edu/calendar/swgen-2018-stochastic-weather-generators-conference<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812048","Nonparametric Inference and Prediction for Complex Data by Data Depth, Confidence Distribution and Monte Carlo Method","DMS","STATISTICS","08/15/2018","08/09/2018","Regina Liu","NJ","Rutgers University New Brunswick","Standard Grant","Yong Zeng","07/31/2022","$150,000.00","Minge Xie","rliu@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","In the era of information and data explosion, the demand of effective data analysis methods for solving problems and assisting decision-making has never been greater. This demand comes from all domains, from modern scientific endeavors, government and industry policy-making, financial and business strategic planning, to even the most basic social-economic studies. Despite recent great strides made in mathematical and statistical sciences, many new challenges have been brought to the fore by the need of confronting the pervasive massive, diverse and complex data. The PIs of this project will develop several novel approaches to addresses general inference and prediction problems in settings where data sources are diverse or where the conventional statistical large sample theory fails to apply.<br/><br/>Motivated by several real applications, this project will develop nonparametric approaches for: individualized inference from diverse data sources (referring to as i-Fusion), prediction for complex data, and exact inference for estimating equations. Underlying these proposed approaches is the common tool kit consisting of data depth, confidence distribution and Monte Carlo methods. The proposed approaches are expected to be broadly applicable, efficient and computationally feasible. Three specific projects are: A. Develop the new i-Fusion for drawing efficient individualized inference by effectively combining learnings from relevant data sources; B. Develop CD Monte-Carlo methods for the exact inference for estimating equations; C. Develop nonparametric predictive distributions for efficient prediction with complex data. The proposed methodologies will be developed with theoretical support and applied to the areas: i) prediction of volumes of application submissions to interrelated units in a government agency; and ii) performance forecast for individual companies by borrowing information possibly shared by others, and, potentially, iii) identification of hot spots in tracking glacial striation around the globe. These applications are motivated by the PIs' ongoing collaborative projects with the CCICADA of Department of Homeland Security, and possibly Rutgers Climate Risk and Resilience Initiative. These projects involve real databases and are ideally suited for engaging and training students and new researchers.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811661","Collaborative Research: Highly Principled Data Science for Multi-Domain Astronomical Measurements and Analysis","DMS","STATISTICS","07/15/2018","07/11/2018","Thomas Chun Man Lee","CA","University of California-Davis","Standard Grant","Gabor Szekely","06/30/2021","$100,000.00","","tcmlee@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","Massive data resources are coming online in every conceivable area of human exploration, and particularly in fields that are heavily observation-based such as astronomy and astrophysics. To extract the most information from these data, scientists and statisticians need to conduct highly principled data science, by using methods that are scientifically justified, statistically principled, and computationally efficient. This project outlines plans to achieve this goal while addressing four specific challenges in astronomical data involving space, time and energy.  The proposed research has the dual impact of more reliable statistical methods in astronomy and of new general statistical inference and computational methods. In addition to providing methods and free software, the investigators also plan to communicate to the astronomical community the benefit of principled statistical methods through workshops and sessions at conferences. A fundamental impact of the proposed research is the more general acceptance and use of principled methods among astronomers. The general methods for efficient modeling of scientific phenomena, science-driven classification and clustering, and for statistical computing, can also help to solve complex data challenges throughout the natural, social, medical, and engineering sciences.<br/><br/>Striking advances in both space-based and terrestrial instrumentation continuously increase the quality and quantity of data available to astronomers. Observations are made across the electromagnetic spectrum and compiled into enormous catalogs of high-resolution, but heterogeneous spectrograph, imaging, and time series data. The proposed research aims to use such multi-domain astronomical measurements to better understand the physical environment, structure, and evolution of astronomical individual sources, clusters, and ultimately of the entire universe. There are four major projects.  (1) The PIs will develop methodology to solve the instrument calibration problem, which is a fundamental challenge in astrophysics, by fitting scientifically motivated statistical models to data from multiple astronomical objects observed by multiple instruments. (2) The PIs propose a statistically and computationally efficient algorithm to detect the boundaries of a power law distribution prevalent in various areas of astronomy and of far-reaching importance. (3) The PIs will extend image-processing algorithms designed for detecting point sources to complex extended multi-scale structures via a post-hoc analysis, which makes the computation efficient. (4) With astronomical images exhibiting complex structure, the PIs propose to explore image segmentation methods to distinguish overlapping point sources; the algorithm achieves the flux-conserving property, which is crucial for giving physically meaningful estimates that existing methods lack. These projects all involve significant challenges in developing efficient statistical methods, designing fast computational algorithms, and balancing subtle trade-offs between complexity and practicality. With their extensive and successful track record, the PIs will address these challenges by developing inferential and efficient computational methods under highly-structured models that involve multi-scale structure and/or multiple levels of latent variables. The central theme of the proposed research is the integration and pursuit of three desiderata in each of its four projects: scientific justification, statistical principles, and computational efficiency. This triple-goal advances the development of specifically designed methods that leverage computationally efficient and statistically principled data-driven techniques which explicitly incorporate scientific understanding of the astronomical sources.  This ensures that the statistical analyses enhance the scientists' ability to answer specific questions about the underlying astronomical and physical processes. This strategy requires state-of-the-art statistical inference, sophisticated scientific computing, and careful model-checking procedures, all of which have been the hallmark of the work by this team of investigators.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811414","Theory and Methods for Inferring Causal Effects with Mendelian Randomization","DMS","STATISTICS","09/01/2018","05/15/2018","Hyunseung Kang","WI","University of Wisconsin-Madison","Standard Grant","Yong Zeng","08/31/2022","$200,000.00","","hyunseung@stat.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","In medicine, many investigators use observational studies as a starting point to uncover causal effects of risk factors on health outcomes. Mendelian Randomization (MR) is one popular approach to draw better causal conclusions from observational data.  In a nutshell, MR is a ""big data"" approach based on methods from economics. Specifically, MR adapts instrumental variables (IV) to large observational genome-wide association studies (GWAS) to determine causal mechanisms. Specialized MR methods and computer software have been developed leading to the widespread use of the approach, but to date little is understood about the statistical properties of these methods. For example, when do MR methods work well (and when do they not)? Other fundamental questions about the underlying causal mechanisms are also poorly understood.  This project will investigate the statistical theory and methods behind MR so that investigators can properly use MR to draw causal conclusions from observational data, leading to better medical and health decisions.<br/><br/>The MR approach requires finding genetic variants in GWAS, called instruments, that satisfy the following assumptions: (A1) Instruments are associated with the risk factor, (A2) Instruments have no direct effect on the outcome, and (A3) Instruments are unconfounded by variables affecting both the outcome and the risk factor.  Specific aims that will be investigated include (1) laying the theoretical foundations for MR including identification and statistical sufficiency, (2) studying the statistical properties, including efficiency, power, and robustness, of popular MR methods, such as the median method or the MR-Egger method, (3) proposing new MR methods that are robust to common problems associated with MR data, most notably violations of the three core assumptions (A1)-(A3), (4) developing numerical and visual diagnostic tools for assumptions underlying MR, and (5) producing easy-to-use software suites.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810960","Statistical Analysis Using Density Surrogates","DMS","STATISTICS","07/01/2018","07/15/2020","Yen-Chi Chen","WA","University of Washington","Continuing Grant","Gabor Szekely","06/30/2021","$100,826.00","","yenchic@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","","$0.00","Density surrogates are quantities that can help measure the likelihood of seeing observations with certain characteristics. This project aims at developing novel statistical approaches based on density surrogates and analyzing the underlying theoretical properties. Methodologies developed from this project will be applied to solving scientific questions such as investigating human activities using GPS data and detecting matter distribution inside our Universe.<br/><br/>This project focuses on developing new statistical methodologies for analyzing complex data sets using density surrogates. Density surrogates are quantities similar to the probability density that characterizes the underlying distribution of the observed random sample. Examples of density surrogates include the linkage criterion in a hierarchical clustering, radius of the k-nearest neighborhood, and expected value of a kernel density estimator. This project proposes several novel density surrogates and designs new statistical tools. These new density surrogates can be used to perform statistical analysis in situations where the conventional density-based approaches fail.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1749857","CAREER: Something Old, Something New: Robust Statistics in the 21st Century","DMS","STATISTICS, Division Co-Funding: CAREER","09/01/2018","08/22/2021","Po-Ling Loh","WI","University of Wisconsin-Madison","Continuing Grant","Yong Zeng","08/31/2023","$315,376.00","","polingloh@gmail.com","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269, 8048","1045","$0.00","Contemporary scientific problems involving large, high-dimensional datasets have ushered in a new era for statisticians. A largely unexplored area of research concerns incorporating ideas from robust statistics into the growing collection of methods for high-dimensional estimation and inference. Preliminary results hold much promise, but a plethora of theoretical and philosophical challenges abound when attempting to generalize notions from classical robust statistics to high-dimensional settings. The investigator will develop new statistical methodology for high-dimensional robust estimation and derive rigorous theory for the proposed estimators. This research project is highly interdisciplinary in nature, cutting across statistics, engineering, and computer science. Results of the research will be disseminated broadly, leading to cross-pollination between fields and revitalized interest in robust statistics. In addition, the investigator will refine and test her methods in radiology applications, instigating new scientific collaborations and leading to more robust medical imaging procedures for deployment in medical research. The investigator will also develop new educational material based on the research that will be incorporated into cross-listed classes in machine learning at the graduate and undergraduate levels. The investigator will work to improve the image of statistics and data science by engaging the wider community through public speaking engagements and visits to high school math circles across the state of Wisconsin.<br/><br/>Questions to be explored in this research project include: (1) How do existing notions of robustness apply to high-dimensional settings? (2) How should high-dimensional estimation procedures be modified to protect against deviations from distributional assumptions? (3) How might one quantify the relative robustness of various proposals? The project aims to generate novel theoretical results that advance the frontiers of both statistics and optimization theory. New algorithms will be devised for high-dimensional statistical estimation with guaranteed accuracy under a broader set of model assumptions. The theoretical analysis will involve studying a variety of non-convex estimators of independent interest in the optimization community, with emphasis on objectives and optimization algorithms that give rise to statistically consistent solutions. The research project will also address long-standing open questions in robust statistics involving optimization of low-dimensional non-convex objective functions, and the investigator will examine a variety of new problem settings arising in machine learning applications, including adversarially contaminated data, non-iid observations, and mislabeled datasets dichotomized into training and testing data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810880","Collaborative Research: Consistent Risk Estimation under High-Dimensional Asymptotics","DMS","STATISTICS","08/01/2018","08/01/2018","Kamiar Rahnama Rad","NY","CUNY Baruch College","Standard Grant","Gabor Szekely","07/31/2021","$69,999.00","","Kamiar.RahnamaRad@baruch.cuny.edu","1 BERNARD BARUCH WAY # D509","NEW YORK","NY","100105585","6463122211","MPS","1269","","$0.00","Learning from large datasets has been the cornerstone of modern innovations and discoveries in science, medicine, and technology. Fast prediction of unseen events is a canonical goal in statistical learning. A classic approach to this end is leave-one-out cross-validation, a time-consuming routine of leaving a datum out,  fitting the model on the rest, and testing it on the left out datum, repeatedly. The recent emergence of massive data has exacerbated the computational infeasibility of such approaches. Moreover, in many recent instances, the number of features per observation can be extremely large, adding another challenging facet to the fast estimation of prediction error. To overcome these problems a new set of scalable and consistent risk estimators will be developed in this project. <br/><br/>The importance of risk estimation has motivated this project of different schemes, such as cross-validation, Stein's unbiased risk estimation (SURE), Generalized cross-validation, Akaike Information Criterion (AIC), and Bootstrap. The emergence of high-dimensional datasets has challenged most classical approaches to risk estimation. For instance, the large discrepancy between in-sample and out-of-sample prediction error, in applications involving predictions based on previously unseen features, makes it hard to rely on popular estimators, such as SURE or AIC, in high-dimensional regimes where the number of predictors is smaller than or at the same order as the number of observations. On the other hand, the information value of a datum in these regimes (as opposed to the information value of a datum in low-dimensional settings) casts doubt on the reliability of other techniques, such as 5-fold cross-validation. The project offers a novel theoretical framework to find the middle ground between scalability and reliability, and specifically, to obtain theoretically consistent and computationally efficient risk-estimation schemes under high-dimensional settings. Since risk estimation is at the core of areas including but not limited to machine learning, signal processing, medical imaging, neuroscience, and social and environmental sciences, any success in this project will lead to reliable and immediate scientific discoveries and better learning systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833447","Symposium on Mathematical Statistics and Applications: From Time Series and Stochastics, to Semi- and Non-Parametrics, to High-Dimensional Models","DMS","STATISTICS","07/01/2018","05/17/2018","Frederi Viens","MI","Michigan State University","Standard Grant","Gabor Szekely","06/30/2019","$25,000.00","Vincent Melfi, Tapabrata Maiti, Weixing Song","viens@rice.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","7556","$0.00","This award supports participation in the conference ""Symposium on Mathematical Statistics and Applications: From Time Series and Stochastics, to Semi- and Non-Parametrics, to High-Dimensional Models"" held September 14 - 16, 2018 at Michigan State University. This conference is designed to benefit graduate students and early-career researchers in mathematical statistics. As the data revolution of the 21st century takes the US to new frontiers in data-based decision-making and predictive analytics in business and government, the role of the mathematically-trained and analytically-critical statistician remains more important than ever, to develop and analyze sound, innovative ways of understanding data science algorithms, methodologies, and procedures. The conference will bring together established and aspiring researchers from around the country and abroad to explore frontiers of mathematical statistics and a host of emerging applications. The exposure and networking opportunities provided by the conference to the dozens of graduate students and early-career researchers will be of great value, and potentially career-changing for some.<br/><br/>Foundational work to further develop the analysis and use of data in all its forms is underway in several areas, particularly in semi- and non-parametric statistics. Such mathematical development is needed across a wide spectrum of the statistical sciences, including high-dimensional statistical inference, non-parametric time series, functional data analysis, empirical processes, and machine learning. The conference will expose the participants to these topics as well as other directions, including asymptotic theory of adaptive and efficient estimation, time series in econometrics, and survival analysis. With ample time built into the schedule for discussions, the conference will give participants opportunities to engage in fruitful cross-group collaborations. A particularly impactful aspect of the conference is the inclusion of several presentations by local non-statisticians who work in areas typically underserved by statisticians but with very significant data analysis needs, all interested in interacting with statisticians from across the country with whom there would ordinarily be no chance to meet. Application areas covered will include quantitative finance, insurance mathematics and risk analytics, nuclear physics, agricultural economics, neuro-imaging, soil science, hospital management, hydrology, and others. The conference website is https://stt.msu.edu/MSUStatSymposium2018/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812054","Regression with Time Series Regressors","DMS","STATISTICS","08/01/2018","07/26/2020","Suhasini Subba Rao","TX","Texas A&M University","Continuing Grant","Yong Zeng","07/31/2022","$120,000.00","","suhasini@stat.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","1269","$0.00","The projects to be investigated are motivated by the following two different problems.  It is well known that sea ice in the Arctic is receding. Changes in climate are thought to be a contributing factor.  Therefore, it is important to understand if, when, and how daily temperatures in the Arctic region are impacting sea ice.  The second motivation comes from neuroscience. Is it possible to predict the decision a person makes based on biometric and neurophysiological responses, such as eye dilation observed over time or an EEG? The objectives in both examples are very different, however they are bound by a common theme; to understand how data observed over time (usually called a time series) affects an outcome of interest. These problems fall under the canopy of regression (a broad topic in statistics), which is a widely researched area in statistics. However, what distinguishes these problems from the classical regression framework is that the regressors have a well-defined structure, which is rarely exploited in most classical regression techniques. By modelling the time series, methods will be developed that exploit the structure of the time series. This will facilitate estimation in models that otherwise would not be possible. The approach will be used to identify salient periods in the time series that have the greatest impact on the outcome and can be used to physically interpret the data. <br/><br/>In recent years, there has been a growing number of data sets, from a wide spectrum of applications ranging from the neurosciences to the geosciences, where an outcome is observed together with a time series that is believed to influence the outcome.  Despite the clear need in applications, there exists surprisingly few results that exploit the properties of a time series in the prediction of outcomes. This project will bridge this gap by developing regression methods that utilize the fact that the regressors are a time series or are spatially dependent.  To achieve these aims, many new statistical methods will be developed. In signal processing, deconvolution methods are often used to estimate the parameters of a two-sided linear filter. This is because the deconvolution is computationally very fast to implement. However, there has been very little exploration on the use of deconvolution methods within the framework of estimating regression parameters. This project will develop deconvolution techniques for (i) linear regression models, and (ii) generalized linear models and when the regressors are stationary and locally stationary. The focus will be on the realistic situation where the time series or spatial data is far larger than the number of responses.  Besides the computational simplicity of deconvolution, by isolating the Fourier transform of the regression coefficients, diagnostic tools to understand the nature of the underlying regression coefficients will be developed. For example, the methods can tell whether the coefficients are smooth, contain periodicities, or are sparse.  The project will develop inferential methods for parameter estimators that allow for uncertainty quantification, construction of confidence intervals, and tests for linear dependence. Included is a new technique for estimating the variance of the regression coefficients.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1753171","CAREER: Calibrating Regularization for Enhanced Statistical Inference","DMS","STATISTICS","07/01/2018","10/21/2020","Daniel McDonald","IN","Indiana University","Continuing Grant","Yong Zeng","06/30/2020","$127,696.00","","dajmcdon@gmail.com","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","MPS","1269","1045","$0.00","Volumes of data that seemed unimaginable only a decade ago are now ubiquitous in scientific research. Investment decisions are based on prices, updated every microsecond, for thousands of securities; atmospheric scientists use multiresolution satellite images to understand climate change; and internet companies exploit massive music collections to infer trends in tastes and preferences. Rigorous analysis of these large datasets requires a balance between computational constraints and statistical performance, and operationalizing such tradeoffs involves a combination of algorithmic and design-based approximations. For example, decreasing the resolution of an image or subsampling high-throughput data enables faster computations but removes potentially valuable information. At the same time, elaborate explanations of the scientific process are credible because they account for real-world complexity, but estimating complex models requires both more data and larger computers. The proposed work investigates the viability of modern statistical and machine learning methodologies for answering applied scientific questions. The project will create new algorithms and open-source software for combining computational approximations with regularization for analyzing large datasets as well as providing theoretical justification for their statistical properties. A more complete picture of the interplay between statistical regularization, computational approximation, and scientific parsimony will enable fundamental scientific advancement. The PI will employ the methodologies developed in this project to facilitate novel science with large datasets in climate science, biology, music analysis, astronomy, economics, and chemistry. Furthermore, the PI will carefully integrate the research aims with educational and outreach objectives to engage elementary and high school music students, introducing them to modern statistics and computer science, as well as integrating underrepresented populations in research.<br/><br/><br/>Computational tractability and statistical efficiency for large datasets necessitate approximation or regularization, either of which heuristically balances fidelity to the data with scientific goals like parsimony, smoothness, sparsity, or interpretability. The PI seeks to elucidate the dual roles of regularization and approximation as tools for better scientific understanding. Current research in computer science has focused on improving algorithms so as to enable computation with minimal approximation. Meanwhile, statisticians have developed regularization techniques in order to take advantage of simple structures - graph topologies, sparse linear models, smooth functions - that, if representative of the truth, will improve inference and prediction. The challenge is to understand the consequences of this coupling for scientific interpretability. The PI seeks to address two important issues (1) how do we select tuning parameters when computations are at a premium? and (2) how does the accuracy and stability of scientific conclusions relate to the approximation/regularization methods used to obtain those conclusions? This project seeks to enable fundamental scientific progress by understanding the connections between computational approximations and statistical regularization, thereby facilitating improved inferences. The PI will fill this gap by deriving practical algorithms with accompanying theoretical justification under more reasonable statistical assumptions. These tools will be tightly coupled with applications in neuroscience, genetics, atmospheric science, and music.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1752614","CAREER: Computer-Intensive Statistical Inference on High-Dimensional and Massive Data: From Theoretical Foundations to Practical Computations","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2018","06/27/2022","Xiaohui Chen","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Yong Zeng","06/30/2024","$400,000.00","","xiaohuic@usc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269, 8048","1045","$0.00","In an era of Big Data, computer-intensive statistical inference faces unprecedented challenges and opportunities. High-dimensional and massive data are now emerging in scientific areas including biomedical engineering, environmental science, financial econometrics, array signal processing, and social networks, among many others. An important associated research challenge is to develop efficient methods to extract information and quantify its uncertainty for a large number of variables and measurements. Data-driven statistical inferential procedures for uncertainty quantification via the bootstrap methods are often computationally intensive for high-dimensional large-scale datasets. On the computational side, this research project will make use of distributed inference via the parallel high-performance computing technique, which is an essential ingredient to speed up bootstrap calculations. On the statistical side, this research will introduce a general framework for studying the performance of various bootstrap methods. This research project aims to lead to a comprehensive understanding of the fundamental tradeoff between statistical and computational concerns in quantifying uncertainty for a broad class of inferential procedures, thus providing guidance to practically optimize statistical accuracy and computational cost in potential real applications. Both undergraduate and graduate students are involved in the project.<br/><br/>The overarching goal of this research project is to provide new insights and deepen the theoretical understanding of strengths and fundamental limitations of fully data-dependent inferential procedures (such as bootstraps) in the high-dimensional and massive data framework on two classical problems: i) change point detection and identification; ii) computationally-aware statistical inference for U-statistics. The research aims to develop statistically correct and computationally scalable inferential procedures when the dimension can be larger (or even much larger) than the sample size. In contrast to existing work, the methods under development have strong theoretical guarantees, are robust under mild assumptions, require no tuning, and are easy to parallelize. Of practical interest, the research will develop needed software tools for researchers from disciplines with applications of high-dimensional and nonparametric statistics. Theoretical contributions of the proposed research include establishing new approximation and coupling theorems (under weaker regularity conditions than existing literature) in high-dimensional and infinite-dimensional spaces of increasing dimension and complexity, where classical probability tools such as the central limit theorem and extreme value theory are no longer applicable. The mathematical theory is of independent interest and will provide powerful new tools to analyze other statistical procedures on high-dimensional and nonparametric models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811768","New Approaches for Censored Quantile Regression Models via Data Augmentation","DMS","STATISTICS","05/15/2018","05/11/2018","Naveen Naidu Narisetty","IL","University of Illinois at Urbana-Champaign","Standard Grant","Pena Edsel","04/30/2022","$150,000.00","","naveen@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","","$0.00","With the availability of massive amounts of data in the modern Big Data era, heterogeneous behavior is a common phenomenon.  It is an important challenge to develop statistical methods for extracting useful insights from datasets exhibiting heterogeneity, without making strong modeling assumptions that could restrict the applicability of these methods. One efficient way of doing this is to model the quantiles of an outcome variable conditional on covariates, a method referred to as quantile regression. The current project develops novel statistical methods and computational techniques for quantile regression models when some of the observations are only partially observed. The proposed research will enable the use of the rich class of quantile regression models for a large class of applications that were previously not amenable to these techniques.  The proposed framework will include the high-dimensional setting where the number of covariates could potentially exceed the number of observations, a common occurrence in many biological, medical, and economics applications. The research will be broadly disseminated in scientific journals, at conferences and seminars, and software packages in R will be developed.  As the proposed research is placed at the intersection of modern statistical methodology, computation, and theory with substantial applications, it will be suitable for training graduate students with a broad range of skills.<br/> <br/>The proposed research will develop novel and efficient statistical methods for quantile regression models when the responses are subject to arbitrary censoring, that is, multiple censoring types including single, double and interval censoring can occur within the same dataset, and the covariates can be high-dimensional.  Three fundamental problems in censored quantile modeling will be considered in this project: (i) develop efficient inferential methods under arbitrary censoring and study their theoretical properties, (ii) devise computationally scalable algorithms having statistically desirable properties that can handle high-dimensional covariates, and (iii) develop models that account for subgroups having heterogeneous quantile effects.  A fundamental challenge that will be tackled is how data augmentation and the Bayesian framework can be efficiently utilized when an explicit likelihood is unavailable. An important feature of the methods is their computational scalability while having desirable statistical properties.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812258","Dynamic Signal Detection in Non- and Semi-Parametric Models","DMS","STATISTICS","08/01/2018","05/13/2020","Lan Xue","OR","Oregon State University","Continuing Grant","Gabor Szekely","07/31/2021","$99,971.00","","xuel@stat.oregonstate.edu","1500 SW JEFFERSON ST","CORVALLIS","OR","973318655","5417374933","MPS","1269","","$0.00","In the big data era, massive data with complex structures are generated in an explosive fashion. Non- and semi-parametric models are powerful statistical tools for exploring nonlinear patterns hidden in complex data, and have been used in a wide range of fields, such as, biomedical science, geology, engineering and social sciences. However, traditional non- and semi-parametric methods are limited in their ability to deal with massive data of high dimensions. The goal of the proposed research is to develop effective tools for dynamic estimation and variable selection for non- and semi-parametric models in the massive complex data setting.  The proposed research generates new methods and theory, and will provide practitioners in different fields with tools to better understand complex and dynamic structures in massive data. <br/><br/>As an effective dimension reduction tool, variable selection is very useful for modeling high-dimensional data. In recent years, the properties of the penalized methods have been well investigated for both linear models and semiparametric models. However, those methods generally do not allow the variable selection to dynamically change with other variables. For many longitudinal or spatial data in practice, there is a need to develop a general framework for dynamic variable selection that allows possibly different sets of relevant variables to be selected in different time periods or at different spatial locations. The objectives of the proposed research include: (1) to develop a novel procedure for dynamic variable selection in the varying coefficient model; (2) to provide large sample properties to ensure that the proposed method provides an optimal solution when the sample size is sufficiently large; (3) to develop an efficient algorithm which allows one to obtain a higher percentage of correct-fitting even when the dimension of covariates is large; (4) to apply the dynamic variable selection to study time-varying network data ; (5) to develop an innovative penalized spline procedure with triangulations  which plays the roles of dynamic local signal detection, as well as efficient estimation of sparse non-parametric  functions on irregular domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812119","Statistical Inference for Networks with Complex Topological Structures","DMS","STATISTICS","08/01/2018","07/03/2018","Michael Schweinberger","TX","William Marsh Rice University","Standard Grant","Gabor Szekely","07/31/2021","$150,000.00","","mus47@psu.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","","$0.00","Understanding the structure of networks is critical to understanding and predicting application phenomena involving networks, such as the resilience of insurgent and terrorist networks and the impact of disease-transmission networks on epidemics. In the past decade, tremendous progress has been made on statistical inference for networks with simple topological features, such as the number of connections in networks and the propensities of network members to form connections. However, statistical inference for networks with complex topological features, such as various forms of network closure that are believed to be crucial, remains underdeveloped, because complex topological features raise challenging conceptual, theoretical, and computational issues. This project addresses fundamental questions underpinning statistical inference for such networks. The statistical models and methods that are developed will be made publicly available in the form of R packages.<br/><br/>This research project is concerned with the foundations of statistical inference for networks with complex topological features.  It starts with a question that is at the heart of statistical inference: What does it mean to observe more data from the same source? The conventional answer is that more data are observed by observing a larger and larger network. Some interesting insights have emerged by studying growing networks. Among them is that models with complex topological features may depend on the size of the network and that using the same model, with the same parameters, for small and large networks may not be meaningful.  Despite such insights, the question of how to model a wide range of complex topological features and how to conduct sound statistical inference remains unanswered. This project attempts to provide answers and is based on the following idea: If models with complex topological features depend on the size of the network, then statistical inference should be based on networks of the same order of magnitude. In other words, statistical inference should be based on replication. At least two forms of replication are possible: replication based on a single network consisting of many subnetworks or many networks of the same order of magnitude, i.e., the size of the largest network is a constant multiple of the size of the smallest network. Such network data, called multilevel network data, has important applications; examples include networks of armed forces consisting of units and subunits and school networks consisting of schools and school classes. Multilevel network data offer outstanding opportunities for modeling, methods, and theory. This project will take advantage of these opportunities to elaborate novel multilevel network models capturing complex topological features in networks, overlapping subsets of nodes, and temporal networks.  Statistical theory will take advantage of the replicative nature of multilevel network data, which provides a path to the first general statistical theory for networks with complex topological features.  Statistical computing will take advantage of the conditional independence structure of multilevel network models and will develop large-scale parallel computing procedures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811568","Collaborative Research: Bayesian Network Estimation across Multiple Sample Groups and Data Types","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","08/15/2018","08/10/2018","Marina Vannucci","TX","William Marsh Rice University","Standard Grant","Gabor Szekely","07/31/2021","$119,943.00","","marina@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269, 7454","068Z, 8091","$0.00","As part of this collaborative research, the investigators will develop new statistical methods for the estimation of multiple graphical networks.  The proposed research will address the challenge of learning networks when there is heterogeneity among both the subjects and the variables considered, breaking new ground in graphical modeling and Bayesian statistics. The methods developed will have the potential for significant impact in statistics and in applied fields in which problems of network estimation naturally arise.  In particular, applications in neuroimaging will be explored. The project will include educational and training activities for graduate students.  Findings will be disseminated to the research community and used to further interdisciplinary collaborative efforts. Software and code will be developed and deposited in public repositories.<br/><br/>When all samples are collected under similar conditions or reflect a single type of disease, methods such as the graphical lasso or Bayesian network inference approaches can be applied to learn the underlying conditional dependence relations. In many studies, however, samples are obtained for different subtypes or disease, under varying experimental settings, or other heterogeneous conditions. The challenge becomes even more formidable when multiple data types are under consideration. This project will focus on the development of Bayesian methods to learn networks for a single data type across multiple sample groups using an approach that both links edge values across groups, and flexibly models which groups are most similar.  Methods will also be extended to a hierarchical modeling framework of networks from both heterogeneous sets of subjects and heterogeneous data types.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811384","Collaborative Research: Theory and Methods for Highly Multivariate Spatial Processes with Applications to Climate Data Science","DMS","STATISTICS","08/01/2018","05/11/2018","Soutir Bandyopadhyay","CO","Colorado School of Mines","Standard Grant","Yong Zeng","07/31/2023","$94,501.00","","sbandyopadhyay@mines.edu","1500 ILLINOIS ST","GOLDEN","CO","804011887","3032733000","MPS","1269","1269","$0.00","Geophysical, environmental and ecological datasets often include many variables observed over a set of irregular geographical locations. While spatial datasets are increasing in size, they are also increasing in complexity with many variables being simultaneously observed, recorded, modeled or derived. Current methods in spatial statistics are unable to cope with such highly multivariate datasets; this research addresses this gap in statistical science, aiming to establish a new framework for multivariate spatial models. The testbed for the new framework is in the field of climate data science. Understanding of the Earth system relies on coupled physical models that represent the dynamic evolution of the atmosphere, ocean, land use, rivers, glaciers and other processes. These models have led to vast amounts of climate model data that severely constrain storage resources. Moreover, statistical emulators are increasingly common and desirable alternatives to running complex physical models directly. Development and validation of compression and emulation algorithms require understanding and maintaining complex dependencies between physical variables, but current tools are univariate or pairwise-based. This research will provide statistical guidance for climate data science applications.<br/><br/>This project focuses on a modeling framework for multivariate spatial processes, and relies on new theory incorporating graphical models in multiscale multivariate spatial process representations.  Moreover, many multivariate datasets exhibit non-Gaussian behavior.  A companion thrust of this work is in introducing and exploring empirical likelihood techniques for large multivariate spatial processes.  Finally, the proposed models and estimation frameworks will be applied to a climate dataset from the Community Atmosphere Model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812197","Decision Theoretic Bayesian Computation","DMS","STATISTICS","08/01/2018","07/29/2020","Vinayak Rao","IN","Purdue University","Continuing Grant","Yong Zeng","07/31/2022","$150,000.00","Harsha Honnappa","varao@purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1269","","$0.00","Decision-makers, whether in business, policy-making, or engineering systems, face the problem of taking action without complete knowledge of the state of the world. Examples of such situations include controlling industrial plants, maneuvering autonomous vehicles, developing new drugs, making investment decisions or staffing decisions in service systems. Modern decision-makers typically use sophisticated probabilistic models to capture uncertainty, and take optimal actions within the framework of such models. In general, the models themselves involve unknown parameters which must be estimated from data. While large datasets improve the estimation of the parameters, leading to more accurate decisions, these big-data settings also raise computational challenges that call for approximations in the estimation. Current methodology typically proceeds in two steps: (1) use the vast statistical and machine learning literature to approximately estimate model parameters, and (2) use the resulting approximations to compute the best possible action. This two-stage procedure can result in sub-optimality of actions, as the approximations computed in the first stage are not tailored to the decision-making problem in the second stage. The objective of this project is to develop and study a methodological framework for approximate computation that puts decision-making at its center, recognizing that the ultimate goal of most big-data analyses is to help decide among actions in the face of uncertainty. The project will provide tools and theory to accurately account for trade-offs between statistical accuracy, decision-theoretic utility and computational complexity, and will integrate decision-making into the computational revolution that has driven much of modern data-science. The tools and theory potentially impact a large range of data-driven decision-making problems. <br/><br/>This project works in the overarching framework of Bayesian statistics, where the primary object of interest is the posterior distribution over the unknown parameters and variables. The research focuses on theoretical and methodological challenges arising from approximate computation for Bayesian decision theory. The investigators consider two complementary problems, (a) Decision-theoretic variational Bayes, and (b) Robust decision-making. The former task analyzes and extends variational methods, developed in the machine learning community to approximate intractable Bayesian posterior distributions, from a decision-theoretic viewpoint. The investigators will theoretically study the optimality of such algorithms with respect to decision-making rather than prediction, and develop novel `loss-calibrated' algorithms that search for approximations using decision-theoretic, rather than inferential criteria. Task (b) recognizes that a model is always an approximation to reality, and is therefore misspecified. As a consequence, a Bayesian posterior distribution, even if calculated exactly, might not actually characterize the distribution over future observations. The investigators explore connections with approximations from the first task, and move from uncertainty about parameters and variables under a specified model, to uncertainty about the choice of model itself. They develop and analyze methodology that allows robust and principled decisions in the face of such `Knightian' uncertainty.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812013","Collaborative Research: Information-Based Subdata Selection Inspired by Optimal Design of Experiments","DMS","STATISTICS","07/15/2018","07/11/2018","HaiYing Wang","CT","University of Connecticut","Standard Grant","Gabor Szekely","06/30/2021","$60,000.00","","haiying.wang@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","","$0.00","Extraordinary amounts of data are collected in many branches of science, in industry, and in government. The massive amounts of data provide incredible opportunities for making knowledge-based decisions and for advancing complicated research problems through data-driven discoveries. To capitalize on these opportunities, it is critical to develop methodology that facilitates the extraction of useful information from massive data in a computationally efficient way. Even the simplest analyses of the data can be computationally intensive or may no longer be feasible for big data. It is however often the case that valid conclusions can be drawn by considering only some of the data, referred to as subdata. This project develops optimal strategies for selecting subdata that retain, as much as possible, relevant information that was available in the massive data set. The methodology helps to identify the most informative data points, after which an analysis can proceed based on the selected subdata only. This facilitates data-driven decisions, scientific discoveries, and technological breakthroughs with computing resources that are readily available.<br/> <br/>Existing investigations for extracting information from big data with common computing power have focused on random subsampling-based approaches, which have as limitation that the amount of information extracted is only scalable to the subdata size, not the full data size. This project develops and expands the Information-Based Optimal Subdata Selection (IBOSS) method proposed by the PIs in the following directions: 1) It combines IBOSS with sparse variable selection methods in linear regression; 2) it develops subdata selection methods for generalized linear models; 3) it constructs computationally efficient algorithms for selecting the most informative subdata; and 4) it develops user-friendly software that supports the methodology.  The research is a significant addition to the field of big data science. It advances a new method for dealing with big data and has the potential to create novel research opportunities in statistical science and other quantitative fields. The results are valuable even when supercomputers are available, because cutting edge high performance computing facilities will always trail the exponential growth of data volume.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811363","Collaborative Research: Information-Based Subdata Selection Inspired by Optimal Design of Experiments","DMS","STATISTICS","07/15/2018","07/11/2018","John Stufken","AZ","Arizona State University","Standard Grant","Gabor Szekely","06/30/2019","$59,994.00","","jstufken@gmu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1269","","$0.00","Extraordinary amounts of data are collected in many branches of science, in industry, and in government. The massive amounts of data provide incredible opportunities for making knowledge-based decisions and for advancing complicated research problems through data-driven discoveries. To capitalize on these opportunities, it is critical to develop methodology that facilitates the extraction of useful information from massive data in a computationally efficient way. Even the simplest analyses of the data can be computationally intensive or may no longer be feasible for big data. It is however often the case that valid conclusions can be drawn by considering only some of the data, referred to as subdata. This project develops optimal strategies for selecting subdata that retain, as much as possible, relevant information that was available in the massive data set. The methodology helps to identify the most informative data points, after which an analysis can proceed based on the selected subdata only. This facilitates data-driven decisions, scientific discoveries, and technological breakthroughs with computing resources that are readily available.<br/> <br/>Existing investigations for extracting information from big data with common computing power have focused on random subsampling-based approaches, which have as limitation that the amount of information extracted is only scalable to the subdata size, not the full data size. This project develops and expands the Information-Based Optimal Subdata Selection (IBOSS) method proposed by the PIs in the following directions: 1) It combines IBOSS with sparse variable selection methods in linear regression; 2) it develops subdata selection methods for generalized linear models; 3) it constructs computationally efficient algorithms for selecting the most informative subdata; and 4) it develops user-friendly software that supports the methodology.  The research is a significant addition to the field of big data science. It advances a new method for dealing with big data and has the potential to create novel research opportunities in statistical science and other quantitative fields. The results are valuable even when supercomputers are available, because cutting edge high performance computing facilities will always trail the exponential growth of data volume.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811745","Methodology for Multi Time-Scale Nonlinear Dynamical Spatio-Temporal Statistical Models","DMS","STATISTICS","08/01/2018","07/20/2020","Christopher Wikle","MO","University of Missouri-Columbia","Continuing Grant","Gabor Szekely","07/31/2021","$225,000.00","","wiklec@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269","9251","$0.00","Scientists and engineers are increasingly aware of the importance of accurately characterizing various sources of uncertainty when trying to understand complex systems such as those that vary across time and space.  Examples of such systems include how ocean heating influences convective clouds in the tropics, which in turn, can influence severe weather and habitat conditions over North America; or, how a migratory species interacts with its environment and competitive pressures from both predators and prey.   When performing statistical modeling on such complex spatio-temporal phenomena, the scientific goal is typically either inference, prediction, or forecasting, all of which require some measure of uncertainty.  To accomplish these goals through modeling, one must synthesize information from a variety of sources, including direct observations, indirect (remotely sensed) observations, surrogate observations (mechanistic model output), previous empirical results, expert opinion, and scientific knowledge.  This information must then integrate into a process model that can represent the complexity of the interacting processes, and account for uncertainty.  This research is concerned with building these models in a way that can account for complex interactions across different time scales.<br/><br/><br/>This project concerns the development of a methodological framework for parsimonious  and computationally efficient models for multi time-scale nonlinear dynamical spatio-temporal processes that accounts for the interaction across processes and time scales in such a way as to accommodate uncertainty in data, processes, and parameters.  In particular, the project will focus on a hybrid model that combines elements of a generalized quadratic nonlinear spatio-temporal dynamical model with a recurrent neural network model.  However, this methodology will focus on models for processes that involve multiple time scales of variability.  This will include the development of computationally efficient algorithms that can deal with the extreme curse of dimensionality in the state and parameter spaces associated with complex interacting nonlinear phenomena by adapting, extending and combining approaches from both statistics and machine learning. Not only will the proposed modeling and computational methodology be an advancement in statistics, but it will be useful across a broad range of disciplines that deal with complex multi time-scale dynamical processes such as brain science, climatology, demography, econometrics, fisheries, ecology, meteorology, oceanography, and wildlife biology.  In addition, the project will contribute to STEM education through training a graduate research assistant, who will gain inter-disciplinary experience.  In addition, the project will foster undergraduate interest in the STEM disciplines by employing undergraduate research assistants to help with the development of visualization tools for spatio-temporal data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811360","New Techniques for High-Dimensional Regression and Applications to Precision Medicine","DMS","STATISTICS","08/01/2018","06/15/2020","Xinge Jeng","NC","North Carolina State University","Continuing Grant","Yong Zeng","07/31/2022","$100,000.00","","xjjeng@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","","$0.00","This project focuses on the study of statistical inference in high-dimensional regression and its applications towards biomedical research. A new analytic framework for inference-based variable selection will be developed to explicate key properties of selection performances, including the numbers of true positives, false positives, true negatives, and false negatives. Consequently, the investigator will develop novel procedures, with which practitioners can select variables based on Type I error control to prevent false discoveries, Type II error control to improve the chances of selecting a large proportion of signals, or some balanced criteria to meet their different needs. This study will be particularly valuable in areas where signals are relatively weak under high-dimensionality; such examples include whole-genome sequencing studies and precision medicine with a large number of prognostic factors. The education and outreach components of the project include new course development, involvement of undergraduates and under-represented groups, and an accessible website resource for the public.  <br/><br/><br/>This project comprises several key innovations in establishing foundations for new high-dimensional variable selection procedures beyond those based on standard penalized regressions. These innovations include (1) accurate approximations for false discovery proportion (FDP) and false negative proportion (FNP) via higher-order Mehler's expansions and new theories for sparse inference under dependence; (2) data-driven approaches that are automatically adaptive to the unknown sparsity level of the regression coefficients and data dependence; (3) derivation of regression-based confidence intervals that are robust to outcome model misspecification in precision medicine. Further, accurate approximations of FDP and FNP lead to new interpretable variable selection procedures for big data applications; data adaptivity will allow the methods to be applicable across a spectrum of data scenarios; whereas robust inferential procedures can provide suitable personalized treatment decisions in the high-dimensional setting.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812082","Optimal and Adaptive p-Value Combination Methods with Application to ALS Exome Sequencing Study","DMS","STATISTICS","07/01/2018","06/07/2018","Zheyang Wu","MA","Worcester Polytechnic Institute","Standard Grant","Yong Zeng","06/30/2022","$150,000.00","","zheyangwu@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","MPS","1269","","$0.00","Statistical theory and methodology play a key role in advancing scientific research. The p-value combination approach is a foundational statistical method for important data-driven research in meta-analysis, data integration, and signal detection.  Despite recent theoretical and methodological advances, significant gaps still exist in the literature.  Many of the assumptions including independence, Gaussianity, and large group size, are not realistic for real data applications.  Further, some methods developed based on ad hoc arguments lack a rigorous study of optimality.  This project seeks to develop new methods that exhibit more powerful and robust performance.  The methods will be applied to the analysis of large exome sequencing data from a study of amyotrophic lateral sclerosis (ALS).  Students from underrepresented groups will be strongly encouraged to participate in this project. <br/><br/>The objective of this project is to develop powerful and robust p-value combination tests that are optimal and data-adaptive under a wide spectrum of signal patterns and readily applicable to real data analysis.  Analytical calculations for p-value and statistical power, and asymptotic techniques, under realistic assumptions of small or moderate group size, non-Gaussian distribution, dependence, and linear-model-based alternative hypotheses, will be developed.  Two statistics families, gGOF for goodness-of-fit type tests, and tFisher for Fisher type p-value combination, will be investigated.  In addition to a study of power and optimality, the project will also develop omnibus tests for adapting to unknown signal patterns.  The project will lead to (1) a new statistical framework for calculating the distributions of generic families of goodness-of-fit type and Fisher type statistics, (2) optimal statistics for given signal patterns as well as data-adaptive methods when patterns are unknown, and (3) genetic association test strategies for detecting genetic effects.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811445","Collaborative Research: Bayesian Network Estimation across Multiple Sample Groups and Data Types","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","08/15/2018","05/15/2020","Christine Peterson","TX","University of Texas, M.D. Anderson Cancer Center","Continuing Grant","Gabor Szekely","07/31/2021","$87,515.00","","cbpeterson@mdanderson.org","1515 HOLCOMBE BLVD","HOUSTON","TX","770304000","7137923220","MPS","1269, 7454","068Z, 8091","$0.00","As part of this collaborative research, the investigators will develop new statistical methods for the estimation of multiple graphical networks.  The proposed research will address the challenge of learning networks when there is heterogeneity among both the subjects and the variables considered, breaking new ground in graphical modeling and Bayesian statistics. The methods developed will have the potential for significant impact in statistics and in applied fields in which problems of network estimation naturally arise.  In particular, applications in neuroimaging will be explored. The project will include educational and training activities for graduate students.  Findings will be disseminated to the research community and used to further interdisciplinary collaborative efforts. Software and code will be developed and deposited in public repositories.<br/><br/>When all samples are collected under similar conditions or reflect a single type of disease, methods such as the graphical lasso or Bayesian network inference approaches can be applied to learn the underlying conditional dependence relations. In many studies, however, samples are obtained for different subtypes or disease, under varying experimental settings, or other heterogeneous conditions. The challenge becomes even more formidable when multiple data types are under consideration. This project will focus on the development of Bayesian methods to learn networks for a single data type across multiple sample groups using an approach that both links edge values across groups, and flexibly models which groups are most similar.  Methods will also be extended to a hierarchical modeling framework of networks from both heterogeneous sets of subjects and heterogeneous data types.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811779","Exact and Asymptotic Distribution Theory for General Gaussian Processes","DMS","STATISTICS","07/01/2018","05/11/2018","Philip Ernst","TX","William Marsh Rice University","Standard Grant","Pena Edsel","06/30/2022","$250,000.00","Frederi Viens","philip.ernst@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","1269","$0.00","This project will further the development of exact and asymptotic distribution theory for Gaussian processes and their quadratic forms. While modern advances in data science owe much progress to computational methods and the rapid growth in computer technology, statistics and applied probability are rife with examples where a careful mathematical analysis allows discoveries that no amount of computational power can uncover. This project is one such example and will use the PI's work on Yule's so called ""nonsense"" correlation, a 90-year old open problem that was solved last year via mathematical analysis tools. This explicit calculation showed the precise scale of the apparent correlation between two independent continuous series of data, such as what one encounters in economics, climate science, finance, and many other fields. This mathematical explanation of an apparent statistical paradox will enable the investigation of other important questions in mathematical statistics. The project will investigate a possible connection between some important open questions and a set of tools in probability theory whose power mathematical statisticians have only begun to investigate. The project will provide fertile ground for statistics graduate student training at Rice and Michigan State Universities; students will benefit from a wide scope of opportunities, from rigorous study of mathematical tools, to their use in statistics, to applications in fields of great societal value.<br/><br/>This project will investigate the probability law of the Pearson correlation between two independent or dependent Gaussian processes. Analyses of distributions in the second Wiener chaos (quadratic forms of normals) are a new set of tools that will be brought to bear. Those tools are flexible enough to handle any Gaussian process via their so-called Karhunen-Loeve expansions. In terms of applications, what is most striking is that any statistical estimation or test based on these projected studies would only require a single or a pair of observations; this is particularly useful for situations, such as in environmental statistics or in economics, where experiments cannot be designed, and one has to work with the available observable data collected dynamically in time. The second emphasis in this study, on Polya frequency functions and related densities, uses some of the same mathematical tools, thanks to a realization that the densities can be represented and expanded explicitly in the second Wiener chaos. The project seeks to prove when a density is strongly log-concave (e.g. its logarithm has a second derivative which is bounded away from zero.) This question, which in mathematical statistics is phrased more broadly in terms of Polya frequency functions, has distribution of sums of independent and non-identically distributed exponentials, expands to the case of general second-chaos distributions. The project could have important consequences in the practice of statistics, especially in areas where comparing non-trivial time series is a challenge, and in many scientific fields informed by properties of log-concavity and strong log-concavity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811812","High Dimensional Semiparametric Estimation and Inferences","DMS","STATISTICS","08/15/2018","08/11/2020","Qifan Song","IN","Purdue University","Continuing Grant","Gabor Szekely","07/31/2021","$159,985.00","Guang Cheng, Qifan Song","qfsong@purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1269","","$0.00","Semiparametric regression model provides data scientists a useful way to analyze complex-structured data sets. It allows researchers to model some features in a linear way, without restricting the effect of the rest covariates. This flexibility can greatly enhance the prediction performance especially when parametric model assumptions are invalid. In practice, the semiparametric modelling is proven useful in many high dimensional applications in Biostatistics, Econometrics and Neuroscience. However in literature, there is a lack of statistical studies on the estimation and inference of high dimensional semiparametric model. This project aims to lay a solid theoretical foundation for high dimensional semiparametric analysis, in both frequentist and Bayesian paradigms. This research will significantly promote the use of semiparametric analysis of high dimensional complex data.<br/> <br/>This project consists of three research components. First, the investigators will establish the frequentist estimation theory and obtain new theoretical insights on the asymptotic behavior of the estimators in high dimensional semiparametric model. Secondly, the investigators will develop novel approach to conduct high dimensional semiparametric inferences such as confidence intervals and explore related semiparametric efficiency issue. Thirdly, Bayesian counterparts of estimation and inference theories will be developed. The investigators will establish the frequentist validity of Bayesian point estimations and interval estimations. These research results will provide important theoretical guidelines for high dimensional semiparametric modeling.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1907316","Fast and Robust Gaussian Process Inference for Bayesian Nonparametric Learning","DMS","STATISTICS","05/17/2018","04/17/2019","Yun Yang","IL","University of Illinois at Urbana-Champaign","Standard Grant","Yong Zeng","05/31/2022","$120,000.00","","yy84@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","8083, 9263","$0.00","Advances in modern technology have empowered researchers to collect massive data to conduct inference and making predictions. With the abundance of available observations, traditional statistical methods under the parametric assumption that a model can be characterized by a pre-specified number of parameters become inadequate and less attractive. Bayesian nonparametric models are attractive in this context which allow the resolution level of the analysis to be determined in a data-driven manner, and provide automatic characterization of uncertainty. The goal of this project is to develop new theory, methodology and computational tools for Bayesian nonparametric inference via Gaussian process priors. <br/><br/>Given the availability of massive data, nonparametric inference offers an attractive framework for flexibly modeling the underlying structure and extracting useful information. For instance, such challenges occur in chemical physics, computational biology, computer vision, engineering, and meteorology. This project aims to lay down a solid methodological, algorithmic, and theoretical foundation for nonparametric inference based on Gaussian processes. In particular, Gaussian process-based approaches tend to be vulnerable to data contamination and have heavy computational costs. To alleviate the high-computational cost of Gaussian process inference procedures, the investigator puts forward two novel computational frameworks which differ at their respective approximating targets as being either the prior or the posterior. To enhance the robustness of Gaussian process inference against data contamination, the investigator proposes a novel class of Bayesian hierarchical models for incorporating this extra measurement error structure, leading to a class of robust Gaussian process inference procedures. The new theoretical development offers valuable insight to experiment-design practitioners into the impact of measurement errors upon prediction and estimation, and provides evidence on the deep connection between computational complexity and statistical learnability. These computational and theoretical frameworks also benefit other disciplines such as applied mathematics, computer science and finance where stochastic processes such as Gaussian processes are routinely used.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811868","Dimension reduction for high-dimensional high-order data","DMS","STATISTICS","06/01/2018","05/15/2018","Anru Zhang","WI","University of Wisconsin-Madison","Standard Grant","Gabor Szekely","05/31/2021","$100,001.00","","anru.zhang@duke.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","8083, 9263","$0.00","High-dimensional, high-order data commonly arise from a wide range of applications, such as neuroimaging, microbiology, bioinformatics, longitudinal studies, and material sciences. These data possess distinct characteristics compared with traditional low-dimensional or low-order data and pose unprecedented challenges to the statistical community. Dimension reduction often becomes the crucial first step in order to better summarize, visualize, and analyze high-dimensional high-order data. One natural approach is to unfold high-order data into matrices, followed by the use of well-established matrix dimension reduction methods. However, such operations often lead to loss of information on the intrinsic data structures and sub-optimal statistical results of subsequent analyses. In addition, the naive generalization of traditional statistical methods to high-dimensional, high-order data is often statistically or computationally infeasible. Therefore, there is a critical need for new high-dimensional high-order data dimension reduction methods.<br/><br/>In this project, the PI plans to develop new theories, methodologies, and computational algorithms to address a series of fundamental problems in high-dimensional, high-order data dimension reduction. The project will be focused on four major areas: (i) a general framework for regularized tensor SVD; (ii) provable regularized tensor decomposition via alternating annealing; (iii) high-order PCA with theoretical guarantees by a new high-order spiked covariance model; (iv) dynamic network community detection via tensor methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811279","Collaborative Research: Multi-Scale Modeling of Non-Gaussian Random Fields","DMS","STATISTICS","09/01/2018","07/12/2018","Tomoko Matsuo","CO","University of Colorado at Boulder","Standard Grant","Gabor Szekely","08/31/2021","$49,998.00","","tomoko.matsuo@colorado.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1269","","$0.00","Data collected on various environmental, geophysical and meteorological processes often exhibit different modes of variability, especially at different scales. An accurate description of the features of the fluctuations in these data can improve scientific understanding of the physical phenomena. Development of new statistical tools for modeling such geophysical processes can also enhance the ability to monitor and predict the impact of their fluctuations on communication systems and sensory networks. Despite the ubiquity of such data, few statistical methodologies are currently available to describe such spatiotemporal scalar and vector random fields globally on a spherical domain. One key objective of this project is to propose a multiscale approach for constructing non-Gaussian random fields on a sphere, that on the one hand provides a flexible mathematical framework for modeling, and on the other hand, enables one to fit these models by using modern computational tools. A further objective is to extend the methodologies to deal with data observed on graphs and networks. The project also aims to demonstrate the effectiveness of the proposed methodologies in enhancing scientific understanding of geophysical processes by analyzing ground-based and satellite-based measurements of the earth's magnetic fields.<br/> <br/>The proposed statistical framework for spherical processes is based on the idea of multiresolution analysis on a sphere. In this application, a class of needlet frames on the unit sphere is utilized as a building block to construct spatio-temporal scalar and vector fields on the unit sphere that satisfy natural physical constraints such as being curl-free or divergence-free, thereby enabling a flexible approach to approximating physical processes.  Parametric statistical models are proposed to model random vector fields on the unit sphere and spherical shells. These random fields are represented in terms of vectorial needlets and can exhibit non-Gaussian features. A suite of methodologies is proposed under this modeling paradigm to analyze and predict large-scale spatiotemporal scalar and vector processes arising in geophysics, such as ground and satellite based measurements on the earth's main magnetic field or on ionospheric electro-magnetic fields. Theoretical questions related to the structure and properties of the proposed vectorial needlets and the random vector fields represented by them are also investigated. The flexible framework of modeling random fields through multiresolution analysis is further exploited to construct non-Gaussian processes on graphs by means of graph spectral wavelets. This collaborative project requires bringing together skills and knowledge from disparate areas such as multiresolution analysis, spatial statistics, spectral graph theory, Bayesian and large-scale computation, space physics, and geophysics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1752280","CAREER: Hierarchical Models for Spatial Extremes","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2018","06/20/2019","Benjamin Shaby","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","11/30/2019","$148,065.00","","bshaby@colostate.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269, 8048","1045","$0.00","Rare events can have crippling effects on economies, infrastructure, and human health and well being. But in order to make sound decisions, understanding how large the most severe events are likely to be is imperative. The PI will focus on developing statistical tools for understanding the spatial structure of the most extreme events. These new tools will improve on existing models because they will be both more realistic and more computationally tractable.  The PI will also apply these tools to help scientists and policymakers study risks posed by severe environmental phenomena like inland floods, wildfires, and coastal storm surges. Furthermore, the PI will organize workshops to foster closer integration of statistical and Earth science research, as well as develop graduate courses and a textbook focused on modern statistical methods for Earth science.<br/><br/>The PI will develop stochastic models for extreme events in space that are 1) flexible enough to transition across different classes of extremal dependence, and 2) permit inference through likelihood functions that can be computed for large datasets.  It will accomplish these modeling goals by representing stochastic dependence relationships conditionally, which will induce desirable tail dependence properties and allow efficient inference through Markov chain Monte Carlo (MCMC). The first research component will develop sub-asymptotic models for spatial extremes using max-infinitely divisible (max-id) processes, a generalization of the limiting max-stable class of processes, based on a conditional representation.  The second research component will develop sub-asymptotic spatial models for extremes based on scale mixtures of spatial Gaussian processes.  The PI will conduct closely interwoven computational development and theoretical explication of the joint tail dependence that the proposed hierarchically specified max-id and scale mixture processes induce.  Finally, the PI will apply these models to problems of high societal impact, such as extreme precipitation risk, wildfire susceptibility, and coastal storm surge exposure.  The PI will enhance connections between extreme value statisticians and communities of climate and atmospheric scientists, mitigation researchers, and stakeholders, through 1) biannual international workshops on weather and climate extremes, 2)  a Ph.D. level course in spatial statistics which will include new advances and applications of spatial extremes, and 3) writing the textbook Modern Statistics for Earth Scientists.  The PI also will add modules on extremes to Penn State's Sustainable Climate Risk Management (SCRiM) summer school, and contribute to SCRiM's electronic resources and interactive teaching materials for educators, decision makers, underrepresented groups, and the general public.  The PI will strengthen existing collaborations with government agencies which are responsible for communicating and mitigating risk to the public posed by extremal environment phenomena.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811376","A Non-Asymptotic Theory of Robustness","DMS","STATISTICS","07/01/2018","05/03/2020","Wenxin Zhou","CA","University of California-San Diego","Continuing Grant","Yong Zeng","06/30/2022","$120,000.00","","wenxinz@uic.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","","$0.00","Modern data acquisitions have facilitated the collection of large-scale data with complex structures, and meanwhile, have introduced a series of new challenges to data analysis both statistically and computationally. The heavy-tailed character of the distribution of empirical data has been repeatedly observed in many fields of research, including microarray studies in genomics, neuroimaging in medicine, and portfolio optimization and risk management in finance. In functional MRI studies, the parametric statistical methods often fail to produce valid cluster-wise inference, where the principal cause is that the spatial autocorrelation functions do not follow the assumed Gaussian shape; in finance, the power-law nature of the distribution of returns has been validated as a stylized fact over the years. The least squares method, albeit being most commonly used in practice due to its simplicity as a once-for-all solution, is sensitive to the tails of sample distributions and is proven to be suboptimal for heavy-tailed data from a non-asymptotic viewpoint. In this project, the PI will develop robust statistical procedures for various problems, ranging from mean estimation, linear regression, high-dimensional sparse regression to large covariance matrix estimation. The main goals of this research are to understand the finite-sample properties of robust learning, and to develop computationally efficient procedures that advance the practical use of robust methods.<br/><br/>In this project, the PI will study robust alternatives to the method of least squares for two fundamental problems: linear regression and covariance estimation. To achieve robustness against asymmetric and heavy-tailed data, the main idea is to use the adaptive Huber loss and its extension on the matrix space. From a non-asymptotic viewpoint, the accompanying scale parameter should adapt to the sample size, dimension and noise level for optimal tradeoff between the gain in stability and cost in bias. The work on the project aims to (i) develop new methods for robust estimation and inference under linear models, and investigate their mathematical underpinnings using techniques from concentration inequality in probability, finite-sample theory for M-estimation in statistics and convex analysis in optimization, and (ii) construct both general and structured large covariance matrix estimators under minimal assumptions on the data. The originality of the project resides in providing new perspectives on robustness, which not only represent useful complements to classical robust statistics but also make important contributions to modern statistical analysis, including high dimensional estimation and large-scale inference. The philosophy of the project echos John Tukey's principles for statistical practice by highlighting the importance of having methods of statistical analysis that are robust to violations of the assumptions underlying their use and allowing the possibility of data's influencing the choice of method by which they are analyzed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812111","The SRCOS Summer Research Conference in Statistics and Biostatistics","DMS","STATISTICS","04/01/2018","02/28/2018","Arnold Stromberg","KY","University of Kentucky Research Foundation","Standard Grant","Gabor Szekely","03/31/2019","$21,340.00","","stromberg@uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","1269","7556, 9150","$0.00","The investigator and his colleagues will organize and conduct a Summer Research Conference (SRC) in statistics and biostatistics at Virginia Beach Resort Hotel and Conference Center in Virginia Beach, Virginia on June 3-6, 2018.  The 2018 SRC will be the 54th anniversary of that conference.  The SRC is an annual conference sponsored by the Southern Regional Council on Statistics (SRCOS).  Its purpose is to encourage the interchange and mutual understanding of current research ideas in statistics and biostatistics, and to give motivation and direction to further research progress.  The project will focus on young researchers, placing them in close proximity to leaders in the field for person-to-person interactions in a manner not possible at most other meetings.  Speakers will present formal research talks with adequate time allowed for clarification, amplification, and further informal discussions in small groups. Under the travel support provided by this award students will attend and present their research in posters to be reviewed by more experienced researchers. <br/><br/>The Southern Regional Council on Statistics is a consortium of statistics and biostatistics programs in the South, stretching as far west as Texas and as far north as Maryland.  It currently has member programs at 45 universities in 16 states in the region.  This project will fund student participation in the 2018 Summer Research Conference (SRC) sponsored by SRCOS.  The meeting is particularly valuable for students and faculty from smaller regional schools at drivable distances, affording them the opportunity to participate and interact closely with internationally-known leaders in the field without the cost of travel to distant national or international venues.  It will strengthen the research of the statistics and biostatistics community as a whole, and particularly in the relatively underdeveloped southern region.  For all details, go to the conference link at http://odu.edu/math/srcos/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1752692","CAREER: Stable and Scalable Estimation of the Intrinsic Geometry of Multiway Data","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2018","06/25/2021","Eric Chi","NC","North Carolina State University","Continuing Grant","Gabor Szekely","11/30/2021","$316,260.00","","echi@rice.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269, 8048","1045","$0.00","Two important challenges of the proposed work are in high-throughput bioinformatics studies and neuroscience studies, which generate data typical of research aligned with two recent White House Initiatives, namely the Precision Medicine and BRAIN Initiatives respectively. Both initiatives are investing heavily in generating high-resolution data, which once analyzed properly, can yield insights that will pave the foundations of advanced treatments for genetic and nervous system disorders. To fully maximize the potential impact of collecting such data, this proposal develops a new framework for identifying complicated underlying patterns in multiway arrays. Specifically, the project fills a gap in nonparametric estimation of low-dimensional structure and geometry in big and noisy data arrays. The PI plans to develop a training program based on applications of the proposed research to recruit and retain talented high school, undergraduate, and graduate students from underrepresented minority (URM) groups for potential careers as innovative data scientists. <br/><br/>Modern data matrices present two challenges to their analyses: (1) they are transposable in the sense that both their rows and columns are often of interest and may contain non-trivial dependencies among them, and (2) they may be very large. The first challenge has only partially been addressed by existing biclustering or co-clustering methods. These methods can identify only very simple coupled structures that organize the rows and columns. In order to flexibly model and extract more complicated patterns in large data matrices, in which both rows and columns are high-dimensional, one requires a new co-manifold learning framework that can discover a wider range of intrinsic geometries of the rows and columns. To meet the first challenge, this project develops a framework for performing joint nonlinear dimension reduction on the rows and columns.  The proposed methods construct multiscale distances that are invariant to row and column permutations, equipping practitioners with a means to estimate the intrinsic organization of the rows and columns of a data matrix without prior information on any row or column orderings. To meet the second challenge, this project formulates the key computations as optimization problems that admit distributed parallel algorithms with nearly linear speed-up. The framework also generalizes naturally to the higher-order generalization of matrices, multiway arrays or tensors. Finally, the estimated intrinsic geometries possess stability guarantees, namely small perturbations in the data due to noise or adjustments to input parameters cannot lead to disproportionately large variations in the estimated intrinsic geometry. The proposed procedures have the potential to enable practitioners to extract complicated patterns stably from massive data tensors with non-trivial dependencies along their modes or axes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811976","Uncertainty Quantification in High-Dimensional Structured Regression Problems","DMS","STATISTICS","06/01/2018","05/11/2018","Pierre Bellec","NJ","Rutgers University New Brunswick","Standard Grant","Yong Zeng","05/31/2022","$179,997.00","","pcb71@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","Structured, high-dimensional regression problems is a current topic of major interest due recent applications in modern scientific fields. Such structured, high-dimensional data arise naturally in bioinformatics, signal processing, quantum mechanics and networks.  Handling this massive high-dimensional data is impossible unless the underlying parameter of interest has additional structure.  Examples of additional structure that arise in applications include sparsity or low-rankness and powerful estimation methods have been developed in the last decade to successfully recover the true parameter whenever such additional structure is present in the data. However, identifying whether such additional structure is present in the data is more challenging than the corresponding estimation problem. New phase transitions are seen and novel methods are needed. The goal of the current project is to develop statistical methodology aimed at identifying the existence of such additional structure, and to study the theoretical phase transitions of the problem.<br/><br/><br/>The high-dimensional setting considers the situation where the number of unknown parameters outnumber the number of samples. Estimation procedures have successfully solves this high-dimensional setting under additional structural assumption on the problem: Common structural assumptions include sparsity or low-rankness, where the unknown parameter possesses some low-dimensional property.  In the so called sparse or low-rank regime, consistent estimation of the unknown parameter becomes possible even in the high-dimensional ""large p small n"" settings.  The proposed research will focus on uncertainty quantification in such structured, high-dimensional settings. Methodologies will be developed to construct confidence sets and confidence bands tailored to structured high-dimensional problems. A major challenge that will be studied both theoretically and empirically is identifying whether the sparse or low-rank regime actually occurs, or equivalently the construction of adaptive confidence sets. An adaptive confidence set captures the true parameter with high probability, and its size shrinks optimally with respect to the unknown sparsity or low-rankness of the problem.  The optimality properties of adaptive confidence sets will be studied, as well as the corresponding phase transitions with respect to the problem parameters.  The proposed research is motivated by and will be directly applicable to scientific fields where high-dimensional data arise.  An adaptive confidence set has major applications in practice as it provides a certificate that the sparse or low-rank regime actually occurs, and such certificate asserts that the high-dimensional estimation and inferential methods used in practice are actually accurate. Identifying whether the sparse regime actually occurs has direct applications in bioinformatics and signal processing, while identifying whether the low-rank regime occurs has direct applications in quantum tomography and matrix completion.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811614","Properties of Approximate Inference for Complex High-Dimensional Models","DMS","STATISTICS","07/01/2018","07/24/2021","Iain Johnstone","CA","Stanford University","Continuing Grant","Yong Zeng","06/30/2024","$600,000.00","David Donoho","imj@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","Modern scientific data acquisition often creates datasets with many measurements on a large sample size of individuals.  The structure of interest in the data may be low dimensional or sparse. Examples arise in scientific domains from econometrics to genomics and image and signal processing and beyond.  The project explores approximate methods of statistical inference for such structure in a representative set of contemporary settings: high dimensional estimation, generalized linear mixed models and low rank multivariate models. It aims to develop approximate methods backed by an optimality theory and/or performance guarantees.  The work is expected to provide new theoretical insights into current problems in specific application domains such as Magnetic Resonance Imaging and quantitative genetics.<br/><br/>The project will bring ideas from classical decision theory to compressed sensing and robust linear modeling, rigorously solving nonconvex optimization problems and obtaining reconstruction performance rigorously better than traditional convex optimization methods.  It will exploit decision theoretic ideas in the proposer's previous work to provide new theoretical insights into a pressing practical problem in compressed sensing -- deriving optimal variable-density sampling schedules applicable to Magnetic Resonance Imaging and NMR spectroscopy.  The project will also study theoretically the statistical performance of some methods for deterministic approximate inference popular in machine learning, but for which little attention has been given to properties such as consistency, asymptotic normality and efficiency.  The study will begin with concrete examples in the realm of generalized linear mixed models, from a frequentist perspective, and seek to establish first-of-kind results for asymptotic efficiency of Expectation Propagation.  Finally, the project will study approximate inference for the eigenstructure of highly multivariate models with low dimensional structure.  It will adapt James' classical framework for multivariate analysis to a broad class of multispike models.  Through collaboration with quantitative geneticists, it will develop methods for inference for low dimensional structure in high dimensional genetic covariance matrices.  In both cases, methods from random matrix theory will be essential.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811245","Theory and Methods for Causal Inference in Chronic Diseases","DMS","STATISTICS","07/15/2018","07/06/2018","Shu Yang","NC","North Carolina State University","Standard Grant","Yong Zeng","06/30/2022","$120,000.00","","syang24@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","","$0.00","Chronic diseases such as cardiovascular disease and HIV create an immense health and economic burden, both within the USA and globally.  With recent technological advances, the chronic disease research enterprise is rapidly becoming data-intensive and data-driven.  Massive and complex data provide unprecedented opportunities for discovering optimal treatment strategies for chronic diseases. However, these complex data also present novel challenges for statistical analysis.  Patients may visit the clinic at irregular intervals, may drop out of studies, and may discontinue prescribed treatments prematurely. In addition, there may be ""confounding by indication"", in that some treatments may have been prescribed preferentially to sicker patients.  These features can be barriers to effectively translating rich information into meaningful knowledge. The overarching theme of this project is to develop new data analysis methods that tackle these important and recurring challenges. This work aims to advance statistical science through the development of novel approaches to address these difficult challenges, where existing methods do not apply or suffer from major drawbacks. The research will also provide subject matter scientists with a principled way to approach scientific questions in these settings to discover optimal treatment strategies for patients.   <br/><br/>This research project has the following goals. 1) Develop estimators of survival distributions as a function of time to treatment discontinuation using a dynamic-regime marginal structural models approach. Treatment discontinuation arises frequently in clinical practice, complicating the analysis and interpretation.  The objective here is to develop an instructive demonstration of how careful conceptualization of this problem leads to an unambiguous definition of a sensible treatment effect and to valid inferences, shaping a principled approach to dealing with treatment discontinuation.  2) Develop efficient estimators for Structural Nested Mean Models (SNMMs) from longitudinal observational studies in the presence of informative censoring using semiparametric theory. Time-varying confounding by indication is a widespread phenomenon and causes selection bias in the estimation of treatment effect.  SNMMs have been proposed to overcome this issue; however, their use in practice is still unpopular, partly because the efficiency of the estimators is highly dependent on the choice of estimating equations, and the theory is still underdeveloped in many settings.  The investigator plans to develop improved estimators of causal parameters in SNMMs in the presence of censoring, which gain both efficiency and robustness to nuisance model specification over existing methods.  3) Develop a new framework of continuous-time SNMMs. In many realistic situations, the outcomes and treatments are more likely to be measured at irregularly spaced time points. Most of the existing SNMMs literature uses a discrete-time setup, which is overly simplified and therefore impractical. The investigator aims to provide a unified framework for SNMMs with continuous-time processes, establishing a novel area of research in causal inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810945","Conference on Predictive Inference and Its Applications","DMS","STATISTICS","05/01/2018","03/29/2018","Daniel Nettleton","IA","Iowa State University","Standard Grant","Nandini Kannan","04/30/2019","$10,000.00","","dnett@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","7556","$0.00","The Department of Statistics at Iowa State University will host the Conference on Predictive Inference and Its Applications on May 7 and 8, 2018, in Ames, Iowa.  The conference program includes plenary presentations from 16 distinguished speakers and a poster session featuring presentations from students, postdoctoral scholars, and early-career researchers.  Goals of the conference include raising awareness about the importance of prediction, showcasing research of current and emerging leaders in the field, motivating the development of more accurate prediction methods, and encouraging interactions and collaborations among a diverse collection of scientists with complementary skills and abilities.  In addition to attracting PhD statisticians and graduate students majoring in statistics, the conference has the potential to draw participation from the broader data science community that includes researchers from bioinformatics, business analytics, computer science, electrical and computer engineering, economics, and social sciences among other areas.  The Conference on Predictive Inference and Its Applications will provide a valuable venue for developing new methodologies that enable accurate prediction and assessment of uncertainty in our data-rich world.  Such methodologies provide valuable insights and lead to better decision making in science and industry.<br/><br/>Problems involving the prediction of unobserved but eventually observable quantities are ubiquitous in the modern world.  Historically, the discipline of statistics has placed greater emphasis on hypothesis testing and parameter estimation than it has on prediction.  Other research communities connected to but not equivalent to statistics, including computer science, machine learning, analytics, big data, data mining, and data science have more fully embraced prediction problems and developed a variety of useful prediction tools.  Nonetheless, statistical thinking and innovation have an important role to play in addressing prediction problems now and in the future.  In some cases, statistical thinking can be used to generate appropriate measures of uncertainty to accompany point predictions provided by existing tools.  In other cases, statistical thinking can lead to methods for predictive inference that strike a better balance between bias and variance or make more efficient use of the data than existing approaches.  The formal presentations and ideas that arise from discussions at the conference are intended to motivate scientific progress and result in publications that will appear in refereed journal articles.  This research will enhance the state of the art in prediction methodology, which in turn will lead to advances in many fields where accurate predictions and accurate assessments of prediction error are essential.  The conference website is available at PredictiveInference.github.io.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811405","Collaborative Research: Multi-Scale Modeling of Non-Gaussian Random Fields","DMS","STATISTICS","09/01/2018","07/12/2018","Debashis Paul","CA","University of California-Davis","Standard Grant","Yong Zeng","08/31/2022","$150,000.00","Thomas Chun Man Lee","debpaul@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","Data collected on various environmental, geophysical and meteorological processes often exhibit different modes of variability, especially at different scales. An accurate description of the features of the fluctuations in these data can improve scientific understanding of the physical phenomena. Development of new statistical tools for modeling such geophysical processes can also enhance the ability to monitor and predict the impact of their fluctuations on communication systems and sensory networks. Despite the ubiquity of such data, few statistical methodologies are currently available to describe such spatiotemporal scalar and vector random fields globally on a spherical domain. One key objective of this project is to propose a multiscale approach for constructing non-Gaussian random fields on a sphere, that on the one hand provides a flexible mathematical framework for modeling, and on the other hand, enables one to fit these models by using modern computational tools. A further objective is to extend the methodologies to deal with data observed on graphs and networks. The project also aims to demonstrate the effectiveness of the proposed methodologies in enhancing scientific understanding of geophysical processes by analyzing ground-based and satellite-based measurements of the earth's magnetic fields.<br/> <br/>The proposed statistical framework for spherical processes is based on the idea of multiresolution analysis on a sphere. In this application, a class of needlet frames on the unit sphere is utilized as a building block to construct spatio-temporal scalar and vector fields on the unit sphere that satisfy natural physical constraints such as being curl-free or divergence-free, thereby enabling a flexible approach to approximating physical processes.  Parametric statistical models are proposed to model random vector fields on the unit sphere and spherical shells. These random fields are represented in terms of vectorial needlets and can exhibit non-Gaussian features. A suite of methodologies is proposed under this modeling paradigm to analyze and predict large-scale spatiotemporal scalar and vector processes arising in geophysics, such as ground and satellite based measurements on the earth's main magnetic field or on ionospheric electro-magnetic fields. Theoretical questions related to the structure and properties of the proposed vectorial needlets and the random vector fields represented by them are also investigated. The flexible framework of modeling random fields through multiresolution analysis is further exploited to construct non-Gaussian processes on graphs by means of graph spectral wavelets. This collaborative project requires bringing together skills and knowledge from disparate areas such as multiresolution analysis, spatial statistics, spectral graph theory, Bayesian and large-scale computation, space physics, and geophysics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810975","Cluster Validation Without Model Assumptiions","DMS","STATISTICS","08/01/2018","07/18/2020","Marina Meila","WA","University of Washington","Continuing Grant","Yong Zeng","07/31/2022","$275,000.00","Zaid Harchaoui","mmp@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","","$0.00","This project uses tools from optimization and statistics to put in the hands of practitioners a suite of methods to validate the results of clustering. Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups. The methods to be developed will recognize the cases when the data is well clustered and, in these cases, will provide a ""certificate of stability"" guaranteeing this. This research will develop validation methods for several widely used clustering paradigms (such as K-means, Spectral Clustering, finite Mixture Models) in realistic data scenarios. These methods will be integrated and disseminated with the big data open source platform megaman, developed and maintained by the group of PI Meila.<br/><br/>The proposed approach is based on the notions of good and stable clustering. ``Good'' means that clustering C fits the data well, according to the current clustering paradigm. ``Stable'' means that the only clusterings that fit well are small perturbations of C. This can only happen when C captures structure present in the data. While goodness can be easily checked in practice, stability is not a property that can be verified directly. The core of this project is to find conditions on the data and C that guarantee stability AND are practically verifyable. Such results are known as stability results. The project outlines two novel approaches to this goal. The first approach is based on using convex relaxations to the original clustering problem. The second approach is based on ``recycling'' existing theoretical results proved under assumptions about the data generating model. Often, the proof of such a result contains the elements of a model free stability proof. Thus, this project will be using existing statistical theory in a novel way. Among convex relaxations for clustering, the relaxations to a Semi-Definite Program (SDP) are especially promising. Therefore, ways to accelerate the validation algorithms by exploiting the special structure of the SDPs will be explored.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811767","Estimation, inference and testing for large-scale directed network models","DMS","STATISTICS","07/01/2018","05/15/2018","Garvesh Raskutti","WI","University of Wisconsin-Madison","Standard Grant","Yong Zeng","12/31/2022","$150,000.00","","raskutti@cs.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","Large-scale interaction networks naturally arise in many modern scientific applications. For example, in biochemistry and systems biology, amino acids in different locations of the protein sequence interact, while in computational neuroscience, connectivity networks amongst neurons in the brain naturally trigger responses to particular stimuli. This project will develop reliable and scalable algorithms for learning the underlying interaction network amongst many nodes. Due to both the scale, complexity, and the changing data technologies in the applications described above, the solutions to the challenges addressed in this project will lead both to the development of novel theory and methodology, and the implementation of new algorithms for the application domains. <br/><br/><br/>The goal of the project is to address the challenge of estimation, inference and testing for large-scale network models. Given the size of the networks generated, this project presents a number of computational and statistical challenges the PI will address by focusing on two methodologies: (i) multivariate time series models; (ii) directed graphical models. The PI's prior work has developed new theory and methodology both for large-scale non-linear time series models and directed graphical models. This prior work points to a number of significant open challenges for both methodologies that this project will. These challenges include: (i) lack of sample size/statistical resources for learning complicated dependence structures; (ii) computational challenges due to non-convexity and large search-spaces for dependence models; (iii) incorporating domain knowledge and scientific experiments into the estimation methodologies; and (iv) exploiting learned networks for hypothesis testing, inference, and parameter estimation. This project will address these challenges and these contributions will lead to the development of new methods for network learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1745640","RTG: Advancing Machine Learning - Causality and Interpretability","DMS","STATISTICS, WORKFORCE IN THE MATHEMAT SCI","08/01/2018","07/20/2022","Deborah Nolan","CA","University of California-Berkeley","Continuing Grant","Yong Zeng","07/31/2024","$1,908,227.00","Samuel Pimentel, Jasjeet Sekhon, William Fithian, Peng Ding, Bin Yu, Avi Feller","nolan@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269, 7335","7301","$0.00","Faculty in the Statistics Department at the University of California, Berkeley develop an integrated program of research and education to support undergraduate research experiences, graduate research traineeships, and postdoctoral fellowships. The common research theme of the training activities is how to leverage the predictive power of statistical machine learning to address questions of causality and interpretability. The project aims to prepare the next generation of statisticians and data scientists to tackle new, important problems that arise from the analysis of massive data. Intuitively it seems that more reliable and precise inferences can be drawn from larger data sets. However, decisions and interventions must be interpretable and justified by statistical measures of uncertainty, which are challenging in this setting.  This program will infuse ideas, energy, and resources in an integrated way at all levels of the educational program, from the undergraduate major to the postdoctoral experience, recruiting students and preparing them to participate in the extraordinary range of opportunities in this exciting new field.<br/><br/>The research in this project will pursue theory to bridge the gap between causal inference and machine learning research, including high-dimensional inference, multiple testing, causal inference with interference, and causality and gene expression. The project is at the frontiers of statistics and data science, bridging the divide between machine learning and causal inference with potential impact far beyond the discipline of statistics. Plans are to redesign and expand the engagement of undergraduates in research through a graduate student mentorship program; to design new courses at the graduate and undergraduate levels, including an introductory course that builds on connections between data science, social sciences, and ethics; and to enhance graduate research training via a research symposium. The program will include a graduate professional development training series that addresses topics in technology, presentation and writing skills, and building an inclusive science community.  The project will also provide significant training in teaching for graduate students and postdoctoral associates. Through a combination of channels, the innovations in training will spread to other institutions and disciplines, e.g., demonstrating the power of machine learning in policy and education settings where causal inference is central. The program also includes the development of educational materials with plans to disseminate them widely throughout the broader community. The project will emphasize recruitment and retention efforts targeted to increase the diversity of domestic students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1749789","CAREER: Advances in Multi-scale Bayesian Inference and Learning on Massive Data","DMS","STATISTICS, Division Co-Funding: CAREER","09/01/2018","07/26/2022","Li Ma","NC","Duke University","Continuing Grant","Yong Zeng","08/31/2024","$400,000.00","","li.ma@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269, 8048","1045","$0.00","Massive data present unprecedented opportunities for advancing our understanding of various scientific and social phenomena. With sufficient data and the appropriate statistical tools, researchers can now hope to recover structures in the data that were once deemed too intricate to identify with traditional ""small"" data.  Extracting complex hidden structures in massive data often requires flexible nonparametric methods; however, there are several fundamental challenges that make existing nonparametric methods impractical or inadequate.  At the core of these challenges is a conflict between two essential aspects in big data analysis: (i) the need for flexible methodology for capturing complex features and (ii) the cost, both computational and statistical, associated with this additional flexibility. Effective resolution of this fundamental conflict requires new paradigms of nonparametric inference. The long-term research objective of this project is to develop inference paradigms, including theory, methods, algorithms, and software, for nonparametric inference and learning that effectively resolve this fundamental conflict.  The research will lead to the development of statistical tools that meet urgent needs for scalable nonparametric data analysis in a wide range of fields, including biology, economics, astrophysics, chemistry, and information technology. The project will address the integration of research with educational activities through teaching and mentoring of undergraduate and graduate students, and outreach to students from local colleges.<br/><br/>This project will develop and investigate a particularly promising paradigm, multi-scale divide-and-conquer, to address the fundamental conflict between flexibility and cost.  Specific inference problems to be addressed cover a wide range of nonparametric inference and learning objectives, and can be organized into three research thrusts: (i) joint nonparametric modeling of multiple data generative processes; (ii) characterizing dependency between random variables/vectors; and (iii) response-domain ensemble supervised learning. Beyond addressing these specific objectives, the proposed research will introduce theoretical and computational devices for evaluating and improving the statistical and computational efficiency of multi-scale divide-and-conquer methods in general. The output of the research will include practical methods and algorithms for carrying out a variety of important nonparametric inference tasks on massive data, as well as general guiding principles for effective multi-scale statistical analysis. The research output will be disseminated through publications, presentations, and open-source software to the scientific community, and society at large.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833416","Emerging Statistical and Quantitative Issues in Genomic Research in Health Sciences","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2018","08/03/2023","Xihong Lin","MA","Harvard University","Standard Grant","Yong Zeng","08/31/2024","$90,000.00","","xlin@hsph.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269, 7454","068Z, 7556","$0.00","The 2018 Conference of the Program in Quantitative Genomics (PQG), entitled ""Biobanks: Study Design and Data Analysis"" will take place at the Joseph B. Martin Conference Center at the Harvard Medical School from November 1-2, 2018.  This is the first of three years of the Program in Quantitative Genomics (PQG) Conference series to be supported by this award. This long-standing conference series, open to the whole research community, focuses on timely interdisciplinary discussions on emerging statistical and computational challenges in genetic and genomic science. The focus of each conference changes to reflect the evolution of scientific frontiers. Key to the success of the series is its interdisciplinary nature, bringing quantitative and subject-matter scientists together to discuss statistical and quantitative issues that arise in cutting-edge genetic and genomic research in human disease. The impetus for the 2018 conference theme comes from the proliferation of large-scale biobanks worldwide, composed of massive genetic and genomic data, epidemiological data, Electronic Medical Records, wearable devices and imaging data. The use of biobanks is becoming an essential and potentially revolutionizing approach to biomedical research, with the capacity to improve the prevention, diagnosis, and treatment of a wide range of illnesses, and to advance personalized health. This conference aims at discussing key statistical and quantitative challenges in biobank studies, including biobank design to meet a wide array of needs, strategies for improving phenotyping accuracy, data harmonization across biobanks, and novel statistical and data science methods for analysis of biobank data. The conference is open to the whole research community and particularly encourages participation of junior faculty and researchers, postdoctoral fellows, students, women and members of underrepresented groups. The participants will discuss and critique existing quantitative methods, discuss in-depth emerging statistical and quantitative issues, and identify priorities for future research in the design and analysis of biobank data. The research presented will be broadly disseminated in publications in scientific journals and websites.  <br/><br/>For the 2018 PQG conference we have assembled an interdisciplinary team of speakers experienced in biobank development and study design, including statisticians, medical informaticians, geneticists, epidemiologists, and clinical scientists. Three emerging topics of substantial current interest include the design of population Biobanks; phenotyping and harmonization across biobanks; and biobank data analysis.  The conference discussion will revolve around 1) examples of different types of biobanks including different examples of recruitment strategies, selection of genotype / technologies, genotype resources and future directions, design strategies: cross-section, longitudinal, representative probability sampling, and increasing diversity and expanding to the Americas, Africa and Asia; 2) issues of phenotyping and harmonization across biobanks including new methods for improving phenotyping, harmonization within and across biobanks, enriching phenotyping using registry data (time to event), combining different biobank/registry data across the world, bias and missing data, misclassification in electronic health records (EHR) and other design heterogeneity issues in EHR databases; and finally 3) issues related to data analysis methods, including how to make analysis computationally scalable, analysis of related samples and admixture samples, multiple phenotypes and PheWAS analysis, missing data, misclassification of phenotypes, handling small number of cases for some diseases, risk prediction, selection of appropriate controls, integrative analysis across biobanks, and integrative analysis of different data types (genotype and phenotype, or transcriptomic and metabolomic data) or replicating phenotypes across biobanks. For more information, see https://www.hsph.harvard.edu/2018-pqg-conference/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840278","Statistics at a Crossroads: Challenges and Opportunities in the Data Science Era","DMS","STATISTICS","08/01/2018","08/07/2018","Xuming He","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","07/31/2021","$175,076.00","","xmhe@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","7556","$0.00","The workshop ""Statistics at a Crossroads: Challenges and Opportunities in the Data Science Era"" is scheduled to take place at the Crystal Gateway Marriot, Arlington, Virginia, October 15-17, 2018.  A previous workshop was held at the National Science Foundation in 2002 to discuss the future challenges and opportunities for the statistics community.  That was a time when the statistics community saw rapid changes and sustained growth from the emergence of more and larger-scale data. Since then, the growth of the field, including the size of the undergraduate and graduate programs in statistics and the breadth of interactions between statistics and other fields, has accelerated.  In the meantime, both the public and the private sectors have embraced big data, as more and more people recognize that big data can provide insights into the nature of biological processes, precision medicine, climate change, social and economic behavior, risk assessment and decision making. The statistics community recognizes that we are at a crossroads with an unprecedented opportunity to modernize the discipline to become the major player in data science, but also with a non-ignorable risk to make ourselves obsolete in the broad community of data science.  The proposed workshop asks a critical question, where do we go from here? The workshop seeks to invite leading researchers and educators in statistics and data science to address the question and to produce a report narrating the challenges to, and opportunities for, statistics stakeholders (e.g., individual researchers, academic departments, and funding agencies). <br/><br/>The steering committee of the workshop has identified six themes for in-depth discussion. They are (1) Foundations of statistics and data science; (2) Statistics and computation; (3) Emerging applications; (4) Data challenges; (5) Inference in the age of big data; and (6) Statistics education in the new era. These themes were identified to cover a wide range of issues and research topics that are timely and forward-looking for the purpose of discussion.  Pre- and post-workshop discussions via online forums are planned to seek broader community input. The workshop report will be made available to the National Science Foundation and to the scientific community at large. The recommendations made at the workshop will provide guidance to young researchers and students who pursue careers in quantitative fields and beyond. The workshop will also promote participation of women and other under-represented groups in the research community. More information can be found at the workshop website https://hub.ki/groups/statscrossroad/overview.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810829","Statistical Methods for Multi-Study Predictions","DMS","STATISTICS","08/01/2018","07/28/2020","Giovanni Parmigiani","MA","Dana-Farber Cancer Institute","Continuing Grant","huixia wang","01/31/2022","$350,000.00","Lorenzo Trippa","gp@jimmy.harvard.edu","450 BROOKLINE AVE","BOSTON","MA","022155418","6176323940","MPS","1269","","$0.00","Science faces significant challenges in relation to replicability of studies. These challenges affect prediction models used in a broad spectrum of business, scientific, and social activities. The investigators have identified underutilized opportunities to make most prediction modeling techniques more likely to produce replicable results by training them on multiple studies, and rewarding good replicability in this training phase.  Recent work indicates that this novel and general strategy provides insight into the replicability of predictions, and is a promising venue for systematic improvement.  As many areas of science and technology are becoming data-rich, multiple datasets are more commonly available for training, and it is also more important that they be simultaneously considered and systematically used for improving replicability. Steps towards more easily replicable predictions would increase public confidence in the scientific process, facilitate dissemination of results, and robustify public engagement with science and technology. <br/><br/>The goal of this project is to make progress in the area of cross-study replication of predictions. The investigators have identified two fundamental and underutilized opportunities: 1) to train on multiple studies; 2) to leverage ensembles of prediction models, each trained on one, or a subset, of the studies. The combination of these two elements can be used to design robust prediction algorithms that are trained to incorporate replicability across different contexts and populations. In this project, the investigators propose to implement and evaluate specific prediction techniques within this paradigm; to investigate their statistical properties theoretically and empirically; to compare them to existing alternative multi-study statistical methods; and to build free, open-source software to implement the successful strategies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811936","Collaborative Research: New Developments in Direct Probabilistic Inference on Interest Parameters","DMS","STATISTICS","08/01/2018","05/11/2018","Todd Kuffner","MO","Washington University","Standard Grant","Pena Edsel","07/31/2021","$100,001.00","","kuffner@math.wustl.edu","ONE BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","1269","9263","$0.00","The Bayesian approach to statistical learning relies on probabilistic models for all observables and unknowns.  The need to model all aspects of the problem can restrict the scope of applications and, more generally, can be a burden to the data analyst who is often only interested in certain features of the unknowns.  This project will develop a mathematically rigorous and computationally efficient framework in which Bayesian learning can be carried out directly in terms of only the features of interest.  This reduces the modeling and computational burden on the data analyst and provides new insights about Bayesian learning more generally<br/><br/>A Bayesian approach is a powerful and rigorous framework for statistical learning.  The downside is that it requires a full model for the observables as well as all unknown quantities, the specification of which can be a burden on the data analyst. In addition to the familiar challenges of prior specification, there are also risks of misspecification biases. A more subtle complication is due to selection effects that result from considering several candidate models.  The data analyst's burden is further exaggerated in situations where only a feature of the unknowns is of interest, i.e., when there is an interest parameter and a (potentially high-dimensional) nuisance parameter and inference is required only for the former.  That is, the Bayesian approach still requires that the data analyst make non-trivial efforts to specify prior distributions and carry out posterior computations relevant only to the nuisance parameter, which can be viewed as a waste.  Yet having access to a posterior distribution for inference on the interest parameter is still a desirable feature, and the proposed research aims to develop a new framework for posterior inference directly on interest parameters.  These direct posteriors (DiPs) effectively target the interest parameter, giving data analysts an opportunity to avoid the seemingly wasteful modeling and computation efforts involving nuisance parameters.  This project will construct DiPs for finite- and infinite-dimensional interest parameters with rigorous theoretical guarantees, and will also develop efficient computational tools to facilitate DiP-based inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811083","Collaborative Research: Highly Principled Data Science for Multi-Domain Astronomical Measurements and Analysis","DMS","STATISTICS","07/15/2018","07/11/2018","Yang Chen","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","06/30/2021","$79,996.00","","ychenang@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Massive data resources are coming online in every conceivable area of human exploration, and particularly in fields that are heavily observation-based such as astronomy and astrophysics. To extract the most information from these data, scientists and statisticians need to conduct highly principled data science, by using methods that are scientifically justified, statistically principled, and computationally efficient. This project outlines plans to achieve this goal while addressing four specific challenges in astronomical data involving space, time and energy.  The proposed research has the dual impact of more reliable statistical methods in astronomy and of new general statistical inference and computational methods. In addition to providing methods and free software, the investigators also plan to communicate to the astronomical community the benefit of principled statistical methods through workshops and sessions at conferences. A fundamental impact of the proposed research is the more general acceptance and use of principled methods among astronomers. The general methods for efficient modeling of scientific phenomena, science-driven classification and clustering, and for statistical computing, can also help to solve complex data challenges throughout the natural, social, medical, and engineering sciences.<br/><br/>Striking advances in both space-based and terrestrial instrumentation continuously increase the quality and quantity of data available to astronomers. Observations are made across the electromagnetic spectrum and compiled into enormous catalogs of high-resolution, but heterogeneous spectrograph, imaging, and time series data. The proposed research aims to use such multi-domain astronomical measurements to better understand the physical environment, structure, and evolution of astronomical individual sources, clusters, and ultimately of the entire universe. There are four major projects.  (1) The PIs will develop methodology to solve the instrument calibration problem, which is a fundamental challenge in astrophysics, by fitting scientifically motivated statistical models to data from multiple astronomical objects observed by multiple instruments. (2) The PIs propose a statistically and computationally efficient algorithm to detect the boundaries of a power law distribution prevalent in various areas of astronomy and of far-reaching importance. (3) The PIs will extend image-processing algorithms designed for detecting point sources to complex extended multi-scale structures via a post-hoc analysis, which makes the computation efficient. (4) With astronomical images exhibiting complex structure, the PIs propose to explore image segmentation methods to distinguish overlapping point sources; the algorithm achieves the flux-conserving property, which is crucial for giving physically meaningful estimates that existing methods lack. These projects all involve significant challenges in developing efficient statistical methods, designing fast computational algorithms, and balancing subtle trade-offs between complexity and practicality. With their extensive and successful track record, the PIs will address these challenges by developing inferential and efficient computational methods under highly-structured models that involve multi-scale structure and/or multiple levels of latent variables. The central theme of the proposed research is the integration and pursuit of three desiderata in each of its four projects: scientific justification, statistical principles, and computational efficiency. This triple-goal advances the development of specifically designed methods that leverage computationally efficient and statistically principled data-driven techniques which explicitly incorporate scientific understanding of the astronomical sources.  This ensures that the statistical analyses enhance the scientists' ability to answer specific questions about the underlying astronomical and physical processes. This strategy requires state-of-the-art statistical inference, sophisticated scientific computing, and careful model-checking procedures, all of which have been the hallmark of the work by this team of investigators.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811734","Incremental Regression Analysis of Streaming Data: Estimating Function Theory and Applications","DMS","STATISTICS","07/01/2018","06/03/2020","Peter Song","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","06/30/2021","$150,000.00","","pxsong@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","The advent of distributed data storage and parallel computing systems such as the Apache Spark has provided opportunities of innovation in data analytics and modeling. This project focuses on regression analysis of streaming data under the Spark's Lambda architecture, aiming to develop a new toolbox of Big Data analytics.  Streaming data refers to a series of data batches that arrives sequentially. Such data collection schemes have become abundant lately in biomedical fields due to the booming of many AI-enhanced medical devices that are designed to monitor safety and effectiveness of medical treatments delivered by smart personalized products, or to measure real-time physiological variables such as heart beats, body temperature, and physical activity. This so-called deep phenotyping technology has significantly changed the way of information acquisition in terms of both volume and velocity. Being the most important data analytics, the regression analysis will be rebuilt in the proposed project to address various challenges from the processing of streaming data. The resulting methodology may be applied to many practical fields, where incremental learning with data streams is of primary interest.  <br/><br/>The overarching goal of this project is to develop an incremental statistical inference to address methodological challenges in regression analysis with streaming data stored in the Spark's Lambda architecture. Efficient incremental methodology requires no use of any historic raw data, rather only historic summary statistics and a newly arrived data batch. At the completion of this project the PI expects to make the following new contributions: (i) To develop a new theory of renewable estimation and incremental inference in the context of estimating functions; (ii) to develop an expansion of speed data flow architecture, called the Rho architecture, in which a new layer is added to carry over updates of inference-related quantities such as the Fisher information; (iii) to apply the proposed methodology in many important regression models, such as the generalized linear models, the generalized estimating equations (GEE), the Cox proportional hazards model, and the quantile regression model. Both python and R packages will be delivered from this project to the public.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811552","Innovated Statistical Inference for Complex and High-Dimensional Data","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2018","06/25/2021","Lingzhou Xue","PA","Pennsylvania State Univ University Park","Standard Grant","Yong Zeng","02/28/2023","$195,115.00","","lzxue@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269, 7454","068Z, 102Z","$0.00","Increasing amounts of high-dimensional data are being collected and analyzed in a diverse range of research areas. In practice, data scientists face significant analytic challenges when exploring and understanding the complex and high-dimensional data. Statistical inference of high-dimensional data is essential in theoretical and applied research of statistics, biostatistics, econometrics, geoscience, machine learning, signal processing, and many others. Big Data has rapidly reshaped statistical modeling and revolutionized statistical analysis. There exist many challenges and open problems, whose solutions require innovative ideas and techniques. This project will address new challenges arising in high-dimensional hypothesis testing. <br/><br/>Testing high-dimensional structural parameters plays a vital role in estimating and quantifying uncertainty, making informed choices, and discovering knowledge from Big Data. In this project, novel statistical methods and theory are developed to study three important topics of high-dimensional hypothesis testing: (1) power enhancement tests for high-dimensional covariance matrices, (2) power enhancement tests for high-dimensional mean vectors, and (3) nonlinear statistical dependence of high-dimensional data. The research outcomes will provide powerful analytic tools for solving open problems in three research topics. The methods and theory are general, and they can be directly extended to address other important hypothesis testings for high-dimensional data such as testing in high-dimensional spiked models. Software packages will be developed to make the research outcomes readily available to other researchers and practitioners.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841149","2018 ENVR Workshop ""Statistics for the Environment: Research, Practice and Policy""","DMS","STATISTICS","08/01/2018","08/03/2018","Veronica Berrocal","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","07/31/2019","$10,500.00","","vberroca@uci.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","7556","$0.00","The workshop, Statistics for the Environment: Research, Practice and Policy, will serve as the biennial workshop of the section on Statistics and the Environment (ENVR) of the American Statistical Association and will be held in Asheville, North Carolina, October 11-13, 2018. The workshop is designed to achieve three major goals: (i) provide a venue for environmental statisticians and practitioners to share ideas, facilitate collaboration and discuss ways to increase the impact and use of statistics in environmental practice and policy; (ii) provide an opportunity for environmental statisticians and environmental scientists to discuss and present novel statistical methods and new research findings in environmental and ecological disciplines; and (iii) provide training in new statistical methods developed to address some of the statistical and computational challenges typically posed by environmental and ecological data.<br/><br/>The workshop will span two and a half days. During the first day, two short courses on high performance computing and hierarchical modeling, respectively, will take place. The second and third day will be devoted to scientific sessions with invited research talks on statistical methods for environmental and ecological data as well as applications. A contributed poster session will be held at the end of the second day. Topics discussed in the invited sessions include: statistical methods to handle massive, spatially correlated datasets and strategies for efficient computing; statistical models for spatial extremes; statistical methods for causal inference in an environmental context and understanding the (causal) effects of environmental exposures on human health and the environment, to name a few. The last day of the workshop will feature an invited panel discussion that will highlight cogent needs in environmental practice and policy and will examine possible avenues for impactful environmental statistics research.  More information on the workshop is available at: http://community.amstat.org/envr/events/envr2018workshop<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810925","Study on Order-of-Addition Experiments","DMS","STATISTICS","08/01/2018","07/22/2020","Dennis Lin","PA","Pennsylvania State Univ University Park","Continuing Grant","Yong Zeng","07/31/2022","$150,000.00","","dkjlin@purdue.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","In many experiments, a number of reactants are added into the system sequentially rather than simultaneously. The formation, e.g., the amount/size/purity of the reaction products often depend on the order-of-addition (OofA) of different reactants. The OofA experiment can be traced back to Fisher (1937), where a lady was able to distinguish (by tasting) from whether the tea or the milk was first added to the cup. This is probably the first popular OofA experiment. In general, there are m required components and one hopes to determine the optimal sequence for adding these m components one after another. Knowing the optimal addition order of components related in production is crucial.  Note that there are m! possible orders to be compared-for the tea-tasting example, there are m!=2!=2 possible orders.  However, it is often unaffordable to test all the m! treatments, and the design problem arises (for example, when m = 10, m! is about 3.6 millions).  Although it is somewhat primitive in Statistics, the OofA effect is frequently mentioned in many areas during the past decades.  This includes bio-chemistry (Shinohara and Ogawa 1998), food science (Jourdain et al. 2009), nutritional science (Karim et al. 2000), and pharmaceutical science (Rajaonarivony et al. 1993), just to name a few. As the order-of-addition effect occurs in all kinds of real experiments as well as computer experiments, successful outcomes of this project can be directly used in many different areas, such as chemistry and biology. Furthermore, the proposed problems provide a golden opportunity for fundamental academic studies in other subjects.<br/><br/>In this project, the PI plans to establish the fundamental theory of evaluating OofA designs. The PI also plans to construct classes of OofA designs, under different requirements of design efficiency and experimental budget.  From the mathematical point of view, the OofA design is to select a subset of the symmetric group formed by all the m! permutations, and some research topics in group theories or combinatorics may arise thereby.  For example, if a pairwise-order (PWO) model is assumed, there are m(m-1)/2 parameters to be estimated, and thus only m(m-1)/2+1 runs are needed (instead of m! runs).  More advanced models are proposed and will be investigated as well.  A fruitful application of computer science is anticipated, because the computational complexity for finding efficient OofA designs tremendously increases as m becomes large. The proposed approach on OofA experiment, although known in many fields, is a novel and new study in Statistics - it is very different in fundamental work for most prior research in this area, with high potential practical values.  Many case studies and applications provide an early indication of the potential success of the proposed study.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1760374","FRG: Collaborative Research: Randomization as a Resource for Rapid Prototyping","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","08/01/2018","04/26/2023","Ilse C.F. Ipsen","NC","North Carolina State University","Standard Grant","Yuliya Gorb","07/31/2024","$366,109.00","","ipsen@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269, 1271","1616, 9263","$0.00","A principled foundation for fast prototyping data analysis methods will be developed.  The main approach will be to use fast randomized matrix algorithms, as developed within the research area known as Randomized Numerical Linear Algebra (RandNLA).  Prior work has shown that these RandNLA algorithms come with strong theory and that they perform well for many practical data science and machine learning problems.  The foundation will develop novel uses of randomization to combine complementary algorithmic and statistical perspectives. The statistical viewpoint attributes randomness to an inherent and desirable property of the data, while the algorithmic viewpoint claims randomness as a computational resource to be exploited.  The coupling of these complementary approaches poses challenging mathematical problems to be investigated in the proposed work.<br/><br/>The proposed work will establish the foundations for fast prototyping in two directions: A Multi-Pronged Direction to bring RandNLA to the next level and explore what is technically feasible; and an overarching Synergy Direction that fuses the results for prototyping. The Multi-Pronged Direction includes the following topics: (i) Matrix perturbation theory, to bridge the gap between traditional worst-case bounds for asymptotically small perturbations on the one hand; and perturbations caused by stochastic noise, and missing or highly corrupted matrix entries on the other hand. (ii) Implicit versus explicit regularization, where randomness as a computational resource for speeding up algorithms additionally contributes to implicit statistical regularization, thereby improving statistical and numerical robustness. (iii) Krylov space methods for fast computation of good warm-starts and computation of surrogate models in the form of low-rank approximations, and specifically a better understanding of these methods in an algorithm-independent setting. (iv) Randomized basis construction methods that use matrix factorizations to compute low-rank approximations at low to moderate levels of accuracy. The Synergy Direction will explore topics like ultra-low accuracy matrix computations in machine learning applications, where merely a correct sign or exponent is sufficient. As a group, the PIs possess unrivaled and complementary expertise in applying fundamental mathematical tools to numerical applications in machine learning, data mining and scientific computing.  Importantly, the proposed methods will have significant impact in big data analysis, scientific computing, data mining and machine learning, where matrix computations are of paramount importance. The proposed work is fundamentally interdisciplinary and will enable fast, yet user-friendly extraction of insight from large-scale data these societally-important scientific domains.  Specifically, the proposed work will (i) create a numerically reliable and robust footing for fast prototyping; (ii) advance mathematics at the interface of computer science and statistics, one of the objectives being a synergy of numerical and statistical robustness; and (iii) advance the development of an interdisciplinary community with RandNLA as a pillar for the mathematics of data. The award will allow the investigators to increase their active engagement in reaching out to undergraduate and graduate students, and research communities in numerical linear algebra, theoretical computer science, machine learning, and scientific domains such as astronomy, materials science, and genetics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1760353","FRG: Collaborative Research: Randomization as a Resource for Rapid Prototyping","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","08/01/2018","06/16/2023","Petros Drineas","IN","Purdue University","Standard Grant","Yuliya Gorb","07/31/2024","$343,223.00","","pdrineas@purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1269, 1271","1616, 9263","$0.00","A principled foundation for fast prototyping data analysis methods will be developed.  The main approach will be to use fast randomized matrix algorithms, as developed within the research area known as Randomized Numerical Linear Algebra (RandNLA).  Prior work has shown that these RandNLA algorithms come with strong theory and that they perform well for many practical data science and machine learning problems.  The foundation will develop novel uses of randomization to combine complementary algorithmic and statistical perspectives. The statistical viewpoint attributes randomness to an inherent and desirable property of the data, while the algorithmic viewpoint claims randomness as a computational resource to be exploited.  The coupling of these complementary approaches poses challenging mathematical problems to be investigated in the proposed work.<br/><br/>The proposed work will establish the foundations for fast prototyping in two directions: A Multi-Pronged Direction to bring RandNLA to the next level and explore what is technically feasible; and an overarching Synergy Direction that fuses the results for prototyping. The Multi-Pronged Direction includes the following topics: (i) Matrix perturbation theory, to bridge the gap between traditional worst-case bounds for asymptotically small perturbations on the one hand; and perturbations caused by stochastic noise, and missing or highly corrupted matrix entries on the other hand. (ii) Implicit versus explicit regularization, where randomness as a computational resource for speeding up algorithms additionally contributes to implicit statistical regularization, thereby improving statistical and numerical robustness. (iii) Krylov space methods for fast computation of good warm-starts and computation of surrogate models in the form of low-rank approximations, and specifically a better understanding of these methods in an algorithm-independent setting. (iv) Randomized basis construction methods that use matrix factorizations to compute low-rank approximations at low to moderate levels of accuracy. The Synergy Direction will explore topics like ultra-low accuracy matrix computations in machine learning applications, where merely a correct sign or exponent is sufficient. As a group, the PIs possess unrivaled and complementary expertise in applying fundamental mathematical tools to numerical applications in machine learning, data mining and scientific computing.  Importantly, the proposed methods will have significant impact in big data analysis, scientific computing, data mining and machine learning, where matrix computations are of paramount importance. The proposed work is fundamentally interdisciplinary and will enable fast, yet user-friendly extraction of insight from large-scale data these societally-important scientific domains.  Specifically, the proposed work will (i) create a numerically reliable and robust footing for fast prototyping; (ii) advance mathematics at the interface of computer science and statistics, one of the objectives being a synergy of numerical and statistical robustness; and (iii) advance the development of an interdisciplinary community with RandNLA as a pillar for the mathematics of data. The award will allow the investigators to increase their active engagement in reaching out to undergraduate and graduate students, and research communities in numerical linear algebra, theoretical computer science, machine learning, and scientific domains such as astronomy, materials science, and genetics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811857","Inference in High-Dimensional Linear Models: Methods, Theory, and Applications","DMS","STATISTICS","08/15/2018","06/14/2020","Zijian Guo","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","07/31/2021","$119,999.00","","zijguo@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","Statistical inferential methods are used to answer questions in a variety of modern science disciplines. For example, in medical studies, whether the treatment effect varies by different individuals and which treatment procedure will be recommended to a future patient given his or her baseline information; in genomics, whether different traits share common genetic etiology and how gene expression levels are regulated by genetic variants. For these complex studies, high-dimensional linear models have become ubiquitous to approximate the data generating mechanism and extract useful information. Despite significant advances on estimation, inference problems in high-dimensional linear models have been far less understood, from both methodological and theoretical perspectives. In this project, the investigator plans to address the methodological and theoretical challenges of high-dimensional statistical inference problems, including confidence interval construction and hypothesis testing, and apply the developed methods to answer important scientific questions encountered in genomics and health studies.                                <br/>                        <br/>                <br/>The research objective is to construct confidence intervals and conduct hypothesis testing in a simultaneous consideration of multiple sparse high-dimensional linear models, which are important in answering scientific questions encountered in a wide range of applications. Specifically, the investigator will tackle statistical inference problems by the following means: (1) constructing confidence intervals for functionals of multiple regression vectors and conduct significance tests for groups of variables; (2) testing the existence of heterogeneity in treatment effects and compare efficacy of different treatment procedures for a specific individual; (3) studying multiple testing procedures for a large number of functionals; and (4) applying the developed inferential procedures and collaborate with researchers in genomics and epidemiology to answer important scientific questions about genetic relatedness network and personalized treatment comparison through analyzing large-scale data sets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812198","Lassoing Eigenvalues: A Classical and a Robust Approach","DMS","STATISTICS","07/01/2018","06/29/2018","David Tyler","NJ","Rutgers University New Brunswick","Standard Grant","Yong Zeng","06/30/2023","$149,997.00","","david.tyler@rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","The need to analyze multivariate data arises in many disciplines, including computer science, engineering, meteorology, chemometrics, psychology, sociology, biology, and genetics, among others. A primary goal of multivariate statistical analysis is to model and understand the complex interrelationships between different measurements or variables. With current trends in the sciences, an increasingly common occurrence is the collection of large amounts of information on each individual sample point or experimental unit, even though the number of sample points or experimental units themselves may remain relatively small. This results in an extremely large number of parameters or interrelationships between variables to consider, but with insufficient data to adequately model these relationships using classical statistical methods. This research project aims to investigate novel ways to model such high-dimensional data based on relatively small sample sizes. Another issue that arises when many measurements are recorded on each sample point is that of large errors or outliers in the measurements. This may make the conclusion based on classical statistical methods suspect if the outliers are not detected.  For high-dimensional data, though, detecting outliers is known to be problematic, and so an alternative is to use robust statistical methods, that is, methods producing valid conclusions even if the data contains bad data points. The robustness of the statistical methods developed within the research project will be evaluated.<br/><br/>This project will use penalization methods, which have a long history within statistics, for developing models and estimation procedures for high-dimensional covariance matrices. It has long been recognized that the larger and smaller sample eigenvalues of random matrices are heavily biased upwards and downwards respectively, even for moderately large sample sizes. This problem can be addressed by using penalization methods, which shrink eigenvalues together. Such shrinkage, though, cannot be accomplished using the usual penalties which are convex functions of the precision matrix. This project will employ geodesic convex penalties. Furthermore, some novel non-smooth geodesic convex penalties are to be introduced, which not only shrink eigenvalues together but also have a lasso-type effect of creating subsets of equal eigenvalues. This non-smooth penalization approach thus yields a model selection method, or more specifically a multi-spiked covariance model selection method. The geodesic convex penalization approach is to be first developed under the classical multivariate normal setting. Methods developed under this setting, though, are well known to perform poorly if the multivariate normal model does not hold.  A simple and often used approach for making classical methods more robust is the plug-in method, that is, to simply replace the role of the sample covariance matrix in a method with a robust alternative. For modest sample sizes relative to the dimension of the data, such plug-in methods tend not to differ greatly in performance from those utilizing the sample covariance matrix. To address this shortcoming, non-smooth penalized M-estimators of the covariance matrix are to be developed and studied. Here, the concept of geodesic convexity plays a crucial role.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811866","Optimal Shrinkage and Empirical Bayes Prediction under Asymmetry, Censoring and Nonexchangeability","DMS","STATISTICS","06/01/2018","05/12/2020","Gourab Mukherjee","CA","University of Southern California","Continuing Grant","huixia wang","12/31/2021","$127,443.00","","gmukherj@marshall.usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","MPS","1269","","$0.00","In every branch of big-data analytics, it is now commonplace to use notions of shrinkage in the construction of robust algorithms and predictors. The concept of shrinkage is important because it provides an elegant framework for combining information from related populations and often leads to substantial improvements in the performances of algorithms used for simultaneous inference. Driven by applications in a wide range of scientific problems over the last decade, the traditional roles of statistical shrinkage have rapidly evolved as new perspectives have been introduced to address and exploit complex, latent structural properties of modern datasets. These new applications often involve non-standard inferential attributes such as asymmetric predictive objectives as well as intricate modeling caveats, such as nonexchangeable prior structures and censored observations. These new age statistical problems pose challenges not only in developing flexible shrinkage algorithms but also in optimally tuning them to obtain efficient shrinkage properties. This project will develop new empirical Bayes predictive methods that possess optimal shrinkage properties and can produce significant enhancements over existing algorithms built on the mathematical convenience of symmetric loss functions and exchangeable prior structures.<br/><br/>The common theme underlying this project is that of using optimal shrinkage properties to develop efficient predictive methods. Existing shrinkage algorithms rely heavily on decision theoretic identities that break down under asymmetry and nonexchangeability, and so, there is an urgent need to develop new statistical methodologies, theories, and algorithms. The PI will develop new conditionally linear decision rules for prediction under asymmetry in nonexchangeable Gaussian hierarchical models and will extend the proposed methodologies to non-Gaussian models as well as to settings with non-linear structural constraints. Additionally, optimal empirical Bayes rules will be developed that will work with censored data and can be used for prediction in multi-stage decision making scenarios with asymmetric objectives. The results developed in this project will provide practitioners with an improved understanding of where existing prediction approaches fail under asymmetry, censoring, and nonexchangeability and why algorithms specifically developed to operate under these conditions should be used.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811315","Statistical Machine Learning Methods for Complex Data Sets","DMS","STATISTICS","08/01/2018","08/03/2018","Kean Ming Tan","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","10/31/2019","$119,988.00","","keanming@umich.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","Recent advances in science and technology have led to the generation of massive amounts of large-scale data with complex structures, including genomics, neuroimaging, and microbiology data. These large-scale datasets pose significant statistical and computational challenges to data analysis. Firstly, widely used statistical methods yield unstable estimates and are not computationally scalable to modeling large-scale data sets. Secondly, complex data sets are often accompanied by outliers due to possibly measurement error or heavy-tailed random noise. For instance, in genomic studies, it has been observed that the distribution of gene expression levels is generally heavy-tailed, that is, the data contain a lot of extremely large values. Classical statistical methods will yield biased estimates and spurious scientific discovery if these outliers are not taken into account during model estimation and inference. This project aims to develop scalable and robust multivariate statistical methods to address the aforementioned problems. <br/><br/>In this project, the investigator uses a combination of regularization and statistical optimization techniques to develop novel multivariate statistical methods for analyzing complex high-dimensional data sets. The first part of the project concerns the sparse generalized eigenvalue problem, which arises naturally in many statistical models such as partial least squares, canonical correlation analysis, sufficient dimension reduction, and Fisher's discriminant analysis. The investigator will develop a general framework for solving the sparse generalized eigenvalue problem and make available a wide range of statistical models for analyzing high-dimensional data. Furthermore, the investigator will study the theoretical properties of sparse generalized eigenvalue problem, and this will lead to the understanding of various statistical models that are previously not well understood in the high-dimensional setting. The second part of the research project focuses on a class of robust sparse reduced rank regression models. The investigator will develop efficient algorithms and high-dimensional asymptotic analysis for the resulting estimators under the Huber loss function, and quantify the bias-robust tradeoff between using Huber loss and squared error loss. This research project will also deliver easy-to-use software packages for fitting the developed methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811740","Statistical and Computational Guarantees of Three Siblings: Expectation-Maximization, Mean-Field Variational Inference, and Gibbs Sampling","DMS","STATISTICS","08/01/2018","07/22/2020","Huibin Zhou","CT","Yale University","Continuing Grant","huixia wang","07/31/2021","$299,999.00","","huibin.zhou@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","9263","$0.00","Three sibling algorithms, expectation-maximization (EM), mean-field variational inference, and Gibbs sampling, are among the most popular algorithms for statistical inference. These iterative algorithms are closely related: each can be seen as a variant of the others. Despite a wide range of successful applications in both statistics and machine learning, there is little theoretical analysis explaining the effectiveness of these algorithms for high-dimensional and complex models. The research presented in this project will significantly advance the theoretical understanding of those iterative algorithms by unveiling the statistical and computational guarantees as well as potential pitfalls for statistical inference. The wide range of applications of EM, mean-field variational inference, and Gibbs sampling ensure that the progress we make towards our  objectives will have a great impact in the broad scientific community which includes neuroscience and social sciences. Research results from this project will be disseminated through research articles, workshops and seminar series to researchers in other disciplines. The project will integrate research and education by teaching monograph courses and organizing workshops and seminars to help graduate students and postdocs, particularly minority, women, and domestic students and young researchers, work on this topic. In addition, the PI will work closely with the Yale Child Study Center and the Yale Institute for Network Science to explore appropriate and rigorous algorithms for neuroscience, autism spectrum disorder, social sciences, and data science education.<br/><br/>The PI studies these iterative algorithms by addressing the following questions: 1) what is the sharp (nearly necessary and sufficient) initialization condition for the algorithm to achieve global convergence to optimal statistical accuracy? 2) how fast does the algorithm converge? 3) what are sharp separation conditions or signal strengths to guarantee global convergence? 4) what are the estimation and clustering error rates and how do they compare to the optimal statistical accuracy? There are three stages to developing a comprehensive theory for analyzing iterative algorithms: 1) studying statistical and computational guarantees of EM for Gaussian mixtures for both global parameter estimation and latent cluster recovery, 2) extending EM to mean-field variational inference and Gibbs sampling, and considering a unified analysis for a class of iterative algorithms, 3) extending Gaussian mixtures and Stochastic Block Models to a unified framework of latent variable models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1832831","Summer 2018 Causal Inference Workshops","DMS","STATISTICS","05/15/2018","05/22/2018","Alexander Volfovsky","NC","Duke University","Standard Grant","Gabor Szekely","04/30/2019","$30,000.00","","alexander.volfovsky@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","7556","$0.00","This award provides partial support for new researchers to attend two conferences on causal inference in summer 2018: The Atlantic Causal Inference Conference (ACIC) held in Pittsburgh, PA on May 22-23, 2018 and the Causal Workshop at the Conference on Uncertainty in Artificial Intelligence (UAI)  held in Monterey, CA on August 6-10, 2018. Both conferences serve as flagship gatherings of leaders in reasoning about causes of observed effects.  In recent years, causal inference has seen important advances, especially through a dramatic expansion in its theoretical and practical domains. Machine learning methods have focused on ultra-high-dimensional models and scalable stochastic algorithms, whereas more classical causal inference has been guiding policy in complex domains involving economics, social and health sciences, and business. Through such advances, a powerful cross-pollination has emerged as a new set of methodologies promising to deliver more robust data analysis than each field could individually.<br/><br/>The primary purpose of the two conferences is to advance the study of causal inference and to bridge gaps between different communities within causal inference, by promoting interaction and networking among new researchers. At both conferences, the supported participants will present their research via invited and contributed talks or via poster presentations. Since the conferences draw on causal inference communities in statistics, biostatistics, computer science, and other disciplines, these conferences present a great opportunity for new researchers to see the breadth of opportunities for research and collaboration in this area.  More information is available at the conference web sites:<br/><br/>ACIC: https://www.cmu.edu/acic2018/ <br/><br/>UAI: https://sites.google.com/view/causaluai2018<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810979","Optimal Nonparametric Estimation of High-Dimensional Functionals in Causal Inference","DMS","STATISTICS","08/01/2018","07/25/2020","Edward Kennedy","PA","Carnegie-Mellon University","Continuing Grant","Yong Zeng","07/31/2022","$150,000.00","","Edward@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","Causality is central to many of the most important questions in science and policy: Which cancer treatments are most effective for which patients? Would more strict gun laws result in fewer homicides? Causal inference is concerned with formulating such questions mathematically, exploring whether answers can be gleaned from data, and if so, determining how well and with what statistical methods. Classical methods in causal inference tend to aim at simple summary effects, such as how outcomes would change on average if a treatment were applied to an entire population versus not at all. However, with big data, investigators can ask more complicated questions, such as how treatment effects vary with complex covariate information, or how outcome densities would change with sequential treatments applied over many timepoints. In this project the PI will develop flexible statistical methods for answering such questions without imposing strong assumptions, and will study optimality, i.e., how well one can possibly answer such questions. <br/><br/>The above questions can be framed as high-dimensional functional estimation problems. The classical approach here is to use strong parametric assumptions to reduce these problems to finite-dimensional ones. This allows for standard methods and a deep understanding of optimality, but when true parametric structure is unknown, incorrect assumptions can result in sizable bias and irrelevant efficiency bounds. In fact, little is known in the nonparametric case. Thus, the PI will develop novel nonparametric estimators of high-dimensional causal functionals, study their risk, provide confidence bands and inferential tools, and explore minimax lower bounds. All methods will be made available in R software. This proposal focuses on the foundational problems of (a) estimating counterfactual densities and (b) heterogeneous treatment effects, in three prominent domains of causal inference: (i) unconfounded point treatments, (ii) instrumental variables, and (iii) time-varying treatments. In addition, the PI will establish a general framework for bias-corrected estimation and inference for high-dimensional functionals.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1840203","Inference of Network Structure from Grouped Data","DMS","STATISTICS","05/25/2018","06/14/2018","Yunpeng Zhao","AZ","Arizona State University","Standard Grant","Gabor Szekely","01/31/2021","$26,367.00","","Yunpeng.Zhao@asu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1269","","$0.00","Networks, which can be viewed as data structures consisting of nodes (vertices) connected by links (edges), have drawn wide attention in a variety of scientific and engineering areas. The applications include friendship and collaboration networks in social sciences, food webs and gene regulatory networks in biology, network games in economics, the Internet and World Wide Web in computer science, as well as many others. Traditionally, statistical network analysis focuses on modeling explicit network structure. For physical networks, like power grids, links between nodes are well defined and can usually be directly observed. By contrast, explicit network structure may not be observable in other fields, especially in social sciences and biology. In these areas, the raw data available is usually behavior of nodes, which is generally presumed to be the result of latent network structure. This project will study the problem of reconstructing implicit networks from a special data structure--grouped data. Each observation of such data is a group of individuals which are observed to appear together. <br/><br/>The project is composed of three parts, all concerning rigorous statistical methods for network inference from grouped data. The first part focuses on networks with continuous edge weights. The PI considers two intriguing properties -- self-sparsity and identifiability of Star Model (recently introduced by the PI and his PhD student), and proposes L1 regularization and low-rank matrix factorization in order to reduce the complexity of this model. In the second part, networks with binary links are considered. The PI proposes to study two different methods to estimate the network structures, including a global model based on Erdos-Renyi process and a non-parametric criterion based on subgraph densities. In the third part, the PI considers dependency structure among groups. The Markov property is assumed here, that is, a group generated at any time point only depends on the group structure at the previous time point and the latent network. The PI proposes an intuitive In-and-Out Model under the Markov assumption. The contribution of this project is twofold. Firstly, it is expected that the concept of implicit networks and the study of network inference from grouped data will change some fundamental viewpoints of statistical network analysis. Secondly, the rigorous statistical methods proposed in this project bring new challenging theoretical and computational questions, which will significantly advance the theoretical understanding and computational techniques in this area."
"1811657","Extremes Models and Methods from Transformed Linear Operations","DMS","STATISTICS","06/01/2018","05/10/2018","Daniel Cooley","CO","Colorado State University","Standard Grant","Yong Zeng","05/31/2022","$244,917.00","","Cooleyd@stat.colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1269","","$0.00","Quantification and assessment of risk associated with extreme-in-magnitude and rare events are important in many science, engineering, and business applications. Univariate extremes methods are well-developed, but there is a need for easily implementable statistical methods to describe and model extremal dependence in high dimensions, and in the time series and spatial contexts.  Linear statistical methods including traditional multivariate analysis, time series, and spatial statistics are ubiquitous in non-extreme statistics.  Recently, the PI and coauthor connected the seemingly disparate areas of linear statistical methods and extreme value analyses by utilizing transformed-linear operations. An extension of this approach will be used to develop methods for analyzing high-dimensional extremal dependence, modeling extremal dependence in time, and modeling spatial extremes. Because the proposed work is inspired by existing linear models and methods in the non-extreme setting, the models and methods will be relatively simple and familiar. <br/><br/>This project will produce statistical methods for describing and modeling extremal dependence via applying transformed-linear operations. In a recently submitted paper, the PI and coauthor link linear algebra to regular variation, a widely-used and theoretically-justified framework for extremal dependence, via transformed linear operations. The PI and coauthor obtain a sensible vector space for extremes yielding the notion of basis, propose an extremes analog to the covariance matrix, and perform an eigendecomposition of this matrix useful for understanding high-dimensional tail dependence.  This project aims to develop a simple linear model for extreme spatial data, analogous to the spatial autoregressive model in non-extreme spatial statistics, and transformed-linear time series models inspired by familiar ARMA models, but appropriate for extreme time series data. This project will also develop inference procedures for both the spatial and time series models.  Furthermore, this project will further develop linear methods for understanding extremal dependence in high dimensions and explore the idea of conditional dependence and independence for extremes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1902432","Collaborative Research:  Critical Points and Excursion Probability of Random Fields: Theory and Statistical Applications","DMS","STATISTICS","09/10/2018","01/27/2022","Dan Cheng","AZ","Arizona State University","Standard Grant","Yong Zeng","06/30/2022","$75,463.00","","chengdan@asu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1269","","$0.00","Random fields are playing increasingly important roles in statistics, due to their use as spatial models in many scientific areas such as neuroimaging, astronomy, geosciences, oceanography and microscopy, where many problems involve dependent data at spatial locations. In these applications, researchers are interested in detecting signals hidden in a noisy background which can usually be modeled as a random field. This project aims to derive theoretical properties of random fields and use these results to create efficient statistical tools to extract important and useful information from spatial data. This is done by focusing on specific features of random fields such as peaks, which serve as local representatives of the signal in their immediate vicinity. Thus, local signals can be discovered by detecting peaks whose height is above what would be expected by chance. This project develops such signal detection procedures with rigorous statistical inference theory. By identifying regions of brain activity in brain images to finding stellar objects against the cosmic background radiation, the aforementioned scientific disciplines will benefit from the proposed methods, which provide new efficient tools to analyze spatially dependent data and detect signals in the presence of noise.<br/><br/>This project will study critical points and excursion probabilities of Gaussian and related random fields, and then apply the obtained theoretical results to important statistical problems involving signal detection in image analysis, multiple hypothesis testing and parameter estimation. For critical points, the project will investigate exact computable formulas for the expected number, height distribution, and overshoot distribution, and establish approximations to the moments of the number of critical points. For excursion probabilities, the project will investigate the expected Euler characteristic approximation for smooth Gaussian fields with non-constant variance and for non-Gaussian fields, and fractional Brownian motion on manifolds.  As statistical applications, the project will devise tests for local maxima in non-stationary Gaussian noise and testing of cluster extent and mass for detecting signal regions.  In these applications, p-values are computed using the developed theory.  Statistical applications also include estimation of parameters in the height distribution of local maxima and detection of change-points. This project uses interdisciplinary tools from probability, statistics, and geometry to develop the desired theoretical results and statistical methods. It will create interesting connections between several mathematical areas involving topology and random matrices theory, and to other disciplines, including neuroimaging, cosmology, and beyond.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1760316","FRG: Collaborative Research: Randomization as a Resource for Rapid Prototyping","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","08/01/2018","08/02/2018","Michael Mahoney","CA","University of California-Berkeley","Standard Grant","Stacey Levine","07/31/2023","$790,668.00","Ming Gu","mmahoney@icsi.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269, 1271","1616, 9263","$0.00","A principled foundation for fast prototyping data analysis methods will be developed.  The main approach will be to use fast randomized matrix algorithms, as developed within the research area known as Randomized Numerical Linear Algebra (RandNLA).  Prior work has shown that these RandNLA algorithms come with strong theory and that they perform well for many practical data science and machine learning problems.  The foundation will develop novel uses of randomization to combine complementary algorithmic and statistical perspectives. The statistical viewpoint attributes randomness to an inherent and desirable property of the data, while the algorithmic viewpoint claims randomness as a computational resource to be exploited.  The coupling of these complementary approaches poses challenging mathematical problems to be investigated in the proposed work.<br/><br/>The proposed work will establish the foundations for fast prototyping in two directions: A Multi-Pronged Direction to bring RandNLA to the next level and explore what is technically feasible; and an overarching Synergy Direction that fuses the results for prototyping. The Multi-Pronged Direction includes the following topics: (i) Matrix perturbation theory, to bridge the gap between traditional worst-case bounds for asymptotically small perturbations on the one hand; and perturbations caused by stochastic noise, and missing or highly corrupted matrix entries on the other hand. (ii) Implicit versus explicit regularization, where randomness as a computational resource for speeding up algorithms additionally contributes to implicit statistical regularization, thereby improving statistical and numerical robustness. (iii) Krylov space methods for fast computation of good warm-starts and computation of surrogate models in the form of low-rank approximations, and specifically a better understanding of these methods in an algorithm-independent setting. (iv) Randomized basis construction methods that use matrix factorizations to compute low-rank approximations at low to moderate levels of accuracy. The Synergy Direction will explore topics like ultra-low accuracy matrix computations in machine learning applications, where merely a correct sign or exponent is sufficient. As a group, the PIs possess unrivaled and complementary expertise in applying fundamental mathematical tools to numerical applications in machine learning, data mining and scientific computing.  Importantly, the proposed methods will have significant impact in big data analysis, scientific computing, data mining and machine learning, where matrix computations are of paramount importance. The proposed work is fundamentally interdisciplinary and will enable fast, yet user-friendly extraction of insight from large-scale data these societally-important scientific domains.  Specifically, the proposed work will (i) create a numerically reliable and robust footing for fast prototyping; (ii) advance mathematics at the interface of computer science and statistics, one of the objectives being a synergy of numerical and statistical robustness; and (iii) advance the development of an interdisciplinary community with RandNLA as a pillar for the mathematics of data. The award will allow the investigators to increase their active engagement in reaching out to undergraduate and graduate students, and research communities in numerical linear algebra, theoretical computer science, machine learning, and scientific domains such as astronomy, materials science, and genetics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811291","Collaborative Research: Information-Based Subdata Selection Inspired by Optimal Design of Experiments","DMS","STATISTICS","07/15/2018","07/11/2018","Min Yang","IL","University of Illinois at Chicago","Standard Grant","Yong Zeng","06/30/2022","$60,000.00","","myang2@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","","$0.00","Extraordinary amounts of data are collected in many branches of science, in industry, and in government. The massive amounts of data provide incredible opportunities for making knowledge-based decisions and for advancing complicated research problems through data-driven discoveries. To capitalize on these opportunities, it is critical to develop methodology that facilitates the extraction of useful information from massive data in a computationally efficient way. Even the simplest analyses of the data can be computationally intensive or may no longer be feasible for big data. It is however often the case that valid conclusions can be drawn by considering only some of the data, referred to as subdata. This project develops optimal strategies for selecting subdata that retain, as much as possible, relevant information that was available in the massive data set. The methodology helps to identify the most informative data points, after which an analysis can proceed based on the selected subdata only. This facilitates data-driven decisions, scientific discoveries, and technological breakthroughs with computing resources that are readily available.<br/> <br/>Existing investigations for extracting information from big data with common computing power have focused on random subsampling-based approaches, which have as limitation that the amount of information extracted is only scalable to the subdata size, not the full data size. This project develops and expands the Information-Based Optimal Subdata Selection (IBOSS) method proposed by the PIs in the following directions: 1) It combines IBOSS with sparse variable selection methods in linear regression; 2) it develops subdata selection methods for generalized linear models; 3) it constructs computationally efficient algorithms for selecting the most informative subdata; and 4) it develops user-friendly software that supports the methodology.  The research is a significant addition to the field of big data science. It advances a new method for dealing with big data and has the potential to create novel research opportunities in statistical science and other quantitative fields. The results are valuable even when supercomputers are available, because cutting edge high performance computing facilities will always trail the exponential growth of data volume.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1809681","Efficient Designs for Scientific Studies with Repeated Measurements Data","DMS","STATISTICS","07/01/2018","05/10/2018","Sam Hedayat","IL","University of Illinois at Chicago","Standard Grant","Yong Zeng","06/30/2022","$120,000.00","","hedayat@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","","$0.00","The PI will study and explore scientific situations dealing with repeated measurements on all experimental units in space and or time. Such studies are recommended by FDA for pharmacokinetic and pharmacodynamics (PK/PD) evaluations in one or more phases of clinical trials or for bio-availability studies in making regulatory decision for generic drugs. Likewise, such experiments are also highly desirable for studies in epidemiology, social networks, and agricultural field experiments, including marketing, physical and virtual stores. The PI will develop ready-to-use statistical/computational soft wares which will be designed, tested, and freely distributed for dissemination of knowledge on repeated measurements studies. The research outcomes will serve as a platform to a broad class of multidisciplinary researchers and practitioners. Scientists in the fields of functional Magnetic Resonance Imaging (fMRI), pharmacy, medical devices, and generic medicine will be provided with highly useful tools to carry out their studies and make proper and prudent evidence based inference from the collected data under repeated measurements.<br/><br/>Repeated measurements on the same experimental unit, also known as crossover designs, often produce residual/neighbor/correlated effects which need to be modeled and explored. The PI will focus on the scientific situations both in terms of data collection (design) and modeling which have not been adequately addressed and studied before. In the process, computational tools will be developed with thorough study of the theoretical issues. Sound innovative techniques will be developed for identifying cost-effective designs in order to ascertain optimal and nearly optimal estimation of the primary and, if needed, secondary parameters embedded in models under repeated observations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812065","Collaborative Research: Statistical Analysis of Partially Observed Shapes in Two Dimensions","DMS","STATISTICS","08/01/2018","06/14/2021","Juliet Brophy","LA","Louisiana State University","Standard Grant","Yong Zeng","07/31/2022","$24,971.00","","jbrophy@lsu.edu","202 HIMES HALL","BATON ROUGE","LA","708030001","2255782760","MPS","1269","9150","$0.00","Classification of shapes has many important applications in a variety of fields. For example, anthropologists use shape classification on fossilized faunal teeth to reconstruct past environments and on variation in the shapes of stone tools to assess different technological strategies.  Other applications include classification of plants based on the shape of their leaves and identification of shapes of tumors.  While there are many statistical tools that can be used for classification, most methods are based on complete shapes with relatively few methods available for partially observed or incomplete shapes. This project focuses of the development of new statistical methodology, based on the ideas of multiple imputation, for the analysis of partially observed shapes.<br/><br/>This project proposes as a starting point leveraging the ideas of nonparametric, hot-deck type multiple imputation to shapes that are defined by unlabeled points and/or functions, as opposed to shapes defined by landmarks where traditional methods of multiple imputation may be applied.  The proposed method involves matching partially observed shapes to fully observed shapes, randomly choosing a fully observed donor shape among the shapes that are good matches for the partial shape, and then completing the partial shape with the unmatched part of the donor shape. A simulation study will be conducted to compare the relative merits of different partial matching methods.  The imputation framework will be tested using teeth from the Family Bovidae, whose classification plays an important role in biological anthropology for identifying the taxa of specimens, which in turn is used to reconstruct paleoenvirnoments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821289","Collaborative Research: Using Precursor Information to Update Probabilistic Hazard Maps","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, CDS&E-MSS","09/01/2018","08/01/2018","Robert Wolpert","NC","Duke University","Standard Grant","Christopher Stark","08/31/2022","$199,997.00","James Berger","rlw@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1253, 1269, 8069","9263","$0.00","In many natural hazard scenarios, precursory information becomes available before an event. Responding to an impending hazard means that time is limited; analysis and decision-making must proceed on an accelerated timetable. This project develops methodology for responding to signals from a variety of data sources that have been interpreted as suggesting that a volcanic eruption is threatened. Prior research has attempted to elucidate how physical processes predict an eruption. For example, seismic signals, gas emissions, and tilt data have all been implicated as volcanic eruption precursors. But none of these signals have been shown to make robust predictions. This project undertakes a different approach, developing methods to integrate precursor data, decide on the likely evolution of eruption scenarios, and rapidly build simulation studies and statistical emulators, to provide timely and actionable information on which to decide a course of action. The new methodology will provide tools to rapidly construct probability-based hazard forecast maps for cascading geophysical events.<br/><br/>The prediction and management of extreme events, from volcanic eruptions to floods to stock market crashes, requires a careful analysis of the hazard event, its inputs, and its consequences. Data of different kinds, and of differing fidelity, must be incorporated into a detailed analysis of the impending hazard. The investigators will build upon their past research characterizing volcanic hazards. This work provides long-term hazard analysis and provides a bridge from incoming precursory information, such as seismic signals and gas emission, to eruption impacts, such as likely paths of mass flows. The investigators aim to develop methodology to update input distributions for physical simulations and to integrate outcomes into new adaptive designs for surrogate construction, rapid evaluations of limited simulations, and massive parallel emulation. The project will also investigate a methodology based on observed power-law relationships between precursory information and their growth to estimate the time to eruption and other outcomes under uncertain data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811317","Nonparametric Statistical Image Analysis: Theory and Applications","DMS","STATISTICS","05/01/2018","05/08/2020","Rabindra Bhattacharya","AZ","University of Arizona","Continuing Grant","Yong Zeng","04/30/2022","$224,999.00","","rabi@math.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1269","8091","$0.00","Non-Euclidean data are ubiquitous. They arise in many forms such  as digital images for (1) medical diagnostics (MRI, CT scans, DTI of the brain), (2) scene recognition from satellite images, (3) identifying defects in manufactured products, (4) artificial intelligence (robotic identification of objects), etc. Proper geometric descriptions of these require tools from modern differential geometry. Classical statistical methods are inadequate for their analysis; parametric models, which assume the form of the distribution of the underlying data modulo a finite number of unknown parameters, are often misspecified. A model-independent methodology developed by the PI and others has been shown to be very effective in analyzing such data. The present project aims at vastly broadening the scope of this methodology for applications. <br/><br/>A basic component of the methodology proposed is the notion of the Fre'chet mean of a probability Q on a metric space, which minimizes the expected squared distance from a point. The metric space is generally a differential manifold, often provided with a natural Riemannian metric. But it may also be a so called geodesic space of non-positive curvature, including many graphical spaces as well as stratified spaces made up of manifolds of different dimensions glued together. For the methodology to work one must establish (a) the uniqueness of the Fre'chet minimizer and (b) the asymptotic distribution of the sample Fre'chet mean. It is one of the goals of the present project to significantly extend the earlier theory in this regard, opening the way to many new applications. Another important objective is to extend to such spaces the nonparametric Bayes theory of density estimation, classification and regression. One special aim here is to explore an intriguing phenomenon:  in simulation studies with moderate sample sizes, the nonparametric Bayes estimator of the density of Q far outperforms not only the kernel density estimator, but also the MLE when the data are simulated from a parametric model! An understanding of this is expected to lead to a wider and more effective use of the nonparametric Bayes methodology. Finally, the PI proposes to develop a graphical method for robotic vision of objects, with much less computational complexity than that of other methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1854934","Causal Inference Methods for Mediation and Comparisons of Confidence Regions","DMS","STATISTICS","07/01/2018","10/19/2018","Judith Lok","MA","Trustees of Boston University","Standard Grant","Yong Zeng","05/31/2022","$153,267.00","","jjlok@bu.edu","1 SILBER WAY","BOSTON","MA","022151703","6173534365","MPS","1269","","$0.00","In epidemiology, clinical research, and the social sciences, inferences about the causal effects of treatments and risk factors are used to design more effective interventions.  This project focuses on the development of statistical methods for causal inference.  The first part of this project will develop a causal inference method for mediation analysis.  If a treatment has a beneficial effect on an outcome, it is often of interest to investigate what are the pathways by which it affects the outcome. Direct and indirect effects decompose the effect of a treatment into the part that is mediated by a covariate (the mediator) and the part that is not.  For example, in HIV/ AIDS research, it is important to estimate how much of the effect of Antiretroviral Therapy (ART) on mother-to-child-transmission of HIV is mediated by the effect of ART treatment on the HIV viral load in the mother's blood.  In medicine, psychology, political science, and economics, differentiating between indirect and direct effects has become increasingly important.  Therefore, it is paramount that appropriate statistical methods are developed to estimate direct and indirect effects in a variety of settings, including the setting in which there are post-treatment common causes of the mediator and the outcome.  The second part of this project will compare confidence regions.  Recently, there has been extensive discussion in the statistical community about a move away from p-values.  P-values can lead researchers to conclude that a treatment has a significant effect even if that effect is very small, and clinically irrelevant.  Confidence regions are the obvious alternative to p-values, as they provide a range of values of the parameters of interest that are most consistent with the data.  While comparisons of p-values have been extensively researched and confidence regions are routinely reported, comparison of confidence regions has received relatively little attention.  In this project, confidence regions will be compared based on the notion of asymptotic equivalence.  <br/><br/>Natural direct and indirect effects use cross-worlds counterfactuals: outcomes under treatment with the mediator ""set"" to its value without treatment.  Cross-worlds counterfactuals can never be observed, as they involve quantities under two different treatments where only one treatment is given to any particular patient or unit.  The PI has recently proposed organic direct and indirect effects to avoid the use of cross-worlds counterfactuals.  Organic direct and indirect effects also apply when the mediator cannot be ""set"".  For example, the HIV viral load in the mother's blood cannot be set; if it could be set, doctors would set it to zero.  In the first part of this project, organic direct and indirect effects will be extended to settings with post-treatment common causes of the mediator and the outcome.  It will be shown that, in contrast to natural direct and indirect effects, estimators and confidence intervals can be developed in that setting for organic effects.  The second part of this project will compare confidence regions. Most work on the comparison of confidence regions has studied coverage probabilities, confidence interval length, and small sample properties. In this project, confidence regions will be compared for large samples, based on the asymptotic behavior of the Hausdorff distance between the different confidence regions. The Hausdorff distance between partly overlapping intervals is simply the maximum of the difference between the left limits and the right limits of the intervals. The Hausdorff distance has also been defined for non-convex sets and in higher dimensions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811920","Efficient Monte Carlo Algorithms for Bayesian Inference","DMS","STATISTICS","08/01/2018","05/05/2020","Wing Hung Wong","CA","Stanford University","Continuing Grant","Yong Zeng","07/31/2022","$200,000.00","","whwong@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","9251","$0.00","Data sets arising from current applications of statistics and machine learning are of very large size and require large models for their analysis. Bayesian inference and global optimization are two powerful methods for learning from such data, but the large size of the data sets and the resulting computational difficulties greatly limit the applicability of these methods.  The research in this project aims to increase computational efficiency of these methods, thereby substantially expanding their usefulness for the analysis of large data sets. The methods and algorithms from this research will be implemented on modern distributed computing platforms and made freely available for the scientific community. The results will have wide applications in statistics and machine learning.<br/><br/>Specifically, the use of mini-batches in Markov Chain Monte Carlo (MCMC) will be investigated. MCMC is perhaps the most widely used computational approach for Bayesian statistical inference. Since each step in the simulation of the Markov chain requires the scanning of all the observations, for a large data set this computation is prohibitive. On the other hand, in the area of machine learning researchers have found that stochastic optimization techniques, which examine only a mini-batch of data points at a time, can deliver excellent performance. In this project, a framework for unifying mini-batch based MCMC and global optimization will be developed. It is showed that simulation from of a tempered version of the posterior distribution can be approximated by a MCMC process with Metropolis-Hasting updates that depend only on mini-batches. This approach will be combined with eqi-energy sampling to achieve a unified simulation and global optimization methodology. This framework will allow us to improve the performance of both MCMC methods and non-convex global optimization methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811308","Collaborative Research: Highly Principled Data Science for Multi-Domain Astronomical Measurements and Analysis","DMS","STATISTICS","07/15/2018","07/11/2018","Xiao-Li Meng","MA","Harvard University","Standard Grant","Yong Zeng","06/30/2022","$179,972.00","","meng@stat.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269","","$0.00","Massive data resources are coming online in every conceivable area of human exploration, and particularly in fields that are heavily observation-based such as astronomy and astrophysics. To extract the most information from these data, scientists and statisticians need to conduct highly principled data science, by using methods that are scientifically justified, statistically principled, and computationally efficient. This project outlines plans to achieve this goal while addressing four specific challenges in astronomical data involving space, time and energy.  The proposed research has the dual impact of more reliable statistical methods in astronomy and of new general statistical inference and computational methods. In addition to providing methods and free software, the investigators also plan to communicate to the astronomical community the benefit of principled statistical methods through workshops and sessions at conferences. A fundamental impact of the proposed research is the more general acceptance and use of principled methods among astronomers. The general methods for efficient modeling of scientific phenomena, science-driven classification and clustering, and for statistical computing, can also help to solve complex data challenges throughout the natural, social, medical, and engineering sciences.<br/><br/>Striking advances in both space-based and terrestrial instrumentation continuously increase the quality and quantity of data available to astronomers. Observations are made across the electromagnetic spectrum and compiled into enormous catalogs of high-resolution, but heterogeneous spectrograph, imaging, and time series data. The proposed research aims to use such multi-domain astronomical measurements to better understand the physical environment, structure, and evolution of astronomical individual sources, clusters, and ultimately of the entire universe. There are four major projects.  (1) The PIs will develop methodology to solve the instrument calibration problem, which is a fundamental challenge in astrophysics, by fitting scientifically motivated statistical models to data from multiple astronomical objects observed by multiple instruments. (2) The PIs propose a statistically and computationally efficient algorithm to detect the boundaries of a power law distribution prevalent in various areas of astronomy and of far-reaching importance. (3) The PIs will extend image-processing algorithms designed for detecting point sources to complex extended multi-scale structures via a post-hoc analysis, which makes the computation efficient. (4) With astronomical images exhibiting complex structure, the PIs propose to explore image segmentation methods to distinguish overlapping point sources; the algorithm achieves the flux-conserving property, which is crucial for giving physically meaningful estimates that existing methods lack. These projects all involve significant challenges in developing efficient statistical methods, designing fast computational algorithms, and balancing subtle trade-offs between complexity and practicality. With their extensive and successful track record, the PIs will address these challenges by developing inferential and efficient computational methods under highly-structured models that involve multi-scale structure and/or multiple levels of latent variables. The central theme of the proposed research is the integration and pursuit of three desiderata in each of its four projects: scientific justification, statistical principles, and computational efficiency. This triple-goal advances the development of specifically designed methods that leverage computationally efficient and statistically principled data-driven techniques which explicitly incorporate scientific understanding of the astronomical sources.  This ensures that the statistical analyses enhance the scientists' ability to answer specific questions about the underlying astronomical and physical processes. This strategy requires state-of-the-art statistical inference, sophisticated scientific computing, and careful model-checking procedures, all of which have been the hallmark of the work by this team of investigators.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812199","Statistical Methods for Detection of Primordial Gravitational Waves","DMS","STATISTICS","07/01/2018","06/29/2018","Ethan Anderes","CA","University of California-Davis","Standard Grant","Yong Zeng","06/30/2022","$149,997.00","Lloyd Knox","anderes@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","With the detections by the LIGO gravitational wave observatories announced in early 2016 the long-awaited era of gravitational wave astronomy has begun. Scientists can now very directly explore nature under extreme conditions such as those that occur with merging black holes or neutron stars. Cosmologists intend to use gravitational waves to probe deep into the earliest moments of the big bang. Rather than monitoring changes to the lengths of the 4km-long arms of the LIGO detectors, cosmologists are seeking the imprint of gravitational waves on polarization patterns in the cosmic microwave background (CMB) -- light that, for the most part, last interacted with matter when the universe was just a few hundred thousand years old. If the simplest and most empirically successful scenario for the generation of density perturbations in the early universe is correct, the resulting signal should be observable. Such a detection would open up a new, and more direct, window on this ultra-early epoch as well as our first experimental probe of quantum-mechanical aspects of the gravitational field and allow us to test theories of the origin of spatial structure (density inhomogeneities) in our universe. To achieve the sensitivity to primordial gravitational waves (PGWs), being targeted by experiments in the planning stages now (such as the ?Stage IV? experiment), requires the development of new statistical tools -- in particular for the quantification of uncertainties in the removal of contaminants to the signal of interest. This project will directly address these statistical challenges by focusing on the two main obstacles for the detection of primordial gravitational waves in the CMB: contamination from gravitational lensing and millimeter wavelength radiation from the interstellar medium in our own galaxy. The statistical methodology resulting from the proposed work will not only enable some very exciting science, but also inform a broad range of statistical problems associated with large spatial datasets. <br/><br/>This project directly addresses the two main statistical challenges associated with the detection of primordial gravitational waves in the cosmic microwave background (CMB): contamination from gravitational lensing and the emission of millimeter wavelength radiation from the interstellar medium in our own galaxy. The first part of the project is the development of a full-scale Bayesian solution to the delensing problem using a new re-parameterization technique derived from a dynamical systems characterization of delensing and an artificial decoherence technique specifically designed to overcome the slow mixing of Gibbs samplers associated with CMB delensing. This custom re-parameterization, using the physics of how lensing aliases E-modes and B-modes in the CMB polarization, can exhibit properties of both a sufficient parametrization for the E-mode and an ancillary parameterization for the polarization B-mode. This is crucial for fast mixing of the main Gibbs chain and for high acceptance rates of Hamiltonian Markov chains for Bayesian delensing. The second main part of the project directly addresses the challenges associated with foreground contaminants: in particular the quantification of uncertainty that propagates through the observations to the estimated B-mode fluctuations from primordial gravitational waves. The project will focus on developing new random field models of the non-stationarity and non-Gaussianity aimed at quantifying uncertainty rather than the estimation, and subsequent removal, of foreground emission. The resulting models and techniques will also inform general statistical applications associated with non-stationary random field models and the hierarchical modeling of non-Gaussian spatial random fields.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812030","New Methods and Theory of Statistical Inference for Non-Gaussian Graphical Models","DMS","STATISTICS","07/01/2018","06/29/2018","Zhao Ren","PA","University of Pittsburgh","Standard Grant","Yong Zeng","06/30/2021","$129,999.00","","zren@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","","$0.00","The undirected graphical model (GM), a powerful tool for investigating the relationship among a large number of random variables in a complex system, is used in a wide range of scientific applications, including image analysis, statistical physics, astrophysics, finance, and biomedical studies.  With recent technological advances, unprecedented amounts of information can be collected for a given system, making meaningful inferential guarantees of GMs more challenging.  Despite recent successes in development of methods and theory for Gaussian GMs, the underlying assumption of continuous and normally distributed data is violated for some important data types.  For example, ordinal, binary and count data are all discrete in nature and cannot be naively transformed into Gaussian distributions.  In biomedical studies, examples of non-Gaussian type data include DNA Copy Number Variation, mutation and (single cell) RNA-sequence data.  Compared to recent advances in Gaussian GM, research in modeling and theoretical foundations for non-Gaussian data types has fallen behind.  To bridge this gap, the PI will identify some of the major modeling and inferential challenges and propose several new graphical models for non-Gaussian data.  In addition, the PI will further develop, evaluate and improve new statistical and computational inference methods for these models with theoretical guarantees. <br/><br/>The proposed research will significantly advance fundamental theoretical understanding on modeling and statistical inference of non-Gaussian data in graphical models via three tasks.  (I) Development of a new two-step inference procedure to employ the covariate-adjusted truncated Poisson graphical model (TPGM) which provides a unified framework for modeling both binary and count type data.  The inferential procedure fully respects the intrinsic sparse structure of the graph making it more reliable.  A novel likelihood-based non-linear score vector for bias correction will be developed.  (II) A novel zero-inflated TPGM fully accounting for the zero-inflation pattern in the data is proposed to model single cell RNA sequence data at the cell level.  The inferential procedure based on EM algorithms paves a road to better understanding of the genetic networks in different cell types, and thus a better understanding of the mechanisms of various diseases.  Theoretically, a composite-likelihood-based EM algorithm is utilized to overcome computational difficulties.  (III) Development of a novel latent semiparametric graphical model to draw inferences on intrinsic graph structure by integrating both ordinal and continuous type data. The method takes into account potential confounding effects to draw meaningful conclusions.  Beyond fundamental advances in statistical modeling and theory of graphical models, the research will have immediate impact in applications from a number of scientific disciplines including biology, pharmacy, finance and genomics.  The results will be disseminated through publications, open-source software and presentations at conferences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811659","Collaborative Research:  Critical Points and Excursion Probability of Random Fields: Theory and Statistical Applications","DMS","STATISTICS","07/01/2018","05/09/2022","Armin Schwartzman","CA","University of California-San Diego","Standard Grant","Yong Zeng","06/30/2022","$74,999.00","","a7schwartzman@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","","$0.00","Random fields are playing increasingly important roles in statistics, due to their use as spatial models in many scientific areas such as neuroimaging, astronomy, geosciences, oceanography and microscopy, where many problems involve dependent data at spatial locations. In these applications, researchers are interested in detecting signals hidden in a noisy background which can usually be modeled as a random field. This project aims to derive theoretical properties of random fields and use these results to create efficient statistical tools to extract important and useful information from spatial data. This is done by focusing on specific features of random fields such as peaks, which serve as local representatives of the signal in their immediate vicinity. Thus, local signals can be discovered by detecting peaks whose height is above what would be expected by chance. This project develops such signal detection procedures with rigorous statistical inference theory. By identifying regions of brain activity in brain images to finding stellar objects against the cosmic background radiation, the aforementioned scientific disciplines will benefit from the proposed methods, which provide new efficient tools to analyze spatially dependent data and detect signals in the presence of noise.<br/><br/>This project will study critical points and excursion probabilities of Gaussian and related random fields, and then apply the obtained theoretical results to important statistical problems involving signal detection in image analysis, multiple hypothesis testing and parameter estimation. For critical points, the project will investigate exact computable formulas for the expected number, height distribution, and overshoot distribution, and establish approximations to the moments of the number of critical points. For excursion probabilities, the project will investigate the expected Euler characteristic approximation for smooth Gaussian fields with non-constant variance and for non-Gaussian fields, and fractional Brownian motion on manifolds.  As statistical applications, the project will devise tests for local maxima in non-stationary Gaussian noise and testing of cluster extent and mass for detecting signal regions.  In these applications, p-values are computed using the developed theory.  Statistical applications also include estimation of parameters in the height distribution of local maxima and detection of change-points. This project uses interdisciplinary tools from probability, statistics, and geometry to develop the desired theoretical results and statistical methods. It will create interesting connections between several mathematical areas involving topology and random matrices theory, and to other disciplines, including neuroimaging, cosmology, and beyond.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812063","Probabilistic Underpinning of Imprecise Probability and Statistical Learning with Low-Resolution Information","DMS","STATISTICS","07/01/2018","07/03/2018","Xiao-Li Meng","MA","Harvard University","Standard Grant","Yong Zeng","06/30/2022","$199,928.00","","meng@stat.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269","","$0.00","All fruitful scientific and statistical analyses require assumptions. Some assumptions rightfully reflect past experience, present consensus, or future speculations. Others are imposed solely due to limitations of the investigation methods.  Useful information in practice often comes in a vague, ""low-resolution"" form, like a blurred picture, both literally and figuratively. Currently, statistical models have largely relied on overly precise model structures, built upon a mix of sound scientific knowledge and some less verifiable assumptions. As models grow larger to accommodate the ever-growing volume and variety of data, statistical inference is faced with the pressing need to accurately and honestly express all types of low-resolution knowledge. Without adequate tools to deal with vague information, investigators are forced to concoct high-resolution assumptions that can neither be trusted nor invalidated in meaningful ways, the culprit in the ongoing crisis of irreplicable research. This project aims to provide scientists and statisticians both a theoretical framework and practical methods to tackle this challenge without having to abandon familiar probabilistic rules and tools, thereby strengthening the effort in reducing irreplicable scientific findings. <br/><br/>The need to reduce unwanted assumptions in scientific and statistical studies has led to an extensive literature on imprecise probability (IP), or more broadly, soft methods in probability and statistics (SMPS). As of today, both have received little attention from the statistics community, which generally equivocates on anything that does not obey precise probabilistic rules. This project demonstrates that both the precise probability and hard statistical principles have much to offer for studying IP and SMPS, with the fundamental realization that once going beyond precise probabilities, the learning rules by which we update the imprecise model must become the vehicle for implicit assumptions, explaining some paradoxes and puzzles that arise in IP and SMPS.  With a clearer understanding of what IP/SMPS can and cannot do, the proposed research contributes in theoretical and practical ways to ensure and enhance replicability of scientific studies that rely on probabilistic reasoning and statistical analysis.<br/>The initial idea of this project stemmed from the PI's realization that in handling low-resolution information, the well-accepted Heitjan-Rubin framework for data coarsening in the literature of missing data induces essentially the same mathematical structure as does the Dempster-Shafer theory of belief function. Consequently, belief function can be understood and studied using ordinary probability. The proposed research explores this link and extensions to its variations, and aims to provide (1) a precise probabilistic formulation of belief function, which offers both insights and questions for the Dempster-Shafer theory, especially Dempster's Rule of Combination; (2) a detailed comparison and contrast of three learning rules for updating and propagating low-resolution information, especially with respect to the phenomena of dilation, contraction, and sure loss; and (3) an exploration of the design and implementation of efficient, MCMC-type algorithms for learning rules of low-resolution inference, in parallel to MCMC for Bayesian inference. The overarching goal of the proposed research is to enhance the scientists' and statisticians' toolkit for conducting more objective inference and data analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810888","Collaborative Research: Consistent Risk Estimation under High-Dimensional Asymptotics","DMS","STATISTICS","08/01/2018","08/01/2018","Mohammad Ali Maleki","NY","Columbia University","Standard Grant","Yong Zeng","07/31/2022","$119,880.00","","mm4338@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","","$0.00","Learning from large datasets has been the cornerstone of modern innovations and discoveries in science, medicine, and technology. Fast prediction of unseen events is a canonical goal in statistical learning. A classic approach to this end is leave-one-out cross-validation, a time-consuming routine of leaving a datum out,  fitting the model on the rest, and testing it on the left out datum, repeatedly. The recent emergence of massive data has exacerbated the computational infeasibility of such approaches. Moreover, in many recent instances, the number of features per observation can be extremely large, adding another challenging facet to the fast estimation of prediction error. To overcome these problems a new set of scalable and consistent risk estimators will be developed in this project. <br/><br/>The importance of risk estimation has motivated this project of different schemes, such as cross-validation, Stein's unbiased risk estimation (SURE), Generalized cross-validation, Akaike Information Criterion (AIC), and Bootstrap. The emergence of high-dimensional datasets has challenged most classical approaches to risk estimation. For instance, the large discrepancy between in-sample and out-of-sample prediction error, in applications involving predictions based on previously unseen features, makes it hard to rely on popular estimators, such as SURE or AIC, in high-dimensional regimes where the number of predictors is smaller than or at the same order as the number of observations. On the other hand, the information value of a datum in these regimes (as opposed to the information value of a datum in low-dimensional settings) casts doubt on the reliability of other techniques, such as 5-fold cross-validation. The project offers a novel theoretical framework to find the middle ground between scalability and reliability, and specifically, to obtain theoretically consistent and computationally efficient risk-estimation schemes under high-dimensional settings. Since risk estimation is at the core of areas including but not limited to machine learning, signal processing, medical imaging, neuroscience, and social and environmental sciences, any success in this project will lead to reliable and immediate scientific discoveries and better learning systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811969","Collaborative Research: Statistical Analysis of Partially Observed Shapes in Two Dimensions","DMS","STATISTICS","08/01/2018","07/26/2018","Ofer Harel","CT","University of Connecticut","Standard Grant","Gabor Szekely","07/31/2020","$50,000.00","","ofer.harel@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","","$0.00","Classification of shapes has many important applications in a variety of fields. For example, anthropologists use shape classification on fossilized faunal teeth to reconstruct past environments and on variation in the shapes of stone tools to assess different technological strategies.  Other applications include classification of plants based on the shape of their leaves and identification of shapes of tumors.  While there are many statistical tools that can be used for classification, most methods are based on complete shapes with relatively few methods available for partially observed or incomplete shapes. This project focuses of the development of new statistical methodology, based on the ideas of multiple imputation, for the analysis of partially observed shapes.<br/><br/>This project proposes as a starting point leveraging the ideas of nonparametric, hot-deck type multiple imputation to shapes that are defined by unlabeled points and/or functions, as opposed to shapes defined by landmarks where traditional methods of multiple imputation may be applied.  The proposed method involves matching partially observed shapes to fully observed shapes, randomly choosing a fully observed donor shape among the shapes that are good matches for the partial shape, and then completing the partial shape with the unmatched part of the donor shape. A simulation study will be conducted to compare the relative merits of different partial matching methods.  The imputation framework will be tested using teeth from the Family Bovidae, whose classification plays an important role in biological anthropology for identifying the taxa of specimens, which in turn is used to reconstruct paleoenvirnoments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812124","Collaborative Research: Statistical Analysis of Partially Observed Shapes in Two Dimensions","DMS","STATISTICS","08/01/2018","07/26/2018","Gregory Matthews","IL","Loyola University of Chicago","Standard Grant","Gabor Szekely","01/31/2021","$75,028.00","George Thiruvathukal","gmatthews1@luc.edu","820 N MICHIGAN AVE","CHICAGO","IL","606112147","7735082471","MPS","1269","","$0.00","Classification of shapes has many important applications in a variety of fields. For example, anthropologists use shape classification on fossilized faunal teeth to reconstruct past environments and on variation in the shapes of stone tools to assess different technological strategies.  Other applications include classification of plants based on the shape of their leaves and identification of shapes of tumors.  While there are many statistical tools that can be used for classification, most methods are based on complete shapes with relatively few methods available for partially observed or incomplete shapes. This project focuses of the development of new statistical methodology, based on the ideas of multiple imputation, for the analysis of partially observed shapes.<br/><br/>This project proposes as a starting point leveraging the ideas of nonparametric, hot-deck type multiple imputation to shapes that are defined by unlabeled points and/or functions, as opposed to shapes defined by landmarks where traditional methods of multiple imputation may be applied.  The proposed method involves matching partially observed shapes to fully observed shapes, randomly choosing a fully observed donor shape among the shapes that are good matches for the partial shape, and then completing the partial shape with the unmatched part of the donor shape. A simulation study will be conducted to compare the relative merits of different partial matching methods.  The imputation framework will be tested using teeth from the Family Bovidae, whose classification plays an important role in biological anthropology for identifying the taxa of specimens, which in turn is used to reconstruct paleoenvirnoments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811818","Statistical Methodology and Applications to Engineering, Economics, and Health Analytics","DMS","STATISTICS","07/01/2018","06/29/2018","Tze Lai","CA","Stanford University","Standard Grant","Gabor Szekely","06/30/2021","$250,000.00","","lait@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","A long-term objective of the proposed research is to develop innovative statistical methodologies and combine them with technological advances for resolving fundamental problems in engineering, economics, and health care. In particular, the past seven years have witnessed the beginning of a big data era in the US health care system, following the health care reform legislation enacted in 2010, and the Precision Medicine Initiative of 2015. This era poses new challenges and opens up new opportunities for the mathematical (including statistical, computational, and data) sciences and their interactions with the biomedical, engineering, and economic sciences. The project will address some of these challenges, and its broader impact includes (i) direct applications in engineering, economics and finance, health, and medicine, and (ii) training the next generation of scientists in academia, industry, and government by involving graduate students in all phases of the research and developing new advanced courses and revising the curriculum in financial and risk modeling, statistics and data science, and clinical trials and biostatistics.<br/><br/>The project is broadly divided into three areas. The first is the development of valid and efficient post-selection multiple testing in the big data era, in which some machine learning/feature engineering/variable selection algorithms are typically used to extract features/variables for subsequent hypothesis generation and statistical testing. The proposed research will address the reproducibility issues and ""replication crisis"" with this data-dependent choice of features and hypotheses for statistical inference from biomedical big data by resolving foundational issues concerning valid post-selection inference.  Initial investigations have already started by considering samples of fixed size, and will proceed with extensions to group sequential designs and then to sequential detection and diagnosis for multistage manufacturing processes, multicomponent systems, and multiple data streams from financial and production networks. The second area is the statistical foundation of gradient boosting, which also has applications to the first area because of its effectiveness in tackling high-dimensional nonlinear and generalized linear models. The third area covers biomarker-guided adaptive design of clinical trials for the development and testing of personalized therapies and in the closely related subject of contextual multi-armed bandits in sequential analysis and reinforcement learning. Innovations in this area can lead to advances toward the Precision Medicine Initiative. Also covered are innovative study designs and analyses of point-of-care trials and observational studies, and development of mobile health platforms and wearable devices to improve and facilitate evidence-based management of chronic diseases.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1820942","An Approach to Robust Performance Analysis Using Optimal Transport","DMS","STATISTICS, CDS&E-MSS","08/15/2018","07/27/2020","Jose Blanchet","CA","Stanford University","Continuing Grant","Christopher Stark","07/31/2021","$240,000.00","","jose.blanchet@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269, 8069","9263","$0.00","The goal of this research is to investigate a comprehensive set of tools to enable robust performance analysis and decision making by building a framework which systematically evaluates the impact of modeling errors. The general philosophy of this research is as follows. Stochastic models are used virtually everywhere and many of these models are convenient because they can be easily calibrated and/or because performance analysis or optimization can be easily done in closed form or algorithmically. But we all recognize that there are trade-offs between the model's fidelity (i.e. their ability to replicate reality) and its tractability. This project investigates a systematic approach which can be used to account for the impact of this trade-off. The PI studies a wide range of models called stochastic networks, which are used to describe virtually any probabilistic system in which there is resource contention. These systems are used in logistics, transportation, communications and systemic risk, among others. The PI plans to apply the developed approach to study robustness questions related to stochastic networks in heavy-traffic utilization and rare events in such systems.<br/>    <br/>This project investigates a comprehensive set of tools which enables the quantification of model errors in the performance analysis and control of a wide range of stochastic systems. The investigator's strategy combines various areas of mathematics, including convex optimization, probability theory, and Monte Carlo methods. The investigator will exploit general duality results which are used to obtain explicit expressions for worst-case expectations among all probability models within a certain tolerance from a baseline probabilistic model (typically chosen for tractability). The metric describing the neighborhood of models is based on optimal transport theory. These results are applicable at the stochastic-process level (for random elements taken values on general Polish spaces), so they can be used to approximate sample-path expectations of complex stochastic systems. A key element in the program is that the worst-case probability of a given event can be expressed explicitly in terms of the probability of a modified (explicit) event under the baseline (tractable) model. The investigator will study a wide range of questions related to rare-event analysis and heavy-traffic approximations of stochastic networks, which are widely used in application areas such as communication networks, call centers, manufacturing systems, and chemical reaction networks, among others.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812128","Modeling Temporal Dynamics of Large Systems from High-Dimensional Time Series Data","DMS","STATISTICS","07/01/2018","06/29/2018","Sumanta Basu","NY","Cornell University","Standard Grant","Yong Zeng","06/30/2022","$125,000.00","","sumbose@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","062Z","$0.00","The answers to many questions in the biological and social sciences require understanding how the components of a large system dynamically interact with each other and give rise to emergent behavior. For instance, simultaneous failure of a handful of small but highly-connected firms can lead to huge systemic losses in the financial system. Interactions among neurophysiological signals in different brain regions relate to the organization of human brain connectome. This project aims to develop rigorous and computationally efficient statistical methods to jointly model the temporal dynamics of such large systems using high-dimensional time series datasets. These methods will enable researchers to gain deeper insights into the structure of these systems and help with more accurate data-driven decision making. It is anticipated that the methods under development can be used in clinical neuroscience to search for functional connectivity patterns associated with neurological disorders, and in financial regulation for monitoring systemic risk and identifying systemically important firms in the financial systems.<br/><br/>Specifically, this project will focus on two classes of estimation and inference problems in high-dimensional time series: (i) developing novel theory and methods for estimating high-dimensional spectral density and coherence matrices, which can be viewed as a natural generalization of covariance and correlation matrix estimation problems to high-dimensional time series, and (ii) developing novel theoretical machinery to quantify uncertainty (confidence intervals and hypothesis tests) in high-dimensional vector autoregressive models. Broadly speaking, the research will attempt to bridge a gap between current frontiers of high-dimensional statistics for independent data and time series data using tools from disciplines including optimization, statistics, signal processing, and random matrix theory. This intellectual unification of ideas may provide novel insights in deciphering the workings of complex systems in a data-driven fashion.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811933","Statistical Analysis of Categorical Time Series through Sparse Markov Models","DMS","STATISTICS","08/15/2018","08/20/2021","Donald Martin","NC","North Carolina State University","Standard Grant","Yong Zeng","07/31/2023","$100,000.00","Soumendra Lahiri","demarti4@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","1269","$0.00","Analysis of a sequence of categorical values is best done by a model that captures the statistical properties of the sequence, while being simple enough so that statistical analysis is feasible.  In many cases, the analysis of such data has been simplified by the sequence having a short memory or Markov property in the sense that conditional probabilities depend on only the very recent past.  Whereas Markov models are applied extensively, typically low-order models are fit because the number of estimated conditional probabilities grows geometrically as the number of past observations used for conditioning increases.  Another drawback involves the lack of model flexibility, as the number of possible Markov models is limited.  Sparse Markov models (SMM) help with these two problems, thus allowing better model fits.  While theoretical results for Markov models are prevalent, those for SMMs are relatively rare, only being considered in the last decade.  Thus, there is tremendous potential for the furthering of knowledge related to theory and applications of SMMs to analyze categorical time series.  This is the fundamental aim of this project. <br/><br/>Sparse Markov models allow the fitting of a parsimonious model that is also flexible enough so that a better trade-off is obtained between bias that arises from conditioning contexts that are shorter than truth and variance from having many parameters to estimate, thus improving inference.  This project studies theoretical properties and applications of sparse Markov models; the special case of variable length Markov chains (VLMCs) to the analysis of categorical time series is considered.  Related objectives are (i) To develop theory for prediction of data from SMMs, central limit theorems, model fitting through regularized regression, and comparisons of asymptotic and finite-sample properties of derived and current methods; (ii) To develop a new model called hidden sparse Markov models (HSMMs); (iii) To extend methods for efficient computation of distributions of pattern statistics in Markovian sequences to both sparse and hidden sparse Markov models; (iv) To apply VLMCs/SMMs and HSMMs to improve the analysis and inference of various categorical time series.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811623","Collaborative Research: Theory and Methods for Highly Multivariate Spatial Processes with Applications to Climate Data Science","DMS","STATISTICS","08/01/2018","06/25/2021","Allison Baker","CO","University Corporation For Atmospheric Res","Standard Grant","Yong Zeng","07/31/2023","$62,500.00","Dorit Hammerling","abaker@ucar.edu","3090 CENTER GREEN DR","BOULDER","CO","803012252","3034971000","MPS","1269","1269","$0.00","Geophysical, environmental and ecological datasets often include many variables observed over a set of irregular geographical locations. While spatial datasets are increasing in size, they are also increasing in complexity with many variables being simultaneously observed, recorded, modeled or derived. Current methods in spatial statistics are unable to cope with such highly multivariate datasets; this research addresses this gap in statistical science, aiming to establish a new framework for multivariate spatial models. The testbed for the new framework is in the field of climate data science. Understanding of the Earth system relies on coupled physical models that represent the dynamic evolution of the atmosphere, ocean, land use, rivers, glaciers and other processes. These models have led to vast amounts of climate model data that severely constrain storage resources. Moreover, statistical emulators are increasingly common and desirable alternatives to running complex physical models directly. Development and validation of compression and emulation algorithms require understanding and maintaining complex dependencies between physical variables, but current tools are univariate or pairwise-based. This research will provide statistical guidance for climate data science applications.<br/><br/>This project focuses on a modeling framework for multivariate spatial processes, and relies on new theory incorporating graphical models in multiscale multivariate spatial process representations.  Moreover, many multivariate datasets exhibit non-Gaussian behavior.  A companion thrust of this work is in introducing and exploring empirical likelihood techniques for large multivariate spatial processes.  Finally, the proposed models and estimation frameworks will be applied to a climate dataset from the Community Atmosphere Model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810914","Order Determination for Hidden Markov and Related Models","DMS","STATISTICS","07/01/2018","05/10/2018","Samuel Kou","MA","Harvard University","Standard Grant","Gabor Szekely","06/30/2021","$250,000.00","","kou@stat.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269","1269","$0.00","Hidden Markov models (HMMs) are powerful tools for processing time series data and are widely used in scientific and engineering applications, including speech recognition, machine translation, computational biology, cryptanalysis, and finance. The fundamental components of an HMM include the noisy observations and the corresponding hidden states. In most applications, the number of hidden states (the order of the HMM) is not known beforehand but conveys important information about the underlying process. For example, in molecular biology, the total number of hidden states may be the number of distinct 3D conformations of a protein; in chemistry, the total number of hidden states may be the number of distinct chemical species in an organic reaction. This project plans to investigate order determination for HMMs, finite mixture models, and hierarchical HMMs; the latter two models are special cases and extensions of HMMs. The project aims to develop a consistent and competitive method for order selection. In addition to a thorough theoretical investigation, comprehensive numerical studies and applications in biology and chemistry will be conducted. The project will not only significantly advance the theoretical understanding of HMMs, but also provide powerful tools for researchers to analyze data. The data applications will help advance molecular biology and biochemistry. The project also aims to support and train undergraduate and graduate students, with special attention being given to recruiting students from under-represented groups into statistics and related fields. Education at the undergraduate and graduate levels will be integrated into the research activities. <br/><br/>The project will establish the marginal likelihood method as a consistent and competitive order selection method for HMMs, finite mixture models, and hierarchical HMMs. Five research studies will be carried out, enumerated as follows. (1) Investigate the order of HMMs, where the goal is to identify and develop consistent methods for HMM order determination. (2) Investigate order selection issues in finite mixture models. Finite mixture models can be reformulated as special types of HMMs. The goal is to develop a method for consistently estimating the number of mixture components. (3) Investigate order determination of hierarchical HMMs, where multiple HMMs are linked through a hierarchical structure. The aim here is to identify and develop consistent methods for determining the order of hierarchical HMMs, taking special effort to address the challenging issue that multiple HMMs often have quite diverse characteristics, such as lengths. (4) Study computational challenges and investigate and implement efficient computational methods for the order determination of HMM and related models, including the implementation and release of an open source, publicly available R package. (5) Apply the new method to ion channel data and single-molecule data on co-translational protein targeting. The PI also plans to develop courses that introduce and guide students in HMMs, mixture models, and hierarchical HMMs. The success of the proposed research will develop a theoretical basis and associated methodology for consistent order determination of HMMs and related models. The research achievements and the education components will broadly impact the analysis of HMMs, hierarchical HMMs, and model selection and also help train a new generation of scholars and researchers in the field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811888","Statistical Modelling of Multivariate Functional and Distributional Data","DMS","STATISTICS","07/01/2018","05/23/2018","Alexander Petersen","CA","University of California-Santa Barbara","Standard Grant","Gabor Szekely","04/30/2021","$149,860.00","","petersen@stat.byu.edu","3227 CHEADLE HALL","SANTA BARBARA","CA","931060001","8058934188","MPS","1269","8091","$0.00","Modern recording devices are collecting data of greater complexity and, when these data are measured over space or time, also at ever-increasing resolution.  While providing more detailed information about the associated physical phenomena occurring around us, they also pose statistical challenges related to interpretable modeling and feasible computation for such data.  Data measured over space, time, or some other continuum, are fittingly termed functional data, and constitute an important subfield of modern statistics.  This project will develop important methodology for the analysis of two types of functional data.  The first set of projects aim at the estimation of dependency patterns between components of so-called multivariate functional data, where a common set of features is measured over time for each subject in a study, such as neuroimaging scans where signals are recorded over time at a variety of spatial locations.  Another important class of functional data are samples of distributions or histograms, which regularly arise in the analysis of demographic data as mortality distributions, for example, but also in other important fields such as neuroscience and finance.  This project outlines methods for dimension reduction and regression that respect the well-known positivity and area-under-the-curve constraints for distributions, yielding interpretable data summaries and model fits that provide the practitioner with a clearer understanding of the information contained in their data.<br/><br/>Both fMRI and EEG yield time-dependent signals at multiple brain locations, resulting in multivariate functional data.  Quantifying connectivity patterns to define brain networks, for example in order to identify normal and pathological characteristics, is an important neuroscientific problem that can be addressed using multivariate functional data techniques.  This project seeks to advance the use of functional graphical models to estimate underlying brain dependency networks, including improved computational efficiency compared to existing methods. These methods are equally applicable in other domains that produce data of similar structure, such as longitudinal medical studies, where a common set of measurements is recorded repeatedly over time.  Also considered in this proposal are methods for distributional data, which can be thought of as collections of curves or surfaces, each corresponding to a probability distribution.  For example, neuroimaging data naturally provide such distributional samples, as levels of myelination or signal correlations within brain regions are high-dimensional data that can be effectively summarized at the subject level by a histogram or distribution.  Given a sample of such distributional data, this project investigates statistical methods of interpretable dimension reduction and dependency of distributional response functions on relevant covariates through distributional regression.  A key tool is the Wasserstein metric for distributions, which has been widely successful in applied settings, but has not been utilized to its full extent in statistics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1833522","NISS Writing Workshop for New Researchers","DMS","STATISTICS","07/15/2018","05/21/2018","James Rosenberger","NC","National Institute of Statistical Sciences","Standard Grant","Gabor Szekely","06/30/2019","$9,990.00","Nell Sedransk, Lingzhou Xue","JRosenberger@niss.org","19 TW ALEXANDER DR","RESEARCH TRIANGLE PARK","NC","277090152","9196859300","MPS","1269","7556","$0.00","Support for this conference provides partial participant support for the 2018 NISS Writing Workshop for Junior Researchers held at the Joint Statistical Meetings (JSM) July 29 and 31, 2018 in Vancouver, BC, Canada.  These funds support a technical writing workshop for 24 junior researchers in statistics, biostatistics, and data science. The writing workshop will provide individual hands-on guidance and mentoring by recent journal editors on how to write journal articles and research funding proposals.  Junior researchers will apply to the workshop by June 1 by submitting a writing sample and will be notified of acceptance no later than June 20 after pairing applicants with eight senior editors.  Previous workshops have been very successful with many workshop participants going on to become associate editors or co-editors of major statistical journals.  More specifically, at least 23 associate editorships and one co-editorship are among the 109 recent graduates from 2007 to 2011.<br/><br/><br/>From 2007 to 2016, NISS organized technical writing workshops for junior researchers in statistics, biostatistics and data science. The 2018 NISS writing workshop consists of two parts. The workshop will open with tutorial sessions on the organization of material for a technical article or a grant application, on techniques for technical writing, and on the specific objectives and audiences of key journals in the statistical sciences. Another session will cover journal specific information for deciding where to publish, and a panel of current and recent editors will discuss the differences among the key journals.  Important ethical issues will also be discussed, and issues of journal choice, and understanding and responding to reviewers' comments will be covered in the final session.   At the end of this part, each participant meets with their mentor who analyzed a draft paper the participant submitted prior to workshop and receives feedback and suggestions.  The second part focuses on the specific issues for participants whose native language is not English, covering the writing process and details of grammar, sentence structure and word choice.  A panel of senior researchers whose native language is not English will discuss their experiences with technical writing in English.  The website for the workshop is:  <br/>https://www.niss.org/events/2018-niss-writing-workshop-junior-researchers-jsm<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811859","Estimation, Sampling, and Simulation for Long-Range Dependent Processes","DMS","STATISTICS","08/15/2018","07/20/2020","Alexandra Chronopoulou","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Yong Zeng","07/31/2022","$100,000.00","","achronop@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","","$0.00","In a variety of phenomena in science and engineering, such as those studied in hydrology, signal processing, computer science, operations research, economics, finance, and biology, it is often the case that observations that are far apart in time or space are strongly correlated. Such phenomena can be modeled by stochastic systems that allow for long-range dependence. For such models to be relevant, it is important to be able to make reliable inferences about the inherent degree of memory in the system, as well as other model parameters.  Moreover, it is critical to have efficient simulation and sampling schemes, tailored to the task of interest and incorporating the system memory. This project aims to develop novel, computationally feasible, rigorous, rate-optimal inferential and sampling methods that overcome limitations of existing methodologies. The results of this research can contribute to a more widespread and reliable use of stochastic systems that account for long-range dependence. <br/><br/>This research aims to resolve various problems regarding the inference, design, and simulation of long-range dependent processes, bridging the gap between the continuous-time nature of these models and the discrete-time nature of their observations. Specifically, the goals are to: (i) develop statistically- and computationally-efficient parameter estimators in a coupled stochastic system where the noise of the observed diffusion is correlated with the noise of a hidden long-memory process; (ii) develop and analyze optimal non-uniform sampling methods for tracking a fractional stochastic differential equation (SDE) with a constraint on the number of observations; (iii) develop efficient simulation algorithms for fractional SDEs with long memory; and  (iv) analyze the approximation error for reflected additive noise fractional SDEs. The research will rely to a considerable extent on the theories of fractional and Malliavin calculus.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1807023","Statistical Inference for High-Dimensional Time Series","DMS","STATISTICS","08/15/2018","05/02/2020","Xiaofeng Shao","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Yong Zeng","07/31/2022","$120,000.00","","xshao@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","","$0.00","Due to the rapid development of information technologies and their applications in many scientific fields, high dimensional time series (HDTS) are routinely collected nowadays. The methods and theory developed for the inference of low and fixed dimensional time series may not be applicable when the dimension is comparable to or exceeds time series length, and there is an urgent need to develop new statistical methods that can accommodate both high dimensionality and temporal dependence. Statistical inference for HDTS is fundamentally important and has many applications in disciplines ranging from climate science to medical imaging and finance, among others.<br/><br/>This project aims to develop innovative theory and methodologies to address several important inference problems in the analysis of HDTS. The research is built on the self-normalized approach, which has found great success in dealing with low dimensional problems. Its advance to the high dimensional context is challenging both methodologically and theoretically, and it requires a new methodological formulation and new theory. This project covers the inference of the mean, covariance matrix, and auto-covariance matrix  for HDTS, and the tests developed can be used to detect change points, certain structure of the covariance matrix and target dense alternative. On the theoretical front, the weak convergence of sequential U-statistic based process will be investigated and is of independent interest.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811632","Collaborative Research:  Critical Points and Excursion Probability of Random Fields: Theory and Statistical Applications","DMS","STATISTICS","07/01/2018","05/11/2018","Dan Cheng","TX","Texas Tech University","Standard Grant","Nandini Kannan","12/31/2018","$99,998.00","","chengdan@asu.edu","2500 BROADWAY","LUBBOCK","TX","79409","8067423884","MPS","1269","","$0.00","Random fields are playing increasingly important roles in statistics, due to their use as spatial models in many scientific areas such as neuroimaging, astronomy, geosciences, oceanography and microscopy, where many problems involve dependent data at spatial locations. In these applications, researchers are interested in detecting signals hidden in a noisy background which can usually be modeled as a random field. This project aims to derive theoretical properties of random fields and use these results to create efficient statistical tools to extract important and useful information from spatial data. This is done by focusing on specific features of random fields such as peaks, which serve as local representatives of the signal in their immediate vicinity. Thus, local signals can be discovered by detecting peaks whose height is above what would be expected by chance. This project develops such signal detection procedures with rigorous statistical inference theory. By identifying regions of brain activity in brain images to finding stellar objects against the cosmic background radiation, the aforementioned scientific disciplines will benefit from the proposed methods, which provide new efficient tools to analyze spatially dependent data and detect signals in the presence of noise.<br/><br/>This project will study critical points and excursion probabilities of Gaussian and related random fields, and then apply the obtained theoretical results to important statistical problems involving signal detection in image analysis, multiple hypothesis testing and parameter estimation. For critical points, the project will investigate exact computable formulas for the expected number, height distribution, and overshoot distribution, and establish approximations to the moments of the number of critical points. For excursion probabilities, the project will investigate the expected Euler characteristic approximation for smooth Gaussian fields with non-constant variance and for non-Gaussian fields, and fractional Brownian motion on manifolds.  As statistical applications, the project will devise tests for local maxima in non-stationary Gaussian noise and testing of cluster extent and mass for detecting signal regions.  In these applications, p-values are computed using the developed theory.  Statistical applications also include estimation of parameters in the height distribution of local maxima and detection of change-points. This project uses interdisciplinary tools from probability, statistics, and geometry to develop the desired theoretical results and statistical methods. It will create interesting connections between several mathematical areas involving topology and random matrices theory, and to other disciplines, including neuroimaging, cosmology, and beyond.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844420","Estimation and Inference for Massive Multivariate Spatial Data","DMS","STATISTICS","05/15/2018","10/18/2018","Joseph Guinness","NY","Cornell University","Standard Grant","Gabor Szekely","07/31/2020","$102,683.00","","guinness@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","8083","$0.00","Satellite observations of the Earth's atmosphere and oceans have the potential to improve forecasting of hurricanes and other extreme weather events. Massive efforts to sample the chemical constituents present in well water can reduce uncertainty in mapping of hazardous materials in groundwater. Observations of chemical reactions at the sub-micron scale may lead to new insights about the behavior of toxic trace elements in soils. However, the value of these expensive efforts to collect massive amounts of data will not be fully realized if the statistical techniques for analyzing them do not keep pace. The current techniques available are inadequate to flexibly model and extract information from massive datasets consisting of many variables collected across a region. This research project aims to develop computationally efficient methods for addressing the central challenges for analyzing massive multivariate spatial data: (1) drawing justifiable conclusions about the relationships among the multiple variables, and (2) making full and appropriate use of all variables when mapping the data. Addressing the first challenge is essential to translating observational and experimental data into scientific knowledge. Addressing the second is crucially important for providing predictions of potentially harmful outcomes, and the key to solving both challenges is integrating the multivariate and spatial data analysis into a unified framework.<br/><br/>The inherent correlation in time series and spatial data is the feature that makes interpolation and forecasting possible, but it also complicates estimation of multivariate relationships. As a result, analyses of time series data often start with a transformation of the data into the spectral domain, in which the transformed data are approximately uncorrelated. Although the spectral domain has played a central role in developing theory for models for spatial data, several issues have hindered the implementation of practical spectral domain methods for spatial data. This project aims to develop methodological innovations to overcome those barriers and provide practitioners with a flexible set of tools to extract information from dozens of spatial variables simultaneously, and predict variables at unsampled locations using all of the available data. The methods employ computationally efficient periodic data augmentations to simplify analyses, dramatically improve the ability to characterize uncertainty, and are supported by novel theoretical results."
"1810831","Fast and Robust Gaussian Process Inference for Bayesian Nonparametric Learning","DMS","STATISTICS","06/01/2018","05/11/2018","Yun Yang","FL","Florida State University","Standard Grant","Nandini Kannan","05/31/2019","$120,000.00","","yy84@illinois.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","8083, 9263","$0.00","Advances in modern technology have empowered researchers to collect massive data to conduct inference and making predictions. With the abundance of available observations, traditional statistical methods under the parametric assumption that a model can be characterized by a pre-specified number of parameters become inadequate and less attractive. Bayesian nonparametric models are attractive in this context which allow the resolution level of the analysis to be determined in a data-driven manner, and provide automatic characterization of uncertainty. The goal of this project is to develop new theory, methodology and computational tools for Bayesian nonparametric inference via Gaussian process priors. <br/><br/>Given the availability of massive data, nonparametric inference offers an attractive framework for flexibly modeling the underlying structure and extracting useful information. For instance, such challenges occur in chemical physics, computational biology, computer vision, engineering, and meteorology. This project aims to lay down a solid methodological, algorithmic, and theoretical foundation for nonparametric inference based on Gaussian processes. In particular, Gaussian process-based approaches tend to be vulnerable to data contamination and have heavy computational costs. To alleviate the high-computational cost of Gaussian process inference procedures, the investigator puts forward two novel computational frameworks which differ at their respective approximating targets as being either the prior or the posterior. To enhance the robustness of Gaussian process inference against data contamination, the investigator proposes a novel class of Bayesian hierarchical models for incorporating this extra measurement error structure, leading to a class of robust Gaussian process inference procedures. The new theoretical development offers valuable insight to experiment-design practitioners into the impact of measurement errors upon prediction and estimation, and provides evidence on the deep connection between computational complexity and statistical learnability. These computational and theoretical frameworks also benefit other disciplines such as applied mathematics, computer science and finance where stochastic processes such as Gaussian processes are routinely used.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811747","Leveraging Covariate and Structural Information for Efficient Large-Scale and High-Dimensional Inference","DMS","STATISTICS","07/01/2018","07/03/2018","Xianyang Zhang","TX","Texas A&M University","Standard Grant","Yong Zeng","06/30/2022","$150,000.00","","zhangxiany@stat.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","","$0.00","The proliferation of big data is accompanied by a vast number of questions, in the form of hypothesis tests, which call for effective methods to conduct large-scale and high-dimensional inferences. These influential methods must involve statistical analysis on many study units simultaneously. Conventional simultaneous inference procedures often assume that hypotheses for different units are exchangeable. However, in many scientific applications, external covariate and structural information regarding the patterns of signals are available. Exploiting such side information efficiently and accurately will lead to improved statistical power, as well as enhanced interpretability of research results. The main thrust of this research is to advance statistical methodologies and theories for large-scale and high-dimensional inference with a particular focus on integrating potentially useful external covariate and structural information into inferential procedures.<br/> <br/>This research aims to develop innovative methodologies and theories to address several significant problems in large-scale and high-dimensional inference. In Project 1, the PI will introduce a new multiple testing procedure that can automatically select relevant covariates to improve the efficiency in inference when a large number of external covariates are available. In Project 2, the PI will develop a new multiple testing framework, which can integrate various forms of structural information. Because prior information is seldom perfectly accurate, a particular focus will be on developing procedures that are robust to misspecified/imperfect prior information. In Project 3, the PI shall propose new procedures for simultaneous inference in high-dimensional regressions with side information. The statistical tools will be used to identify skilled fund managers, assess the performance of climate field reconstructions, and analyze genomic data in an integrative way. Methods and computer code developed will be made publicly available.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1836899","Preparing Graduate Students for Careers in Teaching Statistics and Data Science","DMS","STATISTICS","08/01/2018","07/25/2018","Donna LaLonde","VA","American Statistical Association","Standard Grant","Gabor Szekely","07/31/2019","$19,545.00","Beth Chance, Mine Cetinkaya-Rundel","donnal@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","7556","$0.00","The one-day workshop, ""Preparing Graduate Students for Careers in Teaching Statistics and Data Science"", will be held Saturday, July 28, 2018 in conjunction with the Joint Statistical Meetings in Vancouver, British Columbia.   The workshop will provide graduate students and post-doctoral researchers with the tools necessary to develop innovative and pedagogically-sound learning experiences for their students, both inside and outside the classroom, and to implement formative and summative assessment to guide instructional practice. The ultimate objective of this workshop is to increase the proportion of successful instructors who have the skills, capacity, and inclination to take on the challenges of complex data-oriented teaching. <br/><br/>Existing guidelines provide research-based recommendations for the pedagogy and assessment in introductory statistics courses. Ongoing efforts focus attention on modernizing the content and extending these ideas throughout the undergraduate curriculum. There has also been tremendous activity in developing materials for bringing principles of data science into stand-alone courses for broad student audiences, as well as using data-centric approaches in many types of statistics courses. The goal of this workshop is to equip future educators with concrete information ranging from recommended practices in teaching statistics and data science, to providing repositories of teaching and data resources, and creating the infrastructure knowledge necessary to seamlessly incorporate computing into the curriculum.  Discussions will include decisions involved in developing curriculum, creating an assessment plan, and choosing technology tools.  Workshop attendees will work through several examples and gain first-hand experience with active learning, use of real data, and technology tools such as RStudio and RMarkdown.  More information can be found at the workshop website  https://sites.google.com/view/preparetoteach/home<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1921310","Multiscale Generalized Correlation: A Unified Distance-Based Correlation Measure for Dependency Discovery","DMS","STATISTICS","09/11/2018","06/24/2019","Cencheng Shen","DE","University of Delaware","Continuing Grant","Gabor Szekely","08/31/2020","$142,651.00","","shenc@udel.edu","220 HULLIHEN HALL","NEWARK","DE","197160099","3028312136","MPS","1269","8083","$0.00","Detecting relationships between two data sets has long been one of the most important questions in statistics and is fundamental to scientific discovery in the big-data era. By developing an open-source, robust, efficient, and scalable statistical methodology for testing dependence on modern data, this project aims to advance the understanding and utility of testing dependence, tackle a number of related statistical inference questions, and accelerate a broad range of data-intensive research. The project incorporates fundamental research in mathematics, statistics, and computer science to further develop a multiscale generalized correlation framework to enable discovery and decision-making via analysis of large and complex data. The tools under development will allow scientists to better explore and understand high-dimensional, nonlinear, and multi-modal data in a myriad of applications. The project aims to provide a unified framework for discovery of relationships between observations in an efficient and theoretically-sound manner. <br/><br/>Combining the notion of generalized correlation with the locality principle, multiscale generalized correlation (MGC) is a superior correlation measure that equals the optimal local correlation among all possible local scales. By building upon distance correlation and making use of nearest neighbors, the resulting MGC test statistic is a unique dependence measure that is consistent for testing against all dependencies with finite second moment, and it exhibits better performance than existing state-of-art methods under a wide variety of nonlinear and high-dimensional dependencies. By investigating the theoretical aspects of distance-based correlations, this project aims to further improve the finite-sample performance of MGC-style tests, extend its capability to testing dependence on network and kernel data, and broaden its utility to general inferential questions beyond dependence testing such as two-sample testing, outlier detection, and feature screening, as well as applications to brain activity, networks, and text analysis. Overall, this project intends to establish a unified methodology framework for statistical testing in high-dimensional, noisy, big data, through theoretical advancements, comprehensive simulations, and real data experiments."
"1810958","Estimation of Smooth Functionals of Covariance and Other Parameters of High-Dimensional Models","DMS","STATISTICS","07/01/2018","05/07/2018","Vladimir Koltchinskii","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","06/30/2021","$250,000.00","","vlad@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","","$0.00","A crucial problem in statistical inference for complex, high-dimensional data is to develop statistical estimators of parameters represented by high-dimensional vectors or large matrices. Optimal error rates in such estimation problems are often rather slow due to the ``curse of dimensionality"", and it becomes increasingly important to identify low-dimensional structures and features of high-dimensional parameters that could be estimated efficiently with error rates common in classical, ``low-dimensional"" statistics. Such features are often represented by functionals that depend smoothly of unknown parameters and the goal is to take advantage of their smoothness to develop efficient estimation procedures. The problems of this nature often occur in a variety of applications such as signal and image processing, machine learning and data analytics.  The purpose of this project is to study these problems systematically and to develop new approaches to efficient estimation of smooth functionals. The project is in an interdisciplinary area between mathematics, statistics and computer science and it includes a number of activities to facilitate interactions with researchers in these areas and to ensure the impact of proposed research on education. <br/><br/>The main focus of the project is on the development of general methods of estimation of smooth functionals of covariance operators based on high-dimensional or infinite-dimensional observations. It is expected that these methods will be applicable to other important high-dimensional  models such as Gaussian shift models (both in vector and in matrix case); linear regression models (including trace regression and regression models in quantum state tomography); some non-linear models. The methods to be developed include a new approach to bias reduction in smooth functional estimation problems based on iterative application of bootstrap (bootstrap chains) and concentration and normal approximation bounds needed to establish asymptotic efficiency of estimators with reduced bias. The goal is to determine optimal smoothness thresholds for functionals of interest that ensure their efficient estimation, in particular, in a dimension free high-complexity setting, with complexity of the problem characterized by the effective rank of the true covariance. Other directions include the study of efficient estimation of smooth functionals under regularity assumptions on the parameter set and applications of methods of functional estimation to hypotheses testing for high-dimensional parameters.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1818546","Conference on Statistical Learning and Data Science","DMS","STATISTICS","05/01/2018","04/26/2018","Annie Qu","IL","University of Illinois at Urbana-Champaign","Standard Grant","Gabor Szekely","04/30/2019","$15,000.00","","aqu2@uci.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","7556","$0.00","Columbia University will host a three-day conference on Statistical Learning and Data Science/Nonparametric Statistics, June 4-6, 2018 in New York City. The objective of the conference is to bring together researchers in statistical learning, data science and nonparametric statistics from academia, industry, and government. Statistical machine learning is widely recognized as a very active area of interdisciplinary research, closely related to statistics, optimization, and computer science. In addition, it also plays an essential role in the new important areas of data science and big data. Due to advances in technology, massive and complex data in the ""big data era"" are prevalent in almost every aspect of modern scientific research. It is critical to manage such huge amounts of data, and make reliable prediction and inference. Statistical machine learning techniques have developed substantial flexibility in handling such data, with a wide range of applications in diverse scientific disciplines. <br/><br/>This conference is expected to (1) bring together researchers from different disciplines, including statistics, computer science, machine learning, engineering, and biomedical and other related research fields, to address recent development and emerging issues in statistical learning, data science and nonparametric statistics, (2) promote interactions and collaborations among researchers, (3) discuss new ideas and future research directions for statistical learning and data science, with a focus towards knowledge discovery in sciences and engineering, (4) provide an excellent opportunity for junior researchers to interact and learn from leading scientists in the field. The conference will consist of three plenary sessions, 55 invited sessions and poster sessions. Conference topics include unsupervised, semi-supervised and supervised learning, with applications in rankings, text and web mining, network analysis, bioinformatics, high-dimensional data, functional data, genomics, drug discovery, intrusion and fraud detection. NSF funding will provide travel support to students, post-doctoral scholars, and early-career researchers to encourage their participation in this event. The conference website is https://publish.illinois.edu/sldsc2018/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1707605","Statistical Problems in Large Volatility Matrix Estimation and Quantum Annealing Based Computing","DMS","STATISTICS","01/01/2018","11/13/2020","Yazhen Wang","WI","University of Wisconsin-Madison","Standard Grant","Pena Edsel","12/31/2021","$108,646.00","","yzwang@stat.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","057Z, 7203","$0.00","As the modern ""data deluge"" grows, the importance of statistics continues to increase, and uses of statistical methods rapidly evolve. The new era of data science poses great challenges to traditional statistical tools and computational techniques; yet, at the same time, the data deluge presents unprecedented opportunities to statistics. This project plans to advance research at the frontiers of science with innovative statistical and computational approaches that address the challenges encountered in handling complex problems with big data. The project's statistical research on quantum computing and high-frequency finance will address practical problems, and the projects will create advanced effective statistical tools with direct applications in fields including finance, quantum computation, and quantum information. <br/> <br/>This project seeks to conduct novel research on quantum annealing and statistical inference about large-dimensional matrices. The goals entail developing statistical methodologies, computing techniques, and theories for (i) statistical inference for large diffusion covariance matrices with applications to high-frequency finance, and (ii) statistical research on quantum annealing in quantum computation and quantum information. The project will develop rigorously-supported statistical methods and computational techniques, furthering the theoretical underpinning of these important topics."
"1811998","Development of  a General Framework for Nonlinear Prediction Using Auto-Cumulants: Theory, Methodology, and Computation","DMS","STATISTICS","08/01/2018","07/21/2020","Soumendra Lahiri","NC","North Carolina State University","Continuing Grant","Gabor Szekely","05/31/2021","$150,000.00","Tucker McElroy","s.lahiri@wustl.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","","$0.00","Data exhibiting nonlinear characteristics appear routinely in many areas of applications, such as weather forecasting, signal processing, etc. These features are also present in many economic and demographic time series collected by various national agencies for policy formulations that have important implications for the public and the society. However, the current methodology is heavily reliant upon linear approaches and some ad hoc methods are often used to handle nonlinear data, rendering the final results of analysis difficult to interpret. As a result, there is acute need for systematic development of new theoretical and methodological framework for improved prediction that takes into account the nonlinear features of the time series data. The proposed research seeks to address this need directly by developing new capabilities that will build on the existing linear theory for Gaussian and provide substantially improved prediction. In addition to advancing the statistical science and related scientific applications, it will also have potential impact on the practice of seasonal adjustments for better public policy formulation in the US and other nations.<br/><br/><br/>This project seeks to develop new theory and methodology for prediction for non-Gaussian, nonlinear processes, utilizing the tools of higher-order auto-cumulant functions and polyspectra. Specifically, the goals of the project include : (i) developing quadratic and higher order nonlinear predictors, with demonstrable improvements, (ii) extending forecasting approaches for a new class of so-called quadratically predictable processes; (iii) developing nonlinear models-fitting via an appropriate generalization of the Whittle likelihood, derived from the mean squared error of the one-step ahead quadratic forecasting filter, (iv) developing theoretical foundations of auto-cumulants for multi-linear forms that are paramount to derive third and higher order polynomial predictors,(v) developing algorithms and supporting software in R for implementation of the methodology. The results from the project are expected to provide tools for substantially improved forecasting and signal extraction for univariate and multivariate time series data exhibiting nonlinear characteristics that are prevalent in many areas of sciences (e.g., Astronomy, Atmospheric sciences, Finance, Signal Processing) and real life applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810591","New Development in Point Process Theory, Methods and Applications","DMS","STATISTICS","07/15/2018","07/16/2018","Yongtao Guan","FL","University of Miami","Standard Grant","Gabor Szekely","06/30/2021","$100,000.00","","yguan@bus.miami.edu","1320 S. Dixie Highway Suite 650","CORAL GABLES","FL","331462926","3052843924","MPS","1269","","$0.00","With the rapid development of modern data collection technologies, high-resolution spatial, temporal, and spatial-temporal data have become available at an unprecedented speed in recent years. The complexity and magnitude of these new data call for new statistical modeling tools. The proposed research will develop new modeling tools that can be used to analyze complex and large data.  Novel applications of the proposed methods will be considered to answer scientific questions arising in disciplines such as epidemiology, finance, and sociology.<br/><br/>This project will develop new theory and methods in point processes. In particular, the project will develop (1) more efficient estimation procedures based on quasi-likelihood to fit point process models and (2) a novel framework to conduct principle component analysis for marked point processes. For the first aim, efficient computational algorithms will be developed and theoretical properties of these algorithms will be investigated. For the second aim, the marks and points of the marked point process are linked through two potentially correlated latent processes, and principle component analysis is conducted for each of the two latent processes. Theoretical properties of the proposed method will be investigated. Data driven procedures will be developed to select the tuning parameters.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1811802","Collaborative Research: New Developments in Direct Probabilistic Inference on Interest Parameters","DMS","STATISTICS","08/01/2018","05/11/2018","Ryan Martin","NC","North Carolina State University","Standard Grant","Gabor Szekely","07/31/2021","$199,464.00","","rgmarti3@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","9263","$0.00","The Bayesian approach to statistical learning relies on probabilistic models for all observables and unknowns.  The need to model all aspects of the problem can restrict the scope of applications and, more generally, can be a burden to the data analyst who is often only interested in certain features of the unknowns.  This project will develop a mathematically rigorous and computationally efficient framework in which Bayesian learning can be carried out directly in terms of only the features of interest.  This reduces the modeling and computational burden on the data analyst and provides new insights about Bayesian learning more generally<br/><br/>A Bayesian approach is a powerful and rigorous framework for statistical learning.  The downside is that it requires a full model for the observables as well as all unknown quantities, the specification of which can be a burden on the data analyst. In addition to the familiar challenges of prior specification, there are also risks of misspecification biases. A more subtle complication is due to selection effects that result from considering several candidate models.  The data analyst's burden is further exaggerated in situations where only a feature of the unknowns is of interest, i.e., when there is an interest parameter and a (potentially high-dimensional) nuisance parameter and inference is required only for the former.  That is, the Bayesian approach still requires that the data analyst make non-trivial efforts to specify prior distributions and carry out posterior computations relevant only to the nuisance parameter, which can be viewed as a waste.  Yet having access to a posterior distribution for inference on the interest parameter is still a desirable feature, and the proposed research aims to develop a new framework for posterior inference directly on interest parameters.  These direct posteriors (DiPs) effectively target the interest parameter, giving data analysts an opportunity to avoid the seemingly wasteful modeling and computation efforts involving nuisance parameters.  This project will construct DiPs for finite- and infinite-dimensional interest parameters with rigorous theoretical guarantees, and will also develop efficient computational tools to facilitate DiP-based inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1925845","Hidden Components in Modern Applications","DMS","STATISTICS","07/01/2018","04/17/2019","Zheng Ke","MA","Harvard University","Standard Grant","Gabor Szekely","06/30/2020","$141,781.00","","zke@fas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","8083","$0.00","In the era of Big Data, researchers often encounter datasets that are large in size and complex in structure where the information of interest is usually contained in ""components"" hidden in the enormous amount of noise.  Examples include communities in large social networks, topics in text documents, and confounding factors in genome-wide association studies (GWAS).  Extracting these hidden components is an interesting but challenging problem. This project will address these challenges and include applications to many scientific areas including social networks, text mining, genomics, and genetics. The project will include (a) collection of large social networks data, (b) development of new models, methods, and theory for extracting hidden components in network analysis, text mining, and genome-wide association studies, and (c) a study of knowledge discovery using academic research data such as co-authorship and citation relationships. The research will have an impact in linguistics, social sciences, cancer research, and knowledge discovery.  <br/><br/>This project aims to develop statistical models, methods, and theory for inferring and utilizing hidden components in complex data, especially matrix data.  The goals of the project include: (1) Development of simple and fast methods for network mixed membership estimation and topic model estimation. These methods, based on nontrivial modifications of Principal Component Analysis (PCA), are easy to implement and can handle very large data. (2) New methods and theory for detecting and estimating rare and weak effects in GWAS.   Problems related to optimal ranking of genes in the presence of complex correlation structures and detection of weak spikes in large covariance matrices will be considered. (3) A study of social network structures of scientific researchers.  The PI and her collaborators will collect meta-information from published articles in representative statistics journals to understand social network structures and other features of the statistics community. (4) Development of new random matrix theory (RMT) for statistical analysis."
"1854545","High-Dimensional Bayesian Computations: The Moreau-Yosida Posterior Approximation","DMS","STATISTICS","07/01/2018","09/17/2018","Yves Atchade","MA","Trustees of Boston University","Continuing Grant","Gabor Szekely","06/30/2019","$221,874.00","","atchade@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","MPS","1269","","$0.00","Bayesian inference is a powerful statistical inference method that allows statisticians and other scientists to combine existing knowledge with new data samples for better inferences and decisions. The difficulty of sampling from posterior distributions is one of the biggest impediments to a wider adoption of Bayesian procedures in high-dimensional/big data analysis. There is a need for fast and accurate posterior approximation methods to assist with the practical implementation of Bayesian statistics in high-dimensional problems. This research project will use ideas from the related field of optimization to develop a Bayesian posterior approximation method that satisfies these requirements. The methodology will find applications in a wide-range of areas such as finance, marketing science, epidemiology, biology, medical sciences, and others.<br/><br/>More specifically, there is a need in statistics for posterior approximation methods in high-dimensional problems that: (a) produce approximations that are easier to explore by Markov Chain Monte Carlo (MCMC), and (b) are well-understood from a theoretical viewpoint. This project will use the Moreau-Yosida approximation and related tools from optimization and variational analysis to develop a Bayesian posterior approximation method that satisfies the above two conditions. The research from this project will help clarify similarities and differences between optimization and simulation problems. This research will also contributes to the theoretical analysis of Markov Chain Monte Carlo algorithms, with the special focus on understanding the mixing time of MCMC algorithms in high-dimensional settings. The project will also address open problems in high-dimensional Bayesian variable selection and will develop some novel modeling and computational solutions. There are many applied research areas, including biomedical research, epidemiology, marketing science, and social science research, where variable selection plays an important role. Hence, results from this research will allow researchers in those areas to better handle available data and gain new insights into relevant scientific questions. On the educational side, the material from this research will form a key component of the doctoral dissertation of the Ph.D. students supported by this grant. The project will also enable the PI to use the related scientific problems and datasets to enrich the learning experience of students in his classes and possibly other classes taught by his colleagues. Furthermore, novel methodologies from this research will be widely disseminated to the scientific community through presentation of academic seminars as well as presentations at high-visibility conferences in statistical computing."
"1844975","Graduate Statistics Curriculum at a Crossroads","DMS","STATISTICS","08/15/2018","08/10/2018","Deborah Nolan","CA","University of California-Berkeley","Standard Grant","Gabor Szekely","07/31/2020","$68,378.00","","nolan@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","7556","$0.00","Funding from this award will support the workshop ""Graduate Statistics Education at a Crossroads"", to be held November 2-3, 2018 in Alexandria, VA.  The workshop will identify challenges and barriers that limit graduate statisticians from fully participating in the new era of big data.  To address these challenges and barriers, workshop attendees will craft realizable recommendations for academic departments and institutions. It is expected that these recommendations will promote model curricula, pedagogical approaches, and innovative approaches that have the potential to foster broader engagement with data science and to promote success of future statisticians in data science research and the workforce more broadly. <br/><br/>The workshop aligns with the training aspects of Harnessing the Data Revolution, one of ten big ideas for future NSF investment. More specifically, the workshop aims to identify an array of data-science related training activities that would support the statistics community in order that statisticians fully participate in cutting-edge interdisciplinary data science research. Additional details may be found at https://hub.ki/groups/statisticsgraduateeducation/overview<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841682","Borrowing Strength: Theory Powering Applications","DMS","STATISTICS","08/01/2018","08/03/2018","T. Tony Cai","PA","University of Pennsylvania","Standard Grant","Gabor Szekely","07/31/2019","$25,000.00","","tcai@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","7556","$0.00","A one-and-half-day  workshop will be held at the Wharton School, University of Pennsylvania, on November 30 - December 1, 2018. This workshop will bring together some of the most prominent researchers in the field of statistics from around the world. The main goals are to facilitate the exchange of recent research developments, to provide opportunities for new researchers and underrepresented groups, and to establish new collaborations that will channel efforts into pending problems and open new directions for future investigation.<br/><br/>The workshop will cover topics of fundamental statistical theory having broad applicability. It will also emphasize areas of statistical research offering innovative approaches to problems arising in various branches of the sciences including bioinformatics, image analysis, signal processing and genomics. In addition to the core invited talks, the workshop will feature discussion sessions. Slides of all the talks will be posted on the conference web site https://statistics.wharton.upenn.edu/research/seminars-conferences/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1810837","Causal Inference Methods for Mediation and Comparisons of Confidence Regions","DMS","STATISTICS","06/01/2018","05/10/2018","Judith Lok","MA","Harvard University","Standard Grant","Nandini Kannan","11/30/2018","$153,267.00","","jjlok@bu.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","","$0.00","In epidemiology, clinical research, and the social sciences, inferences about the causal effects of treatments and risk factors are used to design more effective interventions.  This project focuses on the development of statistical methods for causal inference.  The first part of this project will develop a causal inference method for mediation analysis.  If a treatment has a beneficial effect on an outcome, it is often of interest to investigate what are the pathways by which it affects the outcome. Direct and indirect effects decompose the effect of a treatment into the part that is mediated by a covariate (the mediator) and the part that is not.  For example, in HIV/ AIDS research, it is important to estimate how much of the effect of Antiretroviral Therapy (ART) on mother-to-child-transmission of HIV is mediated by the effect of ART treatment on the HIV viral load in the mother's blood.  In medicine, psychology, political science, and economics, differentiating between indirect and direct effects has become increasingly important.  Therefore, it is paramount that appropriate statistical methods are developed to estimate direct and indirect effects in a variety of settings, including the setting in which there are post-treatment common causes of the mediator and the outcome.  The second part of this project will compare confidence regions.  Recently, there has been extensive discussion in the statistical community about a move away from p-values.  P-values can lead researchers to conclude that a treatment has a significant effect even if that effect is very small, and clinically irrelevant.  Confidence regions are the obvious alternative to p-values, as they provide a range of values of the parameters of interest that are most consistent with the data.  While comparisons of p-values have been extensively researched and confidence regions are routinely reported, comparison of confidence regions has received relatively little attention.  In this project, confidence regions will be compared based on the notion of asymptotic equivalence.  <br/><br/>Natural direct and indirect effects use cross-worlds counterfactuals: outcomes under treatment with the mediator ""set"" to its value without treatment.  Cross-worlds counterfactuals can never be observed, as they involve quantities under two different treatments where only one treatment is given to any particular patient or unit.  The PI has recently proposed organic direct and indirect effects to avoid the use of cross-worlds counterfactuals.  Organic direct and indirect effects also apply when the mediator cannot be ""set"".  For example, the HIV viral load in the mother's blood cannot be set; if it could be set, doctors would set it to zero.  In the first part of this project, organic direct and indirect effects will be extended to settings with post-treatment common causes of the mediator and the outcome.  It will be shown that, in contrast to natural direct and indirect effects, estimators and confidence intervals can be developed in that setting for organic effects.  The second part of this project will compare confidence regions. Most work on the comparison of confidence regions has studied coverage probabilities, confidence interval length, and small sample properties. In this project, confidence regions will be compared for large samples, based on the asymptotic behavior of the Hausdorff distance between the different confidence regions. The Hausdorff distance between partly overlapping intervals is simply the maximum of the difference between the left limits and the right limits of the intervals. The Hausdorff distance has also been defined for non-convex sets and in higher dimensions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1812088","Third Workshop on Higher-Order Asymptotics and Post-Selection Inference","DMS","STATISTICS","08/15/2018","04/13/2018","Todd Kuffner","MO","Washington University","Standard Grant","Gabor Szekely","07/31/2019","$7,000.00","","kuffner@math.wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","MPS","1269","7556","$0.00","The Third Workshop on Higher-Order Asymptotics and Post-Selection Inference (WHOA-PSI) will be held on the campus of Washington University in St. Louis from September 8-10, 2018.  Post-selection inference refers to the question of valid statistical inference performed after a model selection procedure has been utilized. Higher-order asymptotics provide the tools and insights needed to refine basic large-sample results for post-selection inference procedures and make them more accurate and powerful, even when the scientist only has access to small to moderate sample sizes. WHOA-PSI seeks to merge these fields, foster collaboration and discussion, and push forward the post-selection inference research frontier in the direction of higher-order accuracy and more powerful inference procedures. NSF funding will support junior researchers attending this conference.<br/><br/>This workshop attempts to address the needs of applied statisticians utilizing model selection procedures who also need extremely accurate and powerful inference procedures with theoretical guarantees. Topics from post-selection inference include selective inference after variable selection, high-dimensional and post-regularized inference, inference after change-point estimation, and simultaneous inference. Some talks will focus on methods from higher-order asymptotics, which can yield insights and improvements to post-selection inference procedures, including series expansions, saddlepoint approximations, and bootstrap-based refinements. Additional talks will present important applied problems for which new post-selection methods are needed. Examples include statistical genetics, neuroscience, and finance. (See http://www.math.wustl.edu/~kuffner/WHOA-PSI-3.html)<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1821785","IISA 2018: From Data to Knowledge, Working for a Better World","DMS","STATISTICS","04/01/2018","02/23/2018","Somnath Datta","FL","University of Florida","Standard Grant","Gabor Szekely","03/31/2020","$20,000.00","Sanjib Basu, Dipankar Bandyopadhyay","somnath.datta@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","7556","$0.00","This award supports a four day 2018 international conference titled ""IISA 2018: From Data to Knowledge, Working for a Better World""  which will take place at the University of Florida, Gainesville, Florida, from May 17 - May 20, 2018. This conference will bring together statisticians from diverse areas of statistics/biostatistics, probability and data science in a collaborative setting to discuss and foster the latest developments of statistical theory, methods and tools for making accurate inference from complex and noisy data sources. The grant funding will be used to cover partial costs of a number of doctoral students and young researchers who will present their research at this conference. <br/><br/>The theme of the conference is to discuss and foster the cutting-edge developments of statistical theory, methods and tools for making accurate inference from complex and noisy data sources. Plenary speakers for this conference include Montserrat Fuentes (Virginia Commonwealth University),  Hira Koul (Michigan State University),  and  Cyrus Mehta (Cytel Inc).  In addition, seven  researchers will be giving Special Invited Talks. A number of junior researchers will be given the opportunity to present in invited sessions. Doctoral students will have an opportunity to give oral presentations on a competitive basic. A student poster competition will also be arranged. For further details, please visit the conference web-site:  http://iisa2018.biostat.ufl.edu/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1854181","Collaborative Research: Theory and Methods for Massive Nonstationary and Multivariate Spatial Processes","DMS","STATISTICS","10/01/2018","09/17/2018","Soutir Bandyopadhyay","CO","Colorado School of Mines","Standard Grant","Gabor Szekely","07/31/2019","$55,941.00","","sbandyopadhyay@mines.edu","1500 Illinois","Golden","CO","804011887","3032733000","MPS","1269","","$0.00","The field of spatial statistics is an expanding subset of statistical science with numerous applications in a wide variety of specialties such as geophysical, environmental, ecological and economic sciences.  Modern datasets in these sciences often involve multiple variables observed at thousands to millions of irregularly spaced geographical locations.  Associated scientific goals include surface estimation, stochastic simulation and statistical modeling to gain insight of underlying phenomena.  Statistical analyses require flexible nonstationary and multivariate constructions, which have heretofore been hampered by a lack of models adequate for datasets of large magnitude.  This project addresses this gap in statistical science, developing a unifying framework for nonstationary and multivariate spatial models capable of modeling complex spatial dependencies.  Additionally, the justification for the use of nonstationary models is generally relegated to empirical results with data and simulation experiments; this research will develop a companion theory for exploring the relative benefit of these more complex spatial models. Using the tools introduced in this project, the final major goal is to develop a gridded data product for the historical climate of the United States based on large, irregularly spaced observational networks with transparent statistical methodology and formal quantification of the uncertainty in such an analysis.  Historical data products such as this are of crucial importance in the fields of atmospheric and climate sciences.<br/><br/>Modern spatial statistics has increased focus on developing methods for massive spatial datasets that involve multiple variables with complex dependency structures.  This research aims to foster a common framework via multiresolution processes for modeling nonstationary and multivariate spatial structures that does not break down in the face of large sample sizes.  Multiresolution processes lend themselves to fast estimation and computation, and also to the linked theoretical questions of asymptotic behavior of spatial estimators.  For example, there is a lack of rigorous theoretical treatment of nonstationary approaches, with current understanding limited to experimental results.  The companion large sample theory of this research is aimed at identifying situations in which nonstationary models provide tangible benefits over simpler stationary cousins.  A linked goal is approximation theory for existing spatial constructions; special multiresolution constructions can approximate existing covariances such as the Matern, allowing for a theoretical treatment of spatial smoothing under these common classes of covariances.  Additionally, the project will generalize the notion of a multiresolution process to the multivariate setting, allowing for feasible and flexible inference-based modeling of massive multivariate spatial datasets."
"1812508","2018 Quantitative Biology (q-bio) Summer School at Rice University","DMS","STATISTICS, MATHEMATICAL BIOLOGY","05/01/2018","04/05/2018","Marek Kimmel","TX","William Marsh Rice University","Standard Grant","Junping Wang","04/30/2019","$32,500.00","John Dobelman","kimmel@stat.rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269, 7334","7556","$0.00","The award provides partial travel/subsistence support for participants of the ""2018 q-bio Summer School at Rice University,' to be held June 11-24, 2018 at Rice University, immediately preceding the international q-bio Conference. ""Q-Bio"" stands for Quantitative Biology and reflects the Summer School's goal to combine biological and mathematical methods to understanding the evolution and function of biological cells in health and disease. Individual cells respond to stimuli such as viruses in different (""noisy"") ways, but their interaction is a coordinated immune response which can be predicted using mathematics. Another example is the effect of chemotherapy on leukemias, which may leave small numbers of residual malignant cells and lead to recurrence of disease. Again, it is possible to predict, using mathematics and computer simulations, how and when this may happen.  The q-bio School encourages early career scientists to pursue quantitative systems biology and provides a forum for the exchange of new results and ideas. The unique formula of the School includes active participation in research projects, some of which lead to scientific progress and publications. The Summer School originated in 2007 on the campus of Los Alamos National Laboratory in New Mexico, and past summer schools have been very successful, with over 85% positive reviews and 76% former participants actively engaged in quantitative biology research. The current School at Rice University campus takes advantage of the Houston Health Center, the Baylor College of Medicine and MD Anderson Cancer Center.  The School will be followed by the Annual q-bio Conference at Rice University. Over 11 years the q-bio Summer School has developed an extensive network of 4-500 alumni and actively promotes participant diversity.<br/><br/>Two educational tracks are envisioned in 2018: Cancer Dynamics and Stochastic Cell Regulation. The program will last two weeks and include: ten 1-hour research seminars; ten 1.5-hour general lectures; 30 hours of in-depth breakout discussions, chalk talks and computer lab demonstrations; 8 career-focused panel discussions;  20 student oral presentations; two poster sessions; 20+ hours of mentored project work;  and formal and informal networking opportunities. The Stochastic Cell Regulation track   explores stochasticity and cell-to-cell variability in the measurement and modeling of biochemical systems, concentrating on how small numbers of important molecules (i.e. genes, RNA, and protein) affect dynamics of living cells. The Cancer Dynamics track addresses biological and mathematical issues related to modeling of evolution of cancer. Lectures cover topics spanning many time- and length-scales, from fundamental issues of cell proliferation and mutation dynamics to molecular events affecting specific pathways in cells and population genetics effects.  This section of the summer school includes instructor-suggested group projects, in which students formulate and explore stochastic models of cancer evolution, applying these tools to model experimental and clinical data. More information can be found at the q-bio School's website <br/><br/> http://q-bio.org/wp/qbss/2018schedule/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."

"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"9510376","Mathematical Sciences:  Self-Consistent Estimators,         Bootstrap and Censored Data","DMS","STATISTICS","06/15/1995","06/02/1995","Jian-Jian Ren","NE","University of Nebraska-Lincoln","Standard Grant","James E. Gentle","11/30/1997","$18,000.00","","jjren@umd.edu","2200 VINE ST BOX 830861","LINCOLN","NE","685032427","4024723171","MPS","1269","0000, 9221, OTHR","$0.00","9510376 Ren  Abstract      The proposed research is concerned with the statistical inference problems using various types of censored data, including right censored, doubly censored, interval censored, left truncated - right censored, and bivariate censored data. Estimation, tests and confidence intervals with incomplete date are all under consideration. One objective of the research is to investigate a new resampling method, called the Leveraged Bootstrap, so that the difficulty of extending the usual estimation and testing procedures for complete data to incomplete data in certain situations or certain complicated models can be avoided. So far, the procedures developed for incomplete data are constructed on an ad hoc basis. The leveraged bootstrap does not rely on these ad hoc extensions. The research will focus on the investigation of the consistency of the leveraged bootstrap in a number of important statistical inference problems with incomplete data, such as empirical likelihood inference, nonparametric likelihood confidence bands, linear regression models, tests of independence for two random variables, tests in some semiparametric change point models, mixture models and biased sampling problems. Another objective of the research is to investigate the consistency of the m out of n bootstrap, i.e., resampling fewer than n observations, in the hypothesis testing problems using various censored data. It is known that the usual nonparametric bootstrap (the n out of n bootstrap) fails when one tries to estimate the distribution of test statistics under a semiparametric (restricted nonparametric) hypothesis and ignores the restrictions imposed by the hypothesis. Since the distributions of test statistics for censored data are often unknown and very complicated due to the unknown distributions of the censoring variables, it is of great interest to investigate possible remedies of the usual bootstrap in order to provide the critical values for the tests. Examples show that in certain situations,  there are no obvious general remedies except the m out of n bootstrap.    This research is primarily motivated by statistical inference problems using incomplete data which are frequently encountered in medical and reliability studies. In particular, doubly censored data and interval censored data occur in breast cancer research and AIDS research, respectively. The usual estimation and testing procedures for complete data are not directly applicable to such complicated incomplete data, and statistical analysis is lacking. The objective of this research is to investigate some general methods which will directly facilitate research in other fields, such as medicine."
"9423247","Mathematical Sciences:  Bandwidth Selection in              Semiparametric Regression Problems","DMS","STATISTICS","06/15/1995","06/07/1995","Naisyin Wang","TX","Texas A&M Research Foundation","Standard Grant","James E. Gentle","05/31/1997","$18,000.00","","nwang@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, 9221, OTHR","$0.00","9423247 Wang  Abstract      This project  investigates bootstrap bandwidth selection procedures in semiparametric regression models. As pointed out in Hsieh and Manski (1987), the performance of the estimated regression parameter in a semiparametric regression model could be rather sensitive to the choice of bandwidth. The traditional plug-in method often requires estimating elements within a complicated second order expansion of the estimated regression parameter. The fact that the properly chosen bootstrap procedure could estimate these complicated terms in an automatic fashion makes this procedure especially appealing. Two classes of semiparametric regression models are considered here: (i) semiparametric heteroscedastic regression models, and (ii) semiparametric measurement error regression models.  Different resampling strategies from those in the parametric and nonparametric regressions have to be pursued due to the different semiparametric structure. Resampling strategies for the aforementioned two semiparametric settings are proposed.  Properties of the proposed bandwidth estimators are investigated theoretically as well as numerically.     The availability of high-speed computing has allowed researchers to use ""semiparametric modeling,"" a new mathematical development which, for most scientific explorations, is preferable to its traditional parametric rivals because of much greater flexibility. Some recent applications of semiparametric modeling include studies which compare the effects of high versus low AZT treatments on the survival of AIDS patients,  the investigation of effects of saturated fat on breast cancer based on self-support food questionnaires and on detailed food records,  the relationship between a patient's age at the time of bone-marrow transplant and the incidence of chronic graft versus host disease (GVHD) afterwards. One of the major difficulties in semiparametric modeling is to properly choose a so-called ""bandwidth"" which controls the amount of data going into the  model simultaneously.. Poor choice can result in loss of information or loss of accuracy. This project investigates bandwidth selection procedures which would be theoretically sound and practically feasible.  The main approach emphasizes taking advantage of modern computing power to help users to avoid complicated  mathematical derivations. Success of this project will certainly further promote the applications of semiparametric modeling in many other fields."
"9504589","Mathematical Sciences:  Some Estimation and Computation     Procedures in Statistics","DMS","STATISTICS","07/15/1995","07/03/1995","Suojin Wang","TX","Texas A&M Research Foundation","Standard Grant","James E. Gentle","06/30/1997","$14,000.00","","sjwang@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","Proposal:  DMS 9504589  PI:  Suojin Wang  Institution:  Texax A&M  Title:  SOME ESTIMATION AND COMPUTATION PROCEDURES IN STATISTICS     Abstract:               The primary objective of this research is to develop new theory  and methods in some important classes of statistical problems in  the areas of sampling, missing data analysis and measurement error  models.  The research topics are motivated by practical problems   and the main focus is on estimation and (approximate) computation   procedures. The research aims to significantly advance statistical   knowledge in these areas, and improve some current practices in   survey agencies in particular. In attacking some of the proposed   problems, small sample asymptotic methods are desired. The   potential use of accurate saddlepoint approximations in these   problems is studied. In other proposed problems, large sample   asymptotic methods are investigated. The research topics include  kernel-type estimations in regression when data are partially   missing or measured with error; sub-domain estimation in sample   surveys; and new and optimal estimation of the distribution   function of a finite population.       In this research project new statistical theory and methods are  developed and investigated in statistical problems that bear   practical importance. The accuracy and simplicity are the main criteria  in developing such statistical methods. They have direct applications   in various agencies, such as the Bureau of Labor Statistics, to   improve the accuracy and speed of the current statistical practice.   Furthermore, this research advances the science and technology  in the Federal Strategic Areas of high performance computing and  the statistical aspects of biotechnology."
"9504882","Mathematical Sciences:  Design of Experiments:  Improving   Practicability of Some Useful Concepts","DMS","STATISTICS","07/01/1995","06/21/1995","John Stufken","IA","Iowa State University","Standard Grant","James E. Gentle","06/30/1998","$75,000.00","","jstufken@gmu.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","9146, MANU","$0.00","Proposal:  DMS 9504882  PI:  John Stufken  Institution:  Iowa State University  Title:  Design of Experiments:  Improving Practicability of some Useful Concepts      Abstract:      This research focuses on developing tools to obtain efficient  designs for various settings. It involves the development of   algorithms and, where needed, supporting theory. Issues that are  addressed include blocking in mixture experiments, avoiding  undesirable level combinations in factorial experiments, and  converting a given block design to a trend-free or nearly trend-free   version of that design. A critical concept is a generalization of the  idea of trade-off in block designs.    Designed experiments are essential in most industrial experiments and   in many areas of scientific research. It is almost always crucial to   use limited available resources efficiently, and to assure that the  collected data can be used to answer questions of interest reliably.   Areas of application include experiments for product quality   assessment and improvement in industrial production processes, and   experiments for quantitative research in, for example, agricultural   sciences, social sciences, and engineering disciplines. Much of the  research in design of experiments concentrates on identifying  efficient designs for a specific problem, often not broad enough to  be relevant for more than one area of application. The usefulness  of previous work has at times been limited by restrictions on the  results induced by the mostly mathematical tools used to derive  the results. Application of some of these methods has also suffered  from the mathematical sophistication that is required from a user.   The investigator develops new methods and algorithms to find   efficient designs for various specific problems. These algorithms,   which can be implemented easily with existing computer software,   facilitate use of the new methods, and do not require any   mathematical sophistication of a user."
"9510435","Mathematical Sciences:  Methods for Smoothing Bivariate,    Irregularly Spaced Data","DMS","STATISTICS","09/01/1995","06/07/1995","Karen Kafadar","CO","University of Colorado at Denver-Downtown Campus","Standard Grant","James E. Gentle","12/31/1996","$30,000.00","","kkafadar@indiana.edu","1380 LAWRENCE ST STE 300","DENVER","CO","802042055","3037240090","MPS","1269","0000, 9222, OTHR","$0.00","9510435 Kafadar  Abstract      The research under this project will develop methods for smoothing bivariate, irregularly spaced data such as environmental, geological, or health-related data with geographically-defined coordinates. Because such data often arise from non-Gaussian distributions with potentially non-stationary noise (e.g., highly skewed values in barometric pressure data, exotic values due to earthquakes in  geophysical data, discontinuities due to faults in geological data),  linear smoothers, or smoothers derived assuming stationary white noise, may not perform as well as more robust, nonlinear smoothers in capturing the underlying trend.  This research will attempt to identidy (1) those situations where nonlinear versus linear smoothers should yield good performance; (2) how smoothed values at the border should be defined (often a source of significant bias in estimating the trend); and (3) how the resulting smoothed trend can be displayed.    In 1974, the National Cancer Institute published the Atlas of Cancer Mortality for U.S. Counties: 1950-1969.  It consisted of U.S. maps, one for each of 35 sites of cancer, where counties were shaded according to the level of the mortality rate for the cancer site being depicted.  These maps illustrated, for example, high rates of lung cancer around the Gulf of Mexico and high rates of bladder cancer around Delaware and New Jersey; subsequently, environmental causes for these high rates were identified.  Further atlases of cancer mortality were published, and an atlas of mortality from causes other than cancer is forthcoming from National Center for Health Statistics.  Because some counties have very small populations, reported mortality rates are very uncertain; regions of elevated risks may be difficult to detect. The objective of this research is to develop methods which will highlight geographical patterns in data such as cancer mortality in U.S. counties. Geographical movement of populations often is responsible for spreading the risk  around.  Thus it is important to identify not just isolated counties of elevated risk but broad regions which may indicate environmental causes for concern.  Conversely, regions of low risk may serve as models for measures of disease prevention and control.  These methods can be applied to other sorts of data to answer similar questions, such as which regions indicate significant seismic activity, or in which places the ozone layer is depleting most rapidly."
"9505124","Mathematical Sciences:  Resampling Methods Under Long Range Dependence","DMS","STATISTICS","07/01/1995","06/30/1995","Soumendra Lahiri","IA","Iowa State University","Standard Grant","James E. Gentle","12/31/1998","$75,000.00","","s.lahiri@wustl.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","0000, OTHR","$0.00","Proposal:  DMS 9505124  PI:  Saumendra Lahiri  Institution:  Iowa State University  Title:  Resampling Methods Under Long Range Dependence    ABSTRACT:    The main thrust of the research is to develop suitable  resampling schemes and to study their properties for data  exhibiting very strong forms of dependence. It is well known  that existing resampling schemes perform very poorly under   such dependence structures. Two new resampling   schemes for long-range dependent data are investigated.   These schemes are  a sampling-window   method based on subsamples of the data and a bootstrap  method based on finite Fourier transformations.   Their   effectiveness in different inference problems is examined.     The phenomenon of long range dependence, such as the   concentration of  an air-pollutant in a city observed at closely   located sites, occurs when the data are highly correlated.  This   sort of data has a high level of variability, even after averaging.    The research involves developing suitable resampling  schemes for long-range dependent data that will provide good statistical  estimates.   The research has direct applications in Hydrology,   Geology, Economics, and Environmental problems."
"9500831","Mathematical Sciences:  Application of Interval Analysis    in Statistical Computing","DMS","STATISTICS","07/01/1995","05/25/1995","William Kennedy","IA","Iowa State University","Standard Grant","Stephen M. Samuels","06/30/1997","$50,000.00","","wjk@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","9216, HPCC","$0.00","9500831 Kennedy  Abstract    The IEEE proposed standard for binary floating-point computer  arithmetic has been adopted by most computer manufacturers.  Included among the features of this floating-point number  system is programmer selectable rounding mode. Two rounding modes, round to plus or minus infinity, give hardware support for implementations of interval analysis computations. Substantive areas outside statistics are employing interval analysis and interval computations to good advantage in research.  Research workers in statistical computing have been slow to recognize or evaluate the potential utility of interval analysis.  The overall objective of this research is to identify areas in  statistical computing where interval analysis can be used to good advantage, and to develop needed methods and algorithms in these areas. Specifically, a library of routines for self- validating approximation of cdf and inverse cdf for most of  the usual univariate continuous probability distributions, and selected multivariate distributions will be developed. Accuracy  essentially to machine precision over very broad areas of the variable/parameter space will be the goal.  Self-validating computational methods for certain kinds of global optimization problems will also be developed.   Scientific computations are usually done using a digital computer. Computers are not, however, capable of expressing exactly most numbers that constitute intermediate and final results for  a computing problem. A part of the inexactness inherent in computer arithmetic is due to the need to round to a specific number of numeric digits after each arithmetic operation.  Therefore answers produced by a computer for difficult computing  tasks are often only rough approximations to the unknown true answers which would be obtained if exact computations could be performed. The objective of this research project is to develop  alternative computational methods that yield more accurate answers in difficult statistical computing ap plications. The  price of increased accuracy is that substantially more computer resources are consumed to produce answers. However, today there are enough computer resources available to support this very computationally intensive approach. The main motivation for  the research is that it is best to spend extra time and effort to insure good answers, instead of settling for poor answers that are obtained quickly."
"9504980","Mathematical Sciences:  Computationally Aggressive          Approaches to Sequential Design","DMS","STATISTICS","07/15/1995","07/01/1999","Quentin Stout","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Joseph M. Rosenblatt","06/30/2000","$80,000.00","Janis Hardwick","qstout@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","Proposal: DMS  95-04980    PI(s):  Janis Hardwick - 1986, Quentin Stout - 1977         Institution: University of Michigan     Title:   Comutationally Aggressive Approaches to Sequential Design      Abstract:    This research is concerned with the design and analysis of sequential  designs for experiments.  Features of particular emphasis are i) obtaining  optimal designs and exact analyses through new computer algorithms and use  of high performance machines;  ii) generating practical designs which protect  against problems such as selection bias or time trends;  iii) flexibility in  design, allowing researchers to realistically represent their goals and  resource constraints.  These techniques are applicable to a wide range of  applications, but particular attention will be paid to clinical trials,  and industrial testing (particularly in reliability testing, using sequential  approaches to reduce costs and/or time).  Both Bayesian and frequentist  models will be considered.    This research explores the design and analysis of sequential designs of  experiments, in which decisions adapt or change as information arising from  the experiment accrues.  Using new computer algorithms and high performance  computers, new designs  are developed for problems in areas such as clinical trials     and industrial testing ."
"9504515","Mathematical Sciences:  Biased Sampling, Bump Hunting and   Confidence","DMS","STATISTICS","07/01/1995","07/03/1995","Michael Woodroofe","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","James E. Gentle","06/30/1997","$59,999.00","Jiayang Sun","michaelw@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","Proposal:  DMS 9504515  PI(s): Jiayang Sun and Michael Woodroofe  Institution:  Univ. of Michigan  Title:   Biased Sampling, Bump Hunting, and Confidence    Abstract:            The research will  include the development of confidence bands   for the    expected  response, viewed  as a   function  of the design   variables.   This will be  accomplished through combination of higher   order  asymptotic  analysis,  numerical calculation,  and simulation.   The  asymptotic  analysis  requires  finding  approximations  to  the   distributions of  maxima  of random  fields,  and even the asymptotic   formulas require some numerical  calculation.  The investigators will   also study maximum likelihood estimation  for biased sampling models,   including  models  in which   subjects  self-select,  and  tests  for   unimodality    versus  multimodality.   They    will  seek  efficient   algorithms, conditions  for consistency, and asymptotic distributions   for  estimation    error  and   test   statistics.    Higher    order   approximations  to confidence  levels  for  parametric models   which   exhibit dependence are another major objective of the research.  Such   models  include   Markov chains,  semi-Markov   processes,  and  time   sequential models in sequential analysis.  A promising approach is to   use Stein's Identity on the signed  root transformation.  In addition,   the   investigators   will   study   non-parametric estimation    for   semi-Markov processes  and  expected    sample sizes and    operating   characteristics for time sequential models.           Determining the amount of confidence that may be attached to an   estimate or projection is a primary objective of the research.  To do   this   the investigators  will  place   probabilistic  bounds  on the   unobservable estimation  error.  They will  focus  their attention on   two novel  contexts both of which involve  measurements which are not   independent  but, rather,  are  related  in  a  complicated way.  For   example,    in  studies  of air   pollution,   measurements at  nearby   locations may  be related.  The research  also includes  the study of   inference from observational  studies, studies in which the inclusion   or  exclusion of subjects is  not entirely  under  the control of the   experimenter.   In such cases  inclusion may depend on other factors,   like the variables of interest.  For example, in animal studies it is   easier  to  find a large  group  of animals than  a   small one.  The   development   of methods for  correcting the  raw  data to form valid   estimates is a major objective of the research."
"9504783","Conference on Statistical Challenges in Modern Astronomy II","DMS","EXTRAGALACTIC ASTRON & COSMOLO, STATISTICS","06/15/1995","02/06/1996","Gutti Babu","PA","Pennsylvania State Univ University Park","Continuing Grant","James E. Gentle","05/31/1997","$12,000.00","Eric Feigelson","babu@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1217, 1269","0000, OTHR","$0.00","Proposal:   DMS 9504783  PIs:  Gutti Babu, Eric Feigelson  Institution:  Penn. State University  Title:  Conference on Statistical Challenges in Modern Astronomy II           2.  Abstract for the project.            This award will support a cross-disciplinary conference on statistics and astronomy.           In recent years, astronomers have shown considerable          interest in applying advanced statistical techniques such as          errors-in-variables regression models for measurement error          problems, survival analysis for surveys with nondetections, and          clustering algorithms for galaxy distri`utions.  Communication          between the two communities was elevated by the successful          cross-disciplinary conference organized by Babu and Feigelson in          August 1991, the first such meeting in the U.S.  Since then,          interest on methodological issues has grown among astronomers,          with nearly 200 papers published annually in the astronomical          literature on statistical issues.  The proposed conference will          feature lectures by distinguished statisticians on advanced          methods applicable to astronomical research.            The proposed conference seeks to bridge the gap between the          communities of astronomers and statisticians, exposing          researchers and students to issues of common interest.  The          sessions will begin with an invited lecture by a statistician          describing important new developments relevant to a class of          astronomical problems.  Applications and suggestions to tune the          methods for the needs of the astronomy community will be          discussed by astronomers.  Specific data analysis problems and          methods will then be addressed in contributed presentations.          There is considerable opportunity for young researchers in both          fields to pursue methodological problems encountered in modern          astronomy."
"9415474","Mathematical Sciences:  DNA Forensic Science:  An Update","DMS","POPULATION DYNAMICS, STATISTICS, LSS-Law And Social Sciences, Biological Anthropology","02/15/1995","02/10/1995","Eric Fischer","DC","National Academy of Sciences","Standard Grant","Stephen M. Samuels","04/30/1996","$75,000.00","John Tucker, Lee Paulson","EFischer@nas.edu","2101 CONSTITUTION AVE NW","WASHINGTON","DC","204180007","2023342254","MPS","1174, 1269, 1372, 1392","9123, BIOT","$0.00","DNA typing has increasingly become a common source of forensic information in the United States and is currently used by federal, state and local crime laboratories. Recently, controversy has grown over how to interpret DNA evidence scientifically.  The debate about this issue must be resolved if DNA typing is to be used appropriately and if confusion in the courts is to be avoided.  The National Research Council is in the process of beginning a study to address the central questions, performed by a committee of distinguished experts.  The committee will include expertise in appropriate subdisciplines of statistics and population genetics, with additional expertise in molecular biology, forensic science, law, and social science, and other areas as necessary.  The study will emphasize statistical and population genetics issues relating to forensic DNA evidence and will pay particular attention to how such evidence is used in the courts.  The  committee will review relevant studies and data, especially those that have accumulated since the 1992  NRC report, DNA Technology and Forensic Science, and will  seek outside input from appropriate  experts.  The committee will meet five times during the course of the study.  The target date for  publication of the report is approximately seven months from the project start date."
"9522317","Mathematical Sciences:  Produce a World Wide Web Program of the Conference on Statistical Challenge– and Possible       Approaches in the Analysis of Massive Data Sets","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","06/01/1995","05/16/1995","John Alexander","DC","National Academy of Sciences","Standard Grant","James E. Gentle","05/31/1996","$18,000.00","","jalexander@amcmail.atlm.peachnet.edu","2101 CONSTITUTION AVE NW","WASHINGTON","DC","204180007","2023342254","MPS","1269, 1271","0000, 9263, OTHR","$0.00","Proposal:  DMS 95-22317  PI:  John Alexander  Institution:  National Academy of Sciences  Title:  Produce a World Wide Wed Program of the Conference on Statistical Challenges and Possible Approaches in the Analysis of Massive Data Sets       ABSTRACT    The workshop on Statistics and Massive Data Sets,  hosted by   the  Committee on Applied and Theoretical Statistics  of the National Research Council,   focuses on how to deal effectively with  massive amounts of data  that need statistical analyses. In numerous practical contexts, researchers   are confronted with the inadequacy of standard statistical techniques  to handle the explosion in the size of current data sets, where the  number of observations can easily run into the giga- and even terabyte  ranges.   Statistical assumptions underlying many  of the most popular methods, such as ""homogeneity"" of the data,  may simply be wrong  in these cases, thus rendering common statistical  tools ineffective and even dangerous.   A corresponding problem is the growth in the  number of dimensions in many current problems.  Again standard  approaches, which were developed  in a different  era, even predating the computer in many cases, are not powerful  enough for these challenges.      The workshop on Statistics and Massive Datasets explores  the challenges  presented by massive data sets, and  does so in the context   of a variety of real world and currently pressing application areas, including  the atmospheric, biological and geo sciences, business and marketing,  ecology, and engineering and technology. With these areas as a backdrop,   specific over-arching multi-disciplinary research issues  on common needs,   including relevant statistical principles, computational aspects, data analysis, and  visualization of massive data sets are identified.   The workshop involves highly  animated discussions among scientists.  These discussions are not easily  captured in a traditional published proceedings format.  In contrast, the  activities are capture d and distributed via the preparation of a videotape  of the workshop followed by a World Wide Web (WWW) dissemination.   This new dissemination method brings the benefit   of the actual discussions to a wide audience in a timely fashion."
"9504242","Mathematical Sciences:  ""Statistical Computation and        Computational Geometry""","DMS","GEOMETRIC ANALYSIS, STATISTICS","07/01/1995","06/18/1997","Daniel Naiman","MD","Johns Hopkins University","Continuing Grant","James E. Gentle","06/30/1998","$150,000.00","","daniel.naiman@jhu.edu","3400 N CHARLES ST","BALTIMORE","MD","212182608","4439971898","MPS","1265, 1269","0000, 9146, MANU, OTHR","$0.00","Geometric tools are developed that will provide   key  developments in computational statistical theory and   methodology. The research direction  is to develop and study   probability bounds for the probability content of convex   polyhedra with special structure via the ""abstract tube   method."" Accounting for this geometric structure can lead, in   some cases, to explicit and easy to calculate bounds for error   probabilities, and in other cases to greater efficiency in the   Monte-Carlo evaluation of error probabilities. This research   which is at the interface between computational statistics and   computational geometry, will produce an expanded collection   of tools for constructing practical and sharp probability   bounds for a vast   array of applications. Particular focus is   given to obtaining new   bounds for the probability content of   bodies in higher dimensions using methods that arise from   the study of lower dimensional problems with similar   structure.           There is an abundance of real world applications in which   stochastic models are utilized to make critical decisions based   on probability calculations. For example,   probability   calculations are used to determine how likely a modeled   system is to fail.  This applies to systems such as a bridge, an   ecosystem, a human body, or a manufacturing method.  The   research explores ways in which special model structure for   situations such as these can be exploited to produce more   efficient and precise probability calculations.  This new   methodology should be directly applicable to manufacturing     and quality control, structural and system reliability,     environmental regulation, and a host of other areas."
"9505440","Mathematical Sciences:  Diagnostics in Structured Data and  Quality Improvement","DMS","STATISTICS","07/01/1995","06/20/1995","Douglas Hawkins","MN","University of Minnesota-Twin Cities","Standard Grant","Joseph M. Rosenblatt","06/30/1999","$114,000.00","","doug@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","Proposal: DMS  95-05440  PI:     Douglas Hawkins   Institution: University of Minnesota     Title:   Diagnostics in Structured Data and Quality Improvement          Abstract:      The principal directions of the work are case and model diagnostics, and  methdologies for handling data with a natural ordering such as time series,  or regression data ordered by one of the predictors.  The first branch of  the work extends methods for identifying atypical cases in multiple  regression, multivariate and time series data, and for moderating their  effects on the subsequent analysis of the data.  The second branch of  the work addresses problems of methodologies for identifying structural  changes in ordered data (for example regression and multivariate data)  and for overcoming the difficulties caused by unknown parameter values.  A major application  for this work is statistical quality improvement.    Many important societal problems involve making valid inferences from  complex data sets.  The decision whether a facility is adding to the   pollution levels in the groundwater, for example, is made on the basis   of series of groundwater samples.  Analyzing these series is complicated  by the presence of apparent maverick values, which may be the result of  laboratory error or may reflect actual pollution events.  The methods   developed with this award help to recognize such values and to resolve the  issue of the true pollution levels.  Quality improvement is     of great importance - both in manufacturing and in all areas  of government and the private sector.  Pointers to quality improvement   for deterioration can be found from identifying structural changes in  series of measurements  The development of powerful easily-used  change diagnostics in this research will  lead to sounder and more reliable  ways of pinpointing the sources of change.  The work has both   methodological and computational elements.  While the computational  aspect benefits from high performance computing equipment and me thods,  the development of better computational procedures also leads to   solutions to problems currently beyond the powers of even the most  powerful computer systems."
"9505168","Mathematical Sciences:  Approximation, Estimation, and      Computation Properties of Neural Networks and Related       Parsimonious Models","DMS","STATISTICS","07/01/1995","06/20/1995","Andrew Barron","CT","Yale University","Standard Grant","James E. Gentle","12/31/1998","$79,500.00","","","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","0000, OTHR","$0.00"," Proposals:  DMS 9505168   PI: Andrew Barron   Ilstitution: Yale University   Title:   APPROXIMATION, ESTIMATION, AND COMPUTATION PROPERTIES                 OF NEURAL NETWORKS AND RELATED PARSIMONIOUS MODELS     Abstract:    Artificial neural networks and related parsimonious models for function   approximation and estimation have attracted recent attention in science   and engineering.  Work by the authors has uncovered several interesting   aspects of these methods.  Approximation bounds have been obtained by   methods taken from the probability theory of empirical processes,   including bounds on the average squared error and the maximal error of   neural network and related approximations.  These approximation bounds   reveal a rate of convergence that is insensitive to the dimension of the   input space for certain nonparametric (infinite dimensional) classes of   functions, specified via the closure of convex hulls of finite dimensional  families of functions.  As a consequence accurate statistical estimation   of functions in these nonparametric classes is possible without recourse   to exponentially large sample sizes.  Unfortunately, computation of neural   net estimates can be an extremely difficult task.  The investigators study   how the problems of accurate approximation, estimation, and computation are   intertwined.  In this research they investigate fundamental mathematical,   statistical, and computational limits of the capacity to approximate and   to estimate these functions accurately by computationally feasible   algorithms.    Empirical modeling techniques used in a variety of scientific and  engineering tasks deal with the problem of how to combine a large  number of observable quantities to best predict or approximate   a response variable.   The input - response relation may be described   by a rather complicated function, and it may be desirable to   approximate it by a combination of a small number of     elementary, comparatively simpler, functions. These models   dif fer from classical techniques in approximation  and statistical estimation in that the functions that are combined   are not fixed in advance, but rather selected and adjusted according   to what is known or observed concerning the intended response variable   so as to provide the best fit.  The investigators are quantifying the   mathematical and statistical advantages of these  adjustable selections.  Artificial neural networks and related techniques are at the heart  of modern models for adaptive and high performance computation.  The investigators study the limits of what is computationally   feasible with these models.  The ubiquity of requirements for accurate  prediction and empirical modeling for use of the scientific method  in general and for nationally strategic topics in particular are  motivating factors in this research."
"9501893","Mathematical Sciences:  Smoothed Nonparametric Hazard       Regression","DMS","STATISTICS","07/01/1995","06/18/1997","Birgit Grund","MN","University of Minnesota-Twin Cities","Continuing Grant","K Crank","06/30/1999","$72,000.00","","","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, 1045, 9178, OTHR, SMET","$0.00","9501893 Grund Abstract  The main objective of the project is to develop smoothing methods for  nonparametric hazard regression with time-dependent regression coefficients,  in the framework of an additive hazard regression model (Aalen model).  The  Aalen model incorporates the possibility that both value and contribution  of  covariates may change over time.  Most important, no particular parametric  shape of this time-dependence is assumed.   In the current project, kernel smoothing will be used to estimate the  regression coefficient curves.  A major objective is to develop data-driven  bandwidth selectors, and to investigate their properties.  It is expected that  the resulting methods will improve upon currently used empirical estimates.   Moreover, kernel methods allow one to estimate the regression coefficient  curves themselves, as opposed to cumulative coefficients.  This is  particularly important for graphical data analysis. The new smoothing methods will be implemented in XLISP-STAT, an object- oriented programming language; a user-friendly interface will be provided.   Dynamic graphics will be used to support visual data exploration.  As part of  the project, the developed smoothing procedures will be used to analyze  epidemiological data. On the side of education, the development of a new lecture course on  ``Smoothing Methods in Curve Estimation'' for statistics majors at the M.S. or  Ph.D. level is proposed.  Research results of the current proposal will be  included.  Dynamic graphics software will be used to demonstrate smoothing  techniques in class, thus giving students access to cutting edge technology   An important problem in medicine is to predict the survival of patients,  given their current condition and treatment.  The condition of a patient is  described by ""covariate values"", such as blood cholesterol, blood pressure,  number of antibodies, etc.  A central problem in analyzing survival data is to  assess the influence of covariates;  for example, to quantify by how  much an  elevated level of blood cholesterol increases the risk of stroke. Often the  influence of covariates is known to change with time.  In this case standard  methods tend to fail. The proposed project develops risk estimates based on the Aalen model.  This  model is extremely flexible.  The influence of covariates is allowed to change  over time, without assuming any particular shape of this time-dependence  beforehand.   In this project,  modern smoothing techniques will be adapted to estimate the  influence of covariates.  Smoothing methods are extremely useful for  descriptive data analysis, and widely used in statistics.  In the context of  survival data, however,  the use of corresponding methods is a very recent  development, with many open problems. Part of the project is the user-friendly computational implementation of the  newly developed estimation procedures.  Interactive graphics will be used  extensively to support visual data exploration.  With the provided software,  smoothing methods in Aalen hazard regression will be available for  practitioners for the first time. The new smoothing procedures will be used to analyze epidemiological data. On the side of education, the development of a new lecture course on  ``Smoothing Methods in Curve Estimation'' for statistics majors at the M.S. or  Ph.D. level is proposed.  Research results of the current proposal will be  included.  Dynamic graphics software will be used to demonstrate smoothing  techniques in class, thus giving students access to cutting edge technology."
"9505199","Mathematical Sciences:  Approximation, Estimation, and      Computation Properties of Neural Networks and Related       Parsimonious Models","DMS","STATISTICS","07/01/1995","06/20/1995","Lee Jones","MA","University of Massachusetts Lowell","Standard Grant","Joseph M. Rosenblatt","06/30/1999","$120,000.00","Yuly Makovoz","Lee_Jones@uml.edu","600 SUFFOLK ST STE 212","LOWELL","MA","018543624","9789344170","MPS","1269","0000, OTHR","$0.00"," Proposals:  DMS 9505199   PIs: Lee Jones and Yuly Makovoz  Institution: University of Massachusetts at Lowell  Title:   APPROXIMATION, ESTIMATION, AND COMPUTATION PROPERTIES                 OF NEURAL NETWORKS AND RELATED PARSIMONIOUS MODELS         Abstract:    Artificial neural networks and related parsimonious models for function   approximation and estimation have attracted recent attention in science   and engineering.  Work by the authors has uncovered several interesting   aspects of these methods.  Approximation bounds have been obtained by   methods taken from the probability theory of empirical processes,   including bounds on the average squared error and the maximal error of   neural network and related approximations.  These approximation bounds   reveal a rate of convergence that is insensitive to the dimension of the   input space for certain nonparametric (infinite dimensional) classes of   functions, specified via the closure of convex hulls of finite dimensional  families of functions.  As a consequence accurate statistical estimation   of functions in these nonparametric classes is possible without recourse   to exponentially large sample sizes.  Unfortunately, computation of neural   net estimates can be an extremely difficult task.  The investigators study   how the problems of accurate approximation, estimation, and computation are   intertwined.  In this research they investigate fundamental mathematical,   statistical, and computational limits of the capacity to approximate and   to estimate these functions accurately by computationally feasible   algorithms.    Empirical modeling techniques used in a variety of scientific and  engineering tasks deal with the problem of how to combine a large  number of observable quantities to best predict or approximate   a response variable.   The input - response relation may be described   by a rather complicated function, and it may be desirable to   approximate it by a combination of a small number of    elementary, comparatively s impler, functions. These models   differ from classical techniques in approximation  and statistical estimation in that the functions that are combined   are not fixed in advance, but rather selected and adjusted according   to what is known or observed concerning the intended response variable   so as to provide the best fit.  The investigators are quantifying the   mathematical and statistical advantages of these  adjustable selections.  Artificial neural networks and related techniques are at the heart  of modern models for adaptive and high performance computation.  The investigators study the limits of what is computationally   feasible with these models.  The ubiquity of requirements for accurate  prediction and empirical modeling for use of the scientific method  in general and for nationally strategic topics in particular are  motivating factors in this research."
"9423706","Mathematical Sciences:  Measurement Error and Statistical   Inference","DMS","STATISTICS","09/01/1995","06/20/1995","Leonard Stefanski","NC","North Carolina State University","Standard Grant","Joseph M. Rosenblatt","08/31/1999","$120,000.00","","stefansk@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","0000, 1306, EGCH, GLCH, OTHR","$0.00","This research develops new methods of statistical inference for  data measured   with error. The methods exploit the availability of high-speed  computing to   simulate the effects of measurement error on a data set. The  results of such   simulation experiments form the basis of a method of inference  called   simulation extrapolation. The first component of the research  adapts the method   of simulation extrapolation estimation to situations in which  replicate   measurements are made on the variables measured with error. The  second   component of the research uses the complex-variable formulation  of simulation   extrapolation to bypass the extrapolation step. This is  accomplished by   performing a simulation experiment using complex pseudo random  variables to   compute unbiased estimating equations for the parameters of  interest. The third   component of the research focuses on two-sample hypothesis  testing problems,   developing tests that are asymptotically valid when data from one  sample are   measured with error.     The presence of measurement error in data can affect the validity  of   conclusions drawn from statistical analyses of such data. For  example,   measurement error in risk factors such as blood pressure or  cholesterol level   affects statistical determination of the relationship of heart  disease to such   risk factors, usually resulting in underestimation of the  beneficial effects of   controlling these risk factors. Similarly, statistical  determination of the   relationship of ecosystem health to ecosystem stressers  (pollutants, for   example) can be obscured by the inability to accurately measure  stresser   levels. This research develops statistical theory and methods for  making valid   inferences from data that are measured with error. The fact that  errors of   measurement occur in many areas of science (survey methodology,  environmental   studies, epidemiology, medicine for example) means that the  research is widely   applicable. The research uses novel  computer simulation methods  to simulate the   effects of measurement error on a data set and on the conclusions  drawn from   such data. The information obtained from the simulation studies  is then used to   make valid inferences from the data that are free from the  biasing effects of   measurement error."
"9504478","Mathematical Sciences:  Bayesian Nonparametric Methods and  Model Selection","DMS","STATISTICS","06/15/1995","06/07/1995","Purushottam Laud","WI","Medical College of Wisconsin","Standard Grant","James E. Gentle","05/31/1998","$60,000.00","","laud@hp05.biostat.mcw.edu","8701 WATERTOWN PLANK RD","MILWAUKEE","WI","532263548","4149558563","MPS","1269","9146, 9161, AMPP, MANU","$0.00"," Proposal:  DMS 9504478  PI:  Purushottam Laud  Institution:  Medical College of Wisconsin  Title:  Bayesian Nonparametric Methods and Model Selection             ABSTRACT:                                     This research develops new statistical methods for Bayesian nonparametric  models, with special attention to problems arising in survival analysis. In  particular, the investigation focuses on (i) Beta process models for the  cumulative hazard function and some attendant Markov chain models, (ii) hazard  rate models using the Extended Gamma process, (iii) Cox's proportional hazards  regression model with priors on hazard rates and cumulative hazards, (iv) the  problem of variable selection in Cox regression, and (v) unimodal distribution  models. In all cases, emphasis is placed on priors for which implementable  computational algorithms can be developed.        With the recent developments in hardware and software, high-speed computer  simulation technology has made it feasible to surmount the computational   problems in Bayesian statistical methods which have been advocated on   foundational grounds by many leading theoreticians for decades. The research  in this project concentrates on furthering such methods in the area of survival  analysis. Originating with applications to product reliability in industry and  to patient survival after intervention in medicine, statistical analyses of   survival data have proved valuable in evaluating new materials, new  manufacturing methods and new medical procedures. The investigator studies and  implements refined techniques for the analysis of data arising in such fields  of application."
"9510348","Mathematical Sciences:  Bayesian Modeling and Inference for Time Series with Stable Innovations","DMS","STATISTICS","06/15/1995","06/02/1995","Nalini Ravishanker","CT","University of Connecticut","Standard Grant","James E. Gentle","05/31/1996","$18,000.00","","nalini.ravishanker@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","0000, 9222, OTHR","$0.00","9510348 Ravishanker  Abstract      The investigator studies Bayesian modeling and inference for time series with infinite variance stable innovations, addressing methodological problems as well as applications to real world data. The justification for the study of infinite variance stable processes stems from empirical evidence for its usefulness in several application areas such as astronomy, economics, engineering, finance and physics. The area of modeling stable processes has several open and challenging problems that are addressed. Available classical methods of estimation and inference do not simultaneously estimate the parameters defining the stable process and the parameters of the time series model. The investigator develops new methodology for modeling data generated by autoregressive fractionally integrated moving average processes with stable innovations that characterize long memory and short memory behavior in 'infinite variance' time series. Inference and prediction are implemented using sampling- based Bayesian techniques through Markov chain Monte Carlo algorithms to generate samples from the target posterior distribution. For stable processes, the form of the likelihood does not, in general, admit a closed analytical form. Hence expressions for the complete conditional distributions of the parameters in terms of the characteristic function of the stable process are combined with the Gibbs sampling algorithm or its variants to generate samples from the required posterior by approximating it by a suitable proposal density. Additionally, the role of algorithms that are useful in generating samples from stable processes directly is studied, incorporating this into the Bayesian framework. Various marginal and joint posterior distributions as well as summary features of these distributions are analyzed, as well as a characterization of predictive distributions that permit model choice and forecasting.        The investigator studies modeling and forecasting for time series data assumi ng that the data can take on more extreme values than would usually be the case. There is considerable empirical evidence for this behavior in diverse areas of application such as telecommunications, hydrology, physics, economics and finance and for modeling quantities such as gravitational fields of stars, temperature distributions in nuclear reactors, stresses in crystalline lattices, annual rainfall, stock prices etc. Currently available methods for modeling and forecasting are few and inefficient . The investigator develops new innovative methodology for modeling under the assumption that the data are generated by a popular and useful time series process and by incorporating prior information into the modeling as well."
"9501570","Bayesian Methods for Multiple Sequence Alignment and RelatedStatistical Problems","DMS","STATISTICS","09/01/1995","06/27/1997","Jun Liu","CA","Stanford University","Continuing Grant","Joseph M. Rosenblatt","08/31/1999","$63,000.00","","jliu@stat.harvard.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, 1045, 9179, OTHR, SMET","$0.00","9501570  Liu  Abstract (technical)    Multiple sequence alignment is crucial in research on the  structure and function of gene   products, through promoting the detection and description of  sequence motifs;  aiding   efforts at protein modeling, structure prediction and  engineering; and  shedding light on   molecular evolution and phylogeny construction in molecular  systematics.  We recently   presented new local  alignment algorithms (Lawrence et al. 1993,  1994) based on Bayesian   modeling and  Gibbs sampling. In this project, we propose to  develop several extensions   of our original Bayesian model  to accommodate various practical  situations, and to   investigate several statistical problems arisen from our study of  the alignment problem. On   the applied side, we wish to accomplish the following: (1) relax  the existence assumption   and develop Bayesian models to simultaneously detect and align  the common  multiple   motifs in the sequences; (2) combine the idea of Markovian  structure modeling of   sequence data (e.g., hidden Markov Model) with the block-based  method of Lawrence et.   al. (1993) to enhance the alignment of multiple motifs; (3)  modify and apply  the   methodology developed previously and in this project to several  other important biological   problems: the detection of subtle sequence signals in noncoding  DNA region; the   prediction of protein secondary structures; and the combination  of threading and alignment.   On the theoretical  side, we attempt to (4) understand the nature  and potential of predictive   updating in connection with the bootstrap,  the Bayesian  bootstrap, and general   nonparametric problems; (5) develop model selection criteria that  are useful for   determining pattern width in alignment problems; (6) study the  influence of prior   specifications  on the results and frequency properties of these  results in  Bayesian   classification problems; and finally, (7) study convergence  properties of several sampling   algorithms.    During  the award period, I will develop my own styles on teaching  introductory probability   and statistics courses with an emphasis on statistical thinking,  and get involved in a   program designed to attract talented undergraduates into our  field. I will also make an effort   to develop new graduate level courses on  current research areas  such as image processing,   missing data, Markov Chain Monte Carlo, genetics and biology. My  guideline for directing   graduate students is: independence, connections, and creativity.            9501570  Liu    Abstract (nontechnical)    A wealth of data concerning life's basic molecules, proteins and  nucleic acids, has emerged   from the biotechnology revolution. The human genome project has  accelerated the growth   of these data. Multiple observations of homologous protein or  nucleic acid sequences from   different organisms are often available.  However, since  mutations and sequence errors   misalign these data, multiple sequence alignment has become an  essential and  valuable    tool for  understanding the structures and functions of these  molecule, aiding efforts at   protein modeling,  structure prediction and engineering; and   shedding light on molecular   evolution and phylogeny construction in molecular systematics.  Traditional algorithms for   finding such alignment are either too computationally expensive  so as to limit their   applications or too heuristic so that the sensitivity to subtle  patterns is lost.  We recently   presented new local  alignment algorithms (Lawrence et al. 1993,  1994) based on a    probabilistic model of the sequences and a  stochastic simulation  technique  called the   Gibbs sampler. In this project, we propose to develop several  extensions of our original   probabilistic  model  to accommodate various practical  situations, and to investigate several   statistical problems arisen from our study of the alignment  problem. On the applied side,   we wish to accomplish the following: (1) relax the existence  assum ption and develop    more general mixture  models to simultaneously detect and align   common  patterns in   multiple homologous biological sequences; (2) investigate the  connection between our   method and  other successful approaches such as the hidden Markov  modeling; (3) modify   and apply  the methodology developed previously and in this  project to several other   important biological problems: the detection of subtle sequence  signals in noncoding DNA   region; the prediction of protein secondary structures; and the  combination of threading and   alignment. On the theoretical  side, we attempt to (4) understand  the nature and potential   of the  predictive updating, a new  iterative simulation method,   in connection with the   bootstrap,  the Bayesian bootstrap, and general nonparametric  problems; (5) develop model   selection criteria that are useful for determining pattern width  in alignment problems; (6)   study the influence of prior specifications  on the final  alignment results; and finally, (7)   study convergence properties of several stochastic simulation   algorithms.     During the award period, I will develop my own styles on teaching  introductory probability   and statistics courses with an emphasis on statistical thinking,  and get involved in a   program designed to attract talented undergraduates into our  field. I will also make an effort   to develop new graduate level courses on  current research areas  such as image processing,   missing data, Markov Chain Monte Carlo, genetics and biology. My  guideline for directing   graduate students is: independence, connections, and creativity."
"9510516","Mathematical Sciences:  Exact Confidence Regions for Multi- dimensional Contingency Tables","DMS","STATISTICS","07/01/1995","06/22/1995","Laura Lazzeroni","CA","Stanford University","Standard Grant","James E. Gentle","12/31/1996","$18,000.00","","laura@playfair.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, 9221, OTHR","$0.00","9510516 Lazzeroni  Abstract      Hardy-Weinberg disequilibrium and linkage disequilibrium are important concepts in population genetics.  In practice, estimation of linkage disequilibrium in haplotype data is equivalent to estimation of the interactions in a large, sparse, multidimensional contingency table. Simultaneous estimation of Hardy-Weinberg and linkage disequilibrium on multilocus genotype data introduces the additional complications of missing information and symmetry constraints on marginal probabilities.  To avoid large-sample approximations that can be unreliable for these data, one approach is to use ""exact"" statistical methods that condition inference on observed marginal totals.  Unfortunately, computing exact confidence intervals and regions is presently infeasible because of the large number of tables consistent with the marginal totals of an observed table.  The principal investigator designs and studies methods for exact inference for multidimensional contingency tables. These methods employ Markov chain Monte Carlo algorithms to simulate the conditional distributions required to construct exact confidence regions for the interaction parameters of a table.  Topics addressed by this research include:  development of useful statistical models of genetic disequilibrium; design, optimization, and convergence properties of the algorithms; and statistical properties of the resulting estimates.  Genetic disequilibrium can point to the association of particular alleles with an increased risk of disease, reveal population substructure, or serve as a guide in positional cloning.  This research provides geneticists with tools for estimating the degree and form of genetic disequilibrium evidenced by a set of data, and to evaluate the precision of those estimates.  The methods will also be useful for estimating interactions in large, sparse, multidimensional contingency tables that arise in other settings."
"9505151","Mathematical Sciences:  Adaptive Estimation:  New Tools,    New Settings","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS, SIGNAL PROCESSING SYS PROGRAM","07/15/1995","06/17/1999","Iain Johnstone","CA","Stanford University","Continuing Grant","Joseph M. Rosenblatt","06/30/2001","$950,000.00","David Donoho","imj@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269, 1271, 4720","9218, 9263, HPCC","$0.00","Proposal: DMS  95- 05151  PI(s):   Iain Johnstone - 1981, David Donoho - 1984        Institution: Stanford     Title:   Adaptive Estimation, New Tools, New Settings         Abstract:     The research develops fundamental tools in theoretical statistics to aid  understanding of such adaptive procedures, and shows how to tune them   so they are noise-cognizant and stable. Underlying the approach are  a) the idea of oracles, which know perfectly well how to adapt representations   ideally,  b) the idea that the goal of adaptation in the presence of noisy data is to   quantify how closely realizable procedures (which do not have privileged   information about the object) can mimic an oracle,  and c) the design of   procedures coming as close as possible to the oracle.  The project  also develops  methods for comparing different adaptation schemes by comparing  oracles of different kinds, for example time-frequency oracles  and time-scale oracles.  This is an outgrowth of our earlier   results on wavelets, where this approach was used to show that wavelets  have a property of being nearly-ideally spatially adaptive.  In addition a  computational environment is being developed for   implementing and systematically testing such approaches.     As a further outgrowth of the proposers' earlier work on wavelets,  the project studies a number of improvements and extensions  of wavelet shrinkage, for example in the directions of classification,  confidence bands, correlated data and selection of orthogonal bases.    This research seeks to develop statistical theory  and computational tools in the general area of adaptive methods of  representing and analyzing signals, images and other objects.  On the one hand, this project is prompted by the extremely  high interest in adaptation on the part of people working  in fields of signal processing, image processing,   time-frequency analysis, speech processing analysis.  New signal representations appear in these fields almost daily,  along with new principles fo r selecting representations.  Such representations are useful for data compression, which is not  the main interest here; but they are also useful for noise removal and   signal interpretation, which are important areas for statisticians to consider.  On the other hand, this project is prompted by the goal  of developing statistical theory which can give clear understanding  of such adaptive schemes. Some of the most highly adaptive schemes  being suggested in signal processing pose a real challenge  to traditional statistical thinking.  For example, some adaptation schemes   search (either explicitly or implicitly) through thousands or millions of  representations of a signal in order to arrive at their final result.  A statistician,  when thinking about applying such methods to a noisy signal, is by training   forced to ask to what extent the result merely reflects the  effects of snooping through noise, detecting pseudo-structure which actually  due to noise, and due to the vigorous search.  The research seeks to develop  fundamental tools in theoretical statistics to aid understanding of  such adaptive procedures, and shows how to tune them so they are  noise-cognizant and stable.  This project may have two spin-offs.  First,  some of the results may be stimulating and/or useful to  the community of ``inventors of adaptive procedures'' in  signal, image, speech, and time/frequency, and related communities.  Second, the theoretical work may stimulate statisticians to take  more interest in making further contributions in such directions."
"9596096","Mathematical Sciences:  Sequential Imputations and Gibbs    Sampling:  Combinations, Comparisons, and Applications","DMS","STATISTICS","01/15/1995","03/06/1995","Jun Liu","CA","Stanford University","Standard Grant","James E. Gentle","07/31/1997","$43,098.00","","jliu@stat.harvard.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00",""
"9504495","Mathematical Sciences:  Flexible Regression and             Classification","DMS","STATISTICS","07/01/1995","08/06/1997","Trevor Hastie","CA","Stanford University","Continuing Grant","James E. Gentle","07/31/1998","$225,000.00","","hastie@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","9216, HPCC","$0.00","Proposal:  DMS 9504495  PI:  Trevor Hastie  Institution:  Stanford University  Title:  Flexible Regression and Classification    Abstract:      The research concerns several research directions with a  common theme: to push widely accepted but limited statistical tools in  more adventurous directions, while retaining some of their attractive  features, such as model interpretability.  Specifically, the research  involves the development of: a)  nonparametric extensions of logistic  regression for multiclass responses, including additive,  projection pursuit and basis expansion techniques, as well as rank  reduced models similar to Fisher's LDA;  b)  a  new adaptive algorithm  for basis selection, similar to  Friedman's MARS model, which uses a  natural penalized criterion to simultaneously select variables and  shrinks their coefficients; c) a technique for locally adapting the  nearest neighbor distance metric to combat the curse of  dimensionality.      Many important problems in data analysis and modeling focus on  prediction. Some important examples include   computer assisted diagnosis of disease (e.g. reading  digital mammograms), heart disease risk assessment, automatic reading  of handwritten digits (e.g. zip-codes on envelopes), speech  recognition, to name a few. This  research is about enriching the  current toolbox of well established statistical models in a natural  way to  address some of these more complex scenarios. Often new  exotic techniques, such as neural networks, are ``black boxes'' that  appear to produce good results, but do not provide the analyst with an  interpretable model, diagnostics or similar feedback to give them  confidence that the box has produced sensible results.     Statistics can play an active role in these important prediction and data  analysis problems through the development  competitive and defensible   models.  This research does just that by   creating a blend between the well understood classical techniques and  the new techniques that allow for  model exploration."
"9504379","Statistical Theory and Methodology","DMS","PROBABILITY, STATISTICS, COMPUTATIONAL MATHEMATICS","08/01/1995","03/24/1999","Bradley Efron","CA","Stanford University","Continuing Grant","Joseph M. Rosenblatt","07/31/2000","$650,000.00","","brad@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1263, 1269, 1271","0000, 9216, 9263, HPCC, OTHR","$0.00","Proposal:  DMS 95-04379  PI:  Bradley Efron  Institution:  Stanford University  Title:   Statistical Theory and Methodology'      ABSTRACT:               The  main research areas  investigated under this award  include:   (1) Exact methods for large, sparse contingency tables based  on Markov Chain Monte Carlo techniques; (2) Improvements on the  accuracy  of classical normal and Edgeworth expansions, based on Stein's  method;  (3) The use of specially designed exponential families for  density estimation  and empirical Bayes situations; (4) The application of  bootstrap-based techniques  for confidence intervals, likelihood analysis, and  bias-correction; and (5)  Length and volume calculations based on Hotelling's tube formula  for routine  use in simultaneous testing situations.            The long-term goal of this research is  the development  of  promising theoretical ideas into methodologies of direct value to  applied  statisticians and their scientific clients. This form of  technology transfer  aims to take advantage of current developments in the range and  power of  new statistical methodology. The ideas here fall into the general  category  of ""computer-intensive methods"", which are methods that take  advantage  of modern computer power to free statistical analysis from the  bounds of  classical mathematical tractability. The research does not banish  mathematics, but uses mathematics to produce statistical tools  that can work  automatically on a wide variety of problems. The  computer-intensive part  comes in at the level of the statistical consumer, where  computation is  substituted for the usual normal theory approximations and  routine but difficult  mathematical computations."
"9504891","Mathematical Sciences:  Asymptotic Methods for Order        Restricted Inference in Survival Analysis","DMS","STATISTICS","07/15/1995","04/02/1996","Jens Praestgaard","IA","University of Iowa","Standard Grant","James E. Gentle","06/30/1998","$30,000.00","","","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1269","9183, BIOT","$0.00","Proposal:  DMS 9504891  PI:  Jens Praestgaard  Institution:  University of Iowa  Title:  Asymptotic Methods for Order Restricted Inference in Survival Analysis    ABSTRACT:    The purpose of the research is to develop large-sample theory for   inference from survival data when the unknown parameters exhibit  order restrictions. Such restrictions occur frequently in survival   analysis. For example, if a clinical trial involves administering   a drug in increasing doses to the cohorts under study, then the   survival curves computed for each group should be ordered according to  dose level. In linkage analysis, for the purpose of   mapping a quantitative genetic trait, it may be known that the trait under  study  is more prevalent for one genotype than for others.    Although appropriate estimators in these settings   are known, their distributional properties have not been studied.   Thus, order restricted methods are often not used when it would   potentially help the data analysis.  For estimation purposes it would  give estimators with higher degree of accuracy.   For testing purposes it would give higher power to be able  to consider order restricted alternatives.        The research centers on investigating statistical methods in the   presence of order restrictions. These restrictions occur often in   biotechnological problems.  For instance, in a clinical trial to  determine drug efficacy, it is often the case that the drug is   administered in increasing doses. In genetical linkage analysis    it is often known that a certain genotype lowers the time until the  onset of a disease more than do other possible genotypes. If the   interest is on determining the drug efficacy (in the first case)  or finding out where the gene is located (second case), then the   statistical methods, which separate the noise in the data from the   signal, must incorporate these known orderings. The investigators  study ways of incorporating such information. They believe that   the resulting statistical method ology will be more appropriate   than present methods for dealing  with biotechnological data   where such orderings are present."
"9504798","Mathematical Sciences:  Nonlinear Modeling in Continuous    Time, Delayed Autoregressive Processes, and Chaos","DMS","STATISTICS","07/01/1995","07/03/1995","Kung-Sik Chan","IA","University of Iowa","Standard Grant","James E. Gentle","06/30/1998","$81,000.00","Osnat Stramer","kchan@stat.uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1269","0000, OTHR","$0.00","Proposal:  DMS 9504798  PI(s):  Kung-Sik Chan and Osnat Stramer  Institution:  University of Iowa  Title:  Nonlinear Modeling in Continous Time, Delayed Autoregressive    Processes, and Chaos       Abstract:    In analyzing  nonlinear time series data sampled irregularly, it  is natural to consider non-linear continuous-time models for the  (unobserved) underlying continuous-time process.  Owing to  the intractability of the likelihood function for such kind of nonlinear  models, the existing estimation methods in the literature  are somewhat ad hoc. This research contributes to the statistical  inference for non-linear continuous-time modeling based on (possibly  irregularly sampled) discrete-time observations. This research consists of four parts.  In part I, the investigators  study  maximum likelihood estimation for the general class of  nonlinear  continuous-time  autoregressive process, abbreviated by NLCAR(p).  The investigators derive a new expression  for the likelihood function   which can be evaluated via simulation.  The investigators   implement the simulation-based maximum likelihood estimation  procedure for some classes of NLCAR(p) models.  The investigators then study the sampling properties of the maximum   likelihood estimator so obtained. In part II,  the investigators    apply the Lagrange Multiplier tests for detecting     non-linearity and also those for other diagnostic purposes, and   study their large sample properties.  In part III, the investigators   extend an approach relating ergodicity and stability  to continuous-time models.  In part IV, the investigators study delayed continuous-time non-linear  autoregressive processes whose state space is an infinite-dimensional Hilbert space.  They also study the  recurrence (stationarity) properties  and the   statistical inference for the delayed non-linear autoregressive   processes.      In this research,  the investigators study new statistical methods useful   for analyzing data which are taken over possibly unequal ti me intervals.    Such kind of data is known as irregularly sampled  time series, and occurs frequently in diverse areas including   business forecasting, environmental statistics, medical statistics, physical  and engineering science. The investigators study computer-intensive   methods which provide the  relevant tools for studying   and testing   for  the nonlinear structure of irregularly sampled time series data.   The investigators also develop some mathematical statistics and  probability theory fundamental to the understanding of the new methods.           LEVEL OF EFFORT STATEMENT    At the recommended level of support, the PI will make every attempt to   meet the original scope and level of effort of the project."
"9502157","Mathematical Sciences:  Second International Workshop       on Bayesian Robustness; May, 1995; Rimini, Italy","DMS","STATISTICS","04/01/1995","03/29/1995","Larry Wasserman","PA","Carnegie-Mellon University","Standard Grant","James E. Gentle","03/31/1996","$10,000.00","","larry@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, OTHR","$0.00","The purpose of this conference is to bring together experts in the area of robust Bayesian inference to discuss applications, recent methodological developments and theory. Of particular interest are practical methods for implementing the theory such as those that exploit recent advances in statistical computing. Among the topics to be discussed at the workshop are: robust Bayesian methods in biostatistics, local sensitivity, hierarchical models, dynamic graphics, Bayes factros, density estimation and time series.  The term ``Bayesian statistics'' refers to a class of data analysis techniques based on probability theory .  These techniques are very general and have been successfully used in such diverse areas as quality  control, engineering, psychology, medicine, environmental sciences and astrophysics. But applications  of Bayesian methods to science and technology have been hindered since Bayesian analysis often  requires the use  of certain simplifying assumptions. ``Bayesian robustness'' is concerned with  developing techniques for understanding how scientific conclusions depend on the assumptions that are  used by statisticians when analyzing the data. Bayesian robustness is crucial for the successful  development of practical Bayesian data analysis methods. It is a relatively new, quickly growing area  and hence it difficult for researchers -- especially young researchers -- to meet and stay in touch  with the latest ideas. The objective of the  workshop is to draw together researchers to discuss the  latest advances in this field.       LEVEL OF EFFORT STATEMENT  The recommended level of support for this conference, though slightly lower than what we had hoped for, is still substantial and, together  with the funds provided by the Italian government, we expect to produce a high quality workshop.  Indeed, preparations for the conference are in progress and many top researchers have already  indicated their intention to participate. Furthermore, the Institute of Mathematical Statistics has  indicated that it is likely they will publish the proceedings in their Lecture Note series. We will  make every effort to ensure a successful conference."
"9505192","Workshop on Model Uncertainty and Model Robustness-         July 14-15, 1995","DMS","STATISTICS","07/01/1995","03/29/1995","Giovanni Parmigiani","NC","Duke University","Standard Grant","James E. Gentle","06/30/1996","$10,000.00","","gp@jimmy.harvard.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","1303, 9187, EGCH, ENVI, GLCH","$0.00","Proposal:  DMS 95-05192  PI:  Giovanni Parmigiani  Institution:  Duke University  Title: WORKSHOP ON MODEL UNCERTAINTY AND MODEL ROBUSTNESS                              PROJECT ABSTRACT    In recent years, advances in statistical methodology and computing have made available powerful modeling tools in a variety of areas. Along with the added modeling flexibility, increasing attention is being directed to assessing the consequences of modeling assumptions on results. Debates on the effect of modeling assumptions on crucial scientific and pmlicy prediction, such as global warming and the health impact of toxic waste, have reached the mass media.  This award supports travel to a workshop in Bath, UK, that will  discuss these issues.    Travel to a  workshop on model uncertainty and model robustness---the quantitative exploration of the mapping from assumptions to conclusions will be supported.  The workshop will  blend methodology and case studies.  The workshop has the potential to significantly advance this field of statistical research and encourage dissemination of an important class of modeling techniques to practitioners. The goals of the workshop are: a) to help eliciting current issues and methods from a host of different application areas and disciplines; b) to favor wider utilization of worthy practical approaches and solutions developed in specific fields; c) to advance understanding of relative merits of existing tools ad approaches, and d) to point to directions for future methodological developments.  The workshop is international, which is a reflection on the composition of the research community in this area.   Participants supported with the award represents a balance between senior and junior researchers as well as women and minorities."
"9424490","Mathematical Sciences:  Workshop on Statistical Mixture     Modelling","DMS","STATISTICS, WESTERN EUROPE PROGRAM","09/01/1995","05/15/1995","Mike West","NC","Duke University","Standard Grant","James E. Gentle","08/31/1996","$20,000.00","","Mike.West@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269, 5980","0000, 5914, OTHR","$0.00","   PROPOSAL:  DMS 94-24490  PI:   Mike West  TITLE:     WORKSHOP ON STATISTICAL MIXTURE MODELLING      ABSTRACT:  The Workshop provides a forum for reviewing and publicizing widely dispersed research activities in mixture modelling, to stimulate fertilization of theoretical, methodological and computational research directions for the near future, and, significantly, to focus attention on the burgeoning arena of applied problems in complex stochastic systems that are inherently structured in mixture terms. The Workshop will heavily concentrate in topical problems with strong cross-disciplinary and computational components, featuring mixture problems in areas such as density estimation, regression and time series; mixtures in statistical image modelling and analysis, neural networks and signal processing; graphical models and networks; stochastic simulation for mixture analysis; clustering and classification problems; model selection and combination; alternative approaches to inference in mixtures; latent variables and incomplete data problems; and applications of mixtures in various scientific areas.    The Workshop is held in recognition of the recent growth and development in the theory and, in particular, applications of statistical methods based on mixtures of distributions. Mixture models play key roles in many complex modelling and inference problems in science, engineering and social sciences, and advances in computational statistical technology have, and continue to, promote their wider use. This  Workshop provides a forum for reviewing and publicizing widely dispersed research activities in mixture modelling, stimulating fertilization of theoretical, methodological and computational research directions for the near future, and focusing attention on the wide variety of significant applied problems in complex stochastic systems that are inherently structured in mixture terms. The workshop will bring together senior researchers, new researchers and students from various backgrounds to prom ote exchange and interactions on the frontiers of statistical mixture modelling, to highlight the development of statistical technology across these fields, to promote interchanges between researchers in various applied fields, and to highlight the utility of mixture models in diverse fields such as genetics, astronomy and physics, the neuro-sciences, and others."
"9505196","Mathematical Sciences:  Model Selection for Mixtures","DMS","STATISTICS","09/01/1995","06/15/1995","Dominique Haughton","MA","Bentley University","Standard Grant","James E. Gentle","08/31/1997","$30,000.00","","dhaughto@bentley","175 FOREST ST","WALTHAM","MA","024524705","7818912660","MPS","1269","9178, 9229, SMET","$0.00"," Proposal:  DMS 9505196  PI:  Dominique Haughton  Institution:  Bentley College  Title:  Model Selection for Mixtures      Abstract    This research is concerned with the problem of selecting a  suitable model when the competing models involve mixtures of  distributions.  Criteria for model selection among mixtures and  convolutions of exponential distributions are proposed and shown  to be asymptotically close to Bayes procedures.  The performance  of the criteria are compared to that of complexity based model  selection criteria.  The methodology is then applied to data from  ethological science.  The case of mixtures of multiple regression  models and logistic regression models is also considered and  applied to the problem of testing for son preference in Vietnam,  using data from the Vietnam Living Standard Survey.      In many modeling situations, a single model is not suitable for  all the data.  For example, when investigating whether there is  significant statistical evidence of son preference (preference for  boys over girls) in countries such as Vietnam, the problem is   complicated by the fact that some families display son preference  and some do not.  In ethological science, an area of behavioral   science concerned with detailed observation and recording of the  behavior of one or more individuals, interesting questions arise  as to whether a behavior can be split into consecutive activities  or is a mixture of several activities.  These problems can be   addressed by modeling with statistical mixtures."
"9505114","Mathematical Sciences:  Robustness and Scale in Spatial     Applications of Markov Chain Monte Carlo for Bayesian       Inference","DMS","STATISTICS","07/01/1995","06/30/1995","David Higdon","NC","Duke University","Standard Grant","James E. Gentle","06/30/1998","$40,000.00","","dhigdon@bi.vt.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, OTHR","$0.00"," Proposal:  DMS9505114   PI:  Dave Higdon   Institution:  Duke University                                   Title:  Robustness and Scale in Spatial Applications of Markov Chain Monte Carlo             for Bayesian Inference       Abstract:    The proposed research develops flexible prior distributions  for Bayesian inference for spatial systems.  From the modeling  standpoint, spatial priors that are mixtures over simple  Markov random fields (MRF's) are developed in two distinct  contexts.  In the first, mixtures will be constructed to allow  robust inference when anomalies, and heterogeneity  are present in the spatial process.  In the second context, mixtures over different scales  will be considered, so that the scale of the MRF can  be rigorously treated as a parameter when making inference.  Only very recent advances in Markov chain  Monte Carlo (MCMC) make inference from such models possible.  Applications in agriculture, imaging and environmental monitoring  will be considered.    The proposed research will develop statistical methodologies for  analyzing spatial data which combines information at various scales.    Development of spatial models suitable for agriculture,  imaging (eg. remote sensing and medical imaging), environmental  monitoring and assessing environmental trends are main goals  of this research.  Current statistical methods typically require  restrictive assumptions which often make formal analyses   in these areas difficult or impossible.  This methodology will relax these  assumptions.  A primary motivation for combining information from various   scales is for dealing with geographical information systems used  for environmental monitoring.  Such systems invariably contain data   at varying scales which can be incorporated in the analysis."
"9504918","Mathematical Sciences:  Weighted Empiricals in Regression   with Survival Data","DMS","STATISTICS","07/01/1995","07/03/1995","Song Yang","TX","Texas Tech University","Standard Grant","James E. Gentle","12/31/1998","$48,000.00","","yang@math.ttu.edu","2500 BROADWAY","LUBBOCK","TX","79409","8067423884","MPS","1269","0000, OTHR","$0.00"," Proposal:  DMS 9504918   PI:  Song Yang  Institution:  Texas A&M  Title:  Weighted Empiricals in Regression with Survival Data     Abstract:      This research involves various semiparametric regression models,   with right, double or interval censoring. These models include   the commonly used multiplicative hazard model and the log   transform of accelerated life model, the relatively new frailty   model, and the rarely studied multiplicative odds ratio model. In   all these models, the covariates are allowed  to be   time-dependent.  The weighted Nelson-Aalen hazard function and   Kaplan-Meier survival function are defined via weighted   empirical processes and some self consistency equations.   Various estimating equations and estimating functions are   established using these weighted functions. Some classes of the   regression estimators, including the rank, minimum distance and   M-estimators, are then  obtained and analyzed.         The  efficiency and robustness of the estimators   and some goodness-of-fit tests of the models is studies, as well as the   practical issues on computing these estimators.     The models and related statistical problems studied in this   research have applications in areas such as medical follow-up   studies and industrial reliability tests. In situations involved   in those areas, the available data are often incomplete due to   various censoring.  The censoring is attributed to  the conditions   of the patients or machine operating systems and is  influenced by   factors such as treatment, age, stress, and  load. These influences can   be formulated by some of the models in this research. Through studies   of these models and related statistical problems, the conditions of the   patients or operating systems can be better understood,   predicted, and controlled.  Compared with the existing research  literature, the new methods in this research  will require   different or more sophisticated computing algorithms.   Implementations of these methods provide  a background to, and are   facilitated by, efficient coding and high performance computing."
"9504485","Mathematical Sciences:  Inverse Estimation Problems","DMS","STATISTICS, SIGNAL PROCESSING SYS PROGRAM","07/01/1995","06/12/1995","Frits Ruymgaart","TX","Texas Tech University","Standard Grant","Joseph M. Rosenblatt","06/30/1999","$105,000.00","","ruymg@math.ttu.edu","2500 BROADWAY","LUBBOCK","TX","79409","8067423884","MPS","1269, 4720","0000, OTHR","$0.00","Proposal:  DMS 9504485  PI:  Frits Ruymgaart  Institution:  Texas Tech  Title: Inverse Estimation Problems    Abstract:      Inverse estimation is concerned with indirect curve estimation:  the  curve of interest is to be recovered from observations, subject to  random blur, on a transformation of the curve.  This class of problems  is very broad and formally contains ordinary, direct, curve  estimation as a special case.  Many interesting examples come from  physics, signal processing, statistics, and applied mathematics.  The  estimation problem can essentially be solved by inverting the  transformation involved.  Since this inverse is not in general  continuous, the problem is typically ill-posed and regularization of  the inverse  is required.  Two important questions are how  restriction, due to limitations in time or space, of the  transformation relate to the unrestricted transformation, and how  recovery of irregular curves can be modified so as to avoid the Gibbs  phenomenon.  Along with a comprehensive study of deconvolution on  locally compact Abelian groups, this research addresses these  questions.  Also, the study of some examples of  practical interest are explored.    Inverse estimation arises when an object, e.g.,     part of the human brain, can only be indirectly observed,  and recovery of information about the object  is required from the indirect measurements.      Indirect measurement techniques, such as those used  in medical imagery, are attractive  because they are noninvasive.  Perfect reconstruction of the image would, in  principle, be possible if an unlimited number of uncorrupted  measurements were available.  In practice, however, one can only  obtain finitely many data that, moreover, are corrupted by  measurement errors.  Here statistical considerations and techniques  play a role in the image reconstruction process.  Aspects  addressed in this  research include how the reconstruction  depends on the specific way in which   the indirect data are collected, and al so how it should be tuned to prior  information about the object to be recovered."
"9504924","Mathematical Sciences:  Statistical Theory and Methods      for Errors in Variables Regression and other Multivariate   Inference Problems","DMS","STATISTICS","07/15/1995","06/16/1997","Leon Gleser","PA","University of Pittsburgh","Continuing Grant","Gabor Szekely","06/30/1999","$135,000.00","","gleser@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","0000, 9169, EGCH, OTHR","$0.00","Proposal:  DMS 9504924  PI:  Leon Gleser  Institution:  U. of Pittsburgh  Title:  STATISTICAL THEORY AND METHODS FOR             ERRORS-IN-VARIABLES REGRESSION AND            OTHER MULTIVARIATE INFERENCE PROBLEMS       Abstract:         This research compares the mean squared error risks of various       estimators of regression slopes that correct for bias due to       measurement error in the predictor variables.  Included in these       comparisons is the naive (uncorrected) least squares estimator that       ignores measurement error.  Emphasis is placed on bias correction that       makes use of knowledge about the repeatability (reliability) of the       vector of measurements on the predictors to ""correct the slope       estimates for attenuation.""  Various ways of obtaining and analyzing       information concerning the reliability of the vector of measured       predictors are considered, including the use of reliability studies of       individual components and/or subvectors of the predictor vector and       the elicitation and use of expert opinion.  These separate sources of       information are combined to yield frequentist or Bayes point       estimators of the reliability matrix of the measured predictors; this       matrix is used to correct least squares regression slopes for       attenuation.  Methods for forming confidence (or credible) intervals       for slopes, and also for components of the reliability matrix are       derived and compared.  Computational algorithms for finding       likelihoods and posterior distributions in both these problems, and       also in multivariate growth curve and seemingly unrelated regression       models, are written and tested on real and simulated data.     Much of scientific research concerning complex systems or organisms is       concerned with relationships among quantities .  For example, the       number of successfully hatched eggs of a certain species of marshland       wildfowl might be related to the concentrations of one or  more pesticides       in the marsh. When the rates of response or change of a variable (such as        the number of successfully hatched eggs) to changes in other predictor        variables (such as various pesticide concentrations) are estimated by       conventional statistical methods, it is assumed that the predictor        variables are measured exactly. Unfortunately, environmental, biological        or psychological variables rarely are measured exactly.  If predictor       variables are measured with error, the conventional estimates of response        rates are biased. To correct for such bias, errors-in-variables regression       models assume that each measurement is the sums of the true value of the        quantity being measured and a random error.  In this case, the proportion        of the unit-to-unit variability of a measurement that is due to the       variability over units of the true value, called the reliability of the       measurement, can be used to correct the bias of the conventional estimates        of response rates. When several variables are simultaneously used as        predictors of another,  more than the reliability of each individual       predictor is  required for this purpose; it is the reliability of the       measurements of  the predictors as a whole, or ensemble, that must be        ascertained. Because the particular collection of predictors may not have       been used before,  information about the reliability of the predictors as       an ensemble must be pieced  together from a variety of sources. The       investigator's research is  concerned with how best to obtain and combine       information from data summaries of prior studies that used some (but not       necessarily all) of the predictor variable, data from the current study        and expert opinion to obtain the required reliability information and        correct bias in the conventional estimates of response rates. Also studied        are ways to determine and summarize the accuracy of  the resulting        estimates. Insights and methodology from this research has broad        applicability to questions of combining information from several small       studies concerning the interrelationships among variables, not all of        which appear in every study. One of the products of the research is        computer software that allows information to be combined from several       studies on the same subjects (or environmental locations) and then       displayed so as to give the relative likelihoods of various statistical       models in the light of the evidence presented by the data itself."
"9420846","Mathematical Sciences:  50th Session of the International   Statistical Institute, Beijing, China, August 21-29, 1995","DMS","STATISTICS","05/15/1995","05/23/1995","Lynne Billard","VA","American Statistical Association","Standard Grant","Stephen M. Samuels","04/30/1996","$20,000.00","","lynne@stat.uga.edu","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","0000, OTHR","$0.00","9420846  Billard    The American Statistical Association (ASA) requests a travel grant for 20 United States participants to attend the 50th Session of the International Statistical Institute (ISI) in Beijing, China from August 21-29, 1995.  The ISI meeting includes the meetings of the Bernoulli Society, International Association for Official Statistics (IAOS), International Association for Statistical Computing (IASC), International Association of Survey Statisticians (IASS), and the International Association for Statistical Education (IASE).  Thus it is an umbrella meeting with sessions of interest for thousands of Statisticians.  The travel grant will provide partial support to defray transportation costs for individuals selected from institutions and non-profit associations.  An emphasis of the award is to encourage and provide the opportunity for younger statisticians to participate in the meeting.  ASA proposes to make twenty travel grants available, with the major emphasis on providing partial travel support to individuals who received their Ph.D. after 1984.  The grant recipients will be primarily engaged in the meeting activities as invited speakers, discussants, organizers, and presenters of contributed papers.  Examples of titles of some of the papers presented in 1993 include, ""Object Oriented Methods in Statistics - Graphical Construction and Editing of Plots"" ""The Strata of Random Mappings,"" ""Bayesian Analysis of Space-Time Evolution of Earthquakes,"" and ""Time Series Applications of Astronomy and Meteorology.""     The American Statistical Association (ASA) requests a travel grant for 20 United States participants to attend the 50th Session of the International Statistical Institute (ISI) in Beijing, China from August 21-29, 1995.  The ISI meeting includes the meetings of the Bernoulli Society, International Association for Official Statistics (IAOS), International Association for Statistical Computing (IASC), International Association of Survey Statisticians (IASS), and the Internat ional Association for Statistical Education (IASE).  Thus it is an umbrella meeting with sessions of interest for thousands of Statisticians.  The travel grant will provide partial support to defray transportation costs for individuals selected from institutions and non-profit associations.  An emphasis of the award is to encourage and provide the opportunity for younger statisticians to participate in the meeting.  ASA proposes to make twenty travel grants available, with the major emphasis on providing partial travel support to individuals who received their Ph.D. after 1984.  These travel grants will not include expenses for housing and meals."
"9505007","Statistical Methods for the Analysis of Functional          Magnetic Resonance Imaging Data","DMS","COMPUTATIONAL NEUROSCIENCE, HUMAN COGNITION & PERCEPTION, BEHAVIORAL NEUROSCIENCE, STATISTICS","07/01/1995","07/19/1995","William Eddy","PA","Carnegie-Mellon University","Standard Grant","James E. Gentle","06/30/1997","$130,000.00","Christopher Genovese","bill@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1162, 1180, 1191, 1269","0000, 9178, 9218, 9251, HPCC, OTHR, SMET","$0.00","Proposal:  DMS 9505007  PI(s):  William Eddy, Chris Genovese  Institution:  Carnegie Mellon University  Title:  Statisitcal Methods for the Analysis of Functional Magnetic Resonance             Imaging Data     Abstract:  This research involves the development of new statistical methods for the  analysis and interpretation of functional Magnetic Resonance Imaging (fMRI)  data.  Such data can be viewed as the realization of a spatio-temporal  process with a very complicated distributional structure.  Models in  current use are grossly simplified for both mathematical and computational  expediency.  The statistical challenges in constructing more realistic  models are difficult and manifold.  Many revolve around understanding the  nature of the noise in the measurements and its effect on successfully  detecting regions of neural activation.  Noise in the data shows  significant spatial and temporal correlations that depend strongly on how  the data are collected.  Outliers are common, and there are strong sources  of systematic variation such as the subject's respiratory and cardiac  cycles.  Variances in the images depend nonlinearly on the means, and the  observed absolute levels of activation tend to shift between sessions  because of subject movement.  Moreover, all of these difficulties occur for  data collected from a single subject; the situation becomes much more  complicated if comparisons across subjects are attempted.  This research  focusses on three general problems in the statistical analysis of fMRI  data: 1. The characterization of the response to an activating stimulus in  the fMRI signal and the use of this information to build more realistic  models and make more precise inferences; 2. The development of robust  procedures for identifying active regions that account for the complexity  of the underlying spatio-temporal process; and 3. The construction of  functional maps within a specified system of the brain (e.g., the visual  system) and the use these maps for making predictiv e inference across  subjects.    Functional Magnetic Resonance Imaging (fMRI) is an exciting new technique  that uses advanced technology to obtain images of the active human brain.  The technique is of particular interest to cognitive neuropsychologists  because of the unique perspective it offers into high-level cognitive  processing in humans: areas of the brain that are activated by a stimulus  or cognitive task ``light up'' in an fMRI image.  This technology will thus  play a critical role in understanding how the brain works; however, before  this potential can be realized, significant statistical challenges in the  interpretation and analysis of fMRI data must be overcome.  For example,  there is substantial uncertainty in the identification of neural activity  from these images and in the attribution of that activity to particular  cognitive processes.  Moreover, there is a need for new methods of making  statistical inferences of scientific interest from these large and complex  sets of data.  This research focusses on three broad aspects of the general  problem: 1. Constructing models for the systematic components of the  process that generates the data, 2. Studying and modeling the properties of  the noise in the measurements so that analysis and inference can be made  more precise, and 3. Developing new methods of inference for addressing  interesting scientific questions with massive sets of data that arise from  measurements over space and time."
"9505461","Mathematical Sciences:  Workshop on Bayesian Statistics     in Science and Technology","DMS","STATISTICS","06/01/1995","06/01/1995","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","James E. Gentle","05/31/1997","$15,000.00","Nozer Singpurwalla, James Hodges, Constantine Gatsonis","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, OTHR","$0.00","Proposal:  DMS 9505461     PIs:  Robert Kass, Constantine Gatsonsi, James Hodges, Nozer   Singpurwalla     Institution:  Carnegie Mellon University     Title:  Bayesian Case Studies Workshop, October 1995           Abstract:          The symposium entitled ``Bayesian Statistics in Science and Technology:   Case  Studies III''   held at Carnegie-Mellon University in Pittsburgh,   Pennsylvania,  includes  extended presentations of applications of Bayesian   methods.  These applications are in  problems  in which the statistician is an   integral member of the research team.   The objectives of the symposium are   to: (1)identify and focus attention on specific implementation and   theoretical problems that hinder applications of Bayesian methods, and to   identify candidate solutions; (ii) provide a forum in which the interplay   between statistical theory and practice will be explored in the context of   concrete research projects; (iii) provide a small-meeting atmosphere within   which junior investigators and graduate students can  explore substantial   Bayesian applications with experienced researchers;  and (iv) produce a    volume containing well-documented case studies and data sets suitable  for   use by researchers, practitioners, educators and students of applied statistics    and other quantitative fields.               Today there are  increasing amounts of  background information available   to  scientists undertaking an investigation.  It is important to utilize   previousknowledge effectively in designing studies and analyzing data.   Bayesian statistical methods are tailored to this purpose.  There have been   many recent advances in Bayesian statistical theory and computation, but   scientific meetings rarely spend  substantial time discussing  successful   applications of these methods to substantive scientific problems. This   symposium  concentrates attention solely on applications  of Bayesian   statistics.  The application areas range from economics to population    biology .  The result will be the elucidation of the interplay between theory   and  practice  and the identification of successful methods that will  point to  important directions for future research.  The interactions and dynamics of   the workshop will be captured and disseminated via video tape and in the   more traditional written  form of a conference proceedings."
"9523602","Mathematical Sciences:  Dealing with Mixed Levels of        Uncertainty","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, INTERNATIONAL SUPPORT","09/01/1995","09/04/1998","M. Granger Morgan","PA","Carnegie-Mellon University","Standard Grant","Joseph M. Rosenblatt","08/31/1999","$240,637.00","","granger.morgan@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1253, 1269, 5419","1317, EGCH","$0.00","This research program revolves around three different but  related projects in discrete choice modeling and estimation.  The common theme is the modeling of consumer choice  behavior, and the estimation of models of consumer behavior  on revealed preference data sets.  With the availability of  electronic scanner panel data, choice models have become  very important for understanding consumer behavior.  For  example, such models may be used to evaluate the impact of  the marketing mix on consumer purchasing behavior, and to  assess the patterns of competition in the market place.  The  three research projects in this study involve the use of  recently developed simulation estimation techniques.  In the  model of the first project, consumer choice probabilities  are high dimensional integrals over unobserved prices and  coupon values, as well as over stochastic preference shocks.  Thus, Monte-Carlo methods must be used to simulate the  likelihood function in the model.  In the second and third  projects, both solution of the dynamic programming problem  and construction of the likelihood function require high  dimensional integrations over stochastic process for prices,  preference shocks, etc.  In these projects we will use Monte-  Carlo methods both to solve consumers' dynamic programming  problems and to simulate the likelihood function."
"9504451","Maximum Likelihood Estimation of Radial Basis Function      Neural Networks for Process Control","DMS","STATISTICS, CONTROL, NETWORKS, & COMP INTE","06/15/1995","06/07/1995","Richard De Veaux","MA","Williams College","Standard Grant","James E. Gentle","05/31/1998","$50,000.00","","deveaux@williams.edu","880 MAIN ST","WILLIAMSTOWN","MA","012672600","4135974352","MPS","1269, 1518","9178, 9229, SMET","$0.00","Radial basis function neural networks provide an attractive model  for high  dimensional nonparametric estimation, particularly for  model-based process  control.  They are faster to train than conventional feedforward  networks  with sigmoidal activation functions , and provide a model  structure better  suited for adaptive control.   Radial basis function neural  networks can be  viewed as a mixture model where the number of components is  chosen  adaptively.  This view allows for statistical analysis and the  extension of current  radial basis function networks.  In particular,  this research  will  develop    models using combinations of local linear models,  invert the  models for  calibration, estimate the local prediction errors,  identify  outliers or  novel data points, adaptively choose the number of basis  functions and  derive algorithms that learn incrementally.     The research extends and improves a certain class of artificial  neural  networks, the radial basis function networks,  for applications  to  (chemical) process control.  Algorithms developed  will  be written so  they can be efficiently updated for use in on-line  process  control applications.   This is a coordinated project between two  investigators, a statistician and a chemical engineer.      A major chemical company, DuPont,  has expressed strong  interest in using these methods and they have supplied data to  test the  methodology of this research ."
"9501926","Mathematical Sciences:  Greedy Growing and its Applications","DMS","PROBABILITY, STATISTICS","07/01/1995","04/24/1997","Andrew Nobel","NC","University of North Carolina at Chapel Hill","Continuing Grant","K Crank","06/30/1999","$72,000.00","","nobel@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1263, 1269","0000, 1045, 9179, OTHR, SMET","$0.00","9501926 Nobel  Binary trees play an important role in the methodology of statistics and engineering.  Classification trees have been applied to variety of statistical problems, ranging from mortality studies to the recognition of functional groups in gene sequences.  Quantization trees have been applied to the compression of medical images and sampled speech.  The problem of designing good classification and quantization trees from finite data sets is usually addressed through the use of greedy growing algorithms.  While the empirical behavior of these algorithms is well understood, there has been little theory to support their use, or to examine their behavior on large data sets.  The proposed research will undertake a systematic study of greedy growing algorithms.  It has three broad objectives: To develop theoretical tools that will provide a means of  rigorously analyzing such algorithms; To apply these tools to the analysis and comparison of existing algorithms; To use these tools, in conjunction with computer simulations, in the design of new algorithms for specific applications. A key feature of the proposed research is that it addresses classification and quantization in the same framework.  Educational activities  will be one of the key responsibilities of the principal investigator during the duration of the grant. The Statistics Department at the University of North Carolina at Chapel Hill  has a strong tradition of graduate and undergraduate education. Maintaining the tradition entails a strong commitment to  teaching, as well as interaction with students, both inside and outside of the classroom. We hope to further the tradition through the development of new courses, which will introduce students to the basic ideas behind the proposed research.  Subject to departmental approval,  graduate level courses in  Statistical Pattern Recognition and Complexity-based Statistical Methods will be developed. A reading course will be designed to encourage advanced graduate students to undertake  superviqed research in the proposed area of study.   Tree-structured methods of data analysis play an important role in statistics and engineering, because they are easy to implement and  lend themselves to ready interpretation. Tree-structured procedures  have been applied to statistical problems ranging from the study of housing prices to the  prediction of heart attacks.   Related procedures  have been applied by engineers to the compression of medical images and human speech.   In each application, a suitable tree must be constructed from  experimental data sets that are typical of the behavior under study. In practice, trees are frequently designed by greedy growing algorithms,  which build a tree iteratively, from the ground up. While these  algorithms are well understood from an experimental standpoint, there has been little theory to support their use, or to examine their behavior on very large data sets.   The proposed research will undertake a systematic study of greedy growing algorithms.  It has three broad objectives: To develop theoretical tools that will provide a means of  rigorously analyzing such algorithms; To apply these tools to the analysis and comparison of existing algorithms; To use these tools, in conjunction with computer simulations, in the design of new algorithms for specific applications. A key feature of the proposed research is that it addresses statistical and engineering applications within the same framework.  Educational activities  will be one of the key responsibilities of the principal investigator during the duration of the grant. The Statistics Department at the University of North Carolina at Chapel Hill  has a strong tradition of graduate and undergraduate education. Maintaining the tradition entails a strong commitment to  teaching, as well as interaction with students, both inside and outside of the classroom. We hope to further the tradition through the development of new courses, which will introduce graduate students to the basic ideas behind the  proposed research. In addition, a reading course will be designed to encourage advanced graduate students to undertake supervised research in the proposed area of study."
"9504470","Mathematical Sciences:  Statistical Inference for Large     Spatial and Space-Time Datasets","DMS","STATISTICS","07/01/1995","07/21/1997","Michael Stein","IL","University of Chicago","Continuing Grant","Joseph M. Rosenblatt","06/30/1999","$150,000.00","","stein@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","1303, EGCH","$0.00","Proposal:  DMS 9504470  PI:  Stein  Institution:  University of Chicago  Title:  Statistical Inference for Large Spatial and Space-Time Datasets    Abstract:     This research considers both applied and theoretical problems arising in  statistical analysis of spatial and space-time data.  The applied aspect  is the detailed modeling and analysis of stratospheric ozone levels as  measured via satellite at over 30,000 locations daily for over a decade.  The goal is to model the data at the high level of spatial and temporal  resolution it is taken and not to aggregate over space and/or time as  has been done in all previous statistical analyses of these data.  To reach this goal, it will be necessary to consider in detail the nature  of the measurement process and the space-time variations of ozone  concentrations due to such factors as long-term trends,  seasonal effects, variations both within and across latitudes   (which are quite different) and possibly the relationship of ozone  to meteorological conditions. The immense size of this dataset provides   a strong connection to the theoretical aspect of this proposal:  the study   of asymptotic problems in the analysis of spatial data.  In particular, a fixed domain asymptotic approach, in which the number of  observations in some fixed region of space increasing, is adopted.  Using this approach, this research studies the asymptotic properties of  spatial periodograms for estimating the high frequency behavior of the  spectral density of a stationary random field and the effect of misspecifying   spectral densities on prediction problems. The ultimate goal is to connect   these two problems and to understand the effect of estimating spectral densities   on subsequent predictions.      Satellite based instruments measure numerous aspects of the earth and  its atmosphere with high resolution in space and time.  For example, the Total Ozone Monitoring Spectrometer (TOMS) measures  ozone levels at over 30,000 locations daily, providing a much  grea ter level of detail than can possibly be attained using ground-based  instruments. However, this high spatial resolution has not been put to much   use, as statistical analyses to date of this dataset have been on ozone levels  averaged over large regions. Using 10 years of TOMS records as a test case,  this project develops statistical and computational methods for analyzing the   spatial-temporal structure of large, high-resolution geophysical datasets.  This research aims to provide a better understanding of the space-time dynamics  of stratospheric ozone and hence the opportunity to more accurately assess the   effect of anthropogenic emissions on ozone levels, a problem of worldwide  environmental concern. The methods developed in this research may prove   useful in analyzing other large satellite based space-time datasets, such as   meteorological conditions relevant to global climate change.  In addition, this research studies theoretical properties of statistical methods   when applied to large spatial and space-time datasets."
"9504414","Mathematical Sciences:  Processing Massive Noisy Data       with Hidden Structure","DMS","STATISTICS, SIGNAL PROCESSING SYS PROGRAM","07/15/1995","08/08/1996","Jianqing Fan","NC","University of North Carolina at Chapel Hill","Continuing Grant","K Crank","12/31/1998","$157,000.00","James Marron","jqfan@princeton.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269, 4720","0000, 1303, EGCH, OTHR","$0.00","Proposal: DMS  95-04414    PI(s):   Jianqing Fan - 1989, Stephen Marron - 1982         Institution:   U of NC Chapel Hill   Title:    Processing Massive Noisy Data with Hidden Structure        Abstract:    This research involves a few inter-related statistical  problems ranging from processing images and finding informative structure   from massive data, to analyzing survival data, evaluating the success of a  business advertisement and understanding the home range of animals.  In contrast with traditional approaches,  the methods do not assume any restrictive form, and are flexible  enough to detect fine structures.  This project covers a wide  array of statistics methodological developments and foundational  research.  The new developments enhance the availability of   statistical tools, and can lead to significant improvements on  existing methods for extracting information from  noisy data. Specific objectives are  develop new statistical methodologies  and investigate their foundational properties for flexible statistical modeling  in processing high-dimensional data, nonparametric confidence intervals,  mode detection and signal processing.    Many scientific disciplines depend in some way on extracting structural  information from noisy data.  Fields ranging from processing noisy images  and evaluating business marketing, to analyzing survival data and   forecasting economic climates, which are universes apart in their backgrounds,  nevertheless have the common  problem  of drawing conclusions via processing noisy signals.  Such problems may be abstracted as a statistical function estimation   problem and can be analyzed by various techniques in this project.  The objective of this research is to develop and  evaluate flexible statistical modeling techniques.  These techniques  can be applied in the Federal Strategic Areas such as monitoring  environmental and global changes via processing massive collected data  where informative structures can hardly be detected by traditional approache s,   and building statistical modeling for economic and business activities  in the civil infrastructure. The investigators will take advantage of modern  computing facilities, and use statistical knowledge to avoid  unnecessary data mining and hence reduce significantly data processing time.  Thus, the knowledge gained in this study will also be useful   in high performance computing of a federal strategic area."
"9505109","Mathematical Sciences:  Statistical Methods for Nonlinear   Inference in Time Series with Stochastic Variance","DMS","STATISTICS","07/01/1995","06/30/1995","Nicholas Polson","IL","University of Chicago","Standard Grant","Gabor Szekely","06/30/1999","$50,000.00","","ngp@gsbngp.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00","Proposal:  DMS 9505109  PI:  Nick Polson  Institution:  University of Chicago  Title:  Statistical Methods for Nonlinear Inference in Time Series with Stochastic Variance       Abstract:     This project develops and applies hierarchical statistical models for  nonlinear inference problems to the new areas of multivariate data with  stochastic variance as well as nonnormal errors. Previous research  by Jacquier, Polson and Rossi (1994) has shown that this methodology is   better than the usual statistical methods, such as generalized methods of   moments, for the case of normal univariate stochastic variance. This research  utilizes Markov chain Monte Carlo techniques and hierarchical modeling to  construct and implement a methodology for inference which incorporates  multivariate stochastic variance and nonnormality. Within this methodology,  it is straightforward to examine the effects of alternative assumptions  such as different error distributions on the resulting inferences. In addition,  the project develops an outlier diagnostic procedure to be used in the presence  of stochastic variance.      This research develops a general methodology for analyzing time series data,   such as stock prices, interest rates and other financial series, where variance,   or volatility, changes over time. Until recently, such data were analyzed assuming  either constant variance or variance changing according to a predetermined pattern.  Recent statistical work has shown that a better way of analyzing these data is to   assume randomly changing variance. This approach is known as stochastic variance   (or volatility) time series  modeling. This research provides a more general  framework for analyzing these models which does not rely on the usual limiting  assumptions of existing methodologies. A number of statistical tools are provided to   analyze the data within the stochastic variance framework. The methodology developed  in this research presents an improved way to analyze time series data, and severa l  examples based on financial time series are included."
"9505043","Mathematical Sciences:  Model-based Statistical Inference   and Data Analysis","DMS","STATISTICS","07/01/1995","06/30/1995","Stephen Stigler","IL","University of Chicago","Standard Grant","James E. Gentle","06/30/1997","$60,000.00","Xiao-Li Meng","","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00"," Proposal DMS 95-05043  Principal Investigators:  Stephen M. Stigler,   Xiao-Li Meng  Institution:  University of Chicago  Title:  Model-based Statistical Inference and Data Analysis    Abstract    The investigators study topics that concern the construction and   investigation of statistical models involving latent structure   for the analysis of complex data sets.  Stigler's research involves   models for communications networks where the available data is in the   form of frequency counts of directed transactions.  The models studied   depart from standard linear models for paired comparisons in that   they are not transitive  - they exhibit cycles - and they incorporate   departures from standard multinomial models for the dispersion of   counts.  Meng's research involves frequentist properties of Bayesian   procedures, and is related to his research on multiple imputation   methodology, a general and efficient inferential tool for handling   the complex problem of nonresponse in sample surveys, especially   those that produce public-use data files which will be analyzed by many   users.  Meng will seek to establish robust frequentist properties of   Bayesian procedures in multiple imputation inference, with specific   reference to confidence validity under uncongenial multiple imputation   inferences, frequency evaluations of posterior predictive p-values,   and unbiased imputations and single observation unbiased priors.    The research is of two parts.  One part studies the nature of scientific   communication, seeking to build statistical models to understand better   the relationship between theoretical research and applied research   (in particular the degrees to which they influence each other), and   the relationships between different fields of science (for example,   do different but related disciplines necessarily form a hierarchy,   or do they exhibit more complicated patterns of ""trade"" in ideas.)    The other part involves the construction of statistical models for   analyzing  survey results when nonresponse is a problem.  All surveys   may be susceptible to bias when potential respondents decline to   participate for reasons related to the answer they would have given,   and the methods studied will ameliorate this problem for complex surveys   such as the U. S. Census, where it is desirable to prepare public-use   data files that address this problem and are not overly sensitive to   the statistical assumptions made."
"9505290","Mathematical Sciences:  Random Coefficient Models and       Robust Analysis","DMS","STATISTICS","07/15/1995","06/30/1995","Douglas Simpson","IL","University of Illinois at Urbana-Champaign","Standard Grant","James E. Gentle","12/31/1998","$107,000.00","","dgs@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","0000, OTHR","$0.00","Proposal:  DMS 95-05290  Principal investigator: Douglas G. Simpson  Institution: University of Illinois  Title: RANDOM COEFFICIENT MODELS AND ROBUST ANALYSIS    Abstract:    Random coefficient models have found wide application for   modeling correlation, incorporating laboratory or study   effects, and in developing Bayesian estimates and inferences.   Marginal analysis is an alternative approach in which models   are developed directly for marginal distributions (integrated  over random effects), and the inferences are at the level of  population average effects rather than at the level of   individual experimental units. Connections and contrasts  between these general modeling strategies and robust   statistics will be developed. The aim is to develop fresh   ideas for attacking hard problems in applied statistics.   Among the problems to be attacked: robust generalized linear   modeling and graphical data analysis; environmental exposure   risk assessment combining information from multiple studies   with varying protocols and endpoint measurements; inferences   for random function data arising from spectral measurement   instruments.    With the wide availability of powerful computers,   increasingly complex data are being collected. In   analytical chemistry it is now routine to qtore   separation spectra electronically. In environmental   toxicology large databases are being assembled on   potentially hazardous pollutants. The proposed research   is directed at the development of statistical methods   for such data. In combining information from multiple   exposure--response studies, it is possible to model   uncertainty due to things such as interlaboratory effects   or species differences using modern computational   techniques and new statistical paradigms. In   environmental chemistry, fluctuations in the  amount of material assayed and in the measuring instrument  itself lead to extra randomness in the measurement. Adjusting   for this effect is critical to the success of the metho ds,   particularly as the number of features measured, e.g., peaks   on a mass spectrum, becomes large. The proposed research will  develop toolkits for analysis, inference and diagnostics.   Key components of the work are algorithm development,   simulation studies, asymptotic analysis, testing of the   methodology, and software development. The major applications  driving this research are in environmental risk assessment   (combining information) and environmental monitoring   (spectral measurements), targeting strategic national   concerns in environmental management.  ??"
"9404906","Mathematical Sciences:  Innovative Statistical Methods for  Biological Life Spans and Oldest-Old Mortality","DMS","POPULATION DYNAMICS, STATISTICS","07/01/1995","02/27/1995","Jane-Ling Wang","CA","University of California-Davis","Standard Grant","James E. Gentle","06/30/1998","$86,310.00","James Carey, Hans-Georg Mueller","janelwang@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1174, 1269","0000, 9187, EGCH, ENVI, OTHR","$0.00","We investigate mortality of the oldest-old by means of nonparametric hazard function estimation based on smoothing   techniques for lifetables. Two- dimensional smoothing methods   for Lexis diagrams will be developed allowing to predict the survival of demographic cohorts, employing kernel and locally   weighted least squares smoothers.  Male-female comparisons and   changes in the level of mortality will be investigated with   change-point techniques. Stochastic process models for samples   of lifetables and associated inference will be introduced.   It is planned to analyze various biological and demographic   data sets with these methods, in particular a huge set   of data on the survival of medflies.    In this interdisciplinary project, newly developed nonparametric   statistical techniques will be applied to approach a complex of   biological and demographic qu estions about longevity and   mortality of the oldest segment of the population. These   questions focus on whether mortality is decreasing for the   oldest segment or is invariably increasing with age, how sex differences   affect changes in mortality, and to what degree one can predict future   mortality and life expectation from current trends. The importance  of these problems derives from their potential impact on social planning for   the future as well as  on the biology of aging. Attacking these   problems requires the development of new statistical  methods and the innovative application of existing statistical methods   to analyze current biological  experiments and demographic data"
"9504525","Mathematical Sciences:  Multivariate Analysis, Rank Data andMultivariate Ranks","DMS","STATISTICS, Methodology, Measuremt & Stats","07/15/1995","04/30/1997","John Marden","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Joseph M. Rosenblatt","06/30/1999","$147,000.00","","marden@stat.uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269, 1333","0000, OTHR","$0.00"," Proposal:  DMS 9504525  PI:  John Marden  Institution:  University of Illinois  Title:  Multivariate Analysis, Rank Data and Multivariate Ranks         ABSTRACT    This research explores several areas of multivariate statistical analysis  and the modeling and analysis of rank data.   In rank data, several   judges rank a set of objects from best to worse.  Research in the analysis and modeling of such data includes   evaluating distances between rank vectors by looking at the set of  distances as a convex cone;  extending unidimensional unfolding models to  several dimensions by using orthogonal ``contrasts"" of objects, each contrast  being a collection of groups of objects; and using quotient groups  of the group of permutations with respect to  particular subgroups to represent contrasts and/or patterns of ties  in rank data. Related to rank data is rank-based nonparametric  analysis of variance, wherein the the cells in  the design are the objects.   Main and interaction effects are defined nonparametrically  in multiway layouts, rank-based procedures for hypothesis testing of  these models are developed, and invariance and likelihood  principles are used to find universally efficient rank-based test procedures.  Finally, an approach to extending univariate rank procedures to multivariate  procedures, including multivariate analogs  of popular univariate statistics (Wilcoxon/Mann-Whitney,  Kruskal-Wallis, Jonckheere-Terpstra, Kendall tau, Spearman rho, etc.)  and tests of symmetry hypotheses on covariance matrices, is detailed relying   on a particular definition of multivariate rank.  Iteration of the rank function may provide  an approach to defining multivariate   probability functions and inverse functions, with applications to generating random   vectors and multivariate QQ-plots.    The main focus of this research is to analyze and extend a number of popular   statistical procedures. Rank data, in which a number of judges ranks a  number of objects from best to worse, arise in many are as, including  political science, sociology, education, psychology, and consumer preferences.   Such  data typically show a high degree of complexity, often due to the presence  of distinct camps among the judges, natural groupings of the objects,  or a continuum of possible preferences of the judges (along, e.g., a  liberal/conservative continuum). New statistical models based on groupings of objects  are able to capture such complexities in many cases.  This research extends the  scope of these models by considering multiple groupings or continuums.  (There may be separate liberal/conservative continuums for social  issues and for economic issues.) Related research is applicable to  many fields, including agriculture, biology, economics, and environmental sciences  as well as the social sciences. Most statistical studies are directed towards  comparing individuals or treatments (e.g., fertilizers, drugs,  educational methods) based on one or more measurement, or finding   relationships between different measurements (e.g., diet and health).  The basic methods depend on fairly restrictive assumptions, assumptions  which often do not apply in real scientific situations. Consequently,  a great amount of work has been dedicated to developing procedures which  work well even when the assumptions are violated to a certain extent.   It is particularly important that the procedures not fail in  the presence of a few unusual values. When comparisons are based on only  one attribute, rank-based procedures are easily applied  and understood, and valid under reasonably mild conditions. These methods are  applied by taking the data (on, e.g., cholesterol level) and replacing  them with their ranks (i.e., the lowest level becomes ``1"",   the next ``2"", etc.) It is very common to wish to   make comparisons based on more than one attribute, e.g.,   cholesterol level and blood pressure. In such cases,  ranking individuals is more problematic since their order with respect to  cholesterol may not be the same as  that with respect to blood pressure.  A major component of this research uses a particular approach  to ranking individuals on several attributes simultaneously.   Procedures are developed based on these rankings that parallel those for   single attributes. These methods are easy to use and widely valid, being   especially resistant to unusual observations."
"9523589","Mathematical Sciences:  Reduced-Form Modelling for          Integrated Assessment","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, Unallocated Program Costs","10/01/1995","04/21/1998","Max Henrion","CA","Institute for Decision Systems Research","Standard Grant","James E. Gentle","09/30/1998","$279,998.00","","henrion@lumina.com","444 High Street","Palo Alto","CA","943011620","4158515266","MPS","1253, 1269, 9199","1317, EGCH","$0.00"," Integrated assessment is increasingly used as a way to bridge the gap between   science and policy in environmental problems. Models of each aspect of a complex   problem are often too cumbersome to integrate directly. Instead, it is often more appealing   to develop reduced-form models for each component model that are small enough to be   comprehensible, flexible, and easily linked to one another. A reduced-form model provides   approximately the same behavior as the model from which it is extracted in a much more   compact and comprehensible form. The goal of this project is to develop and evaluate   techniques to help modellers analyze their models and construct reduce-form models.   The specific objectives are:  1. To design and prototype a modellers' workbench to facilitate specification and   documentation of the inputs, outputs, and assumptions of existing models of   various types and sizes;  2. To provide a range of existing and new sampling techniques to generate   scenarios to exercise these models;  3. To provide a range of existing and new model analysis techniques to identify   key inputs and parameters, identify nonmonotonic and regions of qualitatively   different behavior, and so help develop reduced-form models; and  4. To evaluate these techniques experimentally by application to the development   of reduced-form models and integrated assessment models, and so to provide   guidance on which techniques are most valuable and for what kinds of models.  This project is supported under the Methods and Models for Integrated Assessment   funding opportunity."
"9504425","Mathematical Sciences:  Resampling Methods in Model         Selection and Sample Surveys","DMS","STATISTICS, Methodology, Measuremt & Stats","07/15/1995","07/07/1995","Jun Shao","WI","University of Wisconsin-Madison","Standard Grant","Joseph M. Rosenblatt","06/30/1999","$75,000.00","","shao@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269, 1333","0000, OTHR","$0.00","Proposal:  DMS9504425  PI:  Jun Shao  Institution:  Univ. of Wisconsin - Madison  Title:  Resampling Methods in Model Selection and Sample Surveys      Abstract:    This research involves the following areas of investigation.  (1) The investigator studies the theoretical properties of various   data-resample model selection methods in linear and nonlinear regression,   generalized linear models, time series and multivariate analysis.  New and advanced methods will be developed in problems where the  existing methods lead to unsatisfactory results. (2) Data-resample  methods that can be applied to complex survey data with imputed   missing values will be developed. This includes modification and   adaptation of the existing data-resample methods which are not suitable   for complex survey data. (3) The investigator's study of the  theoretical properties of the data-resample methods include investigations   of the asymptotic (large sample) properties and the fixed (small)   sample performances of the data-resample methods. The relative   performances of different methods will be assessed. The results   will be given in a form which can be easily adopted as a guide for   practical applications. (4) The data-resample methods usually require   repeated computations of some given statistics. In model selection and   sample survey problems the size of the data set is usually large so   that the computation required by some data-resample methods may be cumbersome.  The investigator studies some efficient methods for computations.    Statistical analysis is usually based on a data set that is a sample  from a population which is a collection of values of some variable of  interest. Since the sample is only a part of the population, conclusions   drawn based on the sample are subject to certain statistical errors.  A method for assessing statistical errors, called the data-resample method,  takes many sub-samples from the sample by treating the sample as  the population, and makes inference by applying th e analogous relationships   between the sample and the population  to  the sub-samples and the  sample. This method has caught on very rapidly in recent years because   (1) the existence of inexpensive and fast computing facilities ensures   that this computer-intensive method can be implemented; (2) this   method sometimes provides more accurate and/or stable solutions than the   traditional methods that are commonly used; (3) the theoretical derivations   required in applying the traditional methods are very difficult when the  problem under consideration is complex. Although there are many developments   in using this method over the last two decades, the recency of this   technique has left many questions unanswered which are relevant to practical   application. The aims of this research are in the area of development,   evaluation, and application of many types of data-resample methods in   various complex statistical problems."
"9504949","Multinomial Density Estimation - A Projection Pursuit       Approach and a Wavelet Approach","DMS","STATISTICS","07/01/1995","06/12/1995","Jianping Dong","MI","Michigan Technological University","Standard Grant","James E. Gentle","06/30/1997","$30,000.00","","jdong@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","MPS","1269","9218, HPCC","$0.00","This research involves the development of new estimators, a  projection   pursuit  estimator and a wavelet estimator, for cell  probabilities of   high-dimensional ordered contingency tables.  A high-dimensional  contingency   table has two unique features: it is usually very sparse, and it  has a large   number of boundary cells. The projection pursuit method is used  to study the   features of high-dimensional data by looking at its  low-dimensional   projections.  The projected data is no longer sparse and the  projection   pursuit method does  not suffer from boundary effect since the  method is   not based on local  averages. When a smoothing method is  introduced,   one usually assumes that the contingency table has an underlying  density   function which has a certain  degree of smoothness.  Both the  order of a   kernel function and the optimal  choice of a smoothing parameter  depend on the smoothness of the underlying   density function.  This assumption seems unnatural.  One may not  have a prior   knowledge about the smoothness of an unknown density.  The  wavelet method can   adapt to the smoothness of an unknown density, and it avoids the  problem of   how much to smooth.  Therefore, one does not need to assume the  existence of   an underlying density function.        Contingency or cross tabulation tables occur very frequently  in biomedical science, social science,  and educational research.  For example, to study the public's  opinions on   five different policy issues (health care, tax, . . .) on a five  point scale   (strongly agree, agree, neutral, disagree, strongly disagree), a   five-dimensional cross tabulation table with 3,125 cells would be  needed.    Frequently when the number of cells is large, the table is  usually sparse,   i.e., has many empty cells.  Large and sparse tables provide  many challenges  for statistical analysis.  The research develops new tools for  analyzing this type of    categorical data."
"9500062","Mathematical Sciences:  Conference on Multiple Decision     Theory and Related Topics","DMS","STATISTICS","05/01/1995","03/31/1995","James Berger","IN","Purdue University","Standard Grant","James E. Gentle","04/30/1996","$10,000.00","","berger@stat.duke.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","0000, OTHR","$0.00","   Re:  DMS 95-00061, ""Conference on Multiple Decision Theory and Related Topics""  PI:  James Berger    Abstract:   During June 8-12, 1995, a conference on Multiple Decision Theory and related topics will be held at Purdue University. The conference will consist of two parts, a traditional conference of invited and contributed talks in multiple decision theory, and an intensive workshop on the recent approach to  multiple model selection called ""intrinsic Bayes factors."" The traditional part of the conference will consist of 27 invited talks by leaders in the field, including talks aimed at assessing the current status of research in the area and assessing future needs. The workshop phase will involve approximately 20 active  researchers in development of intrinsic Bayes factors, a very recent and promising methodology which is applicable to virtually any model selection problem, nested or nonnested, without requiring asymptotics. The workshop phase will adopt a non-traditional framework that encourages discussion of unsolved  problems, rather than solely emphasizing presentation of recent research.      Multiple decision theory is concerned with situations in which there are two or more populations, and it is desired to select the ""better"" populations, rank order the populations, or make other decisions involving the comparison of populations. This type of scenario is common to a huge variety of applied problems in virtually all areas of statistical application. The area has seen a great deal of activity in recent years, and a conference on the subject involving the leaders in the field is very timely. The workshop phase of the conference will bring together a smaller group of researchers to study optimal ways to select among multiple statistical models. Model selection is typically the most important phase of a statistical analysis, and the new methodology that is to be discussed is extremely promising in terms of its ability to automatically select the best models.      Level of Effort:   The  level of effort in the conference is being increased, by inclusion of the intensive workshop on intrinsic Bayes factors. The possibility of adding this workshop arose when it was realized that many of the active researchers on intrinsic Bayes factors would be at the Multiple Decision Theory conference; extending the conference for two days to incorporate this workshop thus seemed scientifically and financially wise.      Budget Explanation:   The request for $10,000 from NSF is not being changed. Approximately $5000 will, however, be used for the expenses of those attending the intensive workshop. (These individuals will, of course, also be participating in the earlier portion of the conference.) Additional expenses incurred by the increased level of effort will be met by Purdue University."
"9596248","Mathematical Sciences:  The Generalized MLE and             Redistribution-to-the-Center Estimator of a Survival        Function","DMS","STATISTICS","07/01/1995","09/08/1995","Qiqing Yu","NY","SUNY at Binghamton","Standard Grant","James E. Gentle","08/31/1997","$29,700.00","","qyu@math.binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","MPS","1269","0000, OTHR","$0.00",""
"9505065","Reparameterization and Longitudinal Modeling","DMS","STATISTICS","08/01/1995","06/16/1997","Elizabeth Slate","NY","Cornell University","Continuing Grant","Joseph M. Rosenblatt","07/31/1999","$135,000.00","","eslate@fsu.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","0000, OTHR","$0.00","Proposal: DMS  95-05065    PI:     Elizabeth Slate, 1991  Institution: Cornell University    Title:   Reparameterization and Longitudinal Modeling    Abstract:      This research is concentrated in two main areas of statistical  inference and modeling: reparameterization and hierarchical models for  longitudinal data.  The reparameterization research develops  methodology that identifies parameterizations for a wide class of  statistical models for which normal approximations to the likelihood  function and posterior density are accurate.  Parameterization  diagnostics based on measures of non-normality are used to identify  well-behaved parameterizations and to derive sample sizes sufficient  for reliable inference based on asymptotic normality for many  commonly-used parameterizations.  The hierarchical modeling component  of the research compares and unifies models for longitudinal data,  including Markov chain models, hierarchical mixed effects models, and  fully Bayesian hierarchical models.  Estimation, prediction and design  issues are addressed, and special consideration is given to the  incorporation of random changepoints in these models.       This research studies statistical models which may be uqed  for  processes such as the basis of a feed-forward controller for many of today's  complex manufacturing processes.  Because exact inference can be very  difficult in these complicated models, scientists often resort to the  simplification of summary results which are implicitly understood in   the context of normality (i.e., the bell curve).  The first component of this research   studies procedures for determining when this simplification is misleading and   presents alternative statistical models.   This research will  lead to increased numerical   efficiency in fitting these complex models, permitting more timely response to  changing processes.  The second component of this research studies models for  longitudinal data.  These models are needed when the data consist of  serial mea surements within individuals for a number of individuals.    Such data arise in a wide range of applications, from reliability studies to economic   indicators to health monitoring.  The research compares and unifies a number of  models for longitudinal data, and addresses estimation, prediction and  design issues.  Special consideration is given to the incorporation of  random changepoints in these models (i.e., points in time where the process  exhibits some serious change in behavior).  The   changepoints may represent onset of disease, critical part fatigue, or   changes in consumer behavior.  The research studies how longitudinal   models may be used in a dynamic fashion to detect these changepoints quickly."
"9504596","Mathematical Sciences:  Time Series Models and Extreme       Value Theory","DMS","STATISTICS, SIGNAL PROCESSING SYS PROGRAM","07/15/1995","03/05/1997","Richard Davis","CO","Colorado State University","Continuing Grant","Joseph M. Rosenblatt","06/30/1999","$210,000.00","Murray Rosenblatt, Peter Brockwell","rdavis@stat.columbia.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1269, 4720","0000, OTHR","$0.00","Proposal:  DMS 9504596  PI(s):  Richard Davis, Murray Rosenblatt, Peter Brockwell  Institution:  Colorado State  Title:  Times Series Models and Extreme Value Theory    Abstract:    The research is concerned with problems of estimation and   research for time series models whose theory is not yet   fully understood. The standard linear Gaussian models for   time series data are inadequate to describe many of the   time series observed in practice so it is important to   develop techniques for estimation and prediction based on   more general models. Efficient estimation procedures and   prediction techniques for non-causal and non-invertible   ARMA processes are developed and a study made of the   theory and application of continuous-time linear and   non-linear ARMA processes. Extreme value theory for   linear and non-linear models is also investigated.      Existing methods of forecasting are based on assumptions   which are frequently not satisfied by observed economic   and scientific time series data. This research develops   methods of analysis and forecasting for a more general   class of time series models, leading to more accurate   forecasting of series which do not meet the restrictive   assumptions of the classical theory."
"9522146","Mathematical Sciences:  Computing Science and Statistics:   Symposium on the Interface, 1995","DMS","STATISTICS","06/01/1995","05/01/1995","Michael Meyer","VA","Interface Foundation of North America Inc","Standard Grant","Stephen M. Samuels","05/31/1996","$10,000.00","James Rosenberger","mikem@stat.cmu.edu","P.O. Box 7460","Fairfax Station","VA","220397460","7039931691","MPS","1269","9146, MANU","$0.00","9522146  Meyer    Abstract    Interface '95 is a conference which will bring together researchers and practioners in statistics and computing science, to discuss the theme of ``Statistics and Manufacturing,'' with a sub-theme of green manufacturing, the environment, and quantitative environmental science.  Many of today's environmental concerns are a consequence of past manufacturing decisions--decisions that were often made with statistical insight.  It is surely fair to suggest that in order to mitigate tomorrow's environmental problems we must begin by understanding today's manufacturing processes.  Here the combination of statistics, computing science, manufacturing, and the environment becomes particularly compelling.  The Interface conference provides a unique opportunity for professionals in statistics, computing science, and various application areas to interact on issues at the interface of these disciplines. The Symposium fills a critical gap between the activities of our large professional societies. The 1995 Symposium on the Interface of Computing Science and Statistics will be held June 21-24, 1995 at the Vista Hotel in Pittsburgh Pennsylvania."
"9504463","Mathematical Sciences:  ""Extended Linear Modeling with      Splines""","DMS","STATISTICS","07/01/1995","04/29/1997","Charles Stone","CA","University of California-Berkeley","Continuing grant","Joseph M. Rosenblatt","06/30/1999","$135,000.00","","stone@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, 9178, OTHR, SMET","$0.00","proposal number: DMS-9504463  PI:   Charles Stone  Institution:  UC Berkeley  Title:  Extending Linear Modeling with Splines      An abstract for the project    In many statistical models of current theoretical and   practical interest, the log-likelihood function depends   linearly on the one or more unknown functions. The   theory and methodology of such models, referred to as   extended linear models, are particularly tractable when   the models are concave; that is, when the log-likelihood   depends concavely on the unknown function or functions.   Among the class of concave extended linear models are   ordinary, logistic, probit, Poisson and other   generalized linear models, multiple logistic regression   (which is useful in multiple classification), hazard   regression (for survival analysis), and models for the   estimation of density and conditional density functions.   In the context of such models, polynomial splines and   their tensor products are natural building blocks for   constructing finite-dimensional estimates of   infinite-dimensional main effects and loworder   interactions, and the resulting ANOVA decompositions   provide an insightful tool for data analysis. In this   research, the corresponding nonadaptive theory and   closely related adaptive methodology will further be   developed and refined in a variety of settings. In   particular, an attempt will be made to establish uniform   rates of convergence first in the regression context and   then for other extended linear models, where the maximum   likelihood estimates are intrinsically nonlinear. The   theory for spectral density estimation in the context of   a stationary time series will be extended to handle   mixed spectra. The theory and methodology for survival   analysis will be extended to include time-dependent   covariates and directly to fit a flexible proportional   hazards (Cox) model without modeling the dependence of   the hazard function on time. The methodology and theory   that have already been develo ped for extended linear   models will be modified to handle neural spike train   processes that occur in neurophysiology and event   history analysis, which is commonly used in sociology   and other social sciences.      This research is part of the investigator's long-term   research program in statistics, which began two decades   ago. Prior to that time, the emphasis on statistics had   been on parametric modeling, which involves fitting   models with a fixed, finite number of unknown   parameters. At that time, the investigator joined the   small cadre of statisticians working in the field of   nonparametric modeling. A half-dozen years later, the   investigator further specialized his research program to   what essentially amounts to the synthesis of parametric   and nonparametric modeling; specifically, flexible   parametric models are employed that have increasingly   many parameters as more and more data become available.   A decade ago, the investigator began another long-term   project: teaching and simultaneously writing a textbook   for an upper-division or graduate-level first course in   probability and statistics in which the statistics   portion is presented in an innovative manner suggested  by this synthesis of parametric and nonparametric   modeling. (The textbook will be published this summer.)   This educational project, in turn, has had a catalytic   effect on the investigator's research program. Moreover,   in the direction of human resource development to   improve the civil infrastructure, the investigator has   used the positions of Teaching Assistant for the course   and Research Assistant on previous NSF grants to recruit   Ph. D. students, train them in teaching and in the   investigator's research program, and place them in   academic and industrial research positions. Recently one   such former student, who currently has a tenure-track   position at a leading American statistics department,   assisted an American software company in winning a small   SBIR contract  to create a commercial software   implementation of the state-of-the-art survival analysis   methodology that was developed by the investigator and   this and another former student. The resulting product   should prove very useful in biotechnology. Another   former student, who has a permanent position at one of   America's leading research labs, has continued a   methodological development initiated in his Ph. D.   Dissertation and successfully applied this methodology   to improve the quality of the IC manufacturing process   of the Lab's parent company. Hopefully, the   investigator's continued research and training of Ph. D.   students and his related collaborations with former Ph.   D. students will have more such worthwhile consequences.         ??"
"9504955","Mathematical Sciences:  Hidden Mark CV Models,              Semi-parametric Models, and Sample Reuse Models","DMS","STATISTICS","07/01/1995","04/09/1997","Peter Bickel","CA","University of California-Berkeley","Continuing grant","James E. Gentle","06/30/1998","$150,000.00","","bickel@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","1041, CVIS","$0.00"," Proposal: DMS  95- 04955  PI:      Peter Bickel   Institution:  UC Berkeley    Title:   Hidden Markov Models, Semi-Parametric Models, and Sample    Reuse Models          ABSTRACT              This research involves development of methodology for inference in  hidden Markov models; for model fitting, diagnostics and tests for  semiparametric models; and studies of novel types of resampling methods.  In the first area the main goals are to extend the work of Baum and Petrie  (1966) and the investigators to underlying continuous state and time Markov  processes and Markov random fields; and to study methods which achieve  computational savings without sacrificing efficiency.  In the second area,  the goal of the investigators is to produce a unified approach to testing  and model selection between and within models arising in biostatistics,  econometric and elsewhere.  In the third area, the goal is to study m out  of n with and without replacement bootstraps for various purposes including  setting critical values in difficult testing situations.            The goal of this research is to develop statistically sound and  computationally feasible estimates, tests, and error bounds in some types  of complex models for data that arise, naturally in a number of important  fields.  Applications of hidden Markov models include speech recognition by  machine and estimation of genomic sequence characteristics, of significance  in biotechnology.  Applications of semiparametric models include  transportation choice and other aspects of civil infrastructure studies.  Reliable resampling methods, our final area of study are essential for  calculating error bounds and thresholds for the complex procedures we are  developing and in fact are an essential part of the procedures as a whole.  Their implementation is possible only in our age of ever more rapid  computation."
"9505583","Mathematical Sciences:  High Dimensional Data Analysis","DMS","STATISTICS","07/01/1995","03/21/1997","Ker-Chau Li","CA","University of California-Los Angeles","Continuing grant","Joseph M. Rosenblatt","06/30/1999","$138,000.00","","kcli@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","1325, 9146, EGCH, MANU","$0.00","Proposal:  9505583  PI:  Ker-Chau Li  Institution:  UCLA  Title:  High Dimensional Data Analysis      Abstract:    The research concerns the development of new statistical methods   for exploring nonlinearity in high dimensional data.  Two tools,   sliced inverse regression (SIR) and principal Hessian directions   (PHD), have been developed based on a dimension reduction   theory which underlies the investigator's approach in the   high-Dimension area.  To continue this line of research, the   investigator studies several new problems which have more   complicated structures than the basic regression formulation.    The research topics include the generalization of SIR/PHD to   multiple outcomes, development of PHD-based tree-structured   methods, study of visualization for mathematical/physical/computer   models, and analysis of nonlinear time series data.    The research takes a  combination of dynamic graphics, computation,   and statistical theory to study high dimensional data. Dimensionality is   one of the most challenging problems in maly scientific areas today.     Immediate applications of these research results include analysis of   longitudinal data in biomedical, socio-economic, or industrial studies,   and groundwater modeling in hydrological studies."
"9504507","Mathematical Sciences:  Topics in Multivariate Survival     Analysis and Counting Processes","DMS","STATISTICS","07/15/1995","04/16/1997","Dorota Dabrowska","CA","University of California-Los Angeles","Continuing grant","Joseph M. Rosenblatt","06/30/1999","$119,986.00","","dorota@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","0000, OTHR","$0.00","Proposal:   DMS 9504507  PI:  Dorota Dabrowska  Institution:  UCLA  Title:   Topics in multivariate survival analysis and counting processes    Abstract:    The objective of this research is to extend applications of multiparameter  product integrals and smoothing techniques in survival analysis. Asymptotic  properties of density and nonparametric regression based on smoothed  product integrals will be developed for the following: data adaptive  construction of rank tests for independence from censored data; nonparametric  estimation of multivariate survival functions from truncated data;  goodness-of-fit in frailty models; and quantile derivative regression in  linear transformation models. The results will be applied to the analysis  of data from AIDS and twin studies.      Multivariate survival analysis is a rapidly growing area with  a wide range of applications in genetic epidemiology, astronomy, econometrics  and other fields. The aim of this research is to develop statistical methods  for analysis of data arising in such studies."
"9503104","Mathematical Sciences: Stochastic Models for Reliability of Systems with Dependencies Among Components","DMS","STATISTICS","07/01/1995","03/05/1997","William Padgett","SC","University of South Carolina at Columbia","Continuing grant","James E. Gentle","12/31/1998","$231,000.00","James Lynch, Stephen Durham","padgett@stat.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","MPS","1269","9146, MANU, 0000, OTHR","$0.00","Proposal: DMS 9503104 PIs: W. J. Padgett, J. D. Lynch and S. D. Durham Institution: University of South Carolina - Columbia Title: STOCHASTIC MODELS FOR RELIABILITY OF SYSTEMS WITH DEPENDENCIES AMONG COMPONENTS Abstract: The research involves the reliability of complex systems. There are three main thrusts: (1) modeling component reliability, (2) incorporating component reliability and dependencies into the system reliability which result in tractable data analytic models, and (3) developing a criticality theory for such models. Regarding (1), some models are studied for the analysis of component reliability, including the conditional Weibull and inverse Gaussian distributions and a Poisson-Weibull flaw model. The ""conditional Weibull"" distribution is studied since it fits certain fiber strength data sets well and may be justified due to ""censoring"" considerations in the fiber manufacturing and testing processes. The Poisson-Weibull flaw model with finite-state Markov random intensity has the mixed distribution (zero intensity) and the mixed hazard model (infinite intensity) as extremes. A major objective of this part of the project is the investigation of a unified mixture theory for this flaw model which subsumes the theory of mixtures developed for the extremes of mixed distributions and mixed hazards, respectively. To address (2) and (3), a general model is investigated which is hierarchical in nature. The hierarchy consists of (i) the micro (or component) level, (ii) the subsystem (or ""bundle of components"") level, and (iii) the system (or ""chain of bundles"") level. It has long been known that in many systems, the failure of a component changes the stress applied to the remaining components. Thus, incorporating component dependencies into a reliability model in a realistic manner is highly desirable for accurate results. Here, component dependencies/interactions are incorporated into the model by using ""load-sharing rules,"" m ost applicable to situations where ""loadings"" are due to mechanical or physical considerations. General monotone load-sharing rules are considered for which a method of calculating the system reliability has been developed. The present research includes a special class of these rules where the component load can be calculated using absorption probabilities for random walks on a network. In particular, a number of the popular load-sharing rules can be reduced to the consideration of electrical networks. Energy considerations give insight into the behavior of stress concentrations induced by these rules as components fail. Major objectives are to continue the investigation of these electrical network rules, and other network rules which are appropriate for mechanical load transfer situations, especially for composite materials, and, specifically to study the ""effective distance"" that a load can be transferred via matrix material and its relationship to the ""ineffective length"" around fiber breaks for load transfer through the matrix. Criticality issues are also considered. Preliminary work for k-out-of-n systems and systems with a small number of components suggest that an exact theory may be obtainable when each component has a Weibull failure distribution using a mixed transformed gamma model, where the transformation depends on the Weibull distribution. Such a model also lends itself to calculation of extreme value approximation errors and to system identification using the mixing distribution. The research involves the development of models for describing the failure of complex systems with dependent components. The researchers are investigating reliability models for complex systems of components including complex materials which allow for component dependencies and interactions. Such models have direct application to many problems of current importance, including failure of fibrous composite materials and electrical networks. This has important imp lications for the design and large scale manufacture of complex materials where high reliability."

"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1612867","Nonparametric estimation of integral curves and surfaces","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","08/15/2016","07/06/2018","Lyudmila Sakhanenko","MI","Michigan State University","Standard Grant","Gabor Szekely","08/31/2019","$204,316.00","David Zhu","sakhanen@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1253, 1269","1515, 8091","$0.00","Integral curves are reasonable models for a variety of natural structures that arise in brain imaging and in atmospheric sciences. Natural structures in brain imaging include the axonal fibers connecting neurons. Natural structures in atmospheric sciences include isolines of barometric pressure, storm fronts, and jet streams. Generally, the data are points along the curves and are corrupted by noise. The goal of this research is statistical estimation of these curves. These researchers will investigate a detailed list of issues pertinent to statistical integral curve estimation, which will deepen and widen our understanding of the natural structures. The PIs will validate their statistical methodology by analyzing images on both diseased and healthy brains. The reliable statistical estimation of fibers will include the assessment of uncertainty in the images used for diagnostic procedures in diseases such as Alzheimer's disease, multiple sclerosis, and brain tumors. It will also enhance the imaging tools needed for planning imaging-guided neurosurgeries. In meteorology the correct estimation of these curves can enhance existing weather maps. Finally, the PIs will capitalize on the similarities between modeling curves and modeling surfaces and investigate the statistical estimation of surfaces, which serve as natural models for axonal fiber bundles in brain imaging and iso-surfaces in digitized images used by many sciences.<br/> <br/>Integral curves are solutions of differential equations where the governing vector or tensor fields are observed directly or indirectly and perturbed by noise. Several directions of their statistical analysis are planned, including a fully nonparametric diffusion function model, simultaneous confidence bands for integral curves, adaptive estimation, comparison of images of the same brain obtained via different procedures, model reduction based on tensor's order, and a unified approach to nonparametric estimation of manifolds, where integral curves and surfaces serve as particular cases. Specifically, the PIs plan to provide an end-user with enhanced images that contain not only estimated integral curves, but also simultaneous confidence bands that show the quality of the estimation in a uniform way, and moreover all the tuning parameters would be calculated from the data only. Upon completion of the proposed research, a practitioner would be armed with a procedure of systematic comparison of statistical properties of different tractography algorithms. Finally, the PIs plan to extend their methodology from 1D curves to 2D surfaces. The basis of this research is a synergy of empirical processes theory, Gaussian processes theory, the theory of ordinary differential equations, perturbation theory for tensors, numerical analysis, computer vision, and computational statistics."
"1612885","Estimation, Prediction, and Extremes of Multivariate Random Fields","DMS","STATISTICS","07/01/2016","05/23/2016","Yimin Xiao","MI","Michigan State University","Standard Grant","Gabor Szekely","06/30/2020","$120,000.00","","xiao@stt.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","","$0.00","Data sets with multivariate measurements obtained at spatial locations are nowadays ubiquitous in many scientific areas, ranging from astronomy, environmental, and ecological sciences to image processing. The need to construct and understand multivariate random fields as models for multivariate spatial or spatio-temporal data has increased dramatically in recent years. However, the development of statistical theory and methodology for multivariate random fields is still in an evolutionary stage and poses substantial challenges. This research project is focused on establishing novel results and opening new research directions in parameter estimation, prediction, and extreme value theory of multivariate random fields. It is anticipated that the results of the work will further promote the applicability of multivariate random field models in statistics and other scientific areas. Moreover, involvement in the research will train graduate students and develop their careers in the mathematical sciences.<br/><br/>The research addresses significant questions in statistical inference and extreme value theory of multivariate random fields. Special emphasis is placed on investigating the effects of smoothness/fractal indices and cross-dependence structures of multivariate Gaussian and related random fields on their parameter estimation and prediction under the framework of fixed-domain asymptotics, and on multivariate extreme value theory. Many of the problems under investigation are intrinsically connected with geometric and topological properties of the multivariate random fields. The principal theoretical findings envisaged by this project include simultaneous estimation of multiple parameters and description of their joint performance, prediction under fixed-domain asymptotics, and precise asymptotics for the excursion probabilities, for multivariate Gaussian and related random fields indexed by the Euclidean space or spheres. In previous work, the investigator developed probabilistic and statistical methods for studying multivariate random fields and resolved several outstanding open problems on excursion probabilities, random fractals, Gaussian random fields, Lévy processes, and fractional Lévy random fields. This project aims to yield novel insights into the understanding of multivariate random fields and quantify the influence of their cross dependence structures on the performance of estimators, kriging and co-kriging, and extreme value theory."
"1547396","RTG: Computational and Applied Mathematics in Statistical Science","DMS","APPLIED MATHEMATICS, STATISTICS, WORKFORCE IN THE MATHEMAT SCI","07/01/2016","07/14/2023","Jonathan Weare","IL","University of Chicago","Continuing Grant","Yulia Gel","06/30/2024","$1,749,360.00","Michael Stein, Mihai Anitescu, Jonathan Weare, Rina Barber, Chao Gao, Daniel Sanz-Alonso","weare@nyu.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1266, 1269, 7335","7301","$0.00","This Research Training Group (RTG) project supports creation of a dynamic, interactive, and vertically integrated community of students and researchers working together in computational and applied mathematics and statistics. The activity recognizes the ways in which applied mathematics and statistics are becoming increasingly integrated. For example, mechanistic models for physical problems that reflect underlying physical laws are being combined with data-driven approaches in which statistical inference and optimization play key roles. These developments are transforming research agendas throughout statistics and applied mathematics, with fundamental problems in analyzing data leading to new areas of mathematical and statistical research. A result is a growing need to train the next generation of statisticians and computational and applied mathematicians in new ways, to confront data-centric problems in the natural and social sciences. <br/><br/>The research and educational activities of the project lie at the interface of statistics, computation, and applied mathematics. The research includes investigations in chemistry and molecular dynamics, climate science, computational neuroscience, convex and nonlinear optimization, machine learning, and statistical genetics. The research team is made up of a diverse group of twelve faculty, including researchers at Toyota Technological Institute at Chicago and Argonne National Laboratory. The RTG is centered on vertically integrated research experiences for students, and includes innovations in both undergraduate and graduate education. These include the formation of working groups of students and postdocs to provide an interactive environment where students can actively explore innovations in computation, mathematics, and statistics in a broad range of disciplines. Post-docs will assume leadership roles in mentoring graduate students and advanced undergraduates. Participants in the RTG will receive an educational experience that provides them with strong preparation for positions in industry, government, and academics, with an ability to adopt approaches to problem solving that are drawn from across the computational, mathematical, and statistical sciences."
"1610305","Collaborative Research:  Estimation of Large Species/Population Trees Using Tree Space","DMS","STATISTICS","08/01/2016","08/04/2016","Laura Kubatko","OH","Ohio State University","Standard Grant","Gabor Szekely","07/31/2020","$76,051.00","","lkubatko@stat.osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269","","$0.00","The estimation of the evolutionary history of a collection of organisms based on the information contained in their DNA sequences is a problem of fundamental importance in evolutionary biology. The abundance of DNA sequence data arising from genome sequencing projects has led to important computational challenges in the estimation of these phylogenetic relationships. Among these challenges is the estimation of the evolutionary history for a group of species based on DNA sequence information from several distinct genes sampled throughout the genome. This research is focused on the development of computationally efficient methods for estimating the evolutionary history when the number of species under consideration is very large (i.e., hundreds to thousands). This is accomplished by considering collections of three species at a time, and using properties of the estimated evolutionary history for groups of three to infer the overall evolutionary history. Properties and performance of the method will be evaluated theoretically as well as with both simulated and empirical data sets. This work has numerous practical applications, such as the study of the evolutionary relationships among human populations.<br/><br/>Though the amount of genomic data available for inferring phylogenetic species trees has increased rapidly within the last 10 years, few methods have been developed to efficiently estimate species trees for data sets consisting of hundreds or thousands of species. A fast approximation to the maximum likelihood estimate (MLE) that retains desirable statistical properties, such as consistency and asymptotic efficiency, is proposed. Results from preliminary work suggest that this approach will be significantly faster than existing likelihood and Bayesian approaches, while also being highly accurate. The method can be applied to a range of data types, including allele frequency data arising under a Brownian motion model along the phylogeny and single nucleotide polymorphism (SNP) data arising from the coalescent model. A software package will be developed to implement the methodology. The project will also support one PhD student, who will contribute to the development and implementation of the methodology."
"1613054","A Geometric Approach to Bayesian Modeling and Inference with the Nonparametric Fisher-Rao Metric","DMS","STATISTICS","09/15/2016","09/06/2016","Sebastian Kurtek","OH","Ohio State University","Standard Grant","Gabor Szekely","08/31/2020","$120,000.00","Karthik Bharath","kurtek.1@osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269","","$0.00","Bayesian modeling and inference are commonly used statistical approaches to the analyses of complex high-dimensional data from many scientific fields including computer vision, biology, biometrics, bioinformatics and medicine. This research project is concerned with developing geometry-based, computationally efficient and scalable tools for Bayesian modeling of such datasets that have high potential for revealing novel insights. An example is a Bayesian model for statistical analysis of tumor heterogeneity in cancer with the possibility for improved disease characterization and new treatment approaches. The novelty and potential for high impact of this project come from the utility of an area of mathematics called differential geometry in the study of Bayesian statistical models and inferences. <br/><br/>While much progress has been made in the area of Bayesian modeling and inference both in terms of theory and computation, little attention has been given to studying the underlying geometry of such models. In this project, the PIs focus on developing a practical, unified Riemannian-geometric framework for three main problems: (1) Bayesian sensitivity analysis, (2) geometric variational inference, and (3) geometric nonparametric prior construction; these problems culminate in a fourth one of Bayesian density estimation, wherein the tools described in the first three can be used with obvious advantages. For a Bayesian model with prior, sampling and posterior densities, the geometric properties of the model are investigated and exploited through a square-root transformation, under which the nonlinear manifold of probability densities endowed with the nonparametric Fisher-Rao metric simplifies to the positive orthant of the unit sphere endowed with the Euclidean metric. Because the geometry of the sphere is well-known, important tools for analysis (e.g., exponential and inverse exponential maps, parallel transport, geodesics) are available in closed-form. As a result, this framework is versatile computationally and applicable to parametric, semiparametric and nonparametric Bayesian models. More importantly, it provides a formal mathematical background for defining distances between densities and developing geometrically calibrated measures. Thus, the two main contributions of this project are the development of (1) metric-based inferential methods for Bayesian models that may permit a more intuitive explanation of prior and posterior beliefs, and (2) a geometric quantification of various aspects of posterior inference through intrinsic analysis on the space of all probability densities."
"1613110","Robust Bayesian Analysis with Model Uncertainty for Massive Datasets","DMS","STATISTICS","09/01/2016","08/10/2017","Xinyi Xu","OH","Ohio State University","Continuing Grant","Gabor Szekely","08/31/2020","$250,000.00","Steven MacEachern","xinyi@stat.osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269","","$0.00","Two fundamental problems in Statistics, and in science more generally, are how to extract information from massive data sets and how to exploit this information to make predictions of future uncertain events.  By modeling and predicting market behavior, economists can better control financial risk; by modeling and predicting climate change, scientists can better manage the environment; by modeling and predicting health care needs, policy makers can better allocate resources to meet the needs.  Massive datasets bring a wealth of information, but this information is accompanied by a host of problems -- the most salient of which is varying data quality, ranging from ""good data"" to ""bad data"".  The proposed research will develop methods which provide robust, stable inference and prediction for massive datasets of varying quality.  The general methodology will be applicable to many fields, including environmental sciences, medical sciences and corporate decision analytics, where massive datasets are collected and robust predictive analysis is needed.  <br/><br/>Bayesian methods have enjoyed extraordinary success in the past two decades, and they are widely used throughout the scientific and corporate communities.  Their success has been driven by their unique ability to combine information from an experiment or observational study with extra-experimental information, expressed through the prior distribution.  However, this success hinges on the quality of the data and the quality of the prior.  The proposed research takes aim at these two issues.  For the first, the research develops formal predictive methods in the Bayesian framework which are suited to use with data of modest quality; for the second, the research develops Bayesian methods for model comparison and model averaging which provide robust and stable inference when little prior information is available.  The two portions of the project naturally combine to yield complete, cohesive Bayesian analyses with methods which are robust to model misspecification and outliers, and which require only modest prior information.  The project will provide several students with opportunities for training in research and data analysis, and much of this will be interdisciplinary in nature."
"1555244","CAREER:  Semiparametric and Machine Learning Approaches to Big Data Challenges in Precision Medicine","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2016","04/22/2022","Rui Song","NC","North Carolina State University","Continuing Grant","Yong Zeng","06/30/2023","$400,000.00","","rsong@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269, 8048","1045","$0.00","In the era of Big Data, the goal of better patient outcomes, coupled with lower cost and burden has generated tremendous interest in precision (or personalized, individualized) medicine, which is defined as treatments targeted to the needs of individual patients on the basis of genetic, biomarker, phenotypic, or psychosocial characteristics that distinguish a given patient from other patients with similar clinical presentations (Jameson and Longo, 2015).Precision medicine can be operationalized using individual's health-related metrics and environmental factors to discover individualized treatment regimes (ITRs); methodology for such discovery is an emerging field of statistics. The proposed methods are expected to bring a great impact to accelerate the discovery of new personalized treatment strategies. Therefore the proposed work is directly related with the White House Precision Medicine Initiative(https://www.whitehouse.gov/precision-medicine) as a research effort to revolutionize how to improve health and treat disease. The proposed methods are also general enough to be applied to a variety of data sources including clinical, biomarker, economic and financial data. If successful, the projects will greatly enhance the acquisition and analysis of large-scale data for the scientific and engineering communities.<br/><br/>The main objective of this proposal is to develop cutting-edge semiparametric methods and machine learning tools to realize the promise of precision medicine. Specifically, the PI aims to: develop flexible and efficient methods for discovering optimal ITRs (Aim 1); develop a general class of optimal ITRs (Aim 2); develop optimal ITRs with high-dimensional data (Aim 3); and develop optimal ITRs under population heterogeneity (Aim 4). The proposed work contributes to both semiparametric inference and machine learning fields. Machine learning methods have rarely been studied for doubly robust estimation and optimal ITRs with high-dimensional data. The theoretical developments including driving nonasymptotic distribution, risk bounds, new empirical process technical tools are challenging. The methodologies to be developed in this project will be fundamentally important and generally applicable for studying semiparametric models in high-dimensional setting. Using semiparametric and machine learning methods for precision medicine is an emerging novel area. The integration of research and education is a key aspect of this project. New courses on statistical learning and semiparametric inference will be developed. These courses will broaden the areas of specialized training in a department that has a strong history of attracting under represented groups. The PI is expecting to stimulate interests from a diverse group of researchers in numerous fields. The PI will also reach out to the K-12 education levels by training high school teachers."
"1651714","Workshop on Climate and Weather Extremes","DMS","STATISTICS, CR, Earth System Models","09/01/2016","08/03/2016","Benjamin Shaby","PA","Pennsylvania State Univ University Park","Standard Grant","Nandini Kannan","08/31/2017","$10,000.00","","bshaby@colostate.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269, 8012","1303, 7556, 8060","$0.00","The Workshop on Climate and Weather Extremes will be held in State College, PA, October 23-25, 2016. This workshop will serve two complementary purposes. The first is to introduce graduate students, as well as others wishing to enter into the area of extreme value analysis, to the techniques and challenges that are unique to studying rare events. The second is to provide a venue for statisticians and atmospheric and climate scientists to share ideas and facilitate collaboration aimed at understanding rare, high-impact climate and weather events.<br/><br/>The first day of the workshop will feature an introductory short course on the statistics of extremes, taught by Dan Cooley of Colorado State University. The following two days will feature research talks on statistical methodology and applications related to extremes. These talks will include several by researchers who are not statisticians by training but are deeply involved in studying climate and weather extremes and their impact. Finally, there will be a poster session on the evening of Monday, October 24.  For more information, please visit https://sites.psu.edu/statmos/."
"1613061","Summer Research Conference in Statistics and Biostatistics","DMS","STATISTICS","04/01/2016","03/10/2016","Arnold Stromberg","KY","University of Kentucky Research Foundation","Standard Grant","Gabor Szekely","03/31/2017","$21,315.00","","stromberg@uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","1269","7556, 9150","$0.00","This award supports graduate-student participation in the Summer Research Conference in Statistics and Biostatistics, held June 5-8, 2016 in Bentonville, Arkansas.  The meeting, part of an annual research conference series sponsored by the Southern Regional Council on Statistics, is designed to encourage the interchange and mutual understanding of research ideas in statistics and biostatistics, and to give motivation and direction to further research progress, particularly for young researchers, in a manner not ordinarily possible at other meetings. The meeting is particularly valuable for graduate students and faculty from smaller regional schools in the South. Each year, the location of the conference is chosen to be within driving distance for researchers in the southern region, thus affording them the opportunity to participate and interact closely with internationally-known leaders in the field without the cost of travel to distant venues. Speakers present formal research talks with adequate time allowed for clarification, amplification, and further informal discussions in small groups. Senior participants make themselves accessible for informal discussions of their research as well.<br/><br/>This year's Summer Research Conference centers on five topical areas: Measurement Error Models, Visual Analytics, Alzheimer's Disease Modeling, Survival Analysis, and Big Data Prediction Modeling.  In addition to two plenary speakers for the conference, each topical area features talks by one senior speaker and two junior speakers. A poster session for graduate students will be held on the second day. This award provides travel funding for graduate students who are making poster presentations and in need of support.  For more information, see the conference web page: <br/>www.stat.missouri.edu/~srcos/"
"1612985","Collaborative Research: New Directions in Multidimensional and Multivariate Functional Data Analysis","DMS","STATISTICS","07/01/2016","05/19/2016","Ka Wai Wong","IA","Iowa State University","Standard Grant","Nandini Kannan","06/30/2018","$150,000.00","","raywong@tamu.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","","$0.00","Functional data analysis, which deals with a sample of functions or curves, plays an important role in modern data analysis. Nowadays in the era of ""Big Data"", multidimensional and multivariate functional data are becoming increasingly common, especially in biological, medical, and engineering applications.  There are significant challenges posed by the very large dimension and complex structure of these data.  The proposed research will substantially narrow the gap between the increasing demand for handling such data in practice, and the insufficient development of statistical methods and computational tools.  This research has applications to neuroscience, climate science, and engineering.  It will provide scientists, engineers, and doctors with tools to help understand problems in their area, and enhance interdisciplinary collaborations.<br/><br/>This project offers a comprehensive research plan to advance the understanding and applicability of multidimensional and multivariate functional data.  The research will focus on the following three sub-projects: (1) Develop data-adaptive and interpretable representation of the covariance function for multidimensional functional data, (2) Develop a novel model-free procedure to detect dependency between components of multivariate functional data, and (3) Address the modeling and prediction of multivariate functional time series.  The resulting methods will be applied to neuroimaging and climate data.  The integration of these three sub-projects will foster creative directions and strategies for multidimensional and multivariate functional data."
"1566514","Shape Restrictions, Empirical Processes, and Semiparametric Models","DMS","STATISTICS","07/01/2016","05/22/2018","Jon Wellner","WA","University of Washington","Continuing Grant","Gabor Szekely","06/30/2020","$400,000.00","","jaw@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","","$0.00","Empirical process theory, which originated in the fundamental question of how to estimate a probability distribution from sample data, has developed into a powerful set of tools and unifying methods for the study of the properties of a wide range of statistical procedures. This research project aims to broaden and deepen these tools and to develop additional methods. It is anticipated that this theoretical work will enable progress in several application areas, including sampling designs used in epidemiology and clinical trials, as well as models used for HIV-AIDS data. The project also involves training of graduate students and development of graduate level courses. <br/><br/>The investigator plans to study new statistical methods for shape-restricted models in univariate and multivariate settings. The shape restrictions to be studied include monotone, convex, log-concave, and log-convex functions. He aims to develop new basic empirical process tools and methods and to apply the new tools to problems involving shape-restricted inference, semiparametric models, bootstrap inference methods, and problems in high-dimensional statistics. The investigator intends also to introduce and evaluate new methods of estimation in semiparametric models based on complex sampling designs with missing data by design, and for estimation in semiparametric models defined by shape constraints."
"1649847","Whole Genome Sequencing Analysis: Comprehensive Capture of Genetic Variants","DMS","STATISTICS","09/15/2016","09/08/2016","Xihong Lin","MA","Harvard University","Standard Grant","Nandini Kannan","08/31/2017","$28,499.00","","xlin@hsph.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269","7556","$0.00","The Program in Quantitative Genomics (PQG) at Harvard T.H. Chan School of Public Health is hosting the 2016 conference, ""Whole Genome Sequencing Analysis: Comprehensive Capture of Genetic Variants"", to be held  November 3-4, 2016 at the Joseph B. Martin Conference Center at Harvard Medical School in Boston, MA.  This is the tenth in a very successful conference series on emerging statistical and computational issues in genetics and genomics.  The explosion of massive information about the human genome, including Whole Genome Sequencing data, presents extraordinary challenges in data processing, integration, analysis, and interpretation of results.  A large number of Whole Genome Sequencing (WGS) samples are being generated by the community through the Genome Sequencing Program of the National Human Genome Research Institute, the TopMED Program of the National Heart, Lung and Blood Institute, and the Precision Medicine Initiative.  Analysis of WGS data requires integration of statistical genetics and genomics, computational biology and population genetics, and existing statistical and computational techniques are not directly applicable.  There is a critical need to discuss emerging quantitative issues at the forefront of scientific exploration, and to promote the development of innovative and scalable statistical and computational methods for analyzing massive whole genome sequencing data.  The conference is open to the whole research community and particularly encourages participation of junior faculty and researchers, postdoctoral fellows, students, and women and minorities.  The participants will discuss and critique existing quantitative methods, discuss in-depth emerging statistical and quantitative issues, and identify priorities for future research in the analysis of WGS data. The research presented will be broadly disseminated in publications in scientific journals and websites.  <br/><br/>The conference will focus on the following three topics of critical importance in whole genome sequencing analysis: (1) path to genomics; (2) scaling up phenotypes; (3) new horizons in population genetics.  The first topic discusses statistical and computational methods for rare variant analysis by incorporating functional and regulatory information. The second topic discusses analysis of multiple phenotypes to boost the power for association, and to understand how different phenotypes relate genetically and reveal causal pathways. The third topic discusses new opportunities in population genetics, and the use of this knowledge in understanding human disease biology and etiology.   A key feature of the conference is to provide a timely and interactive platform for cross-disciplinary senior and junior investigators, including statistical geneticists, computational biologists, population geneticists, genetic epidemiologists, molecular biologists, and clinical scientists, to discuss these analytic challenges for WGS data. For more information, visit https://www.hsph.harvard.edu/2016-pqg-conference/ ."
"1607840","New algorithms for consistent model selection beyond linear models","DMS","STATISTICS","09/01/2016","06/03/2018","Xuming He","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","08/31/2020","$300,000.00","","xmhe@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Statistical model building is an important part of scientific discovery. In the big data era, high dimensional data arise frequently.  Model selection in the presence of high dimensional features in the framework of linear models, generalized linear models, and models with censored data has been a very active area of research in recent years.  The PI aims to develop new algorithms for model selection, within a Bayesian computational framework, that are scalable for high dimensional problems.  The PI motivates the proposed research through collaborations with scientists in atmospheric sciences, genetics, and kinesiology, and aims to develop methodologies that are broadly applicable in statistical modeling and data analysis.  <br/> <br/>Much of the recent work has focused on shrinkage through penalization or regularization.  Bayesian computational methods, when interpreted broadly, play a valuable role in statistics, including model selection and estimation, but face important hurdles in high dimensional statistics, both in theoretical intricacy and in computational scalability.  The PI aims to develop a theoretical framework to demonstrate model selection consistency from the frequentist perspective, which offers interesting insights into why Bayesian model selection methods can provide an asymptotic approximation to the L0 penalty. An important part of the proposed work is the development of a modified Gibbs sampler in the selection of sparse models that is much more scalable than standard MCMC algorithms in the presence of high dimensional variables. The Bayesian methods are especially useful in problems with non-convex objective functions, where Bayesian computation methods can be more robust in performance than direct optimization. A primary application of such a problem considered in the project is quantile regression for censored data. In addition to model selection, the PI proposes a new estimation method for censored quantile regression that promises to be computationally and statistically efficient. Equally importantly, the new method adapts easily to general forms of censoring that other estimation methods have found difficult to handle. The PI will continue integrating research with education by working with PhD students and by providing research experiences for undergraduate students. The research output will be properly disseminated through conferences and workshops and through publication in widely read journals in statistical science."
"1612549","Efficient Algorithms with Statistical Guarantees for High Dimensional Time Series","DMS","STATISTICS","09/01/2016","08/30/2016","Ambuj Tewari","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","08/31/2019","$120,000.00","","tewaria@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Various industrial and scientific fields are generating high-dimensional time series data. Time series generated by macroeconomic indicators, stock market performance, and sensors in the industrial internet are of huge value to the economy. Time series generated in systems biology and neuroscience can lead to a better understanding of human health and diseases. Classical statistical methods are limited in their ability to deal with high-dimensional time series. Developing statistical tools for the analysis of high-dimensional time series is thus of pressing concern to society. This project develops both theory as well as scalable algorithms for advancing our ability to analyze and extract meaningful information from high-dimensional time series data.<br/><br/>This project will expand the frontier of research at the interface between three areas: high-dimensional statistics, large-scale optimization, and time series analysis. Three interrelated research aims will be pursued.  First, theoretical tools to analyze the performance of linear prediction methods for high-dimensional time series under mild assumptions about the generative process underlying the time series will be built. Second, statistical guarantees for computationally efficient alternating minimization based alternatives to computing the maximum likelihood estimator for high-dimensional vector autoregressive (VAR) models will be provided.  Third, fast online algorithms for estimation and forecasting in a setting where the high-dimensional time series observations arrive sequentially will be developed."
"1607965","Conference on Nonparametric Statistics-Integration of Theory, Methods and Applications","DMS","STATISTICS","06/01/2016","04/27/2016","Naisyin Wang","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","05/31/2017","$10,000.00","","nwangaa@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","7556","$0.00","The conference Nonparametric Statistics: Integration of Theory, Methods and Applications will be held at the University of Michigan, October 5-7, 2016. (See http://sites.lsa.umich.edu/npworkshop2016/.) The goal of the conference is to bring together researchers in nonparametric statistics, practitioners requiring these methods, and graduate students interested in this area, in a relaxed and stimulating atmosphere focused on research and applications. The development of nonparametric methods contribute to an essential part of modern data analysis and provide important research topics within statistics. Reduction or elimination of overfitting, uncontrollable biases and elevated variation through approaches supported by theoretical and numerical investigations have always been the key exercises in the field of nonparametric statistics. Exposure to and understanding of these methods are essential to practical applications and education of the next-generation statisticians. NSF funding will ensure that junior researchers are able to attend this conference.<br/><br/>The University of Michigan will host a conference on Nonparametric Statistics: Integration of Theory, Methods and Applications, October 5-7, 2016. Plenary speakers for the conference are Peter Bickel, Raymond Carroll, David Ruppert, and Jane-Ling Wang. The goal of the conference is to bring together researchers in nonparametric statistics, practitioners requiring these methods, and graduate students interested in this area, in a relaxed and stimulating atmosphere focused on research and applications. This award provides travel support for junior researchers to attend the conference."
"1608540","Several Problems in Dimension Reduction","DMS","STATISTICS","08/15/2016","05/22/2018","YANYUAN MA","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","07/31/2019","$180,000.00","","yzm63@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","Dimension reduction is almost an unavoidable practice in the modern era with big and complex data, and is the main topic in this proposal. Benefit from our success in this area, our research projects will further broaden the scope of the dimension reduction research domain and resolve several difficult, subtle and under-studied issues in this field. The results will generate wide interest and prompt applications in different fields where measurements may contain errors and/or where the data may not be completely random such as in medical studies. <br/><br/>The principal investigator (P.I.) will study five topics in dimension reduction problems, develop new methodologies and analyze their properties and performances. The first topic concerns two common conditions in dimension reduction: the linearity condition and the constant variance condition. The P.I. will reveal the true effect of these conditions, understand why the current implementation of these conditions are detrimental, prescribe a procedure to optimally utilize these conditions and show the resulting efficiency gain. This research will change the current practice of implementing these conditions. The second topic concerns central variance space estimation. The P.I. will treat the central variance space simultaneously with the central mean space and demonstrate why it is necessary to do so. She will consider both when the two spaces differ and when they coincide, and establish methods for estimation and inference in both cases. The third topic concerns multivariate responses. The P.I. will extend the central space concept to the general framework using a new way of modeling that serves as a middle ground between two current approaches. She will demonstrate the advantages of the new model and propose estimation and inference methods.  The P.I. will also establish the semiparametric efficiency bound for the multivariate response central space estimation,and illustrate why and how to conduct estimation to achieve local efficiency. The fourth topic concerns dimension reduction when covariates contain measurement errors. The P.I. will study a general semiparametric dimension reduction model when the covariate of interest is measured with error and modeled parametrically. A bias-correction procedure will be devised  which does not require modeling the unobservable variable distribution. The estimators will be shown to be consistent, asymptotically normal. The fifth topic concerns dimension reduction arising when analyzing case-control data in secondary analysis. The P.I. will illustrate that when multiple covariates are available, despite of a completely parametric modeling of the regression mean function,  the case-control nature of the data requires various nonparametric estimation  with multivariate covariates, hence leading to the need of reducing dimension. The inherent relation imposed by the original model further leads to dimension reduction with special structure, for which the P.I. will devise consistent estimators and establish their local efficiency and robustness against the misspecification of the regression error distribution."
"1613154","Collaborative Research: Tensor Envelope Model - A New Approach for Regressions with Tensor Data","DMS","STATISTICS, MSPA-INTERDISCIPLINARY, Modulation","09/01/2016","08/30/2016","Xin Zhang","FL","Florida State University","Standard Grant","Gabor Szekely","08/31/2019","$102,911.00","","henry@stat.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269, 7454, 7714","8007, 8091","$0.00","One of the most intriguing questions in modern science is to understand the human brain. In particular, scientists want to understand the differences between the brains of people with neurological disorders and those without. In brain imaging analysis, scientists collect data in the form of images that are used to compare the normal aging process to the development of neurological disorders. Through this project, the PIs seek to develop a toolkit comprised of a set of novel statistical methods, theories, and algorithms for the analysis of brain imaging data, as well as similar data that arise in a variety of scientific and business fields. The proposed research program is expected to make significant contributions on two fronts: timely responding to the growing needs and challenges of array data analysis, and providing a class of associated methodology that advances the statistical discipline. Research proposed in this project is to be disseminated through the investigators' close collaborations with the neuroscientists, as well as substantial educational and outreach activities.<br/><br/>Multidimensional array, or tensor, data are now frequently arising in a wide range of scientific and business fields. Aiming to address some of the most pressing questions in tensor data analysis, this research will integrate advanced statistical modeling devices with modern computational techniques to develop a set of novel tensor regression methods. Whereas there has been an enormous body of literature on high-dimensional regression analysis, nearly all work is with a vector response or predictor. Naively turning a tensor into a vector would result in ultrahigh dimensionality, destroy inherent structural information embedded in the tensor, and often render classical methods inadequate. This research will develop methods and tools for regression modeling of tensor responses or predictors, which both effectively tackles the high dimensionality and simultaneously preserves the tensor structure. Three sets of problems are to be investigated: (1) tensor response regression with envelope, aiming to address questions such as identifying brain regions exhibiting different activity patterns between the disease group and the general population after controlling for a set of potential confounding variables; (2) tensor predictor regression with envelope, aiming at questions of using brain images to diagnose neurodegenerative disorders and to predict onset of neuropsychiatric diseases; and (3) covariance matrix response regression with envelope, aiming to understand brain network alternations and building their associations with pathological phenotypes. The core idea underlying all of these aims is the adoption of a generalized sparsity principle and the development of a class of tensor envelope methods. The classical sparsity principle assumes a subset of individual variables are irrelevant, and various penalty functions are employed to induce such sparsity. By contrast, this generalized sparsity principle assumes linear combinations of variables are irrelevant, and the proposed envelope methods simultaneously identify and exclude such irrelevant information to achieve much improved estimation accuracy and efficiency."
"1622483","On Conditional Statistical Procedures for Simultaneous Model Selection, Inference, and Prediction in Complex Climate Systems","DMS","STATISTICS, CDS&E-MSS","09/01/2016","09/10/2018","Singdhansu Chatterjee","MN","University of Minnesota-Twin Cities","Continuing Grant","Christopher Stark","08/31/2020","$174,997.00","Scott StGeorge","chatt019@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269, 8069","1303, 8060, 8083, 9263","$0.00","This research project aims to develop new methodology for studying climate data as functions of time and space. Many such data series are oscillatory in nature but not strictly periodic, and the phenomena described vary in intensity at different points in time and in different parts of the globe. A thorough study of these irregular oscillatory patterns is of primary importance for planning of infrastructural needs, planning of sustainable development, and management of the planet's food, water, and energy resources.  Understanding of climate data is also essential for decision-making to address human, ecological, and environmental concerns, for better understanding of the physical process of climate systems, and for more accurate predictive systems. This project studies the mutual dependence of these oscillatory patterns and other climate variables. The results are expected to advance knowledge in evaluation of climate models, uncertainty quantification, and tropical cyclone behavior. Software development, a core component of this project, will benefit researchers working on related data analytic problems.  Students will be trained through involvement in the interdisciplinary research project.<br/><br/>The primary statistical challenges in modeling these irregular, multi-scale and multi-dimensional spatio-temporal processes will be addressed using a functional data approach. Computational techniques and theoretical machinery will be developed for simultaneous model selection and inference in functional data and in non-parametric and semi-parametric models involving such data. Computation-based procedures will be developed for testing goodness of fit of models for functional spatio-temporal data and for verifying technical assumptions about the nature of spatial or temporal dependency patterns. Bayesian and resampling-based inferential procedures will be developed and used in multiple datasets."
"1637436","Workshop on the Algorithmic, Mathematical, and Statistical Foundations of Data Science","DMS","STATISTICS, Special Projects - CCF","04/01/2016","03/23/2016","Xiaoming Huo","GA","Georgia Tech Research Corporation","Standard Grant","Nandini Kannan","03/31/2017","$99,998.00","Petros Drineas","xiaoming@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269, 2878","7556, 7796, 8084","$0.00","A workshop on the Algorithmic, Mathematical, and Statistical Foundations of Data Science will be held April 28-30, 2016 in Arlington, VA.  The event will bring together leading researchers in computer science, mathematics, and statistics to address foundational issues related to data science.  The objectives of the workshop are three-fold: (i) identify fundamental areas in the emerging discipline of Data Science where collaboration between computer scientists, mathematicians, and statisticians is necessary to achieve significant progress; (ii) Assess how collaboration between computer scientists, mathematicians, and statisticians could potentially contribute to workforce development by advancing and transforming the Data Science research training of Ph.D. students and post-docs; and (iii) Suggest different infrastructure modalities that could significantly promote and advance such collaborations.  The main deliverable of the workshop is a white paper that will serve as a guideline for professional societies and funding agencies.  <br/><br/>The rapid emergence of the Big Data phenomenon presents both opportunities and challenges.   While massive data may allow the generation of models and the design of algorithms that have improved inferential power, such models and algorithms may be less successful on modest-sized data sets. The challenge for researchers is to develop theoretical principles that will allow the scaling of inference and learning to massive-scale datasets, and algorithms that control errors even in the presence of heterogeneity in the data generation and data sampling processes.  These challenges will require collaborations between researchers representing theoretical computer science, mathematics, statistics, machine learning and data mining, and high performance computing. This award supports a workshop that brings together leaders from these communities of researchers to discuss the challenges and opportunities for collaborative work in this developing field."
"1613152","Computational and Communication Efficient Distributed Statistical Methods with Theoretical Guarantees","DMS","STATISTICS, GOALI-Grnt Opp Acad Lia wIndus","09/01/2016","08/13/2018","Xiaoming Huo","GA","Georgia Tech Research Corporation","Continuing Grant","Gabor Szekely","08/31/2019","$475,000.00","","xiaoming@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269, 1504","019Z","$0.00","In many contemporary data-analysis settings, it is expensive and/or infeasible to assume that the entire data set is available at a central location. In recent works of computational mathematics and machine learning, great strides have been made in distributed optimization and distributed learning (i.e., machine learning). On the other hand, classical statistical methodology, theory, and computation are typically based on the assumption that the entire data are available at a central location; this is a significant shortcoming in modern statistical knowledge. The statistical methodology and theory for distributed inference are underdeveloped. The PI will develop new distributed statistical methods that are computation and communication efficient. He will study the theoretical guarantees of these distributed statistical estimators. The applicability and need of these methods in a wide spectrum of application domains will be explored and demonstrated. This research can have impacts in healthcare, supply chain industries, retail and services, and many more. <br/><br/>Based on recent works in applied mathematics and machine learning, the PI is to explore theory, algorithms, and applications of statistical procedures that are developed for distributed data and aggregated inference (i.e., distributed inference), with considerations on the storage, computational complexity, and statistical properties of the relevant estimators. The project will develop practical models, statistical theory, and computationally efficient and provably correct algorithms that can help scientists to conduct more effective distributed data analysis. Statistical properties of these methods will be thoroughly studied, including analysis of asymptotic properties, simulation studies in finite sample cases, and establishment of effectiveness in some real applications. PhD students will be involved in the research. Course modules will be developed and made available publicly."
"1623541","Meetings of New Researchers in Statistics and Probability","DMS","STATISTICS","06/15/2016","06/14/2016","Alexander Volfovsky","OH","Institute of Mathematical Statistics","Standard Grant","Gabor Szekely","05/31/2020","$93,000.00","","alexander.volfovsky@duke.edu","9760 SMITH RD","WAITE HILL","OH","440949638","2162952340","MPS","1269","7556","$0.00","The University of Wisconsin will host a conference for new researchers in statistics and probability from July 28-30, 2016. (See http://www.stat.wisc.edu/imsnrc18/index.html.) The meeting is organized by the IMS Committee on New Researchers and held for junior researchers working in different areas of Statistics and Probability. The primary objective of the conference is to provide a much needed platform for interaction among new researchers, as well as opportunities to seek mentorship from leading researchers in the field. This conference series is explicitly aimed at training the future leaders and researchers in probability and statistics. This award provides partial funding for the 2016 conference, as well as for the 2017 and 2018 conferences.<br/><br/>The Committee on New Researchers of the Institute of Mathematical Statistics will hold its 2016 conference at the University of Wisconsin, July 28-30, 2016. Each participant will give a brief introduction to his/her research, as well as a poster presentation. Plenary talks will be given by established researchers. There will be panel discussions on teaching and mentoring, publishing, funding, and finding collaborators. NSF is providing funding to partially support this conference for the next three years."
"1613258","Scaling Summaries in Multiscale Domains with Applications","DMS","STATISTICS","09/01/2016","08/01/2018","Yajun Mei","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","08/31/2019","$150,034.00","","yajun.mei@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","","$0.00","In many scientific experiments, the observations often present themselves as noise, making traditional data analytic techniques inadequate.  The main objective of the proposed research is to develop and explore statistical models that produce informative scaling summaries for such noise-driven data with the goal of inference, classification, and prediction.  This is achieved with the help of wavelets, one of the most efficient multiscale tools.  Multiscale methodologies have evolved in the last two decades in disciplines ranging from theoretical statistics to geosciences.  Understanding the scaling in data will lead to significant insights when analyzing massive, multidimensional, noisy, and seemingly chaotic data sets.  The proposed research will impact various scientific fields that produce and utilize high-frequency data and images: most notably the areas of health diagnostics and atmospheric monitoring and prediction. In particular, the proposed methods will be applied to (1) breast cancer and lung cancer diagnostics by screening for the scaling features of digital mammograms and chest x-ray images, and (2) geoscientific analysis of turbulent atmospheric flows such as wind velocities, temperatures, and pollutant concentrations with the goal of modeling and prediction. The project will also contribute to the education and training of students through their deep engagement in the applications of novel techniques to problems from various scientific fields.<br/><br/>The proposed framework for an alternative assessment of scaling present in data utilizes statistical modeling in the domain of real and complex scale-mixing wavelet transforms. The novel scale-mixing hierarchies of wavelet subspaces succinctly describe ""fluxes-in-energy"" among the multivariate components in data.  Such descriptors will provide added insights and informative summaries in the form of monofractal and multifractal wavelet spectra and co-spectra, defined in a robust manner. The three scientific aims of the project include: (i) analyzing the properties of scale-mixing multidimensional wavelet coefficients for different decompositions (orthogonal, non-decimated, and wavelet packets), and investigating their relevance to scaling assessment, (ii) establishing theoretical properties of robust measures for regular and irregular scaling in non-standard multiscale domains, and (iii)  translating theoretical advances of the proposed research to applications in geosciences, bioinformatics, and medical diagnostics."
"1611901","Statistical Analysis of Complex, Highly-structured Functional Data","DMS","STATISTICS","08/15/2016","08/13/2016","Hongxiao Zhu","VA","Virginia Polytechnic Institute and State University","Standard Grant","Gabor Szekely","07/31/2020","$120,000.00","","hongxiao@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1269","8091","$0.00","Functional data, where the observational units represent curves, surfaces, or images, are becoming increasingly prevalent in modern measurement systems.  While most statistical methods focus on simplifying assumptions such as independence, real-world functional data often contain far more complex structures.  For example, in neuroimaging, the dependence network across brain regions needs to be characterized using graphs based on EEG or fMRI measurements; in proteomic and spectral analysis, researchers frequently encounter functional data with featured regions.  These structures represent generic complexities in data that cannot be handled by existing analytical tools.  This project focuses on the development of novel statistical theory and methods to model complex structures.  The proposed tools will be applied to datasets from neuroimaging, mass spectrometry, genomics, and bioinformatics.  The research will be integrated with various educational and outreach activities that will impact teaching and learning both within and beyond the university.<br/> <br/>The project has the following four interrelated objectives:  (1) Develop a functional graphical model framework to characterize conditional independent relationships between random functions, and apply the methods to estimate large-scale brain networks; (2) Develop novel frequentist regularization strategies and Bayesian priors to select regions of functional data; (3) Use Markov random fields to characterize spatial dependency between functions, and apply them to functional areal and functional point-reference data; and (4) Educate and engage students in the field of high-dimensional data analysis, develop K-12 outreach programs, and mentor minority students."
"1612970","New Covariate-Adjusted Response-Adaptive Designs and Associated Methods for Statistical Inference","DMS","STATISTICS","06/01/2016","05/22/2018","Feifang Hu","DC","George Washington University","Continuing Grant","Gabor Szekely","05/31/2020","$150,000.00","","feifang@gwu.edu","1918 F ST NW","WASHINGTON","DC","200520042","2029940728","MPS","1269","","$0.00","Precision medicine is an approach that allows physicians to tailor a treatment regimen based on an individual patient's characteristics (which could be biomarkers or other covariates). With today's modern technology, it is much easier to identify important biomarkers (usually called predictive biomarkers) that may associate with certain diseases and their treatments. To design an efficient clinical study of precision medicine, one should incorporate these useful predictive biomarkers. In this project, the investigator undertakes a systematic and comprehensive study of feasible designs and statistical inference related to clinical trials of precision medicine. The project will introduce and study innovative designs and statistical methods and will investigate how to implement the proposed designs and statistical methods in some real clinical trials. <br/><br/>The objective of this project is to introduce new designs and to provide novel statistical methods for adaptive randomized clinical trials of precision medicine. Recently, advances in genetics have permitted identification of genes (biomarkers) that are linked to certain diseases. These biomarkers (called predictive biomarkers) could be used to predict the response of a specific treatment and the confirmation of their predictive power relies on carefully designed clinical studies. To develop precision medicine, groundbreaking designs are needed for clinical trials so that predictive biomarkers can be incorporated to facilitate treatment selection. Advance and innovative statistical methods are required to match special features of clinical trials of precision medicine. This project focuses on the designs of clinical trials and the corresponding statistical inference for precision medicine. The investigator introduces innovative classes of covariate-adjusted response-adaptive designs, studies the statistical properties of these adaptive designs, develops new methods for statistical inference and obtains their properties, and applies these methods to practical problems."
"1620898","Collaborative Research: Analysis of longitudinal multiscale data in immunological bioinformatics --- Feature selection, graphical models, and structure identification","DMS","STATISTICS, CDS&E-MSS","08/01/2016","06/23/2016","Hua Liang","DC","George Washington University","Standard Grant","Christopher Stark","07/31/2021","$146,190.00","","hliang@gwu.edu","1918 F ST NW","WASHINGTON","DC","200520042","2029940728","MPS","1269, 8069","8083, 9263","$0.00","This project aims to develop a system of statistical analysis tools to tackle several important challenges in analysis of complex bioinformatics data, which involves a variety of response variables and tens of thousands independent variables. The interest often lies in identifying the key independent variables associated with the response variables, and understanding such associations as well as the interactions among the independent variables.<br/><br/>The extreme magnitude and complexity of bioinformatics data have posed serious challenges for data analysis. To overcome these challenges, we propose (i) to systematically and properly integrate multi-scale data before we can apply our novel modeling and analysis methods since the data we explore are collected by numerous independent studies at phenotypic, cellular, protein, and genetic levels with information from very different time and dimension scales; (ii) to develop feature screening criteria for a mixed type of longitudinal data using the combination of correlation tests in bivariate longitudinal regression models and the Benjamini-Hochberg-Yekutieli procedure, (iii) to develop graphical models that allow the variables being a mix of continuous and discrete longitudinal variables, with the nodes representing variables and each edge indicating the dependence of the two relevant variables conditional on the other variables; and (iv) to investigate the functioning form of each predictor by resorting to the data themselves under the framework of a mixed effects regression model with a continuous or discrete response and a high dimensional vector of predictors, with the resulting procedure allowing a user to simultaneously determine the form of each predictor effect to be zero, linear or nonlinear."
"1613261","Random Dynamical Systems and Limit Theorems for Optimal Tracking","DMS","PROBABILITY, APPLIED MATHEMATICS, STATISTICS","06/01/2016","05/23/2016","Kevin McGoff","NC","University of North Carolina at Charlotte","Standard Grant","Tomek Bartoszynski","05/31/2020","$225,000.00","Andrew Nobel, Shayn Mukherjee","kmcgoff1@uncc.edu","9201 UNIVERSITY CITY BLVD","CHARLOTTE","NC","282230001","7046871888","MPS","1263, 1266, 1269","","$0.00","Dynamical systems serve as important mathematical models for a wide variety of physical phenomena, arising in such areas as weather modeling, systems biology, and statistical physics. A dynamical system consists of a state space, in which a point represents a complete description of the state of the system, and a rule governing the evolution of the system from one state to another. This project focuses on the long-term behavior of such systems from two complementary points of view. From the first point of view, the project seeks to describe the behavior of typical systems when the rules of evolution are chosen at random. Such results shed light on what properties one might expect to find in disordered systems. The second point of view, the ""inverse problem,"" concerns the statistical problem of recovering some information from the observation of a dynamical system. While there are many examples of dynamical systems being used as mathematical models, and there is a large statistical literature regarding inference and estimation, the performance of statistical procedures when applied to data generated by nonlinear dynamical systems is poorly understood. This project focuses on characterizing when traditional statistical procedures may be effectively applied in the context of dynamical systems. Beyond the very fertile potential applications, the project will also have broader impact on training of graduate students who will acquire invaluable skills in sound probabilistic modeling and statistical inference by working on the project's research topics.<br/><br/>Symbolic dynamical systems, which may be defined in terms of discrete constraints on the possible trajectories, serve as prototypical models of systems that evolve over time. Random ensembles of these systems may be produced by selecting the constraints at random. Ideas from both discrete probability and dynamical systems may then be used to analyze the structural properties of the resulting systems with high probability. For the inverse problem, this project seeks to evaluate the performance of several statistical inference procedures, both frequentist and Bayesian, when the models involved are dynamical systems. Fundamental questions about convergence and consistency of the procedures may be addressed using tools from ergodic theory, such as joinings and the thermodynamic formalism."
"1564395","FRG: Collaborative Research: Innovations in Statistical Modeling, Prediction, and Design for Computer Experiments","DMS","STATISTICS","07/01/2016","08/05/2021","Thomas Santner","OH","Ohio State University","Continuing Grant","Pena Edsel","06/30/2022","$354,755.00","","santner.1@osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269","1616","$0.00","The explosive growth in the use of computer simulators in the last fifteen years has helped galvanize a revolution in scientific, engineering, and biological research that includes advances in the aerospace industry, material science, renewable energy, and biomechanics. Researchers can make a detailed exploration of scientific design alternatives under a wide set of operating environments using runs from a simulator of a physical system, possibly coupled with those from a traditional physical system experiment. This research project will advance the statistical modeling, design, and analysis of experiments that use computer simulators. The first research area is Improved Modeling of Simulator Output: The investigators will develop flexible stochastic models that will allow more accurate prediction in settings where the simulator provides related multivariate output of the performance of a physical system. Current prediction models either assume output independence (knowledge of one output gives no information about other outputs) or a linear dependence on a common set of latent drivers. The second research area concerns Advances in Emulation: The investigators aim to devise efficient emulators of simulator output for novel input and output settings such as when gradient information is available or when the output consists of both point and integrated measures. They plan to construct predictors that incorporate natural invariances present in the simulator output. For example, the predicted response should be constant under permutations of the inputs when the output satisfies this condition; the project will quantify the uncertainty in the invariant predictors. The investigators also plan to quantify the uncertainty of a recent, theoretically-justified method of calibrating computer simulators based on physical experimental data. The third research area is the Design of Simulator Experiments: Efficient designs of simulator experiments will be devised to minimize the computational effort required to determine the sensitivity of a simulator output to each of its inputs. <br/><br/>This research will build a statistical framework for the modeling, design and analysis of experiments that employ computer simulators. The specific goals are (1) to devise flexible interpolating stochastic models for computer simulators with multivariate output; (2) to invent efficient predictors for novel input and output settings such as when gradient information is available or when the output consists of both point and integrated responses; (3) to develop emulators of simulator output that incorporate the same invariances present in the simulator responses; (4) to quantify the uncertainty of L2 calibrated predictors for expensive computer codes; and (5) to construct new sliced Latin hypercube designs to allow the efficient calculation of global sensitivity indices. The investigators will develop new modes for training statistics graduate students having interests in engineering applications. Opportunities will be created for subject matter specialists to provide critical practical challenges in three areas: aerospace/mechanical engineering, biomechanics, and material science, and to conduct joint applied projects with the researchers."
"1611791","Collaborative Research: New statistically-motivated solutions to classical inverse problems","DMS","STATISTICS","08/01/2016","05/24/2016","Ryan Martin","IL","University of Illinois at Chicago","Standard Grant","Yong Zeng","03/31/2017","$124,354.00","","rgmarti3@ncsu.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","","$0.00","Numerous scientific questions assume the form of inverse problems, in which an unknown input to a system under study gives rise to an observed noisy output, and the goal is to estimate the input from the output. An example of such a problem is calculating the density of the Earth from measurements of the local gravitational field.  Only rarely can such inverse problems be solved analytically, and in general numerical approximations are required to find solutions. In this research project, the investigators aim to introduce a novel iterative algorithm for solving inverse problems, develop its theoretical and computational properties, and establish its performance in applications.  It is anticipated that the new algorithm will be adaptable to a range of problems currently under investigation in applied and numerical mathematics, for example in solving a sparse system of linear equations, currently of great interest in areas including tomography, archaeology, astrophysics, and other sciences.<br/><br/>This research project explores a novel iterative algorithm for the solution of a class inverse problems that includes Fredholm integral equations of the first kind, Laplace transform inversion, mixing distribution estimation in statistics, and solving sparse systems of linear equations. The investigators plan to (i) introduce a novel iterative algorithm for solving inverse problems of these types, and perhaps others, (ii) develop its theoretical and computational properties, and (iii) establish its performance in applications. A motivation for the research is the statistical problem of estimating a mixing density in a nonparametric mixture model. To date, there are no general algorithms that produce globally consistent estimators of the mixing density, in the sense of almost sure convergence with respect to a strong metric; only weak convergence results are available. An important feature of the algorithm under development is that, if it is initialized at a smooth density function, then the estimator is necessarily also a smooth density function. Other algorithms designed by numerical analysts for solving these inverse problems do not have this closure property. The form of the novel iterative algorithm, along with the fact that it yields smooth density estimators, suggests that this open problem can be solved; the investigators aim to establish a general global consistency result and demonstrate rates of convergence."
"1548520","U.S. Participation in Newton Institute Program on Stochastic Dynamical Systems in Biology:  Numerical Methods and Applications","DMS","APPLIED MATHEMATICS, STATISTICS, COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","01/01/2016","07/29/2015","Samuel Isaacson","MA","Trustees of Boston University","Standard Grant","Junping Wang","12/31/2016","$23,530.00","","isaacson@math.bu.edu","1 SILBER WAY","BOSTON","MA","022151703","6173534365","MPS","1266, 1269, 1271, 7334","7556, 9263","$0.00","From January 4 through June 24, 2016, the Isaac Newton Institute for Mathematical Sciences  at Cambridge University in the UK will  host a thematic program on Stochastic Dynamical Systems in Biology: Numerical Methods and Applications. Stochastic dynamical systems are prevalent throughout biology and medicine, and include many key processes in gene regulation, cell signaling, tissue development, and cancer proliferation. Mathematical and numerical methods are key tools in understanding both the function of these systems, and how to control and manipulate their behavior. The program will bring together a diverse set of international researchers from across the mathematical and biological sciences, each working on the design and application of such methods. The program offers a unique opportunity to exchange ideas and work collaboratively in order to tackle the many challenging open problems of the field by enabling a large number of researchers to participate for extended periods of time. To introduce graduate students and postdoctoral fellows to the field, the opening workshop will offer tutorials and plenary talks focused on providing a broad overview of current research  problems. This grant provides travel funding for early career researchers and graduate students from the United States to make extended visits during the program and to participate in the program workshops.<br/><br/>It has become clear that to effectively understand stochasticity in biological systems, a combination of modern numerical analysis, estimation and sampling techniques, and rigorous analysis of stochastic dynamics is required. To highlight open problems and current work in these areas, the Isaac Newton Institute for the Mathematical Sciences is running a thematic program on Stochastic Dynamical Systems in Biology: Numerical Methods and Applications. The program will include an introductory, tutorial-based workshop and three research workshops, broadly focused on: multi-scale methods; well-mixed systems; and spatially distributed systems. In addition, each week of the program twenty to thirty long-term participants will be in residence, each visiting the institute for at least two weeks. The interactions between these researchers are expected to advance the field through the development of new cross-disciplinary collaborations. In the mathematical sciences these collaborations will lead to new mathematical techniques and numerical methods for studying ordinary and partial differential equations, discrete and continuous-time stochastic processes, and a variety of statistical models. To enable the broader scientific community to benefit from the program, all lectures and seminars will be available to view through the  institute website (with the speaker's permission). This grant specifically supports the travel costs to visit the institute and engage in the activities of the program of several early career and trainee mathematical scientists from the United States. Further information on the  program is available on the website, http://www.newton.ac.uk/event/sdb."
"1743054","Advanced Statistical Tools for Ultra-High Dimensional Functional Data with Spatial-Temporal Correlation","DMS","STATISTICS","06/01/2016","04/27/2017","Hongtu Zhu","TX","University of Texas, M.D. Anderson Cancer Center","Continuing Grant","Gabor Szekely","05/31/2018","$167,150.00","","htzhu@email.unc.edu","1515 HOLCOMBE BLVD","HOUSTON","TX","770304000","7137923220","MPS","1269","","$0.00","This project concerns developing innovative advanced statistical tools for the analysis of ultra-high dimensional functional data with spatial-temporal correlation. The primary motivating application is neuroimaging analysis, relevant to the BRAIN Initiative. (However, the developed methods and theory are applicable to a much broader range of fields involving spatial-temporal modeling.) The research program has a strong multidisciplinary collaborative component, with key team members drawn from biostatistics/statistics, computer science, psychiatry, radiology, and psychology. The tools and software under development can have immediate impacts in clinical research, and have wider applications in medical studies of HIV/AIDS, major neuropsychiatric and neurodegenerative disorders, normal brain development, and cancer, among many others. The problems addressed are also of broad interest to general society, since they relate to pressing issues such as health care policies and social security planning.<br/> <br/>With modern imaging techniques, many large-scale studies have been or are being widely conducted to collect a wealthy set of functional data and clinical data. Functional data share four common and important features: (i) extremely high dimensional, (ii) piecewise smooth, (iii) temporally, and (iv) spatially dependent. The analysis of such data and integration of them with clinical data have been hindered by lack of effective statistical tools and theory, underscoring the great need for methodological and theoretical development from a statistical perspective.  The project addresses challenges from three broader perspectives in both time and frequency domains. The first perspective develops spatial-temporal models for adaptive function estimation. The models can effectively extract informative markers from noisy functional data. The second perspective concerns reduced rank models for groups of functional data with spatial-temporal correlation. In addition, the third perspective develops advanced functional mixed effects models for modeling varying association function between repeated functional responses and a set of covariates of interest, while accounting for complex spatial-temporal correlation."
"1612965","Dynamic Prediction of Time to Next Failure Event","DMS","STATISTICS","09/01/2016","08/14/2018","Xuelin Huang","TX","University of Texas, M.D. Anderson Cancer Center","Continuing Grant","Gabor Szekely","08/31/2019","$179,992.00","Ruosha Li","xlhuang@mdanderson.org","1515 HOLCOMBE BLVD","HOUSTON","TX","770304000","7137923220","MPS","1269","","$0.00","In manufacturing and industrial applications, it is critical to regularly inspect all the components of a working system, record data during the inspection, use these data to predict the next potential failure event, and take preventive actions.  In medical practice and research, the health status of patients is measured repeatedly over time during post-treatment follow-up visits.  During each visit, new information is obtained and physicians use that information to predict a patient's prognosis and design an appropriate treatment plan.  These applications involve the use of current information to predict the time until the next failure event (such as disease progression). These continuously updated predictions, called dynamic predictions, are critical for patients with non-curable diseases such as cancer or AIDS. This project will develop and apply modern statistical techniques to extract useful features from massive data sets collected over time, and then use these features to conduct predictions as accurately as possible. When these statistical methods are built into a computer software program, they can be used online to conduct predictions. Patients and physicians can use such programs to evaluate disease progression and to make early decisions about treatment and prevention. Industrial engineers can use such programs to forecast a potential system failure and initiate maintenance. Commercial web sites can collect customers' reaction data online and then apply such methods to better predict customers' needs and improve sales and customer satisfaction.<br/><br/>Many statistical methods assume that longitudinal data trajectories follow parametric models, linear or nonlinear. However, the pattern of longitudinal data trajectories differs in each specific setting, making it difficult to identify a satisfactory parametric family that is suitable for all situations.  Based on this consideration, a functional principal component analysis (FPCA) approach is used to capture the longitudinal data structures and functional patterns. The first goal of this project is to decompose biomarker trajectories into some feature functions, and then incorporate these features as covariates in the Cox proportional hazards model to make dynamic predictions.  Given that the proportional hazards assumption may be too restrictive in some cases, the second goal of this project is to conduct dynamic prediction for the quantile functions of the residual event time under a flexible framework.  The residual lifetime quantile regression model facilitates a meaningful interpretation and offers more direct answers than the Cox model. The third goal of this project is to develop analytic and visualization tools for identifying longitudinal data trajectory patterns prior to a failure event by looking at them backwards in time and aligning them with the failure events. Discerning these patterns can greatly facilitate dynamic prediction of the imminent failure event. The proposed methods are specially designed to handle the complications of censored data, irregular follow-up times and dynamically collected data to facilitate prediction over a range of time points."
"1613176","The Fifth  Workshop on Biostatistics and Bioinformatics","DMS","STATISTICS","09/15/2016","06/02/2016","Yichuan Zhao","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Nandini Kannan","08/31/2018","$5,000.00","","yichuan@gsu.edu","58 EDGEWOOD AVE NE","ATLANTA","GA","303032921","4044133570","MPS","1269","7556","$0.00","The Fifth Workshop on Biostatistics and Bioinformatics will be held on the campus of Georgia State University, Atlanta, May 5-7, 2017.  The objective of the workshop is to reflect new developments in the frontiers of biostatistics and bioinformatics.  The conference will cover a broad range of current research topics, including high-dimensional and big data analysis, high-throughput genomics data analysis, survival analysis, and empirical likelihood methods.  The invited speakers will include leading experts and outstanding junior researchers. The workshop will provide opportunities for the exchange of new ideas and genesis of research collaborations.  Graduate students and postdocs will gain valuable research experience by attending the workshop and making poster presentations. <br/><br/>This award supports the participation of approximately 12 graduate students and junior faculty.  The conference organizers are making special efforts to provide travel support for a diverse group of researchers to attend the workshop.  Participants from underrepresented groups are encouraged to attend and apply for travel support. For detailed information, please visit the website: http://math.gsu.edu/~yichuan/2017Workshop/."
"1613173","Understanding Regression Heterogeneity Through Joint Estimation of Conditional Quantiles","DMS","STATISTICS","08/01/2016","08/01/2016","Surya Tokdar","NC","Duke University","Standard Grant","Gabor Szekely","07/31/2019","$149,979.00","","st118@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","","$0.00","In many data-driven scientific investigations, the primary goal is to understand the relationship between a response variable and a set of predictors.  Standard statistical techniques attempt to detect and quantify the nature of such relationships through changes in the average response.  However, in the real world, predictor-response relationships are often more complex and nuanced.  In disciplines like climate science, ecology, economics, public health and sociology, investigators are often interested in understanding changes to the extreme response percentiles.  Additional insights are gained by quantifying how the rate of change varies as one moves from the average to the extremes.  This project aims to develop sophisticated and theoretically sound statistical tools that can answer these questions from large and complex data sets. <br/><br/>Statistical tools are sought within the recently popularized modeling framework of linear quantile regression.  The proposed framework expands the scope of linear quantile regression to scenarios where the response variables exhibit additional dependency.  Such dependency manifests in many common situations, e.g., when one simultaneously measures multiple response variables per observation unit, when a response is measured repeatedly over time, or, when data is drawn from a network of individuals.  Standing between the promise of quantile regression and its wider applicability is the lack of a proper model-based estimation framework. The PI has recently introduced a modeling framework that leads to Bayesian or penalized likelihood based joint estimation of linear quantile planes over arbitrary predictor spaces. Proposed model extensions augment this framework with autoregressive and copula formulations to address various kinds of structural dependency between the observation units. The project will develop efficient inference algorithms using advanced Bayesian techniques based on stochastic computation, and public, open source software in the form of R packages.  Software development will incorporate possible leveraging of distributed computing architectures to render scalability to big data. For all model extensions, the PI will also carry out sharp analyses of theoretical guarantees on model performance by working out the large sample distribution theory of Bayesian parameter estimates."
"1650520","2017 Quality and Productivity Research Conference - Quality and Statistics: Path to a Better Life","DMS","STATISTICS","09/01/2016","08/09/2016","Nalini Ravishanker","CT","University of Connecticut","Standard Grant","Yong Zeng","08/31/2017","$21,000.00","","nalini.ravishanker@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","7556","$0.00","The 2017 Quality and Productivity Research Conference Quality (QPRC) and Statistics: Path to a Better Life is held during June 13-15, 2017, preceded on June 12 by a short course titled Computational Bayesian Methods for Big Data Problems. QPRC 2017 is an international conference which serve as the annual conference of the Quality and Productivity Section of the American Statistical Association (ASA) in bringing together academics and professionals to disseminate and discuss latest advances in statistical approaches relevant to quality and productivity. The conference and the short course will be especially valuable for research oriented graduate and undergraduate students, as well as junior researchers and professionals. In the long tradition of successful annual QPRCs, the goal of QPRC 2017 is to provide its participants - statisticians and inter-disciplinary professionals ? an opportunity to meet and exchange ideas related to a broad set of topics in the area of quality and productivity, especially on topics relevant to big data in various areas. The NSF supports the participation (registration, travel and accommodation) for about 30 research oriented students with special consideration to women and minority students - graduate students, or senior or junior undergraduate students - from US academic institutions. Participants receiving support are strongly encouraged to actively take part in the conference by presenting a poster, in addition to attending the short course and attending software demos to be provided by vendors. These students are expected to greatly benefit from participating in this conference and attending the short course, and additionally have an opportunity to disseminate their research and to develop strong collaborations. <br/><br/>The main objective of the conference is to bring together both well established and emerging young researchers from academia, industry and government, who are actively pursuing theoretical, methodological, and computational research in the areas of quality and productivity, in order to discuss their research and in particular, their applications in various related fields. The focus topics include, but are not limited to, Bayesian Methods and Computing, Data Mining, Dynamic Modeling, Experimental Design, Health Analytics, Nonparametric methods, Reliability, Stochastic Modeling, Time Series, Variable Selection, etc.  There are three plenary sessions and several interesting invited and contributed talk sessions on these topics. The conference provides an opportunity for all attendees, and especially for junior faculty, graduate students, and undergraduate students to showcase their research, and to also benefit from interactions with senior researchers, leading to acceleration and enhancement of their research. A rapid round precedes the poster session, and each presenter has an opportunity to showcase his/her work in 3 minutes. The short course especially illustrates the importance of techniques that address modeling, methods, and computing for big data (both big n and big p problems). More details about the conference and the short course can be found at the conference website www.qprc2017.org."
"1612625","Collaborative proposal: Variable Selection in the high dimensional, low sample size setting -- Beyond the Linear Regression and Normal Errors Model","DMS","STATISTICS, MSPA-INTERDISCIPLINARY, Systems and Synthetic Biology","08/15/2016","08/19/2016","Haim Bar","CT","University of Connecticut","Standard Grant","Gabor Szekely","07/31/2019","$150,000.00","","haim.bar@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269, 7454, 8011","7465, 8007","$0.00","Revolutionary new technologies are producing high-throughput biological data at a resolution that was unthinkable only a decade ago. These new forms of data pose enormous challenges and opportunities for statisticians and computer scientists. This project develops new sophisticated statistical methods and computational algorithms for analyzing and integrating complex high-dimensional data. The work is motivated by collaborations with leading biological scientists at Cornell-Ithaca and Weill Cornell Medical College working in diverse research areas including plant biology, nutrition, neurology, cancer epigenomics, and veterinary medicine. <br/><br/>The goal of this project is to develop new statistical models and computational algorithms for high-dimensional, low sample size, high-throughput biological data, including new methods for the analysis of microarrays, the identification of quantitative trait loci, association mapping, label-free shotgun proteomics and metabolomics. The proposed methods involve innovative extensions of modern statistical building blocks, including the use of random effects for regularization, shrinkage estimation, Bayesian statistics, and mixtures for posterior classification and prediction. Novel modifications of the expectation-maximization algorithm are proposed for scalable and efficient model fitting and inference."
"1613295","Integrative Multivariate Analysis of Multi-View Data","DMS","STATISTICS","08/15/2016","08/04/2016","Kun Chen","CT","University of Connecticut","Standard Grant","Gabor Szekely","07/31/2020","$150,000.00","","kun.chen@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","","$0.00","Multi-view data, or the measuring of several distinct yet interrelated sets of characteristics pertaining to a single set of subjects and possibly collected from an array of sources, has become increasingly common in the fields of engineering and scientific research. This project innovates new methodologies, statistical theories, and scalable computational tools to tackle a range of statistical learning problems with multi-view data. An integrated statistical analysis of the multi-view data generation mechanisms, enabled by this project, will allow us to gain extraordinary insight of real-world phenomena by utilizing information obtained from different lenses and from different angles. <br/><br/>The PI will develop several generalizations of the reduced-rank matrix structure, to enable a spectrum of multivariate statistical methods for multi-view learning. The general methodology of reduced-rank estimation is one of the most critical ingredients in modern multivariate analysis. However, for handling multi-view data, the potential of the reduced-rank methodology is far from being fully realized or understood. This project presents the following overarching objectives: (1) develop integrative multivariate regression for joint learning, which entails the exploitation of multiple sets of features to build an integrated predictive model of multivariate response; (2) develop integrative canonical correlation analysis for shared learning, by combining the exploration of shared low-dimensional association structures between multiple sets of features and the development of coherent predictive models for multivariate response; (3) develop integrative dimension reduction for multi-scale learning, by utilizing both the global and local low-dimensional structures among sub-matrices of a high-dimensional matrix object; (4) develop diagnostic measures for robust learning, which would enable reliable multi-view data integration and data quality assessment."
"1613026","Computer-Intensive Methods for Nonparametric Analysis of Dependent Data","DMS","STATISTICS","07/01/2016","06/26/2018","Dimitris Politis","CA","University of California-San Diego","Continuing Grant","Gabor Szekely","06/30/2019","$249,955.00","","dpolitis@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","","$0.00","Ever since the recognition of the role of the computer in modern statistics, the bootstrap and other computer-intensive statistical methods have been developed extensively for inference with independent data. Such methods are even more important in the context of dependent data, where the distribution theory for estimators and test statistics may be difficult or impractical to obtain. Furthermore, the recent information explosion has resulted in datasets of unprecedented size that call for flexible, nonparametric, computer-intensive methods of data analysis. Time series analysis in particular is vital in many diverse scientific disciplines, including economics, engineering, acoustics, geostatistics, biostatistics, medicine, ecology, forestry, seismology, and meteorology. This research project aims to develop efficient and robust methods for the statistical analysis of dependent data that will enable more accurate and reliable inferences to be drawn from datasets of practical import, resulting in appreciable benefits to society. Examples include data from meteorology/atmospheric science (e.g. climate data), economics (e.g. stock market returns), biostatistics (e.g. fMRI data), and bioinformatics (e.g. genetics and microarray data). <br/><br/>The project focuses on the development of methods of inference for the analysis of time series and random fields that do not rely on unrealistic or unverifiable model assumptions. In particular, the investigator and colleagues are working on: (a) Markov-type resampling and linear process bootstrap for stationary random fields; (b) local block bootstrap for inference with inhomogeneous marked point processes; (c) estimation of the degree of smoothness and support of the common density of stationary data; (d) improved nonparametric estimation via the use of flat-top kernels; (e) a bootstrap test for the null hypothesis of time series ""over-differencing;"" (f) seasonal block bootstrap for almost-periodic data; (g) model-free point predictors and prediction intervals for locally stationary time series; (h) smooth estimation of time-varying covariance matrices for locally stationary multivariate time series; and (i) different aspects of resampling with functional data, including the difficult open problem of appropriately studentizing a functional statistic."
"1612891","Collaborative Research:  New stochastically-motivated solutions to classical inverse problems","DMS","STATISTICS","08/01/2016","05/24/2016","Stephen Walker","TX","University of Texas at Austin","Standard Grant","Gabor Szekely","07/31/2019","$125,645.00","","s.g.walker@math.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1269","","$0.00","Numerous scientific questions assume the form of inverse problems, in which an unknown input to a system under study gives rise to an observed noisy output, and the goal is to estimate the input from the output. An example of such a problem is calculating the density of the Earth from measurements of the local gravitational field.  Only rarely can such inverse problems be solved analytically, and in general numerical approximations are required to find solutions. In this research project, the investigators aim to introduce a novel iterative algorithm for solving inverse problems, develop its theoretical and computational properties, and establish its performance in applications.  It is anticipated that the new algorithm will be adaptable to a range of problems currently under investigation in applied and numerical mathematics, for example in solving a sparse system of linear equations, currently of great interest in areas including tomography, archaeology, astrophysics, and other sciences.<br/><br/>This research project explores a novel iterative algorithm for the solution of a class inverse problems that includes Fredholm integral equations of the first kind, Laplace transform inversion, mixing distribution estimation in statistics, and solving sparse systems of linear equations. The investigators plan to (i) introduce a novel iterative algorithm for solving inverse problems of these types, and perhaps others, (ii) develop its theoretical and computational properties, and (iii) establish its performance in applications. A motivation for the research is the statistical problem of estimating a mixing density in a nonparametric mixture model. To date, there are no general algorithms that produce globally consistent estimators of the mixing density, in the sense of almost sure convergence with respect to a strong metric; only weak convergence results are available. An important feature of the algorithm under development is that, if it is initialized at a smooth density function, then the estimator is necessarily also a smooth density function. Other algorithms designed by numerical analysts for solving these inverse problems do not have this closure property. The form of the novel iterative algorithm, along with the fact that it yields smooth density estimators, suggests that this open problem can be solved; the investigators aim to establish a general global consistency result and demonstrate rates of convergence."
"1613018","Collaborative Research: New Directions in Multidimensional and Multivariate Functional Data Analysis","DMS","STATISTICS","07/01/2016","05/19/2016","Xiaoke Zhang","DE","University of Delaware","Standard Grant","Nandini Kannan","06/30/2018","$137,981.00","","xkzhang@gwu.edu","220 HULLIHEN HALL","NEWARK","DE","197160099","3028312136","MPS","1269","9150","$0.00","Functional data analysis, which deals with a sample of functions or curves, plays an important role in modern data analysis. Nowadays in the era of ""Big Data"", multidimensional and multivariate functional data are becoming increasingly common, especially in biological, medical, and engineering applications.  There are significant challenges posed by the very large dimension and complex structure of these data.  The proposed research will substantially narrow the gap between the increasing demand for handling such data in practice, and the insufficient development of statistical methods and computational tools.  This research has applications to neuroscience, climate science, and engineering.  It will provide scientists, engineers, and doctors with tools to help understand problems in their area, and enhance interdisciplinary collaborations.<br/><br/>This project offers a comprehensive research plan to advance the understanding and applicability of multidimensional and multivariate functional data.  The research will focus on the following three sub-projects: (1) Develop data-adaptive and interpretable representation of the covariance function for multidimensional functional data, (2) Develop a novel model-free procedure to detect dependency between components of multivariate functional data, and (3) Address the modeling and prediction of multivariate functional time series.  The resulting methods will be applied to neuroimaging and climate data.  The integration of these three sub-projects will foster creative directions and strategies for multidimensional and multivariate functional data."
"1647351","Workshop on Multidisciplinary Complex Systems Research","DMS","Science of Learning, Genetic Mechanisms, APPLIED MATHEMATICS, STATISTICS, FD-Fluid Dynamics, DMR SHORT TERM SUPPORT, Perception, Action & Cognition, MATHEMATICAL BIOLOGY, Robust Intelligence, Dynamics, Control and System D, EPCN-Energy-Power-Ctrl-Netwrks, Integrat & Collab Ed & Rsearch","08/15/2016","02/06/2017","Kimberly Gray","IL","Northwestern University","Standard Grant","Michael Steuerwalt","01/31/2018","$99,998.00","Richard Murray, Abhijit Deshmukh","k-gray@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","004Y, 1112, 1266, 1269, 1443, 1712, 7252, 7334, 7495, 7569, 7607, 7699","155E, 7556","$0.00","This award will support the Multidisciplinary Complex Systems Research Workshop to be held at the National Science Foundation in Arlington, Virginia, September 12-14, 2016. The workshop will bring together a diverse group of experts in complementary areas of complex systems and will be preceded by a series of weekly webinars. The overarching goal of the activity is to address scientific issues that are relevant to the scientific community and bring to surface possible areas of opportunity for multidisciplinary research in the study of complex systems. The specific goals of the workshop include: 1) identifying the most substantive research questions that can be addressed by fundamental complex systems research; 2) recognizing community needs, knowledge gaps, and barriers to research progress in this area; 3) identifying future directions that cut across disciplinary boundaries and that are likely to lead to transformative multidisciplinary research in complex systems. The outcomes of the workshop will include the preparation of a report to inform the scientific community at large of the current status and challenges as well as future opportunities in multidisciplinary complex systems research as perceived by the participants of the workshop.<br/><br/>The workshop is motivated by the observation that many processes in natural, engineered, and social contexts exhibit emergent collective behavior and are thus governed by complex systems. Because challenges in understanding, predicting, designing, and controlling complex systems are often common to many domains, a central objective of the workshop is to facilitate the exchange of ideas across different fields and avoid disciplinary boundaries typical of many traditional scientific meetings. The workshop participants will include experts both in theory and in applications as well as a selection of postdoctoral researchers and graduate students from various domains. Because of the cross-disciplinary nature of the workshop, the participants themselves will become aware of the latest developments in fields related to but different from their own. This environment will foster discussions on the state of the art, potential issues, and most promising directions in multidisciplinary complex systems research. The inclusion of early-career researchers will help to promote the transfer of this expertise to the next generation of engineers, mathematicians, and scientists."
"1608182","Statistical Theory and Methodology","DMS","PROBABILITY, STATISTICS","06/01/2016","06/01/2018","Bradley Efron","CA","Stanford University","Continuing Grant","Gabor Szekely","05/31/2020","$700,000.00","Persi Diaconis","brad@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1263, 1269","","$0.00","Modern computational capabilities, modern theory, and the expanded data sets produced by modern scientific equipment have greatly increased the scope of statistical inference. This research project investigates a set of questions in probability and statistics raised by large-scale data collection. While of increasing use, empirical Bayes methods have proved difficult to justify. The approach under development in this project brings a novel application of exponential family theory to the job, with the goal of clarifying how empirical Bayes analyses converge to traditional Bayes methods as sample sizes increase. The project aims to develop empirical Bayes methods that use large-scale parallel data sets, such as those from microarray studies, to improve estimation in situations reporting many small sub-experiments, each of which by itself has low accuracy; and improved Monte Carlo methods for the computer solution of massive optimization problems.<br/><br/>Specific topics under investigation in this research project include large-scale empirical Bayes strategies, importance sampling for computer-assisted inference in formerly intractable situations, and a theory of stability assessment for traditional methods of accuracy estimation.  Exponential families of probability distributions play a central role in both computation and statistical inference. A particularly stubborn impediment to their use in massive data analyses is the lack of a suitable norming constant in the exponential family density. This research project further develops computational methods based on solutions of appropriate variational problems; a promising application is in the area of graphical models. A second application of exponential families involves efficient deconvolution of datasets to obtain empirical Bayes estimates."
"1608987","Flexible Statistical Modelling","DMS","STATISTICS","07/01/2016","08/16/2019","Robert Tibshirani","CA","Stanford University","Continuing Grant","Gabor Szekely","06/30/2021","$600,000.00","","tibs@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","In this project, the investigator and his colleagues will study selected topics in high-dimensional statistics and data science.  This work will help scientists working in biotechnology and other areas, who generate large scale datasets, to interpret and uncover the important patterns in their data. This should help scientists and doctors to discover the biological bases of many diseases, and improve prognosis and treatment selection for patients.<br/><br/>Specifically, these projects will include an interaction model for high-dimensional regression and classification, a statistical model for studying mouse and human genomic comparisons, a selective inference approach to average treatment effect estimation (ATE), and a research monograph on post-selection inference in statistics."
"1612763","Scaling up Bayesian Variable Selection for High Dimensions","DMS","STATISTICS","08/15/2016","08/11/2016","Joyee Ghosh","IA","University of Iowa","Standard Grant","Gabor Szekely","07/31/2019","$77,913.00","","joyee-ghosh@uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1269","1303","$0.00","In many applications in the biomedical and environmental sciences, the complexity of available data makes the selection of statistical models challenging.  The goal of this project is to advance basic scientific methodology that will help improve the selection of models using state-of-the-art Bayesian methods. For complex data with a large number of features, it is now routine to entertain an enormous number of competing models. The Bayesian approach to model uncertainty provides a natural solution to many problems in the sciences. In particular, new algorithms for model selection and prediction in a Bayesian framework will be developed. The research is motivated by applications ranging from genetics to climate models. Software will be developed and made publicly available so that the new methods are readily accessible. <br/> <br/>When the number of features/predictors is large, Markov Chain Monte Carlo (MCMC) algorithms are often used to identify promising models. For high-dimensional problems, these algorithms exhibit slow convergence and have extremely long running times.  This project addresses the development of flexible models and algorithms for Bayesian variable selection and prediction that scale well to high dimensions. The assumption of normal errors for the linear regression model may not always be appropriate. However, the validity of this assumption is often overlooked for high-dimensional data, when the number of predictors exceeds the sample size.  In this project, Bayesian variable selection methods for robust error distributions will be developed that adapt to unknown degrees of tail heaviness and sparsity.  Flexible hierarchical models will be considered for probabilistic prediction of North Atlantic tropical cyclone activity."
"1612501","Abrupt Structural Changes in Complex Stochastic Systems with Applications to  Economics, Finance, and Genetics","DMS","STATISTICS","09/15/2016","09/08/2016","Haipeng Xing","NY","SUNY at Stony Brook","Standard Grant","Gabor Szekely","08/31/2019","$180,000.00","","haipeng.xing@stonybrook.edu","W5510 FRANKS MELVILLE MEMORIAL L","STONY BROOK","NY","117940001","6316329949","MPS","1269","","$0.00","Abrupt structural changes in complex stochastic systems arise in science and engineering, including economics, finance, genetics, industrial quality control, and public health. An important step to analyze these problems is to develop appropriate models with parameter jumps and efficient inference procedures. In the proposed research, the principal investigator will investigate three complicated problems with abrupt structural changes that recently arise in three different disciplines and develop corresponding statistical methodology for them. The first is to develop a modulated Markov model with unknown structural breaks to characterize U.S. firms' credit rating transitions when the economy undergoes abrupt structural changes. An inference procedure is also proposed to analyze the relationship between structural changes in the U.S. credit market and variations of macroeconomic and firm-specific covariates. The second is to investigate the issue of learning and control in a sharply changing environment and develop an approximate policy optimization and adaptive control method for the analysis of optimal policies, its application to monetary policy analysis is also discussed. The third problem is to develop a segmentation model that identifies topologically associated domains in the analysis of chromatin interactions, which is an important step in the analysis of the next-generation genome-sequencing data. A statistically and computationally efficient segmentation algorithm is also proposed to estimate the boundaries of topologically associated domains. The PI will show how these challenging problems in different areas can be unified and resolved by the proposed statistical models and inference procedures. <br/><br/>Statistical inference problems in complex stochastic systems with abrupt structural changes arise in science and engineering, including economics, finance, risk management, genetics, industrial quality control, and public health. There has been an extensive literature on stochastic systems with simple structural change mechanisms, however, problems of complex stochastic systems with abrupt structural changes have been hampered by their statistical difficulty and hence has not received much attention. In current genetic research, understanding 3D chromosomal structures and chromatin interactions for decoding and interpreting functions of the genome can provide important hints toward decoding the mechanisms of gene regulation and the maintenance of genome stability, as well as DNA replication, repair and modification, an important step in studying these genetic events is to identify the topologically associated domains from chromatin architecture data. In macroeconomic studies, central banks are keen to control the policy target and estimate the impact of policy action simultaneously with the presence of the unobservable economic structural breaks, so that proper monetary and fiscal policies can be taken to mitigate the potential harmful impact of sharp economic turns. In financial studies, the 2008-2009 financial crisis raises the immediate needs for the regulatory authorities that the financial market should be monitored based on solid statistical and econometric models and procedures, and hence an early warning system should be established to surveillance the stability of financial systems. The proposed research explores the possibility of building quantitative and implementable early-warning systems for financial crisis, which aggregates microeconomic information from individual firms and macroeconomic statistics from general economic activities."
"1639100","2016 Writing Workshop for Junior Researchers in Statistical Sciences","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","08/01/2016","08/05/2016","Nell Sedransk","NC","National Institute of Statistical Sciences","Standard Grant","Swatee Naik","07/31/2017","$9,900.00","","sedransk@niss.org","19 TW ALEXANDER DR","RESEARCH TRIANGLE PARK","NC","277090152","9196859300","MPS","1260, 1269","7556","$0.00","This award supports the participation of junior researchers in a workshop held at the Joint Statistical Meetings in Chicago, Illinois in July and August 2016. The workshop focuses on effective technical writing for new researchers in the statistical sciences, who seek to publish their research or to present their research plans in the form of grant proposals for federal funding. Researchers, especially new researchers, often have difficulty disseminating their research results not because of the quality of the research but rather because of inappropriate choices of publication venues for the particular research and/or because of poor presentation of technical material to the chosen audience. The National Institute of Statistical Sciences and the American Statistical Association will manage the Workshop. <br/><br/>This workshop will open with tutorial sessions on the organization of material for a technical article or grant application, on technical writing techniques, and on the specific missions and audiences of key journals in the statistical sciences. Following the introductory tutorial, each participating new researcher will work individually with an experienced journal editor as mentor to address these issues on an individualized basis in a draft of the new researcher's work. Revisions following this guidance will be critiqued by the mentor to assure that the new researcher's implementation of writing techniques has been successful before the article or the grant proposal is submitted for review.  More information about this activity can be found at http://www.amstat.org/meetings/wwjr/index.cfm?fuseaction=main.  This award is jointly supported by the Infrastructure and Statistics programs in the NSF Division of Mathematical Sciences."
"1614526","Collaborative Research: Unifying Mathematical and Statistical Approaches for Modeling Animal Movement and Resource Selection","DMS","POP & COMMUNITY ECOL PROG, STATISTICS, Cross-BIO Activities, MATHEMATICAL BIOLOGY, MSPA-INTERDISCIPLINARY, Animal Behavior","08/01/2016","08/16/2017","James Powell","UT","Utah State University","Standard Grant","Zhilan Feng","07/31/2020","$179,991.00","Thomas Edwards","jim.powell@usu.edu","1000 OLD MAIN HILL","LOGAN","UT","843221000","4357971226","MPS","1182, 1269, 7275, 7334, 7454, 7659","8007, 9251","$0.00","Understanding how individuals move in space, what habitats they prefer, and how the environmental features channel or resist movement is central to landscape ecology and wildlife management.  Dramatic improvements in the acquisition, resolution, and extent of two relevant types of data have recently occurred: remotely sensed environmental data and high-resolution animal location (telemetry) data.  These data drive a statistical industry serving wildlife management agencies, private companies, and academia. Improvements in tracking technology are likely to cause a revolution in movement ecology analogous to the impact of gene sequencing on molecular genetics.  This project synthesizes theoretical advances (statistical techniques for estimating movement probability between sites and how environmental resources are selected), existing results (mathematical techniques for rapidly predicting the envelope of future animal positions using mechanistic assumptions) and untapped data (remotely sensed habitat maps and high resolution individual telemetry) to rigorously characterize how landscape features condition population movement and habitat choice.  The research will encompass case studies investigating the movement of mule deer and elk in Utah, harbor seals off southeastern Alaska, and Canada lynx, which have recently been reintroduced in Colorado and are dispersing throughout the Rocky Mountains.   Research students will be cross-trained in mathematics, statistics, and movement ecology; undergraduates will be included in the research process by developing individual-based models to test estimation technologies.  A teaching lab in mathematical biology, illustrating movement models using real biological systems, will also be developed and distributed.<br/><br/>Statistical point process models provide well-understood statistical approaches for obtaining inference from individual-based telemetry data, with resource selection functions describing individual habitat preferences and availability functions describing dispersal probability between locations. However, point process models require numerical quadrature for proper normalization, making them slow for large data sets.  Classical availability functions are not constructed to handle major issues like movement constraints, autocorrelation, and landscape resistance, affecting quality of resource selection inference and computational feasibility.  However, a parallel and untapped literature of partial differential equations predicts dispersal likelihood based on mechanistic assumptions about individual movement.  Ecological diffusion and ecological telegrapher's equations provide natural scalings from Lagrangian to Eulerian perspectives. They are fully mechanistic and allow for population-level dynamics, but are not inherently statistical nor automatically suited to handling individual-based telemetry data.  This project will reconcile point process modeling with mechanistic dispersal equations to arrive at a unified method for analyzing telemetry data.  Homogenization techniques, which are well-accepted in physical sciences but not often applied in mathematical biology or statistics, will be used to speed up solutions in heterogeneous environments. Coupled point process models and homogenized partial differential equations will accelerate model fitting, provide resource selection inference and naturally accommodate environmental heterogeneity and barriers/constraints to movement.  The ecological movement equations will be homogenized and simplified using asymptotic approximations suitable for point process models, addressing correlation among position observations and velocity constraints.  Rapid numerical techniques for movement models will be developed to allow facile representation of movement barriers (e.g., shorelines, major rivers or roads) as boundary conditions. To develop efficient computational techniques for resource selection functions and landscape resistance inference, the homogenized ecological movement equations will be dovetailed with point process models in a hierarchical framework.  The integrated approach will be applied to telemetry data from foraging ungulates in Utah, harbor seals in the Gulf of Alaska, and Canada lynx in Colorado."
"1553884","CAREER:  Modernizing Classical Nonparametric and Multivariate Theory for Large-scale,  High-dimensional Data Analysis","DMS","STATISTICS, Division Co-Funding: CAREER","08/01/2016","05/26/2020","Jing Lei","PA","Carnegie-Mellon University","Continuing Grant","Gabor Szekely","07/31/2021","$400,000.00","","jinglei@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 8048","1045","$0.00","The constantly increasing dimensionality and complexity of modern data has motivated many new data analysis tools in various fields, and urgently call for rigorous theoretical investigation, such as robustness against different sources of model misspecification,uncertainty quantification in classification and prediction, and statistical performance guarantee of conventional methods under non-standard settings. Although most classical theory are not directly applicable to methods developed for complex data, partially due to highly specialized model assumptions and diversified algorithms, the profound statistical thinking carried in these long-established results can still provide deep theoretical insights. When combined with cutting-edge results in modern context such as random matrix theory, matrix concentration, and convex geometry, these classical theory will lead to novel principled methods for a general class of problems ranging from high dimensional regression and classification to network data analysis and subspace learning. All methods developed in the proposed research will be implemented as standard R packages freely available and will have high pedagogical value and will be used to develop new courses. The proposed research has applications in astronomy and medical screening data. The proposal also provides new inference tools for applied areas in genetics, psychiatry, brain sciences. Integrated educational activities include designing courses on new perspectives in nonparametric statistics and modern multivariate analysis.<br/><br/>The proposed work will further integrate classical nonparametric and multivariate analysis theory with modern elements in four major areas of statistical research, including assumption-free prediction bands in high dimensional regression; a generalized Neyman-Pearson framework for set-valued multi-class classification; statistical performance guarantee of some greedy algorithms in network community detection as well as goodness-of-fit tests for network model selection; and a unified singular value decomposition framework for structured subspace estimation formulated as a convex optimization problem.  These research activities will lead to modernized nonparametric and multivariate analysis courses, featuring new theoretical frameworks such as computationally constrained minimax analysis, additional topics such as functional data analysis, and cutting-edge examples in genetics, brain imaging, traffic, and astronomy."
"1613002","Canonical Linear Methods and Hierarchical Non-Linear Methods in High-Dimensional Statistics","DMS","STATISTICS","07/01/2016","06/27/2019","Bin Yu","CA","University of California-Berkeley","Continuing Grant","Pena Edsel","06/30/2022","$600,000.00","","binyu@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","7433, 8083","$0.00","Statistics is at the heart of extracting meaningful information from big data. Its primary tasks include estimation and uncertainty assessment. The latter is crucial in big data analysis for sound decision making. For the former, the methods employed in deep learning machines, such as those behind Google's Brain and AlphaGo and Microsoft's Cortana, beg understanding. This research project is intended to bridge practice and theory of statistics in these areas. It aims to provide accessible uncertainty measures for linear modeling of big data and to derive insights into how deep learning works, based on mathematical analysis. <br/><br/>This research project develops and analyzes linear and non-linear high-dimensional statistical inferential methods that are easily accessible by practitioners in data science. In the linear case, it develops and analyzes inferential methods based on well-established bootstrap, lasso, partial ridge, and random projection methods. In the non-linear case, it takes the first steps to explain in a principled manner the impressive success of deep learning in practical problems such as image classification and speech recognition. In particular, statistical properties of these methods will be studied under linear and Neyman-Rubin high dimensional models, and via analytical and simulation means. A generative model of a two-layer neural network (or hierarchical non-linear model) will be explored to understand and compare deep learning with other methods, analytically and through simulation studies. Improvements over deep learning as a general supervised learning method are sought by enforcing biologically meaningful constraints from brain connectivity research."
"1622403","Collaborative Research:  Advancing Statistical Surrogates for Linking Multiple Computer Models with Disparate Data for Quantifying Uncertain Hazards","DMS","STATISTICS, CDS&E-MSS","08/15/2016","08/11/2016","Robert Wolpert","NC","Duke University","Standard Grant","Christopher Stark","07/31/2019","$243,785.00","James Berger","rlw@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269, 8069","8060, 8083, 9263","$0.00","The Oso, Washingon landslide of 2014, which resulted in 43 fatalities, and the ash plumes from the Eyjafjallajökull (Iceland) eruption of 2010, which shut down air travel in Europe, are examples of rare and catastrophic geophysical events. Their rare nature makes such events nearly impossible to forecast, if forecasts are based only on previous observations.  To capture rare events, researchers must rely on complex physical and mathematical models that often require significant computational resources to exercise.  Furthermore, events like these may be best described by a series of different models of different phenomena at different scales. For example, a researcher may need to combine a model of rainfall, a model of slope failure, and a model of sliding debris to create on overall model for a landslide event.  The main objective of this research is the development of efficient statistical and computational strategies to combine such models, thus advancing the state of the art in hazard forecasting.<br/><br/>Direct simulation-based hazard assessment would require thousands to tens of thousand of linked, space-time simulations.  Furthermore, to be of most use in hazard assessment, these simulations should be informed and validated by observational data sets, which themselves can range from sparse data (rare events) to massive data (e.g. satellite data), and explored for emerging scenarios.  To complicate the matter, a number of features of the problems of interest are either poorly characterized or unpredictable, and one would like to run the simulation programs at a range of values of each of them; this quickly leads to a perceived need to run a simulation program (which may take hours to complete) for hundreds of thousands or millions of different combinations of parameter values and conditions. There simply is not enough time or enough computing power for such a brute force approach to succeed. To tackle the situation just described, the PIs will continue to develop parallel partial emulators for massive space-time simulator data allowing emulator construction on the adaptive space-time grids commonly used in geophysical simulations, creating smoothers for their output, and enabling the use of reduced input spaces. The PIs will begin the investigation of a strategy for linking multiple simulators via multiple emulators.  A particularly powerful semi-analytic way of linking emulators will be pursued, with a variety of research questions arising centering around the accuracy of the method, as well as the possibility of its implementation in the huge data scenario envisaged for the parallel partial emulator. The PIs will also begin to investigate techniques to extract (nearly) optimal basis sets, data reduction methods, and algorithmic approaches to accelerate the construction of emulators, all of which contribute to a more robust handling of large datasets. These new methodologies will provide tools to rapidly construct probability-based hazard forecast maps for cascading geophysical events. Rapid forecast maps allow end users to perform hazard analysis under a wide variety of aleatoric scenarios. Furthermore this new methodology will enable fast assessment of epistemic uncertainties. This approach constitutes a dramatic improvement in scientifically-based decision support."
"1614593","Student Travel Support for International Society for Bayesian Analysis Conference","DMS","STATISTICS","05/01/2016","04/28/2016","David Banks","NC","Duke University","Standard Grant","Gabor Szekely","04/30/2017","$20,000.00","","banks@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","7556","$0.00","The 13th meeting of the International Society for Bayesian Analysis (ISBA) will take place in Cagliari, Sardinia, Italy, from June 13-17, 2016. (See https://bayesian.org/content/isba-2016-world-meeting.) The International Society for Bayesian Analysis (ISBA, www.bayesian.org) was established in 1992. Its mission is to promote the development and application of Bayesian statistical theory and methods useful in the solution of theoretical and applied problems in science, industry and government. The ISBA meetings are important venues for the exchange of research ideas in Bayesian statistics. This award will provide funding for US Ph.D. students to attend and participate in this meeting.<br/><br/>This meeting on Bayesian analysis will be held June 13-17, 2016, in Sardinia. The conference will include short courses, oral presentations, and poster sessions. The oral presentations include the De Finetti Lecture by Persi Diaconis (Stanford University) and the Susie Bayarri Lecture by James Scott (University of Texas). NSF funding will allow US Ph.D. students to attend and participate in this meeting."
"1612889","Graphical Multi-Resolution Scanning for Cross-Sample Variation","DMS","STATISTICS","07/01/2016","06/03/2018","Li Ma","NC","Duke University","Continuing Grant","Gabor Szekely","06/30/2020","$345,116.00","","li.ma@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","7433, 8083","$0.00","Identifying variation across data sets is one of the most commonly encountered statistical inferential tasks, and it lies at the heart of numerous applications in a wide range of fields from astrophysics and biology to economics and political science. The recent explosion of ""big data"" has raised several critical challenges in detecting cross-sample variation, which render existing methods inadequate and entail an urgent need for new methodologies. The most notable and prevalent challenges include complex distributional structures, the highly local nature of variation, various extraneous sources of variation, data sparsity, and massive computational demand. The overarching aim of this research project is to develop a general framework including theory, methods, algorithms, and software for effectively identifying variation in modern big data sets to address these challenges. <br/><br/>Specific inference problems to be addressed in this research project include: (i) identifying differences, especially highly local variations, across multiple data sets; (ii) separating intrinsic (i.e., scientifically interesting) cross-sample variation from extraneous variation; (iii) decomposing cross-sample variation into contributions from multiple sources; and (iv) identifying cross-sample variation and variance components in general random objects, including a variety of processes and functional observations. The use of multi-scale inference and Bayesian nonparametric modeling has led to development of a general probabilistic model-based framework for detecting cross-sample variation that integrates two powerful inference tactics -- multi-resolution scanning and graphical modeling. Multi-resolution scanning is the strategy of scanning through the sample space using windows of various sizes, carrying out testing or estimation for the structure of interest -- the cross-sample variation -- on each window. A class of graphical models is then designed to incorporate various dependency structures across scanning windows and data samples, thereby allowing borrowing strength among windows and related samples to achieve high statistical efficiency in identifying cross-sample variation. The project aims to construct a suite of computationally efficient and theoretically justifiable inferential methods and algorithms and to investigate their statistical properties."
"1612458","New Inference Methods For Multiway Functional Data and Multilayer Network Data","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","07/01/2016","05/20/2016","Kehui Chen","PA","University of Pittsburgh","Standard Grant","Gabor Szekely","06/30/2019","$300,867.00","","khchen@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1253, 1269","8091","$0.00","This project focuses on developing efficient models for multi-way functional data and multi-layer network data.  Functional data refers to data recorded over a continuum, such as growth curves for many children.  Examples of multi-way functional data include brain-imaging data measured over space and time, and data obtained from mobile tracking apps where daily activity profiles are recorded for a number of individuals over a period of many days.  Despite the growing number of applications, the complex structure and high-dimensionality of the data pose significant challenges for statistical modeling and inference.  The second part of the project focuses on multi-layer network data obtained from multi-modal and multi-task brain connectivity studies.  A rigorous statistical framework for these network structures will be developed.  <br/><br/>In the long history of image processing and spatial-temporal analysis, many methods have relied on separability, the assumption that the covariance can be factorized as a product of a spatial covariance and a temporal covariance.  Recent approaches for repeated functional data and multi-way functional data also invoke separability, either explicitly or implicitly, to achieve efficient dimension reduction.  A new notion of weak separability, that includes covariance separability as a special case, will be introduced.  Tests of weak separability will be developed, and principled answers to several open questions will be provided.  In the second part of the project, generative models of multi-layer network data with community structures will be introduced.  Least squares estimation of memberships will be studied from a novel relational k-means perspective, and theoretical justification provided under a statistical inference framework."
"1602592","Statistical Challenges in Modern Astronomy VI","DMS","SPECIAL PROGRAMS IN ASTRONOMY, STATISTICS","05/01/2016","04/21/2016","Chad Schafer","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","04/30/2017","$20,000.00","Shirley Ho","cschafer@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1219, 1269","1206, 7556","$0.00","A conference on Statistical Challenges in Modern Astronomy VI will be held at Carnegie-Mellon University, June 6-10, 2016. (See http://www.scma6.org/.) Astronomical data are continuously being gathered from surveys that are increasingly ambitious in scope, and astronomers face a wide range of challenges when transforming this information into scientific conclusions. This conference brings together statisticians and astronomers to work on problems in astronomy involving large amounts of data. It has been held every five years since 1991. <br/><br/>The conference Statistical Challenges in Modern Astronomy VI will be held at Carnegie-Mellon University, June 6-10, 2016. Over 15 years the Sloan Digital Sky Survey (SDSS) has collected 100 terabytes data from hundreds of millions of stars and galaxies, as well as a million supermassive black holes; by contrast, the Large Synoptic Survey Telescope (LSST), planned for completion by 2022, will gather 15 terabytes of data per night over its ten-year life. The full scientific potential of these surveys will be realized not only with ingenuity, but also with novel methods of data analysis. Experts in statistics and machine learning can make crucial contributions both to these specific domains, and to general methodological development for Big Data applications. This conference is a major effort to increase the interaction of astronomers and data scientists and to build the next generation of scientists to bridge the gap between these disciplines."
"1723158","Spatial-temporal models and methods for big nonstationary multivariate","DMS","STATISTICS","07/01/2016","12/15/2016","Montserrat Fuentes","VA","Virginia Commonwealth University","Continuing Grant","Gabor Szekely","06/30/2019","$139,729.00","","mfuentes@vcu.edu","910 WEST FRANKLIN ST","RICHMOND","VA","232849066","8048286772","MPS","1269","","$0.00","High dimensional statistical problems are prevalent in the environmental sciences, particularly in soil, atmospheric, and oceanic data applications. In these cases the processes of interest are inherently nonlinear and dynamic. Different sources of information for these systems include spatial observational data as well as physics and chemistry based numerical models. Over the past decade there has been an increase in the amount of available real-time geographic information as well as advances in the sophistication and resolution of deterministic atmospheric and oceanic models. A broad class of spatial-temporal models is developed for multivariate processes on Euclidean spaces and the sphere to explain the variability and the cross-dependency between different variables. This general class of models goes beyond standard assumptions, in particular of stationarity. The properties of the proposed methods, as well as the asymptotic properties of the estimates are studied. Likelihood approximation methods for massive spatial data are presented to efficiently implement the proposed statistical models. The proposed framework and models are used to better model soil pollution, air pollution, and wind fields. These high spatial resolution wind fields are used to predict energy production from windmills, they are also the primary forcing for numerical forecasts of the coastal ocean response to force winds such as the height of the storm surge and the degree of coastal flooding. The goal is to obtain more accurate estimation of wind fields over land and water to improve the quality of storm surge forecasts, and wind energy.<br/><br/>The most important scientific contributions of this research project are: the introduction of flexible spatial models on the sphere for prediction and estimation of environmental spatial processes observed over larger regions on the Earth's surface; methods for likelihood approximation of big spatial temporal lattice data in general situations; general and flexible models for spatial prediction of multivariate environmental processes on spatial lattices, introducing the concept of conditional correlation in spatial lattice models; and advanced methods for spatial prediction and estimation in the presence of massive data from observations and physical and chemistry models. In these cases the processes of interest are inherently nonlinear and dynamic. Different sources of information for these systems include observational data as well as physics-based numerical models. Over the past decade there has been an increase in the amount of available real-time observations as well as advances in the sophistication and resolution of deterministic chemistry, atmospheric and oceanic models. Our methodology  will provide more accurate representation and prediction of the underlying space-time process of interest. Through our collaborative work, we will help the enhancement of science by implementing these methods to hurricane wind fields and to weather and air and soil pollution to improve weather and air/soil quality mapping. The investigators will  disseminate broadly the methods proposed here to enhance mathematical and scientific understanding. The PI will offer  short courses in Spanish in Hispanic countries to broaden the participation of underrepresented geographic and ethnic groups. A course in advanced spatial statistics methods will be taught by the PI, and the new statistical methods proposed here will be introduced to the students.  The investigators will continue their efforts to broaden the participation of minorities and women in Sciences and the PI  through this project will continue  her involvement on K-12 educational efforts, through the Kenan Fellows for Curriculum and Leadership Development Program and the Science House at NCSU."
"1554821","CAREER: New Techniques for Statistical Learning and Multivariate Analysis","DMS","STATISTICS, Cross-BIO Activities, MSPA-INTERDISCIPLINARY, Activation, Division Co-Funding: CAREER","07/01/2016","07/27/2020","Genevera Allen","TX","William Marsh Rice University","Continuing Grant","Pena Edsel","06/30/2022","$400,000.00","","gallen@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269, 7275, 7454, 7713, 8048","1045, 8007, 9179","$0.00","New technologies in many scientific sectors have led to data sets of increasing complexity and size, where the ability to measure and store these vast troves of data has far outpaced the ability to analyze the data to make reproducible scientific discoveries. Examples include genomics and proteomics, neuroimaging, and neural recordings data. Analyzing this big biomedical data is critical to discovering disease biomarkers, making advances in personalized medicine, and understanding the basic workings of complex biological systems. In this work, we seek to develop and study novel statistical learning and multivariate analysis techniques that directly address unresolved problems critical for making discoveries from big scientific data.  Additionally, we will use statistical learning techniques to improve introductory statistics education by developing an online personalized learning system for assignment and content delivery.  <br/><br/>More specifically, this work will focus on using algorithms for large-scale sparse optimization to inspire and develop a new framework for statistical learning that will prove to have superior empirical and theoretical performance for high-dimensional and highly correlated data.  Such data is common in genomics and neuroimaging; the new techniques will be used to identify potential genomic drug targets, to model genetic and brain networks, and for brain decoding from neuroimaging and neural recordings data. We will also use Kronecker product covariances to develop new multivariate analysis models for coupled matrix and tensor data.  These techniques will be used to find joint patterns in integrative genomics data and find patterns of brain activity indicative of behavioral or clinical covariates.  Overall, this work will develop several critically needed statistical techniques to understand large and complex data, have direct impacts in genomics and brain science where the techniques will be applied in collaboration with scientists, and lead to improvements in introductory statistics education.  This award is co-funded by the Directorate for Mathematical and Physical Sciences (MPS) Division of Mathematical Sciences (DMS) and the Directorate for Biological Sciences (BIO) Divisions of Integrative Organismal Systems (IOS) and Emerging Frontiers (EF)."
"1613156","Collaborative Research: Scalable Bayesian Methods for Complex Data with Optimality Guarantees","DMS","STATISTICS","07/01/2016","05/24/2016","Debdeep Pati","FL","Florida State University","Standard Grant","Yong Zeng","07/31/2018","$127,059.00","","debdeep@stat.tamu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","7433, 8083","$0.00","Spectacular advances in data acquisition, processing, and storage present the opportunity to analyze datasets of ever-increasing size and complexity in various applications, such as social and biological networks, epidemiology, genomics, and Internet recommender systems. Underlying the massive size and dimension of these data, there is often a parsimonious structure. The Bayesian approach to statistical inference is attractive in this context in terms of incorporating structural assumptions through prior distributions, enabling probabilistic modeling of complex phenomenon, and providing an automatic characterization of uncertainty. This research project aims to advance eliciting and translating prior knowledge regarding the low-dimensional skeleton of big data to provide realistic uncertainty characterizations while maintaining computational efficiency. Bayesian computation poses substantial challenge in high-dimensional and big data problems. The research aims to develop cutting-edge computational strategies and software packages for implementation to be made available publicly. The project involves graduate students in the research.<br/><br/>The research project focuses on theoretical foundations and computational strategies for Bayesian methods in high-dimensional and big data problems motivated by applications in social networks and epidemiology. Techniques for systematically developing and evaluating prior distributions in high-dimensional problems will be investigated with a special emphasis on the trade-off between statistical efficiency and computational scalability. Specific directions include efficient algorithms for posterior sampling with shrinkage priors, a theoretical framework for divide and conquer strategies in big data problems, fast algorithms for clustering nodes in large networks with unknown number of communities, and methods for discovering structure in sparse contingency tables. The algorithms will be motivated by rigorous theoretical understanding of the behavior of the posterior distribution with a particular emphasis on proper quantification of uncertainty in a distributed computing framework. Software will be developed for each application."
"1613202","Adaptive Thresholding for Hierarchical Clustering of Variables, with Connections to Scan Statistics","DMS","STATISTICS","08/01/2016","08/13/2018","Max G'Sell","PA","Carnegie-Mellon University","Continuing Grant","Pena Edsel","07/31/2020","$150,000.00","","mgsell@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","In modern data analysis with large data sets, a common goal is to detect groups of variables that exhibit similar behavior.  This task is usually referred to as clustering.  In genetics and proteomics, for instance, clustering can reveal structures of scientific interest, such as potential biological pathways.  On top of detecting scientifically relevant structure in the data, clustering can also be used to simplify data representations and analysis.  One of the most widely used approaches to clustering is called hierarchical clustering.  In hierarchical clustering, a measure of similarity, like correlation, is computed between each pair of variables, and then similar groups of variables are repeatedly merged.  This leads to a fundamental question: how much grouping should be done?  The proposed research consists of several projects aimed at developing broadly applicable methods for determining the appropriate amount of clustering, based on the degree of similarity present in the data.  The resulting procedures will also provide statistical guarantees on the meaning of the resulting groups.<br/><br/>This proposal aims to develop practical procedures for adaptive thresholding of hierarchical clustering dendrograms, when applied to pairwise similarities of variables.  These procedures will be connected to inferential guarantees about the false cluster error rate of the resulting clustering.  The results will target a range of common linkages and variable similarity measures.  The PI will also demonstrate these procedures in a modern genetics application.To support these procedures, new theory will be developed describing the large order statistics of variable similarity measures, including new asymptotic bounds on their joint distributions and new finite-sample bounds on their maxima.  The techniques proposed here will also have application to other threshold-based procedures in statistics; in particular, connections may be made between the proposed work and adaptive thresholding procedures for scan statistics."
"1612978","From Approximate to Exact Designs with Applications to Big Data","DMS","STATISTICS","06/01/2016","05/20/2016","Wei Zheng","IN","Indiana University","Standard Grant","Gabor Szekely","04/30/2018","$149,983.00","","wzheng9@utk.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","MPS","1269","7433, 8083","$0.00","Design of experiments is an integral part of the scientific process in many areas of research with a direct impact on society, such as the biological sciences, the health sciences, the social sciences, engineering, marketing, and education.  A well-chosen design facilitates the collection of data that, at a minimum cost, maximizes the information for the scientific questions of interest.  Many scientific studies allow for repeated use of conceptual units, so that developing tools for optimal design for these problems has great potential impact.  Particularly in the realm of big data, there is much room for improvement of existing methods for design of experiments, and the tools and concepts under development in this research project have potential to lead to significant gain of information without increasing computational cost.  Results from the project will be made available to researchers in other areas through easy-to-use software that implements the algorithms to be developed. Graduate students will be trained to become researchers in design of experiments. <br/><br/>This project aims to result in a major leap forward in understanding and knowledge of optimal design of experiments. Recent work in the field has had a significant impact on the advancement of optimal crossover designs and designs for interference models for arbitrarily given covariance structures and design size configurations. However, these results have for the most part been limited to approximate designs for relatively simple models. While these results are arguably important in their own right, this project will extend methods and tools to achieve the ultimate goal of deriving exact designs for a wider spectrum of practical models. The results will be a much needed addition to our collective design toolbox. Most importantly, this project will go beyond the territory of design and apply the tools and ideas from design of experiments to subsampling problems emerging in big data with both statistical and machine learning methods under consideration. Preliminary results indicate that this is an opportune time to make these challenging but critical steps."
"1621231","Diversity Workshop and Mentoring Program","DMS","STATISTICS","05/01/2016","04/25/2016","Donna LaLonde","VA","American Statistical Association","Standard Grant","Yong Zeng","04/30/2017","$15,000.00","Brian Millen, Sydeaka Watson, Jesse Chittams","donnal@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","7556","$0.00","The 2016 JSM Diversity Workshop and Mentoring Program will take place during the Joint Statistical Meetings (JSM), July 31 - Aug 3, in Chicago, Illinois.  It will consist of a series of interactive career development sessions and small group and 1-1 mentoring sessions. The goals are to establish critical mentoring and networking relationships for minority statisticians at early- to mid-career levels, to motivate students to pursue careers and graduate study in the statistical sciences, to share best practices for recruiting and retention of minority students, faculties, and non-academic professionals, and to increase the active participation of minorities in the American Statistical Association and other professional statistics societies or organizations.<br/><br/>Participants in the workshop and mentoring program include underrepresented minority graduate students in statistics or biostatistics programs; top minority undergraduates in statistics, mathematics, or related disciplines; minority statisticians in academia, government, and the private sector; key faculty from minority-serving institutions who advise and mentor undergraduates in math and related disciplines; and faculty influential in the faculty and/or student recruitment processes at their home institutions. This program is unique in that it does not limit its focus to students, but also includes early career professionals in its scope. It is further unique in its holistic approach which allows individuals to mentor more junior colleagues while themselves being mentored by more senior colleagues.  These elements help create a community of support and accountability for the career success of all involved.<br/><br/>The workshop's website is located at: http://community.amstat.org/cmis/events/dmp"
"1642088","Women in Statistics and Data Science Conference","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","09/01/2016","08/09/2016","Donna LaLonde","VA","American Statistical Association","Standard Grant","Yong Zeng","08/31/2017","$20,000.00","Dalene Stangl","donnal@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1260, 1269","7556","$0.00","This project provides support for a two and a half day Women in Statistics and Data Science (WSDS) Conference, which takes place in October 20 to 22, 2016 in Charlotte, NC. The focus of this conference is to empower women statisticians, biostatisticians, and data scientists by exchanging ideas and presenting technical talks on important, modern and cutting-edge research, discussing how to establish fruitful multi-disciplinary collaborations, and showcasing the accomplishments of successful women professionals. In particular, an ultimate objective of this conference is to increase the proportion of successful women researchers who have the technical skills, capacity and inclination to take on the challenges of 'big-data' oriented research in twenty-first century. <br/><br/>The two and half-a-day conference (starting the afternoon of October 20th) has multiple parallel technical sessions providing participants with the opportunity to learn about novel approaches and innovations addressing the challenges of big data. The technical sessions are complemented by career development sessions for all stages of participants starting from graduate students to senior professors, leadership development sessions, and formal and informal mentoring sessions. The conference, through multiple invited technical sessions highlights the research of female rising stars, mid-career and senior researchers and provides the opportunity to establish new collaborations. In addition, the mentoring and networking sessions of academic, government and industry participants guide early-career women participants on a successful career trajectory. <br/><br/>Conference URL: http://www.amstat.org/meetings/wsds/2016/index.cfm"
"1628941","Statistics and Biostatistics Department Chairs Workshop","DMS","STATISTICS","05/15/2016","05/19/2016","Steve Pierson","VA","American Statistical Association","Standard Grant","Nandini Kannan","10/31/2017","$39,962.00","Donna LaLonde","pierson@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","7556","$0.00","This award supports travel for participants in the U.S. Statistics and Biostatistics Department Chairs Workshop held July 12-13, 2016 at the American Statistical Association in Alexandria, Virginia. Enrollments in statistics courses as well as statistics degrees awarded are surging across U.S. universities. The numbers of universities granting both graduate and undergraduate degrees in statistics and biostatistics are at all-time highs. At the same time, data science is emerging as an important research program and the demand for data science skills is rapidly increasing in the public and private sectors. Since statistics represents a foundational discipline in data science, statistics and biostatistics department chairs need to ensure statisticians are effective partners in intramural and extramural university data science programs.  This workshop brings together statistics and biostatistics department chairs to address current challenges and learn directly from funding agency and private sector representatives about data science and research funding trends, in order to help statistics and biostatistics departments ensure a comprehensive approach both to data science research and to training graduates with skills that meet workforce demands. <br/><br/>Statisticians and statistics play a very important role in advancing science. This workshop provides an opportunity for statistics, biostatistics, and related department chairs to discuss current trends, challenges, and opportunities relating to surging enrollments, workforce needs, research trends, data science, and other issues. To inform discussions, participants will hear from private sector representatives concerning workforce needs, funding agency representatives on funding programs and trends, academic leaders on statistics and data science education, and university administrators on cross-campus research and educational efforts. Because a goal of this workshop is to equip department chairs to further engage their faculty in cross-campus interdisciplinary research and data science efforts, the workshop has strong potential to improve scientific research. Proceedings of the workshop will be recorded, summarized, and disseminated in a format constructive to the participants and the broader community. More information can be found at the workshop web page:<br/>https://www.amstat.org/meetings/dcworkshop/"
"1612924","On Statistical Modeling and Parameter Estimation for High Dimensional Systems","DMS","STATISTICS","09/01/2016","08/23/2016","Faming Liang","FL","University of Florida","Standard Grant","Yong Zeng","01/31/2018","$150,000.00","","fmliang@purdue.edu","1523 UNION RD RM 207","GAINESVILLE","FL","326111941","3523923516","MPS","1269","","$0.00","The dramatic improvements in data collection and acquisition technologies over the last decades have enabled scientists to collect massive amounts of high-dimensional data that allow for monitoring and studying of complex systems.  Due to their intrinsic nature, many of the high-dimensional datasets, such as omics and genome-wide association study (GWAS) data, have a much smaller sample size compared to the dimension (referred to as the small-n-large-P problem). Current research on statistical modeling of small-n-large-P data focuses on linear and generalized linear models. However, these approaches are often not adequate for modeling complex systems, and estimation of the model parameters is challenging.  This project addresses two fundamental problems, statistical modeling and parameter estimation, toward a valid statistical analysis of high-dimensional data.  Successful completion of this project will generate hands-on tools for statistical inference of high-dimensional complex systems, which can benefit researchers in many areas of science and technology.  In particular, the proposed applications to biomedical studies will lead to accurate tools for detecting biomarkers associated with disease processes and tailoring optimal therapy for individual patients with complex diseases. The research results will be disseminated to the statistical and biomedical communities, via collaboration, conference presentations, books, and articles to be published in academic journals. The project will also have significant impact on education through the involvement of graduate students in the project, and incorporation of results into undergraduate and graduate courses.  In addition, the R package developed under this project will provide a valuable tool for statistical analysis of high-dimensional data.<br/><br/>The current approach to modeling small-n-large-P data focuses on linear and generalized linear models, and casts the problem as variable selection by imposing a sparsity constraint on parameter values.  Although these models have many advantages, such as simplicity and computational efficiency, estimation of the parameters is still a challenging problem.  While regularization is often used in these situations, it can perform poorly when the sample size is small and the variables are highly correlated.  Two new methods are proposed to address these concerns, namely, Bayesian neural network (BNN) and blockwise coordinate consistency (BCC).  The BNN method works by first fitting the data with a feed-forward neural network, conducting variable selection through network structure selection under a Bayesian framework, and resolving the associated computational difficulty via parallel computing. Compared to existing methods, BNN can lead to much more precise selection of relevant variables and outcome prediction for high-dimensional nonlinear systems.  The BCC method works by maximizing a new objective function, the expectation of the log-likelihood function, using a cyclic algorithm and iteratively finding consistent estimates for each block of parameters conditional on the current estimates of the other parameters.  The BCC method reduces the high-dimensional parameter estimation problem to a series of low-dimensional parameter estimation problems.  The preliminary results indicate that BCC can provide a drastic improvement in both parameter estimation and variable selection over regularization methods. The validity of the proposed methods will be rigorously studied and applied to biomarker discovery, precision medicine, and joint estimation of the regression coefficients and precision matrix for high-dimensional multivariate regression."
"1620945","Collaborative Research: Analysis of longitudinal multiscale data in immunological bioinformatics - Feature selection, graphical models, and structure identification","DMS","STATISTICS, CDS&E-MSS","08/01/2016","06/23/2016","Pang Du","VA","Virginia Polytechnic Institute and State University","Standard Grant","Yong Zeng","07/31/2020","$125,226.00","","pangdu@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1269, 8069","8083, 9263","$0.00","This project aims to develop a system of statistical analysis tools to tackle several important challenges in analysis of complex bioinformatics data, which involves a variety of response variables and tens of thousands independent variables. The interest often lies in identifying the key independent variables associated with the response variables, and understanding such associations as well as the interactions among the independent variables.<br/><br/>The extreme magnitude and complexity of bioinformatics data have posed serious challenges for data analysis. To overcome these challenges, we propose (i) to systematically and properly integrate multi-scale data before we can apply our novel modeling and analysis methods since the data we explore are collected by numerous independent studies at phenotypic, cellular, protein, and genetic levels with information from very different time and dimension scales; (ii) to develop feature screening criteria for a mixed type of longitudinal data using the combination of correlation tests in bivariate longitudinal regression models and the Benjamini-Hochberg-Yekutieli procedure, (iii) to develop graphical models that allow the variables being a mix of continuous and discrete longitudinal variables, with the nodes representing variables and each edge indicating the dependence of the two relevant variables conditional on the other variables; and (iv) to investigate the functioning form of each predictor by resorting to the data themselves under the framework of a mixed effects regression model with a continuous or discrete response and a high dimensional vector of predictors, with the resulting procedure allowing a user to simultaneously determine the form of each predictor effect to be zero, linear or nonlinear."
"1612914","Conference on Modeling Neural Activity: Statistics, Dynamical Systems, and Networks","DMS","STATISTICS","05/15/2016","05/10/2016","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","04/30/2017","$20,000.00","","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","7556","$0.00","This award supports participation in the conference on Modeling Neural Activity: Statistics, Dynamical Systems, and Networks (MONA2) held June 22-24, 2016 in Lihue, Hawaii. Many disorders, such as ADHD, autism, and schizophrenia, as well as stroke and various neurodegenerative diseases, are thought to involve dysfunction of neural networks. Because computational neuroscience aims to supply principles for understanding the activity of individual and collective neural firing patterns, its successes can help in formulating mechanistic descriptions of pathophysiology. This conference will bring together statisticians and computational neuroscientists from the US and Japan in order to enhance collaborations between scientists in the two countries. NSF funding will help support the involvement of the US researchers. <br/><br/>Computational neuroscience has grown, in distinct directions, from the success of biophysical models of neural activity, the attractiveness of the brain-as-computer metaphor, and the increasing prominence of statistical and machine learning methods throughout science. This has helped create a rich set of ideas and tools associated with ""computation"" to study the nervous system, but it has also led to a kind of balkanization of expertise. There is, especially, very little overlap between mathematical and statistical research in this area. Important breakthroughs in computational neuroscience could come from research strategies that are able to combine what are currently largely distinct approaches. This award seeks to encourage the creation and enhancement of collaborations between scientists from the US and Japan.  More information can be found on the conference web site http://www.stat.cmu.edu/mona2."
"1554123","CAREER: Locally Adaptive Nonparametric Estimation for the Modern Age - New Insights, Extensions, and Inference Tools","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2016","07/21/2020","Ryan Tibshirani","PA","Carnegie-Mellon University","Continuing Grant","Gabor Szekely","06/30/2021","$400,000.00","","ryantibs@berkeley.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 8048","1045","$0.00","Nonparametric modeling---which means, roughly, flexible modeling of smooth trends without specific assumptions about their form or shape---finds diverse applications in many areas such as epidemiology, astrophysics, finance, and artificial intelligence. It is also a field ripe for modern statistical development, since nonparametric models are in a sense even more appealing in the ""big data"" era, as it is precisely in data-rich settings that the increased flexibility of these models will begin to show real rewards in terms of statistical accuracy. The proposed work will develop nonparametric methods (and affiliated software) that will be useful to data scientists who model smooth, nonlinear trends in areas like those mentioned above, as well as many others. A specific scientific emphasis will be the forecasting of influenza and dengue fever. Such forecasts will help policy makers design and implement more effective countermeasures towards these diseases. The proposal puts forward two main ideas for educational training, closely related to the research aims to be pursued. The first is a set of short videos on nonparametric smoothing, intended as supplements to an undergraduate level course called Advanced Methods for Data Analysis. They will be integrated with an interactive quiz system, and will be made freely available (on YouTube) so that others outside the class may watch too. The second idea is a statistical computation training group, for PhD students from Statistics and Computer Science.<br/><br/>""Locally adaptive"" nonparametric methods offer more fine-grained flexibility than traditional nonparametric methods, in that they can simultaneously represent different amounts of smoothness at different parts of the function domain. Currently, locally adaptive nonparametric methods are not often used in big, modern data sets, likely because of their computational inefficiency, and the general inavailability of locally adaptive methods in many modern problem settings. The proposed work seeks to change this, and to push the state of the art in modern locally adaptive nonparametric estimation. The research aims are to: deepen the theoretical understanding of existing locally adaptive methods for univariate problems; efficiently scale these methods and extend these theories to problems where data are collected in high dimensions and over graphs; and develop inferential tools for all of these locally adaptive procedures. The specific contributions will be balanced between the theoretical (statistical theories that describe the underpinnings of the methods in question) and computational (practical algorithms that describe implementation of these methods at scale) perspectives. A final more applied research aim is to use the proposed methods to improve and extend a forecasting system for major epidemics such as influenza and dengue fever."
"1613219","Estimation and Inference for Massive Multivariate Spatial Data","DMS","STATISTICS","08/15/2016","05/20/2016","Joseph Guinness","NC","North Carolina State University","Standard Grant","Nandini Kannan","11/30/2018","$159,999.00","","guinness@cornell.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","8083","$0.00","Satellite observations of the Earth's atmosphere and oceans have the potential to improve forecasting of hurricanes and other extreme weather events. Massive efforts to sample the chemical constituents present in well water can reduce uncertainty in mapping of hazardous materials in groundwater. Observations of chemical reactions at the sub-micron scale may lead to new insights about the behavior of toxic trace elements in soils. However, the value of these expensive efforts to collect massive amounts of data will not be fully realized if the statistical techniques for analyzing them do not keep pace. The current techniques available are inadequate to flexibly model and extract information from massive datasets consisting of many variables collected across a region. This research project aims to develop computationally efficient methods for addressing the central challenges for analyzing massive multivariate spatial data: (1) drawing justifiable conclusions about the relationships among the multiple variables, and (2) making full and appropriate use of all variables when mapping the data. Addressing the first challenge is essential to translating observational and experimental data into scientific knowledge. Addressing the second is crucially important for providing predictions of potentially harmful outcomes, and the key to solving both challenges is integrating the multivariate and spatial data analysis into a unified framework.<br/><br/>The inherent correlation in time series and spatial data is the feature that makes interpolation and forecasting possible, but it also complicates estimation of multivariate relationships. As a result, analyses of time series data often start with a transformation of the data into the spectral domain, in which the transformed data are approximately uncorrelated. Although the spectral domain has played a central role in developing theory for models for spatial data, several issues have hindered the implementation of practical spectral domain methods for spatial data. This project aims to develop methodological innovations to overcome those barriers and provide practitioners with a flexible set of tools to extract information from dozens of spatial variables simultaneously, and predict variables at unsampled locations using all of the available data. The methods employ computationally efficient periodic data augmentations to simplify analyses, dramatically improve the ability to characterize uncertainty, and are supported by novel theoretical results."
"1613035","Variable Selection via Inverse Modeling for Detecting Nonlinear Relationships","DMS","STATISTICS","08/01/2016","08/16/2018","Jun Liu","MA","Harvard University","Continuing Grant","Gabor Szekely","07/31/2020","$200,000.00","","jliu@stat.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269","","$0.00","With the ever-growing amount of data in many application areas, effective methods for detecting factors influencing the value of a response variable are in high demand. It is of growing importance to develop methods for detecting variables that exert significant nonlinear response. Inspired by the sliced inverse regression method developed in the early 1990s, the PI proposes a general framework for developing effective variable selection strategies in nonlinear systems of high dimension. The PI will further study theoretical properties of these variable selection algorithms. The proposed theoretical investigation will provide theoretical understanding of limitations of existing dimension-reduction techniques when the dimensionality grows with the sample size. <br/> <br/>With the ever-growing amount of data in many application areas, effective methods for detecting factors that may influence the value of a target quantity of interest (response variable) are in high demand. The problem is termed as ""variable (or feature) selection"" in regression modeling and statistical learning, and is a long-standing problem in statistics and machine learning. The PI focuses here on the detection of factors that may exert nonlinear and/or interactive effects on the response variable. Recent studies from the PI's group reveal that the sliced inverse regression (SIR) and inverse modeling strategies provide a powerful framework for developing effective variable selection strategies in nonlinear systems of high dimension. The PI aims at developing more robust and effective tools for detecting such complex relationships and studying theoretical properties of SIR-based algorithms. The proposed method will also be applicable to do robust variable selection for classification problems. The proposed theoretical investigations will provide (a) theoretical understanding of limitations of existing dimension-reduction techniques when the dimensionality grows with the sample size; (b) guidance on the construction of necessary sparsity conditions that can guarantee consistency of variable selections in ultra-high dimensional nonlinear problems; (c) the optimal convergence rate of that the best possible learning algorithm can achieve in such settings; and (d) theoretical justifications whether the proposed algorithms can achieve or are not far from the optimality."
"1613016","A New Approach Toward Optimal and Adaptive Nonparametric Methods for High-Frequency Data","DMS","STATISTICS","09/01/2016","08/30/2016","Jose Figueroa-Lopez","MO","Washington University","Standard Grant","Gabor Szekely","08/31/2019","$99,949.00","","figueroa@math.wustl.edu","ONE BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","1269","9150","$0.00","High frequency monitoring of complex systems, which operate in continuous time, is increasingly important in many fields such as neural science, turbulence, and environmental sciences. This tendency, however, has been particularly predominant in financial markets with the advent of transaction and Limit Order Book data sets, which constitute two prototypical examples of ""big data"" in statistics. In light of the just mentioned technological advances, statistical inference methods for continuous time processes based on high-frequency observations have seen a rapid evolution in the last few years. One of the crucial issues with the current state of art of the subject lies in the fact that most of the proposed methods critically depend on tuning parameters that need to be calibrated. This, of course, is the case with most of the nonparametric methods used in other classical statistical problems. However, the extensive literature for resolving these problems in other frameworks has not yet been fully translated into the context of high-frequency-based inference for stochastic processes. Another important issue comes from the common practice of adopting artificial models specified by stochastic dynamical systems, which are known to lack sufficient accuracy for describing the stylized features of asset prices at ultra high frequency. This, in turn, has motivated the introduction of the concept of microstructure noise, but sometimes, again, assuming unnatural assumptions. Hence, there is a real need for bottom-up derivations of models that allow a better understanding of the underlying asset price formation.<br/><br/>The principal investigator will tap on the previously mentioned needs and is expected to significantly advance the area in the following three primary directions of theoretical and practical relevance: (1) Devise new methodologies towards the implementation of ""optimal"" inference methods in regard to the intrinsic tuning parameters of the methods; (2) Develop a new approach, together with the necessary theoretical foundations, for adaptive estimation methods based on data-driven fixed point procedures; (3) Better incorporation of Limit Order Book data in the modeling of both the underlying approximating jump-diffusion process and the microstructure noise with a view of enhancing the estimation of latent process parameters based on limit order book information."
"1612906","Large-Scale Multiple Testing for High-Dimensional Covariance Structures with Applications to Genomics and Neuroimaging","DMS","STATISTICS","06/01/2016","05/27/2016","Yin Xia","NC","University of North Carolina at Chapel Hill","Continuing Grant","Gabor Szekely","05/31/2019","$53,613.00","","xiayin@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","7433, 8091","$0.00","Analysis of high-dimensional data has emerged as one of the most important and active areas of research in statistics. In particular, hypothesis testing for high-dimensional covariance structures is an interesting and challenging problem with a wide range of applications in areas such as genomics, medical imaging, and the social sciences.  The methodology developed in this project will be used to study important problems in genomics and neuroimaging, including identification of interactions between gene pathways and detection of spatial voxels that are activated due to certain experimental stimuli.  The proposed methods will be applied to a breast cancer gene expression study and data from fMRI experiments. <br/> <br/>In this project, the investigator aims to develop methodology and theory for large-scale multiple testing on high-dimensional covariance structures. The lack of suitable test statistics, as well as the complex entry-wise dependence structures, impose significant methodological and technical challenges not seen in the conventional multiple testing problem. The proposed research is anticipated to make significant contributions to large-scale and high-dimensional inference for covariance structures and its applications to genomics and neuroimaging. Web pages will be created to enable quick access to software implementation of the new methods, as well as technical reports and relevant references."
"1619855","Conference on Statistical Machine Learning and Data Science","DMS","STATISTICS","06/15/2016","06/09/2016","Yufeng Liu","NC","University of North Carolina at Chapel Hill","Standard Grant","Gabor Szekely","05/31/2017","$15,000.00","","yfliu@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","7556","$0.00","The University of North Carolina at Chapel Hill will host a three day conference on Statistical Machine Learning and Data Science from June 6-8, 2016. (See http://www.unc.edu/~yfliu/sldm2016/.) The objective is to bring together researchers in statistical learning and data science from academia, industry, and government in a stimulating atmosphere focused on the development of statistical learning theory, methods and applications. Statistical machine learning is a relatively new discipline, evolving from machine learning methods of artificial intelligence and multivariate statistics. It also plays an essential role for the new important area of data science and big data. NSF funding will provide travel support to increase the number of junior researchers who are able to attend.<br/><br/>The Section on Statistical Machine Learning and Data Science of the American Statistical Association will hold a meeting at the University of North Carolina at Chapel Hill from June 6-8, 2016. Topics of the conference include, but are not limited to, big data analytics, classification, computational biology, covariance estimation, graphical models, high dimensional data, learning theory, model selection, network analysis, precision medicine, and signal and image processing. This award will provide travel support for junior researchers."
"1613112","Geometric Perspectives on the Correlation","DMS","STATISTICS","08/01/2016","08/04/2016","Kai Zhang","NC","University of North Carolina at Chapel Hill","Standard Grant","Gabor Szekely","07/31/2019","$120,000.00","","zhangk@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","In modern statistical analysis, datasets often contain a large number of variables with complicated dependence structures. This situation is especially common in important problems in economics, engineering, finance, genetics, genomics, neurosciences, etc. One of the most important measures on the dependence between variables is the correlation coefficient, which describes their linear dependence. In the new paradigm described above, understanding the correlation and the behavior of correlated variables is a crucial problem and prompts statisticians to develop new theories and methods. Motivated by this challenge, the PI proposes to study the correlation through novel geometric perspectives. The overall objective is (1) to develop useful theories and methods on the correlation and (2) to build a stronger connection between geometry and statistics. The PI anticipates the achievement of his goals through an integration of research and education plans.<br/><br/>The research agenda is to systematically investigate three fundamental aspects of the correlation: (1) the magnitude and distribution of the maximal spurious sample correlation; (2) the detection of a low-rank correlation structure; and (3) the probability measure over the space of correlation matrices. In these studies, the novel integration of statistical and geometric insights characterizes the proposed solutions and facilitates precise probability statements. Completion of the proposed research will provide a comprehensive understanding of the correlation and a stronger connection between geometry and statistics. The PI also has comprehensive plans on educating graduate and undergraduate students and on disseminating the research results to the broader scientific community."
"1613072","Iterative testing procedures and high-dimensional scaling limits of extremal random structures","DMS","STATISTICS","08/01/2016","08/13/2018","Andrew Nobel","NC","University of North Carolina at Chapel Hill","Continuing Grant","Gabor Szekely","07/31/2020","$374,999.00","Sreekalyani Bhamidi","nobel@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","Over the past ten years, networks and network models have seen increasing use and importance in a variety of fields, including economics, neuroscience, genomics, and biomedicine. Work in these fields has driven an increase in statistical research concerning modeling of, and inference about, complex networks. The PIs will pursue several new directions in statistical network research, a key theme being the application and extension of recent work in probability on the theory of complex, random and geometric networks. In particular, the PIs will develop iterative testing methods to identify relational changes in large data sets, and to enhance the power of genomic studies that link genetic variation to global changes in gene expression. They will extend existing probabilistic techniques to provide theoretical support for the iterative testing procedure, and to address broader statistical questions concerning inference about complex associations between the features of large, high dimensional data sets. Methodological development and application will be carried out in cooperation with researchers in genomics, biomedicine, and sociology at UNC, with whom the PI and co-PI have long standing collaborations. <br/><br/><br/>Motivated in large part by the increasing use and importance of networks in a variety of fields, there has been a great deal of work in the statistics community devoted to the problem of testing and estimating associations between variables in high dimensional data sets. Concurrent with this statistical activity, recent developments in the fields of probabilistic combinatorics have significantly advanced our understanding of discrete random structures that capture the association of high-dimensional objects. The PIs will bring a number of these probabilistic tools to bear on association based inference problems. In particular, the PIs will develop and implement an iterative testing procedure that identifies self-associated sets of vertices in a graph, and self-associated sets of variables in a high dimensional data set. Within the framework of the iterative testing procedure they will develop computationally efficient methods for several applied problems: mining of block correlation differences in two sample studies, and identifying groups of mutually correlated variables in studies where each sample is assessed with two or more measurement platforms. As a special case of the latter problem, they will develop tools to enhance the power of genomic studies that link local genetic variation to global changes in gene expression.  A second component of the proposed research is to adapt and extend existing techniques in probabilistic combinatorics to provide supporting theory for the iterative testing procedure, and to address broader statistical questions concerning the testing and estimation of correlations. Development and application of the methods will be carried out in cooperation with researchers in genomics, biomedicine, and sociology at UNC, with whom the PI and co-PI have long standing collaborations."
"1612948","Statistical Estimation in Resource-Constrained Environments:  Computation, Communication and Privacy","DMS","STATISTICS","07/01/2016","08/13/2018","Martin Wainwright","CA","University of California-Berkeley","Continuing Grant","Gabor Szekely","06/30/2020","$300,000.00","","wainwrigwork@gmail.com","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","","$0.00","The past decade has witnessed an explosion in the scale and richness of data sets that arise in both science and engineering.  A wide variety of application areas have lead to large-scale data sets. Examples include social networks such as Facebook, on-line recommender systems such as Amazon and Netflix, neuroscience data including fMRI, EEG, and brain-machine interfaces, and image/video processing which includes face recognition, surveillance, and security.  All of these areas require effective methods for statistical inference---that is, methods that lead to actionable conclusions from the data. The classical approach in statistics is to study inference algorithms without consideration of their computational and storage requirements; this approach leads to many methods that simply cannot be implemented for large-scale problems.  The goal of this research is to develop a principled framework for characterizing the fundamental limits of statistical estimation under computational and storage constraints.  This shift in perspective will lead to the development of new and computationally efficient methods for statistical estimation in resource-constrained environments.<br/><br/>While the notion of minimax risk characterizes the fundamental limits of statistical estimation, it is based on taking an infimum over all measurable functions of data, thereby allowing for estimators that have exponential computational complexity, require prohibitive amounts of storage, and/or reveal sensitive data.  The goal of this proposal is to study various constrained forms of statistical minimax based on limiting the class of possible estimators.  The proposed work is interdisciplinary in nature, combining ideas from mathematical statistics, information theory, optimization theory, and computational complexity.  The first research thrust concerns the tradeoffs between computational costs and statistical accuracy.  The main goal is to understand when there are gaps between the classical minimax risk, and the lowest risk achievable by algorithms that run in polynomial-time. Specific model classes of interest include high-dimensional forms of sparse regression, sparse principal component analysis, and classification problems in neural networks.  The second research thrust focuses on estimation in distributed settings.  Many data sets are so large so that they cannot be stored at a single central location, but instead must be split into many pieces, and stored on separate machines that can communicate only relatively small amounts of information. Thus, an important problem is to characterize the minimal amount of communication needed for a distributed implementation to match the performance of the centralized estimator."
"1612456","A Spectral Framework for Network-Driven Sampling","DMS","STATISTICS, Methodology, Measuremt & Stats, SCIENCE RESOURCES STATISTICS, ","09/01/2016","08/19/2016","Karl Rohe","WI","University of Wisconsin-Madison","Standard Grant","Gabor Szekely","08/31/2019","$172,365.00","","karlrohe@stat.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269, 1333, 8800, p239","","$0.00","Probability sampling drastically reduces the burden of research in various disciplines because statistical inference can extend conclusions from a sample to the entire population.  However, classical sampling techniques require a sampling frame that lists each individual in the population and a way of contacting each individual. In many settings, a sampling frame is not available.  In others, a sampling frame is too expensive to compile or only covers a biased subset of the population.  Particularly with hard-to-reach populations, network-driven sampling provides one of the only ways to find members of the population. Leveraging a network to find a target population appears in many disciplines with a multitude of names: respondent-driven sampling, snowball sampling, web crawling, link-tracing, breadth-first search, co-immunoprecipitation, and chromatin immunoprecipitation.  These disparate techniques all provide access to hard-to-reach and networked populations by essentially asking participants to refer friends. As a result, these are all network-driven techniques. Classical sampling theory does not apply to network-driven sampling because friends are similar; this induces dependence between samples that is influenced by the underlying social network.  Preliminary research conducted by the investigator identifies a critical threshold that relates the structure of the social network to the referral rate in the sampling tree; beyond this critical threshold, standard network-driven approaches produce highly uncertain estimates. This research aims to produce new statistical techniques that continue to perform well beyond the critical threshold. Moreover, this project will study novel forms of network-driven data collection that incorporate additional information to produce more representative samples. <br/><br/>Classical sampling results are not applicable to network-driven sampling because friends are similar, inducing dependence between samples.  Previous theoretical results show that some network-driven studies do not obtain square root n-consistent estimators. Whether a study obtains square root n-consistency depends on both (i) the spectral properties of the underlying social network and (ii) the growth of the sampling tree. This research aims to provide new estimators that correct for the dependence between samples. These dependence-corrected estimators can obtain square root n-consistency, even when current estimators do not. This project will also construct new diagnostics and new sampling designs for network-driven sampling. The new spectral framework will provide a suite of theory, methodology, and practices that will enable studies to obtain square root n-consistent estimators."
"1613218","Resampling Methods for High-Dimensional and Large-Scale Data","DMS","STATISTICS","07/01/2016","05/20/2016","Miles Lopes","CA","University of California-Davis","Standard Grant","Gabor Szekely","06/30/2020","$150,000.00","","melopes@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","Resampling methods are a broad class of tools that serve to measure the variability of statistical results, for example, allowing a researcher to determine whether or not the outcome of an experiment is significant.  Over the course of the last few decades, these methods have been extensively studied, and they have become fundamental to the practice of statistics - in large part because they can solve complex problems while relying on relatively few assumptions. Nevertheless, much remains to be understood about the performance of resampling methods in the context of modern data analysis, where observations tend to have large numbers of features (high-dimensional data), or where the quantity of data is so large that it outstrips computational resources (large-scale data). In both of these challenging settings, the proposed research will extend the applicability of resampling methods, and these efforts will be guided by two research themes discussed below.<br/><br/>First, in the setting of high-dimensional data, the understanding of inference problems, including tests and confidence intervals, remains underdeveloped in comparison with estimation and prediction problems. Given that resampling methods are a general-purpose approach to inference, it is important to know how they are influenced by the effects of low-dimensional structure and regularization. In particular, the proposed research will study the performance of resampling methods in high-dimensional models involving structured covariance matrices. Second, in the setting of large-scale data, randomized algorithms have received growing attention for their ability to produce fast approximate solutions. Although the outputs of such algorithms are random, their fluctuations can often be reduced at the expense of greater computation. This general trait of randomized algorithms leads to the problem of optimizing a tradeoff between precision and computational cost. Towards a solution, the proposed research will investigate how resampling methods can be used to measure this tradeoff for a collection of popular randomized algorithms."
"1613338","Development of a general classification framework under the Neyman-Pearson Paradigm, with biomedical and social applications","DMS","STATISTICS","08/15/2016","08/11/2016","Xin Tong","CA","University of Southern California","Standard Grant","Gabor Szekely","10/31/2019","$120,000.00","Jingyi Jessica Li","xint@marshall.usc.edu","3720 S FLOWER ST FL 3","LOS ANGELES","CA","900074304","2137407762","MPS","1269","","$0.00","Classification has broad applications in various fields, including biological sciences, medicine, engineering, finance, and social sciences. The aim of classification is to accurately predict class labels for new observations based on labeled training data. For example, an email service provider needs to decide whether an incoming email is spam.   Among different types of classification problems, binary classification is the most basic and important type for theoretical, methodological and algorithmic development. An important question in binary classification is how to control a prioritized type of error, either the type I error (the chance of misclassifying a class 0 data point as class 1) or the type II error (the chance of misclassifying a class 1 data point as class 0). The Neyman-Pearson (NP) classification paradigm is a theoretic framework aiming to control the type I (or type II) error with theoretic guarantee. Yet how to implement the NP paradigm with practical classification algorithms remains a great challenge. In this research, the PIs will tackle this challenge by developing new statistical theory, methods, algorithms, and a novel evaluation metric under the NP paradigm. Results from this proposal will have broad potential applications, such as reducing false positive rates in disease diagnosis and improving prediction accuracy of social events from social media data. The PIs will supervise graduate and undergraduate students of diverse background in the proposed project, and the project outcomes will be taught in graduate-level seminar courses. To aid statistical and interdisciplinary research, the PIs will distribute methods developed in this project as open-source software packages.<br/><br/><br/>The PIs will develop new statistical theory, methods, algorithms and applications to control asymmetric classification errors under the Neyman-Pearson (NP) paradigm. The NP paradigm addresses cases where users insist on a specific bound on type I error while keeping type II error to a minimum. Although the NP paradigm has a century-long history in hypothesis testing, until recently it did not receive much attention in the classification area, and its theory and methodologies are as yet incomplete. With the following four aims, the PIs will develop a general NP classification framework and show how it can be applied in the biomedical and social sciences. Under Aim I, the PIs will develop new NP classification theory and methods by exploring feature dependency and interactions for different data structures and sample sizes. Under Aim II, the PIs will design an umbrella algorithm to adapt popular classification methods to the NP paradigm. Under Aim III, the PIs will construct an NP version of Receiver Operating Characteristic (ROC) curves: ""NP-ROC"", a new evaluation metric based on the NP classification theory and methodologies. Under Aim IV, the PIs will apply the novel NP classification methodologies developed in Aims I-III to large-scale biomedical and social applications."
"1613190","Group-Specific Individualized Modeling and Recommender Systems for Large-Scale Complex Data","DMS","STATISTICS","09/01/2016","11/15/2019","Xiaofeng Shao","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Gabor Szekely","08/31/2020","$250,001.00","","xshao@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","","$0.00","This research project aims to develop new statistical theory, methods, and computing algorithms to solve practical problems where the data present unique features such as large volume, large velocity of dynamic changes, and highly heterogeneous information from different individuals. The traditional one-model-fits-all paradigm may not have sufficient power to detect important predictors for heterogeneous subgroups. This research aims to develop alternative methods applicable to electronic health record data and valuable for assigning effective personalized treatments for more effective medical care. It is anticipated that the project will stimulate interdisciplinary collaborations with other scientists from disparate fields and that the work will also have applications in marketing, business, and financial services. The software under development will be disseminated to facilitate applications for large-scale complex data, and will be made available to industry in a timely manner to maximize the impact on society. Training of graduate students through involvement in the research is a part of this project.<br/> <br/>This project aims to develop a new collaborative filtering method utilizing cluster information from users and items to provide more efficient recommender systems.  The research also targets the development of personalized variable selection, while improving the estimation efficiency of the personalized variable coefficients and the prediction power. In addition, a mixed-effects estimating equation approach will be developed to reduce the estimation bias for informative missing data. Another research goal is to develop efficient computational algorithms and tools applicable for large-scale complex data. Each component of the research plan contains a range of topics, from methodological and computational development to applications in real world problems. In addition, the project will help to tackle fundamental questions in statistical science and will stimulate interest from large groups of scientists in the fields of recommender systems, random effects modeling, high-dimensional model selection, subgrouping and clustering, longitudinal/correlated data, informative missing data, and refreshment sampling.  The development of advanced optimization techniques, algorithms, and computational technology will be valuable for other types of complex data problems as well."
"1607489","Collaborative Research: Statistical Inference for Functional and High Dimensional Data with New Dependence Metrics","DMS","STATISTICS","06/01/2016","05/24/2016","Xiaofeng Shao","IL","University of Illinois at Urbana-Champaign","Standard Grant","Gabor Szekely","05/31/2020","$184,996.00","","xshao@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","8083","$0.00","Due to the rapid development of information technologies and their applications in many scientific fields such as climate science, medical imaging, and finance, statistical analysis of high-dimensional data and infinite-dimensional functional data has become increasingly important. A key challenge associated with the analysis of such big data is how to measure and infer complex dependence structure, which is a fundamental step in statistics and becomes more difficult owing to the data's high dimensionality and huge size. The main goal of this research project is to develop new dependence measures for quantifying dependence of large scale data sets such as temporally dependent functional data and high dimensional data, and utilize these new measures to develop novel statistical tools for conducting sparse principal component analysis, dimensional reduction, and simultaneous hypothesis testing. Building on the new dependence metrics that can capture nonlinear and non-monotonic dependence, the methodologies under development are expected to lead to more accurate prediction and inference, as well as more effective dimension reduction in the analysis of functional and high dimensional data. <br/><br/>The research consists of three projects addressing different challenges in the analysis of functional and high dimensional data. In Project 1, the investigators introduce a new operator-valued quantity to characterize the conditional mean (in)dependence of one function-valued random element given another, and apply the newly developed dependent metrics to do dimension reduction for functional time series under a new framework of finite dimensional functional data. In Project 2, the investigators explore a new dimension reduction framework for regression models with high dimensional response, which requires less stringent linear model assumptions and is more flexible in terms of capturing possible nonlinear dependence between the response and the covariates. In Project 3, the investigators develop new tests for the mutual independence of high dimensional data via distance covariance and rank distance covariance using both sum of squares and maximum type test statistics. Overall, the three lines of research are all related to big data, and they touch upon various aspects of modern statistics; the project aims to push the current frontiers in areas including sparse principal component analysis, inference for dependent functional data, and high dimensional multivariate analysis to another level."
"1613005","Theory and Methods for Simultaneous Signal Analysis in Integrative Genomics","DMS","Genetic Mechanisms, STATISTICS, Cross-BIO Activities, MSPA-INTERDISCIPLINARY","08/15/2016","05/20/2016","Sihai Zhao","IL","University of Illinois at Urbana-Champaign","Standard Grant","Gabor Szekely","07/31/2019","$353,663.00","","sdzhao@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1112, 1269, 7275, 7454","7433, 8007","$0.00","The main objective of this project is to develop new methodology and theory for statistical challenges motivated by integrative genomics, a collection of quantitative approaches in genomics research that centers around the joint analysis of multiple datasets.  Integrative analysis has tremendous potential to precipitate the next wave of scientific discoveries in genomics and is also a crucial component of emerging conceptions of data science in general. This project aims to develop new analysis procedures for 1) detecting whether two or more datasets share the same significant genomic features, 2) identifying these features, and 3) leveraging them to improve genomic prediction models.<br/><br/>This project opens important questions in integrative genomics up to rigorous methodological and theoretical development by framing them in terms of cutting-edge statistical issues, including signal detection, multiple testing, and high-dimensional classification. The proposed methodological research will develop new nonparametric tests and false discovery rate control procedures for detecting and identifying shared genomic features, as well as new nonparametric empirical Bayes approaches for high-dimensional integrative classification and regression. The proposed theoretical research will explore the fundamental limits of these problems. The results of this project will lead to more powerful and rigorous methods and theory for integrative analysis in genomics and elsewhere."
"1564438","FRG: Collaborative Research: Innovations in Statistical Modeling, Prediction, and Design for Computer Experiments","DMS","STATISTICS","07/01/2016","07/02/2018","C. F. Jeff Wu","GA","Georgia Tech Research Corporation","Continuing Grant","Gabor Szekely","06/30/2020","$390,503.00","","jeffwu@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","1616","$0.00","The explosive growth in the use of computer simulators in the last fifteen years has helped galvanize a revolution in scientific, engineering, and biological research that includes advances in the aerospace industry, material science, renewable energy, and biomechanics. Researchers can make a detailed exploration of scientific design alternatives under a wide set of operating environments using runs from a simulator of a physical system, possibly coupled with those from a traditional physical system experiment. This research project will advance the statistical modeling, design, and analysis of experiments that use computer simulators. The first research area is Improved Modeling of Simulator Output: The investigators will develop flexible stochastic models that will allow more accurate prediction in settings where the simulator provides related multivariate output of the performance of a physical system. Current prediction models either assume output independence (knowledge of one output gives no information about other outputs) or a linear dependence on a common set of latent drivers. The second research area concerns Advances in Emulation: The investigators aim to devise efficient emulators of simulator output for novel input and output settings such as when gradient information is available or when the output consists of both point and integrated measures. They plan to construct predictors that incorporate natural invariances present in the simulator output. For example, the predicted response should be constant under permutations of the inputs when the output satisfies this condition; the project will quantify the uncertainty in the invariant predictors. The investigators also plan to quantify the uncertainty of a recent, theoretically-justified method of calibrating computer simulators based on physical experimental data. The third research area is the Design of Simulator Experiments: Efficient designs of simulator experiments will be devised to minimize the computational effort required to determine the sensitivity of a simulator output to each of its inputs. <br/><br/>This research will build a statistical framework for the modeling, design and analysis of experiments that employ computer simulators. The specific goals are (1) to devise flexible interpolating stochastic models for computer simulators with multivariate output; (2) to invent efficient predictors for novel input and output settings such as when gradient information is available or when the output consists of both point and integrated responses; (3) to develop emulators of simulator output that incorporate the same invariances present in the simulator responses; (4) to quantify the uncertainty of L2 calibrated predictors for expensive computer codes; and (5) to construct new sliced Latin hypercube designs to allow the efficient calculation of global sensitivity indices. The investigators will develop new modes for training statistics graduate students having interests in engineering applications. Opportunities will be created for subject matter specialists to provide critical practical challenges in three areas: aerospace/mechanical engineering, biomechanics, and material science, and to conduct joint applied projects with the researchers."
"1737929","Collaborative Research: New statistically-motivated solutions to classical inverse problems","DMS","STATISTICS","09/01/2016","02/28/2017","Ryan Martin","NC","North Carolina State University","Standard Grant","Gabor Szekely","07/31/2020","$124,354.00","","rgmarti3@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","","$0.00","Numerous scientific questions assume the form of inverse problems, in which an unknown input to a system under study gives rise to an observed noisy output, and the goal is to estimate the input from the output. An example of such a problem is calculating the density of the Earth from measurements of the local gravitational field.  Only rarely can such inverse problems be solved analytically, and in general numerical approximations are required to find solutions. In this research project, the investigators aim to introduce a novel iterative algorithm for solving inverse problems, develop its theoretical and computational properties, and establish its performance in applications.  It is anticipated that the new algorithm will be adaptable to a range of problems currently under investigation in applied and numerical mathematics, for example in solving a sparse system of linear equations, currently of great interest in areas including tomography, archaeology, astrophysics, and other sciences.<br/><br/>This research project explores a novel iterative algorithm for the solution of a class inverse problems that includes Fredholm integral equations of the first kind, Laplace transform inversion, mixing distribution estimation in statistics, and solving sparse systems of linear equations. The investigators plan to (i) introduce a novel iterative algorithm for solving inverse problems of these types, and perhaps others, (ii) develop its theoretical and computational properties, and (iii) establish its performance in applications. A motivation for the research is the statistical problem of estimating a mixing density in a nonparametric mixture model. To date, there are no general algorithms that produce globally consistent estimators of the mixing density, in the sense of almost sure convergence with respect to a strong metric; only weak convergence results are available. An important feature of the algorithm under development is that, if it is initialized at a smooth density function, then the estimator is necessarily also a smooth density function. Other algorithms designed by numerical analysts for solving these inverse problems do not have this closure property. The form of the novel iterative algorithm, along with the fact that it yields smooth density estimators, suggests that this open problem can be solved; the investigators aim to establish a general global consistency result and demonstrate rates of convergence."
"1609699","Collaborative Research: Estimation of Large Species/Population Trees Using Tree Space","DMS","STATISTICS","08/01/2016","08/04/2016","Arindam RoyChoudhury","NY","Columbia University","Standard Grant","Gabor Szekely","04/30/2018","$149,999.00","","arr2014@med.cornell.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","","$0.00","The estimation of the evolutionary history of a collection of organisms based on the information contained in their DNA sequences is a problem of fundamental importance in evolutionary biology. The abundance of DNA sequence data arising from genome sequencing projects has led to important computational challenges in the estimation of these phylogenetic relationships. Among these challenges is the estimation of the evolutionary history for a group of species based on DNA sequence information from several distinct genes sampled throughout the genome. This research is focused on the development of computationally efficient methods for estimating the evolutionary history when the number of species under consideration is very large (i.e., hundreds to thousands). This is accomplished by considering collections of three species at a time, and using properties of the estimated evolutionary history for groups of three to infer the overall evolutionary history. Properties and performance of the method will be evaluated theoretically as well as with both simulated and empirical data sets. This work has numerous practical applications, such as the study of the evolutionary relationships among human populations.<br/><br/>Though the amount of genomic data available for inferring phylogenetic species trees has increased rapidly within the last 10 years, few methods have been developed to efficiently estimate species trees for data sets consisting of hundreds or thousands of species. A fast approximation to the maximum likelihood estimate (MLE) that retains desirable statistical properties, such as consistency and asymptotic efficiency, is proposed. Results from preliminary work suggest that this approach will be significantly faster than existing likelihood and Bayesian approaches, while also being highly accurate. The method can be applied to a range of data types, including allele frequency data arising under a Brownian motion model along the phylogeny and single nucleotide polymorphism (SNP) data arising from the coalescent model. A software package will be developed to implement the methodology. The project will also support one PhD student, who will contribute to the development and implementation of the methodology."
"1623028","Higher-Order Asymptotics and Post-Selection Inference","DMS","STATISTICS","08/01/2016","06/08/2016","Todd Kuffner","MO","Washington University","Standard Grant","Gabor Szekely","07/31/2017","$10,000.00","","kuffner@math.wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","MPS","1269","7556, 9150","$0.00","The Workshop on Higher-Order Asymptotics and Post-Selection Inference (WHOA-PSI) will be held on the campus of Washington University in St. Louis from September 30-October 2, 2016. (See http://www.math.wustl.edu/~kuffner/WHOA-PSI.html.) Post-selection inference is relevant to virtually all areas of statistical application. Higher-order asymptotics provides the tools and insights needed to refine basic large-sample results for post-selection inference procedures, and make them more accurate in the practically realistic setting of small to moderate sample sizes. WHOA-PSI seeks to merge these fields, foster collaboration and discussion, and push forward the post-selection inference research frontier in the direction of higher-order accuracy for inference procedures. NSF funding will provide travel support for junior researchers to attend this conference.<br/><br/>Washington University will host a workshop on higher order asymptotics and post-selection inference September 30 ? October 2, 206. Speakers include Brad Efron, Jianqing Fan, and Rob Tibshirani. This workshop attempts to address the needs of applied statisticians utilizing model selection procedures who also need extremely accurate inference procedures. The workshop will begin with tutorials on the main topics. This award provides funding for junior researchers to attend the conference."
"1613193","Collaborative Research: Scalable Bayesian Methods for Complex Data with Optimality Guarantees","DMS","STATISTICS","07/01/2016","05/24/2016","Anirban Bhattacharya","TX","Texas A&M University","Standard Grant","Gabor Szekely","06/30/2020","$134,138.00","","anirbanb@stat.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1269","7433, 8083","$0.00","Spectacular advances in data acquisition, processing, and storage present the opportunity to analyze datasets of ever-increasing size and complexity in various applications, such as social and biological networks, epidemiology, genomics, and Internet recommender systems. Underlying the massive size and dimension of these data, there is often a parsimonious structure. The Bayesian approach to statistical inference is attractive in this context in terms of incorporating structural assumptions through prior distributions, enabling probabilistic modeling of complex phenomenon, and providing an automatic characterization of uncertainty. This research project aims to advance eliciting and translating prior knowledge regarding the low-dimensional skeleton of big data to provide realistic uncertainty characterizations while maintaining computational efficiency. Bayesian computation poses substantial challenge in high-dimensional and big data problems. The research aims to develop cutting-edge computational strategies and software packages for implementation to be made available publicly. The project involves graduate students in the research.<br/><br/>The research project focuses on theoretical foundations and computational strategies for Bayesian methods in high-dimensional and big data problems motivated by applications in social networks and epidemiology. Techniques for systematically developing and evaluating prior distributions in high-dimensional problems will be investigated with a special emphasis on the trade-off between statistical efficiency and computational scalability. Specific directions include efficient algorithms for posterior sampling with shrinkage priors, a theoretical framework for divide and conquer strategies in big data problems, fast algorithms for clustering nodes in large networks with unknown number of communities, and methods for discovering structure in sparse contingency tables. The algorithms will be motivated by rigorous theoretical understanding of the behavior of the posterior distribution with a particular emphasis on proper quantification of uncertainty in a distributed computing framework. Software will be developed for each application."
"1737933","Collaborative Research: Optimal Bayesian Concentration Rates from Double Empirical Priors","DMS","STATISTICS","09/01/2016","02/28/2017","Ryan Martin","NC","North Carolina State University","Standard Grant","Gabor Szekely","07/31/2018","$87,369.00","","rgmarti3@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","","$0.00","Statisticians frequently encounter problems that involve complicated models with high-dimensional parameters, particularly in ""big data"" settings. From a Bayesian perspective, it is imperative in these problems that the prior distribution be chosen to sit in a good position. Information about where is a good starting position can come from the data. There is a potential danger with this basic strategy, namely, that a double use of data might cause the model to track the data too closely, resulting on over fitting. To avoid this, the PIs introduce a regularization technique that suitably re-weights the likelihood, preventing the model from learning too quickly. This general ""double empirical Bayes"" strategy, where the prior is centered on the data and the likelihood is re-weighted, will be applied to several important and challenging high-dimensional problems, including estimation of sparse high-dimensional precision matrices, which is relevant to estimation of large complex networks. <br/><br/>In this project, the PIs will develop this new double empirical Bayes framework for inference on high-dimensional parameters with a relatively low ""complexity"" or ""effective dimension"". For example, in function estimation problems, posited smoothness on the function is a constraint on its complexity. The first step of the double empirical Bayes strategy is to use a prior, indexed by the complexity of the parameter, centered at a complexity-specific estimate of the parameter based on data. To prevent the posterior from tracking the data too closely, the second step is to re-weight the likelihood to be combined with the data-dependent prior. The result is a sort of posterior distribution on the parameter space, and the PIs will provide general conditions for this posterior to concentrate around the truth at optimal rates. An additional advantage of this new approach is that the complexity-specific priors, for suitable centering, can be taken of relatively simple form, which facilitates computation. The PIs will investigate the double empirical Bayes analysis of several important high-dimensional inference problems, including density and function estimation, variable selection problems in non-linear models, and estimation of sparse precision matrices. Software will be developed for each application."
"1614145","Conference on Frontiers in Applied and Computational Mathematics","DMS","APPLIED MATHEMATICS, STATISTICS","06/01/2016","05/03/2016","Michael Booty","NJ","New Jersey Institute of Technology","Standard Grant","Victor Roytburd","05/31/2017","$27,000.00","Michael Siegel, Amitabha Bose","booty@njit.edu","University Heights","Newark","NJ","071021982","9735965275","MPS","1266, 1269","7556","$0.00","The Principal Investigators will organize the conference ""Frontiers in Applied and Computational Mathematics 2016"", to be held at New Jersey Institute of Technology (NJIT), Newark, New Jersey, June 3-4, 2016.  Contributions from all areas of applied mathematics and statistics are welcome.  The main conference themes will be on mathematical and computational aspects of materials science, mathematical biology, wave propagation, biological and microscale fluid dynamics, statistics, and biostatistics.  Each of these topics is at the forefront of current developments of mathematical and computational techniques.  Together they have practical applications that include the understanding and development of new materials, the central nervous system, acoustic and electromagnetic wave phenomena, the behavior of cellular and other soft matter, and improved analysis of data from clinical studies and spatial and societal statistics.  The conference participants will include mathematicians, statisticians, scientists and engineers from academia, industry, and government laboratories.  The conference will bring together students, junior researchers, and leaders in their field from the US and other countries to interact and exchange ideas in an informal and approachable setting.  A goal of the organizers is to introduce future leaders of applied mathematics and statistics to established investigators and emerging research areas.  Participation among graduate students and postdocs is greatly encouraged through selected contributed talks being given in minisymposia alongside leading scientists.  For students and postdocs, this will be a learning and networking experience that will help them with their research and career paths.  Special efforts will be made to continue the participation of underrepresented groups.<br/><br/>The two-day conference will include four plenary talks, one in each of this year's focus topics: materials science, the interface between statistics and neuroscience, wave propagation, and biologically motivated fluid dynamics.  Morning and afternoon minisymposia will be given in three parallel sessions: one in an area of statistics or biostatistics, and two in this year's focus areas of applied mathematics.  Poster presentations will be actively encouraged from all areas of applied mathematics and statistics.  The conference is expected to attract 140-160 participants.  Substantial funds will be devoted to support the participation of students, postdoctoral fellows, junior researchers, and underrepresented groups.  The Department of Mathematical Sciences at NJIT has hosted a conference of this type on different focus areas each year since 2004.  More information can be found at the conference website http://m.njit.edu/Events/FACM16/"
"1612901","Design and Analysis of Optimization Experiments with Internal Noise to Maximize Alignment of Carbon Nanotubes","DMS","STATISTICS","09/01/2016","08/31/2016","Tirthankar Dasgupta","MA","Harvard University","Standard Grant","Gabor Szekely","07/31/2017","$150,000.00","Chad Vecitis","tirthankar.dasgupta@rutgers.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","7237, 8037","$0.00","Over the past several decades, carbon nanotubes (CNT) have risen to the forefront of scientific research due to their unique electrical, mechanical and optical properties. However, transferring these properties from nanoscale materials to industrial-scale products often requires alignment (orientation in the same direction) of CNT.  One of the important consequences of alignment is improved conductivity, a highly desirable property in electro-chemical water treatment and closely associated with research endeavors to improve quality of drinking water. Therefore, identification of scalable and cost-effective experimental conditions that maximize alignment of CNT is an important research problem. This is addressed in the project.<br/><br/>The proposed research aims to establish statistical methodologies for designing and analyzing efficient experiments that determine conditions for maximizing alignment of CNT, when one or more input factors are prone to internal noise. The proposed research consists of three tasks, with particular focus on addressing the challenges arising from presence of factors with internal noise and complexity of the response surface. (i) Developing a Bayesian approach to response-surface optimization with noisy inputs. Such an approach allows the experimenter to combine data on output, controllable input, and uncontrollable input from different sources; is a natural way of incorporating expert knowledge into the analysis; and provides a natural framework for optimal design of experiments with noisy inputs. (ii) Efficient design of optimization experiments with noisy inputs. The research will focus on developing a comprehensive design strategy, which is a combination of model-free and Bayesian model-based optimal designs. The model-free design will address the challenges arising from internal noise and complex response surface. (iii) Demonstration and validation of the developed methodologies in the co-PI's lab. A series of experiments will be planned to apply the developed statistical methodology in an attempt to identify factors that trigger alignment of CNT and also to identify their optimum levels to maximize alignment. The proposed framework will allow an experimenter to effectively capture the transmission of uncertainty from input variables to output variables by combining data from different sources, and by utilizing a combination of model-free and model-based experimental designs for efficient exploration of complex response surfaces. From a material scientist's perspective, the proposed method will provide a much more accurate quantification of uncertainty, resulting in more reliable predictions about optimal process conditions as determined from laboratory experiments."
"1613063","Bayesian Global-Local Shrinkage in High Dimensions","DMS","STATISTICS","09/01/2016","08/24/2016","Anindya Bhadra","IN","Purdue University","Standard Grant","Gabor Szekely","08/31/2019","$100,000.00","Nicholas Polson","Bhadra@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","8083","$0.00","High-dimensional data are ubiquitous in many modern applications such as genomics, finance, and image analysis. Developing approaches that are computationally scalable to the size of these data sets while retaining strong theoretical justifications remains a challenge. The goal of the project is to develop new methodology to enable tractable analysis of modern high-dimensional data sets. Software developed from this research will be made publicly available.<br/><br/>Bayesian methodology for high-dimensional data traditionally relies on point mass mixture priors that have attractive theoretical properties but often scale poorly due to the computational difficulties associated with searching a high-dimensional discrete space. The goal of the project is to explore the use of global-local alternatives to high-dimensional problems. Many recent investigations using global-local priors, while showing signs of promise, have been restricted to studying the simple normal means model. The PIs will employ the techniques of global-local shrinkage to problems of fundamental interest in statistics, such as regression, nonlinear function estimation and covariance estimation. More specifically, the PIs aim to show in regression problems that using global-local shrinkage instead of purely global shrinkage methods such as ridge regression or principal components regression can result in improved prediction. They aim to show global-local shrinkage priors are good candidates for non-informative analysis of low-dimensional functions of high-dimensional parameters. The PIs also propose to use global-local shrinkage in covariance estimation and joint mean-covariance estimation problems and apply the developed methodology in suitable applications arising from genomics or finance."
"1612873","Semiparametric Estimation and Variable Selection in the Presence of Nonignorable Nonresponse","DMS","STATISTICS, Methodology, Measuremt & Stats, ","09/01/2016","08/24/2016","Jun Shao","WI","University of Wisconsin-Madison","Standard Grant","Gabor Szekely","08/31/2020","$290,431.00","","shao@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269, 1333, p322","","$0.00","Nonresponse exists in many statistical applications. In most survey problems, many sampled units fail to provide answers to some or all survey questions.  In medical or health studies, the percentages of incomplete data are often appreciable. Handling nonresponse is very challenging when nonresponse is related to the missing data. Since this research is motivated by problems in survey agencies such as the U.S. Census Bureau and Statistics Canada, or by data sets in medical and health studies, results obtained from this research will have significant impacts on the methodology of handling nonresponse for estimation and inference in practice. The results from this research will also shed light on further research in this area.<br/> <br/>When the nonresponse mechanism or propensity depends on observed data only, the nonresponse is called ignorable; otherwise, it is nonignorable. There is a rich literature on methodology of handling ignorable nonresponse. Handling nonignorable nonresponse is much more challenging, since assumptions have to be imposed to ensure the identifiability and estimability of unknown population characteristics and these assumptions are hard to check using data with nonignorable nonresponse. Applying methods developed for ignorable nonresponse to data with nonignorable nonresponse may create serious biases in statistical estimation and inference. This research focuses on estimation based on data with nonignorable nonresponse in the following two general topics. (1) Semiparametric estimation. If a fully parametric model is assumed on the nonresponse propensity and the population distribution of interest, then valid estimators of parameters of interest may be derived using the parametric likelihood under some identifiability assumption.  However, this parametric approach is sensitive to model misspecification, especially when nonresponse is nonignorable. On the other hand, unlike the situation with no nonresponse, a purely nonparametric approach cannot identify the population. This research studies semiparametric methods, assuming one component of the propensity or population distribution is parametric and the others are nonparametric. Efforts will be made to study robustness and efficiency of various methods under different assumptions, longitudinal or multivariate outcomes with nonignorable nonresponse, problems with both missing outcomes and covariates, and unmeasured confounders or systematic missing covariate data in meta analyses. (2) Model and variable selection. When nonresponse is nonignorable, a covariate called nonresponse instrument needs to be used, which is always observed and helps to identify population parameters. In addition, a parametric component of either the propensity or the population distribution has to be assumed. Thus, it is desired to perform model and/or variable selection to ensure that the assumed parametric component and the selected nonresponse instrument are appropriate. Because of nonignorable nonresponse, the existing model and variable selection techniques are not applicable. This research will develop new techniques for model selection and the selection of nonresponse instruments. Furthermore, in the big data era there exists an extremely large set of auxiliary variables that can be used as covariates and this research will study dimension reduction and variable selection for accurate estimation in the presence of nonignorable nonresponse."
"1613003","Spatio-temporal Point Process models on a global scale and their application to global lightning occurrences","DMS","STATISTICS","08/15/2016","08/15/2016","Mikyoung Jun","TX","Texas A&M University","Standard Grant","Gabor Szekely","07/31/2019","$120,000.00","","mjun@central.uh.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1269","","$0.00","With recent technological advances and increased network connectivity, many environmental, biological, and socio-economic datasets are collected on a global scale.  However, many of the current statistical models and methods are not suitable for environmental applications where the data cover the surface of the sphere.  The goal of this project is to develop spatial and spatio-temporal point process models on a global scale.  The research is motivated by a study of global lightning occurrence, and the impact of climate change on global lightning occurrence patterns.  The research will provide useful information for scientists, policy makers, and the general public.<br/> <br/>The proposed models will address the spatial inhomogeneity of the intensity functions as well as their nonstationarity, both of which are essential for environmental applications. In the spatio-temporal case, parametric covariance models for the stochastic intensity function that can represent complex spatio-temporal interactions will be studied, and univariate as well as multivariate point patterns that account for the cross-dependence between multiple point patterns will be considered."
"1617066","Latent Variables Conference 2016","DMS","STATISTICS","06/15/2016","06/08/2016","John Grego","SC","University of South Carolina at Columbia","Standard Grant","Gabor Szekely","05/31/2017","$10,000.00","","grego@stat.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","MPS","1269","7556, 9150","$0.00","This award supports participation of junior researchers in the research conference ""Latent Variables 2016"" held at the University of South Carolina, October 12-14, 2016. The statistical concept of a latent variable (a quantity whose values are inferred rather than directly measured) is important for statistical analysis in a wide variety of fields, from medicine to social science to machine processing of natural language. The purpose of the Latent Variables conference is to encourage the exchange of research ideas in latent variables, and to motivate and direct future research in this and related areas.  The conference provides an excellent opportunity for young researchers to present their work, to learn about recent developments in the field, and to initiate new research collaborations.<br/><br/>The conference features four plenary talks by leaders in the field, as well as six pairs of concurrent sessions, each with three speakers, that will include address research progress in several areas related to latent variables theory and analysis, including multiple testing, frailty models, objective Bayes analysis, factor models, diagnostic screening, model misspecification, group testing and biomarker pooling, item response theory, model-based clustering, generalized linear latent variable models, and measurement error models.  Poster sessions will provide additional opportunities for exchange of ideas, and one of the poster sessions is designated specifically for graduate students to present their research.  Additional information can be found at the conference web site:<br/>http://www.stat.sc.edu/latent-variables-2016"
"1613192","Higher Order Asymptotics for Some Nonstandard Problems in Time Series and in High Dimensions","DMS","STATISTICS","07/01/2016","06/03/2018","Soumendra Lahiri","NC","North Carolina State University","Continuing Grant","Gabor Szekely","12/31/2019","$250,000.00","","s.lahiri@wustl.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","","$0.00","Correlated and high dimensional data appear routinely in many areas of sciences, including atmospheric sciences, finance, and molecular genetics, as well as in an ever increasing number of everyday activities such as social networking. While a vast amount of data are being generated and are available for analyses, traditional methods often fail to elicit information in such applications. This research project has two major goals.  First, it seeks to develop new mathematical tools for analyzing a recent complex statistical approach for correlated data that has been known to produce astonishingly accurate results in empirical studies, but lacks any theoretical justification. It is hoped that the new theoretical tool will lead to further refinements of existing statistical methodology for correlated data. The second part of the project is concerned with complex inferential issues for high dimensional data where the number of unknown parameters far exceeds the sample size, such as determining the role of a few important genes among a collection of several thousand genes from data on a few hundred patients. The project seeks to develop theoretical and methodological statistical tools to enable researchers to address important inference questions without stringent model assumptions. <br/><br/>The project aims to develop some critical theoretical tools and nonparametric statistical methodology for the analysis of time series and high dimensional data. Specifically, this project will focus on (i) developing asymptotic expansion results for the ""fixed-b"" asymptotic approach in time series that has shown significant improvement over traditional methods in several empirical studies but with very little theoretical underpinning; (ii) investigating higher order properties of some general classes of statistical tests (e.g., Wald tests) and of some more recently proposed nonstandard empirical likelihood tests, both under the ""fixed-b"" formulation; (iii) developing new pivotal quantities for block bootstrap in time series that nearly match the accuracy of bootstrap under independence; (iv) developing asymptotic expansion results in high dimensions under sparsity by exploiting some novel tools from approximation theory and Banach space theory; (v) applying the asymptotic expansion results from (iv) to investigate the ""phase transition"" phenomenon in asymptotic properties of statistical methods in high dimensions, and (vi) investigating properties of resampling methods for post-variable selection inference in high dimensions."
"1613060","Prediction Models Based on Large Scale Image Data","DMS","STATISTICS","08/01/2016","07/25/2016","Xiao Wang","IN","Purdue University","Standard Grant","Gabor Szekely","07/31/2019","$100,000.00","","wangxiao@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","8091","$0.00","Research in statistics involves the development and understanding of models based on data. Generally, these data are in the form of numbers, but more recently, statisticians have begun to develop models for data in the form of images. These functional image models have broad applications in neuroscience, engineering, and biomedical practice. This research will further the development of these image models. This project will also include the development of new courses at the undergraduate and graduate levels to train students in the use and understanding of these models. <br/><br/>This project is to develop an integrated research program that studies a broad class of large scale functional image models. The PI aims to develop the adaptive and/or local region regression, the finite mixture regression, and the transformation survival regression with ultra-high dimensional image data. The key advantages of these models are to preserve sharp edges for better interpretation, to incorporate the heterogeneity in the population for better representation, and to handle sophisticated censored data. The theoretical contributions of the proposed research are made towards addressing fundamental issues across several disciplines, including nonparametric statistics and machine learning. These functional image models have broad applications in neuroscience, engineering, and biomedical practice. Courses will be developed to train students in the use and understanding of these models."
"1636648","2016  International Indian Statistical Association conference `Statistical and Data Sciences: A Key to Healthy People, Planet and Prosperity'","DMS","STATISTICS","08/01/2016","07/05/2016","Debashis Mondal","OR","Oregon State University","Standard Grant","Gabor Szekely","07/31/2018","$20,000.00","","mondal@wustl.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","MPS","1269","7556","$0.00","The 2016 meeting of the International Indian Statistical Association (IISA) will take place at Oregon State University, from August 18-21, 2016. (See http://blogs.oregonstate.edu/iisa/.) The main objective of the conference is to bring together both well established and emerging young researchers from around the world who are actively pursuing theoretical and methodological research in statistics and data science and their applications in various allied fields. The conference aims to provide a forum for leading experts and young researchers to discuss recent progress in statistical theory and applications and in data science, thereby providing new directions for statistical inference in various fields. It aims to promote education, research and application of statistics, probability and data science throughout the world with a special emphasis on the Indian subcontinent, foster the exchange of information and scholarly activities between various countries as well as among other national/international organizations for the development of statistical and data science, serve the needs of young statisticians and data scientists and encourage cooperative efforts among members in education, research, industry and business.<br/><br/>Oregon State University will host the 2016 meeting of the International Indian Statistical Association (IISA) from August 18-21, 2016. Plenary speakers will be Kanti Mardia (University of Leeds) and Xiao-Li Meng (Harvard University). Keynote speakers include Kathy Ensor (Rice University), Debashis Ghosh (University of Colorado, Denver), Kannan Natarajan (Novartis), and Ajit Tamhane (Northwestern University). Activities for junior researchers include a session on early career development and a student paper competition. This funding will assure the participation of junior researchers and graduate students."
"1607320","Collaborative Research: Statistical Inference for Functional and High Dimensional Data with New Dependence Metrics","DMS","STATISTICS","06/01/2016","05/24/2016","Xianyang Zhang","TX","Texas A&M University","Standard Grant","Gabor Szekely","05/31/2019","$115,000.00","","zhangxiany@stat.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1269","8083","$0.00","Due to the rapid development of information technologies and their applications in many scientific fields such as climate science, medical imaging, and finance, statistical analysis of high-dimensional data and infinite-dimensional functional data has become increasingly important. A key challenge associated with the analysis of such big data is how to measure and infer complex dependence structure, which is a fundamental step in statistics and becomes more difficult owing to the data's high dimensionality and huge size. The main goal of this research project is to develop new dependence measures for quantifying dependence of large scale data sets such as temporally dependent functional data and high dimensional data, and utilize these new measures to develop novel statistical tools for conducting sparse principal component analysis, dimensional reduction, and simultaneous hypothesis testing. Building on the new dependence metrics that can capture nonlinear and non-monotonic dependence, the methodologies under development are expected to lead to more accurate prediction and inference, as well as more effective dimension reduction in the analysis of functional and high dimensional data. <br/><br/>The research consists of three projects addressing different challenges in the analysis of functional and high dimensional data. In Project 1, the investigators introduce a new operator-valued quantity to characterize the conditional mean (in)dependence of one function-valued random element given another, and apply the newly developed dependent metrics to do dimension reduction for functional time series under a new framework of finite dimensional functional data. In Project 2, the investigators explore a new dimension reduction framework for regression models with high dimensional response, which requires less stringent linear model assumptions and is more flexible in terms of capturing possible nonlinear dependence between the response and the covariates. In Project 3, the investigators develop new tests for the mutual independence of high dimensional data via distance covariance and rank distance covariance using both sum of squares and maximum type test statistics. Overall, the three lines of research are all related to big data, and they touch upon various aspects of modern statistics; the project aims to push the current frontiers in areas including sparse principal component analysis, inference for dependent functional data, and high dimensional multivariate analysis to another level."
"1612984","Equilibrium in Multivariate Nonstationary Time Series","DMS","STATISTICS","07/01/2016","05/23/2016","Mohsen Pourahmadi","TX","Texas A&M University","Standard Grant","Gabor Szekely","06/30/2020","$150,000.00","","pourahm@stat.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1269","","$0.00","Nonstationary time series systems appear routinely in economics, seismology, neuroscience, and physics, where stationarity is usually synonymous with equilibrium. Such systems are usually multidimensional, and their modeling, prediction, and control have tremendous social and scientific impacts. Isolating and identifying equilibrium or stationary features are of fundamental importance in prediction and control of such systems. This research project aims to develop methodologies for extracting aspects of multivariate nonstationary processes that display a sense of equilibrium or stationarity. It is interdisciplinary in nature and has immediate applications to the analysis of economics, seismology, and neuroscience data. A graduate student will be involved in the research. <br/><br/>This research project aims to elevate the concept and theory of cointegration from multivariate integrated time series rooted in economics theory to the more general multivariate nonstationary time series setup in probability and statistics. In spite of its central role in econometrics in the last four decades and well-founded motivations in economics, the cointegration theory suffers from the requirements that the series be integrated (unit-root nonstationary) and satisfy a vector autoregressive and moving average model. The goal of this project is to avoid such restrictions and focus on general multivariate nonstationary time series. Three distinct methods for computing analogues of cointegrating vectors and the cointegrating rank will be developed. The first is a time-domain method in line with the classical (Johansen's) approach that relies on the reduced rank regression and likelihood ratio tests.  The second method is in the spectral domain and relies on the idea of projection pursuit. It searches for coefficients of candidate linear combinations by minimizing a projection index measuring the discrepancy between time-varying and constant spectral density functions. The third method is concerned with a time-varying cointegration setup where the coefficients are piecewise constant over time. Its successful implementation rests on a good solution of the problem of change-point detection for nonstationary processes, and a novel solution is explored in this research. The results will have immediate impact in settings where multivariate time series data are collected, such as in financial markets, epidemiology, environmental monitoring, and global change."
"1564376","FRG: Collaborative Research: Innovations in Statistical Modeling, Prediction, and Design for Computer Experiments","DMS","STATISTICS","07/01/2016","05/29/2018","Peter Chien","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","06/30/2020","$299,330.00","","peterq@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","1616","$0.00","The explosive growth in the use of computer simulators in the last fifteen years has helped galvanize a revolution in scientific, engineering, and biological research that includes advances in the aerospace industry, material science, renewable energy, and biomechanics. Researchers can make a detailed exploration of scientific design alternatives under a wide set of operating environments using runs from a simulator of a physical system, possibly coupled with those from a traditional physical system experiment. This research project will advance the statistical modeling, design, and analysis of experiments that use computer simulators. The first research area is Improved Modeling of Simulator Output: The investigators will develop flexible stochastic models that will allow more accurate prediction in settings where the simulator provides related multivariate output of the performance of a physical system. Current prediction models either assume output independence (knowledge of one output gives no information about other outputs) or a linear dependence on a common set of latent drivers. The second research area concerns Advances in Emulation: The investigators aim to devise efficient emulators of simulator output for novel input and output settings such as when gradient information is available or when the output consists of both point and integrated measures. They plan to construct predictors that incorporate natural invariances present in the simulator output. For example, the predicted response should be constant under permutations of the inputs when the output satisfies this condition; the project will quantify the uncertainty in the invariant predictors. The investigators also plan to quantify the uncertainty of a recent, theoretically-justified method of calibrating computer simulators based on physical experimental data. The third research area is the Design of Simulator Experiments: Efficient designs of simulator experiments will be devised to minimize the computational effort required to determine the sensitivity of a simulator output to each of its inputs. <br/><br/>This research will build a statistical framework for the modeling, design and analysis of experiments that employ computer simulators. The specific goals are (1) to devise flexible interpolating stochastic models for computer simulators with multivariate output; (2) to invent efficient predictors for novel input and output settings such as when gradient information is available or when the output consists of both point and integrated responses; (3) to develop emulators of simulator output that incorporate the same invariances present in the simulator responses; (4) to quantify the uncertainty of L2 calibrated predictors for expensive computer codes; and (5) to construct new sliced Latin hypercube designs to allow the efficient calculation of global sensitivity indices. The investigators will develop new modes for training statistics graduate students having interests in engineering applications. Opportunities will be created for subject matter specialists to provide critical practical challenges in three areas: aerospace/mechanical engineering, biomechanics, and material science, and to conduct joint applied projects with the researchers."
"1613137","Collaborative Research: Tensor Envelope Model - A New Approach for Regressions with Tensor Data","DMS","STATISTICS, MSPA-INTERDISCIPLINARY, Modulation","09/01/2016","08/30/2016","Lexin Li","CA","University of California-Berkeley","Standard Grant","Gabor Szekely","08/31/2020","$130,000.00","","lexinli@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269, 7454, 7714","8091","$0.00","One of the most intriguing questions in modern science is to understand the human brain. In particular, scientists want to understand the differences between the brains of people with neurological disorders and those without. In brain imaging analysis, scientists collect data in the form of images that are used to compare the normal aging process to the development of neurological disorders. Through this project, the PIs seek to develop a toolkit comprised of a set of novel statistical methods, theories, and algorithms for the analysis of brain imaging data, as well as similar data that arise in a variety of scientific and business fields. The proposed research program is expected to make significant contributions on two fronts: timely responding to the growing needs and challenges of array data analysis, and providing a class of associated methodology that advances the statistical discipline. Research proposed in this project is to be disseminated through the investigators' close collaborations with the neuroscientists, as well as substantial educational and outreach activities.<br/><br/>Multidimensional array, or tensor, data are now frequently arising in a wide range of scientific and business fields. Aiming to address some of the most pressing questions in tensor data analysis, this research will integrate advanced statistical modeling devices with modern computational techniques to develop a set of novel tensor regression methods. Whereas there has been an enormous body of literature on high-dimensional regression analysis, nearly all work is with a vector response or predictor. Naively turning a tensor into a vector would result in ultrahigh dimensionality, destroy inherent structural information embedded in the tensor, and often render classical methods inadequate. This research will develop methods and tools for regression modeling of tensor responses or predictors, which both effectively tackles the high dimensionality and simultaneously preserves the tensor structure. Three sets of problems are to be investigated: (1) tensor response regression with envelope, aiming to address questions such as identifying brain regions exhibiting different activity patterns between the disease group and the general population after controlling for a set of potential confounding variables; (2) tensor predictor regression with envelope, aiming at questions of using brain images to diagnose neurodegenerative disorders and to predict onset of neuropsychiatric diseases; and (3) covariance matrix response regression with envelope, aiming to understand brain network alternations and building their associations with pathological phenotypes. The core idea underlying all of these aims is the adoption of a generalized sparsity principle and the development of a class of tensor envelope methods. The classical sparsity principle assumes a subset of individual variables are irrelevant, and various penalty functions are employed to induce such sparsity. By contrast, this generalized sparsity principle assumes linear combinations of variables are irrelevant, and the proposed envelope methods simultaneously identify and exclude such irrelevant information to achieve much improved estimation accuracy and efficiency."
"1611893","Collaborative proposal: Variable Selection in the high dimensional, low sample size setting -- Beyond the Linear Regression and Normal Errors Model","DMS","STATISTICS, MSPA-INTERDISCIPLINARY, Systems and Synthetic Biology","08/15/2016","08/19/2016","James Booth","NY","Cornell University","Standard Grant","Gabor Szekely","07/31/2020","$200,000.00","Martin Wells","jb383@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269, 7454, 8011","7465, 8007","$0.00","Revolutionary new technologies are producing high-throughput biological data at a resolution that was unthinkable only a decade ago. These new forms of data pose enormous challenges and opportunities for statisticians and computer scientists. This project develops new sophisticated statistical methods and computational algorithms for analyzing and integrating complex high-dimensional data. The work is motivated by collaborations with leading biological scientists at Cornell-Ithaca and Weill Cornell Medical College working in diverse research areas including plant biology, nutrition, neurology, cancer epigenomics, and veterinary medicine. <br/><br/>The goal of this project is to develop new statistical models and computational algorithms for high-dimensional, low sample size, high-throughput biological data, including new methods for the analysis of microarrays, the identification of quantitative trait loci, association mapping, label-free shotgun proteomics and metabolomics. The proposed methods involve innovative extensions of modern statistical building blocks, including the use of random effects for regularization, shrinkage estimation, Bayesian statistics, and mixtures for posterior classification and prediction. Novel modifications of the expectation-maximization algorithm are proposed for scalable and efficient model fitting and inference."
"1614392","Collaborative Research: Unifying Mathematical and Statistical Approaches for Modeling Animal Movement and Resource Selection","DMS","POP & COMMUNITY ECOL PROG, STATISTICS, Cross-BIO Activities, MSPA-INTERDISCIPLINARY, Animal Behavior","08/01/2016","08/03/2016","Mevin Hooten","CO","Colorado State University","Standard Grant","Junping Wang","07/31/2020","$125,543.00","","mevin.hooten@austin.utexas.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1182, 1269, 7275, 7454, 7659","8007","$0.00","Understanding how individuals move in space, what habitats they prefer, and how the environmental features channel or resist movement is central to landscape ecology and wildlife management.  Dramatic improvements in the acquisition, resolution, and extent of two relevant types of data have recently occurred: remotely sensed environmental data and high-resolution animal location (telemetry) data.  These data drive a statistical industry serving wildlife management agencies, private companies, and academia. Improvements in tracking technology are likely to cause a revolution in movement ecology analogous to the impact of gene sequencing on molecular genetics.  This project synthesizes theoretical advances (statistical techniques for estimating movement probability between sites and how environmental resources are selected), existing results (mathematical techniques for rapidly predicting the envelope of future animal positions using mechanistic assumptions) and untapped data (remotely sensed habitat maps and high resolution individual telemetry) to rigorously characterize how landscape features condition population movement and habitat choice.  The research will encompass case studies investigating the movement of mule deer and elk in Utah, harbor seals off southeastern Alaska, and Canada lynx, which have recently been reintroduced in Colorado and are dispersing throughout the Rocky Mountains.   Research students will be cross-trained in mathematics, statistics, and movement ecology; undergraduates will be included in the research process by developing individual-based models to test estimation technologies.  A teaching lab in mathematical biology, illustrating movement models using real biological systems, will also be developed and distributed.<br/><br/>Statistical point process models provide well-understood statistical approaches for obtaining inference from individual-based telemetry data, with resource selection functions describing individual habitat preferences and availability functions describing dispersal probability between locations. However, point process models require numerical quadrature for proper normalization, making them slow for large data sets.  Classical availability functions are not constructed to handle major issues like movement constraints, autocorrelation, and landscape resistance, affecting quality of resource selection inference and computational feasibility.  However, a parallel and untapped literature of partial differential equations predicts dispersal likelihood based on mechanistic assumptions about individual movement.  Ecological diffusion and ecological telegrapher's equations provide natural scalings from Lagrangian to Eulerian perspectives. They are fully mechanistic and allow for population-level dynamics, but are not inherently statistical nor automatically suited to handling individual-based telemetry data.  This project will reconcile point process modeling with mechanistic dispersal equations to arrive at a unified method for analyzing telemetry data.  Homogenization techniques, which are well-accepted in physical sciences but not often applied in mathematical biology or statistics, will be used to speed up solutions in heterogeneous environments. Coupled point process models and homogenized partial differential equations will accelerate model fitting, provide resource selection inference and naturally accommodate environmental heterogeneity and barriers/constraints to movement.  The ecological movement equations will be homogenized and simplified using asymptotic approximations suitable for point process models, addressing correlation among position observations and velocity constraints.  Rapid numerical techniques for movement models will be developed to allow facile representation of movement barriers (e.g., shorelines, major rivers or roads) as boundary conditions. To develop efficient computational techniques for resource selection functions and landscape resistance inference, the homogenized ecological movement equations will be dovetailed with point process models in a hierarchical framework.  The integrated approach will be applied to telemetry data from foraging ungulates in Utah, harbor seals in the Gulf of Alaska, and Canada lynx in Colorado."
"1602493","International Conference On Design of Experiments - ICODOE 2016","DMS","STATISTICS","04/01/2016","02/08/2018","Manohar Aggarwal","TN","University of Memphis","Standard Grant","Gabor Szekely","06/30/2018","$10,000.00","E Olusegun George","maggarwl@memphis.edu","Administration 315","Memphis","TN","381523370","9016783251","MPS","1269","7556, 9150","$0.00","The International Conference on Design of Experiments (ICODOE 2016) will be held at the University of Memphis on May 10-13, 2016. The design and analysis of experiments is at the foundation of the scientific method; it is used in applications across all scientific disciplines and engineering. The conference will help in the advancement of knowledge in the area of experimental design as prominent researchers from all over the world will be participating in the conference. This award provides travel support to increase the number of junior researchers who are able to attend.<br/><br/>The primary objective of the conference is to encourage and support young researchers in the field, to make them aware of the latest developments, and also to bring them together with established researchers in experimental design. The conference will also catalyze research in new directions by bringing together leading researchers from all over the world in the area of design and analysis of experiments and practitioners in the pharmaceutical, chemometric, physical, biological, medical, social, psychological, economic, engineering, and manufacturing sciences. Topics of the conference include design of experiments for big data, design for uncertainty quantification, computational methods, and computer experiments. The focus will be on emerging areas of research in experimental design as well as innovations in traditional areas.  Additional information is available at the conference web site:<br/>www.memphis.edu/msci/news/icodoe2016.php"
"1554804","CAREER: Statistical inference of network and relational data","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2016","07/01/2019","Yang Feng","NY","Columbia University","Continuing Grant","Gabor Szekely","02/29/2020","$316,597.00","","yf31@nyu.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269, 8048","1045","$0.00","Technological innovations have provided a primary force in advancement of scientific research and in social progress.  Large scale network data and relational data are frequently encountered in genomics and health sciences, economics, finance and social media.  The proposed project will (1) enhance methodological and theoretical developments for statistical analysis of network and relational data. (2) advance the understanding of the community structure of social network. The research emanating from this grant will advance the frontiers of theory and methods for network data modeling.  The new developments will provide better understandings of large scale network data for researchers from diverse fields of sciences and humanities, e.g., understanding  the social behavior of individuals and the dynamic nature of social network.<br/><br/>The proposed project has the following three interrelated objectives under the theme of statistical inference for large scale network and relational data. (1) To introduce a new framework for community detection with covariate information. There have been many existing approaches to  community detection. However, a majority  of  them focus on analyzing the network without considering the covariate information, which could be valuable for achieving greater accuracy of community detection. The goal of this research is to study when and how will covariate information help in terms of  the community detection accuracy.  (2) To develop a new dynamic stochastic block model framework with applications in change point detection. The stochastic block model along with its variants are usually defined for a static network. The goal here is to define a dynamic version of the stochastic block model, with a clear interpretation of how the network evolves over time. A general dynamic spectral clustering method will be proposed and its theoretical properties established. The important problem of change point detection of the dynamic network will be studied in details.  (3) To introduce a conditional dependency measure with applications in undirected graphical models. It is of fundamental interest to ascertain variables or factors underlining the network dependency structure. The goal is to introduce a flexible conditional dependency measure, which can capture a wide range of different dependency structures. The PI will develop a new method for generating a general undirected graph with desirable features by making use of the resulting conditional dependency measure."
"1547433","RTG: Cross-Training in Statistics and Computer Science","DMS","STATISTICS, WORKFORCE IN THE MATHEMAT SCI, CDS&E-MSS","08/15/2016","08/28/2018","Marina Vannucci","TX","William Marsh Rice University","Continuing Grant","Gabor Szekely","07/31/2020","$1,400,004.00","Luay Nakhleh","marina@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269, 7335, 8069","7301, 8083","$0.00","Data science is rapidly evolving as an essential interdisciplinary field, where advances often result from combinations of ideas from various parts of mathematics, statistics, computer science, physical sciences, and engineering, as well as other disciplines. New types of (large) data have emerged, presenting unprecedented complexities and challenges that require a new way of thinking. The goal of this Research Training Group (RTG) project is to provide inherently interdisciplinary training to undergraduate and graduate students, as well as post-doctoral fellows, developing skills that transcend individual disciplines. Training students through research involvement in areas that combine statistical and computational modeling and inference will provide the intellectual foundation for a new generation of scientists poised to make novel breakthroughs in this exciting area, and contribute to the research and design efforts in the private and public sectors. <br/><br/>This project will catalyze original research on modeling and inference for large datasets. The program will involve three undergraduate students, six graduate students, and two postdoctoral fellows at any given time. The undergraduate component will be an integrated research experience, consisting of a seminar course, where students will learn and present material on key topics, and active participation in research projects, where these topics are put into practice. Each trainee will work with at least two RTG faculty members to ensure a truly interdisciplinary training experience. A new course in ""data science"" will be developed, for education of trainees across traditional department boundaries. Throughout the calendar year, the Research Training Group will sponsor advanced courses and research lectures for the benefit of the graduate and postdoctoral participants. The program builds upon the strengths and interactions of a dynamic group of faculty, with expertise in the general area of probabilistic models and methods for computational inference.  Trainees will benefit from individualized mentoring activities and participation in structured research groups. The interdisciplinary nature of these activities will lead to a newly trained generation of researchers capable of generating new approaches and ideas. Resources and tools for cross-training will be developed and disseminated to the community. Exposure to modern aspects of data science and computing will enhance the professional development of the trainees. Cutting-edge research at the interface between statistics and computer science will be enhanced."
"1555141","CAREER:   Big Computation and the Management of Emerging Infectious Diseases","DMS","STATISTICS, Division Co-Funding: CAREER","06/01/2016","06/10/2020","Eric Laber","NC","North Carolina State University","Continuing Grant","Gabor Szekely","06/30/2021","$400,000.00","","eric.laber@duke.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269, 8048","1045","$0.00","Emerging infectious diseases (EIDs) account for more than 25% of global disease burden and more than 32% of global deaths. Current EIDs like Middle East Respiratory Syndrome Coronavirus (MERS) and antibiotic-resistant superbugs have the potential to make devastating impacts on public health. The methodologies under development in this project can be used to translate real-time data on EIDs into recommendations about where, when, and to whom to apply interventions so as to minimize negative impacts of the disease while reducing overall resource consumption. Furthermore, these recommendations are designed to be immediately interpretable in a subject matter context, thereby empowering decision makers to incorporate information from complex and heterogeneous data streams into disease management. Application of these methodologies has the potential to reduce mortality and morbidity at lower cost than existing management plans. Furthermore, models underpinning intervention recommendations will generate new knowledge about EID dynamics. <br/><br/>This research project aims to make fundamental contributions to online sequential decision making and to create a new statistical framework for data-driven management of EIDs. We conceptualize the EID as spreading across a finite set of locations, which might be physical locations in space or nodes in a network. An allocation strategy formalizes management of an EID and is represented by a sequence of functions, one per intervention decision, that map up-to-date information on an EID to a subset of locations recommended for treatment. An optimal allocation strategy maximizes some mean utility function over the duration of the EID. Construction of an optimal allocation strategy from data on an EID is challenging because: (i) the number of allocations is exponential in the number of locations; (ii) estimation and management must occur simultaneously; (iii) spatial proximity induces causal interference; and (iv) an allocation strategy must be interpretable to subject matter experts. We integrate ideas from statistics, computer science, optimization, and disease ecology to overcome these challenges. We combine simulation-optimization with policy-search algorithms to construct an online estimator of the optimal allocation strategy; this strategy trades off exploring allocation choices that improve estimates of disease dynamics with exploiting current estimated dynamics to immediately slow spread of the EID. We show that the treatment allocation problem can be recast as an infinite-dimensional bandit problem. We leverage this connection to derive estimation algorithms that scale to very large allocation problems and are amenable to theoretical study. We combine our policy-search and bandit-based estimators with a novel class of allocation strategies that can be expressed as a sequence of if-then statements that are immediately interpretable to subject-matter experts and can be readily adjusted based on expert judgment. We derive a non-parametric lower bound on the approximation error of an estimated allocation strategy within this class; this bound is used to perform goodness-of-fit tests for the estimated optimal allocation strategy."

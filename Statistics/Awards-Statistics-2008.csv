"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"0806120","Collaborative Research: A paradigm for dimension reduction with respect to a general functional","DMS","STATISTICS","07/01/2008","03/30/2010","Xiangrong Yin","GA","University of Georgia Research Foundation Inc","Continuing Grant","Gabor Szekely","06/30/2012","$124,197.00","","yinxiangrong@uky.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","The proposed research aims to developing a general formulation and the related methods for sufficient dimension reduction (SDR) where a specific functional (or parameter) of the conditional distribution is of interest. The past two decades have seen vigorous development of the SDR methods and have accrued a striking record of their successful applications. However, to a large extent these methods treat the conditional distribution as the object of interest, without discriminating between parameter of interest and nuisance parameter. While there are methods that target statistical functionals, they are specific to the parameter in consideration and as such  are difficult to apply to other parameters. The investigators propose a new paradigm for SDR that focuses on a functional of the conditional distribution, which can be any one in a very wide class that covers most of applications. In addition, the investigators propose to develop a coherent collection of associated techniques for estimation, computation, and asymptotic inference.<br/><br/>High throughput technologies that produce massive amount of complex and high-dimensional data are increasingly prevalent in such diverse areas as  business, government administration, environmental studies, machine learning, and bioinformatics. These provide considerable momentum in the Statistics community to develop new theories and methodologies, and to reformulate the existing ones, that are capable of discovering critical evidence from high-dimensional and massive data. SDR is a recent area of statistical research that arose amidst, and has been propelled by, these new demands. The investigators propose to reformulate the theories and methodologies of SDR so that they can be specifically tailored to target to be estimated. This new paradigm not only synthesizes, broadens, and deepens the recent advances in SDR, but brings the understanding of SDR on a par with classical statistical inference theory, by following the tradition of sufficiency, efficiency, information, parameter of interests, and nuisance parameters, which are the key ideas that has helped to propel classical inference to its maturity."
"0805758","High Dimension, Low Sample Size Discrimination","DMS","STATISTICS","06/01/2008","03/03/2010","Jeongyoun Ahn","GA","University of Georgia Research Foundation Inc","Continuing Grant","Gabor Szekely","05/31/2012","$105,000.00","","jyahn@uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","Proposed research is motivated from the discrimination problem with high dimension, low sample size data. The investigator studies the intrinsic difficulties of the discrimination problem by exploring asymptotic geometric structure of such data. Three main activities are proposed: a) the asymptotic inconsistency of leave-one-out cross-validation. The study is expected to explain why it shall fail when the number of variables greatly exceeds the number of observations; b) the effect of the relationship between the dimensionality and the sample size on the difficulty of discrimination task; and c) a discriminant direction vector that only exists for the data with high dimension, low sample size.  The data points collapse on this direction vector and also are most separated by group labels. The investigator plans to study its theoretical and empirical properties of the procedure such as its optimality, uniqueness, and asymptotic performances.<br/><br/>The overall goal is to investigate the nontraditional and unique challenges in high dimension, low sample size discrimination. The proposed approach may be regarded atypical, but it is more relevant to the problem itself. The applications of proposed research include text document classification such as Spam email filter, medical imaging such as functional magnetic resonance imaging, and bioinformatics such as microarray gene expression and proteomics.<br/><br/><br/>"
"0806058","Collaborative Research:  A Paradigm for Dimension Reduction with Respect to a General Functional","DMS","STATISTICS","07/01/2008","03/31/2010","Bing Li","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","06/30/2011","$47,039.00","","bing@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, OTHR","$0.00","The proposed research aims to developing a general formulation and the related methods for sufficient dimension reduction (SDR) where a specific functional (or parameter) of the conditional distribution is of interest. The past two decades have seen vigorous development of the SDR methods and have accrued a striking record of their successful applications. However, to a large extent these methods treat the conditional distribution as the object of interest, without discriminating between parameter of interest and nuisance parameter. While there are methods that target statistical functionals, they are specific to the parameter in consideration and as such are difficult to apply to other parameters. The investigators propose a new paradigm for SDR that focuses on a functional of the conditional distribution, which can be any one in a very wide class that covers most of applications. In addition, the investigators propose to develop a coherent collection of associated techniques for estimation, computation, and asymptotic inference. <br/><br/>High throughput technologies that produce massive amount of complex and high-dimensional data are increasingly prevalent in such diverse areas as business, government administration, environmental studies, machine learning, and bioinformatics. These provide considerable momentum in the Statistics community to develop new theories and methodologies, and to reformulate the existing ones, that are capable of discovering critical evidence from high-dimensional and massive data. SDR is a recent area of statistical research that arose amidst, and has been propelled by, these new demands. The investigators propose to reformulate the theories and methodologies of SDR so that they can be specifically tailored to target to be estimated. This new paradigm not only synthesizes, broadens, and deepens the recent advances in SDR, but brings the understanding of SDR on a par with classical statistical inference theory, by following the tradition of sufficiency, efficiency, information, parameter of interests, and nuisance parameters, which are the key ideas that has helped to propel classical inference to its maturity"
"0806096","Beyond Stationarity: Statistical Inference for Nonstationary Processes","DMS","STATISTICS","08/01/2008","07/08/2010","Suhasini Subba Rao","TX","Texas A&M Research Foundation","Continuing Grant","Gabor Szekely","07/31/2012","$115,475.00","","suhasini@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","The investigator develops new methods for analysing nonstationary time series and their properties. Many methods in time series are developed under the premise that the observations are stationary. This assumption simplifies both the estimation procedure and asymptotic analysis. However, in real life this assumption is often quite unrealistic. Ignoring nonstationarity in the data and treating the observations as if they were stationary, could give misleading conclusions. Therefore it is important to develop methods for dealing with data that is either temporally or spatially nonstationary. The investigator focuses on three areas where, in applications, nonstationarity can arise (i) statistical inference for time-varying ARCH-type processes (ii) nonstationary random correlated (stochastic) coefficient regression models (iii) analysis of spatially nonstationary spatio-temporal models. These are summarised below. The investigator develops methods which test or track structural changes in time-varying ARCH and GARCH processes. In order to develop sampling properties for the proposed methods, mixing of the time-varying ARCH-type processes is required, and the investigator studies the mixing properties of such processes. Random correlated coefficient regression (RCCR) models are often used to explain the nonstationarity seen in the data. Despite its advantages, until recently the statistical analysis of RCCR models has been quite limited. The investigator develops statistical sound and computationally efficient parameter estimation methods for RCCR models. <br/>Observations from spatio-temporal processes can arise frequently in several disciplines, and several factors could cause the observations to come from a spatially nonstationary process. The investigator investigates spatially nonstationary, spatio-temporal processes. In particular, the investigator considers methods which decompose estimates of the model into a global spatially stationary process, and an additional locally nonstationary term.  <br/><br/>In several disciplines, it is assumed that the main character of data observed over time (usually known as a time series), for example volatility, is not influenced by time. This time invariance property is known as stationarity and it is often the underlying assumption in many current statistical methodologies, because stationarity can often simplify the analysis. However, statistical methods which overlook the nonstationarity can lead to misleading or incorrect conclusions. There are several real data examples where there is empirical evidence to suggest that stationarity is an oversimplification. A particularly pertinent example is global temperature anomolies, where there is plenty of evidence to suggest that both the average temperature and the variation have changed over the past 150 years. In this project we develop statistical methods for nonstationary time series, in particular to identify where changes have occured and factors which have caused the changes. By developing methods that do not ignore the nonstationarity, we are better able to understand the mechanisms driving the data, which leads to better forecasts. These methods can be applied a wide range of subjects, including economics (identifying factors behind the current credit crunch) and climatology (test whether the rise in CO2 levels, has an influence on the amount of variation in the global temperatures). <br/><br/><br/>"
"0806176","Integral Curve Estimation:   New Methodology and Applications to Diffusion Tensor Imaging","DMS","STATISTICS","07/01/2008","06/23/2008","Lyudmila Sakhanenko","MI","Michigan State University","Standard Grant","Gabor Szekely","08/31/2011","$104,871.00","","sakhanen@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","0000, OTHR","$0.00"," The focus of the project is to model and to estimate the integral curves that are driven by an unknown vector field which is not observed directly. Instead, a noisy data is available for a related spatial field. The investigator proposes integral curve estimators and studies their consistency and asymptotic distribution. Probabilistic methods are utilized to address these non-parametric inference problems. This work is motivated by Diffusion Tensor Imaging (a special type of Magnetic Resonance Imaging) where there is a growing need for statistical methods. Given observations of signal intensities that are related to water diffusion tensor, one can find the vector-field corresponding to the maximal eigenvalue of the tensor, which shows the main direction of diffusion. Then the integral curve corresponding to this vector-field is a fiber, since water molecules diffuse mostly along fibers in some tissues such as white matter in brain. One of important questions is whether a fiber starting at a specific point reaches a certain region. The proposed methodology provides answers to this connectivity problem and more importantly it assesses statistical quality of those answers. To summarize, the investigator develops statistical methods that complement Diffusion Tensor Imaging technology to help study architecture of soft tissues such as brain or muscle.<br/><br/>This proposal outlines mathematical and statistical issues underpinning the development of a novel model for estimation of fibers based on observed noisy data of signal intensities. This model provides answers to questions in Diffusion Tensor Magnetic Resonance Imaging (DT-MRI, a brain imaging technique), which in turn could help to improve understanding of physics and biology of live brain and to advance diagnostical methods for brain diseases and disorders. The proposed research increases the role of statistics in DT-MRI. It integrates mathematics and statistics with physics and medicine. As a natural generalization, the proposed methodology for curve (fiber) estimation, based on not only vector or tensor data but some spatial field, is of interest by itself. Its analysis shows how probabilistic methods are invaluable for problems in non-parametric statistical inference. The investigator also sees potential applications of this work to other fields of science such as meteorology. Furthermore, this project builds a base for attracting and training graduate students with interest and skills to work in interdisciplinary research."
"0805533","Inference for dynamical systems","DMS","STATISTICS","06/01/2008","04/02/2010","Edward Ionides","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","05/31/2012","$200,000.00","","ionides@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","The starting point of the proposed research is a new algorithm that has recently been shown to make maximum likelihood estimation feasible for previously intractable partially-observed nonlinear stochastic dynamical systems.  The algorithm is based on a sequence of filtering operations which converges to a maximum likelihood parameter estimate, and is therefore termed iterated filtering.  The availability of iterated filtering methodology opens up many possibilities for developing new classes of stochastic dynamic models for use as data analysis tools.  One component of the proposed research program is development of a new class of Markov chain models appropriate for biological systems, consisting of interacting Poisson processes whose rates are subject to white noise. Another goal is to broaden the class of dynamical systems for which likelihood based inference is practical, via increased theoretical understanding of iterated filtering.  Specifically, a new theoretical framework for iterated filtering will be developed, based on identifying a relationship with previously studied stochastic approximation techniques. Techniques of averaging over iterations and searching over a sequence of random directions, which have good theoretical and practical properties for other stochastic approximation methods, are expected to be applicable to iterated filtering.  The third component of the proposed research is to demonstrate the role of the new methodology in facilitating a novel and scientifically relevant data analysis of malaria transmission. <br/>Infectious diseases pose challenging and important questions which have long been a testing ground for inference methodology for dynamical systems.  Carrying out data analysis via new classes of continuous time dynamic models will require handling novel situations for diagnosing goodness of fit, and appropriate techniques will be developed and demonstrated.<br/><br/>Nonlinear stochastic dynamical models are widely used to study systems occurring throughout the sciences and engineering.  Such models are natural to formulate and can be analyzed mathematically and numerically. Despite decades of work, carrying out statistical inference for nonlinear dynamical models remains a challenging and important problem.  Recently, progress has been made possible by new methodology taking advantage of increasing computational resources.  Continued progress requires building theoretical understanding of successfully demonstrated methodology, developing new methodologies, and showing how these advances can be used to further scientific knowledge about dynamical systems of interest. Recent motivations for understanding infectious disease dynamics include the threats posed by emerging diseases (HIV/AIDS, SARS, pandemic influenza), re-emerging diseases (malaria, tuberculosis) and bioterrorism. Inference for dynamical systems arises in many diverse fields, including economics, neuroscience, chemical engineering, signal processing, and molecular biochemistry.  The field of Statistics forms a natural bridge to make methodological advances available to a wider research community.<br/>"
"0805798","Discovering Sparse Covariance Structures in High Dimensions","DMS","STATISTICS","06/01/2008","06/01/2010","Elizaveta Levina","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","05/31/2012","$249,993.00","","elevina@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","This project focuses on discovering and exploiting sparse structures in the data to improve estimation of covariance matrices in high dimensions. The covariance matrix plays a key role in many data analysis methods, including principal component analysis, discriminant analysis, inference about the means in multivariate analysis, and inference about independence  and conditional independence relationships in graphical models. Advances in random matrix theory have shown that the traditional estimator, the sample covariance, performs poorly in high dimensions.  The existing research on alternative estimators, including previous work of the PI, focuses mostly on the situation when there is a notion of distance or ordering for the variable indexes (time series, longitudinal data, spatial data, spectroscopy, etc).  However, there are many applications where such ordering is not available: for example, genetics, financial, social and economic data.   This project develops several methods for constructing regularized sparse estimators that are invariant to variable permutations, both for the covariance matrix and its inverse.  The main building blocks of the methods are thresholding, smooth penalties that encourage sparsity, permutation-invariant loss functions, adaptive weights, and manifold projections to discover potential structured re-orderings of the variables.  Analytical results establishing consistency and convergence rates of the proposed estimators in high dimensions are fully developed. These theoretical results in high dimensions require tools that are different from standard asymptotic analysis, and there are few available in the existing literature. Efficient optimization algorithms needed to compute these estimators are developed, with the emphasis on the computational cost growing as slowly as possible with dimension.  Some of the estimators proposed carry a very low computation cost by design, while others require computational ingenuity to be feasible in really high dimensions.  The proposed methodology is tested extensively, both in simulations and on a number of applications through the PI's interdisciplinary collaborations.<br/><br/><br/>Massive amounts of data collected in the modern world are creating new challenges for statisticians.  There is an urgent need for new theoretical and practical methods that deal with high-dimensional data, and a vast number of applications where high-dimensional covariance matrices  need to be estimated as part of data analysis: finance, genetics, spectroscopy, remote sensing, climate studies, brain imaging, speech recognition, and many others.   The PI has ongoing  collaborations with chemists on Raman spectroscopy of bone, with oceanologists on using spectral data for remote ocean sensing, with climate scientists on temperature modeling and with a biostatistician on a new type of gene expression technology that works at protein level.  The PI also works actively in the area of statistical signal processing by wireless sensor networks, where spatial covariance estimation is important, and which has many security applications. The new methodology for estimating high-dimensional covariances developed in this project is analyzed theoretically and tested and validated in these applications, and in turn,  the directions in which the project develops at later stages are influenced by the issues and needs of the applications.  The project also contributes to educating graduate students in an important area of modern statistics.<br/>"
"0806131","""Collaborative Research: Regression Problems in Functional Data Analysis""","DMS","STATISTICS","06/01/2008","03/03/2010","Yehua Li","GA","University of Georgia Research Foundation Inc","Continuing Grant","Gabor Szekely","07/31/2012","$100,000.00","","yehuali@iastate.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","Modern data collection methods are now frequently returning observations that could be viewed as the results of digitized recording or sampling from random functions. This project investigates regression problems for which the response is scalar but some of the predictors are functional. The general goal is to gain understanding on the inference of the models based on partially observed and error-contaminated functional data. Distinctions will be made between dense functional data, usually obtained from images, and sparse functional data, usually obtained from longitudinal studies. The specific topics include the consideration of (i) a functional generalized linear model for dense functional data using a penalized likelihood approach, (ii) dimension reduction methodologies based on sliced inverse regression and sliced average variance estimation, and (iii) a functional generalized linear model for sparse functional data using an approximated quasi-likelihood approach. New approaches will be proposed in the consideration of these problems, and asymptotic theories will be proved to validate the approaches.  The sparse functional generalized linear model will be considered in a framework of joint modeling between a longitudinal life style profile and an endpoint health outcome. This involves the study of a new type of error-in-variable problem, which is expected to extend the horizon of longitudinal-data modeling.<br/><br/>An important current focal point of statistical research is the so-called high-dimensional data analysis. Indeed, high-dimensional data are a fact of life. This is evidenced by our increasing need for larger storage devices on our computers. Roughly speaking, functional data are high-dimensional data which can be approximated by smooth curves or functions. Such data are abundant in scientific investigations, and it is of crucial importance to be able to effectively analyze such data. The PI will investigate approaches that will fundamentally contribute to the practice of functional data analysis. Direct applications of the research can be found in areas including image analysis, bioinformatics, and medicine. Research-level classes on functional data analysis based on this research will be offered at both University of Georgia and University of Michigan."
"0805245","Symbolic Inference for Very Large Datasets","DMS","STATISTICS","08/01/2008","08/01/2008","Lynne Billard","GA","University of Georgia Research Foundation Inc","Standard Grant","Gabor Szekely","07/31/2012","$150,000.00","","lynne@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","Datasets that are complex with the data themselves ""complex"", and/or with structures that impose complications) are becoming more and more routine with the impact of contemporary computer capacity. What is not routine is how to analyse these data. Indeed, the data ""collection"" is fast outpacing the ability to analyse them. It is evident that, even in those situations where in theory available methodology might seem to apply, routine use of such statistical techniques is often inappropriate. Some methods (e.g. squashing) take representative ""`samples""' and then use standard procedures on the sampled data. Others seek sub/patterns (e.g., data mining) and then try to focus on the data behind those patterns. Others aggregate the data in some meaningful way. One such aggregation method produces so-called symbolic data (such as lists, intervals, distributions, etc.). An advantage of symbolic data is that unlike those in sampled sets, a symbolic-value retains all the original data, while simultaneously reducing the size of the dataset. Further while the massive datasets encountered today are one source of symbolic data, there are many data that are naturally symbolic (be these small or large datasets). All are better analysed by methods developed for symbolic data. The investigator addresses three major areas. One area is classification trees. Here, distances measures for interval and histogram-valued data are developed; and then they are used in new algorithms which extend the classical CART methodolgy to symbolic data. Secondly, regression methods, in particular, logistic regression and Cox's proportional hazard models, are adapted to symbolic data. Finally, factor analysis and principal component methodoly is developed for symbolic data. <br/><br/>With the impact of contemporary computer capacity, datasets that are complex with the data themselves ""complex"" are becoming more ubiquitous. Yet those same computers often lack the capacity to analyse these massive datasets. Therefore, new ways to handle them must be developed. One way is to aggregate the data in a scientifically meaningful way (with the actual aggregation being dictated by the question at hand). Such aggregation will necessarily produce data that form lists, intervals, histograms, etc. The investigator develops new methodologies for interval data in three major areas, classification trees after rst nding distance measures for intervals and histograms, regression methods especially logistic regression, and factor analysis. The results are applied to data. A synergism is achieved by the integration of mathematical/ statistical/computational arenas in addressing real issues encountered by contemporary datasets. The outcomes cannot be achieved by the tools of just one of these disciplines but needs all three. The new methodologies will have wide applicability to those datasets generated in, e.g., meteorology, environmental science, social sciences, health-care programs, industry, and the like, well beyond those motivating the work. This will have enormous impact on US science. Further since doctoral students will be engaged as collaborators and since international researchers will be active participants, the research helps in the internationalization of the next and future generation of US scientists."
"0806094","Extremes: Short and Long-Range Dependence; Modeling and Inference with Applications to Computer Networks and Risk Analysis","DMS","STATISTICS","06/01/2008","04/02/2010","Stilian Stoev","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","05/31/2012","$345,300.00","George Michailidis","sstoev@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","This research program addresses problems arising in the modeling and analysis of computer network, insurance and financial risk data. Specifically, it develops global network models and also deals with clustering of extreme values. A number of associated statistical issues, such as network-wide prediction, identifiability of parameters of interest and efficient estimation of the Hurst, tail and extremal indices are also investigated. The proposed global network models are based on a physically interpretable 'bottom-up' approach, where first a low level model is constructed for each source-destination pair of network nodes and subsequently the trafficis aggregated. Under certain limiting regimes, when the number of users grows and with appropriate rescaling of time, a limit approximation of the fluctuations of the network-wide traffic is obtained that is based on functional fractional Brownian motion, a novel class of Gaussian processes. The limit process captures traffic dependencies induced by the topology of the network. In a related direction, the study of clustering of extreme values is undertaken and a number of new estimators for the key parameter of the extremal index are investigated. This provides a new perspective in the study of burstiness in network traffic. Further, a flexible non-asymptotic model of the times between extremes is proposed, which allows better prediction of the frequency at which extreme values occur.<br/><br/>The current work is motivated by problems in modern computer networks, where there is a lot of interest in characterizing traffic fluctuations and burstiness, in order to identify bottleneck links and detect network failures in the form of routing faults or malicious activities. The proposed global network-wide models that take into consideration the network topology together with the temporal dependence in a principled manner allow one to achieve these goals. Further, the development of new methodology for the clustering-of-extremes phenomenon will prove useful in assessing the presence and impact of burstiness in network traffic. The understanding and insight gained as a result of the proposed research will lead to a core of basic principles for network traffic analysis. Understanding better and quantifying the clustering-of-extreme phenomenon will have a broad impact on measuring risk, by incorporating the temporal dependence in extreme financial losses. Finally, the proposed models and techniques will be integrated into open source tools."
"0804093","Collaborative Proposal: Novel Semiparametric Two-part Models: New Theories and Applications","DMS","STATISTICS","08/01/2008","07/30/2008","Xiao-Hua Zhou","WA","University of Washington","Standard Grant","Gabor Szekely","09/30/2012","$104,507.00","","azhou@u.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, OTHR","$0.00","The long term goal of this study is to develop novel semiparametric analysis tools for biological, economical, demographical, and medical studies. In this study, the investigators propose novel semiparametric two-part models for analysis of (1) case I and (2) case k interval censored data with a cured subgroup, and (3) left censored data. Compared with existing ones, the proposed models have greater flexibility by allowing for semiparametric transformation functions and partially linear covariate effects. This study is the first to systematically investigate semiparametric two-part cure rate models with interval censored data, and two-part partially linear transformation models with left censored data. The investigators rigorously establish asymptotic properties of the proposed estimates using advanced empirical process techniques, and demonstrate a systematic framework for future semiparametric analysis of such data. Intensive numerical studies are employed to demonstrate superiority of proposed methods and provide guidelines for practical data analysis.<br/><br/><br/>This study has the following scientific merits. (1) From a modeling point of view, it enriches semiparametric methodologies in general. The proposed models differ significantly from existing parametric or semiparametric alternatives, and are valuable additions to the family of semiparametric models. (2) From a methodology point of view, the investigators establish a general framework of penalized estimation and resampling-based inference for semiparametric models using advanced empirical process techniques. This framework is useful for many other studies. (3) From a statistical practice point of view, the proposed study provides powerful tools for analyzing ongoing studies and has a direct impact. (4) In addition, extensive numerical studies show that the proposed methods are more efficient, more flexible, and less sensitive to model mis-specification. In large scale studies, variables can have complex, nonlinear associations, and observations may be extremely expensive to collect.<br/>The proposed models, along with rigorous analysis techniques, provide more efficient usage of such data, reveal more subtle structures, and benefit the whole scientific society in the long run. The proposed study has the following beneficial, educational and social impacts. (1) It fosters more intensive collaborations among investigators from different institutes and background.<br/>(2) It promotes teaching, training and learning at Yale University and University of Washington. (3) The investigators attend statistical and scientific meetings and present their research, which may promote interdisciplinary research among other scientists.<br/>"
"0805809","Dynamic Models and Decision Making for Complex Reliability Systems","DMS","STATISTICS","06/15/2008","05/09/2010","James Lynch","SC","University South Carolina Research Foundation","Continuing Grant","Gabor Szekely","08/31/2011","$249,999.00","Edsel Pena","lynch@stat.sc.edu","915 BULL ST","COLUMBIA","SC","292084009","8037777093","MPS","1269","0000, 9150, OTHR","$0.00","The major challenge for assessing system reliability or decision-making regarding maintenance or replacement policies is that many systems are composed of, or can be viewed as, complex systems of components (or of subsystems) whose behavior within the system can be quite complicated due to omponent interactions and dependencies. This challenge requires model selection that incorporates ""physics-of-failure"" considerations into the model, integrates component reliability information into system reliability models that realistically models the censoring for these types of systems, and accounts for uncertainties in the model.  Crucial to the decision making process is the proper model choice (model selection) as well as the integration of component reliability information into the system reliability assessment where in many cases the component information obtained from system data involves complicated censoring mechanisms. The major goal of this proposed project is to study dynamic load-sharing reliability models and decision-making with model selection for such models. Specifically, the aims of this project are: 1. to propose dynamic load-sharing models for reliability systems that incorporate ""physics-of-failure"" considerations and the dynamic interactions and dependencies among components or subsystems; 2. to obtain probabilistic properties of these load-sharing systems, in particular, to derive the system life distribution; 3. to examine data-accrual schemes for such systems and to develop statistical inference procedures for these load-sharing systems that also account for censoring; and 4. to develop decision-making strategies in the context of reliability systems when there are several competing models, leading in particular to decision-making with model selection or, possibly, model-averaging.<br/><br/>The assessment of system reliability requires accurate prediction of system failure. This is also essential to decision making regarding maintenance or replacement policies and is especially important for key systems or equipment in attempting to prevent catastrophic failures during critical operations. The merit of this proposed project emanates from the fact that the dynamic load-sharing reliability models will take into account model component interactions and dependencies. These generic reliability models will also be useful for other complex systems, such as in physical, biological, and medical sciences. For example, for a mechanical system under increasing load, (such as a composite under tensile loading where fiber segments are components or a routing system under increasing traffic, where the nodes are components) the load-sharing rule describes how the load or traffic is transferred/redistributed from failed components to working components and takes into consideration the ""physics-of-failure."" The proposed models have the potential to synthesize ""physics-of-failure"" and ""statistical reliability"" concepts to describe how damage to the system contributes to system failure. The statistical inference aspects of this project, including the model selection and the decision-making portion, will address important problems pertaining to the estimation of model parameters based on complex data for these dynamic models. This inference problem has not been dealt with extensively in the reliability literature, hence this project is expected to provide significant advances on this direction.  The results of the investigations are expected to impact engineering and other sciences by providing novel and more realistic models for system failure and inference procedures for prediction of failure and for making maintenance and replacement decisions."
"0804587","Shape Restrictions, Semiparametric Models, and Empirical Processes","DMS","STATISTICS","07/01/2008","04/06/2010","Jon Wellner","WA","University of Washington","Continuing Grant","Gabor Szekely","06/30/2012","$260,000.00","","jaw@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, OTHR","$0.00","Jon A. Wellner  will carry out research on empirical process methods and computational strategies for a variety of semiparametric and nonparametric models for shape restrictions, censored data, and missing data problems.<br/>In particular, Wellner and University of Washington graduate student Marios Pavlides  will conduct research on several models involving multivariate monotone density functions and multivariate current status observation schemes, a class of models currently of interest in connection with HIV-AIDS studies. Wellner and University of Washington graduate student Arseni Seregin will conduct research on nonparametric estimation of log-concave densities and generalizations thereof in one and higher dimensions.  Some of the research on empirical processes will be carried out jointly with Aad van der Vaart (Free University, Amsterdam). Some of the research on inference under shape constraints will be carried out jointly with Fadoua Balabdaoui, Marloes Maathuis, and Shuguang Song (former Ph.D. students). The work on new computational algorithms will be carried out jointly with Hanna Jankowski, a recent post-doctoral student at the University of Washington and now a faculty member at York University, Canada. These investigations will involve nonstandard asymptotics for maximum likelihood estimators, likelihood ratio statistics, and distribution theory for new nonstandard limit distributions.<br/>The research will involve development of basic empirical process tools and methods, and applications of these new tools and methods to statistical problems concerning semiparametric models, shape restricted models, and inverse problems. Applications include regression models for panel count data, bivariate interval censored data of several kinds, regression models for survival data with missing covariate data, studies of non- and semi-parametric maximum likelihood estimators used in AIDS research, new confidence bands for shape restricted inference, and both basic empirical process theory and new semiparametric estimation procedures for two-phase data dependent designs. <br/><br/><br/>The advances in inference methods for competing risks models are used to study causative effects of viral loading in HIV vaccine trials.  The developments in bivariate censored data are applied to studies of mother-to-child transmission of HIV-AIDS.  The work on two-phase data dependent designs has application to new designs with increased efficiency for clinical trials and case-cohort sampling in epidemiology. The tools of empirical process theory allow investigations of many problems of current interest in other areas of statistics involving high-dimensional data and parameter spaces. The research benefits education and human development by the training of graduate students and the inclusion of the resulting new statistical methods in statistics courses at the University of Washington and elsewhere."
"0927126","Semiparametric regression for correlated data","DMS","STATISTICS","10/01/2008","03/19/2009","Tatiyana Apanasovich","PA","Thomas Jefferson University","Standard Grant","Gabor Szekely","06/30/2009","$25,749.00","","apanasovich@gwu.edu","1020 WALNUT ST","PHILADELPHIA","PA","191075567","2155036976","MPS","1269","0000, OTHR","$0.00","There has been growing interest in developing innovative statistical models for dependent data. The project aims to address the following issues which have arisen in modeling correlated data: extra zeros in count data, non-linear functional relationships and measurement errors.  Frequently it is of interest to make marginal inference about trends and effects of explanatory variables on correlated counts. Failure to account for the extra zeros may result in biased parameter estimates and misleading inferences. The investigator proposes a generalization of the standard zero-inflated regression model to the correlated data case, allowing for a Heckman-type selection process. Measurement Error Models offer coverage of estimation for situations where the model variables are observed subject to measurement error. The second part of the project focuses on developing Semiparametric Measurement Error Model for dependent data using two approaches: the first is based upon the idea of Monte Carlo corrected scores and the second is a generalization of the SIMEX (simulation-extrapolation) method to general semiparametric models.<br/><br/>Semiparametric regression is concerned with the flexible incorporation of non-linear functional relationships in regression analyses. The investigator is particularly interested in correlated data which arise in a variety of settings in many areas of applications: Biology, Genetics, Bioinformatics, Biostatistics, Medicine, Econometrics, Engineering and Sociology. In many practical situations the attribute or event of interest is rare and/or other variables preclude observation of an event, consequently a higher proportion of the counts than implied by standard models may equal zero. Part of the focus of this proposal is to develop semiparametric models for the correlated data with extra zeros. When there is an uncertainty in measuring covariates, the usual regression estimators are biased and when the measurement error is substantial, alternative procedures are necessary. Part of the emphasis of the proposal is on developing semiparametric measurement error regression models for dependent data. The statistical methodology to be developed will be circulated to the statistical community in academia and industry through series of papers and publicly available programming code. On the educational front, some of the research material in this proposal will be incorporated in courses at the undergraduate and graduate level. Some projects will also serve as dissertation topics for Ph.D. advisees and will therefore play an important role in the training of future statisticians.<br/><br/>"
"0806098","""Collaborative Research: Regression Problems in Functional Data Analysis""","DMS","STATISTICS","06/01/2008","04/02/2010","Tailen Hsing","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","05/31/2012","$90,000.00","","thsing@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","Modern data collection methods are now frequently returning observations that could be viewed as the results of digitized recording or sampling from random functions. This project investigates regression problems for which the response is scalar but some of the predictors are functional. The general goal is to gain understanding on the inference of the models based on partially observed and error-contaminated functional data. Distinctions will be made between dense functional data, usually obtained from images, and sparse functional data, usually obtained from longitudinal studies. The specific topics include the consideration of (i) a functional generalized linear model for dense functional data using a penalized likelihood approach, (ii) dimension reduction methodologies based on sliced inverse regression and sliced average variance estimation, and (iii) a functional generalized linear model for sparse functional data using an approximated quasi-likelihood approach. New approaches will be proposed in the consideration of these problems, and asymptotic theories will be proved to validate the approaches.  The sparse functional generalized linear model will be considered in a framework of joint modeling between a longitudinal life style profile and an endpoint health outcome. This involves the study of a new type of error-in-variable problem, which is expected to extend the horizon of longitudinal-data modeling.<br/><br/>An important current focal point of statistical research is the so-called high-dimensional data analysis. Indeed, high-dimensional data are a fact of life. This is evidenced by our increasing need for larger storage devices on our computers. Roughly speaking, functional data are high-dimensional data which can be approximated by smooth curves or functions. Such data are abundant in scientific investigations, and it is of crucial importance to be able to effectively analyze such data. The PI will investigate approaches that will fundamentally contribute to the practice of functional data analysis. Direct applications of the research can be found in areas including image analysis, bioinformatics, and medicine. Research-level classes on functional data analysis based on this research will be offered at both University of Georgia and University of Michigan."
"0748389","CAREER: Statistical Learning from Data with Graph/Network Structures","DMS","STATISTICS","07/01/2008","07/06/2012","Ji Zhu","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","06/30/2014","$400,000.00","","jizhu@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, 1045, 1187, OTHR","$0.00","The research aims to develop new statistical methodologies and associated theory that incorporate the network/graph structure in the data.  Such data are becoming increasingly common in various fields.  Specifically, the investigator studies three different but related problems: a) statistical learning on networks via random walks, which includes semi-supervised classification for two and multiple classes, clustering, and analysis of categorical data; b) learning network structures, which deals with the situation where one is interested in identifying the underlying network structure from the data; c) variable selection with structural constraints, which deals with variable selection when there is inherent structure among the variables or parameters.<br/><br/>Recent advances in computing and measurement technologies have led to an explosion in the amount of data that are being collected in all areas of application.  Much of these data have complex structure, in the form of text, images, video, audio, streaming data, and so on.  This proposal focuses on one important class of problems, viz, data with network or graph structure.  Such data are common in diverse engineering and scientific areas, such as biology, computer science, electrical engineering, economics, sociology and so on.  While there has been extensive research on networks (primarily outside the field of Statistics), much of it deals with characterizing and modeling network structures.  The goal of the current research program is to exploit the network structure as additional information and develop statistical methods that take into account the structure of relationships between the data.  The research program will make significant contributions in several areas, including Statistics, Biology, Computer Science, Electrical Engineering, IOE, Physics, Psychology and Sociology.  The educational program also includes substantial initiatives that will involve undergraduate and graduate students and expose them to state-of-the-art research in the topics related to the proposal.  These include new courses, summer workshops, mentoring, and software development."
"0753635","Infinite Possibilities Conference","DMS","STATISTICS","01/01/2008","12/18/2007","Kimberly Weems","NC","North Carolina State University","Standard Grant","Gabor Szekely","12/31/2008","$10,013.00","Tanya Henneman, Leona Harris","weems@stat.ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","0000, 7556, OTHR","$0.00","The Infinite Possibilities Conference promotes, educates, encourages and supports minority women interested in mathematics and statistics.  This national event assembles female mathematical scientists from underrepresented groups for a two-day conference at North Carolina State University.  The plenary speakers include the following inspiring mathematical scientists:  Dr. Alicia Carriquiry, professor of statistics at Iowa State University and editorial board member of several Latin American journals of statistics and mathematics; Dr. Iris Mack, the second African-American woman to earn a Ph.D. in applied mathematics from Harvard University; and Dr. Freda Porter, President and CEO of Porter Scientific and founding member of the North Carolina American Indian Chamber of Commerce.  These speakers showcase the importance of mathematics and statistics in forensic, financial and environmental applications. <br/>Participants present research projects during the poster session and concurrent research talks.  Also, participants learn how to best write a proof and improve technical writing skills at workshops. Through the Dialogues in Mathematics sessions, the conference creates a forum to discuss experiences unique to minority female mathematicians and statisticians and identifies factors leading to success in the mathematical sciences for underrepresented minority women.<br/><br/>The goal of the Infinite Possibilities Conference, by presenting examples and showcasing role models, is to make all participants aware of the unlimited ways in which mathematicians and statisticians contribute to the scientific community and society at large.  This conference provides a forum for underrepresented minority females to share experiences in mathematics and statistics as well as an opportunity to share resources and build networks. Participants include high school students, undergraduates, graduate students, and professionals from the various fields in mathematics and statistics.  The Infinite Possibilities Conference increases awareness of existing organizations and programs that support women and underrepresented minority groups.  In addition, it celebrates and honors the achievements of minority women in the mathematical sciences.<br/><br/>"
"0805598","Fully Nonparametric Models for Random Effects, Order Thresholding, Boostrap Testing, and Applications","DMS","STATISTICS","07/15/2008","07/15/2008","Michael Akritas","PA","Pennsylvania State Univ University Park","Standard Grant","Gabor Szekely","07/31/2012","$150,000.00","","mga@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, OTHR","$0.00","<br/>Technological advancements in information gathering, and the increased fusion of mathematical innovation with biological, oceanic/atmospheric, and psychosocial sciences, have created a plethora of highly complex and very high-dimensional data sets in interdisciplinary research contexts. The non-standard features of such data include non-normality, complex heterogeneity and dependence structures, high-dimensionality, low sample sizes and unbalanced designs. The investigator puts forth more realistic statistical models for such data sets and develops advanced statistical methods for their analysis. This project has five specific aims: to enhance the modeling alternatives by proposing fully nonparametric models for crossed and nested two-way random and mixed effects designs, to construct statistical procedures for the common hypotheses of interest under each of these models (including robust rank-based procedures), to propose order thresholding (thresholding based on L-statistics) for reducing the dimensionality of the alternative hypothesis and for identification of the signal location, to propose a bootstrap testing method for improved accuracy of the test procedures, and to explore applications of the aforementioned test procedures to classification problems, through the recently developed test-based classification method. The proposed models and methods are fundamentally different in approach from the standard likelihood methods, the non- and semi-parametric models, and the Bayesian techniques.<br/><br/>The significance of this project stems from the fact that statistical analysis is the final, and often the most important, stage of many expensive scientific investigations. Does the concentration of certain contaminants in coastal waters have a decreasing or an increasing trend? Is a trend in the concentration of contaminants a result of natural processes or is it caused by human activity? Are gene expressions different under different biological environments and which genes are responsible for this difference? Has the Gang Resistance Education and Training<br/>(G.R.E.A.T.) program been effective in reducing adolescent deviant/illegal activities in urban areas? In early detection of the use of bioweapons, is there a signal (a certain symptom at rates higher than background) and if so where is it located? Typically, the data collected for answering such questions exhibit highly non-standard features. The data being collected by the Mussel Watch Project of NOAA's National Status and Trends program for monitoring marine environmental quality, is a good example of the type of complex features such data can exhibit. Due to their non-standard features, the data may fail to satisfy the regularity conditions that alternative models and methods require. Analyzing data under assumptions that are not satisfied may lead to incorrect conclusions regarding the statistically significant factors and trends, or may fail to identify an existing signal or the genes that are affected by a disease. The objective of this proposal is to develop advanced methods for data analysis based on realistic statistical models, and to develop software for their implementation.<br/>"
"0805409","Model free variable selection in sufficient dimension reduction","DMS","STATISTICS","09/01/2008","08/27/2008","Liqiang Ni","FL","The University of Central Florida Board of Trustees","Standard Grant","Gabor Szekely","08/31/2011","$61,594.00","","lni@mail.ucf.edu","4000 CENTRAL FLORIDA BLVD","ORLANDO","FL","328168005","4078230387","MPS","1269","0000, 1269, OTHR","$0.00","This project is aimed to develop a model free variable selection method under the framework of sufficient dimension reduction. With many advantages over more traditional subset selection methods in linear models, regularization has been introduced to model free framework. However, there are only a few results on the consistency of variable selection and the efficiency of the estimation of the dimension reduction subspace. This is partly due to the fact that regularization is often applied to eigenvectors of candidate matrices instead of individual predictors. The investigator proposes a penalized objective function with adaptive shrinkages to select variables without rigid parametric or semi-parametric model assumptions. The proposed new approach synthesizes the progresses in two fronts of statistical research: regularization method in linear models and sufficient dimension reduction in regression. <br/><br/>Selection of important factors can provide a better understanding and interpretation of underlying scientific process, obtain numerically efficient and consistent results, and improve accuracy of prediction whenever applicable. It has become the focus of current research in areas for which data sets with hundreds and thousands of variables are quite common. Examples include bioinformatics, data mining, classification, and pattern recognition. With limited knowledge of the underlying models, good model free methods for selecting important variables are in high demand. This project will investigate model free methods for the selection of key variables. The research findings are expected to yield substantial gains in the efficiency of data management, especially for high dimensional cases which have been the hallmark of contemporary social, economic, and scientific data analysis."
"0902332","Advanced Bayesian Approaches for Heterogeneous Temporal Genomic MetaData","DMS","STATISTICS","08/15/2008","05/27/2009","Yulan Liang","MD","University of Maryland at Baltimore","Standard Grant","Gabor Szekely","05/31/2009","$36,889.00","Arpad Kelemen","ylian001@umaryland.edu","220 ARCH ST OFC LEVEL2","BALTIMORE","MD","212011531","4107063559","MPS","1269","0000, OTHR","$0.00","<br/>Recent genomic and proteomic data sets are so disparate and complex that not too many studies have provided robust and sophisticated modeling for the latent information.  The principal research of the underlying project strives to make efficient and maximal use of the generated information from heterogeneous temporal genomic sources where the special features are present. The investigator aims to develop, implement and validate the innovative advanced Bayesian modeling techniques, such as Bayesian state space models for studying the dynamics of heterogeneous temporal genomic metadata for (i) inferring and predicting the genomic profiles associated with diseases and treatments; (ii) estimating the important hidden biological parameters; (iii) constructing gene-time-gene and protein-protein interaction networks and pathways for hybrid biological systems. This should be sufficient to explain causal and probable relations about the interactions of genes-treatments-diseases and gene-environment. The investigator evaluates the efficacy and sensitivity of the proposed models through detailed study of specific diseases, such as time course of lymphocyte gene expression data from interferon-beta-1a treated multiple sclerosis patients and multiple tissues polygenic data such as kidney and liver data of animals sacrificed at 17 time points following administration of a bolus dose of MPL. Graphic display of the results from each model are provided to explore the dynamics of the modeling processes, which marks an important intermediate goal that allows visual examination of the degrees of heterogeneity between models. Through the research and educational activities the heterogeneous raw temporal genomic data are converted into scientific knowledge that advances our understanding of today's common complex diseases, biological processes and potentially identify new modalities of treatment.<br/><br/>This project describes a novel area for the field of statistical genomics and bioinformatics, which is driven by the over-availability of a variety of heterogeneous temporal genomic data and methodologies. Through the research and teaching activities, systematic understanding and overall knowledge is generated for efficient data exploration methodology, primarily contributing to the areas of statistics and computer science. If fully successful, its contribution to the fields of biology, pharmaceutical sciences, and medical sciences could be invaluable, potentially speeding up research, diagnosis, drug development, and medical decision making ultimately improving human and other life. Biomedical/genomic applications of the developed methodologies would support biologists and medical researchers to better understand the underlying causes of diseases, the risks and offer a more powerful diagnostic tool and predictive treatment and provide customized solutions to genomic data analysis. Both the theoretical and the practical foundations of the activity will make an impact on higher education, especially in training the current and next generation of statisticians and computational scientists to tackle challenges involved in the human genome research. The techniques developed in this project are be used to augment and develop related courses and made available on the internet for outreach at large."
"0804547","New multiple testing methods for pairwise comparisons & other dependent data cases","DMS","STATISTICS","09/01/2008","08/19/2008","Harold Sackrowitz","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","08/31/2011","$140,206.00","","sackrowi@rci.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","The investigators will study new methodology in multiple testing in the case where test statistics are dependent.  The objective is to provide extensions of the multiple residual down method (MRD) developed by these investigators.  The extensions encompass important topics such as pairwise comparisons, contrasts, a variety of exponential family distributions, and tests for independence of collections of contingency tables embedded in a large contingency table.  The new methodology and its extensions are expected to have several desirable advantages over current methodology.  <br/><br/>Multiple testing is one of the most important practical areas of statistical inference.  Here is a list of areas in which multiple testing is utilized and has impact:  (i) microarray analyses ? the identification of which genes are related to cancer,(ii)monitoring spatial and temporal abundance of bacteria in air samples in US cities, (iii)identifying successful mutual funds, (iv)identifying individual or groups that are vulnerable to disclosure risk, (v)identifying changes in quality in production processes, (vi)determining which blood analytes remain unchanged when new and more efficient blood collection tubes are used, and (vii)identifying which schools are not meeting expected standards. In addition, applications of multiple testing have been used in imaging, analysis of proteins, and numerous other branches of medicine.  Research can provide better and more efficient methodology, which can greatly benefit all those cognate fields."
"0804626","Collaborative Research: Fast Functional MRI","DMS","STATISTICS","07/01/2008","08/02/2009","Larry Shepp","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","06/30/2012","$234,459.00","Cun-Hui Zhang","shepp@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","The problem addressed by the proposal is to find, for a given higher cognition task, one or more regions of the brain where oxygen is consumed in performing the task. The investigators approach is to use the so-called initial negative dip, which is caused by the initial consumption of oxygen in the blood reserve pool in an active brain region, in order to observe, in real time the location and timing order of brain activation when a human performs the higher cognition task. The standard approach, which relies on the stronger but much slower positive rise, due to the resupply of blood to the regions several seconds after activation, is inadequate for determining the temporal ordering of brain activity in different regions. The investigators have developed an elegant and highly efficient echo-volumar imaging (EVI) sampling scheme for performing fast functional Magnetic Resonance Imaging (fMRI), along with the necessary tools needed for image reconstruction and statistical analysis of the resulting data sets. They have used their techniques to conduct higher cognition experiments using a variety of paradigms, and were consistently able to detect the negative dip and exhibit its statistical significance. In addition, they demonstrated that the negative dip appears in different brain regions in the same temporal sequence as the corresponding brain activation according to the experimental paradigms, and that the timing of the positive rise is at times confounded. The proposed research, which will further develop the investigators methods, includes the design of EVI trajectories for the newest and most powerful 7 Tesla scanners and more complicated multiple coil systems, in order to improve the spatial and time resolutions of their approach. New image reconstruction methods will also be developed for the multi-coil data and the investigators statistical analysis tools will be validated and carefully improved.<br/><br/>This proposal focuses on developing statistical methods and related theory for performing fast fMRI. The proposed research will further advance and use the methods developed by the principle investigators and their collaborators to sharply improve the time-resolution for the blood oxygen level dependence technique of functional magnetic resonance imaging. In its current implementation the investigators method is able to measure brain volumes every 100ms compared to 2000ms for standard fMRI, thereby allowing fMRI studies to be performed at an unprecedented temporal resolution. Fast fMRI is expected to have profound and far-reaching consequences in the understanding of brain function, a problem of central scientific interest at the present time."
"0804759","North American Meeting of Researchers in Statistics and Probability, Summer 2008, Boulder, CO","DMS","PROBABILITY, STATISTICS","03/01/2008","02/25/2008","Rebecka Jornsten","NJ","Rutgers University New Brunswick","Standard Grant","Yazhen Wang","02/28/2009","$21,300.00","","rebecka@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1263, 1269","0000, 7556, OTHR","$0.00","The investigator and her colleagues are organizing and hosting a conference aimed specifically at new researchers in statistics and probability. The conference constitutes a unique venue and opportunity for junior researchers to interact with their peers in an informal setting. During the conference, the participants will present their research ideas in a series of talks and poster sessions. The investigator and her colleagues have invited several senior researchers to the conference. The senior participants will host panel session where the junior researchers can learn about funding opportunities, how to mentor students, and how to publish their research in professional journals. The primary goals of the conferences are professional education of new researchers and to help create a network among these junior researchers, both vital parts of professional growth. Women, minorities, and the disabled are explicitly encouraged to attend. <br/><br/>The investigator plans to organize and host the Eleventh Meeting of New Researchers in Statistics and Probability. The primary objective is to provide a venue for interaction among new researchers. The proposed conference will take place July 29 - August 2, 2008 at the University of Colorado, Boulder, and the National Center for Atmospheric Research (NCAR), to immediately precede the annual Joint Statistical Meetings (JSM) in Denver. Participants will be statisticians and probabilists that have received their Ph.D. within the past five years or are expecting to receive their degree within the next year. Each participant will present a talk or poster. Topics will range across a variety of areas in statistical research from theory and methods to applications. Senior speakers will discuss topics of particular interest to new researchers. Panel discussions during the conference will cover the topics of journal publications, opportunities in statistics laboratories and industry, funding, and mentoring.<br/>"
"0806076","Collaborative Research: Fence Methods for Complex Model Selection Problems","DMS","STATISTICS","09/01/2008","08/28/2008","Jonnagadda Rao","OH","Case Western Reserve University","Standard Grant","Gabor Szekely","10/31/2011","$44,946.00","","js-rao@umn.edu","10900 EUCLID AVE","CLEVELAND","OH","441061712","2163684510","MPS","1269","0000, OTHR","$0.00","Many model search strategies involve trading off model fit with model <br/>complexity in a penalized goodness of fit measure. Asymptotic properties <br/>for these types of procedures in some conventional situations, such as <br/>regression and ARMA time series have been studied. Yet, such strategies <br/>do not always translate into good finite sample performance. Furthermore, <br/>such standard model selection procedures encounter difficulties for <br/>nonconventional model selection problems as well. This project aims at <br/>developments of a new model selection strategy, called fence methods, in <br/>following four major areas of methodology research and applications: (i) <br/>development of adaptive fence methods for high dimensional and complex <br/>model selection problems using the idea of restricted maximum likelihood; <br/>(ii) development of data adaptive fence methods for nonparametric model <br/>selection problems such as penalized smoothing spline estimation; (iii) <br/>development of fence methods for quantitative trait loci (QTL) mapping; <br/>and (iv) development of user-friendly standalone software for implementing <br/>the fence methods. <br/><br/>The fence idea is generally based on building a statistical fence, or <br/>barrier, to carefully eliminate incorrect models. This is done by <br/>determining which models are within variation of a goodness-of-fit <br/>measure of an anchor model. Once the fence is constructed, the optimal <br/>model is selected from amongst those within the fence according to <br/>a criterion which can be made flexible. For example, the criterion can <br/>incorporate scientific or economic concerns. The adaptive fence method <br/>may be viewed as comparing signals with noises to come out with an optimal <br/>decision supported by the data. Given such a wide spectrum of models that <br/>can be handled, the range of applications seems enormous. Of particular <br/>interests are applications in human genetics, medical research and surveys. <br/>To facilitate such translational research, the investigators plan to freely <br/>disseminate available computer software to implement the fence methods. <br/> <br/>"
"0806178","Output Analysis for Markov Chain Monte Carlo","DMS","STATISTICS","06/15/2008","03/15/2010","Galin Jones","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","05/31/2011","$183,252.00","","galin@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","Markov chain Monte Carlo (MCMC) has several disadvantages when compared to classical Monte Carlo methods. In particular, an important issue that every practitioner faces when implementing MCMC is when to stop the computation. Typically, a mixture of experience and ad hoc methods is employed to make this decision. Thus one is forced to wonder about the quality of the inference. The investigator studies sequential fixed-width methods that allow construction of an interval estimator for the quantity of interest. The interval estimator describes the confidence in the point estimate. The investigator uses this to study the development of valid stopping rules when the MCMC computation is aimed at estimating general quantities of the target distribution.<br/>These methods require the Markov chain to converge at a geometric rate which in turn implies there is a limiting distribution of the point estimate in the settings of interest. Thus the investigator studies the convergence rates of Markov chains encountered in two broad classes of Bayesian models.<br/><br/><br/>MCMC methods have become a standard technique in the toolbox of applied statisticians (and many scientists in other disciplines). Indeed, it is not much of an overstatement to say that it has revolutionized applied statistics, especially that of the Bayesian variety. Unfortunately, MCMC methods are not always used carefully leading to dubious claims in the literature. In particular, there has been little effort to include measures of uncertainty in inferential conclusions. Rigorously addressing the issue of stopping rules in terms of these measures of uncertainty enhances infrastructure for research and education by providing statisticians and other scientists valid techniques for using MCMC to make inference in their research setting."
"0805984","Collaborative Proposal: Novel Semiparametric Two-part Models: New Theories and Applications","DMS","STATISTICS","08/01/2008","07/30/2008","Shuangge Ma","CT","Yale University","Standard Grant","Gabor Szekely","07/31/2012","$105,000.00","","shuangge.ma@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","0000, OTHR","$0.00","The long term goal of this study is to develop novel semiparametric analysis tools for biological, economical, demographical, and medical studies. In this study, the investigators propose novel semiparametric two-part models for analysis of (1) case I and (2) case k interval censored data with a cured subgroup, and (3) left censored data. Compared with existing ones, the proposed models have greater flexibility by allowing for semiparametric transformation functions and partially linear covariate effects. This study is the first to systematically investigate semiparametric two-part cure rate models with interval censored data, and two-part partially linear transformation models with left censored data. The investigators rigorously establish asymptotic properties of the proposed estimates using advanced empirical process techniques, and demonstrate a systematic framework for future semiparametric analysis of such data. Intensive numerical studies are employed to demonstrate superiority of proposed methods and provide guidelines for practical data analysis.<br/><br/><br/>This study has the following scientific merits. (1) From a modeling point of view, it enriches semiparametric methodologies in general. The proposed models differ significantly from existing parametric or semiparametric alternatives, and are valuable additions to the family of semiparametric models. (2) From a methodology point of view, the investigators establish a general framework of penalized estimation and resampling-based inference for semiparametric models using advanced empirical process techniques. This framework is useful for many other studies. (3) From a statistical practice point of view, the proposed study provides powerful tools for analyzing ongoing studies and has a direct impact. (4) In addition, extensive numerical studies show that the proposed methods are more efficient, more flexible, and less sensitive to model mis-specification. In large scale studies, variables can have complex, nonlinear associations, and observations may be extremely expensive to collect. The proposed models, along with rigorous analysis techniques, provide more efficient usage of such data, reveal more subtle structures, and benefit the whole scientific society in the long run. The proposed study has the following beneficial, educational and social impacts. (1) It fosters more intensive collaborations among investigators from different institutes and background.<br/>(2) It promotes teaching, training and learning at Yale University and University of Washington. (3) The investigators attend statistical and scientific meetings and present their research, which may promote interdisciplinary research among other scientists.<br/>"
"0852498","Innovation and Inventiveness in Statistical Methodologies","DMS","STATISTICS","12/01/2008","11/03/2008","Huibin Zhou","CT","Yale University","Standard Grant","Gabor Szekely","11/30/2009","$25,000.00","","huibin.zhou@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","0000, 7556, OTHR","$0.00","The field of statistics continues to offer profound contributions to many branches of science, medicine, and industry. In the rapidly-evolving information age, we have access to overwhelming quantities of data for the study of complex phenomena. This creates both challenges and opportunities for statisticians, and demands innovative and inventive statistical methodologies. Although some research has been done on high-dimensional, complex data, the field continues to evolve, offering a plethora of promising research topics. Now is a critical time for the development of deep, broad, and formal statistical theory and methodology; our field is in a position to grow and make influential, invaluable contributions to society at large. This workshop will bring together some of the most prominent researchers in the field. They will review the most important and influential statistical advances in the comparatively short history of our discipline while embracing the present and future challenges and opportunities for the profession. The workshop will provide a venue for promising young researchers to interact with these leaders of the field and each other, presenting their new research, generating fresh ideas and leading to future collaborations and discoveries.<br/>    <br/>This workshop emphasizes the balance between fundamental statistical theory and innovative and inventive research that reaches out to areas like bioinformatics, biology, finance, image analysis, analysis of sparse signals and genomics. Yale is an ideal venue for a meeting of this type. Not only is the Yale Department of Statistics widely known for theoretical work strongly motivated by applications, it is closely linked to Yale departments in the biological, medical, physical and social sciences. Significant attendance from these groups is assured, guaranteeing important interactions during both the workshop's scientific meetings and other, less formal discussions. The NSF grant is primarily used for getting young researchers and graduate students, in particular minorities and under-represented groups. We will advertise in IMS bulletin and AMStat News and send fliers and emails to related departments of US universities to reach junior faculty and graduate students."
"0746667","SGER: Digital Art Authentication Using Regularities in Spatial and Photometric Statistics","DMS","STATISTICS, ANALYSIS PROGRAM","02/01/2008","04/21/2009","Daniel Rockmore","NH","Dartmouth College","Continuing Grant","Bruce P. Palka","01/31/2010","$200,000.00","","rockmore@cs.dartmouth.edu","7 LEBANON ST","HANOVER","NH","037552170","6036463007","MPS","1269, 1281","0000, 9150, 9237, OTHR","$0.00","This award will partially support the production of a documentary video that will be used as a high school/college teaching tool and will also be suitable for airing on public television. The purpose of the video is to improve the public's perception of mathematics and thereby encourage an increased number of students to pursue careers in the mathematical sciences. This will be accomplished by portraying the human side of mathematical discovery and creativity. This project is being supported by the Divisions of Astronomy, Materials Research, Mathematics, and Physics within the Directorate of Mathematics and Physical Sciences, and the Division of Elementary, Secondary, and Informal Education within the Directorate of Education and Human Resources.<br/><br/>"
"0806011","Collaborative Research:   Nonparametric Theory on Manifolds of Shapes and Images, with Applications to Biology, Medical Imaging and Machine Vision","DMS","STATISTICS","07/01/2008","09/16/2011","Rabindra Bhattacharya","AZ","University of Arizona","Standard Grant","Gabor Szekely","09/30/2012","$191,909.00","","rabi@math.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1269","0000, OTHR","$0.00","Much of the focus of this collaborative project is on the analysis of landmark based shapes in which a k-ad, i.e., a set of k points or landmarks on an object or a scene are observed in 2-D or 3-D, usually with expert help, for purposes of identification, discrimination, or diagnostics. Depending on the way the data are collected or recorded, the appropriate shape of an object is the maximal invariant specified by the space of orbits under a group G of transformations. In particular, Kendall's shape spaces of k-ads are invariant under scaling and Euclidean rigid motions. While this is a proper choice for many problems in biology and medical imaging, other notions of shape such as affine shape and projective shape are important in machine vision and bioinformatics. All these spaces are differentiable manifolds, often with natural Riemannian structures for measuring lengths and angles. The statistical analysis based on Riemannian structures is said to be intrinsic. In other cases, proper distances are sought via an equivariant embedding of the manifold M in a vector space E. Corresponding statistical analysis is called extrinsic. Finding proper Riemannian structures and equivariant embeddings is one of the objectives of this project, which is crucial for the statistical inference proposed. Establishing broad conditions for the existence of the Frechet mean, as the unique minimizer of the Frechet function the expected squared distance from a Q-distributed random shape is important for statistical inference; and it is a goal of the project to pursue, especially for intrinsic analysis where it has remained an outstanding open problem from the inception of shape theory. Reconstruction of a scene from two (or<br/>more) aerial photographs taken from a plane is one of the research problems in affine shape analysis. Potential applications of projective shape analysis proposed here include face recognition and robotics-for robots to visually recognize a scene.<br/><br/>Statistical analysis of data on geometric objects, or manifolds, is an exciting and challenging field of research, where statistical theory and differential geometry are inextricably intertwined, and implementation requires innovative algorithms and high speed computation. The project proposed here deals with the development of nonparametric methodology in this context, which must also resolve associated geometric issues and problems of implementation. Past progress in this field by the PIs has laid the foundation for the present project. The statistical analysis proposed has wide ranging applications, especially in biology and bioinformatics, health sciences, and machine vision. Under this project, the PIs plan to train both undergraduate and graduate students, as well as at least one postdoctoral fellow, in theory and in its practical implementation. This continues much further the present activities of the PIs in this regard. In addition, computational algorithms and codes are made available on websites to create and disseminate this research and its applications.<br/>"
"0806133","Change-Point Estimation Under Abrupt Changes","DMS","STATISTICS","06/01/2008","03/04/2010","Venkata Jandhyala","WA","Washington State University","Continuing Grant","Gabor Szekely","05/31/2011","$149,967.00","Stergios Fotopoulos","jandhyala@wsu.edu","240 FRENCH ADMINISTRATION BLDG","PULLMAN","WA","991640001","5093359661","MPS","1269","0000, OTHR","$0.00","There have been attempts in the literature to derive the asymptotic distribution of the maximum likelihood estimate (mle) of a change-point for offline change-point models under the abrupt change formulation.  These attempts, thus far, have yielded results that are either of analytical interest only, or they are available only in the form of bounds and approximations.  Here, the investigators develop exact computable expressions for the asymptotic distribution of the change-point mle when an abrupt change occurs in the parameters of some of the most basic and commonly encountered change-point models.  Specific models considered include: (i) change in the parameter of a univariate exponential distribution; (ii) change in the parameters of a multivariate exponential distribution; (iii) change in the mean only of a univariate normal distribution; (iv) change in the variance only of a univariate normal distribution; (v) change in both mean and variance of a univariate distribution; (vi) the extensions of problems (iii)-(v) for the multivariate normal distribution; and (vii) extensions to situations such as Poisson mixtures of generalized inverse Gaussian distributions.  Depending upon the model, the derived distributions for the change-point mle may be either symmetric or possibly asymmetric.  On the other hand, distributions derived in the literature under the additional assumption of smoothing are known to be always symmetric under all modeling scenarios.<br/> Change-point methods are important scientific tools for the detection and estimation of changes that occur in variables monitored over time.  Such methods are at the forefront of identifying changes in variables related to issues of contemporary significance such as climate change, ozone depletion, water pollution, air pollution, and nuclear waste monitoring programs.  For example, on the basis of various scientific theories, claims are made that global warming has begun some years ago.  The question is whether relevant data collected overtime support such claims.  If so, when did such a change happen?  In their attempts to monitor air quality and water quality, scientists at the Environmental Protection Agency (EPA) collect data upon a number of hydrological and atmospheric variables on a regular basis.  The scientists are then confronted with identifying changes that may have occurred in the variables being monitored.  Answers to such questions require studied application of change-point methods to the collected data.  Thus, the investigators make advances to change-point methods mainly related to the problem of estimating the time of change.  Since practical data from different sources give raise to different probability models, the developed methods are broad enough that they are applicable to a wide range of practical scenarios.   <br/>"
"0806104","Estimation for Conditionally Heteroskedastic Time Series Processes","DMS","STATISTICS, COFFES","06/01/2008","05/22/2008","Margaret Andrews","IL","Northwestern University","Standard Grant","Gabor Szekely","05/31/2011","$73,895.00","","bandrews@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1269, 7552","0000, OTHR","$0.00","Many observed time series processes are correlated and also exhibit volatility clustering. Volatility clustering is the tendency of observations relatively small in absolute value to be followed by other small observations, and the tendency of observations relatively large in absolute value to be followed by other large observations.  Autoregressive-moving average (ARMA) models, the standard linear time series models for stationary data, cannot alone be used to describe the dependence in these series because ARMA processes with independent and identically distributed errors have constant conditional variances.  Nonlinear white noise models with time-dependent conditional variances, most notably generalized autoregressive conditionally heteroskedastic(GARCH) models, are, therefore, often used to describe the ARMA errors of time series with these features.  This project addresses ARMA-GARCH model estimation.  Specifically, the investigator is researching the distribution for least squares estimators of ARMA model parameters when the white noise error process is dependent and heavy-tailed.  A rank-based technique for estimating GARCH model parameters that is both robust and efficient is also being developed.  Both standard symmetric GARCH models and asymmetric GARCH models are being considered, and model order selection procedures are being designed.<br/>       <br/><br/>Correlated processes exhibiting volatility clustering include, for example, atmospheric measurements and economic indicators.  In practice, stationary time series of this nature are often first modeled as an ARMA process using least squares and then, if the ARMA residuals exhibit conditional heteroskedasticity, a GARCH model is fit to the residuals.  The investigator is developing theory behind this standard procedure for fitting ARMA-GARCH models and developing a robust and efficient GARCH estimation technique.  The project results are, therefore, improving the efficiency of model-fitting and enhancing the understanding of corresponding parameter estimates. The overall project impact is a greater understanding of physical and economic systems."
"0805879","Statistical Methodology and Applications to Genetics, Engineering and Economics","DMS","STATISTICS","07/01/2008","05/20/2008","Tze Lai","CA","Stanford University","Standard Grant","Gabor Szekely","12/31/2011","$587,375.00","David Siegmund","lait@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","Searching a parameter set for the location of one or more signals in a noisy background arises in gene mapping, brain mapping, bioinformatics and astronomy. An important part of the solution of these problems involves the probability that a random field exceeds a high threshold. The proposed research develops a unified analytic approach to evaluate these boundary crossing probabilities, and importance sampling and sequential Monte Carlo methods to supplement the analytic approximations. A closely related area of research is estimation and forecasting problems in time series models and stochastic dynamical systems whose parameters  may change with time. Although  in practice  abrupt parameter changes typically occur very infrequently, the unknown times of their occurrence have led to prohibitive complexity of the Bayes estimators and predictors in the literature. By using parallel recursive algorithms and combining some new ideas in change-point detection with empirical Bayes methodology, the proposed research develops asymptotically efficient estimation and prediction schemes with manageable complexity.<br/>Bayesian models of parameter changes in Markovian systems are special cases of hidden Markov models. A goal of the proposed research is to develop a comprehensive theory of efficient parameter estimation, filtering and smoothing in hidden Markov models on general state spaces. Another related direction of research is econometric time series and stochastic optimization problems in financial economics.<br/><br/>Important  objectives of the proposed research  are to develop statistical methods for gene mapping, signal processing, adaptive control of engineering systems, and decision and pricing problems in financial markets. The broader implications of the research include (i) direct applications in engineering, finance, and genetics where mapping can be the first step towards better diagnostics or treatment of a disease or towards improving plant or animal stock, and (ii) training the next generation of scientists by  involving graduate students in all phases of the research.<br/>"
"0806097","Development of Model Selection for Semiparametric Models in Analysis of High-Dimensional Data","DMS","STATISTICS","09/01/2008","08/19/2008","Hua Liang","NY","University of Rochester","Standard Grant","Gabor Szekely","08/31/2011","$99,898.00","Anna Liu","hliang@gwu.edu","910 GENESEE ST","ROCHESTER","NY","146113847","5852754031","MPS","1269","0000, OTHR","$0.00","The investigator proposes a class of variable selection procedures for partially linear models with measurement errors using non concave penalized likelihood. The proposed procedures can estimate the coefficients of significant variables in the model and simultaneously exclude the insignificant ones. The sampling properties of these estimators will be established. The investigator also extends the ideas of the penalized least squares approaches to robust partially linear models with measurement errors, partially linear models for longitudinal data, and partially linear single index models. The rates of convergence of the resulting estimators and the oracle properties for the resulting estimators with proper choice of the tuning parameters will be investigated. Monte Carlo simulations are to be conducted to examine the finite sample performance of the proposed procedures.<br/><br/>The proposed models and methods are motivated by the investigator?s study of the role of HIV-1 RNA (viral load), CD4+ T cell count, and other potentially useful biomarkers in HIV/AIDS clinical trials. The results of this project will help to identify important markers and trace the disease progressing in AIDS research. The theoretic results will contribute to the advancement of the statistical theory on variable selections and semiparametric inference. <br/><br/>"
"0808859","Travel support for the 9th ISBA world meeting","DMS","STATISTICS","06/01/2008","05/01/2008","Peter Mueller","TX","University of Texas, M.D. Anderson Cancer Center","Standard Grant","Yazhen Wang","05/31/2009","$10,000.00","","pmueller@math.utexas.edu","1515 HOLCOMBE BLVD","HOUSTON","TX","770304000","7137923220","MPS","1269","0000, 7556, OTHR","$0.00","The funding is for travel support for US participants in the ""9th ISBA World Meeting on Bayesian Statistics"", to be held July 21 through 25, 2008, in Hamilton Island, Australia. The grant will pay partial travel expenses for 7 young investigators, including at least 4 women and other underrepresented minorities.  The purpose of the conference is to bring together the diverse international community of investigators in statistics and in other areas who develop and use Bayesian methods to share recent findings and to present new and challenging problems to be addressed by the community.<br/><br/>This grant enables 7 young investigators from US institutions to participate in the main international meeting for Bayesian statistics which is held every two years.  The use of Bayesian methods in academia, government and industry has been steadily increasing over the past 20 years, with many US researchers playing a leading role. This grant will help to maintain and strengthen this leading role of US scientists. The project will do so in particular by providing young investigators with important opportunities for networking and dissemination of their research results. The meeting is organized by the International Society for Bayesian Analysis (ISBA,  www.bayesian.org). The mission of ISBA is to promote the development and application of Bayesian statistical theory and methods useful in the solution of theoretical and applied problems in science, industry and government."
"0806009","Statistical Theory for Astrophysical Problems","DMS","STATISTICS","09/01/2008","08/22/2008","Christopher Genovese","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","08/31/2011","$189,624.00","Larry Wasserman","genovese@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, OTHR","$0.00","Nonparametric inference has become an essential tool for studying the cosmos.<br/>This project consists of two intertwined components: (a) development of new theoretical tools and nonparametric methodologies that are inspired by problems in astrophysics but apply more broadly, and (b) application of these tools to two important astrophysical problems, which will frame the need for and guide the development of new statistical theory.  Specifically, the investigators will focus on inference for the dark energy equation of state and on identifying filamentary structures from point process data such as that produced by galaxy surveys.  The first problem gives rise to a challenging nonlinear inverse problem and demands a nonparametric approach, given what little is known about the dark energy equation of state. The investigators will develop new theory for nonlinear inverse problems that allow for accurate estimates and sharp confidence statements about the unknown function.  These techniques will then be applied to Type Ia supernova data, possibly combined with other data sources, to make inferences about dark energy.  The second problem gives rise to challenging spatial and inference problems. Current theory in the statistical literature applies to a single filament only, and techniques in the astronomical literature are not supported by theory.  The investigators on this project will close that gap, developing theory for defining, identifying, and making inferences about the filamentary structures.  The investigators will test this technique and apply it to galaxy survey data.<br/><br/>One of the most important problems in cosmology is understanding dark energy.<br/>The relationship between observable quantities and dark energy produces a challenging nonlinear inverse problem.  With very little strong a priori information about the nature of dark energy, parametric approaches to the problem are limited and suboptimal.  And with the promise of much larger data sets in the near future, there will be need and opportunity to extract fine-scale features of the dark energy equation of state.  The investigators will develop new theory of inference for such problems, with a focus on estimation under shape constraints, sharp hypothesis testing, and accurate confidence sets.  The goal is a substantial improvement in accuracy over the current best techniques.  In particular, the investigators will focus on the problem of understanding dark energy and on identifying filamentary structures in distribution of matter.  The former is one of the central problems in modern cosmology and demands state of the art statistical techniques to get the most from the data.  The investigators will develop new statistical theory and methodologies that substantially improve the precision with which features of dark energy can be estimated from supernova data and other data sources.  The latter problem is central to understanding the distribution of matter in the universe. Current statistical theory only applies to a limited version of the problem, and current astronomical methodologies do not have strong theoretical support. The investigators will close that gap and develop a method and corresponding theory that can handle realistic versions of the problem and give optimal or near-optimal performance.<br/><br/>"
"0805965","Unified Dynamic Modeling of Event Time Data with Semiparametric Profile Estimating Functions:    Theory, Computing, and Applications","DMS","STATISTICS","07/01/2008","06/23/2008","Jun Yan","CT","University of Connecticut","Standard Grant","Gabor Szekely","06/30/2012","$150,000.00","","jun.yan@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","0000, OTHR","$0.00","Event time data under various censoring schemes arise in a variety of fields. The temporal dynamics of covariate effects on the occurrence of events can be flexibly studied in a partly functional regression model on the mean function of temporal processes defined by event times, with some covariate coefficients time-varying while others time-independent. Statistical inferences are challenging with the presence of multiple time-varying components, particularly when event times are not continuously but discretely observed, as under interval censoring. The investigator studies a unified semiparametric profile estimating function approach and an estimated version of it when the event times are only discretely observed. For continuously observed data, the model parameters are estimated from an algorithm that alternately updates the current estimate between an estimating equation for time-varying coefficients and an estimating equation for time-independent coefficients. For interval censored or doubly censored data, estimating functions will be estimated first using multiple imputation and then solved alternately. The methodology will be implemented in an R package under the quality assurance scheme of the R Project. The proposed approach provides a unified theoretical framework of dynamic regression models for event times, enabling a synthesized investigation of computing<br/>algorithms and statistical inferences. It allows efficient estimation and successive hypotheses test of covariate effects. Application of the method to a cystic fibrosis disease registry data will generate new knowledge on the temporal nature of the association between malnutrition and pulmonary disease progression.<br/><br/>The proposed method is motivated by the need of modeling temporal dynamics of covariate effects on the occurrence of certain events. Events of interests can be, for example, the recurrences of some chronic disease symptoms such as lung infection in cystic fibrosis patients, or the breakdowns of some engineering or electronic systems such as access failure of computer disks. A blend of time-varying effects and time-independent effects allows most flexibility in assessing the efficacy of a treatment of a disease or a design of new system over time. The method has an impact on the practice of event time data analysis when the temporal nature of covariate effects are of important interests. A publicly available software will get the methodology into the hands of those who will profit from using them, and, therefore, help to understand the temporal dynamics of covariate effects on event occurrences.<br/>"
"0904825","Collaborative Research: Spectral and Connectivity Analysis of Non-Stationary Spatio-Temporal Data","DMS","STATISTICS","10/01/2008","11/25/2008","Wesley Thompson","CA","University of California-San Diego","Standard Grant","Gabor Szekely","08/31/2010","$29,987.00","","wes.stat@gmail.com","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","0000, OTHR","$0.00","The focus of this research is the development of new statistical methodologies for modeling connectivity in non-stationary spatio-temporal data. The investigators will develop four specific methods and models which will be applied to data provided by t he investigator's collaborators. First, motivated by the need for more sophisticated methods to investigate complex dependencies between two time series (e.g., brain regions), the investigators will build tools for exploring non-linear and time-evolving dependence between signals using dynamic mutual information in the spectral domain. Second, the notion of spatially-varying and temporally-evolving spectrum will be made precise via a stochastic representation of non-stationary spatio-temporal processes. An asymptotic framework for consistent estimation and inference will be developed. Third, a general spectral model for connectivity in a multi-subject experiment via a latent network model will be formulated. The empirically-driven model will incorporate items such as stimulus types, exogeneous time series, and subject-specific random effects. Finally, to complement this exploratory approach for modeling spectral data and connectivity, the investigators will build a scientifically-motivated semi-parametric state-space model of effective connectivity using multi-subject data.<br/><br/>The overarching goal of this research is the development of new statistical methodologies for analyzing data that has both a time and space dimension. Spatio-temporal data are prevalent in many disciplines, including the environmental and soil sciences, meteorology and oceanography, neuroscience and the emerging fields of health and bioterrorism surveillance. The primary data source for the investigators is time-sequenced data of brain activity measured at many locations in the brain. These signals contain information on how the brain functions, how it responds to outside stimuli, and where synchronization of functionality occurs. The statistical models the investigators are developing help sift through this information, allowing for the detection of trends in brain functionality, and estimation of population- and individual-level differences in performance. The empirical nature of the models allows for data-driven confirmation and discovery of neuroscientific theory. The statistical models will also be predictive, aiding in the quest for personalized diagnosis and treatment of depression, anxiety, and other neurological conditions. While the statistical research is motivated by the investigators' ongoing collaboration with neuroscientists, there is a unified statistical theme applicable to many other areas of interest."
"0805670","Efficient Bi-Level Variable Selection in High-Dimensional Models","DMS","STATISTICS","08/01/2008","06/04/2010","Jian Huang","IA","University of Iowa","Continuing Grant","Gabor Szekely","07/31/2011","$133,597.00","","jian-huang@uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1269","0000, OTHR","$0.00","This project proposes a class of novel bi-level variable selection method<br/>in high-dimensional regression models when there is group structure in covariates. The existing variable selection methods are designed for either individual variable selection or group selection, but not for both. Furthermore, standard methods for evaluating a statistical procedure assume that the number of variables in a model is fixed and much smaller than the sample size which, in general, are not applicable for high-dimensional settings. Analysis of high-dimensional data presents novel and challenging theoretical and computational questions in statistics. The proposed methods are capable of simultaneous group and individual variable selection within selected groups. For the proposed bi-level selection methods, computational algorithms will be developed and the theoretical properties in a class of important regression models will be investigated. The proposed methods are expected to be able to correctly select the important groups and variables simultaneously with high probability in sparse models even when the number of covariates is much larger than the sample size.<br/><br/>High-dimensional data arise in many scientific fields, including biology, economics, finance, information technology, and health sciences. In all these fields, the identification of important features from data is a crucial step in the process of scientific discovery. The intended applications of the proposed study are to the analysis of high-dimensional genomic data. In particular, the proposed research will obtain novel methods for genome wide association studies and genetic pathway regression analysis. These are two of the most important approaches for understanding how genes and genetic pathways cause common and complex diseases such as various types of cancers.  The proposed research aims to translate novel statistical approaches into new methodologies for analyzing high-dimensional genomic data."
"0804324","Statistical Theory and Methodology","DMS","STATISTICS","07/01/2008","06/09/2008","Bradley Efron","CA","Stanford University","Standard Grant","Gabor Szekely","12/31/2011","$494,214.00","Persi Diaconis","brad@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","The investigators consider a class of problems in probability and statistical inference arising from large-scale scientific investigations.  Massive hypothesis testing situations, with tens or hundreds of thousands of cases to consider simultaneously, are treated using empirical Bayes methods, as an efficient compromise between frequentist and Bayesian analyses.  In a microarray experiment, for example, an empirical Bayes False Discovery Rate approach helps sift through thousands of z-values when searching for those genes of significant interest. Special attention if given to the appropriate choice of the null hypothesis, which may be quite different than classical text-book recipes.  Markov chain monte carlo tech- niques are analyzed using eigen decompositions of the transition matrix to quantify how quickly initial conditions dissipate as the sampling algorithm proceeds toward steady state. Advanced applications involve implementing random sampling over curved manifolds in high-dimensional spaces.<br/><br/>The general theme of this project is the development of mathematical methods of genuine practical utility, both in probability and statistical inference. Classical statistical methods were developed in a scientific world of small data sets investigating individual questions. Modern scientific technology, like microarrays, fMRI devices, and satellite imagers, now produce enormous data sets that aim to simultaneously investigate thousands of possibilities. This project proposes methods for dealing with the new scientific environment. New techniques of statistical inference and probabilistic modelling are proposed, based on recent experience in the biological, social science, and physical science worlds. <br/><br/>"
"0805890","Travel support for MCQMC July 2008, Montreal, Canada","DMS","STATISTICS","07/01/2008","07/30/2009","Art Owen","CA","Stanford University","Standard Grant","Gabor Szekely","06/30/2010","$15,000.00","","owen@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, 7556, OTHR","$0.00","<br/>This project funds the travel expenses of US based researchers to the eighth International conference on Monte Carlo and quasi-Monte Carlo methods, to be held in Montreal Canada in July 2008. The meeting will consider advance methods for psuedo-random number generation, construction of low discrepancy point sets, complexity of algorithms.There will be tutorial sessions on the role of quasi-Monte Carlo sampling in statistics, finance, and computer graphics. The talks will cover these topics and others in the physical sciences.<br/><br/>The Monte Carlo (MC) method is a computer based simulation using random number generators. The name was given by atomic researchers in the 1940s who likened their simulation methods to keeping score in a casino in order to learn some odds. Monte Carlo methods are used in every branch of science and engineering because they allow brute force computer power to be used on problems that are too complicated to solve mathematically. Quasi-Monte Carlo (QMC) methods replace simulated random numbers by strategically chosen ones. By leaving less to chance, large improvements in accuracy are possible. MC and QMC methods are widely applied in computer graphics to make animated movies and other images, in computational finance to control risks, in statistical inference to separate real findings from chance fluctuations, and in many other areas. Much of the top QMC work is done in Europe, Asia, Canada and Australia. This conference will bring together leading researchers from around the world to share results. This project will support travel expenses of US based researchers to participate in this exchange of knowledge. Two of the researchers are US experts giving major talks. The majority of the funds is reserved for young US researchers in the mathematical sciences, including PhD students, postdocs, and junior professors."
"0824297","A NISS/ASA Writing Workshop for New Researchers","DMS","STATISTICS","06/01/2008","05/02/2008","Keith Crank","VA","American Statistical Association","Standard Grant","Gabor Szekely","05/31/2009","$20,000.00","Nell Sedransk","kcrank@gmu.edu","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","0000, 7556, OTHR","$0.00","The American Statistical Association and the National Institute for Statistical Science propose to run a workshop to help junior researchers write better journal articles and grant proposals. Journal editors will work individually with the participants to improve the quality of their publications."
"0757527","FRG: Collaborative Research:  Prediction and Risk of Extreme Events Utilizing Mathematical Computer Models of Geophysical Processes","DMS","STATISTICS","07/01/2008","04/26/2010","Maria-Jesus Bayarri","NC","National Institute of Statistical Sciences","Continuing Grant","Gabor Szekely","06/30/2012","$219,468.00","Elaine Spiller","susie@samsi.info","19 TW ALEXANDER DR","RESEARCH TRIANGLE PARK","NC","277090152","9196859300","MPS","1269","0000, 1616, OTHR","$0.00","The goal of this project is to develop the mathematical, statistical and computational tools needed to assess and predict the risk associated with geophysical hazards such as volcanic pyroclastic flows.  Based on a preliminary data analysis, the investigators develop stochastic models beginning with stationary independent increment processes employing (possibly tapered) Pareto distributions for the volumes of pyroclastic flows exceeding some observational threshold, in the domain of attraction of an alpha-stable process governing the aggregate flow volume of multiple smaller eruptions.  Von Mises distributions are used for flow initiation angles.  The deterministic TITAN2D two-dimensional computational environment is employed, which uses available digital elevation maps to predict the impact at various sites of interest from flows of specified volume and initiation angles.  TITAN2D is a depth-averaged, thin-layer computational fluid dynamics code based on an adaptive grid Godunov solver, suitable for simulating geophysical mass flows.  A rapid emulator based on a simple Gaussian random-field approximation to the TITAN2D model enables the investigators to emulate hundreds of thousands of TITAN2D runs and construct an estimate of the set of possible flow volumes and initiation angles that would lead to significant impact; a hierarchical Bayesian statistical model then reflects the probability of such an impact over a specified period of time.<br/><br/>Recent advances in computing power and algorithms have led to the application of mathematical and computer modeling to such highly complex phenomena as storms, floods, earthquakes and volcanic eruptions.  It is increasingly being understood that development of mathematical models of these phenomena is only one part of a much more complex process needed for making reliable estimates and predictions of risk.  This project develops the mathematical, statistical and computational tools needed for assessing and predicting the risk associated with such natural hazards.  A particular focus of the work is the study of how these risks vary in space and time, and of how uncertain they are.  This methodology is developed in context of the specific problem of volcanic avalanches and pyroclastic flows (so-called geophysical mass flows), but much of it will be applicable more broadly to problems in the analysis and quantification of risk in problems featuring spatial variability and model uncertainty.  It brings together a unique team of scientists with specialties including volcanology, to guide the development of realistic models for the geophysical processes under study; in stochastic processes, to reflect uncertainty and variability about initial conditions, flow frequencies, and other features in realistic and verifiable ways; in deterministic computer modeling, for the difficult task of making detailed spatial predictions of the consequences of the most probable and of the most hazardous possible events; in computer model emulation, to accelerate many thousand-fold the computations necessary for predicting the risk of rare events under a wide range of possible scenarios; and in statistical modeling and analysis, to reflect honestly all the different sources of uncertainty and variability in this analysis, leading to a full quantification of the risk of hazardous events.  Only with such a broad range of expertise can investigators build the tapestry of science that is required to develop tools for studying devastating natural hazards.<br/>"
"0908613","CAREER: Inferences on Large-Scale Multiple Comparisons:  The Temptation of the Fourier Kingdom","DMS","STATISTICS","09/05/2008","05/09/2011","Jiashun Jin","PA","Carnegie-Mellon University","Continuing Grant","Gabor Szekely","06/30/2012","$342,188.00","","jiashun@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, 1045, OTHR","$0.00","This research project is to create new tools for large-scale multiple comparisons.   In particular, the investigator develops new tools in the frequency domain to tackle problems in this field. The project includes (a). Introduce Fourier analysis as a tool for multiple comparisons. The investigator devotes to push the boundary of the field by harnessing the power of Fourier analysis. The Fourier analysis has been repeatedly proven to be a powerful tool in many scientific areas, but has seldom been used in the field of large-scale multiple comparisons. (b). Develop practically feasible tools, and lay out theoretic frameworks for studying the optimality of the tools. (c). Extend and apply  the developed methodology and theory to the analysis of massive data  generated in  various scientific fields,  including comparative genomic hybridization (CGH),   cosmology and astronomy, and gene microarray.  <br/><br/>Modern data acquisition routinely produces massive data sets in many scientific areas, e.g. genomics, astronomy, functional Magnetic Resonance Imaging (fMRI), and image processing.  The vision is that advances in massive data analysis will enable scientist from various fields to quickly extract the information they need, and at the same time,   benefit the statistical discipline both with a broader scope of theory and methodology but also with a deeper understanding of nature and science. The project pushes the boundary of the field by introducing new ideas for problem solving, developing new tools and novel theory, and applying the tools to other scientific fields including but not limited to comparative genomic hybridization (CGH), cosmology and astronomy, and gene microarray. <br/>  <br/>"
"0805748","""Nonparametric Regression Methods For Nonlinear Time Series Models""","DMS","STATISTICS","06/01/2008","04/22/2010","Michael Levine","IN","Purdue University","Continuing Grant","Gabor Szekely","05/31/2012","$100,000.00","","mlevins@stat.purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1269","0000, OTHR","$0.00","The aim of this project is to develop the methodology for estimation and testing of the nonlinear time series models involving a large number of predictor variables. The key idea is to adapt a number of techniques that are used in the nonparametric regression theory and that can be, after suitable justification, transferred to the time series setting. In addition to the above, the decision-theoretic properties of the resulting estimators are established for the first time. This is achieved by establishing asymptotic equivalence results between the nonparametric regression and various nonlinear time series models. <br/><br/>The models studied in this project are very commonly encountered in different areas of application. Many of these are the areas of federal strategic interest. As an example, one can mention forecasting the levels of future flooding and predicting the volume of production in many areas of industry. Another very important area of application is building the models that explain long-term changes in sea surface temperature and, by doing so, help explain and predict the future changes in the global climate. All of the above models often include a lot of predictor variables and this makes the choice of the model needed very difficult in practice. Based on the methods and tests proposed in this project, efficient model selection procedures can be enacted that can greatly help in choosing the right model. After the right model is chosen, high quality forecasts can be obtained that are not only of interest for researchers in science, but that also benefit society as a whole. <br/>"
"0805632","Asymptotic Methods in Quantum Statistics","DMS","STATISTICS, COFFES","07/15/2008","05/09/2010","Michael Nussbaum","NY","Cornell University","Continuing Grant","Gabor Szekely","06/30/2011","$240,000.00","","nussbaum@math.cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269, 7552","0000, OTHR","$0.00","The processing of quantum information is emerging as a challenging new field for statisticians. While the concept of inherent randomness is central to quantum mechanics, it cannot be described in terms of traditional probability alone, i.e. notions such as observed random variables, sample spaces etc. are not sufficient. On the simplest level, finite probability laws have to be replaced by states, which are defined as complex positive definite Hermitian matrices of trace one. A very general framework is provided by the theory of operator algebras. In quantum statistical decision theory, families of states generalize families of probability measures (statistical experiments), and the tensor product of many  states replaces the classical simple random sample. One problem which is already known in the classical context is the risk asymptotics for symmetric hypothesis testing, or Bayesian discrimination of two states with equal prior weights (the Chernoff bound on the exponential rate of decay of the error probability). In the recent solution of this problem on the quantum level by the investigator and coauthors, a new method has been developed to reduce the quantum risk to a classical one, via associating a pair of probability distributions to a pair of states by measurement on a purification. The present project aims at exploring further this new method, with regard to wider applicability in quantum testing, estimation and possibly in approximation of quantum statistical experiments. Further subjects of study are quantum statistical applications of information theoretic concepts like channel capacity, Kolmogorov complexity, and rate distortion.<br/><br/>Natural phenomena on the very small (subatomic) level are governed by quantum theory, where physical laws and cause-effect relationships are thoroughly different from the world of ordinary human experience. Physicists and computer scientists realized some time ago that these phenomena might be harnessed to build computers of extraordinary speed, and also allow rapid advances in cryptography such as breaking all presently known secret codes or constructing new unbreakable ones. While quantum computers have long remained an abstract idea and have only been built at an embryonic stage so far, the theoretical groundwork for far-reaching applications is already being laid in the interdisciplinary field of Quantum Information Theory. In the need for finding benchmarks for optimal performance, researchers in this field have recently begun to exploit some well  developed theories of signal processing and statistics. The present project is situated precisely at this new frontier between traditional statistics and quantum theory. The aim is to achieve a better mathematical and statistical understanding of quantum computing and communication, areas which promise to be of great technological impact once they reach an applied stage. <br/><br/>"
"0804165","Omnibus and change point tests for functional time series","DMS","STATISTICS","06/01/2008","03/04/2010","Piotr Kokoszka","UT","Utah State University","Continuing Grant","Gabor Szekely","05/31/2011","$129,982.00","","Piotr.Kokoszka@colostate.edu","1000 OLD MAIN HILL","LOGAN","UT","843221000","4357971226","MPS","1269","0000, OTHR","$0.00","A functional time series consists of a collection of curves or surfaces, rather than scalars or vectors, recorded sequentially over time or space. Functional observations are obtained from high resolution measurements (physics, engineering, finance) or from smoothing irregularly spaced observations (biology, psychometrics, environmental science). The last decade has seen the emergence of a number of models for such data.  Unlike for scalar or vector time series, no systematic methodology is available to verify if a given model is appropriate for the data, or for choosing among several competing models by using a measures of fit.  No methodology to check if a single stochastic structure can be assumed for the whole functional record is available either.  The proposed research focuses on two types of tests: 1) omnibus tests intended to detect departures in any direction from a specified model, and 2) change point tests designed to detect a model change at some unknown time.  Change point tests are particularly important for time series, as ""conditions"" may change with time, and assuming one model for the whole realization may lead to very misleading inference.  Practical implementations is based on solid theoretical understanding which requires overcoming challenges not encountered in modeling scalar time series.  While many approaches seem intuitively appealing, those that are feasible and optimal are focused on, and nontrivial details are worked out.  A tool box of tests and comprehensive methodology validated by theory, simulations and a number of applications  is developed.<br/><br/>Recent advances in measurement and data storage technology have led to the emergence of functional time series in many fields of science and engineering. A functional time series consists of a collection of curves or surfaces, rather than numbers.  For example, rather than looking at a closing daily value of an economic indicator index like Dow Jones or NASDAQ, in times of uncertainty and high volatility, regulators and market participants focus on the intra-day evolution of the curve which shows how an index changes from minute to minute. Understanding how typical daily index curves look like, how much can be explained by regular variability, and what is unusual and requires action are important practical questions. Functional time series appear in many other fields, most notably in physics, engineering, biology, medicine and environmental science. In the latter, daily curves showing the concentration of a pollutant every 15 minutes are much more informative than a maximum or average daily values, which may not be useful in assessing the actual risk to the public. The research develops statistical procedures for detecting departures from a ususal pattern of curves. Special emphasis is placed on detecting a sudden change in these patterns. The methods are automated to a large degree and facilitate decision making."
"0757549","FRG: Collaborative Research:  Prediction and Risk of Extreme Events Utilizing Mathematical Computer Models of Geophysical Processes","DMS","STATISTICS","07/01/2008","04/21/2010","Robert Wolpert","NC","Duke University","Continuing Grant","Gabor Szekely","06/30/2012","$479,724.00","James Berger","rlw@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, 1616, OTHR","$0.00","The goal of this project is to develop the mathematical, statistical and computational tools needed to assess and predict the risk associated with geophysical hazards such as volcanic pyroclastic flows.  Based on a preliminary data analysis, the investigators develop stochastic models beginning with stationary independent increment processes employing (possibly tapered) Pareto distributions for the volumes of pyroclastic flows exceeding some observational threshold, in the domain of attraction of an alpha-stable process governing the aggregate flow volume of multiple smaller eruptions.  Von Mises distributions are used for flow initiation angles.  The deterministic TITAN2D two-dimensional computational environment is employed, which uses available digital elevation maps to predict the impact at various sites of interest from flows of specified volume and initiation angles.  TITAN2D is a depth-averaged, thin-layer computational fluid dynamics code based on an adaptive grid Godunov solver, suitable for simulating geophysical mass flows.  A rapid emulator based on a simple Gaussian random-field approximation to the TITAN2D model enables the investigators to emulate hundreds of thousands of TITAN2D runs and construct an estimate of the set of possible flow volumes and initiation angles that would lead to significant impact; a hierarchical Bayesian statistical model then reflects the probability of such an impact over a specified period of time.<br/><br/>Recent advances in computing power and algorithms have led to the application of mathematical and computer modeling to such highly complex phenomena as storms, floods, earthquakes and volcanic eruptions.  It is increasingly being understood that development of mathematical models of these phenomena is only one part of a much more complex process needed for making reliable estimates and predictions of risk.  This project develops the mathematical, statistical and computational tools needed for assessing and predicting the risk associated with such natural hazards.  A particular focus of the work is the study of how these risks vary in space and time, and of how uncertain they are.  This methodology is developed in context of the specific problem of volcanic avalanches and pyroclastic flows (so-called geophysical mass flows), but much of it will be applicable more broadly to problems in the analysis and quantification of risk in problems featuring spatial variability and model uncertainty.  It brings together a unique team of scientists with specialties including volcanology, to guide the development of realistic models for the geophysical processes under study; in stochastic processes, to reflect uncertainty and variability about initial conditions, flow frequencies, and other features in realistic and verifiable ways; in deterministic computer modeling, for the difficult task of making detailed spatial predictions of the consequences of the most probable and of the most hazardous possible events; in computer model emulation, to accelerate many thousand-fold the computations necessary for predicting the risk of rare events under a wide range of possible scenarios; and in statistical modeling and analysis, to reflect honestly all the different sources of uncertainty and variability in this analysis, leading to a full quantification of the risk of hazardous events.  Only with such a broad range of expertise can investigators build the tapestry of science that is required to develop tools for studying devastating natural hazards.<br/>"
"0751511","Request for Student Travel Support to the Isaac Newton Institute Workshops on Statistical Methods for Complex, High-Dimensional Data","DMS","STATISTICS","01/01/2008","11/07/2007","David Banks","NC","Duke University","Standard Grant","Gabor Szekely","12/31/2008","$15,000.00","","banks@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, 7556, OTHR","$0.00","The Isaac Newton Institute at the University of Cambridge has organized a six-month research program on ""Statistical Methods for Complex, High-Dimensional Data""  that starts in January, 2008.   This program <br/>will bring together some of the world's foremost researchers on critical problems related to manifold learning, ""large p, small n"", model capacity,  PAC bounds, overcomplete bases, kernel methods, and related topics.  The investigator will  use the NSF funds to ensure that ten of the best  U.S. graduate students working in this area have the opportunity to travel to Cambridge and to participate in one of the three workshops that anchor the research program.<br/><br/>The Isaac Newton Institute is the major center for mathematical research in Europe.  For 2008, it has organized a program that addresses the major challenge problem confronting statistics in this decade:  how to analyze highly multivariate and complex data sets.  Progress on this problem would significantly improve the understanding of genetic data from microarray experiments, the analysis of atmospheric data from distributed sensor systems, complex financial models, proteomics, text mining, medical imaging, and many other scientific domains.  The NSF funds in this award will be used to support travel by 10 of the top U.S. <br/>graduate students working in these problem areas to attend one of the the three workshops that have been organized as part of the research program.<br/><br/>    <br/><br/>"
"0804858","Collaborative Research: Spectral and Connectivity Analysis of Non-Stationary Spatio-Temporal Data","DMS","STATISTICS","09/01/2008","09/03/2008","Wesley Thompson","PA","University of Pittsburgh","Standard Grant","Yazhen Wang","12/31/2008","$29,987.00","","wes.stat@gmail.com","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","0000, OTHR","$0.00","The focus of this research is the development of new statistical methodologies for modeling connectivity in non-stationary spatio-temporal data. The investigators will develop four specific methods and models which will be applied to data provided by t he investigator's collaborators. First, motivated by the need for more sophisticated methods to investigate complex dependencies between two time series (e.g., brain regions), the investigators will build tools for exploring non-linear and time-evolving dependence between signals using dynamic mutual information in the spectral domain. Second, the notion of spatially-varying and temporally-evolving spectrum will be made precise via a stochastic representation of non-stationary spatio-temporal processes. An asymptotic framework for consistent estimation and inference will be developed. Third, a general spectral model for connectivity in a multi-subject experiment via a latent network model will be formulated. The empirically-driven model will incorporate items such as stimulus types, exogeneous time series, and subject-specific random effects. Finally, to complement this exploratory approach for modeling spectral data and connectivity, the investigators will build a scientifically-motivated semi-parametric state-space model of effective connectivity using multi-subject data.<br/><br/>The overarching goal of this research is the development of new statistical methodologies for analyzing data that has both a time and space dimension. Spatio-temporal data are prevalent in many disciplines, including the environmental and soil sciences, meteorology and oceanography, neuroscience and the emerging fields of health and bioterrorism surveillance. The primary data source for the investigators is time-sequenced data of brain activity measured at many locations in the brain. These signals contain information on how the brain functions, how it responds to outside stimuli, and where synchronization of functionality occurs. The statistical models the investigators are developing help sift through this information, allowing for the detection of trends in brain functionality, and estimation of population- and individual-level differences in performance. The empirical nature of the models allows for data-driven confirmation and discovery of neuroscientific theory. The statistical models will also be predictive, aiding in the quest for personalized diagnosis and treatment of depression, anxiety, and other neurological conditions. While the statistical research is motivated by the investigators' ongoing collaboration with neuroscientists, there is a unified statistical theme applicable to many other areas of interest."
"0805050","Statistical Methods for Dependent Data","DMS","STATISTICS","07/01/2008","04/09/2012","David Stoffer","PA","University of Pittsburgh","Continuing Grant","Gabor Szekely","06/30/2014","$319,999.00","Robert Krafty","stoffer@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","0000, OTHR","$0.00","This proposal concentrates on various topics relating to the statistical analysis of dependent data. The first project extends the spectral envelope concept for analyzing DNA sequences. A common problem in analyzing long DNA sequence data is in identifying protein-coding sequences that are dispersed throughout the sequence and separated by regions of noncoding. DNA sequences are heterogeneous, so it is necessary to expand the methodology to capture the local behavior of such sequences. To address the problem of local behavior, a local spectral envelope with estimation via mixtures of smoothing splines will be explored. It is the hope that this methodology will help emphasize any periodic feature that exists in a categorical sequence of virtually any length in a quick and automated fashion. Projects such as the human genome project have produced large amounts of data and the methods established in this project will prove to be useful in the analysis of the vast quantities of data being produced by various genome projects. In another project, the focus is on the analysis of longitudinal data and the development of a practical nonparametric procedure for the estimation of the within-subject correlation structure. This technique is used to develop a data driven functional principal components analysis procedure (FPCA). Because longitudinal data often possess the property that observations made within a subject are correlated, an effective analysis of these data is required to account for this within-subject correlation. When a parametric form for the covariance structure is unknown, using a misspecified structure can result in biased and inefficient estimates. This project focuses on the analysis of longitudinal data that can be modeled as observations from smooth subject trajectories that are realizations of a stochastic process observed at discrete time points with noise. The high dimensionality and complexity of longitudinal data has made FPCA a popular tool for data reduction and visualization by capturing the primary modes of variation of the stochastic process generating the data. Scientists are often interested in using longitudinal data to determine the effect that a set of possibly time-varying covariates have on a given response over time. Functional linear models, and in particular the varying-coefficient model, provide a framework for analyzing such data. In many of these data sets, the functional coefficients have shapes that cannot be modeled parametrically. An effective analysis of these data is required to both account for the within-subject correlation and to allow for the flexible shapes of the coefficients. Because a parametric form for the within-subject covariance is not always known, a third project focuses on creating an iterative data-driven spline based procedure for fitting varying-coefficient models. <br/><br/>This proposal concentrates on solving problems involved in the analysis of dependent data. The first project will develop a method for detecting genes in a long DNA sequences. Projects such as the human genome project have produced large amounts of data and the methods established in this project will prove to be useful in the analysis of the vast quantities of data being produced by various genome projects. A second proposed project focuses on the analysis of complex data collected over time. This project is also motivated by the analysis of DNA, and in particular, the analysis of gene expression data. In a third project, the investigators will focus on a technique called functional linear models. For example, techniques will be developed for studying the effect that a growth factor should have on the decision to supplement chemotherapy with antiangiogenic therapy when treating ovarian cancer."
"0805786","Nonparametric Outlyingness and Descriptive Measures in Multivariate and General Data Settings","DMS","STATISTICS","08/01/2008","08/02/2009","Robert Serfling","TX","University of Texas at Dallas","Standard Grant","Gabor Szekely","08/31/2011","$107,438.00","","serfling@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1269","0000, OTHR","$0.00","This project extends foundations in two closely interactive areas of core statistical science: nonparametric outlier identification, and robust descriptive measures. Multivariate and more complex data types are emphasized. Data points apart from the main body (""outliers"") can adversely affect statistical analyses unless identified and taken into account. This concern is arising in new contexts calling for updated and broadened formulations of current methods. Multivariate modeling with heavy tailed data and with skewness and kurtosis descriptive measures in addition to location and skewness involves increased concern with outliers. Diverse new data types (functional, shape, image, set, symbolic, sensor, stream, tree, graph, etc.) being treated by sophisticated but ad hoc computer science data mining approaches need more systematic treatment. Shape-fitting problems in computational geometry impose new forms of outlier issues. The study develops new general foundational approaches to outlier detection, eliminates reliance on algorithms that only handle outliers without actually identifying them in the input space, eliminates undue reliance upon elliptical outlyingness contours, and strengthens the accommodation of heavy tailed data. The overall project goals are to establish extended conceptual statistical foundations for outlier detection and to develop new structures for robust descriptive measures of location, dispersion, skewness, kurtosis, etc., with the aim of broad application across general data settings.<br/><br/>With advancing computational resources, the scope of statistical data <br/>analysis and modeling is widening to accommodate pressing new arenas of application. Data in all areas of science and engineering has complex multidimensional structure, typically with large sample sizes and involving curves, images, text, and other objects, often within astream or network structure. This is generating major new problems in detection and handling of ""anomalous"" data points (""outliers""). Which cases stand apart? How do the ""unusual"" cases impact statistical analyses on the full data set? What computational steps efficiently find the outliers when the data is massive and involves many variables? What general principles apply across diverse new situations such as fraud detection, intrusion detection, network analysis, and data mining? How to define ""outlier"" relative to a fusion of several related data sets, for example image, text, and sensor data, as might arise in Homeland Security? This study addresses these basic practical questions with the aim of developing new methodological approaches soundly based upon established statistical principles."
"0748409","CAREER:  Optimal Design of Experiments for Generalized Linear Models","DMS","STATISTICS","06/01/2008","04/09/2012","Min Yang","MO","University of Missouri-Columbia","Continuing Grant","Gabor Szekely","05/31/2013","$400,000.00","","myang2@uic.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269","0000, 1045, 1187, OTHR","$0.00","In this work, the PI is to develop a novel approach for identifying optimal and efficient designs under Generalized Linear Models (GLMs) including the fundamental logistic, probit and loglinear models and some other nonlinear models. Specifically, this project intends to achieve the following three objectives: (i) Identify optimal designs for GLMs under non-homogeneous subjects. In this study, optimal designs are derived for models in which the subjects are divided into two or more groups using one or more factors, allowing the intercept or slope to vary from one group to another. Random subject effects can also be allowed for differences among subjects within groups. (ii) Identify optimal designs for GLMs with multiple covariates. There are very few optimality results for GLMs with more than one covariate. The PI will study optimal designs for GLMs when multiple covariates exist. These models can also account for subject heterogeneity. (iii) Identify optimal designs for some other nonlinear models. In nonlinear models, most optimal results were derived under D-optimality for all parameters. The PI will investigate a general approach to identify optimal designs for nonlinear models with three or more parameters under commonly used optimality criteria when all or some of the parameters are of interest. The proposed research will have a tremendous impact because it will fill several gaps in the literature: the models in the proposed research accommodate heterogeneity among subjects and multiple covariates; general solutions for optimal designs of nonlinear models with three parameters will be provided. The technique in the proposed research is innovative in that it yields very general results that go beyond solving problems on a case-by-case basis. It helps to identify the support of locally optimal designs for many of the commonly studied models and can be applied for all the common optimality criteria based on information matrices. It works both with a constrained and unconstrained design region. Furthermore, it can be applied to multistage experiments, where an initial experiment may be used to provide a better idea of the unknown parameters.<br/> <br/>GLMs and other nonlinear models have been used in a wide range of social and natural science fields, such as biological sciences, pharmaceutical research, agricultural science, economics, marketing, etc. The results of this study will have a deep impact on the application of GLMs in these fields. For example, when the findings are applied to the design of clinical trials during new drug discovery and development, they will significantly reduce the time, money, and number of patients needed in these trials. In fact, this research can help the U.S. Food and Drug Administration to improve its guidelines for clinical trials. To effectively disseminate the results of this research, the PI will develop a user-friendly software package targeting non-expert users. To successfully integrate research and education, the PI will develop advanced experimental design courses at the University of Missouri-Columbia incorporating findings of this project. Graduate students will be trained to study optimal designs in the new fields, under the PI?s guidance. Finally, the proposed research has the potential to stimulate new research and to provide tools for identifying optimal designs under GLMs or nonlinear models used in other areas, such as longitudinal data analysis and survival analysis.<br/> <br/><br/>"
"0805977","Collaborative Research:    Nonparametric Theory on Manifolds of Shapes and Images, with Applications to Biology, Medical Imaging and Machine Vision.","DMS","STATISTICS","07/01/2008","12/20/2012","Victor Patrangenaru","FL","Florida State University","Standard Grant","Gabor Szekely","03/31/2013","$132,000.00","","vic@stat.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","0000, OTHR","$0.00","Much of the focus of this collaborative project is on the analysis of landmark based shapes in which a k-ad, i.e., a set of k points or landmarks on an object or a scene are observed in 2-D or 3-D, usually with expert help, for purposes of identification, discrimination, or diagnostics. Depending on the way the data are collected or recorded, the appropriate shape of an object is the maximal invariant specified by the space of orbits under a group G of transformations. In particular, Kendall's shape spaces of k-ads are invariant under scaling and Euclidean rigid motions. While this is a proper choice for many problems in biology and medical imaging, other notions of shape such as affine shape and projective shape are important in machine vision and bioinformatics. All these spaces are differentiable manifolds, often with natural Riemannian structures for measuring lengths and angles. The statistical analysis based on Riemannian structures is said to be intrinsic. In other cases, proper distances are sought via an equivariant embedding of the manifold M in a vector space E. Corresponding statistical analysis is called extrinsic. Finding proper Riemannian structures and equivariant embeddings is one of the objectives of this project, which is crucial for the statistical inference proposed. Establishing broad conditions for the existence of the Frechet mean, as the unique minimizer of the Frechet function the expected squared distance from a Q-distributed random shape is important for statistical inference; and it is a goal of the project to pursue, especially for intrinsic analysis where it has remained an outstanding open problem from the inception of shape theory. Reconstruction of a scene from two (or<br/>more) aerial photographs taken from a plane is one of the research problems in affine shape analysis. Potential applications of projective shape analysis proposed here include face recognition and robotics-for robots to visually recognize a scene.<br/><br/>Statistical analysis of data on geometric objects, or manifolds, is an exciting and challenging field of research, where statistical theory and differential geometry are inextricably intertwined, and implementation requires innovative algorithms and high speed computation. The project proposed here deals with the development of nonparametric methodology in this context, which must also resolve associated geometric issues and problems of implementation. Past progress in this field by the PIs has laid the foundation for the present project. The statistical analysis proposed has wide ranging applications, especially in biology and bioinformatics, health sciences, and machine vision. Under this project, the PIs plan to train both undergraduate and graduate students, as well as at least one postdoctoral fellow, in theory and in its practical implementation. This continues much further the present activities of the PIs in this regard. In addition, computational algorithms and codes are made available on websites to create and disseminate this research and its applications.<br/>"
"0805031","Reliability Inference and Degradation Modeling based on a Class of Nonhomogeneous Levy Processes","DMS","STATISTICS","06/01/2008","05/20/2008","Xiao Wang","MD","University of Maryland Baltimore County","Standard Grant","Gabor Szekely","05/31/2010","$59,474.00","","wangxiao@purdue.edu","1000 HILLTOP CIR","BALTIMORE","MD","212500001","4104553140","MPS","1269","0000, OTHR","$0.00","This proposal aims to study semiparametric likelihood inference of some nonhomogeneous Levy processes for degradation data. In some studies where the subjects are put on test at time zero, these subjects degrade over time. Failure is defined as the time when the amount of degradation reaches a pre-specified critical level. Data of this type are called degradation data. The setting for observed data is one on which n independent units, each with a nonhomogeneous Levy process with common shape function and scale parameter, are observed at several possibly different times during the study. The difficulty is that unknown parameters of these processes include a monotone function and a finite dimensional parameter and also the data themselves are correlated. The investigator studies the maximum likelihood estimator (MLE) and maximum pseudo-likelihood estimator (MPLE) of the unknown parameters and develops efficient algorithms to compute both the MLE and the MPLE. Asymptotic properties of these estimators including consistency, convergence rate and asymptotic distribution are established. Related problems, including semiparametric inference of models with random effects and/or time-independent covariates, joint modeling of failure time data and degradation data, and a variation of the Neyman-Scott problem on variance estimation are also investigated.<br/><br/>Traditional analysis in reliability focuses on collecting and modeling failure time data. This poses difficulties in high-reliability applications where there are few failures. Advances in sensing technologies are making it possible to collect extensive amount of data on degradation associated with systems and components. The degradation modeling allows the manufactures to obtain the reliability information required in a timely manner such that they can make effective business decisions regarding warranty periods or demonstrate that the product meets the customer's reliability specifications. The proposed work develops a class of flexible models to analyze the degradation data and has direct applications in industrial engineering and AIDS patients' immune system study, as it formulates real degradation data. The investigator develops numerical software in the forms of R and/or MATLAB for degradation analysis for public use through the internet."
"0747575","CAREER: Flexible Statistical Learning for Complex Data","DMS","STATISTICS","06/01/2008","05/21/2012","Yufeng Liu","NC","University of North Carolina at Chapel Hill","Continuing Grant","Gabor Szekely","05/31/2014","$400,000.00","","yfliu@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","0000, 1045, 1187, OTHR","$0.00","Statistical learning is widely recognized as a very active area of interdisciplinary research, which lives between statistics, computer science, and optimization. This research offers a host of new statistical learning techniques for solving complicated learning problems, especially for high dimensional and noisy data. In particular, the investigator develops (1). several novel large-margin classifiers which are expected to yield highly competitive classification accuracy, class probability estimation, as well as variable selection; (2). a new regularization approach to estimate the covariance matrix for a class of nonstationary spatial autoregressive model; (3). a novel technique to assess statistical significance of clustering for high dimensional data.<br/><br/>With the rapid advance of technology, massive and complex data are being generated across many different scientific fields. Analyzing such data becomes more and more challenging. A major goal of this research is to provide a set of flexible statistical learning techniques. These tools should have beneficial impact on cancer research, medical imaging, microarray data analysis and spatial-temporal modeling. The investigator collaborates with a number of scientists in various fields outside of statistics such as biology, computer science, pharmacy, and genetics. The new developments allow scientists to analyze complex data with high prediction accuracy and increased interpretability. Efficient algorithms and software are developed for public use. The integration of the research goals with educational activities aims to help students at graduate, undergraduate, and high school levels and researchers from various disciplines to acquire state-of-the-art statistical learning methods and tools.<br/><br/>"
"0810224","Workshop on Statistical Analysis of Neuronal Data (SANDS) - Participant Support, Pittsburgh, PA, May 2008","DMS","STATISTICS, Robust Intelligence","05/15/2008","05/20/2008","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Yazhen Wang","04/30/2009","$10,000.00","","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 7495","0000, 7556, OTHR","$0.00","Statistical Analysis of Neuronal Data (SAND4) is a workshop devoted to defining important problems in neuronal data analysis and useful strategies for attacking them. The conference will display novel statistical methods in the context of substantive neuroscientific research, and will highlight problems that require new statistical approaches. Throughout the conference an attempt will be made to discuss the interplay of statistical theory and practice. There will be 5 scientific sessions, at which 9 senior investigators and 16 junior investigators will speak. There will also be a poster session. A special issue of the Journal of Computational Neuroscience is planned for publication of selected papers from the conference.<br/><br/>Statistical Analysis of Neuronal Data (SAND4) is the fourth international workshop in the series that began in 2002. The workshops are held in even years at Carnegie Mellon University during the Spring. The fourth workshop will occur May 29-31, 2008. SAND4 will bring together neurophysiologists, statisticians, physicists, computer scientists, and engineers who are interested in quantitative analysis of neuronal data. This workshop series aims to foster communication between experimental neuroscientists and those trained in statistical and computational methods; and to provide further dissemination of the findings presented at the workshop via a set of peer-reviewed articles. Secondary objectives are to encourage young researchers, including graduate students, to present their work; expose young researchers to important challenges and opportunities in this interdisciplinary domain, while providing a small meeting atmosphere to facilitate the interaction of young researchers with senior colleagues; and include as participants women, under-represented minorities and persons with disabilities who might benefi from the small workshop environment."
"0805829","Development of Complex Stochastic Models of Carcinogenesis","DMS","STATISTICS","09/01/2008","08/19/2008","Wai-Yuan Tan","TN","University of Memphis","Standard Grant","Gabor Szekely","08/31/2009","$50,000.00","King-Thom Chung, Lih-Yuan Deng","waitan@memphis.edu","101 WILDER TOWER","MEMPHIS","TN","381523520","9016783251","MPS","1269","0000, 9150, OTHR","$0.00","The investigators propose to develop new innovative approaches for<br/>modeling and analyzing carcinogenesis models involving multiple<br/>pathways with each pathway being a stochastic multi-stage model and<br/>with intermediate initiated cells subjecting to stochastic<br/>proliferation and differentiation and genetic and epigenetic<br/>changes. The investigators also propose to apply these new<br/>innovative approaches to develop carcinogenesis models for some<br/>human cancers for assessing risk factors and to develop predictive<br/>procedures for these cancers. Specifically, the investigators will<br/>apply the methods to develop new stochastic models for human liver<br/>cancer because of the importance of this cancer and because of the<br/>availability of some precious data for this type of cancer. The<br/>investigators note that the similar methododology can be applied to<br/>other human cancers as well, which will be investigators' future<br/>research.<br/><br/>The investigators study some cancer models for human liver cancer<br/>based on the existing data already collected. Such models are<br/>considered to follow EPA proposed cancer guidelines (Federal<br/>Register 51(85), 33992-34003, 1996).  The use of such model is<br/>possible because sufficient biological information on the human<br/>liver cancer is available. The end result of this research should be<br/>useful for cancer prevention and cancer control. The investigators<br/>believe that this along with other advances will render many current<br/>practices in experimental biology, including many steps of drug<br/>development, unnecessary. This will in turn make the drug<br/>development process become more productive and efficient.<br/>"
"0803531","Collaborative Research: Detecting false discoveries under dependence using mixtures","DMS","STATISTICS","09/01/2008","09/03/2008","Anindya Roy","MD","University of Maryland Baltimore County","Standard Grant","Gabor Szekely","08/31/2010","$81,091.00","","anindya@umbc.edu","1000 HILLTOP CIR","BALTIMORE","MD","212500001","4104553140","MPS","1269","0000, OTHR","$0.00","Statistical analysis of multiple testing problems revolves around the distribution of the collection of  p-values arising from simultaneous tests. Data from fMRI, Proteomics, Microarray and other biomedical experiments exhibit dependence among p-values. Statistical inference yields biologically irrelevant conclusions if such dependence is not taken into consideration while estimating error control measures such as the false discovery rate. This proposal delineates a model oriented approach to multiple hypotheses testing by flexible and accurate modeling of the joint distribution of the p-values in dependent situations using mixtures. An additional theoretical goal the investigators study properties of skew-mixture models. By incorporating dependence in the model for the p-values, the proposed research provides valid controls of false discoveries, especially in complex biomedical applications. The proposed methodologies provide a foundation for statistical analysis in large dependent multiple testing situations and will spawn new research in the area of false discovery control.<br/><br/>Multiple hypothesis testing is one of the primary  statistical tools available to the scientists for efficiently analyzing large-scale complex biomedical data such as gene-expression data, protemics data or brain imaging data. Disease association studies in such biomedical applications require testing significance of association of several thousand genes or proteins or brain regions, simultaneously. Identification of a gene or a protein as being potentially associated with a given  disease is called  a discovery. However,  in large scale biomedical studies there is a risk of accumulating error via making too many false discoveries. The proposed research substantially influences the practice of statistics in biomedical applications by providing accurate estimates of error rates in large scale disease association studies. The investigators specifically develop error control mechanism for brain imaging applications in MRI studies of autistic patients.  The project impacts human resource development in the form of graduate student education and training.<br/><br/>"
"0805577","Distributions of patterns and statistics in Markovian sequences","DMS","STATISTICS","07/01/2008","06/23/2008","Donald Martin","NC","North Carolina State University","Standard Grant","Gabor Szekely","06/30/2013","$150,000.00","","demarti4@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","0000, OTHR","$0.00","In this project the investigator studies the computation of distributions of patterns and statistics in sequences through auxiliary Markov chains.  In the method, Markovian structure in the original sequence is exploited to associate an auxiliary Markov chain with the sequence in such a manner that an event of interest in the original sequence occurs if and only if the auxiliary Markov chain lies in a class of states that corresponds to the event.  Once the auxiliary chain is set up, probabilities for the event may be computed by tracking movements through the chain and then extracting the desired probabilities.  The goals of this work are threefold: (1) to compute distributions of complex patterns that have not been addressed to date; (2) to apply probabilistic tools that are developed to statistical testing and data analysis; and (3) to quantify uncertainty in statistics of labeled and segmented data modeled by probabilistic graphical models.  These goals are integrated, since probabilistic approaches to computing distributions of patterns and statistics provide the mathematical tools necessary for the statistical applications of goals (2) and (3), and in turn those applications drive the need for computing distributions in increasingly complex situations.  Whereas satisfying the first two goals will provide an important contribution to the literature, the major contribution of the research is represented by goal (3).  The computation of sampling distributions of statistics of hidden state sequences provides a method of quantifying uncertainty in labeled and segmented data, an area that has not been adequately addressed.  In cases where one is interested in inference on statistics of labeled data, a typical approach is to determine the most likely sequence of states given the observations, and then obtain the value of the statistic of interest from that state sequence.  However, whereas the most likely states are optimal if one is interested in the best set of labels, it may not be so for inference on statistics of the labels.  This work provides a novel approach to compute the exact sampling distribution of statistics of labeled data, providing a means for more accurate inference.  Sensitivity of computed distributions to estimated parameters and applications to change points will also be considered.          <br/>      <br/>The need for distributional properties associated with patterns and statistics in sequences, both realizations of data emanating from a model and hidden sequences used to label and segment observed data, arises in many practical fields of study with massive data sets, such as bioinformatics, time series, information theory, economics, data mining, and quality control.  In this research computational tools are developed for computing such distributions.  Results for distributions of patterns and statistics may be applied to many practical problems, such as detecting genes, promoters, or other functionally significant patterns in DNA sequences, and determining probabilities related to classification of observations in health-related studies, change points that indicate new regimes in economic data, patterns that indicate an intrusion, or of patterns associated with surveillance work.  The theory may be used to compute distributions of patterns in underlying sequences that are corrupted by noise or missing observations, and also distributions of statistics that are intractable by combinatorial or other means.  Thus this research facilitates new scientific studies that rely on results for patterns or statistics that have not been computed to date.  <br/><br/><br/>"
"0804937","Statistical Inference for Long Memory and Nonlinear Time Series","DMS","STATISTICS","06/01/2008","04/02/2010","Xiaofeng Shao","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Gabor Szekely","05/31/2011","$75,000.00","","xshao@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","0000, OTHR","$0.00","The proposal aims to develop methodological and theoretical tools for statistical inference of long memory and/or nonlinear time series, for which the traditional methods and theory developed for linear ARMA-type series are not known to be applicable. Since the applications of long memory and nonlinear models are rapidly growing, there is an urgent and crucial need to either provide a theoretical justification for existing methods or propose novel methods that are able to accommodate long memory and nonlinear features. To meet this need, the investigator proposes to study the following research problems: confidence interval for spectral means and ratio statistics; Whittle estimation and diagnostic checking for fractionally integrated time series with uncorrelated but dependent errors; new tests of independence and non-correlations between two stationary time series; frequency domain semiparametric inference for bivariate fractionally integrated nonlinear time series. All of them are linked to the second order properties of the long/short time series with nonlinear features, and together, they cover a wide spectrum of important inference issues for such series.<br/><br/>Time series with long memory and nonlinearities occur in various fields, including atmosphere science, environmental science, geophysics, hydrology, economics, finance and others. This work will greatly enhance the available methodologies and theories, provide more tools and have potential applications in all such fields. The proposed research has significant impact on education through involvement of Ph.D students directly in the proposed research and incorporation of results into graduate statistical courses.<br/>"
"0757367","FRG: Collaborative Research:  Prediction and Risk of Extreme Events Utilizing Mathematical Computer Models of Geophysical Processes","DMS","STATISTICS","07/01/2008","05/09/2010","E Bruce Pitman","NY","SUNY at Buffalo","Continuing Grant","Gabor Szekely","06/30/2012","$285,648.00","Eliza Calder","pitman@buffalo.edu","520 LEE ENTRANCE STE 211","AMHERST","NY","142282577","7166452634","MPS","1269","0000, 1616, OTHR","$0.00","The goal of this project is to develop the mathematical, statistical and computational tools needed to assess and predict the risk associated with geophysical hazards such as volcanic pyroclastic flows.  Based on a preliminary data analysis, the investigators develop stochastic models beginning with stationary independent increment processes employing (possibly tapered) Pareto distributions for the volumes of pyroclastic flows exceeding some observational threshold, in the domain of attraction of an alpha-stable process governing the aggregate flow volume of multiple smaller eruptions.  Von Mises distributions are used for flow initiation angles.  The deterministic TITAN2D two-dimensional computational environment is employed, which uses available digital elevation maps to predict the impact at various sites of interest from flows of specified volume and initiation angles.  TITAN2D is a depth-averaged, thin-layer computational fluid dynamics code based on an adaptive grid Godunov solver, suitable for simulating geophysical mass flows.  A rapid emulator based on a simple Gaussian random-field approximation to the TITAN2D model enables the investigators to emulate hundreds of thousands of TITAN2D runs and construct an estimate of the set of possible flow volumes and initiation angles that would lead to significant impact; a hierarchical Bayesian statistical model then reflects the probability of such an impact over a specified period of time.<br/><br/>Recent advances in computing power and algorithms have led to the application of mathematical and computer modeling to such highly complex phenomena as storms, floods, earthquakes and volcanic eruptions.  It is increasingly being understood that development of mathematical models of these phenomena is only one part of a much more complex process needed for making reliable estimates and predictions of risk.  This project develops the mathematical, statistical and computational tools needed for assessing and predicting the risk associated with such natural hazards.  A particular focus of the work is the study of how these risks vary in space and time, and of how uncertain they are.  This methodology is developed in context of the specific problem of volcanic avalanches and pyroclastic flows (so-called geophysical mass flows), but much of it will be applicable more broadly to problems in the analysis and quantification of risk in problems featuring spatial variability and model uncertainty.  It brings together a unique team of scientists with specialties including volcanology, to guide the development of realistic models for the geophysical processes under study; in stochastic processes, to reflect uncertainty and variability about initial conditions, flow frequencies, and other features in realistic and verifiable ways; in deterministic computer modeling, for the difficult task of making detailed spatial predictions of the consequences of the most probable and of the most hazardous possible events; in computer model emulation, to accelerate many thousand-fold the computations necessary for predicting the risk of rare events under a wide range of possible scenarios; and in statistical modeling and analysis, to reflect honestly all the different sources of uncertainty and variability in this analysis, leading to a full quantification of the risk of hazardous events.  Only with such a broad range of expertise can investigators build the tapestry of science that is required to develop tools for studying devastating natural hazards.<br/>"
"0902232","CAREER: Semiparametric and Non-Parametric Models for Correlated Data","DMS","STATISTICS","10/01/2008","12/02/2009","Annie Qu","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Gabor Szekely","12/31/2009","$123,923.00","","aqu2@uci.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","0000, 1045, 1187, OTHR","$0.00","CAREER: Semiparametric and nonparametric models for correlated data<br/><br/>Abstract:<br/><br/>This research project is aimed at developing statistical theory and<br/>practical methodology for complex high dimensional correlated data<br/>where the full parametric likelihood function of the model is difficult<br/>to specify or intractable, and partial data information is not accurate<br/>or is missing. The PI and her collaborators  will develop<br/>efficient and robust estimation procedures by incorporating<br/>correlation structures  into the models where high dimensional<br/>nuisance parameters are present, and develop inference functions<br/>for hypothesis testing with low computational intensity.<br/>Part of research goals for the 5-year plan are:<br/>to provide an explicit maximum number of contaminated<br/>clusters allowed to maintain the consistency of the estimator using<br/>quadratic inference functions; to develop unbiased and efficient<br/>estimating functions if missing responses are missing at random,<br/>and inference functions for testing the model assumption;<br/>to develop an efficient esimator using a nonparametric regression<br/>spline with relatively low demand on computation, and introduce a<br/>goodness-of-fit test with a chi-squared property for testing whether<br/>coefficients in nonparametric regression  are time-varying or time<br/>invariant; and, to develop semi-nonparametric models for cell cycle<br/>microarray data to incorporate both temporal correlation within genes<br/>and correlation between biologically related genes.<br/><br/><br/>This research will have significant impact and many applications in<br/>biomedical research, econometrics, environmental studies, oceanography,<br/>social science and public health where correlated data arise<br/>often. The outlined research projects help to tackle fundamental<br/>questions in statistical science and will stimulate interest  from a<br/>large group of scientists. It also makes connections between<br/>theory and methods developed in econometrics, statistics and<br/>biostatistics. The proposed research will benefit biomedical research<br/>to help combat life threatening diseases such as AIDS and cancer,<br/>and will make contributions to identifying cell cycle regulated genes<br/>more accurately. It will integrate current states of knowledge of<br/>proposed research areas substantially into  educational activities<br/>through development of<br/>new courses on  nonparametric methods and microarray data analysis.<br/>It will advance undergraduate and graduate students' learning and<br/>training in semiparametric and nonparametric methods. Furthermore,<br/>it will broaden opportunities and enable the<br/>participation of all citizens from various disciplines, including<br/>underrepresented minorities and international partnerships.<br/>"
"0746265","CAREER: Statistical Inference in Algebraic Models with Singularities","DMS","STATISTICS","07/01/2008","01/12/2012","Mathias Drton","IL","University of Chicago","Continuing Grant","Gabor Szekely","05/31/2013","$400,000.00","","md5@uw.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, 1045, 1187, OTHR","$0.00","NSF CAREER proposal DMS-0746265<br/><br/>This project is concerned with statistical models whose parameter spaces have singularities.  The investigator studies how singularities impact the behavior of existing statistical methods and develops new techniques for adequate assessment of statistical significance.  The focus is on algebraic statistical models, that is, models that have (semi-)algebraic sets as parameter spaces.  The class of algebraic models comprises many of the singular models employed in practice and can be studied using tools from computational algebraic geometry.  Importantly, the well-behaved local geometry of semi-algebraic sets makes it possible to obtain general results without having to assume difficult to verify regularity conditions.  The statistical techniques under study include classical procedures from likelihood inference such as likelihood ratio and Wald tests as well as information criteria.<br/><br/>Modern scientific studies often require analysis of data on several jointly observed variables.  Statistical models of dependence relationships among the different variables are often formulated using additional variables that are not observable (or hidden).  A common feature of hidden variable models is that their statistical properties are not entirely understood because of a lack of smoothness properties that makes them irregular.  This is the primary motivation for this project that develops theory and methods that have a bearing on problems such as determining the number and type of unobserved variables to be included in a statistical model.  Such problems arise in particular in applications in the social sciences where key concepts such as intelligence are not directly observable, and in computational biology where hidden variables are employed, for example, when DNA of present-day species is used to validate evolutionary theories that involve extinct species.  More broadly, the work is relevant for any study, medical or otherwise, in which the existence of influential unobserved variables cannot be excluded.<br/>"
"0806128","Statistical Analysis for Models involving Riemannian Manifolds","DMS","STATISTICS","09/01/2008","08/29/2008","Jie Peng","CA","University of California-Davis","Standard Grant","Gabor Szekely","08/31/2010","$59,991.00","Debashis Paul","jiepeng@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","0000, 1269, OTHR","$0.00","This application is motivated by real life problems whose modeling and analysis involve non-Euclidean Riemannian manifolds. In particular, it is motivated by problems arising from analyzing  diffusion tensor imaging (DTI) data and longitudinal data analysis, with both having important implications in various scientific fields such as neuroscience. Although, there has been a considerable amount of theoretical developments for statistical models involving manifolds, as well as developments of computational tools for special manifolds, there are still many gaps that need to be bridged both in terms of theory and computation. For instance, there is a pressing need to develop new and sophisticated statistical techniques when data lie on a manifold. There is also a scope to extend available computational tools to complex statistical models. In this application, the investigators address the issues of computation, estimation as well statistical inference in a unified manner in the models involving manifolds. In particular, the investigators 1) develop a framework for non-parametric smoothing when data lie on a special manifold; and study the asymptotic properties of the resulting estimators; 2) develop computational algorithms and softwares for parameter estimation, non-parametric smoothing as well as model selection; 3) develop a general framework for statistical inference when the parameter space is a special manifold; 4) apply the analytical and computational tools to problems in areas including (but not limited to), cognitive neuroscience, longitudinal studies and psychometry. Together, this work enhances understanding of the role of geometry in statistical modeling and analysis for a broad range of problems. It also contributes to the development of computational tools for various fields as mentioned above.<br/><br/>In this application, the investigators develop quantitative tools in areas such as cognitive neuroscience, where the problems have specific geometric structures. They aim to extract important features of the data by explicitly utilizing these structures. These investigators address the issues of computation, estimation and prediction for such problems under a unified statistical modeling framework. This application is partly motivated by studies in discovering the structure and functionality of brain tissues through images generated by technologies such as diffusion tensor imaging (DTI). The fundamental problems associated with the cognitive function of brains become a rich and extremely important field of study.  The implication of ongoing research in this field is tremendous in terms of understanding and treating complex brain disorders such as Alzheimer's disease and autism. Moreover, this application results in developments of open source softwares and quantitative techniques that can be extended to a broader range of complex scientific problems, including longitudinal studies and psychometry. Hence, the understanding of many problems in the field of biology, health and medicine is also likely to be benefited from this application. This application also has a broader educational impact through interdisciplinary research between statisticians and neuroscientists."
"0806127","Fence Methods for Complex Model Selection Problems","DMS","STATISTICS","09/01/2008","08/28/2008","Jiming Jiang","CA","University of California-Davis","Standard Grant","Gabor Szekely","08/31/2011","$120,337.00","","jiang@wald.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","0000, OTHR","$0.00","Many model search strategies involve trading off model fit with model<br/>complexity in a penalized goodness of fit measure. Asymptotic properties<br/>for these types of procedures in some conventional situations, such as<br/>regression and ARMA time series have been studied. Yet, such strategies<br/>do not always translate into good finite sample performance. Furthermore,<br/>such standard model selection procedures encounter difficulties for<br/>nonconventional model selection problems as well. This project aims at<br/>developments of a new model selection strategy, called fence methods, in<br/>following four major areas of methodology research and applications: (i)<br/>development of adaptive fence methods for high dimensional and complex<br/>model selection problems using the idea of restricted maximum likelihood;<br/>(ii) development of data adaptive fence methods for nonparametric model<br/>selection problems such as penalized smoothing spline estimation; (iii)<br/>development of fence methods for quantitative trait loci (QTL) mapping;<br/>and (iv) development of user-friendly standalone software for implementing<br/>the fence methods.<br/><br/>The fence idea is generally based on building a statistical fence, or<br/>barrier, to carefully eliminate incorrect models. This is done by<br/>determining which models are within variation of a goodness-of-fit<br/>measure of an anchor model. Once the fence is constructed, the optimal<br/>model is selected from amongst those within the fence according to<br/>a criterion which can be made flexible. For example, the criterion can<br/>incorporate scientific or economic concerns. The adaptive fence method<br/>may be viewed as comparing signals with noises to come out with an optimal<br/>decision supported by the data. Given such a wide spectrum of models that<br/>can be handled, the range of applications seems enormous. Of particular<br/>interests are applications in human genetics, medical research and surveys.<br/>To facilitate such translational research, the investigators plan to freely<br/>disseminate available computer software to implement the fence methods.<br/>"
"0806175","Monte Carlo Methods for Complex Problems: From Data Augmentation to Likelihood Free Inference","DMS","STATISTICS","09/01/2008","06/06/2010","Yuguo Chen","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Gabor Szekely","08/31/2011","$259,888.00","","yuguo@uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","0000, OTHR","$0.00","Highly complex stochastic models arise frequently in scientific applications. They often lead to statistical inference problems with analytically or computationally intractable likelihood functions. Such problems lie beyond the limit of current Monte Carlo methods. The goal of this proposal is to develop efficient Monte Carlo algorithms for statistical inference problems with intractable likelihoods. Proposed research considers two categories of problems without likelihoods. In the first category, the likelihood is available in analytical forms if the problem is put into an appropriate augmented space. For such problems, a new data augmentation scheme is proposed which leads to a more efficient Markov chain Monte Carlo algorithm. In the second category, the model is a ""black box"" and only a generating stochastic mechanism is available to simulate data from the model. For such problems, several likelihood-free Monte Carlo algorithms are proposed which extend the power of current Monte Carlo methods.  The proposed methods are applied to inference problems in population genetics, panel studies, and hydrological models.<br/><br/>The proposed research addresses the urgent need to develop innovative Monte Carlo methodology for problems with intractable likelihood functions. This is of fundamental importance in statistics. It allows researchers to concentrate on scientifically plausible statistical models without worrying about mathematical intractability. Applications of the proposed methods include statistical inference in molecular population genetics which can help locating genes that are responsible for genetic diseases, and Bayesian calibration of hydrological models which can be used to predict ground-water flow. The proposed research has significant impact on education through involvement of graduate and undergraduate students directly in the proposed research, incorporation of proposed algorithms into related courses, and dissemination of research results to the scientific communities.<br/>"
"0804140","Flexible Statistical Methods for the Analysis of Correlated Multivariate Data","DMS","STATISTICS, COFFES","08/01/2008","07/30/2008","Ori Rosen","TX","University of Texas at El Paso","Standard Grant","Gabor Szekely","07/31/2011","$119,998.00","","orosen@utep.edu","500 W UNIVERSITY AVE","EL PASO","TX","799680001","9157475680","MPS","1269, 7552","0000, OTHR","$0.00","The proposed research focuses on a number of topics related to the analysis of correlated multivariate data. Bayesian methods in combination with mixture models and nonparametric techniques result in novel flexible methods. In one project, the investigator proposes a method for the analysis of multivariate functional data. Cubic splines will be used to estimate the unknown mean functions, as well as the subject-specific functions. To accommodate correlation and unequally spaced measurement times, the multivariate Ornstein-Uhlenbeck process will be explored. Another topic of interest is parsimonious estimation of the covariance structure of longitudinal data when the assumption of a constant covariance matrix over time is unreasonable. To allow for an evolving parsimonious covariance matrix, the Cholesky decomposition of the inverse covariance matrix will be used along with mixture modeling. In a third project, the investigator's previous work on spectral matrix estimation for stationary multivariate time series will be extended to the case of locally stationary multivariate time series. The estimated spectral matrix along with the spectral envelope technique can be used to analyze categorical time series, such as DNA sequences. This analysis, however, is limited to short DNA sequences for which the assumption of stationarity is reasonable. In this project, local behavior of longer DNA sequences will be accommodated by extending the investigator's previous work via mixture modeling. In another project, modeling a covariance matrix using the spectral decomposition will be explored. This approach can be used as a building block in other applications such as model-based clustering.<br/><br/>The proposed research develops novel flexible statistical methods for the analysis of data consisting of several correlated variables. One area of application is psychiatric research, where despite decades of clinical trial experience in major depression, there is only limited understanding of which patients with major depressive disorder respond better to psychotherapy or to pharmacotherapy. Of particular interest is the identification of baseline subject characteristics which differentially predict treatment response in the two groups. The response variables in this case may be different measures of depression taken longitudinally on psychiatric patients. Another area of application is the analysis of DNA sequences. A DNA sequence can be described as genes containing coding regions separated by noncoding regions. Coding regions within genes code for the proteins which determine the organism's structure and functioning. Detecting genes in a DNA sequence is important for analyzing the genome of an organism. A third area of application is clustering of microarray data to identify functional groups of genes. The goal of cluster analysis for microarray data is to group genes into clusters with similar profiles. Knowledge about expression levels of genes from different cells may help in diagnosing diseases or in finding drugs to cure them."
"0805157","Statistical Methods from Spectral Analysis with Markov Chains","DMS","STATISTICS","08/01/2008","07/30/2008","Julia Salzman","NY","Columbia University","Standard Grant","Gabor Szekely","06/30/2009","$113,883.00","","julia.salzman@gmail.com","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","0000, OTHR","$0.00","Modern science, especially biochemistry, has become dependent on numerical analysis of large amounts of data generated in most every experiment.  Scientific advancement in biology and in understanding disease pathogenesis will likely depend on the analysis of the huge corpus of biomolecular data (eg. microarray, RNA and DNA sequence data). This advancement is linked to the field's ability to continue developing statistical methodologies capable of identifying a robust ``signal'' which can be reproducibly identified in multiple experiments all of which generate noisy data.  The PI has shown how the theoretical framework of spectral analysis with Markov chains unifies several statistical methods for identifying structure in data that is observed with noise: discrete Fourier analysis, correspondence analysis, principle components analysis, as well as spectral clustering.  This unifying framework also provides insight into, and  generalization of, the more traditional methods listed above. Therefore, the PI's proposed research has two major directions.  In one direction, it will continue basic methodological development of exploratory data analysis with a focus on methods capable of identifying biological signals observed in noisy experimental conditions.  In another, it will focus on rigorous statistical analysis of this methodology which is in wide use in statistics, computer science and bioinformatics.<br/><br/>Statistical methods developed here will be particularly aimed at the study of cellular regulation of gene and protein expression.  These cellular mechanisms have wide ranging importance in understanding human disease including cancer and infectious disease.  The data analytic methods developed under this grant will be implemented and made publicly available through Bioconductor, a package in R.  The broad goal of this proposal is to work towards providing a methodological unification of methods in statistics, biology and computer science to biomolecular data.  Thus, it falls roughly into the field of bioinformatics."
"0806199","Functional Models for Complex and High-Dimensional Data","DMS","STATISTICS","07/15/2008","04/05/2010","Hans-Georg Mueller","CA","University of California-Davis","Continuing Grant","Gabor Szekely","06/30/2011","$180,015.00","","hgmueller@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","0000, OTHR","$0.00","Innovative methodology for Functional Data Analysis facilitates improved data analysis for longitudinal studies, e-commerce online bidding, genomic studies, (bio)demography and many other areas of the social, biological and physical sciences problems. The proposed functional approaches provide highly flexible ways to characterize such data, and especially to study their time-dynamic aspects. The investigator extends the applicability of Functional Data Analysis to data structures that have not been widely considered within this framework. This includes point processes, high-dimensional (large p, small n) data and sparsely observed stochastic processes as they occur in longitudinal and repeated measurements data. Especially for high-dimensional non-functional and point process data, Functional Data Analysis approaches have the potential to lead to transformative rather than merely incremental improvements. The investigator develops flexible functional and varying-coefficient models for functional regression and correlation. Current modeling approaches are too restrictive to be of broad applicability and more general models are needed. Similarly, the subarea of curve warping has seen much development lately but there remain many important open questions to be investigated. The investigator combines theoretical analysis, simulations, and data applications to conduct this research and applies the methods to data from e-commerce, biodemography, longitudinal studies and gene expression.<br/><br/>The investigator develops statistical methodology that is immediately useful for the analysis of large and complex data in genomics, demography and biodemography. These new analysis tools, which fall into the field of Functional Data Analysis, are geared towards gaining a better understanding of time-dependent processes. These include a variety of commonly observed phenomena such as growth, aging, bidding during an online auction, or repeated observations of a recurring incident such as an asthma attack. The methods developed by the investigator elucidate the underlying dynamics of such phenomena. Application of these methods in particular enables insights into the mechanisms of aging and longevity, the dynamics of on-line auctions, and other instances of e-commerce. The investigator extends the scope of these methods further such that for example improved prediction of specific risks becomes feasible, based on a recording of a subject's gene expression profile.<br/>"
"0805481","Asymptotically Sufficient Statistics in Nonparametric Curve Estimation","DMS","STATISTICS","07/01/2008","06/04/2010","Andrew Carter","CA","University of California-Santa Barbara","Continuing Grant","Gabor Szekely","06/30/2011","$110,101.00","","carter@pstat.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","MPS","1269","0000, OTHR","$0.00","An important area of mathematical statistics are nonparametric curve estimation problems in their various forms. One growing approach to these complicated problems is to find asymptotically sufficient statistics to simplify them. These statistics can also be used to find asymptotically equivalent experiments that can unify the approaches to inference and<br/>estimation for a number of different types of nonparametric problems. One class of nonparametric problems is the class of regression models where the mean of the data is a smooth function of the design points at which the data is observed. It is well known that these models can be thought of as discrete samples of a continuous, signal-plus-white-noise model where, as the size of the sample increases, negligible information is lost about the mean function. Specifically, the increments of the continuous process observed by the<br/>white-noise model are asymptotically sufficient and have nearly the same distribution as the regression observations. This project finds asymptotically sufficient statistics in some nonparametric regression models that allow estimation of nuisance parameters such as the variance or covariance of the errors or the density of the randomly placed design points. These statistics consist of two parts: one corresponds to the increments of some process, and the second contains information about the value of the nuisance parameter. These asymptotically sufficient statistics lead to an asymptotically equivalent model that observes a continuous Gaussian process where the mean function is first transformed by an observed filter. These white-noise approximations provide a new and unifying approach to a number of nonparametric regression problems. There are other nonparametric models, such as density estimation from independent data, that have been shown to be asymptotically equivalent to the white-noise model. The same technique of finding auxiliary estimators of nuisance parameters and then approximating the conditional distribution given these estimators can also be applied to the density estimation experiment. These sufficient statistics are a step in the direction of finding an approximation to the density estimation experiment on two dimensional sample spaces.<br/><br/>This project seeks to find better techniques for estimating a signal in the presence of noise by constructing approximations that are appropriate for large sample sizes. The work will have an impact on the theory of nonparametric curve estimation, the basis for applications such as<br/>analyzing financial series or medical images, as well as a broad impact on research and education in mathematical statistics. In many areas of technology today, large sets of data that do not follow a typical pattern are difficult for scientists to analyze. Non-regular estimation problems like this might come up in problems such as filtering noisy medical images or signals, estimating distributions of plant species, or analyzing financial series. signal interpretation, financial analysis, and other applications. In this project, the investigator transforms the problem into a form that can be solved by already existing statistical tools. As a result, existing methods of analysis can be applied to a wider range of technological problems."
"0804597","Regularized Dimension Reduction for High Dimensional Data","DMS","STATISTICS","07/01/2008","06/28/2010","Sunduz Keles","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","06/30/2011","$100,001.00","","keles@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","0000, OTHR","$0.00","With the recent advancements in biotechnology such as the use of genomewide microarrays and high throughput sequencing, regression-based modeling of high <br/>dimensional data in biological sciences has never been more important. The investigator aims to develop a regularized dimension reduction method for very high dimensional linear regression problems. The main thrust of the research is based on a well-established dimension reduction technique named Partial Least Squares (PLS) regression which has been heavily used in several scientific research areas where ill-posed problems commonly arise.  The proposed work 1) theoretically investigates the suitability of PLS for very high dimensional regression settings where the number of predictors highly exceeds the available sample size; 2) proposes a regularization scheme that promotes variable selection in addition to dimension reduction; constructs rigorous mathematical formulations of the regularization scheme and characterizes their analytical solutions; 3) develops an efficient algorithm implementing the proposed framework. Extensions to interrelated classification and censored data settings are also considered. <br/><br/>The proposed work, when completed and disseminated, will provide a powerful simultaneous dimension reduction and variable selection framework relevant for all fields of scientific research that concern high dimensional ill-posed regression problems. This will allow scientists to analyze high-dimensional data with efficient dimension reduction and increased interpretability. The PI is actively involved in collaborations with biologists, biochemists, geneticists, and medical doctors. The research emanating from this proposal will therefore have strong interdisciplinary flavor and will be implemented, tested and tuned to address many real scientific questions of interest. The PI will apply the proposed research to problems arising in studying the variation of gene expression, transcription regulation, and binding properties of DNA binding proteins, where the selection of relevant variables is as important as having excellent predictive power. The project will integrate research and education by working closely with both graduate and undergraduate students."
"0833323","Spatial and Spatio-temporal Processes: Asymptotics, Misspecification and Multivariate Extension","DMS","STATISTICS","02/27/2008","06/24/2008","Hao Zhang","IN","Purdue University","Standard Grant","Gabor Szekely","05/31/2011","$152,108.00","","zhanghao@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","0000, OTHR","$0.00","The investigator develops appropriate infill or fixed domain asymptotic results for the evaluation of approximation methods for spatial data.  The infill asymptotic framework is generally preferred for spatial data. However, infill asymptotic results for estimation are in general difficult to derive, and there exist only a few explicit infill asymptotic results pertaining to specific models. In this proposal, a general approach to establishing the infill asymptotic properties is outlined and followed to establish asymptotic distributions of estimators that maximize some approximated likelihood functions such as Vecchia's approximation and covariance tapering. These infill asymptotic results assure that the approximation methods may yield asymptotically efficient estimators. In addition, for spatio-temporal data, the investigator considers a new asymptotic framework in which the temporal domain is increasing while the spatial sampling domain is fixed. Under this asymptotic framework, the investigator shows that an incorrect covariance function (such as covariance tapering) generally results in biased estimators. The investigator establishes asymptotic results that allow for the correction of biases. The adjusted estimators are expected to be asymptotically normal and unbiased. These asymptotic results allow one to employ incorrect but simpler spatio-temporal covariance functions and then adjust for the bias. Finally, the investigator extends the results from the univariate case to the multivariate case when multiple spatial variables are observed across space and/or over time.<br/><br/>Data across space and time are routinely observed in many scientific studies that are very important to the society such as those on global warming, environmental monitoring, precision agriculture, epidemiology, hot spot detection in homeland security, etc. The immense amount of data and the correlation across space and time have raised new challenges to the modeling and analysis of such space-time data. The primary goal of this project is tackle these challenges by studying computationally feasible and efficient approaches to the analysis of vast space-time data, which also bear no or little loss of statistical efficiency.  It is expected that this project will make more accessible and feasible the analysis of huge spatial and spatial-temporal data to scientists in broader disciplines, and thus enables scientists to retrieve significant and reliable information from the vast amount of data.<br/>"
"0803540","Collaborative Research: Detecting false discoveries under dependence using mixtures","DMS","STATISTICS","09/01/2008","09/03/2008","Subhashis Ghoshal","NC","North Carolina State University","Standard Grant","Gabor Szekely","08/31/2011","$59,999.00","","subhashis_ghoshal@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","0000, OTHR","$0.00","Statistical analysis of multiple testing problems revolves around the distribution of the collection of p-values arising from simultaneous tests. Data from fMRI, Proteomics, Microarray and other biomedical experiments exhibit dependence among p-values. Statistical inference yields biologically irrelevant conclusions if such dependence is not taken into consideration while estimating error control measures such as the false discovery rate. This proposal delineates a model oriented approach to multiple hypotheses testing by  flexible and accurate modeling of the joint distribution of the p-values in dependent situations using mixtures. An additional theoretical goal the investigators study properties of skew-mixture models. By incorporating dependence in the model for the p-values, the proposed research provides valid controls of false discoveries, especially in complex biomedical applications. The proposed methodologies provide a foundation for statistical analysis in large dependent multiple testing situations and will spawn new research in the area of false discovery control.<br/><br/>Multiple hypothesis testing is one of the primary statistical tools available to the scientists  for efficiently analyzing large-scale complex biomedical data such as gene-expression data, protemics data or brain imaging data. Disease association studies in such biomedical applications require testing significance of association of several thousand genes or proteins or brain regions, simultaneously. Identification of a gene or a protein as being potentially associated with a given disease is called a discovery. However,  in large scale biomedical studies there is a risk of accumulating error via making too many false discoveries. The proposed research substantially influences the practice of statistics in biomedical applications by providing accurate estimates of error rates in large scale disease association studies. The investigators specifically develop error control mechanism for brain imaging applications in MRI studies of autistic patients. The project impacts human resource development in the form of graduate student education and training.<br/><br/>"
"0804198","Collaborative Research: Fast Functional MRI","DMS","STATISTICS","07/01/2008","06/30/2008","Martin Lindquist","NY","Columbia University","Standard Grant","Gabor Szekely","06/30/2011","$54,610.00","","martin@stat.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","0000, OTHR","$0.00","The problem addressed by the proposal is to find, for a given higher cognition task, one or more regions of the brain where oxygen is consumed in performing the task. The investigators approach is to use the so-called initial negative dip, which is caused by the initial consumption of oxygen in the blood reserve pool in an active brain region, in order to observe, in real time the location and timing order of brain activation when a human performs the higher cognition task. The standard approach, which relies on the stronger but much slower positive rise, due to the resupply of blood to the regions several seconds after activation, is inadequate for determining the temporal ordering of brain activity in different regions. The investigators have developed an elegant and highly efficient echo-volumar imaging (EVI) sampling scheme for performing fast functional Magnetic Resonance Imaging (fMRI), along with the necessary tools needed for image reconstruction and statistical analysis of the resulting data sets. They have used their techniques to conduct higher cognition experiments using a variety of paradigms, and were consistently able to detect the negative dip and exhibit its statistical significance. In addition, they demonstrated that the negative dip appears in different brain regions in the same temporal sequence as the corresponding brain activation according to the experimental paradigms, and that the timing of the positive rise is at times confounded. The proposed research, which will further develop the investigators methods, includes the design of EVI trajectories for the newest and most powerful 7 Tesla scanners and more complicated multiple coil systems, in order to improve the spatial and time resolutions of their approach. New image reconstruction methods will also be developed for the multi-coil data and the investigators statistical analysis tools will be validated and carefully improved.<br/><br/>This proposal focuses on developing statistical methods and related theory for performing fast fMRI. The proposed research will further advance and use the methods developed by the principle investigators and their collaborators to sharply improve the time-resolution for the blood oxygen level dependence technique of functional magnetic resonance imaging. In its current implementation the investigators method is able to measure brain volumes every 100ms compared to 2000ms for standard fMRI, thereby allowing fMRI studies to be performed at an unprecedented temporal resolution. Fast fMRI is expected to have profound and far-reaching consequences in the understanding of brain function, a problem of central scientific interest at the present time."
"0805946","Exact inequalities and limit theorems for Rademacher and self-normalized sums, and related statistics","DMS","STATISTICS","08/01/2008","07/30/2008","Iosif Pinelis","MI","Michigan Technological University","Standard Grant","Gabor Szekely","07/31/2011","$150,000.00","","ipinelis@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","MPS","1269","0000, OTHR","$0.00","The main objectives of the project are as follows:<br/> * Prove the longstanding conjecture on the best constant factor in the Rademacher-Gaussian tail comparison.<br/> * Prove another longstanding conjecture, on the asymptotic domination of the Rademacher tail by the Gaussian one.<br/> * Consider also the ``asymmetric'' case.<br/> * Extend to the case of moderate deviations the result due to Shao et al. on the saddle-point approximation to large-deviation probabilities <br/> of a self-normalized sum of independent random variables.<br/> * Obtain limit theorems, including Berry-Esseen-type bounds and Cramer-type large-deviation asymptotics, for Pearson's product-moment sample correlation coefficient and a number of similar and more general statistics.<br/> Thus, the investigator aims to solve longstanding and difficult problems of probability theory and mathematical statistics. The first two of them concern some of the most important properties of such a classical and fundamental object as the Rademacher sums, whose distributions play the role of the extreme points of the set of the distributions of sums (and self-normalized sums) of any independent   symmetric random variables. Extensions to the ``asymmetric'' case will also be considered. Closely related are other main objectives of the project, concerning limit theorems for self-normalized sums (or, equivalently, for Student's statistic).<br/><br/> The main impact will be in significantly better understanding of important properties of some of the most fundamental objects in probability theory and mathematical statistics. The successful completion of the project will also result in novel and important applications to such classical objects in statistics as Student's test and Pearson's correlation test, which are some of the very few hypotheses tests used most broadly in sciences and engineering. While there are great difficulties to overcome, it appears that the attainment of these objectives is within reach, given a number of advances already made by the investigator and his rather unique expertise in various areas of probability and statistics, as well as his demonstrated abilities to identify and solve difficult and longstanding problems and also to work effectively in a wide and highly diverse range of fields, including mechanical engineering, biology, operations research and combinatorics, and geometry and physics. Efforts will be made to disseminate results, not only via publication in wide-circulation journals, but also via news networks (stories on the investigator's work on evolution modeling and the Eiffel tower shape modeling have already been broadcast around the world by the United Press International and other news agencies). A number of graduate students will be involved into the project; efforts will be made to recruit from underrepresented minorities."
"0805559","Statistical peak detection, adaptive classification and protein-protein network construction using mass spectra","DMS","STATISTICS","06/01/2008","03/30/2010","Susmita Datta","KY","University of Louisville Research Foundation Inc","Continuing Grant","Gabor Szekely","05/31/2011","$149,391.00","","susmita.datta@ufl.edu","Atria Support Center","Louisville","KY","402021959","5028523788","MPS","1269","0000, 9150, OTHR","$0.00","The main goal of this proposal is to develop novel and improved statistical methods for analyzing high dimensional proteomic data generated from mass spectrometers. These data usually consist of spectra each with thousands of features. However, these features contain true signals of proteins/peptides and noises. This proposal focuses on three interconnecting (and sequential) goals: 1) separating the true peaks from chemical noise in a mass spectrum using statistical modeling and hypotheses test, 2) comprehensive evaluation and aggregate ranking of a number of classification techniques to classify the case and control samples using proteomic profiles and construction of an adaptive classifier which is expected to perform better than individual classifiers under an ensemble of performance measures and 3) construction of a protein-protein association network from the truly classifying peaks in a case-control study by reverse engineering. An overall and ultimate goal of this proposed research is to study the performance of the three pieces put together in a sequential manner to understand the inner working of proteins in a case-control study based on mass spectrometry data.<br/><br/>High throughput proteomic profiling using mass spectrometry measurements have enormous potential in scientific/biomedical research. Identification of proteomic biomarkers for complex diseases and conditions like cancer, acute renal disorder and fetal alcohol syndrome etc. from easily available bodily fluids like blood, plasma, urine, amniotic fluid and serum could be very beneficial. These biomarkers are expected to be much more sensitive and specific than the existing ones and hence are better in terms of early detection and prevention of such diseases and conditions. Proteomic signature profiling also can be used to quickly identify different biological agents (as for example, anthrax). This particular application demonstrates its implication in the matters related to homeland security. Similarly, proteomic profiling of bodily fluids of subjects exposed to different environmental toxins can also be useful. However, complexity of these data poses new statistical challenges for their analysis. Hence proper analytic tools are much needed for the proper utilization of these data. The proposed research is expected to make significant contribution towards this relatively new area of research. Last but not the least, the analytical and computational tools developed for this project can be used to analyze other types of high dimensional data."
"0804575","New Developments of Nonlinear Dependent Models, with Applications in Genetics, Finance and the Environment","DMS","STATISTICS","06/01/2008","03/23/2010","Zhengjun Zhang","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","05/31/2012","$180,000.00","","zjz@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","0000, OTHR","$0.00","High-dimensional and complex data are now collected routinely in the fields of environment, financial markets, and signal and image processing. A major challenge is to find methods to analyze the structure of such data sets, to fit models with desired dependence properties, and to identify and validate patterns. A major goal of the proposal is to make significant methodological and theoretical contributions to the important and challenging low-sample and high-dimension statistical inference problems such as dimension reduction in bio-informatics, and extreme dependence problems arising from environmental, and financial markets. The proposal consists of four important steps. First, the proposal pursues a series of developments of new measures for nonlinear dependencies. The investigator studies the limiting distributions of dependence measures (quotient correlation coefficients) and analyzes asymptotic powers when the dependence structures are specified. In DNA microarray data analysis, the quotient correlation is used to select the best feature subset of genes, and then the selected subset is used to predict classes for all sample data. Second, a tail dependent measure (a tail quotient correlation coefficient) with varying threshold is introduced. This measure is related to the study of statistics of multivariate extremes, and is used to assess asymptotic (in)dependencies in environmental variables. Third, the proposal includes the development of statistical estimation methods for asymptotically (in)dependent multivariate maxima and moving maxima processes. This allows one to efficiently study clustered spatial-temporal extreme observations. Fourth, the proposal studies GARCH(r,s) models with m-dependent residuals.<br/><br/>The intellectual merit of the proposal in a first instance stems from an efficient dimension reduction approach using the quotient correlation concept. In DNA microarray data analysis, in which there are thousands of variables (genes) in gene expression profiles, and class prediction is an important problem. It is important to identify subsets of genes to work with, due to the high dimensional feature and small sample size of the data under investigation.  The ultimate goal is to select the smallest subset of genes which contribute toward the classifications and predictions. Among existing gene selection methods, it is hard to find one which always performs better than the rest when applying them to different data sets. The proposal specifically aims at finding a solution for this.  Beyond methodological merits and specific applications, the proposal also has a considerable broad impact. Throughout applications in diverse fields (like above), extreme risks play an important scientific, societal, as well as (possibly) political role.  The dissemination of new statistical tools leading to a better understanding of the occurrence of joint extremes is of great importance. This can be well achieved at the level of new graduate courses, publications in journals aimed at a broad audience and in discussion with scientists from other fields. To name just one potential example where the proposal has great impact, let us consider financial risk management. Due to the establishment of new regulatory-rules for banking solvency (so-called Basel II proposal), banks have to come up (in their analysis of credit risk, for instance) with stress testing procedures which can immediately be formulated in terms of extremal co-movements. Similarly, in multi-line insurance, underwriters have to take care of joint large losses in many different lines of business. It is exactly for these kinds of applications that the proposal yields new tools."
"0806088","Sparse predictors in functional data analysis","DMS","STATISTICS","07/01/2008","06/25/2010","Ian McKeague","NY","Columbia University","Continuing Grant","Gabor Szekely","06/30/2012","$190,592.00","","im2131@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","0000, OTHR","$0.00","Proposed research is motivated from the discrimination task with high dimension, low sample size data. The investigator studies the intrinsic difficulties of the discrimination problem by exploring asymptotic geometric structure of such data. Three main activities are proposed: a) the asymptotic inconsistency of leave-one-out cross-validation. The study is expected to explain why it shall fail when the number of variables greatly exceeds the number of observations; b) the effect of the relationship between the dimensionality and the sample size on the difficulty of discrimination task; c) a discriminant direction vector that only exists for the data with high dimension, low sample size.  The data points collapse on this direction vector and also are most separated by group labels. The investigator explores its various theoretical and empirical properties such as its optimality, uniqueness, and asymptotic performances.<br/><br/>Even though these topics are loosely related one another in their technical aspects, their goals are essentially the same: exploring the nontraditional and unique challenges in high dimension, low sample size discrimination. While it has been an actively researched area over recent years, however, understanding fundamental challenges of high dimension, low sample size problems is yet satisfactory. This research approaches this problem in a way that may be regarded atypical in a traditional sense, but is more relevant to the problem itself. The applications of proposed research include text document classification such as Spam email filter, medical imaging such as functional magnetic resonance imaging, and bioinformatics such as microarray gene expression and proteomics.<br/><br/>"
"0840795","Algebraic Statistical Models","DMS","ALGEBRA,NUMBER THEORY,AND COM, STATISTICS","07/01/2008","05/01/2009","Seth Sullivant","NC","North Carolina State University","Continuing Grant","Tie Luo","07/31/2010","$87,808.00","","smsulli2@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1264, 1269","0000, OTHR","$0.00","Algebraic statistics is concerned with the use of commutative algebra, algebraic geometry and combinatorics in statistical inference.  The connection between algebra and statistics arises from the fact that many statistical models for discrete random variables have the structure of algebraic varieties.  This underlying algebraic structure can be exploited to develop new tools for analyzing statistical data and also suggests new research directions in algebra and combinatorics.  <br/>The research undertaken by the PI concerns the study of the algebraic structure of statistical models.  In particular, the research focuses on the study of the ideals defining the statistical models as algebraic varieties, ways that the ideal generators and Grobner bases can be used as tools in statistical inference, and the development of algebraic techniques for studying and computing these defining ideals.  The particular models studied by the PI are log-linear models, phylogenetic models, and mixture models.  Among the varieties that arise in this study are toric varieties, determinantal varieties, secant varieties, and many new varieties which deserve further study.<br/><br/><br/>Statistical models for discrete data are increasingly used throughout the social and biological sciences.  Of particular note is the emergence of statistical techniques for the analysis of biological sequence data (i.e. DNA, RNA, and protein sequences).  These statistical models are families of probability distributions inside a finite dimensional space, and these families often have an algebraic structure.  The PI proposes to further his work exploring the algebraic structure of these statistical models, increasing the interaction between two important areas (algebra and statistics) in the mathematical sciences.   <br/>"
"0805722","Studies in Fractional Factorial Design","DMS","STATISTICS","08/01/2008","05/09/2010","Ching-Shui Cheng","CA","University of California-Berkeley","Continuing Grant","Gabor Szekely","07/31/2013","$260,000.00","","cheng@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","This project is concerned with how to choose good fractional factorial designs.  Minimum aberration is a well accepted optimality criterion for selecting the so called regular fractional factorial designs.  Using results from finite projective geometry and coding theory, the investigator continues his work on the determination of minimum aberration designs, in particular, those of resolution IV.  Resolution IV designs have nice structures and good statistical properties, but have not been well studied in the past, partly due to the lack of a good structural theory. Some recent results in finite projective geometry provide powerful tools for understanding the structures of resolution IV designs and for helping solve the problem of constructing optimal and efficient designs. The investigator also studies the construction of new orthogonal arrays of strength three, the nonregular counterpart of regular designs of resolution IV.  Finally, the investigator addresses several issues of designing experiments that involve multiple processing stages, ranging from formulation of optimality criteria to theoretical and algorithmic construction of good designs.<br/><br/>Statistical design of experiments is used in a wide range of scientific and industrial investigations. Experiments need to be properly designed so that valid information can be extracted at a lower cost. In industrial experiments, often a large number of factors have to be studied, but the experiments are expensive to conduct. In this case, only a small fraction of all the possible combinations of the factors can be observed, and how to choose a good fraction is an important issue. The study of such designs has received considerable attention, mainly due to the success in applications to experiments for improving quality and productivity in industrial manufacturing. This research is to study the construction of efficient designs to extract more information. Better industrial experiments can improve the quality of products and reduce production cost.  Experimenters will be benefited by having a greater repertoire of new and good designs at their disposal, and will be able to run their experiments more efficiently. For example, one of the proposed activities is concerned with experiments with multiple processing stages which often arise in industrial applications such as the fabrication of integrated circuits.<br/>"
"0805491","Statistical Methods for Integrated Gene Regulation Analyses","DMS","STATISTICS","07/15/2008","03/30/2010","Qing Zhou","CA","University of California-Los Angeles","Continuing Grant","Gabor Szekely","06/30/2012","$138,767.00","","zhou@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","0000, OTHR","$0.00","The fast development in statistical methodology is mostly driven by the necessity to describe, model and analyze complex large-scale data sets generated from various scientific and engineering disciplines. In order to make full use of available and incoming large amount of data in gene regulation, this proposal aims (1) to develop predictive modeling approaches to combine sequence analyses, gene expression data, and protein binding data; and (2) to develop a full Bayesian model for de novo identification of cis-regulatory modules (combinatorial patterns of multiple sequence motifs that mediate the interactions between regulatory proteins and DNA sequences) in multiple related species. For the first project, the use of many contemporary statistical learning methods is investigated, such as boosting, random forests, MARS and BART, for detecting influential sequence signals and predicting protein-DNA interactions. Multi-level models are proposed to incorporate the uncertainty in covariates into a statistical learning framework and efficient computational algorithms are developed for the inference. The statistical aspects of the second project involve modeling multiple interacting stochastic processes by coupling chains of random variables. Efficient algorithms that utilize two-dimensional dynamic programming and advanced Monte Carlo techniques such as tempering and equi-energy jumps are developed for the challenging Bayesian inference on the proposed model.<br/><br/>The proposed research is expected to have direct and immediate impact on various fields in molecular biology, genetics, and medical sciences, in which gene regulation analyses play critical roles. In addition to methodological development, algorithms and software will be delivered for biologists to use on their own experimental data. Many statistical components in these projects, such as the coupling of hidden Markov models and the design of advanced Monte Carlo sampling with dynamic programming, are expected to contribute significantly to statistics and other computational sciences as well.<br/><br/>"
"0803456","A Novel Model for Competing Risks Data with Masking","DMS","STATISTICS","06/01/2008","03/04/2010","Qiqing Yu","NY","SUNY at Binghamton","Continuing Grant","Gabor Szekely","05/31/2011","$194,999.00","","qyu@math.binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","MPS","1269","0000, OTHR","$0.00","A masked competing risks (MCR) model is a popular model for studying the lifetime (denoted by T) and the associated failure cause (denoted by C) of a J-component series system.  The system fails if one of its components fails.  The failure time might be censored and the failure cause might be masked. The investigator discovered that some of the assumptions used in the MCR modelare erroneous. In this project, the investigator proposes a new and a more realistic model, which is a significant improvement of the MCR model. Based on the new model, the PI proposes to study the parametric, non-parametric and semi-parametric estimation problems of the joint distribution of T and C.<br/><br/>The MCR data appear in numerous medical and industrial applications. For example, Dinse (1982) presents the MCR data of time until progression and the patient status at the time of progression for patients with glioblastoma (a cancer of the brain). The patient status  may not be identified. Reiser  et al.(1995) show that the MCR data arises from the testing of a particular type of IBM PS/2 computer.  The cause of failure of a computer may only be narrowed down to a set of possible causes. The estimation of  the joint distribution of T and C has a great impact in detecting the failure  cause efficiently.  The results of this research would provide a novel and realistic model for the MCR data and would provide statistical tools for analyzing the MCR data."
"0835552","Wavelet-based Statistical Modeling and Applications","DMS","STATISTICS","06/01/2008","06/09/2008","Marina Vannucci","TX","William Marsh Rice University","Continuing grant","Gabor Szekely","08/31/2010","$62,000.00","","marina@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","0000, OTHR","$0.00","This proposal summarizes current interests of the P.I. and future directions, in both research and education.  Topics all involve the development of wavelet-based methods and represent natural extensions of the P.I.'s previous work. The three main areas of interest are: (1) Bayesian Clustering of Functional Data. The objective is to develop novel Bayesian methods for clustering of functional data. The approach proposed by the P.I. is model-based and uses infinite mixture models together with the selection of wavelet coefficients describing discriminatory features of the data.  (2) Analysis of Protein Mass Spectra. The overall goal of the P.I. is to develop methodologies for extracting important features of proteomic data whileincorporating dimension reduction wavelet techniques. The P.I. has a growing interest in the area of Bioinformatics and has <br/>established collaborations with a number of investigators at Texas A&M.  (3) Wavelet-based Methods for Long Memory Data.  This project relates to the development of wavelet methods for time series modelling.  The P.I. plans to build on her previous work on long memory estimation and on change-point detection and to explore novel applications to functional Magnetic Resonance Imaging (fMRI) data. <br/><br/>The novel methodologies developed in this proposal constitute advances in the theory and practice of wavelet-based methods. Applications to data arising from interdisciplinary collaborations demostrate the practical usefulness of the proposed methods, and confirm the success of wavelets as a tool for analysing data. The proposed clustering methods are quite general and can be applied to a number of different contexts that involve functional data. The P.I. has previous experience with the analysis of data from studies involving Near Infrared spectra and of biomedical data. She has also established several collaborations in the area of Bioinformatics and plans to develop wavelet methods for the analysis of high-throughput protein mass spectra.  Broader impacts of this proposal are in the collaborative nature of the proposed research but also in its educational and training objectives and in its efforts to disseminate results. The P.I. is engaged in several collaborations with investigators in the life sciences, both at Texas A&M and at other universities. She continues her engagement in the mentoring of graduate students and in training activities. She also maintains an updated webpage on her research activities where papers and accompanying software are posted in a timely <br/>manner.<br/>"
"0806106","Collaborative Research: Spectral and Connectivity Analysis of Non-Stationary Spatio-Temporal Data","DMS","STATISTICS","09/01/2008","09/03/2008","Hernando Ombao","RI","Brown University","Standard Grant","Gabor Szekely","08/31/2010","$128,000.00","Crystal Linkletter","hombao@uci.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1269","0000, OTHR","$0.00","The focus of this research is the development of new statistical methodologies for modeling connectivity in non-stationary spatio-temporal data. The investigators will develop four specific methods and models which will be applied to data provided by t he investigator's collaborators. First, motivated by the need for more sophisticated methods to investigate complex dependencies between two time series (e.g., brain regions), the investigators will build tools for exploring non-linear and time-evolving dependence between signals using dynamic mutual information in the spectral domain. Second, the notion of spatially-varying and temporally-evolving spectrum will be made precise via a stochastic representation of non-stationary spatio-temporal processes. An asymptotic framework for consistent estimation and inference will be developed. Third, a general spectral model for connectivity in a multi-subject experiment via a latent network model will be formulated. The empirically-driven model will incorporate items such as stimulus types, exogeneous time series, and subject-specific random effects. Finally, to complement this exploratory approach for modeling spectral data and connectivity, the investigators will build a scientifically-motivated semi-parametric state-space model of effective connectivity using multi-subject data.<br/><br/>The overarching goal of this research is the development of new statistical methodologies for analyzing data that has both a time and space dimension. Spatio-temporal data are prevalent in many disciplines, including the environmental and soil sciences, meteorology and oceanography, neuroscience and the emerging fields of health and bioterrorism surveillance. The primary data source for the investigators is time-sequenced data of brain activity measured at many locations in the brain. These signals contain information on how the brain functions, how it responds to outside stimuli, and where synchronization of functionality occurs. The statistical models the investigators are developing help sift through this information, allowing for the detection of trends in brain functionality, and estimation of population- and individual-level differences in performance. The empirical nature of the models allows for data-driven confirmation and discovery of neuroscientific theory. The statistical models will also be predictive, aiding in the quest for personalized diagnosis and treatment of depression, anxiety, and other neurological conditions. While the statistical research is motivated by the investigators' ongoing collaboration with neuroscientists, there is a unified statistical theme applicable to many other areas of interest."
"0838278","2009 Workshop on Semiparametric methodology","DMS","STATISTICS","11/01/2008","11/03/2008","Michael Daniels","FL","University of Florida","Standard Grant","david stoffer","10/31/2009","$5,000.00","Ronald Randles, Linda Young","mdaniels@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","0000, 7556, OTHR","$0.00","Semiparametric methods continue to be an active research area,<br/>especially as computing resources and power grow. These methods are used<br/>for inference in models with both parametric and nonparametric<br/>components. Work in this setting ranges from semiparametric modeling and<br/>efficiency to semiparametric regression. The former involves making<br/>efficient inferences in the presence of infinite dimensional nuisance<br/>parameters. The latter corresponds to regression models with parametric<br/>errors and unspecified (nonparametric) regression functions. Fundamental<br/>work remains to be done in high dimensional missing data problems, both<br/>in the setting of frequentist and Bayesian inference and in estimating<br/>regression functions with the complications posed in many different<br/>areas of application. The workshop brings together leaders in the field<br/>of semiparametric methodology and young researchers. The invited<br/>speakers all are very distinguished individuals who have done<br/>outstanding work in semiparametrics and have co-authored three of the<br/>seminal texts. <br/><br/>Flexible modeling with few assumptions is an integral component of<br/>solving many scientific problems. Semiparametric methods are a way to<br/>accomplish this. Major advances have been made in the past thirty years.<br/>However, fundamental work remains to be done in high dimensional<br/>problems with incomplete data, in estimating regression functions with<br/>the complications posed in many different areas of application, among<br/>other areas. Applications for such work are wide ranging and include<br/>spatio-temporal models, capture-recapture models, and measurement error<br/>models in environmental applications, classification and testing in in<br/>genetics and genomics, and missing data models in longitudinal<br/>observational studies and clinical trials. The workshop provides an<br/>excellent opportunity to discuss the many recent significant<br/>developments in semiparametric methodology and to identify important<br/>problems and new research directions. <br/>"
"0805860","Development and Analysis of MCMC Algorithms and Computational Methods in Bayesian Sensitivity Analysis","DMS","STATISTICS","08/01/2008","07/30/2008","James Hobert","FL","University of Florida","Standard Grant","Gabor Szekely","07/31/2012","$179,992.00","Hani Doss","jhobert@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","0000, OTHR","$0.00","The use of Bayesian statistics in the applied sciences has increased dramatically over the last decade, largely because of the availability of Markov chain Monte Carlo methods to estimate the posterior distributions.  Along with this increased use, researchers are now routinely considering more complex models, for example hierarchical models with many levels and regression models with many potential predictors.  Consideration of more sophisticated models has two consequences.  Because the parameters live in larger spaces, there is a stronger need for the development of MCMC methods that give accurate estimates of the relevant posterior distributions and expectations for a given model, and there is a stronger need for methods for doing model diagnostics and selection.  An important component of this project is the development of tools for model assessment and sensitivity analysis.<br/>The investigators develop methods for efficiently calculating a very large number of Bayes factors, and plotting them.  They consider situations in which there is a set of models indexed by several continuous hyperparameters.  Calculation of the Bayes factors helps determine whether a subset of hyperparameter values constitutes a class of reasonable choices.  The investigators also develop a set of computationally efficient schemes for estimating the posterior expectation of a function of a parameter as the prior is varied continuously.  This enables users to determine which aspects of the prior have the biggest impact on the posterior.  Markov chains used in complex settings often involve enhancements designed to speed up convergence.  But little is known theoretically regarding the effect of these enhancements.  In this project techniques from operator theory are used to analyze the long-term behavior of Markov chains.  The operator theory framework provides tools to better understand the behavior of these chains and this understanding enables the development of results regarding the accuracy of estimates produced by these chains and also suggests ways to improve these chains or design better ones.  The investigators apply the theoretical results obtained from operator theory to very concrete problems of model selection and assessment.<br/><br/>Model selection in complex situations is an important and pervasive problem in scientific and medical research.  It includes in particular variable selection in regression, where a few important variables are to be selected from many candidates and used for understanding, prediction and decision making.  Different models can lead to different conclusions, with potential impact on public policy.  Whereas for frequentist methods there is available an extensive body of material for doing diagnostics, for Bayesian methods the methods that exist are much more limited.  The tools for Bayesian model assessment and sensitivity analysis, together with the theoretical results regarding Markov chains that will be obtained in this project, will enable researchers to correctly evaluate the accuracy of estimates produced from Markov chains that explore very large spaces, and will enable them to correctly determine how long chains need to be run in order to provide a required level of accuracy.<br/><br/>"
"0805865","Applications and Computational Issues Involving Generalized Linear and Mixed Models","DMS","STATISTICS","06/01/2008","05/14/2008","James Booth","NY","Cornell Univ - State: AWDS MADE PRIOR MAY 2010","Standard Grant","Gabor J. Szekely","05/31/2011","$149,829.00","","jb383@cornell.edu","373 Pine Tree Road","Ithica","NY","148502488","6072555014","MPS","1269","0000, OTHR","$0.00","The proposed research concerns theoretical and computational issues arising in the context of generalized linear and mixed effects models. A new multivariate model is suggested as a construct for analysis of covariance which provides a unified framework for adjusting treatment means in balanced and unbalanced design settings. Second, a new approximation for the number of contingency tables and tables of zeros and ones, meeting certain linear constraints, is proposed. This is relevant, for example, in determining the feasibility of exact conditional analysis of log-linear models. The approximation arises from a novel formulation of the problem in terms of a generalized linear model for geometric responses. The approximation is much more generally applicable, and appears to be far more accurate, than exiting competitors. Third, a practical fitting algorithm for a broad class of mixed effects models with analytically intractable likelihood functions is proposed. The approach involves an implementation of the Monte Carlo EM algorithm that uses a randomized spherical-radial integration rule at the E-step. Use of this integration rule reduces the required Monte Carlo sample size by two orders of magnitude in test cases.<br/><br/>Statistical models are ubiquitous in almost all areas of modern research, including such diverse fields as agriculture, economics, medicine, and sociology. Advances in computing power enable statisticians to consider models and do calculations that were not feasible even a few years ago. This research targets three problems related to widely-used statistical models. The first concerns a technique for adjusting treatment means in designed experiments to account for observed covariates related to the response of interest. This is a classical problem with its roots in agricultural field trials. The technique has a long history dating back to the mid-20th century. It is somewhat surprising then that there is still disagreement on the correct way to make the adjustments, even in simple balanced experiments. An explanation is that the mathematical tools and computing power necessary for a complete solution were not available when the method was first developed. The second problem relates to the feasibility of exact statistical tests when data is sparse, and the standard approximations break down. Exact methods are used, for example, in medical studies testing for factors associated with various diseases. Finally, a new fitting algorithm is proposed for an important class of statistical models. Test cases suggest that the methods will significantly extend the range of models for which the computations are practically feasible.<br/>"
"0806137","Construction and Analysis of Nonregular and Supersaturated Designs","DMS","STATISTICS","06/01/2008","05/29/2008","Hongquan Xu","CA","University of California-Los Angeles","Standard Grant","Gabor J. Szekely","12/31/2011","$120,000.00","","hqxu@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","0000, OTHR","$0.00","Nonregular and supersaturated designs are commonly used in screening experiments where the goal is to identify a few dominant factors among a large number of candidate factors.  The objectives of this proposal are to construct efficient nonregular and supersaturated designs and to develop new methodology for analyzing data from such experiments.  A general method is introduced for constructing nonregular designs from linear codes over a finite ring.  These nonregular designs have better statistical properties than the existing designs in the literature in terms of aberration, resolution and projectivity.  The concept of constant weight codes in coding theory is used for studying supersaturated designs. Linear programming technique is used to establish new upper bounds on the number of columns in a supersaturated design.  New algorithms are developed for constructing efficient nonregular and supersaturated designs for practical use.  A graphical procedure and an automatic procedure are proposed for screening active effects for nonregular and supersaturated designs.  Simulation shows that this method performs well compared to existing methods in the literature and is more efficient at estimating the model size.<br/><br/>Statistical design and analysis of experiments have been widely used in scientific and industrial research and development.  As science and technology have advanced to a higher level, investigators are becoming more interested in and capable of studying large-scale systems via computer simulations and high-performance computing.  Two fundamental questions are how to design an efficient experiment and how to analyze the data properly from the experiment.  This proposed research aims at developing novel methods for constructing optimal designs and for analyzing such experiments.  New efficient nonregular and supersaturated designs are constructed and will be made available online for broad and quick dissemination. The results of the proposed research can be quickly assimilated into graduate courses on design and analysis of experiments. This proposal employs a combination of mathematical and computational tools and emphasizes an important interdisciplinary connection between information theory and design theory.  The proposed research will lead to remarkable new advances in design theory and better practice in data analysis."
"0805975","Asymptotic Theory of Penalized Splines and Calibration of Computationally Expensive Models","DMS","STATISTICS","07/01/2008","06/30/2008","David Ruppert","NY","Cornell University","Standard Grant","Gabor J. Szekely","06/30/2012","$179,768.00","","dr24@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","0000, OTHR","$0.00","The investigator studies two areas. The first is the large-sample theory of penalized splines. The second is Bayesian calibration and uncertainty analysis for computational expensive models. The theory for univariate penalized splines<br/>is generalized to quadratic and higher degree splines and to higher order difference penalties. The univariate theory is extended to additive models and to bivariate regression. Penalized splines methods are developed for deconvolution with heteroscedastic measurement error. Bayesian calibration and<br/>uncertainty analysis is studied in the cases of spatial-temporal correlations, measurement error in environmental inputs, and multiple responses.<br/><br/>Splines are mathematical tools for defining curves and surfaces. Splines are used, for example, to describe the shape of an automobile body in computer-assisted design. In statistics, splines are used to describe curved relationships between variables. For example, a recent study of the<br/>relationship between blood-lead concentration and IQ in children used splines and found an unexpected and important result. The dose-response curve is steepest at low doses, meaning that, at low doses, IQ declines with increasing lead concentrations more rapidly than previously realized. This unanticipated finding implies that, if environmental lead concentrations are reduced, then the intellectual development of children will be improved by an amount exceeding what was previously thought. Splines are particularly useful in this<br/>type of study because they can be combined with adjustments for maternal IQ and other factors and for correlations between multiple IQ measurements on a single subject. The investigator studies splines to improve the precision of estimation and to extend the range of applicability. Calibration of complex<br/>models is used in a variety of applications including, for example, petroleum exploration and management of watersheds. The PI and his colleagues study the Cannonsville watershed, a source of drinking water for New York City. Calibration of a model means using data to estimate unknown parameters. Uncertainty analysis measures the precision of the estimates. Calibration and uncertainty analysis is needed so that these models can be used for management. For example, the model for the Cannonsville Reservoir helps NYC manage the watershed to maintain water so that expensive filtration systems are not needed."
"0754198","Travel grants for World Congress in Probability and Statistics July 14-19, 2008, Singapore","DMS","PROBABILITY, STATISTICS","02/01/2008","01/23/2008","Richard Durrett","NY","Cornell University","Standard Grant","Tomek Bartoszynski","01/31/2009","$20,000.00","","rtd@math.duke.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1263, 1269","0000, 7556, OTHR","$0.00","Travel support is requested for 20 U.S. participants (young researchers, women, and members of underrepresented minority groups) to attend the World Congress in Probability and Statistics, an international meeting to be held July 14-19, 2008, in Singapore. The conferences on Stochastic Processes and their Applications have become the principal annual international forum for researchers studying applied and theoretical problems in stochastic processes. Every four years that sequence of meetings is interrupted by a World Congress in Probability and Statistics, a much larger conference that bring together distinguished scholars from all over the world and provides a broad overview of current research in probability and statistics, and their applications.<br/><br/>An important part of the intellectual development of young researchers is to attend international conferences where they have an opportunity to listen to stimulating lectures and interact with probabilists and statisticians who work on a wide variety of topics. The World Congress in Probability and Statistics, which meets every four years, and will be held in Singapore in 2008 will feature one hour talks by about a dozen internationally known researchers and special sessions on 33 topics. Travel support is requested for 20 U.S. participants (young researchers, women, and members of underrepresented minority groups) to attend this important meeting. <br/>"
"0806134","Topics in Computer Experiments","DMS","STATISTICS, SPECIAL PROJECTS - CCF","07/15/2008","07/09/2008","Thomas Santner","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","Gabor J. Szekely","06/30/2012","$359,979.00","Angela Dean","santner.1@osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269, 2878","0000, OTHR","$0.00","This project will study problems that occur in the design of computer and physical experiments and the analysis of their output.  Physical experiments are the gold standard for measuring input-output relationships but these measurements that contain noise and possibly unrecognized sources of bias.  Computer codes that model input-output relationships are used increasingly in place of, or in conjunction with, physical experiments because of they can provide decreased lead times in the engineering design of manufactured goods as well as for the assessment of designs in varying field-use conditions and varying conditions of fabrication. In general a deeper understanding of product and process performance can be made by using a combination of computer and physical experiments. However, computer codes provide a biased measurement of input-output relationships because of the inevitably inadequate physics or biology used in their development. This proposal will address the following problems that arise from the challenges sketched above. 1. To increase the computational efficiency of screening methodology for identifying the most important inputs to a computer code. 2. To develop methodology for estimating the upper (or lower) percentile of a computer code output with respect to the distributionof the field inputs. 3. To estimate the set of Pareto optimal input values for multivariate computer output and the corresponding Pareto Frontier of output vectors corresponding to the Pareto optimal inputs. 4. To develop statistical methodology for simultaneous determination of calibration<br/>parameters and tuning inputs for computer models, including those with mixed quantitative and qualitative inputs.<br/><br/>Experimental modeling using computer codes is increasingly prevalent in engineering, in biomechanics, in the physical sciences, in the life sciences, in economics, and other areas of natural science such as the assessment of climate change and cosmology. Over the past twenty years, the use of computer codes as experimental tools has become increasingly sophisticated due to the fact that the number of inputs to such codes has steadily increased as well as the level of detail embodied by the codes.  Researchers can now vary both ``engineering'' inputs as well as inputs that describe the operating conditions in the model.  In addition, more detailed mathematical models of the input-output relationship as well as more accurate numerical solution of these models often produce codes that require 5-24 hours for a single run.  In addition, multiple codes are often used to describe different aspects of the phenomenon being studied. This project will study problems that occur in the design of experiments involving such computer codes including screening the important inputs to such codes to determine important inputs, the efficient use of code output for optimization and other problems, and the calibration of corresponding physical experiments to further improve their prediction accuracy."
"0806030","Variable Selection in High Dimensional Feature Space with Applications to Covariance Matrix Estimation and Functional Data Analysis","DMS","STATISTICS","06/01/2008","05/13/2008","Jinchi Lv","CA","University of Southern California","Standard Grant","Gabor J. Szekely","05/31/2011","$80,270.00","","jinchilv@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","1269","0000, OTHR","$0.00","Variable selection plays an important role in high dimensional statistical modeling which nowadays arises in many scientific investigations. The investigator studies variable selection techniques built upon the machinery of regularization for high dimensional statistical modeling, develops new approaches to variable screening for high dimensional feature space, and explores the applications of these techniques to high dimensional sparse inference. Three interrelated research topics are proposed for investigation. First, the investigator studies a unified approach to variable selection with penalized likelihood which simultaneously selects significant variables and estimates their regression coefficients, and develops sure screening techniques applicable to ultra-high dimensional feature space. Second, factor models are proposed to estimate large scale covariance matrices while variable selection techniques are invoked to construct the factors, and covariance selection is also investigated. Third, the investigator studies the applications of variable selection techniques to functional data analysis where the regression coefficient functions exhibit various kinds of sparsity.<br/><br/>The analysis of vast data sets now commonly arising in many scientific disciplines and engineering problems poses numerous challenges to statistical theory and methodology that are not present in smaller scale studies. A major goal of this proposal is to make methodological and theoretical contributions to the important and challenging topic of high dimensional variable selection. These new developments provide further understanding of various regularization methods in high dimensions, and allow scientists to analyze high dimensional data with efficient dimension reduction and increased interpretability. The challenges of high dimensionality arise from diverse fields of sciences and humanities ranging from genomics and health sciences to economics and finance. The proposed work on variable selection in high dimensions will not only help better identify factors that are important to, for instance, public health or market risk, but also benefit a broad range of scientists and researchers in various fields."

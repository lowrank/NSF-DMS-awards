"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"2210358","Change Point Detection for Data with Network Structure","DMS","STATISTICS","08/15/2022","07/07/2022","George Michailidis","FL","University of Florida","Standard Grant","Yulia Gel","10/31/2023","$300,000.00","","gmichail@ucla.edu","1523 UNION RD RM 207","GAINESVILLE","FL","326111941","3523923516","MPS","1269","1269","$0.00","Detecting breaks and anomalies in a mechanism that drives the generation of data represents a critical task, due to numerous applications in high-impact areas including health, social, and engineering sciences. This project aims to advance the state of the art of change point analysis for big and complex data, by developing a simple to implement, yet powerful, scalable algorithmic framework, thus providing new tools to examine high-dimensional, long streams for events of interest. The potential application domains of this project include but not limited to occurrence of seizure in brain connectivity data sets, coordinated market and other systemic failures in economic and finance data, and identification of orchestrated malicious activities in computer network streams. The developed algorithms and methodology will be implemented in open-source software, while curated data sets will be made available to the community for use in change point analysis investigations. The project will offer multiple unique opportunities for interdisciplinary research training of the future generation of statisticians and for further enhancement of diversity in mathematical sciences.<br/><br/>To achieve the stated goals, the project (i) develops a unified detection framework for change points in complex statistical models for network and high dimensional time streams and (ii) provides a rigorous theoretical analysis of their accuracy in the form of consistency, finite sample bounds, and asymptotic distributions for the change points and other model parameters. The framework leverages a simple, easy to implement two-step strategy, wherein the first step one selects windows of the time series of appropriate length and using a standard exhaustive search strategy identifies at most a single change point in each of them. In the second step, a second search based on a global information criterion is employed to eliminate spurious change points. The strategy exhibits linear complexity in time (and thus matches the fastest available in the literature), yet is simple to implement and theoretically analyze, in particular for complex statistical models that exhibit network and low rank structure. Further, the following issues are rigorously addressed: (i) conditions of identifiability of the model parameters and the change points and (ii) probabilistic guarantees and uncertainty quantification for them in the presence of high dimensionality, network structure, temporal dependence, as well as dependence across data streams.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210468","Best Subset Selection: Statistics Meets Quantum Computing","DMS","STATISTICS","08/01/2022","07/27/2022","Yuan Ke","GA","University of Georgia Research Foundation Inc","Standard Grant","Yong Zeng","07/31/2025","$127,176.00","","yuan.ke@uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","7203","$0.00","Recent developments in quantum computing have shown that quantum computers can outperform classic computers in specific problems. However, these problems are highly physics-oriented and are not appealing to the statistics and data science community. Natural questions arise, such as whether quantum computers will benefit the statistics community and what kind of statistical problems can be sped-up by quantum computers? There are significant challenges in developing quantum algorithms to solve statistical problems. First, a quantum computer does not provide deterministic results but gives a random result due to its intrinsic mechanism. Second, in general, existing quantum search algorithms depend on a function that can tell us if the solution is correct or not. For example, we can define a function to check if a Sudoku solution is correct or not, although such a function cannot help us directly solve the Sudoku problem. Unfortunately, we are not able to define this type of function for most statistics problems due to the randomness in observations. Third, the development of public available quantum computers is still prototypical. The capacity of the state-of-the-art quantum computer is far from enough to conduct big data applications. This project aims at developing a set of transformative quantum algorithms to bridge the gap between quantum computing and statistical learning. The principal investigator (PI) will investigate a series of well-defined research problems, including methodological validity, algorithm complexity, theoretically rigorous, and empirical versatility. Completing the project can invigorate statistical learning with powerful quantum computers and provide key insights into the new research area of quantum statistical learning. The PI plans to develop efficient quantum computing software packages to disseminate the results. The project will offer undergraduate and graduate students opportunities to participate in cutting-edge and interdisciplinary research. <br/><br/>Best subset selection has been a statistically attractive but computationally challenging problem. Solving it involves a combinatorial search over all subsets and hence is an NP-hard problem. In this project, the PI will establish a quantum statistical learning framework for the best subset selection problems by investigating three closely related research aims: (i) explore a novel non-oracular quantum search algorithm that achieves near-optimal computational complexity without requiring any oracle information of the true solution; (ii) develop an efficient quantum linear prediction algorithm through the compact singular value decomposition and estimates the inverse singular values by utilizing a recently developed quantum tomography technique; (iii) design a hybrid quantum-classical network structure to advance complementary advantages of quantum and classical computing by implementing computational demanding steps on quantum nodes and running capacity demanding steps on classical nodes. This project will benefit the subsequent studies in quantum algorithm development for solving statistical problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2152950","Collaborative Research: Empirical Frequency Band Analysis for Functional Time Series","DMS","MSPA-INTERDISCIPLINARY, CDS&E-MSS","09/01/2022","06/24/2022","Scott Bruce","TX","Texas A&M University","Standard Grant","Yulia Gel","08/31/2025","$126,408.00","","sabruce@tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","7454, 8069","1269, 1303, 5294, 9263","$0.00","Monitoring seasonal storms over time is an essential component of understanding long-term atmospheric trends and producing reliable seasonal forecasts. A well-established measure of the location and intensity of mid-latitude storms uses the temporal variance of wind velocity within a certain frequency band. There are two areas of large variance, extending from the East Coasts of Asia and North America out into the Pacific and Atlantic Oceans respectively, and the location of these ""storm tracks"" and their intensities vary from year to year. However, the measured configurations and strength of the storm tracks are sensitive to the choice of frequency band, and there are currently no data-driven techniques for identifying frequency bands that appropriately characterize trends in wind velocity variability. Understanding the relative strength of higher frequencies (cyclone growth and propagation) vs. lower frequencies (cyclone occlusion and decay) as a function of location and season, as well as long-term trends in the locations and characteristics of storm tracks, aids climate researchers in assessing spatial and time trends in atmospheric conditions. This project aims to develop data-driven procedures to enhance this effort by establishing optimal summary measures for characterizing differences in wind velocity trends among frequencies. The research team will develop, validate, and openly share software and analytical tools for adaptive frequency band estimation that adequately summarizes time-varying dynamics and accounts for the complex interaction between the spatial and temporal dependence in atmospheric conditions. In addition, the project includes professional training through a transdisciplinary program in atmospheric sciences and computational and theoretical statistics. The activities include supervision of doctoral theses and undergraduate capstone projects, as well as talks for local high school students interested in research at the intersection of statistics and atmospheric sciences.<br/><br/>The frequency-domain properties of nonstationary functional time series often contain valuable information. These properties are characterized through their time-varying power spectra. Practitioners seeking low-dimensional summary measures of the power spectrum often partition frequencies into bands and create collapsed measures of power within these bands. However, standard frequency bands may not provide adequate summary measures of the power spectrum. There is need for a standardized, quantitative approach to objectively identify frequency bands that can best summarize spectral information, which for nonstationary functional time series is especially challenging due to the high dimensionality. This project seeks to establish a new data-driven framework for adaptive frequency band estimation for nonstationary functional time series that adequately summarizes the time varying dynamics of the series and simultaneously accounts for the complex interaction between the functional and temporal dependence structures. The three specific aims associated with this effort are: (1) to develop methodology for local frequency band estimation of a nonstationary functional time series that best preserves nonstationary spectral information localized within the functional domain, (2) to develop new methodology for local frequency band estimation of a multivariate nonstationary functional time series that best preserves the joint nonstationary spectral information among multiple functional time series localized within the functional domain, and (3) to develop new approaches for multivariate frequency band estimation of a nonstationary functional time series that best preserves nonstationary spectral information localized within a multidimensional frequency domain. Theoretical validity of these procedures will be established, and computationally efficient estimation procedures will be designed to ensure scalability. Extensive simulation studies will be conducted to explore the empirical and computational properties of the new methods, which are expected to enhance understanding of hidden mechanisms behind storm dynamics and thereby contribute to enhanced resilience to adverse climatic events.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210372","Collaborative Research: Modernizing Mixed Model Prediction","DMS","STATISTICS, Secure &Trustworthy Cyberspace","09/01/2022","06/14/2022","Thuan Nguyen","OR","Oregon Health & Science University","Standard Grant","Yong Zeng","08/31/2025","$121,885.00","","nguythua@ohsu.edu","3181 SW SAM JACKSON PARK RD","PORTLAND","OR","972393011","5034947784","MPS","1269, 8060","025Z","$0.00","The information explosion in many areas of society, from medicine to economics and business to social media, has resulted in pressing questions for modern data science regarding subject-level knowledge, such as in precision medicine, focused marketing, family economics, and many other areas. These include effective methods for data analysis and prediction in important areas of application ranging from privacy protection via differential privacy (DP) to precision medicine and public health disparities focusing on the prediction of epigenetic markers, and to predictions with employment data from the U.S. Bureau of Labor Statistics (BLS). This project aims to develop and employ new methods known as mixed model prediction. Particularly, for the DP application, the investigators will apply the methods to the publicly released 2020 U.S. decennial census; for the BLS application the investigators will target questions regarding volatility during the ongoing COVID-19 pandemic that thus require robust modifications from traditional approaches. The research will be carried out in conjunction with collaborators who are immersed in a particular application area.<br/><br/>In this project, the investigators will focus on three major aims: 1) multivariate mixed model prediction (MMP) in genomic prediction problems where correlated DNA methylation markers reflect underlying disease biology and improved prediction accuracy is possible by borrowing strength across this multivariate structure; 2) MMP for differentially private (DP) data in which cluster or grouping identities are contaminated by design and not released to protect privacy; and 3) MMP with non-Gaussian random effects and errors, which greatly can expand the range of circumstances in which MMP can be applied beyond the classical normality assumptions that do not fit many modern datasets. The investigators will develop the required methodology for each aim, study the procedures theoretically, and carry out extensive empirical simulation studies to compare the new methods with other methods. Furthermore, the investigators will work closely with their collaborators in the subject fields on implementing the methods developed in this project to answering practical questions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210368","New Challenges in Statistical Inference with Regularized Optimal Transport","DMS","STATISTICS","07/01/2022","06/14/2022","Kengo Kato","NY","Cornell University","Standard Grant","Yong Zeng","06/30/2025","$270,000.00","Ziv Goldfeld","kk976@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","075Z, 079Z","$0.00","Driven by the abundance of data and computational advances, the application domain of statistical inference is ever-growing. Human-facing technologies, such as autonomous vehicles or robotic-assisted surgery, demand principled inference methods subject to rigorous performance guarantees. As many inference tasks reduce to comparing probability distributions, optimal transport theory ? which provides a powerful framework for doing so ? has emerged as a tool of choice for designing and analyzing inference methods. However, statistical optimal transport is bottlenecked by the curse of dimensionality, whereby quantitative results either deteriorate exponentially with dimension (for example, estimation rates) or are largely unavailable (for example, limit distributions, resampling, and more). To overcome this impasse, this project will explore modern regularization techniques for optimal transport distances and develop a comprehensive statistical theory to facilitate principled inference in high dimensions. This innovation is expected to have a strong impact on the broad application domain of statistical inference in industry, commerce, science, and society, by promoting principled implementations at scale backed by theoretical assurances. In conjunction, the educational component will provide rigorous training and diverse recruitment opportunities for students, along with a deliberate plan to promote collaborations between statistics and engineering communities working on optimal transport and related fields.<br/><br/>This project will explore three prominent optimal transport regularization methods: (1) smoothing via convolution with a chosen kernel; (2) slicing via lower-dimensional projections; and (3) convexification via an entropic penalty. These techniques preserve the virtuous structure of classic optimal transport but reduce its complexity, which opens the door to a scalable statistical theory. The research agenda will tackle key theoretical challenges concerning statistical inference with regularized optimal transport distances, encompassing empirical error rates, limit distributions, semiparametric efficiency, resampling methods, Berry-Esseen type bounds, and computational-statistical gaps. The developed theory will be leveraged to address various inference applications, including generative modeling, testing, vector quantile regression, and intrinsic dimension estimation. The project will result in the theoretical underpinnings of inference methods at scale based on optimal transport theory, providing guidance and insight for practical implementations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210337","Collaborative Research: Emerging Variants of Generalized Fiducial Inference","DMS","STATISTICS","09/01/2022","06/14/2022","Jan Hannig","NC","University of North Carolina at Chapel Hill","Standard Grant","Yong Zeng","08/31/2025","$160,000.00","","jan.hannig@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","Fiducial inference is an alternative framework to making statistical inference that opens doors to solve many important statistical challenges arising in various fields of science and industry. This project aims at exploring the evolution of the fiducial argument as a response to modern data science questions and techniques. Results of this research are expected to expand the understanding of the foundations of statistics and data science. Emphasis will be given to applications of the new ideas in forensic science, genomics, differential privacy, and spatial statistics. Graduate students, including members of underrepresented groups, will receive training through research involvement in the project.<br/><br/>Having given due consideration to areas of statistical inference where the fiducial approach is expected to lead to new and useful results, the project will conduct research in the following directions. (1) Since the analytic calculation of fiducial distributions for many practical questions is not feasible, the project will develop easy-to-implement algorithms to sample from generalized fiducial distributions. These algorithms will significantly improve the practical applicability of generalized fiducial inference (GFI) and serve as a starting point for developing new techniques for the theoretical study of GFI. (2) The project will undertake an in-depth investigation of fundamental issues of GFI so that it can be applied on manifolds. (3) The project will lay the groundwork to make GFI applicable to non-parametric problems. The flexibility of non-parametric models will provide a challenge to GFI that will have to be overcome by introducing additional constraints. (4) As an important application, the project will develop post-hoc calibration of the strength of evidence in forensic science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2231896","Interdisciplinary Workshop on Weather and Climate Extremes","DMS","STATISTICS","09/01/2022","08/03/2022","Whitney Huang","SC","Clemson University","Standard Grant","Yong Zeng","08/31/2023","$33,147.00","Brook Russell","wkhuang@clemson.edu","201 SIKES HALL","CLEMSON","SC","296340001","8646562424","MPS","1269","090Z, 1303, 5294, 7556, 9150","$0.00","This project is for the purpose of organizing and holding a workshop titled ""Interdisciplinary Workshop on Weather and Climate Extremes"" which will be held in Clemson, South Carolina on May 16-18, 2023. Recent years have seen an increase in economic losses due to climate and weather extremes, such as wildfires, heat waves, storm surges. Extreme value theory (EVT) provides a framework for studying weather and climate extremes. Compared to more traditional statistical methods, EVT is less well known to many geoscientists and statisticians. It is therefore critical that statisticians with specialized knowledge of EVT interact with geoscientists and applied statisticians working at the interface of statistics, atmospheric and hydrological sciences. This workshop have the following major aims: (i) to facilitate collaborations among statisticians, atmospheric and hydrological scientists; (ii) to present state-of-the-art methods; and (iii) to develop new talent in studying weather and climate extremes. The workshop will include several organized activities, including short courses, panel discussions, invited scientific presentations, a poster session, and team building and brainstorming exercises to establish new research directions and groups. <br/><br/>The program of the forthcoming Workshop on Weather and Climate Extremes features the following topics: i) Models for Spatial and Spatio-Temporal Extremes; ii) Using Climate Models to Project Future Extremes; iii) Detection and Attribution of Extremes; iv) Coastal Risk and Tropical Cyclones; v) Machine/Deep Learning and Extremes; vi) Linking Large-Scale Conditions and Extreme Events. This project will encourage the participation of graduate students, early-career researchers, and members of groups under-represented in statistics and geosciences. More details about the workshop can be found at: https://whitneyhuang83.github.io/WCE2023.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2152814","Methods for Nonlinear, Non-Gaussian, and Data-Driven Ensemble Data Assimilation in Large-Scale Applications","DMS","STATISTICS, MSPA-INTERDISCIPLINARY, CDS&E-MSS","08/01/2022","07/11/2022","Ian Grooms","CO","University of Colorado at Boulder","Standard Grant","Yulia Gel","07/31/2025","$149,999.00","","ian.grooms@colorado.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1269, 7454, 8069","1269, 1303, 5294, 9263","$0.00","A wide range of disciplines, from weather to epidemiology to reservoir management, depend on data assimilation, that is, a set of methods that combine incomplete and imperfect observations with a forecasting model to estimate and predict the state of a complex, evolving system. For large-scale systems such as those in weather forecasting, computational efficiency is essential, so the methods used often rely on a Gaussian approximation - assuming that properties are distributed according to a bell curve - because this approximation unlocks highly efficient algorithms. However, in practice many quantities of interest, from sea ice thickness to rain rates, are not described by a bell curve, and predictions can be inaccurate. This project aims to develop new algorithms that are not based on a bell curve approximation but that can still be used in large-scale applications where computational efficiency is crucial. This inherently interdisciplinary project will provide a multitude of opportunities for training and professional development of the next generation of statisticians and data scientists, with a particular focus on enhancing diversity and inclusion.<br/><br/>The new insight on which the research project is built is a novel representation of the Bayesian posterior distribution through the introduction of a new synthetic random variable. The Bayesian posterior can be represented as the expected value of the probability density of the state variable conditioned on the new variable, where the expectation is taken with respect to the posterior on the new variable. This insight enables the use of a two-step approach to sampling from the posterior. The first step is standard Bayesian sampling but with a lower dimensionality, while the second step is based on regression. The project will combine methods from low-dimensional Bayesian computation for the first step with generalized linear regression and/or machine-learning regression for the second step. A skeleton of the two-step approach is already implemented in the Data Assimilation Research Testbed (DART) software suite, and DART enables data assimilation with over 25 geoscientifically-relevant models including the National Water Model and the Community Earth System Model. The research findings will be implemented in a form of new advanced two-step non-Gaussian algorithms in DART, making them available to a wide range of users.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2201218","Workshops in Geometry and Mathematical Physics and in Probability and Statistics","DMS","PROBABILITY, GEOMETRIC ANALYSIS, STATISTICS","06/01/2022","12/08/2022","Siu-Cheong Lau","MA","Trustees of Boston University","Standard Grant","Joanna Kania-Bartoszynska","12/31/2024","$26,650.00","Steven Rosenberg, Michael Salins, Siu-Cheong Lau","lau@math.bu.edu","1 SILBER WAY","BOSTON","MA","022151703","6173534365","MPS","1263, 1265, 1269","7556","$0.00","This project funds participant support for the 11th and 12th annual Boston University/Keio University/Tsinghua University Workshops, with the 11th Workshop to be held June 27-July 1, 2022, at Boston University, focusing on Geometry and Mathematical Physics. The 2023 workshop will focus on Probability and Statistics. The funding is dedicated to travel and local expenses for advanced graduate students and recent PhDs based at US institutions outside the Boston area. The workshops will provide participants with the opportunity to hear advanced talks in their research fields, to present their own work in a professional setting, and to network with each other and with senior experts. Through their own sources of funding, Keio University and Tsinghua University faculty will bring similar participants to these workshops from Japan and China. As a result, these workshops strengthen research ties between the US, Japanese, and Chinese mathematics and statistics communities.<br/><br/>The workshops follow a format in which senior faculty lecture on recent research directions in the morning, and recent PhD recipients and graduate students present their own work in the afternoon. Poster sessions accommodate additional presentations, and each workshop includes a discussion section on open problems in the field and other issues of particular interest for early-career participants, such as career placement. The workshops provide websites with lecture notes and slides from the research presentations, which serve to disseminate these cutting-edge research results to the broad communities. More information can be found at the workshop website http://math.bu.edu/BKT2022.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2150112","Symposium on Statistical Innovation in the Era of Artificial Intelligence and Data Science","DMS","STATISTICS","05/15/2022","05/02/2022","Samuel Wu","FL","University of Florida","Standard Grant","Yulia Gel","04/30/2023","$17,000.00","Somnath Datta","samwu@biostat.ufl.edu","1523 UNION RD RM 207","GAINESVILLE","FL","326111941","3523923516","MPS","1269","7556","$0.00","This award supports the participation of graduate students, early-career researchers, and members of groups underrepresented in statistics in the ICSA 2022 Applied Statistics Symposium: Statistical Innovations in the Era of Artificial Intelligence and Data Science, held at the University of Florida, Gainesville, Florida, from June 19 ? June 22, 2022. The event serves as the official annual meeting of the International Chinese Statistical Association (ICSA) and provides the broader statistical community a unique opportunity to meet and exchange ideas. The main objective of the conference is to bring together both well established and junior researchers from around the nation and the world who are actively pursuing theoretical and methodological research in statistics, biostatistics, and data science as well as their applications. The conference will provide a forum for discussion of cutting-edge developments in statistical theory, methods, and tools for making accurate inference from complex and noisy data sources.  <br/><br/>The format of the conference includes multiple plenary talks, special invited talks, invited sessions, and contributed posters. The organizers strive to enhance diversity of speakers and maintain a healthy inclusive atmosphere. In addition, a special panel discussion will be organized on mentoring and career development, especially geared toward women and researchers from groups underrepresented in STEM. For students, the conference will provide opportunities such as a student paper competition and contributed poster sessions.  The website with details on the symposium is https://symposium2022.icsa.org/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210019","Statistical Methods for Analyzing Complex Structured and Count Data","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","09/15/2022","06/15/2022","Fang Han","WA","University of Washington","Standard Grant","Yong Zeng","08/31/2025","$200,000.00","","fanghan@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269, 7454","068Z, 075Z, 079Z","$0.00","How can treatment/intervention effects in a complex social environment be measured? How can single-cell data be used to diagnose complex diseases such as autism spectrum disorder? This project aims to address these questions by developing statistics and machine learning methods that enable robust, interpretable, efficient, and fast analysis of big datasets routinely produced in biology, neuroscience, social sciences, politics, and epidemiology. The project encompasses two main tracks: (1) structured analysis for large datasets, for which the goal is to devise methods that can make efficient use of the intrinsic structure of the possibly very high-dimensional data without having to estimate the structure first; and (2) analysis of large count datasets, for which the goal is to design robust nonparametric models and algorithms that can handle complex, likely heterogeneous, count data. The investigator also plans to mentor and support graduate and undergraduate students majoring in statistics and related fields and broaden the participation of underrepresented minority students.<br/><br/>This project will advance the current state of knowledge in big structured and count data analyses by putting forward two main tracks of studies. The first is centered on random graph-based statistical inference through nearest neighbors (NN) or minimum spanning tree. Two main working examples in this track are NN matching for inferring the average treatment effect and graph-based correlation coefficients to infer marginal and conditional dependence strength. The investigator aims to revise and generalize these two families of methods to boost their efficiency while maintaining their robustness and computational speed. The second is centered on nonparametric univariate or multivariate Poisson mixture models. The investigator aims to bridge heterogeneous count-valued mixtures to nonparametric models (e.g., fully nonparametric, shape-constrained, nonnegative matrix factorization-based, etc.) under the umbrella of heterogeneous mixture model-based inference. The investigator will explore and settle several theory, method, computation, and application questions in the two tracks. Some preliminary results made in the first track have already stimulated new work in the causal inference community, and the results produced from the second track are expected to help with the early diagnosis of autism.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210282","Collaborative Research: Novel modeling and Bayesian analysis of high-dimensional time series","DMS","STATISTICS","09/01/2022","07/27/2022","Anindya Roy","MD","University of Maryland Baltimore County","Standard Grant","Yong Zeng","08/31/2025","$109,999.00","","anindya@umbc.edu","1000 HILLTOP CIR","BALTIMORE","MD","212500001","4104553140","MPS","1269","","$0.00","Every aspect of modern life including economy and finance, communication, and medical records, is associated with large amounts of data on several measurements, often evolving over time. Understanding the progress over time, finding an intrinsic relationship among different variables, and predicting future observations are essential components of decision and policy-making. However, apparent relations between two variables can appear in data caused by their shared association with other components. The principal investigators (PIs) will develop a model to re-express the multi-dimensional time series in independent, one-dimensional, latent time series. The representation will explain the evolution of the data over time and the intrinsic relations present in the component variables. It can also help find a more accurate, efficiently computable prediction formula for future observations by pulling information across different components and time. The approach's simplicity and generality will make it widely applicable and adaptable to diverse fields in economics, finance, social sciences, communications, networks, neuroimaging, and others. The PIs plan to develop free software packages to disseminate the results. They are committed to supporting young researchers and promoting diversity through graduate student training and involvement in the REU program.<br/><br/>The developed framework is based on representing an observed multi-dimensional time series as a linear combination of several independent stationary latent processes. The individual latent time series are modeled flexibly with unspecified spectral densities. The PIs will study the conditional independence structure among component time series and the causality of the time series over the temporal domain using a Bayesian approach. They will put independent priors on individual spectral densities through a finite random series prior, and on the matrix of the linear transformation decomposed as a product of a sparse matrix and an orthogonal matrix, the former of which induces a graphical structure for conditional independence among component series. Through this representation, desirable stationarity and causality structures can be imposed. Decoupling through the Whittle likelihood approximation and Hamiltonian Monte-Carlo methods will allow efficient posterior sampling. The causality over nodal time series will be addressed by a Direct Acyclic Graph modeling of the residual process. The formulation seamlessly addresses a mixed frequency sampling situation, difficult to incorporate into competing methods. The developed framework efficiently addresses both temporal and nodal causality respectively by characterization in terms of the Schur-complementation and using a directed acyclic graph, allowing a natural interpretation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210637","Dependable Predictive Inference with Uncertainty-Aware Machine Learning","DMS","STATISTICS","08/15/2022","04/28/2023","Matteo Sesia","CA","University of Southern California","Continuing Grant","Yong Zeng","07/31/2025","$160,000.00","","sesia@marshall.usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","MPS","1269","079Z","$0.00","Complex statistical and machine learning models, including deep neural networks, are widely applied in many fields and they are becoming increasingly central to data-driven science, despite serious concerns about their reliability. These models cannot always be trusted, especially in sensitive and high-noise applications such as those found in genomics, as well as in all of those contexts in which machine learning predictions will affect people?s health or welfare. A crucial current limitation of machine learning models is that they may not adequately capture uncertainty and their predictions often tend to be overconfident. Further, machine learning models are known to sometimes reinforce latent biases hidden in the data, and thus they may lead to predictions that are systematically biased against certain groups of individuals. Finally, many statistical and machine learning models may perform well within the specific data set in which they are trained, but their predictions are not robust to changing data environments, such as those corresponding to the genetic analysis of individuals from populations with different ancestries. To address the above limitations, this research project will develop general methods for accurate, fair, and robust uncertainty estimation in machine learning. In the specific contexts of genomics, this work will lead to improved genetic risk prediction across human populations, facilitating further developments in personalized medicine, bridging health disparities across populations, and helping deepen our scientific knowledge of heritable diseases. This project will support education in statistical and machine learning research by providing training opportunities for graduate students. This project will also help promote diversity in statistical and machine learning research by helping support the investigator?s involvement with the Diversity, Inclusion, Access JumpStart initiative of the University of Southern California. In particular, the investigator will offer summer research opportunities focusing for undergraduate students on the topics of this project.<br/><br/>This research consists of three distinct but closely connected parts. The first part will develop novel conformal inference methods to train and calibrate uncertainty-aware machine learning models that are both accurate and reliable. This research will involve the development of novel loss functions and innovative stochastic optimization algorithms. The second part of this project will develop methods for training and calibrating uncertainty-aware machine learning models that treat individuals belonging to different groups fairly, carefully using hold-out observations to correct for possible algorithmic or data biases. The third part of this project will develop methods based on data holdout and conformal inference to construct predictive models that are more robust to possible shifts in the covariate distribution. These models will be able to leverage possible interactions among the available predictive variables and ultimately lead to powerful multivariate models of genetic risk for heritable diseases that may be relied on across different populations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210557","Discriminant Analysis in High-Dimensional Latent Factor Models","DMS","STATISTICS","08/01/2022","07/20/2022","Marten Wegkamp","NY","Cornell University","Standard Grant","Yong Zeng","07/31/2025","$180,000.00","","marten.wegkamp@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","079Z","$0.00","This research project concerns classification of high-dimensional features, an important part of statistical learning theory. The project will formulate high-dimensional latent factor models that have a low-dimensional, hidden structure to guarantee successful statistical classification performance based on suitable projections of the high-dimensional data. Results of this research are expected to advance understanding on how to achieve optimal classification. This project has important applications to recent advances in immunology and cancer studies, which revealed that hidden mechanisms can be directly connected to health outcomes. This project offers a principled way to analyze such high-dimensional datasets and will provide computationally efficient classification rules. The project will involve collaboration with computational biologists to validate the new models and methodology.<br/><br/>Specifically, this project constructs novel classifiers based on principal component analysis with a necessary debiasing part followed by linear discriminant analysis and develops their statistical and computational properties. This project focuses on study of the important subclass of tuning-free classifiers that interpolate the data, but still possess good predictive power. In addition, this research aims to develop minimax adaptive bounds for the excess misclassification error under general latent factor model specifications and to prove that the new methods achieve these bounds, thereby establishing their rate optimality. Finally, the usefulness of the new techniques will be demonstrated via applications to data from immunology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210563","Collaborative Research: Statistical Optimal Transport in High Dimensional Mixtures","DMS","STATISTICS","07/01/2022","06/16/2022","Florentina Bunea","NY","Cornell University","Standard Grant","Yong Zeng","06/30/2025","$199,999.00","","fb238@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","079Z","$0.00","This project studies high-dimensional mixture models, a class of statistical models that can be used to analyze data arising in linguistics, computational biology, and particle physics. This research project aims to define a new measure of distance between distributions that measures their similarity with respect to a mixture model and offers a principled way to compare, transform, and analyze high-dimensional data sets. As part of this project, the investigators will develop fast algorithms for estimating this distance and theoretical guarantees allowing this distance to be used for statistical inference.<br/><br/>Specifically, this project defines a sketched Wasserstein distance (SWD) and will develop its computational and statistical properties. The primary aims are to establish duality relations for this distance, develop computationally feasible estimators for SWD using both primal and dual formulations, and to study the rates of convergence of the new estimators. In addition, the research aims to develop lower bounds to establish the rate optimality of these estimators and establish distributional limits to allow for the construction of asymptotically valid confidence intervals. These tools will be applied to data in text analysis, systems biology, and high-energy physics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2147546","Collaborative Research: Fine-Grained Statistical Inference in High Dimension: Actionable Information, Bias Reduction, and Optimality","DMS","STATISTICS","07/01/2022","06/27/2022","Yuting Wei","PA","University of Pennsylvania","Continuing Grant","Yong Zeng","06/30/2024","$101,829.00","","ytwei@wharton.upenn.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","1269","079Z","$0.00","Emerging data science applications require efficient extraction of actionable insights from large and messy datasets. The number of relevant features often overwhelms the volume of data that is available, which dramatically complicates the statistical inference tasks and subsequent decision making. In the existing statistical literature, most of theory aims at understanding the average or global behavior of a statistical estimator in high dimensions. In many applications, however, it is often the case that the goal is not to explore the global behavior of a parameter estimator, but rather to perform inference and reasoning on its local, yet important, operational properties.  The techniques and methods developed in the project will further advance the interplay between a broad range of areas including high-dimensional statistics, harmonic analysis, statistical physics, optimization, complex analysis, and statistical machine learning. The project provides research training opportunities for graduate students.<br/><br/><br/>This project pursues fine-grained inferential procedures and theory, aimed at enlarging the uncertainty assessment toolbox for various low-complexity models in high dimensions. Focusing on a few stylized problems, this research program consists of four major thrusts: (1) construct optimal confidence intervals for linear functionals of eigenvectors in low-rank matrix estimation; (2) design fine-grained hypothesis testing procedures for sparse regression under general designs; (3) develop entry-wise inference schemes for principal component analysis with missing data; and (4) conduct reliable and adaptive statistical eigen-analysis under minimal eigen-gaps. Emphasis is placed on algorithms that are model-agnostic and fully adaptive to data heteroscedasticity. Addressing these issues calls for the development of new statistical theory that enables reliable inference for a broad class of local properties underlying the unknown parameters.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2301050","Iterative Algorithms for Statistics: From Convergence Rates to Statistical Accuracy","DMS","STATISTICS","10/01/2022","10/19/2022","Martin Wainwright","MA","Massachusetts Institute of Technology","Continuing Grant","Yong Zeng","06/30/2023","$259,825.00","","wainwrigwork@gmail.com","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","MPS","1269","","$0.00","Science, engineering, and industry are all being revolutionized by the modern era of data science, in which increasingly large and rich forms of data are now available.  The applications are diverse and broadly significant, including data-driven discovery in astronomy, statistical machine learning approaches to drug design, and decision-making in robotics and automated driving, among many others.  This grant supports research on techniques and models for learning from such massive datasets, leading to computationally efficient algorithms that can be scaled to the large problem instances encountered in practice. The PI plans to integrate research and education through the involvement of graduate students in the research, the inclusion of the research results in courses at UC Berkeley and in publicly available web-based course materials, as well as in mini courses at summer schools and workshops. This project will also provide mentoring and support for graduate students and postdocs who are female or belong to URM communities.<br/><br/><br/>Many estimates in statistics are defined via an iterative algorithm applied to a data-dependent objective function (e.g., the EM algorithm for missing data and latent variable models; gradient-based methods and Newton's method for M-estimation; boosting algorithms used in non-parametric regression).  This projectl gives several research thrusts that are centered around exploiting the dynamics of these algorithms in order to answer statistical questions, with applications to statistical parameter estimation; selection of the number of components in a mixture model; and optimal bias-variance trade-offs in non-parametric regression.  In more detail, the aims of this project include (i) providing a general analysis of the EM algorithm for non-regular mixture models and related singular problems, in which very slow (sub-geometric) convergence is typically observed; (ii) developing a principled method for model selection based on the convergence rate of EM, and to prove theoretical guarantees on its performance; developing a general theoretical framework for combining the convergence rate of an algorithm with bounds on its (in)stability so as to establish bounds on the statistical estimation error; and (iii) providing a complete analysis of the full boosting path for various types of boosting updates, including kernel boosting, as well as gradient-boosted regression trees, and to analyze the ""overfitting"" regime, elucidating conditions under which overfitting does or does not occur.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210860","Integrative approaches with applications in eQTL analysis and randomized trials","DMS","STATISTICS","09/01/2022","08/09/2023","Fei Xue","IN","Purdue University","Continuing Grant","Yong Zeng","08/31/2025","$125,000.00","","feixue@purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1269","068Z","$0.00","Multisource data frequently arise in many real data applications, where different data sources contain complementary information, but each has only limited samples. To make the best use of the limited data from different sources, there is a great need for integrative approaches to jointly analyze all the datasets instead of separately analyzing every single dataset. Motivated by this need, this project will develop new integrative statistical methods to improve parameter estimation accuracy and hypothesis testing power. The results of the project will empower researchers in scientific fields such as genetics, biology, and medicine that face multisource data problems. The project will provide a broad range of interdisciplinary training opportunities for undergraduate and graduate students, especially students from underrepresented groups. <br/> <br/>The technical goal of this project is to study the integration of multisource data in the following three aspects. First, the principal investigator (PI) plans to build an empirical Bayes regression model for genotype-expression association analysis integrating data from multiple tissues. Second, the project will improve the test of the genotype-expression association via borrowing shared information across genes. Third, the PI will develop a covariate-adjusted method for causal effects on high-dimensional outcomes. The developed methods and results in this project will deepen understanding of genetics and biology, yield a more powerful test of treatment or drug effects on patients, and hence foster significant biological and medical benefits.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210734","Nonparametric Methodology for Learning from People: Inference, Algorithms, and Optimality","DMS","STATISTICS","07/01/2022","06/07/2023","Ashwin Pananjady","GA","Georgia Tech Research Corporation","Continuing Grant","Yong Zeng","06/30/2025","$121,246.00","Cheng Mao","apm7@gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","","$0.00","Learning from people represents a new and exciting paradigm for research in statistics and data science and is useful both in understanding behavioral patterns (for example in marketing) and for informing interventions (for example in education). Data from humans can also be elicited to inform various downstream tasks, with crowdsourcing being routinely used to collect data across applications spanning bioinformatics, epidemiology, computer vision, and environmental modeling. A common characteristic of such data is its scale and noisiness; having flexible and interpretable models for such data is broadly useful in downstream decision-making. The investigators aim to develop and thoroughly study flexible classes of models and methods for drawing inferences from large-scale data collected from people in a variety of such contexts.<br/><br/>Specifically, the investigations will focus on developing three key facets of such nonparametric methodology: (a) Computationally efficient, assumption-lean methods to fit expressive models to data; (b) Equipping models to integrate application-specific information in a general-purpose fashion; and (c) Developing models and methods that accommodate dynamically varying data streams. The research formulates a host of theoretical and methodological questions whose solutions would constitute fundamental progress in nonparametric inference, while touching upon practical and timely issues such as fairness in ranking systems. This project will also provide training and research opportunities for the next generation of data scientists by encouraging them to model the entire data analysis pipeline, from data collection to inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2152998","Collaborative Research: Bayesian Residual Learning and Random Recursive Partitioning Methods for Gaussian Process Modeling","DMS","STATISTICS, MSPA-INTERDISCIPLINARY, CDS&E-MSS","08/01/2022","07/13/2022","Pulong Ma","SC","Clemson University","Standard Grant","Yulia Gel","10/31/2023","$180,000.00","","plma@iastate.edu","201 SIKES HALL","CLEMSON","SC","296340001","8646562424","MPS","1269, 7454, 8069","1269, 1303, 5294, 9263","$0.00","Rare natural hazards (for example, storm surge and hurricanes) can cause loss of lives and devastating damage to society and the environment. For instance, Hurricane Katrina (2005) caused over 1,500 deaths and total estimated damages of $75 billion in the New Orleans area and along the Mississippi coast as a result of storm surge. Uncertainty quantification (UQ) has been used widely to understand, monitor, and predict these rare natural hazards.  The Gaussian process (GP) modeling framework is one of the most widely used tools to address such UQ applications and has been studied across several areas, including spatial statistics, design and analysis of computer experiments, and machine learning. With the advance of measurement technology and increasing computing power, large numbers of measurements and large-scale numerical simulations at increasing resolutions are routinely collected in modern applications and have given rise to several critical challenges in predicting real-world processes with associated uncertainty. While GP presents a promising route to carrying out UQ tasks for modern emerging applications such as coastal flood hazard studies, existing GP methods are inadequate in addressing several notable issues such as computational bottleneck due to big datasets and spatial heterogeneity due to complex structures in multi-dimensional domains. This project will develop new Bayesian GP methods to allow scalable computation and to capture spatial heterogeneity. The new methods, algorithms, theory, and software are expected to improve GP modeling for addressing data analytical issues across a wide range of fields, including physical science, engineering, medical science, public health, and business science. The project will develop and distribute user-friendly open-source software and provide interdisciplinary research training opportunities for undergraduate and graduate students.<br/><br/>This project aims to develop a new Bayesian multi-scale residual learning framework with strong theoretical support that allows scalable computation and spatial nonstationarity for GP modeling. This framework integrates and extends several powerful techniques respectively arising in the literature on GP and that on multi-scale modeling, including predictive process approximation, blockwise shrinkage, and random recursive partitioning on the domain. This framework decomposes the GP into a cascade of residual processes that characterize the underlying covariance structures at different resolutions and that can be spatially heterogeneous in a variety of ways. The new framework allows for adoption of blockwise shrinkage to infer the covariance of the residual processes and incorporates random partition priors to enable adaptivity to various spatial structures in multi-dimensional domains. New recursive algorithms inspired by wavelet shrinkage and state-space models will be developed to achieve linear computational complexity and linear storage complexity in terms of the number of observations. The resulting GP method will guarantee linear computational complexity in a serial computing environment and also be easily parallelizable. This Bayesian multi-scale residual learning method provides a new approach to addressing GP modeling issues among spatial statistics, design and analysis of computer experiments, machine learning, and nonparametric regression.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210672","Collaborative Research: Use of Random Compression Matrices For Scalable Inference in High Dimensional Structured Regressions","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","06/15/2022","06/14/2022","Rajarshi Guhaniyogi","TX","Texas A&M University","Standard Grant","Yulia Gel","05/31/2025","$179,988.00","","rajguhaniyogi@tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269, 7454","068Z, 1269","$0.00","As the scientific community moves into a data-driven era, there is an unprecedented opportunity to leverage large scale imaging, genetic and EHR data to better characterize and understand human disease to improve treatment and prognosis. Consequently, analysis of such datasets with flexible statistical models has become an enormously active area of research over the last decade. To this end, this project plans to develop a completely new class of methods, which are based on the idea of fitting statistical models on datasets obtained by compressing big data using a well designed mechanism. The development enables efficient modeling of massive data on an unprecedented scale. While the motivation of the investigators comes primarily from complex modeling and uncertainty quantification of massive biomedical data, the statistical methods are general enough to set important footprints in the related literature of machine learning and environmental sciences. The overarching goal also includes the development of software toolkits to better serve practitioners in related disciplines.  Further, the projects will provide first hand training opportunities for graduate and undergraduate students, including female and students from minority communities, in state-of-the-art statistical methodologies and imaging/genetic/EHR data. By disseminating the outcome of the project among high school students in terminology that they can understand, the project can have far reaching effects to enhance public scientific literacy about statistics.<br/><br/><br/>Two crucial aspects of modern statistical learning approaches in the era of complex and high dimensional data are accuracy and scale in inference. Modern data are increasingly complex and high dimensional, involving a large number of variables and large sample size, with complex relationships between different variables. Developing practically efficient (in terms of storage and analysis) and theoretically ?optimal? Bayesian high dimensional parametric or nonparametric regression methods to draw accurate inference with valid uncertainties from such complex datasets is an extremely important problem. To offer a general solution for this problem, the investigators will develop approaches based on data compression using a small number of random linear transformations. The approach either reduces a large number of records corresponding to each variable using compression, in which case it maintains feature interpretation for adequate inference, or, reduces the dimension of the covariate vector for each sample using compression, in which case the focus is only on prediction of the response. In either case, data compression facilitates drawing storage efficient, scalable and accurate Bayesian inference/prediction in presence of high dimensional data with sufficiently rich parametric and nonparametric regression models. An important goal is to establish precise theoretical results on the convergence behavior of the fitted models with compressed data as a function of the number of predictors, sample size, properties of random linear transformations and features of these models. The approaches will be used to study neurological disorders by combining brain imaging data, genetic data and electronic health records (EHR) data from the UK Biobank database. The project will also contribute on a broader front to advancing the interdisciplinary research training and broadening participation in statistical sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210891","Testing and Deep Learning for Functional Data","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","07/01/2022","05/20/2022","Jane-Ling Wang","CA","University of California-Davis","Standard Grant","Yulia Gel","06/30/2025","$299,997.00","","janelwang@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269, 7454","068Z, 079Z, 1269","$0.00","The proposed research involves two distinctive fields, functional data analysis (FDA) and deep learning. Functional data are random functions, which have become increasingly common due to technological advances to handle massive data.  Examples include climate or air pollution data collected over a period of time. The field has emerged as a mainstream research area, but the literature is mainly focused on estimation problems and has not yet leveraged the advantages of deep learning methods. This project aims to fill these gaps. It includes several new tests for functional data and employs deep learning, instead of the conventional nonparametric smoothing methods, to handle functional data. The proposed approaches will be applied to various functional data, including evaluating the effect of pollutants on lung cancer mortality and explaining the effects of physical activity on health.  A major emphasis is the development of new theory and algorithms. Computer code associated with the research will be publicly disseminated as R- or Python packages. The research findings will be incorporated in graduate curricula, undergraduate research projects, and short courses at workshops.  They will also be presented at professional meetings. Student researchers will receive training in research, computing and communication skills.<br/><br/><br/><br/>Although functional data are intrinsically infinite dimensional, measurements are only available at discrete locations, which may vary from subject to subject. The number of measurement locations per subject can be small (sparse functional data) or grow with the sample size (intensely sampled functional data). The proposed research covers all types of sampling plans and employs, whenever feasible, a single platform that is universally applicable. Such an approach is important as it is not trivial to judge whether the sampling plan for a particular dataset is intense or sparse. It also has the merit that the theory is unified and automatically reveals the phase transitions of the convergence rates of the corresponding estimators. Project 1 (Hypothesis Testing for Functional Linear Models) aims at developing a general framework for hypothesis testing under the setting of functional linear models.  Existing methods focus on testing a specific null hypothesis using a tailored test and are not well suited for testing the temporal duration of the effect of a functional covariate, such as the impact of PM2.5 on lung cancer. None of them has been shown to be optimal for a composite null hypothesis. We propose a single platform to test the null hypothesis that the regression coefficient of a functional covariate resides in a closed subspace of all possible coefficient functions.  The proposed test, which resembles the classical F-test, is simple and includes tests for global nullity, partial nullity and domain of the coefficient function as special cases. Project 2 (Testing Homogeneity and Independence for Functional Data) addresses the challenges of two fundamental tasks, testing the homogeneity (equal distributions) and independence of functional data. Such tests are infeasible when the functional process can only be sampled at a few discrete locations, a situation that is ubiquitous in longitudinal studies. For each task, we propose a customized version, marginal homogeneity or marginal independence, that has practical implications and is feasible for theory and implementation. Project 3 (Deep learning for Functional Data) aims at bringing the success of deep learning to bear with functional data. Surprisingly, the application of deep neural networks to functional data has been scarce and remains an open problem. A recent approach, developed by a team led by the PI, uses neural networks to search for the optimal basis functions to represent a functional input that automatically adapts to the prediction task in hand. We propose to expand the reach and theoretical understanding of this adaptive basis approach. Another objective is to design new methodology to impute partially observed functional data that uses Transformers, a deep neural network that transforms a given sequence of elements, such as the sequence of words in a sentence, into another sequence. The project will offer a broad range of new opportunities for interdisciplinary training of a future generation of statisticians and will contribute to enhancing a more inclusive atmosphere in statistical sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210104","Matching and Data Integration","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","07/01/2022","06/15/2022","Zongming Ma","PA","University of Pennsylvania","Standard Grant","Yong Zeng","10/31/2023","$239,925.00","","zongming.ma@yale.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","1269, 7454","068Z","$0.00","Demand for matching and integrating datasets arises in a wide range of application fields where the data collection process is distributed and cumulative, where data related to different aspects of a complex system must be collected separately through different protocols, and where anonymity is maintained in data communication. There is a great need to develop efficient algorithms and sound theoretical understanding for matching in these settings. Ideally, these developments should be based on concrete application scenarios, such as those arising in single-cell biology and privacy-aware social network analysis. The project will provide modeling, methods, theory, and software implementations for matching and data integration to researchers in the broader scientific community, including but not limited to cell biology, psychology, telecommunications engineering, and medicine. These methods will be especially attractive to medical researchers and cell biologists as they bear the potential of unleashing the full power of single-cell data these researchers have accumulated over time with substantial human resources and financial costs. This project will also make contributions to human resource development. The investigator will focus on improving diversity in statistics and data science research through active recruitment of students to work on the project.<br/><br/>This project will pursue three progressively more challenging goals. The first is to study the matching of two datasets with partially overlapping features, emphasizing the benefit of including non-overlapping features. The second is to develop methods for matching two datasets with disjoint feature sets. They jointly serve as preparatory steps toward the third goal: to develop theoretically and/or empirically justifiable pipelines for matching more than two datasets and integrating them into a single dataset to be used in downstream analyses. The developed methods and pipelines will be benchmarked and validated on real data through close collaborations with field experts and research labs at Stanford University, the University of Pennsylvania, and the University of North Carolina.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210913","Innovations in Statistical Methodology and Applications to Economics, Engineering, Health, and Medicine","DMS","STATISTICS","09/01/2022","06/20/2023","Tze Lai","CA","Stanford University","Standard Grant","Yong Zeng","05/31/2023","$0.00","","lait@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","075Z, 079Z","$0.00","The new millennium has witnessed the Big Data and Multi-cloud era which poses new challenges and opens up new opportunities for the mathematical (including statistical, computational, and data) sciences and their interactions with engineering, economics, finance, health and medicine. A long-term objective of this project is to develop innovative statistical methodologies and combine them with technological advances for resolving fundamental problems in these fields. Results of the project will have importance and relevance in personalized medicine, health and recommender systems, optimal dose-finding designs, reproducibility of scientific results under complex experiments, and on the foundations of machine and deep learning.<br/><br/>The project is broadly divided into four areas. The first is the theory on implementation of nonparametric contextual bandits, with novel applications to personalized medicine and health and recommender systems. The second is nature-inspired metaheuristic optimization in artificial (machine) intelligence, particularly the solution of the long-standing open problem concerning on-line optimization of the tuning parameters of the metaheuristic algorithm in complex high-dimensional settings. One of its applications is optimal design of dose-finding trials in master protocols, which are studied in the third area of the project. The third area covers valid and efficient post-selection multiple testing in biomedicine and information technology in the big data era, for which some machine learning/feature engineering/variable selection algorithm is typically used to extract features/variables for subsequent hypothesis generation and statistical testing. The project will address the reproducibility issues and ?replication crisis? with this data-dependent choice of features and hypotheses for statistical inference data by resolving foundational issues concerning valid post-selection inference. It also covers precision-guided drug and vaccine development and master protocols for early-phase and confirmatory clinical trials. Also covered are innovative study designs and analyses of point-of-care trials and observational studies, and development of mobile health platforms and wearable devices to improve and facilitate evidence-based management of chronic diseases. The fourth area is the statistical foundation of deep learning and provides the mathematical theory of convolutional neural networks and gradient descent, and applications to medical imaging and neuroscience. It also develops a novel Markov chain Monte Carlo method and closely related efficient adaptive particle filters in nonlinear state space models that have far-ranging applications in engineering and economics. The broader impacts of the project includes (i) direct applications in engineering, finance, insurance, risk management and surveillance, and (ii)  developing new advanced courses and revising the curriculum in financial and risk modeling, statistics and data science, and clinical trials and biostatistics, which could positively impact the training of graduate and undergraduate students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210505","Collaborative Research: Towards designing optimal learning procedures via precise medium-dimensional asymptotic analysis","DMS","STATISTICS","08/01/2022","06/13/2022","Haolei Weng","MI","Michigan State University","Standard Grant","Yong Zeng","07/31/2025","$124,826.00","","wenghaol@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","079Z, 1269","$0.00","In the past decade, data science and artificial intelligence (AI) have successfully addressed some of the most important scientific and engineering challenges faced in various domains of life like healthcare, education and autonomous systems. A recent example is the deep learning program AlphaFold developed by Google?s DeepMind which can predict a protein?s 3D structure from its amino acid sequence with accuracy competitive to experiment. Despite remarkable progress made in recent years, the design of efficient statistical learning procedures for big data ? a core component in modern data science and AI, has remained ad-hoc, and the precise theoretical understanding of such designed learning schemes is in its infancy. In particular, the following fundamental questions have remained open: (i) how to gain actionable insights into the performance of a learning algorithm without computationally demanding experimental methods? (ii) what is the optimal learning procedure in a given data-intensive environment? Answering such questions will pave the way for the development of the next generation of data science and AI, ultimately contributing to a better quality of life. This project aims to develop a novel analysis approach that can address the above challenges. The new framework is expected to establish quantitatively precise characterizations of the performance of diverse learning algorithms and provide a general recipe for designing optimal learning procedures.<br/><br/>Most of the state-of-the-art learning systems consider sophisticated models in which the number of parameters, p, is substantially large. In most cases p is either much larger than or comparable to the number of observations, n, in the data in use. This new routine has challenged our theoretical understanding of ubiquitous statistical models and procedures in science and technology. On one hand, classical analysis techniques based on the assumption that n is large and p is much smaller than n do not provide valid predictions for statistical learning in the aforementioned contemporary scenarios. On the other hand, modern non-asymptotic analysis frameworks which have been very successful in order-wise risk characterizations, often fall short of delivering sharp results. Hence, it remains largely unclear how to solve various learning problems in an optimal fashion for a variety of statistical models. This project aims to fill this gap by providing a precise theoretical understanding of a large family of statistical models including generalized linear models as a subset. It focuses on the medium-dimensional regime where p scales linearly with n, and creates new tools for studying the accuracy of different learning schemes and characterizing the optimal performances. The expected outcomes of this project are: (i) discovering the precise performance limits of a broad class of learning methods and (ii) evaluating the gaps between information-theoretic lower bounds and performance of the existing algorithms. Such results will ultimately shed light on the design of optimal learning procedures. The proposal will also provide numerous opportunities for interdisciplinary research training and professional career development of future generation of statisticians.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2230797","Collaborative Research: High-Dimensional Decision Making and Inference with Applications for Personalized Medicine","DMS","STATISTICS","01/01/2022","08/13/2022","Xingyuan Fang","NC","Duke University","Continuing Grant","Yong Zeng","05/31/2024","$160,000.00","","xingyuan.fang@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","079Z","$0.00","With the advent of data collection and storage technology, researchers can obtain large-scale and high-dimensional datasets at a low price. Such datasets offer exciting opportunities to make better decisions and reveal new discoveries to improve decision making in various applications, and meanwhile, also raise statistical challenges. Over the past decades, regularization methods such as Lasso, SCAD, and MCP have been proposed to conduct model estimation in the presence of high dimensional covariates. Various numerical algorithms have been developed for these methods, and their theoretical properties are well studied. However, questions of how to efficiently and effectively utilize high-dimensional data to make optimal decisions and conduct inference are relatively less studied, although such problems are of vital practical importance. This project will develop new methods and theories for making optimal decisions and conducting valid inference under high-dimensional settings. The methods have wide applications, for instance, in personalized medicine where the goal is to determine the optimal treatments for a patient based on predictor information, including several thousand genetic markers.  The principal investigators will develop and distribute user-friendly open-source software to practitioners and provide training opportunities to students at different levels.  <br/><br/>The project has three research aims. The first aim is to study the high-dimensional contextual bandit problem with binary actions, which is an online decision-making problem that finds applications in personalized healthcare and precision medicine. In this problem, the player sequentially chooses one action and observes a reward, where the goal is to maximize the reward. The principal investigators will develop a new algorithm to provide an optimal decision rule, which achieves the minimax optimal regret. The second aim is to study general inference problems that arise from high-dimensional stochastic convex optimization, where the goal is to quantify the uncertainties of the optimal objective value. The third goal is to consider the general stochastic linear bandit problem with a finite and random action space. The principal investigators will develop a new algorithm by using a best-subset-selection type estimator, and the approach achieves a ""dimension-free"" regret and meets existing lower-bound under the low-dimensional setting.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2208892","Dynamic Modeling and Risk Prediction with Complex Observational Semi-Competing Risks Data","DMS","STATISTICS","09/01/2022","06/17/2022","Hong Zhu","TX","University of Texas Southwestern Medical Center","Standard Grant","Yong Zeng","08/31/2025","$174,995.00","","hzhu2m@virginia.edu","5323 HARRY HINES BLVD","DALLAS","TX","753907208","2146484494","MPS","1269","","$0.00","Observational studies using secondary data sources, such as registry, claims, and electronic health records, are primary research tools to assess treatment effects and predict disease outcomes in real-world settings. Observational data, however, often present many complexities, for which substantial methods development is needed to obtain valid results. Particularly, semi-competing risks data arise when a terminal event (e.g., death) can prevent the observation of a non-terminal event (e.g., cancer recurrence), but not vice versa. The analysis of such data is further complicated with clustered outcomes, time-varying treatment effects, confounding, and dynamic prediction. This project will develop novel dynamic modeling and risk prediction methods with complex observational semi-competing risks data. The project is motivated by cancer studies. The developed methods will also be broadly applicable in other health conditions, reliability studies, and social science, where such data commonly arise. The investigator will integrate research and education by training graduate students, designing advanced topic courses, and engaging underrepresented minority students. The investigator will also develop open-source, user-friendly software packages in R to disseminate the results.<br/><br/>The project has three research aims. The first aim is to develop a copula-based, time-varying coefficient, random-effect model for multilevel semi-competing risks data. The second aim is to develop a propensity score matching based method to control for confounding in multilevel observational semi-competing risks data. The impact of omitting unmeasured confounders will be studied. The third aim is to develop a novel dynamic risk prediction tool for non-terminal and terminal events with such data. Unlike traditional prediction models, the developed model will utilize data on patients? dynamic disease progression and characteristics. The investigator will derive large sample properties of the new estimators, conduct simulations for evaluation, and apply the methods to analyze real-world data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210662","Methodology for Qualitative Constraints in Semi-Parametric Models","DMS","STATISTICS","09/01/2022","08/08/2022","Arun Kuchibhotla","PA","Carnegie-Mellon University","Standard Grant","Yulia Gel","08/31/2025","$200,000.00","Rohit Patra, Eric Tchetgen Tchetgen","arunku@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","1269","$0.00","Qualitative constraints such as concavity (law of diminishing returns) are ubiquitous in social sciences and economics. In order to incorporate these important constraints into statistical modeling, users often resort to simpler models, for example, linear regression. However, these simpler models are inflexible and cannot fully explain complex scientific phenomena. In turn, semi-parametric models provide necessary flexibility and interpretability. However, in fitting these semi-parametric models, qualitative constraints are often left unexploited. Ignoring these constraints, will not only lead to a loss in interpretability but also forgo some accuracy in the performance of the estimates. The broad goal of this proposal is to develop new statistical methods that respect subject matter qualitative constraints and make such methods more accessible to researchers via open-source software implementation.<br/><br/>This project has three main aims: (1) to develop general non-parametric regression estimators that account for available subject matter constraints and adapt to the smoothness of the underlying truth; (2) to explore systematic approaches for semi-parametric estimators that incorporate naturally occurring shape constraints on the nuisance components; and (3) to assess improved doubly robust estimators of functionals that can be represented in terms of variationally dependent nuisance parameters, whose relationship is shape-constrained on a subject matter basis. <br/>This in return would allow for significant improvement of estimation accuracy, thereby outperforming the existing tools that do not incorporate such information. The results of the project will find utility in addressing the interpretability and reproducibility concerns that have recently emerged in a broad range of domain knowledge disciplines, from social sciences to economics to epidemiology. The project will offer a multitude of opportunities for research training and professional development of the next generation of statisticians and will also engage in bolstering diversity in statistical sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210469","Methods and Theory for Estimating Individual-Specific and Cell-Type-Specific Gene Networks","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2022","06/13/2022","Emma Jingfei Zhang","FL","University of Miami","Standard Grant","Yong Zeng","06/30/2023","$199,999.00","","jingfei.zhang@emory.edu","1320 S DIXIE HWY STE 650","CORAL GABLES","FL","331462919","3052843924","MPS","1269, 7454","068Z","$0.00","In the existing literature on biological network analyses, most approaches assume a common or stratified network structure across subjects. Consequently, they are not able to flexibly account for heterogeneity in individual-level networks. For example, the individual-level networks may differ due to the complex effects of genetic variants, sex, and varying compositions across biological samples. Characterizing such network heterogeneity presents an urgent need for new statistical methodology and theory. Motivated by gene co-expression analyses, this project aims to make substantial progress in network analysis with heterogeneity. The developed methods can impact a wide range of topics in human genetics and genomics, precision health, and medicine; they are also more broadly applicable to scientific fields such as neuroscience, finance, and social science. The PI will integrate research into education by training undergraduate and graduate students and developing special topics courses.<br/><br/>This project aims to provide novel and fundamental perspectives on the emerging challenges in estimating high-dimensional covariances with heterogeneity. The first part of the project breaks new ground on estimating individual-specific graphical models. The PI will develop a new graphical regression model that relates the conditional dependence structure to covariates of high dimensions. The second part addresses the challenge in inferring cell-type-specific gene networks from aggregated data with different compositions. The PI will develop a flexible framework that does not make specific assumptions on the distributions of expressions and consider a novel least squares estimation. The developed methods in this project have appealing features, including identifiability and interpretability, efficient computation, quantifiable statistical errors, and valid statistical inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210796","Latent Dependence and Identifiable, Graphical, Deep Modeling of Discrete Latent Variables","DMS","STATISTICS","09/01/2022","07/28/2023","Yuqi Gu","NY","Columbia University","Continuing Grant","Yulia Gel","08/31/2025","$150,000.00","","yuqi.gu@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","079Z, 1269","$0.00","In the data science era, complex dependent and heterogeneous data emerge in various subject areas, from education to psychology to medicine. Latent variable models are powerful statistical approaches to tackle such complex data. However, existing statistical methods for analysis of latent variables are mostly limited to relatively simple settings and cannot meet the need for modern high dimensional applications. For example, one critical motivating example for this project is personalized learning, for which educators aim to diagnose individual students? latent strengths and weaknesses across many skills based on educational assessment data. In this scenario, it is highly desirable to make discrete statistical diagnoses about student?s fine-grained skills, to understand the relationships between various latent skills and the underlying cognitive processes, and to develop targeted remedial instructions. To achieve these goals, this project aims to develop a suite of new statistical tools for discrete latent variable modeling. The new statistical methodology is intended to apply not only to educational data, but also to data from psychology, medicine, genetics, and health sciences. The tools will be implemented in publicly available software. These research tools are expected to help practitioners to uncover hidden information about students, patients, and biological systems in a statistically principled manner. In addition, this project will provide multiple training opportunities for graduate and undergraduate students, introducing them to the important area of latent variable models in modern statistics.<br/> <br/>This project aims to advance the statistical theory and methodology of discrete latent variable modeling and providing novel statistical algorithms applicable to education and other applications. The project has three objectives. The first is to develop new mathematical machinery to study identifiability in general discrete models with latent and graphical components. These techniques will be used to derive sharp identifiability conditions for models motivated by education sciences. The second objective is to elaborate two new families of generative models with discrete latent variables: deep generative models with multilayer latent structures, and probabilistic graphical models encoding hard hierarchical latent constraints. Identifiability of these models will be established, which would guarantee the validity of statistical inference. The resulting models are expected to shed light on latent dependencies in several applications, particularly, in conjunction with educational diagnoses and personalized learning. The third objective is to develop novel hypothesis testing of identifiability, flexible Bayesian methods to simultaneously infer latent dimensions and other parameters, and efficient structure learning procedures to estimate the latent graphical constraints. The project will offer opportunities for professional development of trainees at the interface of statistics, data science, psychology, and educational sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210481","Copula-Based Methods for Multivariate Survival Analysis","DMS","STATISTICS, EPSCoR Co-Funding","08/15/2022","07/21/2022","Qian Zhou","MS","Mississippi State University","Standard Grant","Yong Zeng","07/31/2025","$150,000.00","","qz70@msstate.edu","245 BARR AVE","MISSISSIPPI STATE","MS","39762","6623257404","MPS","1269, 9150","9150","$0.00","Nonfatal disease-related hardship diminish quality of life. For example, cancer survivors often experience adverse health outcomes resulting from treatment or disease progression. Understanding the dependence among times to such events is crucial for designing effective care. This project will develop new statistical methods to model such dependencies and how the occurrence of these nonfatal events affects overall survival. The results of this research can inform new treatment strategies and improve the quality of life for many patients. This project will integrate the research with curriculum development and the training of graduate students and provide research opportunities for undergraduate and high school students.<br/><br/>The project leverages copulas, popular statistical tools for modeling the dependence among event times, to address three major aims. The first aim is to develop a likelihood-based goodness-of-fit test for detecting copula misspecification, which would lead to invalid estimation. The second aim focuses on missing event times caused by censoring, such as the termination of follow-up. New procedures will be designed to impute the missing event times via inter-event dependence. The third aim is to establish a vine copula framework to model the dependence among times to multiple nonfatal events and their relationship with time to death. The theoretical aspects of this research effort will be complemented by developing open-source software packages in R for use by the research community. This project is jointly funded by the Statistics Program and the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210208","Collaborative Research: Modernizing Mixed Model Prediction","DMS","STATISTICS, Secure &Trustworthy Cyberspace","09/01/2022","06/14/2022","Jonnagadda Rao","FL","University of Miami School of Medicine","Standard Grant","Yong Zeng","08/31/2025","$238,250.00","","js-rao@umn.edu","1400 NW 10TH AVE","MIAMI","FL","331361000","3052843924","MPS","1269, 8060","025Z","$0.00","The information explosion in many areas of society, from medicine to economics and business to social media, has resulted in pressing questions for modern data science regarding subject-level knowledge, such as in precision medicine, focused marketing, family economics, and many other areas. These include effective methods for data analysis and prediction in important areas of application ranging from privacy protection via differential privacy (DP) to precision medicine and public health disparities focusing on the prediction of epigenetic markers, and to predictions with employment data from the U.S. Bureau of Labor Statistics (BLS). This project aims to develop and employ new methods known as mixed model prediction. Particularly, for the DP application, the investigators will apply the methods to the publicly released 2020 U.S. decennial census; for the BLS application the investigators will target questions regarding volatility during the ongoing COVID-19 pandemic that thus require robust modifications from traditional approaches. The research will be carried out in conjunction with collaborators who are immersed in a particular application area.<br/><br/>In this project, the investigators will focus on three major aims: 1) multivariate mixed model prediction (MMP) in genomic prediction problems where correlated DNA methylation markers reflect underlying disease biology and improved prediction accuracy is possible by borrowing strength across this multivariate structure; 2) MMP for differentially private (DP) data in which cluster or grouping identities are contaminated by design and not released to protect privacy; and 3) MMP with non-Gaussian random effects and errors, which greatly can expand the range of circumstances in which MMP can be applied beyond the classical normality assumptions that do not fit many modern datasets. The investigators will develop the required methodology for each aim, study the procedures theoretically, and carry out extensive empirical simulation studies to compare the new methods with other methods. Furthermore, the investigators will work closely with their collaborators in the subject fields on implementing the methods developed in this project to answering practical questions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2202327","International Conference on Design of Experiments 2022","DMS","STATISTICS","07/01/2022","06/16/2022","Manohar Aggarwal","TN","University of Memphis","Standard Grant","Yulia Gel","06/30/2023","$9,900.00","Dale Bowman, E Olusegun George","maggarwl@memphis.edu","101 WILDER TOWER","MEMPHIS","TN","381523520","9016783251","MPS","1269","7556","$0.00","This award supports participation of junior researchers in the 6th International Conference on the Design of Experiments (ICODOE 2022) held at the University of Memphis during May 8-11, 2022. The principles of design and analysis of experiments form a fundamental part of research and development in areas as diverse as medical trials, pharmaceuticals, manufacturing, engineering, market research, agriculture, and many other fields. The conference will enable vital interaction between outstanding researchers in academia and industry who are advancing experimental design methodology with a view to improving systems for everyday life, from food delivery to cancer treatment. In addition, the conference will have a significant impact on the education of the next generation of researchers in experimental design, both junior researchers and those from underrepresented groups, by introducing them to innovative new methodology and new application areas, and by providing ample opportunity for these new researchers to interact with established researchers. <br/><br/>In the 21st century, the uses of experimental design have multiplied considerably. For example, the Design of Experiments (DOE), the science of pursuing knowledge through the process of efficient data collection, can enable progress in the search for more efficient and speedy artificial intelligence (AI) learning, which underpins the so-called ""fourth industrial revolution."" The techniques and principles that are fundamental to DOE have already found use in testing new algorithms. Sessions on reinforcement learning and machine learning at the ICODOE conference will introduce these topic areas to the DOE community and help to bridge knowledge between these communities. Other new developments in experimental design include its use in on-line experiments within networks, where lack of independence between treated and control groups is being tackled. Important advances on the design of experiments in the pharmaceutical industry have been made; conference sessions will present recent research dealing with COVID trial recruitment, oncology dose finding, and adaptive designs for use in clinical trials. In industry, the design of computer experiments and model calibration remain of high importance. Related to this is screening to determine the factors that most affect a response, and the use of sampling to analyze enormous data sets. In addition, traditional areas such as optimal design are being adapted to practitioner needs without losing efficiency. With sessions devoted to all these topics, the ICODOE conference will provide multiple unique interdisciplinary opportunities for early career researchers to form networks for future collaboration and will also actively bolster participation of underrepresented groups in mathematical and data sciences. Details on ICODOE 2022 can be found at https://www.memphis.edu/msci/icodoe22.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210392","Scientific Findings across Multiple Environments: Replication, Robustness, and Equity in Genetic Association Studies","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2022","07/21/2022","Chiara Sabatti","CA","Stanford University","Standard Grant","Yong Zeng","08/31/2025","$250,000.00","","sabatti@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269, 7454","068Z, 075Z, 079Z","$0.00","This project is motivated by the need to explore scientific questions of interest in multiple environments/populations and the challenges that this task presents both in terms of methods and communication of results. On the one hand, replicability is the cornerstone of science, and no finding can be considered part of the established corpus of knowledge unless the experiment/observation that led to it has been repeated under at least slightly different circumstances. On the other hand, overly substantial changes in environment might influence the very mechanism behind the original finding, so that, rather than invalidating it, variation of a result simply underscores the need to account for heterogeneity. Determining when the evidence accumulated in favor of a finding is enough to consider it corroborated and determining the variability of a pattern across different environments are challenging tasks. This research project aims to advance both the methods used to address these questions and the tools employed to communicate findings to the larger community of scientists and the public. The project includes outreach and communication activities, as well as training of graduate students through their involvement in the research. <br/><br/>The investigator and collaborators aim to develop new statistical methods to measure the replicability of a finding across multiple studies and to evaluate the uncertainty associated with it in different environments. They will leverage recent methodological developments, such as the notion of e-values, conformal prediction intervals, and knockoffs inference. Exploiting the links that have been established between robustness of findings and causal mechanisms, this project intends to provide users with predictive models for outcomes of interest that are interpretable, have been validated across variable conditions, and have appreciable out-of-sample performance.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210569","Collaborative Research: Modernizing Mixed Model Prediction","DMS","STATISTICS, Secure &Trustworthy Cyberspace","09/01/2022","06/14/2022","Jiming Jiang","CA","University of California-Davis","Standard Grant","Yong Zeng","08/31/2025","$139,678.00","","jiang@wald.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269, 8060","025Z","$0.00","The information explosion in many areas of society, from medicine to economics and business to social media, has resulted in pressing questions for modern data science regarding subject-level knowledge, such as in precision medicine, focused marketing, family economics, and many other areas. These include effective methods for data analysis and prediction in important areas of application ranging from privacy protection via differential privacy (DP) to precision medicine and public health disparities focusing on the prediction of epigenetic markers, and to predictions with employment data from the U.S. Bureau of Labor Statistics (BLS). This project aims to develop and employ new methods known as mixed model prediction. Particularly, for the DP application, the investigators will apply the methods to the publicly released 2020 U.S. decennial census; for the BLS application the investigators will target questions regarding volatility during the ongoing COVID-19 pandemic that thus require robust modifications from traditional approaches. The research will be carried out in conjunction with collaborators who are immersed in a particular application area.<br/><br/>In this project, the investigators will focus on three major aims: 1) multivariate mixed model prediction (MMP) in genomic prediction problems where correlated DNA methylation markers reflect underlying disease biology and improved prediction accuracy is possible by borrowing strength across this multivariate structure; 2) MMP for differentially private (DP) data in which cluster or grouping identities are contaminated by design and not released to protect privacy; and 3) MMP with non-Gaussian random effects and errors, which greatly can expand the range of circumstances in which MMP can be applied beyond the classical normality assumptions that do not fit many modern datasets. The investigators will develop the required methodology for each aim, study the procedures theoretically, and carry out extensive empirical simulation studies to compare the new methods with other methods. Furthermore, the investigators will work closely with their collaborators in the subject fields on implementing the methods developed in this project to answering practical questions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2152966","Collaborative Research: Empirical Frequency Band Analysis for Functional Time Series","DMS","MSPA-INTERDISCIPLINARY, CDS&E-MSS","09/01/2022","06/24/2022","Pramita Bagchi","VA","George Mason University","Standard Grant","Yulia Gel","08/31/2025","$250,669.00","David Straus","pbagchi@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","7454, 8069","1269, 1303, 5294, 9263","$0.00","Monitoring seasonal storms over time is an essential component of understanding long-term atmospheric trends and producing reliable seasonal forecasts. A well-established measure of the location and intensity of mid-latitude storms uses the temporal variance of wind velocity within a certain frequency band. There are two areas of large variance, extending from the East Coasts of Asia and North America out into the Pacific and Atlantic Oceans respectively, and the location of these ""storm tracks"" and their intensities vary from year to year. However, the measured configurations and strength of the storm tracks are sensitive to the choice of frequency band, and there are currently no data-driven techniques for identifying frequency bands that appropriately characterize trends in wind velocity variability. Understanding the relative strength of higher frequencies (cyclone growth and propagation) vs. lower frequencies (cyclone occlusion and decay) as a function of location and season, as well as long-term trends in the locations and characteristics of storm tracks, aids climate researchers in assessing spatial and time trends in atmospheric conditions. This project aims to develop data-driven procedures to enhance this effort by establishing optimal summary measures for characterizing differences in wind velocity trends among frequencies. The research team will develop, validate, and openly share software and analytical tools for adaptive frequency band estimation that adequately summarizes time-varying dynamics and accounts for the complex interaction between the spatial and temporal dependence in atmospheric conditions. In addition, the project includes professional training through a transdisciplinary program in atmospheric sciences and computational and theoretical statistics. The activities include supervision of doctoral theses and undergraduate capstone projects, as well as talks for local high school students interested in research at the intersection of statistics and atmospheric sciences.<br/><br/>The frequency-domain properties of nonstationary functional time series often contain valuable information. These properties are characterized through their time-varying power spectra. Practitioners seeking low-dimensional summary measures of the power spectrum often partition frequencies into bands and create collapsed measures of power within these bands. However, standard frequency bands may not provide adequate summary measures of the power spectrum. There is need for a standardized, quantitative approach to objectively identify frequency bands that can best summarize spectral information, which for nonstationary functional time series is especially challenging due to the high dimensionality. This project seeks to establish a new data-driven framework for adaptive frequency band estimation for nonstationary functional time series that adequately summarizes the time varying dynamics of the series and simultaneously accounts for the complex interaction between the functional and temporal dependence structures. The three specific aims associated with this effort are: (1) to develop methodology for local frequency band estimation of a nonstationary functional time series that best preserves nonstationary spectral information localized within the functional domain, (2) to develop new methodology for local frequency band estimation of a multivariate nonstationary functional time series that best preserves the joint nonstationary spectral information among multiple functional time series localized within the functional domain, and (3) to develop new approaches for multivariate frequency band estimation of a nonstationary functional time series that best preserves nonstationary spectral information localized within a multidimensional frequency domain. Theoretical validity of these procedures will be established, and computationally efficient estimation procedures will be designed to ensure scalability. Extensive simulation studies will be conducted to explore the empirical and computational properties of the new methods, which are expected to enhance understanding of hidden mechanisms behind storm dynamics and thereby contribute to enhanced resilience to adverse climatic events.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210371","Bayesian Learning for Spatial Point Processes: Theory, Methods, Computation, and Applications","DMS","STATISTICS","08/01/2022","06/14/2022","Guanyu Hu","MO","University of Missouri-Columbia","Standard Grant","Yong Zeng","07/31/2025","$151,210.00","","guanyu.hu@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269","1269","$0.00","Scientists, engineers, economists, and sports practitioners are increasingly aware of the importance of accurately understanding underlying clusters when trying to recover complex patterns that vary across time and space.  Examples of such patterns include earthquake occurrences over North America, tree locations in Barro Colorado Island, field goal attempts of professional players over basketball courts, and bullet-screen comments from live streams. When performing statistical analysis on such complex point process patterns, the scientific goals often involve either intensity estimation or cluster learning.  To help achieve the scientific goals, this project will develop methods to reveal hidden spatial homogeneity within spatial point processes and underlying heterogeneity among different univariate or multivariate processes. The project will advance knowledge within the statistical sciences and contribute useful tools to the work of government agencies, environmental scientists, social scientists, and practitioners in the sports industry. The project will also provide training opportunities to undergraduate and graduate students. <br/><br/>This project will fill the gap between nonparametric Bayesian methods and spatial point processes, including intensity estimation and heterogeneity learning for univariate and multivariate processes. The research will focus on three topics based on a nonparametric Bayesian framework with applications to different socio-economic problems. In the first topic, the investigator will construct a Markov constraint nonparametric Bayesian prior to learn the point process?s intensity surface of with spatial homogeneity. The investigator will develop a method for jointly estimating intensity surfaces and latent group information for multiple point processes in the second topic. Lastly, the investigator will develop a multivariate point process model with complex intensity function and latent group structure for each type of points.  The investigator will establish consistency and asymptotic distributions of the new estimators and develop e?icient algorithms together with publicly available software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210576","Collaborative Research: Design-Based Optimal Subdata Selection Using Mixture-of-Experts Models to Account for Big Data Heterogeneity","DMS","STATISTICS","08/15/2022","08/02/2022","John Stufken","NC","University of North Carolina Greensboro","Standard Grant","Yong Zeng","01/31/2023","$149,961.00","","jstufken@gmu.edu","1000 SPRING GARDEN STREET","GREENSBORO","NC","274125068","3363345878","MPS","1269","","$0.00","With technological advances, it has become easy to collect massive amounts of data for most areas of research. But with the size of datasets measured in terabytes or even petabytes, analyzing such datasets can become an expensive computational challenge and may be impossible on a typical desktop or laptop computer. However, for making impactful discoveries, it may be unnecessary to analyze an entire dataset. Consequently, there is great interest in developing and studying methods for selecting a subset from a massive dataset and for drawing conclusions based on the much smaller selected dataset. Such methods are known as subdata selection or subsampling methods. One obvious subsampling method consists of randomly selecting data from the entire dataset. While this is often the simplest and fastest option, it has been established that better options are often available. In this project, the principal investigators (PIs) aim to develop and study a rigorous framework and new methods for optimal subdata selection by using models that account for heterogeneity in the data, which is often present in large datasets. Research findings will be incorporated in topical courses to train graduate students in large-scale data analysis. The work will also be disseminated via the PIs? collaborations in public health, biomedical science, and business.<br/><br/>Rather than assuming a multiple regression model, the PIs plan to develop and study subdata selection methods based on mixture-of-experts (ME) models, which can account for heterogeneity in the data. The PIs will initially develop and study subdata selection methods for a subclass of the ME models, known as clusterwise linear regression models, for which the gate functions are constant. This will be followed by studying logistic-normal mixture models, in which the gate functions depend on the regression variables. For both cases, the investigators plan to develop information-based optimal subdata selection methods, first for continuous response variables and then for binary response variables, study their statistical properties, and develop efficient algorithms for the methods that will be made available in an R package.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210216","Numerical Construction of Optimal Estimators Using Machine Learning Tools","DMS","STATISTICS","09/15/2022","06/16/2022","Alex Luedtke","WA","University of Washington","Standard Grant","Yong Zeng","08/31/2025","$175,000.00","Marco Carone","aluedtke@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","079Z","$0.00","Optimal statistical procedures make maximal use of available data, making it possible to answer pressing scientific questions more precisely and cost-effectively. These procedures are traditionally derived via analytic calculations that require expert knowledge achieved over many years of training. In this project, the investigators will study two novel strategies for deriving optimal procedures. Compared to existing approaches, these strategies require more expertise in computational methods and less expertise in statistical theory. As a result, this project will broaden the pool of researchers who can develop optimal statistical procedures. If preliminary results support the strong performance of the new methods, the investigators will incorporate them into vaccine clinical trial data analyses. Through this project, the investigators will engage undergraduates in statistical research and advance the understanding of mentored graduate students.<br/><br/>The investigators will consider both local and global notions of optimality. The first strategy will use novel representations of the efficient influence function (EIF). The EIF is a critical ingredient for constructing asymptotically efficient estimators, particularly in nonparametric and semiparametric models. It also provides a principled approach to debias machine learning-based estimators to recover valid statistical inference. Unfortunately, the conventional approach for deriving the EIF involves advanced theory that is often not taught in statistical curricula. Additionally, in some problems, the EIF does not have a closed form, rendering its use difficult even for experts. The investigators will derive a novel representation of the EIF that lends itself to computerization and study how it can be used to derive novel asymptotically efficient estimators. The second strategy will use ideas from deep reinforcement learning, as used recently to build self-learning game playing algorithms with super-human performance, to adversarially learn (globally and locally) minimax optimal statistical procedures with computational tools. Except in simple cases, analytic calculations have thus far only been successfully used to derive estimators that are asymptotically minimax optimal. However, asymptotic optimality does not generally guarantee optimality in small samples. Existing works on numerically learning minimax optimal estimators use a Bayesian formulation of the minimax problem. This formulation results in learning schemes that are too computationally prohibitive to be applicable to most problems. This project will develop and study an alternative means to construct these estimators that can readily leverage massively parallel computing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210388","Collaborative Research: Emerging Variants of Generalized Fiducial Inference","DMS","STATISTICS","09/01/2022","06/14/2022","Thomas Chun Man Lee","CA","University of California-Davis","Standard Grant","Yong Zeng","08/31/2025","$170,000.00","","tcmlee@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","Fiducial inference is an alternative framework to making statistical inference that opens doors to solve many important statistical challenges arising in various fields of science and industry. This project aims at exploring the evolution of the fiducial argument as a response to modern data science questions and techniques. Results of this research are expected to expand the understanding of the foundations of statistics and data science. Emphasis will be given to applications of the new ideas in forensic science, genomics, differential privacy, and spatial statistics. Graduate students, including members of underrepresented groups, will receive training through research involvement in the project.<br/><br/>Having given due consideration to areas of statistical inference where the fiducial approach is expected to lead to new and useful results, the project will conduct research in the following directions. (1) Since the analytic calculation of fiducial distributions for many practical questions is not feasible, the project will develop easy-to-implement algorithms to sample from generalized fiducial distributions. These algorithms will significantly improve the practical applicability of generalized fiducial inference (GFI) and serve as a starting point for developing new techniques for the theoretical study of GFI. (2) The project will undertake an in-depth investigation of fundamental issues of GFI so that it can be applied on manifolds. (3) The project will lay the groundwork to make GFI applicable to non-parametric problems. The flexibility of non-parametric models will provide a challenge to GFI that will have to be overcome by introducing additional constraints. (4) As an important application, the project will develop post-hoc calibration of the strength of evidence in forensic science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2304767","Collaborative Research: Design-Based Optimal Subdata Selection Using Mixture-of-Experts Models to Account for Big Data Heterogeneity","DMS","STATISTICS","11/01/2022","12/08/2022","John Stufken","VA","George Mason University","Standard Grant","Yong Zeng","07/31/2025","$149,961.00","","jstufken@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1269","","$0.00","With technological advances, it has become easy to collect massive amounts of data for most areas of research. But with the size of datasets measured in terabytes or even petabytes, analyzing such datasets can become an expensive computational challenge and may be impossible on a typical desktop or laptop computer. However, for making impactful discoveries, it may be unnecessary to analyze an entire dataset. Consequently, there is great interest in developing and studying methods for selecting a subset from a massive dataset and for drawing conclusions based on the much smaller selected dataset. Such methods are known as subdata selection or subsampling methods. One obvious subsampling method consists of randomly selecting data from the entire dataset. While this is often the simplest and fastest option, it has been established that better options are often available. In this project, the principal investigators (PIs) aim to develop and study a rigorous framework and new methods for optimal subdata selection by using models that account for heterogeneity in the data, which is often present in large datasets. Research findings will be incorporated in topical courses to train graduate students in large-scale data analysis. The work will also be disseminated via the PIs? collaborations in public health, biomedical science, and business.<br/><br/>Rather than assuming a multiple regression model, the PIs plan to develop and study subdata selection methods based on mixture-of-experts (ME) models, which can account for heterogeneity in the data. The PIs will initially develop and study subdata selection methods for a subclass of the ME models, known as clusterwise linear regression models, for which the gate functions are constant. This will be followed by studying logistic-normal mixture models, in which the gate functions depend on the regression variables. For both cases, the investigators plan to develop information-based optimal subdata selection methods, first for continuous response variables and then for binary response variables, study their statistical properties, and develop efficient algorithms for the methods that will be made available in an R package.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210850","Estimation and Inference with High-Dimensional Data","DMS","STATISTICS","07/15/2022","07/08/2022","Cun-Hui Zhang","NJ","Rutgers University New Brunswick","Standard Grant","Yulia Gel","06/30/2025","$289,999.00","","czhang@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","1269","$0.00","High-dimensional data are collected in a wide range of disciplines, from biology and medical research, natural sciences, and engineering to social sciences, economics, and finance. Statistical inference with such data has become increasingly important. The objective of this research project is to develop improved statistical methods, algorithms, and theory for estimation and inference with high-dimensional data. The project will implement the new methods in several applications that demonstrate their feasibility, effectiveness, and usefulness. The project will also carry out comprehensive numerical experiments to verify the computational efficiency of the new algorithms and to prove the relevance of the related theory in realistic settings. The numerical work aims to produce concrete evidence of the utility of the approach in wide contexts. The work will foster collaborations between researchers with different expertise, allow students and young researchers to align quickly with cutting edge research, and encourage them to embark on a host of exciting research topics. Special efforts will be devoted to recruiting and encouraging students from underrepresented groups. Software and other tools will be made available to the public, enhancing scientific and data-driven decision making in practical applications.<br/><br/>High-dimensional data is an intense area of research in statistics due to its central role in the development and theoretical understanding of some of the most widely used statistical methods in modern time. This research project intends to establish a solid foundation for future work in the emerging topic arising from the convergence of differential-based statistical inference methods and approximate message passing, and their connection to empirical Bayesian methods. It aims to develop the central limit theorem for Stein's unbiased risk estimate, new methods and theory for regularized estimation, new methods and theory for de-biased statistical inference including confidence intervals and regions, and empirical Bayes methods in approximate massage passing. The project intends to produce a rich collection of new tools for such statistical inference, study the theoretical and empirical properties of the newly developed methods, and set the scene for their application in important fields including sociology, economics, neural imaging, signal processing, communications, social networks, bioinformatics, and text analysis. The project findings are expected to have impact as well in other fields of statistics, including causal inference, missing data, survival analysis, compressed sensing, information retrieval, and signal processing, significantly advancing statistics and data science research in general.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2229109","Collaborative Research: AMPS Stochastic Algorithms for Early Detection and Risk Prediction of Hidden Contingencies in Modern Power Systems","DMS","AMPS-Algorithms for Modern Pow, ","09/01/2022","08/08/2022","Masoud Nazari","MI","Wayne State University","Standard Grant","Yulia Gel","08/31/2025","$200,000.00","Le Yi Wang","masoud.nazari@wayne.edu","5700 CASS AVE STE 4900","DETROIT","MI","482023692","3135772424","MPS","045Y, W325","1269, 5294, 8396, 9263","$0.00","Modern power systems (MPS) are complex systems involving conventional and renewable generators, smart distribution networks, and advanced information exchanges. High penetration of random low-inertia renewable energy sources, increased natural disasters such as the 2021 Winter storm Uri, and unprecedented man-made cyber-physical attacks have posed a threat to the reliability and security of MPS. Several cascading failures in MPS started with smaller undetected contingencies such as California's wildfires (e.g., Camp Creek Fire, Zogg Fire, and Dixie Fire) caused by equipment failures. Smaller contingency events, particularly on the distribution side of the grid, may not be directly detected. This project focuses on the early detection and risk prediction of hidden contingencies in MPS. The research fits within efforts to enhance the resilience of the U.S. power grid and move toward carbon-free energy infrastructure. Therefore, it has broader impacts on the carbon-free economy and social welfare. This project will also enhance teaching, training, and learning in mathematics and statistics, renewable energy, smart grids, and green technologies. The team plans to develop new courses for undergraduate and graduate students to facilitate the training of next-generation scientists and engineers. Every effort will be made to promote the participation of underrepresented students in the research project.<br/><br/>This research project introduces a novel framework of stochastic prediction, estimation, and early detection (SPEED) for MPS. Covering a broad range of cyber-physical contingencies (CPC), this research will have the following distinct and novel aims and outcomes. First, the project introduces a new stochastic hybrid system (SHS) model, consisting of continuous dynamics and discrete events. Second, the project will develop new estimation and prediction computational methods. Starting from the Wonham filter for hidden Markov chains, to detect discrete jump changes, this research will focus on finding more computationally feasible schemes. Furthermore, rates of convergence of the algorithms will be obtained, and extensive numerical experiments will be performed. Third, fundamental concepts such as joint observability will be introduced. New estimation algorithms will be developed for joint estimation and prediction of CPC in SHS.  Fourth, since early and quick detection of abrupt changes is vitally important for the risk management of MPS, this project will provide a new computable scheme based on Markov chain approximation for optimal stopping and will quantitatively predict risks of potential near-future cascading CPC. Fifth, evaluation and validation of the theoretical findings will be conducted through utility-level operational data, large-scale power grid simulations, and hardware-in-the-loop emulation on a microgrid. The synthetic operational and summary data of the distribution power grids and transmission systems will be incorporated into the validation and evaluation of the study.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210640","Collaborative Research: Integrative Heterogeneous Learning for Intensive Complex Longitudinal Data","DMS","STATISTICS","08/01/2022","07/26/2022","Annie Qu","CA","University of California-Irvine","Standard Grant","Yong Zeng","07/31/2025","$200,000.00","","aqu2@uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1269","8091","$0.00","This project addresses several fundamental statistical questions related to biomedical information. These questions arise in complex biomedical studies when the data present high heterogeneity and the number of decision stages is large. The investigators aim to develop new statistical methods and efficient computational tools to address practical applications such as treatment of post-traumatic stress disorder (PTSD) and the use of mobile health data in management of diabetes. The research is expected to be useful in health care and stimulate interdisciplinary research in neuroscience, mental health, nursing, infectious diseases, epigenetics, biology, and computer science. The software to be developed will be widely disseminated for use by industry partners. The project provides training through research involvement for graduate students. <br/><br/>The investigators will study three research topics. The first is motivated by identifying heterogeneous epigenetic effects of PTSD patients. This research will push the current boundaries by developing a new model to identify high-dimensional DNA methylation mediators. The second topic is motivated by recent advances in mobile health technology, which effectively monitors individuals' health statuses and delivers personalized treatment. The concept of ""value enhancement"" is used to select the optimal treatment. The investigators will study a novel estimator when the number of decision stages can diverge to infinity. The optimal regime could be sparse and adaptive, making it more advantageous if there are many treatment options. It will also be robust to unexpected situations such as temporary medication shortages or budget constraints. In the third topic, the investigators plan to develop a double encoders approach to estimate the optimal omni-channel individualized treatment rule by incorporating interaction effects such as synergistic or antagonistic effects due to combination treatments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2206934","World Meeting of the International Society for Bayesian Analysis 2022","DMS","STATISTICS","05/01/2022","04/19/2022","Matthias Katzfuss","TX","Texas A&M University","Standard Grant","Yulia Gel","04/30/2023","$12,000.00","","katzfuss@gmail.com","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","7556","$0.00","This award provides travel support for US-based participants in the 2022 World Meeting of the International Society for Bayesian Analysis (ISBA), to be held from June 25 to July 1, 2022, in Montreal, Canada. The conference themes are theory, modeling, and applications of Bayesian statistics. The focus of this award is on funding for junior researchers (graduate students and postdoctoral researchers) from U.S.-based institutions to travel to the conference. Emphasis will be placed on supporting women and members of underrepresented groups. Participating in the conference will inform junior statisticians about key problems and methods that shape research in modern Bayesian statistics and provide them with opportunities to learn from more established researchers and to build mentoring and collaborative relationships. More information on the conference is available on the meeting web page: https://isbawebmaster.github.io/ISBA2022/<br/><br/>Statisticians play an indispensable role in making decisions based on noisy, complex data structures. Statistical methods find application in myriad areas, including biomedical research, environmental science, finance, marketing, psychology, public health, and genomics. Within statistics, the Bayesian approach offers many appealing features: Bayesian methods provide a coherent framework for integrating information from different sources and communicating findings and conclusions using probabilities; Bayesian hierarchical models can capture different sources of variability in data and processes; relevant knowledge can be incorporated easily; and all relevant uncertainties are coherently propagated and incorporated into the final inference. Consequently, Bayesian analyses are commonplace across a wide variety of application areas. ISBA 2022 will bring together a diverse international community of researchers and practitioners who develop and use Bayesian statistical methods to share recent findings, exchange ideas, and discuss new, challenging questions. As a truly international meeting, it will provide participants with access to ideas and colleagues from other countries with whom they may not ordinarily interact. Meeting organizers hope to provide a venue that facilitates the exchange of ideas and cross-fertilization, is welcoming to young researchers, and promotes collaborations and interactions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210735","Models and Inferences for Heterogeneous Interaction Patterns in Social Networks","DMS","STATISTICS","08/01/2022","07/25/2022","Jun Yan","CT","University of Connecticut","Standard Grant","Yong Zeng","07/31/2025","$360,000.00","Xianyang Zhang","jun.yan@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","","$0.00","The rapid growth of social network platforms brings opportunities as well as challenges to identify, extract, and investigate the users? sentiment on subjects of scientific and public interest. This research project aims to identify the sources of social network heterogeneity. The project will study the likelihood of replies to a posting as well as the propagation of sentiments through a social network. Results of the study are expected to help identify sentiment exchange patterns through interactions between users; detect sub-networks of posts with emotional clusters or exhibiting sentiment polarization; and understand how contagious sentiments propagate through the network of Twitter posts. The project will be integrated into multilevel education through graduate student advising, modernized graduate course components, undergraduate research on carefully devised small problems, and outreach to high school students. Broad participation will be encouraged through partnerships with organizations promoting STEM among under-represented groups. Results from the project will be shared via public repositories for data/codes, software packages, online tutorials, and social media. <br/><br/>Dynamic networks are formed among Twitter users by evolving tweets on a certain theme (e.g., depression) through retweets, replies, and mentions, which are critical in sentiment analysis. Existing dynamic network models not accounting for heterogeneous behavioral patterns cannot provide adequate fits for large, real networks. This project aims to tackle two sources of heterogeneity in dynamic network modeling: reciprocity in emotion sharing and similarity clustering in terms of behavioral features. The investigators plan to develop two novel models that modify the preferential attachment (PA) model. The first one retains the scale-free property and allows personalized heterogeneous tendencies to generate reciprocal edges. The second model is a practical spatial superstar PA model that allows a higher level of interaction among nodes of closer social ties measured in a feature space. Their theoretical properties will be studied via rigorous asymptotic analyses. Inferences about the model parameters when fitting observed networks will be developed through likelihood-based, Bayesian, moment-based, and extreme value approaches. Implementations of the models and inferences will be made publicly available via user-friendly, open-source software packages. All the methods under development will be validated through simulation studies, and the validated methods will then be applied to dynamic networks from Tweets on mental-health-related tags.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2219336","Conference: UConn Sports Analytics Symposium: Engaging Students into Data Science","DMS","STATISTICS","09/01/2022","08/26/2023","Jun Yan","CT","University of Connecticut","Continuing Grant","Yong Zeng","08/31/2025","$33,324.00","Kun Chen, Elizabeth Schifano, Laura Burton, Robert Huggins","jun.yan@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","7556","$0.00","The UConn Sports Analytics Symposium (UCSAS) will be held on Saturday, Oct. 8, 2022, and annually on the second Saturday in October 2023-2024. The symposium is designed to attract more students outside of traditional data science programs into data science through sports analytics. Unlike other sports analytics conferences that are often not accessible to students due to the technical level, cost, or space limitations, the UCSAS has a unique focus on pre-college, undergraduate, and graduate students who are interested in sports analytics as well as data science in general. The event aims to showcase sports analytics to students at an accessible level, train students in data analytics with application to sports data, and foster collaboration between academic programs and the sports industry. The program features student-friendly keynote presentations, student poster competitions, training workshops at different levels, data challenges, and career panels, with every component highlighting students' needs. All outcomes from the event are permanently archived with open access. The UCSAS accelerates the data literacy and ""citizen data science"" movements from society's perspective.<br/><br/>The UCSAS has great potential to advance knowledge in sports analytics and the general data science field. It has been well-documented that, as an interface between sports and data science, sports analytics is extremely powerful in engaging students from different backgrounds to learn more about and become good at data science. The UCSAS brings students with a common interest in analytics together under a data science umbrella. It facilitates the advancement of understanding about sports analytics across the analytics and data science worlds. The student poster competition and the data challenge will stimulate creative and potentially transformative contributions to sports analytics. The training workshops tailored to different levels offer an effective jump-start model for engaging students in data science. Through these activities, the UCSAS fosters partnerships between academia and the sports industry. The educational materials, video recordings of the keynote presentations, poster submissions, and data challenge submissions will be widely disseminated via social media and GitHub repository in addition to standard channels such as websites and journal publications. More details about the symposium can be found at: https://uconnsportsanalytics.org/index.html.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2230866","SRCOS Summer Research Conference in Statistics and Biostatistics","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","08/01/2022","08/01/2022","Katherine Thompson","KY","University of Kentucky Research Foundation","Standard Grant","Yong Zeng","07/31/2023","$50,000.00","Arnold Stromberg","katherine.thompson@uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","1260, 1269","7556, 9150","$0.00","The Southern Regional Council on Statistics (SRCOS) is a consortium of statistics and biostatistics programs in the South, with member programs at 45 universities in 16 states in the region. This grant provides support for graduate and undergraduate students to participate in the 57th annual SRCOS Summer Research Conference (SRC) in statistics and biostatistics, held in Jekyll Island, Georgia on October 2-5, 2022. The purpose of the conference is to encourage interchange and mutual understanding of current research ideas in statistics and biostatistics and to provide motivation and direction to further research progress. The SRC is particularly valuable for graduate students, undergraduate students, isolated statisticians, and faculty from smaller regional schools in the southern region within driving distance. The meeting provides an opportunity for young researchers to interact closely with leaders in the field in a manner not possible at larger meetings.<br/><br/>Speakers will present formal research talks with adequate time allowed for clarification, amplification, and further informal discussions in small groups. Under the travel support provided by this award, graduate students will attend and present their research in posters to be reviewed by more experienced researchers. In addition, the meeting will also include the annual Statistical Undergraduate Research Experience (SURE), a conference within a conference aimed to encourage the participation of undergraduate students from under-represented groups to pursue graduate education and career opportunities in STEM fields. SURE will include events specifically for undergraduate students and undergraduate mentors, such as a panel about career opportunities in statistics, a ?Real Data Analytics Workshop,? and a speed-mentoring session with current statistics and biostatistics graduate students. This award will support SURE, encouraging under-represented students to enter STEM fields and providing training to support this endeavor. More information about the conference can be found at the website: https://www.srcos.org/conference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210546","Collaborative Research: Design-Based Optimal Subdata Selection Using Mixture-of-Experts Models to Account for Big Data Heterogeneity","DMS","STATISTICS","08/15/2022","08/02/2022","Min Yang","IL","University of Illinois at Chicago","Standard Grant","Yong Zeng","07/31/2025","$150,000.00","","myang2@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","","$0.00","With technological advances, it has become easy to collect massive amounts of data for most areas of research. But with the size of datasets measured in terabytes or even petabytes, analyzing such datasets can become an expensive computational challenge and may be impossible on a typical desktop or laptop computer. However, for making impactful discoveries, it may be unnecessary to analyze an entire dataset. Consequently, there is great interest in developing and studying methods for selecting a subset from a massive dataset and for drawing conclusions based on the much smaller selected dataset. Such methods are known as subdata selection or subsampling methods. One obvious subsampling method consists of randomly selecting data from the entire dataset. While this is often the simplest and fastest option, it has been established that better options are often available. In this project, the principal investigators (PIs) aim to develop and study a rigorous framework and new methods for optimal subdata selection by using models that account for heterogeneity in the data, which is often present in large datasets. Research findings will be incorporated in topical courses to train graduate students in large-scale data analysis. The work will also be disseminated via the PIs? collaborations in public health, biomedical science, and business.<br/><br/>Rather than assuming a multiple regression model, the PIs plan to develop and study subdata selection methods based on mixture-of-experts (ME) models, which can account for heterogeneity in the data. The PIs will initially develop and study subdata selection methods for a subclass of the ME models, known as clusterwise linear regression models, for which the gate functions are constant. This will be followed by studying logistic-normal mixture models, in which the gate functions depend on the regression variables. For both cases, the investigators plan to develop information-based optimal subdata selection methods, first for continuous response variables and then for binary response variables, study their statistical properties, and develop efficient algorithms for the methods that will be made available in an R package.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210912","Developing Modern Spatial and Shape Analysis for New Heterogeneous High-dimensional Geospatial Data","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","07/01/2022","05/25/2022","Qiwei Li","TX","University of Texas at Dallas","Standard Grant","Yong Zeng","06/30/2025","$199,999.00","","qiwei.li@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1269, 7454","068Z","$0.00","Modern data science involves more and more heterogeneous geospatial data where the features of objects are measured as discrete variables. The quantification of spatial correlation of qualitative marks has been a longstanding focus in spatial statistics. It is a key aspect of population forestry and ecology theory but receives little attention in biomedicine. Recent developments in deep learning and biotechnology have greatly facilitated generating massive high-dimensional genome sequence count data with spatial information. This creates an urgent need for innovation to analyze such complex data, driving the methodological modernization and theoretical developments in spatial statistics. In this project, the investigator will develop a series of computationally efficient spatial and shape methods that are theoretically sound and practically useful for addressing the heterogeneity issue commonly seen in the new geospatial data (e.g., spatial transcriptomics data). The investigator also plans to develop user-friendly and open-source software. The project will expose undergraduate and graduate students to advanced science, technology, engineering, and mathematical skillsets.<br/><br/>The investigator will develop three modeling frameworks to analyze heterogeneous geospatial data at different spatial resolutions. First, an energy-based framework that characterizes heterogeneous spatial patterns for both grided and point data will be developed. Compared with the traditional kernel-based methods, the new method is more robust to noise and computationally more efficient. Towards the goal, the investigator will incorporate a novel feature selection mechanism into the framework to jointly identify multiple spatial patterns for the high-dimensional geospatial data. Then, the study will explore how to construct an interpretable low-dimensional representation of the geospatial data. In particular, the investigator will focus on integrating multi-modal geospatial data to improve the accuracy and resolution of spatial domain partition. Once the spatial domains have been segmented, an immediate task is to characterize their complex shapes. Finally, a landmark-based framework that quantifies a spatial domain?s boundary will be studied to account for heterogeneous boundary roughness. The developed methodologies will contribute to much-needed theories and applications in Bayesian spatial and shape analysis. The investigator will also study the potential benefits and shortcomings of parametric and nonparametric Bayesian methods in this context from both theoretical and computational aspects. Results will be disseminated through workshops, publications, and new courses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210064","Functional Regression and Classification for Data Supported on Complex Geometries","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","07/01/2022","06/06/2022","Eardi Lila","WA","University of Washington","Standard Grant","Yulia Gel","06/30/2025","$119,981.00","","elila@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269, 7454","068Z, 1269","$0.00","This project aims to develop novel statistical methods for functional/imaging data that are located on complex geometries. Thanks to advances in imaging technology, such data are now ubiquitous in the fields of medicine, biology, and climatology, among others. Perhaps the most common task is to use these complex data (for example, brain activity on the cortical surface) to predict a response variable (for example, age or disease status) by employing regression and classification models. The project will develop a novel framework for regression and classification that leverages mathematical tools including partial differential equations and differential geometry to integrate additional information available and define more accurate models. Moreover, efficient computational implementations and theoretical guarantees on the models' performance on previously unseen data will also be provided. The software implementations of the new models will be made publicly available. The research activities will offer numerous opportunities for interdisciplinary research training of the next generation of statisticians and data scientists.<br/><br/>The framework under development will generalize current functional data methodology to complex settings that arise from the analysis of modern imaging data. Specifically, the research activities include development of regularized linear models and generalized linear models for predictors that are functional data supported on multidimensional non-linear domains. These models will be formulated as an infinite-dimensional minimization over spaces of smooth functions. To efficiently approximate their solutions, the project will employ tools from numerical analysis of partial differential equations and elements of calculus of variations. Further, the new framework will be generalized to situations where the functional predictors display tensor structure, which can be leveraged to control the complexity of the solution via low-rank constraints.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2229108","Collaborative Research: AMPS Stochastic Algorithms for Early Detection and Risk Prediction of Hidden Contingencies in Modern Power Systems","DMS","AMPS-Algorithms for Modern Pow, ","09/01/2022","08/08/2022","Gang George Yin","CT","University of Connecticut","Standard Grant","Yulia Gel","08/31/2025","$109,797.00","","gyin@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","045Y, W325","1269, 5294, 8396, 9263","$0.00","Modern power systems (MPS) are complex systems involving conventional and renewable generators, smart distribution networks, and advanced information exchanges. High penetration of random low-inertia renewable energy sources, increased natural disasters such as the 2021 Winter storm Uri, and unprecedented man-made cyber-physical attacks have posed a threat to the reliability and security of MPS. Several cascading failures in MPS started with smaller undetected contingencies such as California's wildfires (e.g., Camp Creek Fire, Zogg Fire, and Dixie Fire) caused by equipment failures. Smaller contingency events, particularly on the distribution side of the grid, may not be directly detected. This project focuses on the early detection and risk prediction of hidden contingencies in MPS. The research fits within efforts to enhance the resilience of the U.S. power grid and move toward carbon-free energy infrastructure. Therefore, it has broader impacts on the carbon-free economy and social welfare. This project will also enhance teaching, training, and learning in mathematics and statistics, renewable energy, smart grids, and green technologies. The team plans to develop new courses for undergraduate and graduate students to facilitate the training of next-generation scientists and engineers. Every effort will be made to promote the participation of underrepresented students in the research project.<br/><br/>This research project introduces a novel framework of stochastic prediction, estimation, and early detection (SPEED) for MPS. Covering a broad range of cyber-physical contingencies (CPC), this research will have the following distinct and novel aims and outcomes. First, the project introduces a new stochastic hybrid system (SHS) model, consisting of continuous dynamics and discrete events. Second, the project will develop new estimation and prediction computational methods. Starting from the Wonham filter for hidden Markov chains, to detect discrete jump changes, this research will focus on finding more computationally feasible schemes. Furthermore, rates of convergence of the algorithms will be obtained, and extensive numerical experiments will be performed. Third, fundamental concepts such as joint observability will be introduced. New estimation algorithms will be developed for joint estimation and prediction of CPC in SHS.  Fourth, since early and quick detection of abrupt changes is vitally important for the risk management of MPS, this project will provide a new computable scheme based on Markov chain approximation for optimal stopping and will quantitatively predict risks of potential near-future cascading CPC. Fifth, evaluation and validation of the theoretical findings will be conducted through utility-level operational data, large-scale power grid simulations, and hardware-in-the-loop emulation on a microgrid. The synthetic operational and summary data of the distribution power grids and transmission systems will be incorporated into the validation and evaluation of the study.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210775","Dimension Reduction and Data Visualization for Regression Analysis of Metric-Space-Valued Data","DMS","STATISTICS","08/01/2022","07/08/2022","Bing Li","PA","Pennsylvania State Univ University Park","Standard Grant","Yulia Gel","07/31/2025","$290,000.00","Lingzhou Xue","bing@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","1269","$0.00","The goal of this project is to systematically develop a set of data exploration and visualization tools for a new type of regression analysis for a form of data that has become increasingly common in recent applications. Such data, known as random objects, do not possess some basic properties of conventional data: for example, they do not have directions or angles that are taken for granted in conventional analysis. Examples include mortality distributions, large-covariance matrices, and observations on spheres. Many existing statistical tools, such as least squares, regression, R-squares, and dimension reduction, cannot be directly applied. A new type of regression, called Frchet regression, has recently been developed to handle this data type. The current project aims to fill the gap between the new data type and conventional methods by transforming random objects into forms that are accessible by conventional methods with high efficiency. The project will focus on sufficient dimension reduction for the new data type. The results are expected to provide data analysis tools and related computer packages for the new type of regression, to assist preliminary data exploration, data visualization, model diagnostics, and improved estimation accuracy. The project will also involve training and mentoring for graduate students in modern statistical sciences.<br/><br/>The project aims to develop flexible and computationally scalable methods for sufficient dimension reduction for a new type of regression where both the predictor and the response can be metric-space-valued random objects. The results are intended to apply in both linear and nonlinear cases. The underlying idea can be used convert existing methods from the multivariate setting to metric-space-valued random elements. The main difficulty in dealing with metric-space-valued random objects is that there are no inner or outer products between observations, which are required by most of the traditional statistical tools, such as covariance matrices, correlation, projection, regression, and ANOVA decomposition. To circumvent this difficulty, the project employs a universal kernel that bridges the gap between metric spaces and Hilbert spaces, which allows construction of an independence structure within the framework of Hilbert spaces though the process of orthogonalization. The bridge provided by the universal kernel is of fundamental importance in metric-space-valued data analysis in general, going far beyond the current setting of sufficient dimension reduction, because a great number of current methods for multivariate and functional data analysis can only be used in the Hilbert space setting.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210805","Spectral methods for single and multiple graph inference","DMS","STATISTICS","08/01/2022","06/16/2022","Minh Tang","NC","North Carolina State University","Standard Grant","Yong Zeng","07/31/2025","$150,000.00","","mtang8@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","","$0.00","Networks provide an elegant and natural representation for describing a collection of entities and their interactions. Network data appear prominently in many scientific domains such as ecology (food webs), sociology (social networks), biology (protein-protein interactions), telecommunications, and cybersecurity (cellular and computer networks).  This project addresses two important inference problems in network science. The first is to quantify the similarities between a collection of networks for the purpose of classifying a network into categories such as being typical or anomalous. The second is dimension reduction for large and complex networks; this is essential in the development of memory and computationally efficient algorithms for analyzing network data. The investigator will complement theoretical and methodological investigations by developing open-source software packages for network analysis, and mentoring graduate students in statistics and data science.<br/><br/>The research program has three main aims. The first is to study efficient parameters estimation for latent position graphs. Given a latent position graph, the investigator will derive both uniform error bounds and normal approximations for its estimated latent positions. The second aim is to develop valid and robust two-sample testing procedures for latent position graphs with a particular emphasis on the setting where the link function is an unknown radial function. Combining these two aims allows practitioners to compare graphs while ignoring irrelevant features such as difference in edge densities or nodes relabeling in real data. The third aim is to conduct perturbation analysis of randomized singular value decomposition (RSVD) when used for dimension reduction of large, noisy graphs. Viewing the observed adjacency matrix as arising from a general ?signal-plus-noise? framework, the investigator will derive upper bounds for the spectral and two-to-infinity-norm distances between the approximate singular vectors of the observed matrix and the true singular vectors of the signal matrix. These upper bounds will depend on the signal-to-noise ratio and the number of power iterations. Finally, as part of this third aim the investigator will also derive uniform entrywise approximation for recovery of a low-rank signal matrix using RSVD. Results established under this third aim can be applied to general matrix-valued data beyond graphs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210504","Nonparametric Estimation via Mixed Derivatives","DMS","STATISTICS","07/01/2022","06/14/2022","Adityanand Guntuboyina","CA","University of California-Berkeley","Standard Grant","Yong Zeng","06/30/2025","$175,000.00","","aditya@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","","$0.00","Understanding the precise relationship between a specific variable of interest and a related set of covariables is a fundamental problem arising in many areas of science and engineering. It is usually necessary to combine two different sources of information to solve this problem. The first source of information is domain knowledge or theory, which often suggests certain basic forms for the relationship between the variables. The second source of information is observed data on the values of the variables for a set of experimental conditions or subjects. Effective solutions to the problem can only be obtained by efficiently combining both sources of information. The development of such a methodology is the primary goal of this project. Assuming the function governing the relationship between the variables is smooth in a certain sense, the observed data is then brought in to find the function satisfying the assumed smoothness that best explains the observed data. This project will explore different ways of carrying out this scheme and develop novel methods and computational algorithms useful to practitioners in broad scientific areas. Most existing methods for studying these problems either make strong prior assumptions that are unrealistic and lead to wrong conclusions on the relationship between the variables, or weak prior assumptions that lead to requiring unrealistically enormous sizes of datasets for reliable conclusions. The developed approach based on ""mixed partial derivative smoothness constraints"" effectively compromises these two extreme approaches and will lead to methods of great practical value. The material emanating from this research will be disseminated through seminars and undergraduate as well as graduate teaching. The methods from this project will be made available to the wider statistics and scientific community through the development of software packages. <br/><br/>This project focuses on nonparametric function estimation problems under smoothness constraints involving mixed partial derivatives. The investigator plans to expand recently developed methodology for nonparametric regression under mixed partial derivatives of first and second orders to allow for restricted interaction orders, design faster algorithms for computation, and prove theoretical accuracy results under more general design assumptions. The possibility of near parametric rates under strong sparsity settings will be explored in a more general setting involving tensor product bases (including complex exponentials and radial basis functions). Uncertainty quantification will be systematically explored using Bayesian approaches with a main focus on Cauchy priors. New mixed derivative approaches will be explored in shape-constrained regression, including those based on Entire Monotonicity with restricted interactions and total Popoviciu convexity. Mixed derivative approaches for spectral density estimation of time series and density estimation will also be studied.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210272","Development of Modal Regression","DMS","STATISTICS","08/01/2022","06/14/2022","Weixin Yao","CA","University of California-Riverside","Standard Grant","Yong Zeng","07/31/2025","$220,000.00","","weixin.yao@ucr.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","MPS","1269","","$0.00","This project seeks to develop a set of new statistical regression models for abnormal data, such as skewed, truncated, heterogeneous, or noisy data with outliers, which are commonly seen in economics, sociology, medicine, and biology. The new regression method, named modal regression, finds the conditional most probable value (mode) of a dependent variable given covariates, rather than the mean/quantile that the traditional regression models focus on. As a complement to existing regression tools, the modal regression could reveal interesting new data structure that is possibly missed by the conditional mean or quantiles. In addition, modal regression is resistant to outliers and measurement errors, and can provide shorter prediction intervals when the data are skewed, such as salary, prices, and expenditures in economics and church sizes and symptom indices in sociology. Furthermore, unlike traditional mean or quantile regression, the modal regression can be directly applied to the truncated data, which arises when the data are observed only when the dependent variable has a lower or upper limit, such as an economic index measured within some range. This work will benefit scientists and researchers who want to analyze skewed or truncated data in fields that include economics, social sciences, marketing, medical studies, public health, biology, and agriculture. This project will provide training opportunities for graduate students. Software developed for implementing the new modal regression will be made publicly available.<br/><br/>Parallel to existing regression models, the investigator will develop a wide variety of parametric and nonparametric modal regression models for both independent and dependent (time series or spatial) data by imposing some model assumptions on the conditional mode of a dependent variable Y given covariates x. The new method avoids the nonparametric estimation of conditional density of Y given x, which is difficult when the dimension of x is large. The investigator will develop a modal expectation-maximization algorithm to simplify the computation of the modal regression. The convergence rate and sampling properties of the resulting estimators will be systematically studied. For high dimensional data, the investigator will consider a new feature selection tool and variable selection methods for modal regression. In addition, the investigator will develop a new sufficient dimension reduction method to reduce the dimension of covariates for modal regression. Furthermore, the investigator will develop a modal clustering tool for heterogeneous/mixture data where multiple modal regression curves exist. The modal clustering method can serve as an alternative tool for mixture regression models to reveal the clustered/inhomogeneous data structure and provide a natural way to estimate the number of components/clusters, which has long been a challenging problem.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210402","Modeling and Inference for Data with Network Dependency","DMS","STATISTICS","08/01/2022","07/27/2022","Kehui Chen","PA","University of Pittsburgh","Standard Grant","Yong Zeng","07/31/2025","$150,000.00","","khchen@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","","$0.00","Data with complex interpersonal dependency characterized by networks are increasingly encountered in many scientific areas. For example, in some survey studies for school students, a friendship network among students may also be collected in addition to traditional variables collected on each unit such as drug use, smoking and mental health status. The response of interest such as drug use is likely to have dependency across units through friendship networks. As another example, brain functional connectivity studies have consistently discovered functional linkage among brain regions, where dependency among brain regions arises through a network structure. The analysis of network-linked data calls for statistical inference tools and theories that consider network dependency. The developed methods will be applied to data analyses of stress and suicidal studies, helping understand the pathological and biological mechanisms underlying suicidal behaviors. The principal investigator (PI) plans to develop open-source software packages to disseminate the results and provide training opportunities for graduate students.<br/><br/>The first part of the research focuses on developing methods and theory for the inference of regression coefficients and dependency measures between two variables of interest, when there is network dependency across sample units. In the second part of the research, the PI will focus on analyzing network-linked data with replicates. In some applications, multiple independent units may exist, and the observed multivariate data within each unit may have dependency through a network structure. The analysis of this type of network-dependent data exhibits its own features due to the availability of independent realizations. The challenges of dealing with network dependency are at least twofold. The first challenge is the infinite dimensionality, which can be understood through the notion of network neighborhood growth. The second challenge is node heterogeneity, which means a general network does not have the symmetric structure as in a Euclidean lattice space. This research will develop new statistical inference tools and theories addressing these challenges.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210280","Collaborative Research: Novel modeling and Bayesian analysis of high-dimensional time series","DMS","STATISTICS","09/01/2022","07/27/2022","Subhashis Ghoshal","NC","North Carolina State University","Standard Grant","Yong Zeng","08/31/2025","$180,000.00","","subhashis_ghoshal@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","","$0.00","Every aspect of modern life including economy and finance, communication, and medical records, is associated with large amounts of data on several measurements, often evolving over time. Understanding the progress over time, finding an intrinsic relationship among different variables, and predicting future observations are essential components of decision and policy-making. However, apparent relations between two variables can appear in data caused by their shared association with other components. The principal investigators (PIs) will develop a model to re-express the multi-dimensional time series in independent, one-dimensional, latent time series. The representation will explain the evolution of the data over time and the intrinsic relations present in the component variables. It can also help find a more accurate, efficiently computable prediction formula for future observations by pulling information across different components and time. The approach's simplicity and generality will make it widely applicable and adaptable to diverse fields in economics, finance, social sciences, communications, networks, neuroimaging, and others. The PIs plan to develop free software packages to disseminate the results. They are committed to supporting young researchers and promoting diversity through graduate student training and involvement in the REU program.<br/><br/>The developed framework is based on representing an observed multi-dimensional time series as a linear combination of several independent stationary latent processes. The individual latent time series are modeled flexibly with unspecified spectral densities. The PIs will study the conditional independence structure among component time series and the causality of the time series over the temporal domain using a Bayesian approach. They will put independent priors on individual spectral densities through a finite random series prior, and on the matrix of the linear transformation decomposed as a product of a sparse matrix and an orthogonal matrix, the former of which induces a graphical structure for conditional independence among component series. Through this representation, desirable stationarity and causality structures can be imposed. Decoupling through the Whittle likelihood approximation and Hamiltonian Monte-Carlo methods will allow efficient posterior sampling. The causality over nodal time series will be addressed by a Direct Acyclic Graph modeling of the residual process. The formulation seamlessly addresses a mixed frequency sampling situation, difficult to incorporate into competing methods. The developed framework efficiently addresses both temporal and nodal causality respectively by characterization in terms of the Schur-complementation and using a directed acyclic graph, allowing a natural interpretation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210210","Leveraging Background Knowledge for Identification and Estimation of Causal Effects in the Presence of Latent Variables","DMS","STATISTICS","09/01/2022","07/20/2023","Emilija Perkovic","WA","University of Washington","Continuing Grant","Yong Zeng","08/31/2025","$98,599.00","","perkovic@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","075Z, 079Z","$0.00","This project focuses on causal inference using observational data and expert knowledge. Estimating causal effects is a goal of many scientific endeavors, and often, the only available data are those from observational studies. However, estimating causal effects based on observational data alone is not always possible. A hybrid method that leverages both expert knowledge and observational data may help estimate a causal effect or narrow down a range of likely estimates. Such hybrid methods are currently limited to assuming that all variables in the causal system are observed and measured. This assumption is often too stringent for many real-world applications. This project will develop methods for using expert knowledge to help estimate causal effects from observational data in the presence of hidden variables. The methods developed in this project would be immediately applicable to an extensive range of scientific disciplines, most notably epidemiology, economics, personalized medicine, and the study of algorithmic fairness.<br/><br/>The research in this project is grounded on probabilistic graphical models that can be used to represent conditional independence relationships on the observed set of variables. In general, based on observational data alone, the causal graphical model can be identified only up to an equivalence class of such graphs. The first goal of the project is to develop a complete set of rules for updating the set of compatible graphical models based on expert knowledge of certain causal relationships. The second goal of the project concerns the development of graphical criteria for causal effect identification and estimation based on the updated set of models. Furthermore, the investigator will study the updated equivalence class in terms of computational and statistical efficiencies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2141808","CAREER: Inference with graphs: density skeleton and Markov missing graph","DMS","STATISTICS","07/01/2022","07/31/2023","Yen-Chi Chen","WA","University of Washington","Continuing Grant","Yong Zeng","06/30/2027","$138,564.00","","yenchic@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","1045","$0.00","This project introduces novel frameworks for using graphs in analyzing complex datasets. These new applications of graphs allow researchers to investigate the intricate relation among quantities of interest. The newly developed methods will offer novel directions for studying the growth and evolution of a galaxy. The PI also plans to develop methodologies to handle complex missing data problems in the National Alzheimer's Coordinating Center's database. The project highlights how abstract mathematical objects like graphs offer a unified treatment on problems arising from different fields such as astronomy and dementia studies. The PI will also initiate several new educational programs and engage both graduate and undergraduate students in research in various ways. <br/><br/>The PI plans to investigate two novel research directions of applying graphs to statistical problems. In the first direction, the PI develops a novel graphical approach called density skeleton, an undirected graph summarizing the shape of the covariate distribution. The PI will study how to apply density skeleton to various statistical learning problems, including regression, algorithmic fairness, and topological data analysis. In the second part of the project, the PI develops a new graph-based method called Markov missing graph to handle missing data problems. The Markov missing graph defines an identifying assumption to recover the missing entries' distribution. The PI intends to study how the modeling, computation, and efficiency theory interacts with graph geometry.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210802","Random Matrices with Application to Quantum Computing and Econometrics","DMS","PROBABILITY, STATISTICS","08/15/2022","08/03/2022","Tiefeng Jiang","MN","University of Minnesota-Twin Cities","Standard Grant","Yong Zeng","07/31/2025","$180,000.00","","jiang040@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1263, 1269","079Z, 7203","$0.00","This project develops new statistical and probabilistic methods and extends some existing methods with novel applications in statistics and econometrics. The core topic of these subjects is random matrix theory. The efficiency of the proposed methodologies will be demonstrated via simulations and applications to real data sets. The outcomes of the project will enhance the applicability of methods for high-dimensional data settings, and the research results will be applied in statistics and econometrics. The project will promote teaching, training, and learning activities at the University of Minnesota. The main educational goals are (a) to train PhD students at the University of Minnesota; and (b) to promote collaboration among experts and students. The research results will be disseminated through conference presentations and publications. <br/><br/>The investigator plans to develop new methodologies to investigate properties of a few types of random matrices. They include Haar orthogonal/unitary matrices, sample correlation matrices, Macdonald measures and circular orthogonal ensembles. The investigator will then apply them to answer statistics and econometrics problems. The project consists of the following main themes: (1)  the investigator plans to study the approximation of Haar-invariant orthogonal/unitary matrices by independent normals. The solutions are known when the approximation errors are measured by some well-known distances, including the total variation distance.  Besides their relevance in theoretical research, they are also applied to data storage. The investigator plans to study the same approximation question under the Wasserstein distance; (2) the investigator plans to explore the Macdonald measure and show the eigenvalues are asymptotically a Gaussian Free Field. A function of these eigenvalues is shown to converge to a Gaussian multiplicative chaos. Nowadays active investigations such as  the construction of the Liouville measure in 2d-Liouville quantum gravity or thick points of the Gaussian Free Field have appeared in many branches. Inspired by this understanding, the investigator will study the Macdonald measure; (3) the investigator plans to study the largest entries of sample correlation matrices for dependent data. The same problem for independent data is well-understood. Although the dependent case is more applicable, the technical steps to analyze them are more intensive. The investigator proposes new methods to solve this question;  and (4) by using a new method on asymptotic independence between sums and maxima of dependent data, which was established recently by the investigator and his co-authors, it is planned to study problems dealing with high-dimensional panel data arising in econometrics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210657","Collaborative Research: Integrative Heterogeneous Learning for Intensive Complex Longitudinal Data","DMS","STATISTICS","08/01/2022","07/26/2022","Ruoqing Zhu","IL","University of Illinois at Urbana-Champaign","Standard Grant","Yong Zeng","07/31/2025","$120,000.00","","rqzhu@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","8091","$0.00","This project addresses several fundamental statistical questions related to biomedical information. These questions arise in complex biomedical studies when the data present high heterogeneity and the number of decision stages is large. The investigators aim to develop new statistical methods and efficient computational tools to address practical applications such as treatment of post-traumatic stress disorder (PTSD) and the use of mobile health data in management of diabetes. The research is expected to be useful in health care and stimulate interdisciplinary research in neuroscience, mental health, nursing, infectious diseases, epigenetics, biology, and computer science. The software to be developed will be widely disseminated for use by industry partners. The project provides training through research involvement for graduate students. <br/><br/>The investigators will study three research topics. The first is motivated by identifying heterogeneous epigenetic effects of PTSD patients. This research will push the current boundaries by developing a new model to identify high-dimensional DNA methylation mediators. The second topic is motivated by recent advances in mobile health technology, which effectively monitors individuals' health statuses and delivers personalized treatment. The concept of ""value enhancement"" is used to select the optimal treatment. The investigators will study a novel estimator when the number of decision stages can diverge to infinity. The optimal regime could be sparse and adaptive, making it more advantageous if there are many treatment options. It will also be robust to unexpected situations such as temporary medication shortages or budget constraints. In the third topic, the investigators plan to develop a double encoders approach to estimate the optimal omni-channel individualized treatment rule by incorporating interaction effects such as synergistic or antagonistic effects due to combination treatments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2209975","Advancing Theory and Methodology for Tree-Based Algorithms in High Dimensions","DMS","STATISTICS","07/15/2022","07/08/2022","Bin Yu","CA","University of California-Berkeley","Standard Grant","Yong Zeng","06/30/2025","$330,000.00","","binyu@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","075Z, 079Z","$0.00","Predictive statistical modeling has long been part of the backbone of science and engineering. In recent years, the proliferation of big data has led to a need to go beyond traditional linear models, and a need for flexible models that can exploit complicated nonlinear relationships. Models based on decision trees have emerged as an easy-to-use and high performing class of models, especially for unstructured tabular datasets such as electronic health records, in which they have been found to typically outperform neural networks. Furthermore, since decision trees can be easily visualized and simulated by non-experts, this makes them easier to audit than black box machine learning models, which is especially important when predictions are used to guide high-stakes decisions in the clinic or the courtroom. Unfortunately, models based on decision trees are not well understood statistically, and it is still unclear when and why various models obtain better relative predictive performance. The project plans to bridge this gap by identifying structural properties in real world datasets that make them either amenable or not amenable to current tree-based models. This understanding will then be used to develop better algorithms based on decision trees, as well as methodology to extract reproducible scientific insights from these models. In the duration of the project, graduate students will be trained in theory, domain-driven data science, and open-source software development. Research results will further be disseminated through courses, an upcoming book, and presentations at workshops and conferences.<br/><br/>The project plans two thrusts to develop relevant theory for decision trees and random forests. First, it will analyze the generalization performance of tree-based algorithms on a range of different generative regression models in order to elicit their inductive bias. Inductive bias is a well-known concept from machine learning, and is defined as the assumptions an algorithm makes when generalizing to new data. Since real world datasets often present some structure that can be exploited using the right inductive bias, results of this project will allow better identification of which algorithm to choose in a given application, thus improving on classical nonparametric regression analysis of decision trees and random forests. Second, the project will study a new general framework for obtaining model-agnostic nonlinear feature significance measures using mean decrease in impurity (MDI) feature importance. This framework makes use of a novel interpretation of MDI in terms of r-squared values from linear regression, and is asymptotically valid even if the decision tree used to generate MDI is not necessarily a good model for the underlying regression function.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210729","SCience-INtegrated Predictive modeLing (SCINPL): a novel framework for scalable and interpretable predictive scientific modeling","DMS","STATISTICS","08/01/2022","06/16/2022","Simon Mak","NC","Duke University","Standard Grant","Yong Zeng","07/31/2025","$200,000.00","David Dunson","sm769@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","079Z","$0.00","Scientific modeling is at a critical and defining crossroads. With breakthroughs in experimental technology, high-quality data can now be obtained for complex scientific and engineering problems. However, the generation of such high-quality data entails large experimental and computational costs, resulting in limited data for scientific investigation. While predictive modeling provides some relief, recent work has revealed two key shortcomings with existing models: they often yield poor predictive performance when trained with limited data, and can violate established scientific principles, which may lead to erroneous and spurious scientific conclusions. This project will develop a novel SCience-INtegrated Predictive modeLing (SCINPL) framework which addresses these limitations. SCINPL paves the road for transformative scientific research, equipping practitioners with accurate, cost-efficient and interpretable predictive models for guiding scientific progress. This framework can catalyze closer collaborations between the scientific and data science communities, by demonstrating the practical advantages of science-driven statistical learning and data-driven scientific discovery. SCINPL provides a radical paradigm shift for scientific discovery in a broad range of fields, enabling scientists to push forward the frontiers of scientific knowledge and engineering via improved science-based data science tools.<br/><br/>SCINPL features a suite of new probabilistic Bayesian models, which are capable of integrating a wide range of prior scientific domain knowledge as prior beliefs for predictive modeling. This integration of scientific knowledge with data-driven models not only provides improved predictive performance with reduced uncertainty, but also enables better interpretability and thus scientific discovery given limited training data. The first model, called the Boundary-constrained GP model, integrates known boundary information for the response surface within a Gaussian process (GP) framework. The second model, the Graphical Multi-fidelity GP model, embeds dependency information between scientific models for predictive modeling. The third model, the Gaussian Process Subspace regression model, integrates subspace information representing dominant physics for GP modeling. For each model, the investigators will (i) establish a solid theoretical foundation for predictive modeling, which demonstrates the improved predictive performance via the integration of scientific information, (ii) present a comprehensive methodological framework and efficient suite of algorithms for performing this desired integration of scientific principles within probabilistic modeling, and (iii) demonstrate the usefulness of such models for cost-efficient, interpretable and principled scientific discovery. Major emphasis is placed on demonstrating the effectiveness of SCINPL in tackling a broad range of complex and expensive scientific problems, including the design of 3D-printed aortic valves, the study of heavy-ion collisions, and the optimization of rocket engines for spaceflight.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210583","Collaborative Research: Statistical Optimal Transport in High Dimensional Mixtures","DMS","STATISTICS","07/01/2022","06/16/2022","Jonathan Niles-Weed","NY","New York University","Standard Grant","Yong Zeng","06/30/2025","$175,000.00","","jdw453@nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1269","079Z","$0.00","This project studies high-dimensional mixture models, a class of statistical models that can be used to analyze data arising in linguistics, computational biology, and particle physics. This research project aims to define a new measure of distance between distributions that measures their similarity with respect to a mixture model and offers a principled way to compare, transform, and analyze high-dimensional data sets. As part of this project, the investigators will develop fast algorithms for estimating this distance and theoretical guarantees allowing this distance to be used for statistical inference.<br/><br/>Specifically, this project defines a sketched Wasserstein distance (SWD) and will develop its computational and statistical properties. The primary aims are to establish duality relations for this distance, develop computationally feasible estimators for SWD using both primal and dual formulations, and to study the rates of convergence of the new estimators. In addition, the research aims to develop lower bounds to establish the rate optimality of these estimators and establish distributional limits to allow for the construction of asymptotically valid confidence intervals. These tools will be applied to data in text analysis, systems biology, and high-energy physics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210827","Mean Field Asymptotics in Statistical Inference: Variational Approach, Multiple Testing, and Predictive Inference","DMS","STATISTICS","07/01/2022","04/28/2023","Song Mei","CA","University of California-Berkeley","Continuing Grant","Yong Zeng","06/30/2025","$149,176.00","","songmei@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","075Z, 079Z","$0.00","The era of big data poses unprecedented statistical and computational challenges in high-dimensional statistical inference. One challenge is the ?dual objective? nature of various statistical inference tasks: statisticians hope to design procedures that achieve near-optimal statistical efficiency and satisfy desired validity guarantees even under model misspecification. Furthermore, many statistical inference procedures involve a Bayesian component, and performing exact Bayesian inference on large-scale datasets is computationally challenging. This project will address these challenges in some high dimensional statistical inference tasks. The techniques and methods developed in the project will further advance the interplay between a broad range of areas including high-dimensional statistics, statistical physics, optimization, information theory, and statistical machine learning. Results from this project are anticipated to have applicability in computational biology, computer vision, neuroscience, natural language processing, and multiple testing. Graduate and undergraduate students will be exposed to these results through involvement in the project, and the results will be incorporated in courses.<br/><br/>This project aims to resolve statistical and computational challenges in multiple testing and predictive inference, using the mean field asymptotic theory of statistical inference. Focusing on a few stylized problems, the program consists of three major research thrusts: 1) analyze the non-convex landscape of Thouless-Anderson-Palmer (TAP) variational inference objective functions and design efficient algorithms for optimizing these functions; 2) in the task of false discovery rate (FDR) control, design procedures that maximize the number of discoveries when models are correctly specified while controlling the frequentist FDR even under model misspecification; and 3) in the task of predictive inference, design procedures that give reasonably small prediction sets while maintaining the frequentist validity of coverage in the presence of model misspecification. This research will develop new techniques for studying the mean field asymptotics of high-dimensional statistical models, which will likely be applicable beyond the specific statistical models and will be relevant in other areas of science and engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210687","Novel p-Value Based Multiple Testing Methods for Variable Selection with False Discovery Rate Control","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","07/01/2022","05/23/2022","Sanat Sarkar","PA","Temple University","Standard Grant","Yong Zeng","06/30/2025","$269,695.00","Cheng Yong Tang","sanat@temple.edu","1801 N BROAD ST","PHILADELPHIA","PA","191226003","2157077547","MPS","1269, 7454","068Z, 079Z","$0.00","Multiple testing is one of the most common statistical challenges encountered in modern scientific investigations. This project aims at resolving some longstanding issues with application of multiple testing methods. One of these issues arises in the context of discovering, among a large collection of variables, those that are important influences on an outcome of interest. Inapplicability of standard multiple testing methods due to the unknown interdependency of the variables is such an issue. The methods under development aim to provide new approaches to discovering important variables no matter how the variables depend on each other, with the guarantee that, on average, only a small, controlled fraction of unimportant variables end up as false discoveries. An example application is in the identification of genetic variants which, among many thousands of them, can influence a certain disease. The new methods can aid in identifying genes as being relatively more relevant for therapeutic intervention. The fundamental theoretical and methodological ideas behind the development of these methods will be extended towards resolving similar issues with multiple testing methods in other experimental settings as well. The research to be carried out in the project will be incorporated into courses, benefiting the training of undergraduates and graduate students.<br/><br/>This research project is focused on addressing important theoretical and methodological issues related to multiple testing. For instance, feature/variable selection under the setting of multiple linear regression with Gaussian noise, which plays an important role in data science and is a ubiquitous statistical framework in scientific investigations, is often framed as a multiple testing problem. A p-value based multiple testing method, irrespective of what error rate is being considered to control the falsely discovered important explanatory variables, capturing the correlation matrix of the explanatory variables in full without losing control over the error rate, would be most ideal. Unfortunately, such methods are yet to be developed in a non-asymptotic setting. Similarly, for the related problem of simultaneous testing of multivariate Gaussian means with non-diagonal correlation matrix, subject to a control of an error rate, a p-value based multiple testing method fully capturing the correlation information without losing control over that rate is largely absent from the literature. The challenges will be met by research that cross-fertilizes two seminal ideas on multiple inference: 1) the use of p-value based multiple testing methods to control false discoveries; and 2) the use of the knockoff of the design matrix for variable selection in linear regression settings. Concretely, the project aims at developing novel p-value based false discovery rate and other powerful error rates controlling multiple testing methods for 1) variable selection in multiple linear regression with Gaussian noise, both in low- and high-dimensional settings; and 2) simultaneous testing of multivariate Gaussian means with a general non-diagonal covariance matrix.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2142476","CAREER: High-dimensional inference and applications to modern biology","DMS","STATISTICS","07/01/2022","05/24/2023","Zhou Fan","CT","Yale University","Continuing Grant","Yong Zeng","06/30/2027","$160,064.00","","zhou.fan@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","062Z, 068Z, 075Z, 079Z, 1045","$0.00","In recent years, a burgeoning field of high-dimensional statistical inference has witnessed astounding advances, providing new theoretical tools to characterize exact distributional behavior for an increasingly large class of statistical and machine-learning methods. These advances hold the promise of improved statistical procedures with more precise quantifications of uncertainty across many fields of modern biology. This research will extend the scope of these high-dimensional inferential methods, which currently remain restricted to more stylized statistical models, to address a broader range of scientific problems having complex latent structure. The research will also enable the PI to continue his educational outreach activities in the K-12 levels in Connecticut public schools, as well as his experimentation in the teaching of introductory courses at Yale University by focusing the discussion of statistical concepts and ideas on motivating real-life examples.<br/><br/>On the theoretical front, this research will improve our understanding of mean-field phenomena in non-i.i.d. contexts, including disordered systems and spin glass models with statistically dependent couplings, as well as variational Bayesian approximations to regression models with correlated designs. This research will also further our understanding of asymptotic freeness phenomena for random matrix models arising in statistical settings. On the applications front, this research will improve our understanding of likelihood-based inference for molecular structure determination in cryo-electron microscropy, and investigate possibilities for more robust and efficient reconstruction algorithms. This research will also develop new Bayes and empirical Bayes procedures for fine-mapping of genetic causal variants and for dimensionality reduction of genetic sequence data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2143215","CAREER: Statistical Learning from a Modern Perspective: Over-parameterization, Regularization, and Generalization","DMS","STATISTICS","09/01/2022","07/14/2023","Yuting Wei","PA","University of Pennsylvania","Continuing Grant","Yong Zeng","08/31/2027","$75,980.00","","ytwei@wharton.upenn.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","1269","062Z, 075Z, 079Z, 1045","$0.00","Statistical methods have been a major driving force towards interpretable, actionable, and trustworthy machine learning. However, the existing statistical theory remains highly inadequate in explaining many new phenomena that emerge, and become pervasive in modern machine learning applications. For instance, the prevalence of over-parameterized models (i.e., the ones that have more model parameters than samples) challenges our classical statistical insights about the bias-variance tradeoff; the fact that many learning algorithms exhibit favorable algorithmic regularization to alleviate overfitting is largely beyond the reach of previous statistical literature, and the unconventional shapes of the risk curves in modern applications puzzle many statisticians. Compared to the rich theory developed for classical settings, however, the statistical underpinnings for these curious yet mysterious phenomena remain far from sufficient. Motivated by this, the overarching goal of the project is to enrich the statistical foundation of machine learning by adapting it to contemporary settings, thereby bridging classical statistics and cutting-edge machine learning. In addition, the project will provide valuable opportunities for training students (particularly underrepresented groups) at all levels across multiple disciplines in the STEM field, and will exert scientific and societal impacts on several domains beyond the tasks described herein, including but not limited to neuroscience, online education, and equitable machine learning.<br/><br/>Striving for interpretability and actionable insights, this project plans to revisit multiple classical statistical problems---ranging from minimum-norm interpolation, risk estimation, cross validation, kernel boosting, data-imbalanced classification, to transfer learning---with an emphasis on unveiling new insights for modern yet under-explored regimes. Several recurring themes include: (i) characterizing precise risk behavior in the face of large model complexity; (ii) reconciling the seemingly conflicting goals of over-parameterization and regularization; (iii) developing algorithm-specific statistical reasoning tools; and (iv) exploring the interplay between regularization and generalization. The project comprises three distinct yet related thrusts: (1) statistical insights for over-parameterization: which explores the prolific interplay between model complexity and out-of-sample performance; (2) algorithmic regularization via early stopping: which aims to develop statistical principles that underlie early stopping; (3) risk (non)-monotonicity with imbalanced data: which is motivated by the non-monotonicity of generalization errors in the sample size and pursues principled debiasing methods to rectify it. The project will develop a suite of statistical insights that can inform cutting-edge machine learning practice, as well as an array of statistical methodologies that will be practically appealing for modern data-driven applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210281","Collaborative Research: Novel modeling and Bayesian analysis of high-dimensional time series","DMS","STATISTICS","09/01/2022","07/27/2022","Arkaprava Roy","FL","University of Florida","Standard Grant","Yong Zeng","08/31/2025","$110,000.00","","ark007@ufl.edu","1523 UNION RD RM 207","GAINESVILLE","FL","326111941","3523923516","MPS","1269","","$0.00","Every aspect of modern life including economy and finance, communication, and medical records, is associated with large amounts of data on several measurements, often evolving over time. Understanding the progress over time, finding an intrinsic relationship among different variables, and predicting future observations are essential components of decision and policy-making. However, apparent relations between two variables can appear in data caused by their shared association with other components. The principal investigators (PIs) will develop a model to re-express the multi-dimensional time series in independent, one-dimensional, latent time series. The representation will explain the evolution of the data over time and the intrinsic relations present in the component variables. It can also help find a more accurate, efficiently computable prediction formula for future observations by pulling information across different components and time. The approach's simplicity and generality will make it widely applicable and adaptable to diverse fields in economics, finance, social sciences, communications, networks, neuroimaging, and others. The PIs plan to develop free software packages to disseminate the results. They are committed to supporting young researchers and promoting diversity through graduate student training and involvement in the REU program.<br/><br/>The developed framework is based on representing an observed multi-dimensional time series as a linear combination of several independent stationary latent processes. The individual latent time series are modeled flexibly with unspecified spectral densities. The PIs will study the conditional independence structure among component time series and the causality of the time series over the temporal domain using a Bayesian approach. They will put independent priors on individual spectral densities through a finite random series prior, and on the matrix of the linear transformation decomposed as a product of a sparse matrix and an orthogonal matrix, the former of which induces a graphical structure for conditional independence among component series. Through this representation, desirable stationarity and causality structures can be imposed. Decoupling through the Whittle likelihood approximation and Hamiltonian Monte-Carlo methods will allow efficient posterior sampling. The causality over nodal time series will be addressed by a Direct Acyclic Graph modeling of the residual process. The formulation seamlessly addresses a mixed frequency sampling situation, difficult to incorporate into competing methods. The developed framework efficiently addresses both temporal and nodal causality respectively by characterization in terms of the Schur-complementation and using a directed acyclic graph, allowing a natural interpretation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2213432","LEAPS-MPS: Computational Modeling to Characterize and Attribute Uncertainty in Future Coastal Risk","DMS","APPLIED MATHEMATICS, STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2022","07/25/2022","Anthony Wong","NY","Rochester Institute of Tech","Standard Grant","Eun Heui Kim","08/31/2024","$179,999.00","","aewsma@rit.edu","1 LOMB MEMORIAL DR","ROCHESTER","NY","146235603","5854757987","MPS","1266, 1269, 7454","1303, 5294, 9251","$0.00","The project aims to examine how coastal areas can mitigate damages from sea-level change and coastal flooding. Rising global sea levels and intensifying storms cause risks for people and property in coastal areas. Strategies to manage these risks include protective measures like building seawalls, elevating existing structures, and relocation away from the coast. However, uncertainty is inherent in the geophysical processes, the mathematical models, and the observational data used to calibrate those models.  These modeling uncertainties lead to uncertainty in the optimal strategy to defend against coastal hazards. This research will assess how different geophysical and socioeconomic factors lead to uncertainty in the decisions and costs to effectively protect coastal areas, and uncertainty in the estimated damages from potentially under-protecting coastal assets. The project will provide training opportunities for students to develop software and conduct computer model experiments. These activities will support the representation and persistence of students from underrepresented minority groups by enhancing students? sense of science identity through engaging in projects. Further, the research will be conducted at a Carnegie R2 university, where the resources made available through this project will have positive impacts. <br/><br/>The research will investigate models for sea-level change and coastal impacts by exploiting mathematical structures for coastal adaptation decision-making and gridded global climate data. Machine learning and statistical tools will be integrated with existing geophysical and socioeconomic models, bridging coastal adaptation decisions to uncertainties in geophysical processes, in climate and socioeconomic models, and in observational data. This coupled modeling framework will serve as a laboratory to characterize uncertainty in future coastal adaptation costs and decisions. The uncertainty will be decomposed and attributed to geophysical drivers of risk and socioeconomic uncertainties using supervised machine learning and global sensitivity analysis methods. The research will assess the extent to which different uncertainties affect the optimal strategies for adapting coastal areas to the risks posed by sea-level change. As optimality is typically defined by minimizing expected loss, additional decision-making criteria will be implemented in the models employed, thereby enabling an exploration of the role of risk aversion and imperfect information. A standard modeling framework for integrated assessment will be used, which will facilitate future efforts to expand this work to examine the larger role of sea-level hazards in integrated assessments of climate risks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2224121","ENVR 2022 Workshop: Environmental and Ecological Statistical Research and Applications with Societal Impacts","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, MSPA-INTERDISCIPLINARY","07/15/2022","07/05/2022","Matthew Heaton","UT","Brigham Young University","Standard Grant","Yong Zeng","06/30/2023","$24,000.00","","mheaton@stat.byu.edu","A-153 ASB","PROVO","UT","846021128","8014223360","MPS","1253, 1269, 7454","1303, 5294, 7556, 8396","$0.00","The ""ENVR 2022 Workshop: Environmental and Ecological Statistical Research and Applications with Societal Impacts"" will be held in Provo, Utah, on October 6-8, 2022. Environmental Statistics and Data Science (ESDS) is quickly emerging as an integral field for conducting interdisciplinary research in the environmental sciences.  Environmental statisticians are charged with processing, summarizing, and analyzing a massive inflow of environmental data from climate models, in situ observations, and remote sensing measurements, to name a few, to understand and mitigate environmental degradation and foster human and ecological well-being. To manage and facilitate growth in ESDS and foster new interdisciplinary research teams, this project will support the above workshop by providing travel support for junior ESDS researchers to attend the workshop.  The goals of the workshop are to (i) facilitate interdisciplinary research, (ii) present state-of-the-art methods, and (iii) develop new talent in ESDS.  The workshop will include various activities, including panel discussions on the current and future state of ESDS, invited scientific presentations on interdisciplinary topics in ESDS, short courses, a poster session, and team building and brainstorming exercises to establish new research directions and groups.<br/><br/>Modern complex questions in the environmental sciences are increasingly turning to more complex datasets from a wide variety of sources for answers.  Examples of such complex datasets include, but are not limited to, massive correlated datasets generated from climate models and remote sensing, data collected on non-Euclidean spaces such as river networks, rare-event data on meteorological extremes, and multivariate spatio-temporal data such as pollution compositions.  Scientific answers derived from such complex datasets require specialized skills from statisticians and data scientists trained to converse and work with environmental scientists.  This project will provide funding to support the participation of junior environmental data science researchers.  Speakers from statistics and applied environmental sciences will present their state-of-the-art scientific methods for dealing with data arising in the environmental sciences.  Methods discussed will include deep learning for environmental data, computation for massive spatially correlated data, exploiting network information in data collected on river networks, and modern techniques for dealing with point pattern (location) data.  These presentations, along with panel discussions, will outline future interdisciplinary research in environmental statistics and data science. More details about the workshop can be found at: https://community.amstat.org/envr/events/workshops/envr2022workshop.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210506","Collaborative Research: Towards designing optimal learning procedures via precise medium-dimensional asymptotic analysis","DMS","STATISTICS","08/01/2022","06/13/2022","Mohammad Ali Maleki","NY","Columbia University","Standard Grant","Yong Zeng","07/31/2025","$130,000.00","","mm4338@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","079Z, 1269","$0.00","In the past decade, data science and artificial intelligence (AI) have successfully addressed some of the most important scientific and engineering challenges faced in various domains of life like healthcare, education and autonomous systems. A recent example is the deep learning program AlphaFold developed by Google?s DeepMind which can predict a protein?s 3D structure from its amino acid sequence with accuracy competitive to experiment. Despite remarkable progress made in recent years, the design of efficient statistical learning procedures for big data ? a core component in modern data science and AI, has remained ad-hoc, and the precise theoretical understanding of such designed learning schemes is in its infancy. In particular, the following fundamental questions have remained open: (i) how to gain actionable insights into the performance of a learning algorithm without computationally demanding experimental methods? (ii) what is the optimal learning procedure in a given data-intensive environment? Answering such questions will pave the way for the development of the next generation of data science and AI, ultimately contributing to a better quality of life. This project aims to develop a novel analysis approach that can address the above challenges. The new framework is expected to establish quantitatively precise characterizations of the performance of diverse learning algorithms and provide a general recipe for designing optimal learning procedures.<br/><br/>Most of the state-of-the-art learning systems consider sophisticated models in which the number of parameters, p, is substantially large. In most cases p is either much larger than or comparable to the number of observations, n, in the data in use. This new routine has challenged our theoretical understanding of ubiquitous statistical models and procedures in science and technology. On one hand, classical analysis techniques based on the assumption that n is large and p is much smaller than n do not provide valid predictions for statistical learning in the aforementioned contemporary scenarios. On the other hand, modern non-asymptotic analysis frameworks which have been very successful in order-wise risk characterizations, often fall short of delivering sharp results. Hence, it remains largely unclear how to solve various learning problems in an optimal fashion for a variety of statistical models. This project aims to fill this gap by providing a precise theoretical understanding of a large family of statistical models including generalized linear models as a subset. It focuses on the medium-dimensional regime where p scales linearly with n, and creates new tools for studying the accuracy of different learning schemes and characterizing the optimal performances. The expected outcomes of this project are: (i) discovering the precise performance limits of a broad class of learning methods and (ii) evaluating the gaps between information-theoretic lower bounds and performance of the existing algorithms. Such results will ultimately shed light on the design of optimal learning procedures. The proposal will also provide numerous opportunities for interdisciplinary research training and professional career development of future generation of statisticians.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2209685","Unsupervised and Semisupervised Heterogeneity Analysis Based on Gaussian Graphical Models","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2022","06/06/2022","Shuangge Ma","CT","Yale University","Standard Grant","Yong Zeng","08/31/2026","$199,661.00","","shuangge.ma@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269, 7454","068Z","$0.00","Many complex diseases such as cancer are heterogeneous, with seemingly similar patients having different clinical behaviors and varying responses to treatment. To better understand disease biology and more effectively describe and treat diseases, it is of essential importance to accurately model disease heterogeneity, which has been made possible by the fast accumulation of omics data. The existing studies are limited by analyzing simple data distributional properties. This project will advance the paradigm of disease heterogeneity analysis by accommodating how omics measurements are connected. Additionally, the investigator will comprehensively study multiple data scenarios, including when disease outcome (for example, survival) is completely unknown or known for some patients, and when additional data (for instance, on demographics and clinical history) is also available. The investigator will develop a set of leading-edge statistical methods and conduct rigorous theoretical and numerical investigations to compare with existing approaches. This project will fundamentally advance multiple subfields of statistics, including heterogeneity analysis, analysis of high-dimensional data, model selection, and optimization with high-dimensional data. Equally importantly, applications of the developed methods will lead to more accurate identification of heterogeneous patient groups and their omics characteristics for multiple cancer types. This will facilitate the identification of disease subtypes, treatment selection, and prediction of disease paths, having a direct and profound impact on clinical decision-making. Taking advantage of TCGA (The Cancer Genome Atlas) data, the investigator will deliver important heterogeneity models for lung and skin cancer, valuable to basic science and clinical researchers. Additionally, this project will benefit the education and training of undergraduate and graduate students at Yale University, and foster additional collaborations.<br/><br/>Heterogeneity analysis plays an important role in statistics and biomedicine. The development of high-throughput profiling has made it possible to conduct more informative analysis but has also brought numerous statistical challenges. Many commonly used methods are limited to marginal measures especially mean and variance. In this project, building on a recent successful GGM (Gaussian Graphical Model)-based heterogeneity analysis, the investigator will systematically develop GGM-based unsupervised and semisupervised heterogeneity analysis. In particular, the investigator will examine the complicated scenarios with the presence of latent effects and regulating effects as well as heterogeneity analysis under a hierarchy. A series of leading-edge methods built on the penalized fusion technique will be developed. The consistency properties of developed methods will be established under ultrahigh-dimensional settings. The project will also develop efficient computational algorithms and conduct extensive simulations and comparisons. The investigator plans to analyze the TCGA (The Cancer Genome Atlas) data on lung and skin cancer and deliver heterogeneity models along with variable selection and model estimation results. Statistical investigations under this project will broadly shed insight into high-dimensional statistics, heterogeneity modeling, penalization, and network-based analysis. Data analysis will significantly move the field of cancer omics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2209974","RUI: A Family of Versatile Mixture Models for Analyzing Mixed-Type Data with Asymmetry, Outliers, and Missing Values","DMS","STATISTICS","07/15/2022","07/15/2022","Cristina Tortora","CA","San Jose State University Foundation","Standard Grant","Yong Zeng","06/30/2025","$150,000.00","","cristina.tortora@sjsu.edu","210 4TH ST 3RD FL","SAN JOSE","CA","951125569","4089241400","MPS","1269","079Z, 9229","$0.00","Cluster analysis aims to discover patterns and homogeneity in the data. It reveals subgroups in a population of study, and it has applications in many fields. For example, in psychology, the clusters can be groups of patients that can benefit from a specific set of treatments. To apply cluster analysis to a data set, the data need to have some characteristics; for example, some techniques require the data to be continuous, and oftentimes they need pre-treatments. This project will develop a series of new clustering techniques that are suitable for challenging data sets without pre-treatments, such as those with high dimension, missing values, non-continuous variables, or with outliers. Novel statistical approaches and software packages will be produced and made available to general users. Undergraduate students will be directly involved in the research project, and together with graduate students, they will be trained to conduct research in data analysis. Many more students will be involved through class projects and the research outcomes will enrich the content of some of the offered courses. <br/><br/>A widely used approach for cluster analysis is model-based clustering. It assumes that a population is a mixture of subpopulations, each of which can be represented by a density function. A variety of clustering methods and algorithms exist; however, they still have a series of limitations. Outliers and missing data can impact the clustering results, the high number of parameters makes the techniques not usable on high-dimensional data sets. Moreover, many algorithms assume continuous data; and they are not readily adaptable to handle discrete, binary, categorical, or a mixture of continuous and categorical data types. This is a major limitation because, in many fields such as medicine, biology, marketing, and many others, the data have all those characteristics. In this project, new clustering techniques based on non-Gaussian model-based clustering will be developed that will circumvent existing limitations on cluster shape, outliers, missing data, dimension, and data type of current methods. The novel methods will improve the flexibility in detecting skewed clusters and in obtaining robustness when dealing with outliers and missing data. Implicit and explicit dimension reduction techniques will be used for dimension reduction and latent class models will be adopted to deal with mixed-type data. The project will include a study on the indices to select the number of clusters and a thorough comparison with existing methods on real and simulated data will be undertaken, giving the users a guideline on which model to use based on the goal and challenges in their data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210833","Interface of Statistical Learning and Optimal Decisions","DMS","STATISTICS","07/01/2022","06/14/2022","Jianqing Fan","NJ","Princeton University","Continuing Grant","Yong Zeng","06/30/2026","$350,837.00","","jqfan@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","075Z, 079Z","$0.00","Massive datasets are routinely collected in the fields of biological, natural, and social sciences, and engineering and have had a huge impact on statistical analysis, personalized treatments, and decision-making. The driving engines behind these successes are the representation power of deep learning and the dynamic policy optimization framework of Markov decision processes, in addition to the availability of big data. However, training algorithms still take enormous amounts of time and computing power, while statistical and algorithmic efficiencies are also still poorly understood. The aim of this project is to understand and improve statistical methods used in deep learning, reinforcement learning, and big data analysis, with an emphasis on the interfaces between statistical modeling and optimal policy learning. It aims to advance knowledge in AI research, automatic driving and control, e-commerce, molecular mechanisms, biological processes, genetic associations, brain functions, and economic and financial risks. The project will integrate research and education by working closely with undergraduate students, graduate students, and postdoctoral fellows, and develop publicly available computer software with sound theoretical support.<br/><br/>The project aims at developing and understanding various new statistical methods used in deep learning, introducing statistical modeling and learning techniques to enhance policy optimization in reinforcement learning, and addressing several important issues in the analysis of big data. The first aim is to provide a theoretical understanding of various techniques used in deep learning. The investigator will study the role of over-parametrization in nonlinear models and low-rank matrix recoveries, understanding minimum norm interpolation and elucidating the interactions between neural network models and the tails of the data distribution. The second aim is to study the interface between statistical modeling and optimal decision. The investigator plans to study contextual dynamic pricing using semiparametric models and structured nonparametric models and to unveil the statistical theory that underpins the success of deep reinforcement learning from an adaptive function approximation point of view using hierarchical composition models. The investigator will also introduce new dimensionality reduction techniques and theories for policy learning to improve both statistical and algorithmic efficiencies. The third aim is to address several stylized issues in big data analytics. These include Markovian dependence, missing data, highly correlated measurements, censored responses, and distributed data, among others.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210689","Collaborative Research: Theoretical and Algorithmic Foundations of Variational Bayesian Inference","DMS","STATISTICS","07/01/2022","06/14/2022","Anirban Bhattacharya","TX","Texas A&M University","Standard Grant","Yong Zeng","06/30/2025","$199,338.00","Debdeep Pati","anirbanb@stat.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","075Z, 079Z","$0.00","Spectacular advances in data acquisition, processing and storage techniques offer modern-day statisticians a unique opportunity to analyze large and complex datasets of unprecedented richness which arise in many scientific investigations and in studies in the social and economic fields.  Bayesian inference, which combines prior knowledge and data information into a posterior distribution, provides a popular paradigm for probabilistic modeling of complex multi-level datasets and for performing associated inferential or predictive tasks in a principled fashion. For most practical problems, computing the posterior probabilities require numerical approximations; to that end, sampling-based approaches such as Markov chain Monte Carlo and deterministic approximations have both received widespread attention. Among deterministic approaches based on optimization, variational approximations, also commonly referred to as variational inference, is highly popular due to its scalability to large datasets. Through this project, the investigators will explore statistical and algorithmic properties of popular variational procedures and develop new methodology and computational tools grounded on a strong theoretical foundation. The results are targeted to empower practitioners with a better understanding of situations where variational inference is likely to be successful and where potential pitfalls exist. The research will be disseminated through articles and talks at prominent outlets. Additionally, software packages for the methods developed will be made available publicly. The investigators are committed to enhancing the pedagogical component of the proposal through advising students and developing graduate and undergraduate topic courses at their respective institutions.<br/><br/>Motivated by the increasing need to mitigate scalability issues in Bayesian computation, variational inference has tremendously grown in popularity over the last two decades as an approximate Bayesian computational technique. Despite the proven empirical successes of variational inference in large complex data domains, systematic investigations into its statistical properties have commenced only recently. Through this project, the investigators will pose a number of foundational questions to address theoretical challenges in understanding and explaining the great empirical success of variational approximations in parameter estimation, statistical inference, and model selection, coupled with applications in novel domains. The investigators will also develop general purpose sufficient conditions to certify convergence of popularly used variational algorithms. The theoretical development will employ tools from dynamical systems, functional optimization, and optimal transport, leading to a unified treatment of statistical and algorithmic aspects of variational inference. In light of this new theory, the investigators will propose modifications to existing algorithms with certifiably better convergence behaviors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210664","New Statistical Methods for Computer-Assisted Inversion with Applications to Satellite Remote Sensing","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","07/01/2022","06/16/2022","Yves Atchade","MA","Trustees of Boston University","Standard Grant","Yong Zeng","06/30/2025","$365,262.00","","atchade@bu.edu","1 SILBER WAY","BOSTON","MA","022151703","6173534365","MPS","1269, 7454","079Z, 1303, 5294","$0.00","The transformation of satellite-based remotely sensed data into useful climate and geophysical information requires solving radiative transfer equations. Due to the need to process data at the scale of the entire planet, the methodologies currently used to approximate solutions of these complicated equations are limited and do not systematically exploit multi-sensor measurements, ground measurements, and the temporal dynamics at play. At the same time, countries around the world are increasingly turning to remote sensing data to cope with the challenges related to climate change and environmental degradation. To accurately inform stakeholders, better statistical models are needed, particularly for imaging of regions in the developing world where ground measurements are limited. The goal of this research is to develop a new generation of statistical methods for solving, at a more local level, the equations at the heart of satellite remote sensing data processing.<br/><br/>The transformation of satellite-based remotely sensed data into useful climate and geophysical information requires solving some highly non-trivial radiative transfer inverse problems. This project aims to develop a Bayesian framework that combines algorithm unrolling deep learning models and a forward computer code into an inversion map. A transfer learning and a reinforcement learning framework will be developed to combine the inversion map learned in-silico with ground measurements, to adjust for distributional mismatch and to maintain the accuracy of the inversion procedure over time, even as the satellite data distribution changes over time. The project will also contribute at the theoretical level to a statistically deeper understanding of reinforcement learning and algorithm unrolling models. The project aims to improve analysis of global remote sensing data with applications to climate change, bridging of the disciplines of remote sensing, machine learning, and statistics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210312","Nonparametric Inference for Convex Functions and Continuous Treatment Effects","DMS","STATISTICS","07/01/2022","06/23/2023","Charles Doss","MN","University of Minnesota-Twin Cities","Continuing Grant","Yong Zeng","06/30/2025","$96,623.00","","cdoss@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","075Z, 079Z","$0.00","It is both advantageous and necessary in the modern data landscape to use statistical methods that are very flexible and allow the data to ""speak for themselves,"" rather than having researchers make strong unjustifiable prior assumptions about the data.  Unfortunately, such flexible methods generally require the practitioner to ""tune"" the methods in order to get reliable results.  This introduces an ad-hoc element to data analysis and, if incorrectly tuned, such statistical procedures may return incorrect results.  One broad theme of this project is the development of statistical methods (relying on convexity) that are both very flexible and also fully automated, meaning they do not depend on user-chosen tuning parameters.  Another theme is studying very flexible yet efficient procedures for learning about causality when the treatment variable is continuous (e.g., ""what was your drug dosage,"" in the setting of a drug treating an illness) rather than binary (""did you receive the drug, yes or no""); it will use the fully automated methods from the first theme also in the second theme.  It will focus on going beyond estimation and actually performing inference, meaning that it will quantify how reliable the estimates actually are so they can be used for policy/decision making.  In the causal setting, it will consider varied examples such as the effect of number of nurse staffing hours on hospital efficacy, clinical measurements such as BMI (body mass index) on health outcomes, or time spent on education on career outcomes.  The tuning parameter-free procedures based on convexity have many uses in the study of economic data and in optimization questions arising in operations research.<br/><br/>This project is focused on several nonstandard statistical problems that are unified practically by their answering sophisticated questions in modern data settings, and are unified theoretically by their having non-standard rates of convergence and, frequently, non-normal limit distributions. The investigator will consider the following two broad thrusts: (a) nonparametric estimation and inference for shape-constrained convex functions, (b) performing nonparametric tests and/or confidence intervals for a causal continuous treatment effect curve (based on observational data) and related parameters. It is often preferable to use flexible nonparametric methods so that estimation and inference yield reliable results without depending on strong assumptions. Unfortunately, most classical nonparametric methods rely heavily on selection of (potentially many) tuning parameter(s), whose selection can be challenging. In this project, the investigator will study so-called shape constraints that are nonparametric and yet also allow estimation/inference without requiring the choice of a tuning parameter. Assessing causality is one of the most fundamental, but also challenging, tasks of scientific inquiry. With observational data, the gold standard are so-called doubly robust estimators, where ?doubly robust? means optimally efficient.  The investigator will develop the first (pointwise) confidence intervals and hypothesis tests, as well as intervals and tests for the argmax of a concave treatment curve.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210717","Collaborative Research: Theoretical and Algorithmic Foundations of Variational Bayesian Inference","DMS","STATISTICS","07/01/2022","06/14/2022","Yun Yang","IL","University of Illinois at Urbana-Champaign","Standard Grant","Yong Zeng","06/30/2025","$134,186.00","","yy84@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","075Z, 079Z","$0.00","Spectacular advances in data acquisition, processing and storage techniques offer modern-day statisticians a unique opportunity to analyze large and complex datasets of unprecedented richness which arise in many scientific investigations and in studies in the social and economic fields.  Bayesian inference, which combines prior knowledge and data information into a posterior distribution, provides a popular paradigm for probabilistic modeling of complex multi-level datasets and for performing associated inferential or predictive tasks in a principled fashion. For most practical problems, computing the posterior probabilities require numerical approximations; to that end, sampling-based approaches such as Markov chain Monte Carlo and deterministic approximations have both received widespread attention. Among deterministic approaches based on optimization, variational approximations, also commonly referred to as variational inference, is highly popular due to its scalability to large datasets. Through this project, the investigators will explore statistical and algorithmic properties of popular variational procedures and develop new methodology and computational tools grounded on a strong theoretical foundation. The results are targeted to empower practitioners with a better understanding of situations where variational inference is likely to be successful and where potential pitfalls exist. The research will be disseminated through articles and talks at prominent outlets. Additionally, software packages for the methods developed will be made available publicly. The investigators are committed to enhancing the pedagogical component of the proposal through advising students and developing graduate and undergraduate topic courses at their respective institutions.<br/><br/>Motivated by the increasing need to mitigate scalability issues in Bayesian computation, variational inference has tremendously grown in popularity over the last two decades as an approximate Bayesian computational technique. Despite the proven empirical successes of variational inference in large complex data domains, systematic investigations into its statistical properties have commenced only recently. Through this project, the investigators will pose a number of foundational questions to address theoretical challenges in understanding and explaining the great empirical success of variational approximations in parameter estimation, statistical inference, and model selection, coupled with applications in novel domains. The investigators will also develop general purpose sufficient conditions to certify convergence of popularly used variational algorithms. The theoretical development will employ tools from dynamical systems, functional optimization, and optimal transport, leading to a unified treatment of statistical and algorithmic aspects of variational inference. In light of this new theory, the investigators will develop modifications to existing algorithms with certifiably better convergence behaviors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2152780","Randomized quasi-Monte Carlo sampling for scientific computing","DMS","STATISTICS","09/01/2022","08/09/2022","Art Owen","CA","Stanford University","Standard Grant","Yong Zeng","08/31/2025","$200,000.00","","owen@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","079Z","$0.00","This project will improve methods of handling enormous numbers of variables that one must account for in scientific, engineering and commercial computation. In graphical rendering for scientific visualization or for computer games one has to account for many different paths that light could take to form an image.  In financial forecasting it is necessary to model future prices changes of one or more asset prices over many future time steps.  Models for the flow of pollutants must account for varying permeability of soil in many places. All of these problems yield high dimensional problems where the dimension is the number of underlying variables that have to be accounted for.  The standard problem is to average a quantity of interest with respect to random values of all those unknowns.  Related problems involve identifying which of those unknowns is most important.  This project will develop more computationally efficient ways to solve these problems.  The broader impacts include uses in graphics, finance and other scientific and engineering computations.  They also include training of doctoral students to solve these methods and  presentation of the work to other researchers.<br/><br/>The specific methods under study are known as randomized quasi-Monte Carlo (RQMC) sampling. These methods are much more efficient than the more familiar Monte Carlo (MC) sampling which makes random choices of the inputs.  Quasi-Monte Carlo (QMC) makes deterministic and very balanced input choices.  For smooth enough problems QMC improves the convergence rate of MC sampling. RQMC randomizes the QMC points to retain their balance but allow  statistical error estimates by replication.  For smooth enough problems RQMC improves the rate attained by QMC.  One part of the project develops a median of means strategy to combine independent RQMC replicates.  This method is  much more accurate than the usual mean of means because it excludes outliers which can then provide yet more improvements in the convergence rate.  It requires new theoretical inputs to QMC/RQMC coming from analytic combinatorics including a famous theorem of Hardy and Ramanujan.  The project also includes active subspace methods for reducing the effective dimension of high dimensional integrands.  Active  subspaces are a newly evolving method from the uncertainty quantification (UQ) literature that this project will merge with RQMC. UQ methods are becoming more prominent in engineering where users want better ways to judge the accuracy of their numerical methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2152289","FRG: Collaborative Research: Mathematical and Statistical Analysis of Compressible Data on Compressive Networks","DMS","STATISTICS","07/01/2022","06/21/2022","Kai Zhang","NC","University of North Carolina at Chapel Hill","Continuing Grant","Yong Zeng","06/30/2025","$550,825.00","Shahar Kovalsky, Jingfang Huang, Jeremy Marzuola, Yao Li","zhangk@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","075Z, 079Z, 1269, 1616","$0.00","Large-scale high-dimensional data sets are becoming ubiquitous in modern society, particularly in the areas of physical, biomedical, and social applications. This focused research group (FRG) will address the foundational challenges, both computational and theoretical, arising in the analysis of high-dimensional data by leveraging its compressible features. Discovering such compressible features is a major challenge in data analysis, which the team of investigators will approach using hierarchical decompositions derived from spectral, statistical, and algebraic geometric analysis of data. In contrast to interpolation-based methods, such as deep neural networks which are often difficult to interpret, the group will construct optimally defined compressive networks, specifically tailored to such compressible features.  Doing so will enable an accurate and efficient extraction and manipulation of sparse representations of high-dimensional data in an inherently interpretable manner. For instance, one focus of the project is to extend the binary expansion testing methods developed by members of the group, which have shown promise in both statistical power and computational complexity in low-dimensional settings. A high-dimensional generalization of binary expansion testing would, in turn, enable the direct application to selecting personalized medical treatment plans based on increasingly complex data sets. <br/><br/>The FRG investigators will collaborate across the disciplines of mathematical analysis, data science, statistics, and computation, as well as across institutions. The specific goals of this project include generalizing classical concepts of ""compressible"" features using ideas from spectral theory, algebraic geometry, energy and optimization, and network interactions. This will lead to a deeper understanding of the mathematical and statistical foundations of compressible high-dimensional data sets on compressive networks.  Using newly developed compressible features, the FRG team will then design and develop accurate and efficient computational tools for large-scale high-dimensional data sets.  All the work to be done will be aimed at collaborating directly with application domain scientists to enhance the efficacy of the proposed methods. The FRG investigators will also jointly mentor graduate and undergraduate students, who will then have the benefits of training across disciplines and access to a variety of ideas and tools in complementary and integrative research areas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2152070","FRG: Collaborative Research: Mathematical and Statistical Analysis of Compressible Data on Compressive Networks","DMS","STATISTICS","07/01/2022","06/21/2022","Yichao Wu","IL","University of Illinois at Chicago","Continuing Grant","Yong Zeng","06/30/2025","$241,321.00","Ping-Shou Zhong","yichaowu@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","075Z, 079Z, 1616","$0.00","Large-scale high-dimensional data sets are becoming ubiquitous in modern society, particularly in the areas of physical, biomedical, and social applications. This focused research group (FRG) will address the foundational challenges, both computational and theoretical, arising in the analysis of high-dimensional data by leveraging its compressible features. Discovering such compressible features is a major challenge in data analysis, which the team of investigators will approach using hierarchical decompositions derived from spectral, statistical, and algebraic geometric analysis of data. In contrast to interpolation-based methods, such as deep neural networks which are often difficult to interpret, the group will construct optimally defined compressive networks, specifically tailored to such compressible features.  Doing so will enable an accurate and efficient extraction and manipulation of sparse representations of high-dimensional data in an inherently interpretable manner. For instance, one focus of the project is to extend the binary expansion testing methods developed by members of the group, which have shown promise in both statistical power and computational complexity in low-dimensional settings. A high-dimensional generalization of binary expansion testing would, in turn, enable the direct application to selecting personalized medical treatment plans based on increasingly complex data sets. <br/><br/>The FRG investigators will collaborate across the disciplines of mathematical analysis, data science, statistics, and computation, as well as across institutions. The specific goals of this project include generalizing classical concepts of ""compressible"" features using ideas from spectral theory, algebraic geometry, energy and optimization, and network interactions. This will lead to a deeper understanding of the mathematical and statistical foundations of compressible high-dimensional data sets on compressive networks.  Using newly developed compressible features, the FRG team will then design and develop accurate and efficient computational tools for large-scale high-dimensional data sets.  All the work to be done will be aimed at collaborating directly with application domain scientists to enhance the efficacy of the proposed methods. The FRG investigators will also jointly mentor graduate and undergraduate students, who will then have the benefits of training across disciplines and access to a variety of ideas and tools in complementary and integrative research areas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210561","Nonparametric Estimation and Inference with Network Data","DMS","STATISTICS","08/15/2022","08/09/2023","Matias Cattaneo","NJ","Princeton University","Standard Grant","Yong Zeng","07/31/2025","$350,000.00","Ricardo Masini","cattaneo@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","","$0.00","Network data is ubiquitous in the statistical, social, behavioral, and biomedical sciences. This type of dependent data captures interactions between the units of study, such as output between firms, trade between countries, or fundraising between politicians. Network-based information is widely used nowadays for both testing domain-specific hypotheses and policy-making decisions in data and decision sciences. However, the remarkable proliferation of network data in empirical work has not been accompanied by a complete development of statistical methods guiding its correct use and providing valid estimation and inference procedures. Current practice employing network data is limited by the few results available in the literature, and many estimation and inference problems of practical importance remain unresolved. In this project, the investigators seek to undertake a comprehensive study of non-parametric and semi-parametric statistical methods employing dyadic data, data indexed by pairs of units such as trade between two countries. The established methods and theory will serve as a building block for the analysis of more general network data. The investigators plan to develop general-purpose software to implement the main theoretical and methodological results. The project will provide training opportunities for graduate students.<br/><br/>The research will focus on non-parametric and semi-parametric estimation and inference methods employing dyadic network data, which poses specific technical challenges due to its inherent lack of statistical independence. The project's ultimate goal is to develop comprehensive large-sample approximations leading to optimal and/or robust point estimation and statistical inference procedures for functional estimation, covering density and regression functions as special cases. To this end, the investigators will develop novel strong approximation results for stochastic processes, which will then be deployed to approximate the distribution of functional statistics based on dyadic network data. Minimax optimal uniform convergence rates for different non-/semi-parametric estimators using network data will also be established. The main theoretical results will then be applied to semiparametric estimation relying on network data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2140413","Advances in Data Science: Theory, Methods and Computation","DMS","STATISTICS","03/01/2022","02/16/2022","Moumita Karmakar","TX","Texas A&M University","Standard Grant","Yulia Gel","02/28/2023","$18,605.00","Bani Mallick","mkarmakar@stat.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","7556","$0.00","Due to advancements in data acquisition techniques over the last two decades, new types of exceedingly complex datasets have emerged and present tremendous challenges that require synergy of interdisciplinary ideas for analysis and decision making. As a result, the field of data-science is rapidly evolving as an interdisciplinary field, where advances often result from combinations of ideas from multiple disciplines. A convening of leading experts, early-career researchers, and students from varied disciplines to exchange ideas is essential for progress in this field. Texas A&M University will host a two-day conference on Advances in Data Science in February 2022. More information on the conference can be found at  https://stat.tamu.edu/advances-in-data-science-conference/.<br/> <br/>The primary objective of the conference is to provide a much-needed platform for accelerating the depth and quality of research on the foundations of data science through interdisciplinarity. The conference will bring together researchers from three major disciplinary areas (Statistics, Mathematics and Engineering) for presentation and dissemination of their research, to engage in discussions and foster future collaborations. This conference will involve women, minorities and young researchers across the nation. The conference will present a tremendous opportunity for first generation undergraduate students to be inspired and pursue careers in data-science in both academia and industry.  The conference will feature a number of activities to engage the students and recognize their contributions through awards.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210439","Statistical Modeling for Complex Networks","DMS","STATISTICS","09/01/2022","08/02/2022","Ji Zhu","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yong Zeng","08/31/2025","$350,000.00","Elizaveta Levina","jizhu@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Recent advances in technology have led to an explosion of data being collected in many areas of application. Many of these data have complex structures, in the form of text, images, video, audio, streaming data, etc, for example. This project focuses on one important type of complex data structure, networks, or graphs. Such data are common in diverse engineering and scientific areas, including biology, medicine, sociology, computer science, electrical engineering, economics, and so on. While there has been extensive research on networks, much of it only deals with the presence/absence of pairwise relationships. However, real-world relationships are often more complicated. The current research program aims to go beyond the pairwise presence/absence relationship and develop statistical methods to characterize and model more complex network structures. The research program will make significant contributions in several areas, including Statistics, Biology, Computer Science, Healthcare, Electrical Engineering, Medicine, Physics, Psychology, and Sociology. The investigators plan to train STEM workforce members by mentoring undergraduate and graduate students, developing a new course, and organizing interdisciplinary workshops. The educational program also includes substantial initiatives in maintaining a research group with a significant portion of women and continuing to actively recruit and support a diverse group of students.<br/><br/>The research aims to develop new statistical methodologies and associated theory that incorporate higher-order structures into network modeling. Such data structures are becoming increasingly common in various fields. Specifically, the investigators aim to study three different but related problems: a) leveraging subgraphs or higher-order structures in a network and developing new community detection methods for networks with dependent edges; b) developing novel latent space models and theory that accommodate the well-known balance theory, i.e., ""the friend of my friend is my friend"" and ""the enemy of my enemy is my friend,"" for signed networks; c) developing new latent space models and theory for the less studied, though commonly encountered, polyadic relations that involve more than two nodes simultaneously, i.e., hypergraphs, using determinantal point processes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210712","Assessment of Regression Models with Noncontinuous Outcomes","DMS","STATISTICS","07/01/2022","07/31/2023","Lu Yang","MN","University of Minnesota-Twin Cities","Continuing Grant","Yong Zeng","06/30/2025","$134,790.00","","luyang@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","Regression models are widely used to synthesize researchers' knowledge about relationships between predictors and an outcome of interest, for example, the effects of treatments on mortality. However, researchers' prior information may not adequately describe the patterns in the data. The resulting model deficiency can lead to biased parameter estimates, misleading conclusions, lack of generalizability of results, and unreliable predictions, among many other detrimental consequences. Judging a model's adequacy to describe the data is thus a routine and critical task in statistics. Currently, there is a lack of valid tools for assessing regression models with noncontinuous outcomes. Noncontinuous data are found frequently in many domains of science. Examples include stages of cancer in medical research (ordinal), the number of offspring of organisms in ecology (count), and rainfall amounts in climate research (semicontinuous with a probability of zero corresponding to no rain). This project will fill this gap and provide a framework for model assessment with noncontinuous outcomes. The project outputs will benefit researchers and practitioners in a wide range of areas by enabling them to draw conclusions from their models with statistical confidence and find directions for model improvements. The project will integrate research into course development and graduate student mentoring and develop free software for broad dissemination. <br/><br/>The assessment of regression models with noncontinuous (e.g., count, binary, ordinal, and semicontinuous) outcomes is challenging and has many fundamental issues. In this context, standard regression model assessment tools such as Pearson and deviance goodness-of-fit tests do not follow their null distributions under the true model, calling into question the legitimacy of model assessment based on these tools. In addition, existing assessment tools might have little statistical power in detecting model misspecification. The long-term goal of this project is to establish a principled framework for assessing regression models with noncontinuous outcomes. The envisioned framework includes first an omnibus goodness-of-fit test for regression models with general discrete outcomes. This goodness-of-fit test is based on the distance between the quasi-empirical residual distribution function and its null pattern (the identity function). It can assist analysts in obtaining p-values and confidence statements on the adequacy of their models. The theoretical properties of the test statistic will be studied using empirical process theory. Extensive simulation studies and real data analysis will be conducted for evaluation. Second, the investigator will study a new type of residuals for discrete outcomes, which can help identify potential causes of misspecification and provide directions for model improvement. Third, this framework will be extended to semicontinuous outcomes, which are scarcely studied in the literature. Completion of this research will fill the gap between regression models and an informative assessment framework for noncontinuous outcomes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210790","Low-Rank Functional Data Analysis for Time-Resolved Spectroscopy and the Search for Earth-Like Exoplanets","DMS","STATISTICS","08/01/2022","08/09/2022","Thomas Loredo","NY","Cornell University","Standard Grant","Yulia Gel","07/31/2025","$304,670.00","David Ruppert","loredo@astro.cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","1214, 1269","$0.00","Spectroscopy is essential in many scientific fields, including astronomy, where comparisons of lab-based spectra to stellar spectra reveal the chemical compositions of stars. Time-resolved spectroscopy?measuring spectra changing over time?has emerged as a powerful tool for probing the dynamics of many systems. It is used to study the variability of stars, the molecular dynamics of complex compounds, and time-dependent chemical processes in biological systems. This project aims to develop a framework for analyzing time-resolved spectroscopic data in settings where incomplete measurements are made across related objects, or repeatedly for one system, with random variations seen across the replications. Careful pooling and analysis of data across the replications can identify signatures of the processes producing spectral variability. The main application will be modeling time-resolved spectroscopy of stars as candidate hosts of extrasolar planetary systems, particularly data from searches for Earth-like planets orbiting Sun-like stars (exo-Earths). Small extrasolar planets are not directly visible, but their presence can be discerned: the tug of a planet on its star produces a small wobbling motion, which can be detected by measuring extremely small, time-varying Doppler shifts of spectral lines. Currently, the main obstacle to observing small planets is the spectral activity of the host star?the comings and goings of dark sunspots, bright plages, and flares can mask or mimic a planet's time-dependent signal. The project will develop new algorithms to disentangle stellar activity signals from planet signals in time-resolved spectral data. The project will also support training of a diverse population of astronomy students and postdoctoral researchers in advanced statistics, many of whom will go on to pursue non-academic STEM careers involving data science. <br/><br/>A time-resolved (dynamic) spectrum can be described with a bivariate function of wavelength and time representing the (relative) intensity of light measured by a spectrograph versus wavelength and time. The goal of this project is to develop a framework to model data measuring a single dynamic spectrum, or many related dynamic spectra, with incomplete sampling and noise, for example, from observations of many candidate exoplanet systems with similar host stars. The framework will integrate techniques from approximation theory, and from functional data analysis (FDA), the branch of statistics concerned with analyzing data comprising measurements of ensembles of functions. A core component of the framework will be use of separable expansions, writing the dynamic spectrum as a sum of products of paired univariate functions of wavelength (""speclets"") and time (""modulators""). When the bivariate function of wavelength and time is given, approximation theory identifies optimal speclets and modulators, using the asymmetric Hilbert-Schmidt decomposition, a procedure resembling singular value decomposition (SVD). Real data do not provide precise, dense sampling of the bivariate function of wavelength and time. The project will build on FDA approaches, including functional principal components analysis (FPCA) and hierarchical Bayesian stochastic process models, to enable speclet and modulator basis discovery that accounts for noise and incomplete, irregular sampling. The framework will be applied to simulated spectra of populations of stars to build a model for stochastic stellar variability that will enable discovery of exo-Earths in Doppler radial velocity searches for exoplanets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210456","High-Dimensional Nonstationary Processes for Spatial Analysis and Machine Learning","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2022","06/06/2022","Huiyan Sang","TX","Texas A&M University","Standard Grant","Yulia Gel","08/31/2025","$179,970.00","","huiyan@stat.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269, 7454","1269, 1303, 5294","$0.00","Numerous application problems in geosciences, climate and environmental sciences, public health, social sciences and traffic statistics involve large amounts of spatial data collected from complex constrained domains with non-trivial geometries, such as irregular boundaries with sharp concavities, interior holes due to geographic constraints, and river or road networks. Practitioners are interested in modeling complex spatial dependence because it plays the most important role in the estimation and prediction of spatial problems.  However, there is very limited tool for the estimation and prediction of spatial problems on complex domains. This project aims to develop new statistical models and algorithms to better characterize the potentially much more heterogeneous spatial dependence in large data sets while respecting irregular geometries in the data. The methodology will be applicable to a broad range of real problems in multiple interdisciplinary fields. The proposed research initiatives will offer numerous opportunities for interdisciplinary research training at undergraduate and graduate levels, with a particular focus on advancing diversity and inclusion in statistical sciences.<br/><br/>This project will introduce a new class of nonstationary models with flexible locally stationary dependence structures for large spatial data. The detection of locally stationary structures is achieved by a novel manifold partition model with flexible partition boundaries while respecting irregular shapes of domain boundaries. The project will further develop a novel framework to build a valid stochastic process model to knit together local models. Both parameter estimation and prediction can be performed under a unified framework, and both discontinuities/abrupt changes and smoothness in spatial random field can be captured.  Moreover, the project will result in new scalable and parallelizable divide-merge-conquer inference tools, harnessing the power of locally stationary assumptions. The performance of the proposed methods will be tested with simulation studies and applied to real-life applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210837","Minipatch Learning for Selection, Stability, Inference, and Scalability","DMS","STATISTICS","08/01/2022","07/20/2022","Genevera Allen","TX","William Marsh Rice University","Standard Grant","Yong Zeng","07/31/2025","$261,146.00","","gallen@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","068Z, 079Z, 8091","$0.00","Massive amounts of data are now collected by nearly every industry and academic discipline. Uncovering the hidden insights in such data holds the key to major scientific challenges such as understanding how the brain works, discovering mechanisms leading to diseases such as cancer and Alzheimer's disease, and combating climate change, among many others. But discovering key features and important relationships in complex and huge data poses major statistical and computational challenges. The investigator aims to develop new statistical machine learning approaches and theory for this task that break up huge data sets into small random subsets called minipatches to facilitate both faster computation and improved statistical efficiency. The new methods will be implemented in open-source software and applied to huge biomedical datasets in genomics and neuroscience. The project will provide undergraduate and graduate students training and professional development opportunities.<br/><br/>Discovering key features and important relationships in complex and huge data commonly found in biomedicine poses not only major computational challenges but also critical statistical challenges. To tackle these challenges, the investigator plans to develop a new framework termed minipatch learning. Inspired by the successes of random forests, stability approaches in high-dimensional statistics, and stochastic optimization strategies, the investigator will build ensembles from many random tiny subsets of both observations and features or variables called minipatches. While ensemble learning strategies are commonly used in supervised machine learning, the investigator will use minipatch learning for the tasks of feature selection, model-agnostic inference for feature importance, and learning relationships amongst features through graphical models. The approach, which trains on very tiny subsets of the data, is expected to have dramatic computational and memory savings. The investigator aims to show both theoretically and empirically that such a strategy poses significant statistical advantages as well.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210849","Scalable Algorithm Design for Unbiased Estimation via Couplings of Markov Chain Monte Carlo Methods","DMS","STATISTICS","07/01/2022","07/31/2023","Guanyang Wang","NJ","Rutgers University New Brunswick","Continuing Grant","Yong Zeng","06/30/2025","$110,340.00","","guanyang.wang@rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","Markov chain Monte Carlo methods have revolutionized statistics and data science in the past several decades. These methods are routinely used for simulation and numerical integration in nearly all scientific areas. In practice, however, parallel implementation is a long-standing bottleneck for Markov chain Monte Carlo methods. Existing Monte Carlo estimators generally suffer from bias, which precludes their direct use of modern parallel computing devices. This project aims to design a new framework to construct unbiased estimators based on Markov chain Monte Carlo outputs. Results of the project will advance the development of unbiased estimators and efficient algorithms that can scale up for massive datasets. The new method will empower practitioners in scientific fields such as chemistry, biology, and computer science that face high-dimensional simulation problems. This project will provide training opportunities to undergraduate and graduate students. <br/><br/>The technical goals of this project include two interconnected aspects. The first focus is on unbiased estimators for general simulation-based inference problems by combining the idea of the existing unbiased Markov chain Monte Carlo and Multilevel Monte Carlo methods. The second aspect focuses on designing fast algorithms for unbiased estimation. The efficiency of the developed method relies on the underlying Markov chain Monte Carlo algorithm and the design of a coupling strategy between the two Markov chains. This project will theoretically investigate the convergence speed of different existing algorithms and develop practical, implementable algorithms. The new method will be applied to solve problems arising from diverse areas such as operation research, optimization, and machine learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2229408","Collaborative Research: AMPS: Robust Failure Probability Minimization for Grid Operational Planning with Non-Gaussian Uncertainties","DMS","AMPS-Algorithms for Modern Pow, ","12/01/2022","08/09/2022","Anirudh Subramanyam","PA","Pennsylvania State Univ University Park","Standard Grant","Yulia Gel","11/30/2025","$290,000.00","","azs7266@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","045Y, W325","1269, 5294, 8396, 9263","$0.00","The electric power industry accounted for the second-largest portion of all carbon emissions across economic sectors in 2020. Renewable energy resources, particularly wind and solar, are critical to decarbonizing the grid and ensuring the nation's future prosperity and welfare. However, because of their inherent and unavoidable intermittency and variability, successful integration of renewable energy resources in the nation's energy mix poses fundamental challenges for day-to-day grid operations. Failure to account for this uncertainty during planning can result in loss of service and grid de-stabilization, thus jeopardizing not only the achievement of decarbonization targets but also system reliability. This project develops the next generation of mathematical methods, computer models, and algorithms for grid operational planning, which accurately and systematically take into account the non-normal and multi-modal nature of renewable uncertainty, as well as the nonlinear and often counter-intuitive physical laws that govern electric power networks. The project's methods and computer implementations shall benefit and inform diverse planning tools, both within the electric power sector as well as the broader energy sector, including those of private companies and vendors who specialize in power systems software. The project further impacts education and the broader society by training undergraduate and graduate STEM students in energy systems optimization and the foundations of electric power grid operations, thereby enabling them to apply their analytical skills to design more environmentally- and economically-efficient future energy systems.<br/><br/>The project contributes a general methodology, including new mathematical models, theory, and algorithms, to systematically account for non-Gaussian error distributions of renewable energy forecasts, in one of the most fundamental power system planning problems called AC Optimal Power Flow. A general treatment of non-Gaussian errors in electric load and renewable energy forecasts has not been considered before in grid planning, despite being exhibited in data. The project rigorously integrates risk and uncertainty in this context by developing a novel methodology for optimization under non-Gaussian probabilistic constraints. This is achieved by exploiting the representability and analyticity of Gaussian mixture models and by designing algorithms that are modular enough to allow current methods which are proven to work well for Gaussian errors to be reusable with only minor modifications. The generality of the approach is expected to spur new algorithms in the broader field of chance-constrained optimization, including nonlinear nonconvex problems whose constraints are affected by Gaussian mixture uncertainties. The project also rigorously accounts for misspecification of the mixture model parameters by designing novel non-Gaussian ambiguity sets, which have not been studied before but have the potential to enable the discovery of robust network operating points with improved out-of-sample performance and reliability. The project uses real utility data to guide model validation and experimentation and also provides a set of practical recommendations for system operators to facilitate the adoption of the developed methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2143468","CAREER: New Paradigms of Estimation and Inference in Constrained Nonparametric Models","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","07/01/2022","07/31/2023","Qiyang Han","NJ","Rutgers University New Brunswick","Continuing Grant","Yong Zeng","06/30/2027","$142,857.00","","qh85@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1253, 1269","1045, 7715","$0.00","This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2). Nonparametric methods are a basic toolkit for analyzing multivariate and high-dimensional data in modern statistics. However, many standard nonparametric methods are known to face two key challenges. First, the performance of these methods is usually sensitive to multiple subjective choices of tuning parameters. Second, the methods developed for the purpose of estimation typically cannot be directly used for statistical inference. This project aims to systematically develop a new paradigm of multi-dimensional nonparametric methods under natural shape constraints that simultaneously resolves these two critical issues. In particular, the shape-constrained methods to be developed in this project will not only be fully automated without ad-hoc tuning, but also enjoy simultaneous optimal estimation and inference merits. The project will integrate research with education through course development, research mentoring for undergraduate and graduate students, especially those from underrepresented groups, and summer programs. <br/><br/>This project will focus on two complementary categories of research problems. Problems in the first category aim at understanding the potentials of a class of non-standard generalized block estimators, and the drawbacks of standard methods such as the maximum likelihood or least squares. Problems in the second category aim at developing fully automated inference procedures for several canonical local and global inference targets using the non-standard methods, in a few related models. The common ground for the solutions to these problems lies in an emerging research area of non-standard distributional characterizations of multi-dimensional shape-constrained estimators initiated recently by the PI and his coauthors.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210819","A New Stochastic Neural Network: Statistical Perspectives and Applications","DMS","STATISTICS, Secure &Trustworthy Cyberspace","08/01/2022","06/16/2022","Faming Liang","IN","Purdue University","Standard Grant","Yong Zeng","07/31/2025","$330,000.00","","fmliang@purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1269, 8060","025Z, 075Z, 079Z","$0.00","The integration of computer technology into science and daily life has enabled scientists to collect massive volumes of data. Deep learning has been developed as a major tool for big data analysis. However, the structure and parameters of the deep neural network (DNN) are hard to interpret, which can cause severe issues in human-machine trust when applying the DNN to real-life settings. To address this concern, researchers have made considerable progress in sparse deep learning, which provably leads to consistent selections of relevant variables for the underlying nonlinear system. However, the internal nodes and parameters of the sparse DNN are still hard to interpret due to the black-box nature of the DNN. The investigator will develop a new type of stochastic neural network (StoNet), which is a composition of many simple regressions. The StoNet is asymptotically equivalent to the conventional DNN in function approximation as the training sample size becomes large, while its structure and parameters are more interpretable from statistical perspectives. The StoNet can be employed to address many fundamental statistical tasks such as nonlinear sufficient dimension reduction, causal inference, missing data, and private deep learning that are difficult to handle with the conventional DNN. The StoNet bridges linear models and deep learning by its compositional regression structure, which deepens people?s understanding of deep learning. The StoNet has potentially immense benefits to the development of trustworthy artificial intelligence (AI) and data driven technologies. The research results will be disseminated to communities of interest via collaborations, publications, and conference presentations. The project will also have significant impacts on education by directly involving graduate students in the research and incorporating the research results into undergraduate and graduate courses. <br/><br/>The StoNet provides a more general and powerful model for big data analysis than the conventional DNN. It can be employed to address many fundamental statistical tasks that are frequently encountered in modern data science. The investigator will show that the StoNet results in a novel nonlinear sufficient dimension reduction method by imposing a Markovian structure on its layers in training. The resulting method is scalable and can deal with much larger datasets than can the existing methods. For causal inference, the investigator will develop a causal-StoNet as a variant of StoNet, where the treatment variable is included as a visible unit in a middle layer of the network. The causal-StoNet provides a convenient way for modeling the outcome function and propensity score, imputing missing data, and identifying relevant covariates for high-dimensional problems. For private deep learning, the investigator will develop a varying truncation noisy stochastic gradient descent algorithm for training the StoNet. Compared to the existing private stochastic gradient descent algorithms, the proposed algorithm avoids gradient clipping and improves convergence and utility of deep learning while ensuring rigorous differential privacy guarantees.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210675","Collaborative Research: Learning Graphical Models for Nonstationary Time Series","DMS","STATISTICS","07/01/2022","06/08/2022","Sumanta Basu","NY","Cornell University","Standard Grant","Yulia Gel","06/30/2025","$150,000.00","","sumbose@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","1269","$0.00","In the biological and social sciences, many central questions require one to understand how interactions within a complex system evolve over time. For example, in neurosciences, monitoring time-varying connections among different brain regions is important for studying the progression of neurodegenerative diseases. As another example, risk management and monitoring in interconnected financial markets often require learning how the linkages among different firms evolve over time. These examples demonstrate the need to develop rigorous and scalable statistical methods that are able to learn the evolution of connectivity from large-scale  complex time series data. Such methods can help offer insights into the working of a complex system and guide data-driven policy making.<br/><br/>While graphical models (GM) offer a powerful  framework for data-driven discovery of network architecture,  existing statistical research in this area has focused primarily on modeling time-invariant connections from stationary time series. This project will develop estimation and inference methods for a nonstationary graphical model framework called NonStGM. This framework captures  nonstationary dynamics in a multivariate system in the form of a sparse operator in the Fourier domain, whose structure can in turn be estimated from data using regularized regression methods. Key emphasis will be given on two classes of structured nonstationarity which are prevalent in many applications: (a) local stationarity that allows both abrupt changes and smooth evolution of the temporal dynamics, and (b) periodic stationarity. NonStGM structures learned from large-scale time series data  will be used to build directed graphs with time-varying vector autoregressive (VAR) models. Algorithms developed in this project  will be validated with extensive numerical experiments and real electroencephalogram  (EEG) data sets. All products will be made publicly available in the form of open-source software packages. These products are expected to aid clinical researchers, amongst others, in their understanding of connectome abnormalities in the brains of patients suffering from neurological disorders. Research outcomes will be integrated into educational modules of graduate level courses. The project will provide numerous opportunities to train graduate students in a topical research area of large-scale time series modeling and will actively focus on enhancing diversity and inclusion in statistical sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210658","Collaborative Research: Semiparametric and Reinforcement Learning for Precision Medicine","DMS","STATISTICS","08/15/2022","08/02/2022","Xinyi Li","SC","Clemson University","Standard Grant","Yulia Gel","07/31/2025","$133,850.00","","lixinyi@clemson.edu","201 SIKES HALL","CLEMSON","SC","296340001","8646562424","MPS","1269","079Z, 1269","$0.00","Precision medicine seeks to optimize the medical treatments tailored to individual characteristics, including genetic features, demographic information, environmental factors, etc. Individualized treatment rule formalizes the process of decision making that translates the patients? information into the recommended treatment, and a dynamic treatment regime consists of the sequence of individualized treatment decisions for one or more treatment decision times. Meanwhile, recent developments in medical imaging technologies dramatically affect disease and health studies. Biomedical imaging and imaging-guided interventions are key in the infrastructure for precision medicine. It is of great importance to developing an approach for incorporating imaging data along with other abundant information in precision medicine research. However, the current exploration for these aforementioned abundant features in precision medicine study is far from sufficient. Motivated by this, the project targets to build the statistical analysis framework in precision medicine incorporating abundant features and provide the support of data-driven decision making, which will not enrich statistical methodological studies but provide an integrated early diagnosis tool and an informative tool to guide treatment and lifestyle intervention in health science. In addition, the project will provide training and support for graduate students, as well as instructions in both undergraduate- and graduate-level courses.<br/><br/>The PIs will adapt the Q-learning, semiparametric learning, functional data analysis, and reinforcement learning frameworks to precision medicine with abundant features, including medical images, genetic features, demographic information, environmental factors, etc. Focusing on different scenarios, this research program consists of three components: (i) functional individualized treatment regime study incorporating abundant features, along with the development of a novel basis expansion tool to handle the multi-dimensional image feature; (ii) generalized functional individualized treatment regime study incorporating abundant features, which allows the response variable discrete; and (iii) functional Q-learning with abundant features, which extends the methodology to the multi-stage decision setting. The investigators will conduct the theoretical developments, develop efficient algorithms, and implement and apply the tools to real-world data for all these components in this project. From the statistical point of view, the theoretical explorations will yield more insights into semiparametric and reinforcement learning in precision medicine with abundant features. From the computational point of view, efficient and scalable algorithms will be developed and implemented in a form of publicly available software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210659","Collaborative Research: Semiparametric and Reinforcement Learning for Precision Medicine","DMS","STATISTICS","08/15/2022","08/02/2022","Michael Kosorok","NC","University of North Carolina at Chapel Hill","Standard Grant","Yulia Gel","07/31/2025","$260,000.00","","kosorok@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","079Z, 1269","$0.00","Precision medicine seeks to optimize the medical treatments tailored to individual characteristics, including genetic features, demographic information, environmental factors, etc. Individualized treatment rule formalizes the process of decision making that translates the patients? information into the recommended treatment, and a dynamic treatment regime consists of the sequence of individualized treatment decisions for one or more treatment decision times. Meanwhile, recent developments in medical imaging technologies dramatically affect disease and health studies. Biomedical imaging and imaging-guided interventions are key in the infrastructure for precision medicine. It is of great importance to developing an approach for incorporating imaging data along with other abundant information in precision medicine research. However, the current exploration for these aforementioned abundant features in precision medicine study is far from sufficient. Motivated by this, the project targets to build the statistical analysis framework in precision medicine incorporating abundant features and provide the support of data-driven decision making, which will not enrich statistical methodological studies but provide an integrated early diagnosis tool and an informative tool to guide treatment and lifestyle intervention in health science. In addition, the project will provide training and support for graduate students, as well as instructions in both undergraduate- and graduate-level courses.<br/><br/>The PIs will adapt the Q-learning, semiparametric learning, functional data analysis, and reinforcement learning frameworks to precision medicine with abundant features, including medical images, genetic features, demographic information, environmental factors, etc. Focusing on different scenarios, this research program consists of three components: (i) functional individualized treatment regime study incorporating abundant features, along with the development of a novel basis expansion tool to handle the multi-dimensional image feature; (ii) generalized functional individualized treatment regime study incorporating abundant features, which allows the response variable discrete; and (iii) functional Q-learning with abundant features, which extends the methodology to the multi-stage decision setting. The investigators will conduct the theoretical developments, develop efficient algorithms, and implement and apply the tools to real-world data for all these components in this project. From the statistical point of view, the theoretical explorations will yield more insights into semiparametric and reinforcement learning in precision medicine with abundant features. From the computational point of view, efficient and scalable algorithms will be developed and implemented in a form of publicly available software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210007","Collaborative Research: Statistical Inference for Multivariate and Functional Time Series via Sample Splitting","DMS","STATISTICS","07/01/2022","06/13/2022","Runmin Wang","TX","Texas A&M University","Standard Grant","Yulia Gel","06/30/2025","$110,000.00","","runminw@smu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","1269","$0.00","Multivariate and functional time series are prevalent and routinely collected in many fields. Statistical inference of such time series is a fundamental problem in modern time series analysis and has broad applications in many scientific areas, including bioinformatics, business, climate science, economics, finance, genetics, and signal processing. Compared with existing methodologies, this research project will provide nonparametric inference procedures that can accommodate a wide range of dimensionality and require weak assumptions on the data generating processes. The methodology ensuing from the project will be disseminated to the relevant scientific communities via publications, conference and seminar presentations, and the development of open-source software. The project will involve multiple research mentoring initiatives, including efforts on broadening participation, and will offer advanced topic courses to introduce the state-of-the-art techniques in time series analysis. The project will provide a broad range of interdisciplinary training opportunities at all educational levels and will contribute to the future workforce professional development.<br/><br/>The project will develop a systematic body of methods and theory on inference for both multivariate (including high-dimensional) time series and functional time series based on sample splitting (SS) and self-normalization (SN). Recently, the SN technique has been advanced to the inference of high-dimensional time series, but it requires the use of a trimming parameter. Also, its scope of applicability is limited to high-dimensional time series with weak panel dependence which might be unrealistic in many modern time series applications. In turn, the existing SN for functional time series relies on dimension reduction by functional principal component analysis and, hence, the resulting procedure may be powerless when the alternative is orthogonal to the space spanned by the top principal components used in the procedure. To address these major limitations, this project will develop a new unified framework based on SS-SN, in conjunction with inference for multivariate and functional time series, and investigate its utility in application to analysis of time series of low, medium, high or infinite dimensions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2150557","Conference on Advances in Bayesian and Frequentist  Theory and Methods for Complex Data","DMS","STATISTICS","02/01/2022","01/28/2022","Zhiqiang Tan","NJ","Rutgers University New Brunswick","Standard Grant","Yulia Gel","01/31/2023","$15,000.00","","ztan@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","7556","$0.00","This project supports the Conference on Advances in Bayesian and Frequentist Theory and Methods for Complex Data, which will be held on April 8-9, 2022 at Rutgers University, Piscataway, New Jersey. The conference will bring together established, mid-career, and early-career researchers with diverse expertise to discuss the latest advances, current challenges, and emerging directions in statistical theory and methodology, with a particular focus on high dimensional problems. The conference will provide an important venue to stimulate interdisciplinary collaboration for tackling problems involving large and complex data. This initiative will also provide support for participation of early-career researchers and graduate students, including women and those from underrepresented minority groups.<br/><br/><br/>There has been a substantial and diverse development in statistical sciences to analyze and extract knowledge from high-dimensional and complex data, which can be found in various settings, from scientific research to industrial applications. Such development has often been powered by the connection and cross-fertilization between frequentist and Bayesian statistics. This conference will provide a unique opportunity for direct comparison and connection between frequentist and Bayesian approaches in high dimensional settings. The event will feature 18 plenary talks which will highlight the most recent research results in the field. A poster session will also be organized to encourage presentations from early career researchers and graduate students. More information can be found on the conference webpage: https://www.stat.rutgers.edu/conference-on-bayesian-and-frequentist-theory-and-methods<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2152746","Computationally Tractable Inference for Multi-Messenger Astrophysics","DMS","CDS&E-MSS","08/01/2022","06/26/2023","Galin Jones","MN","University of Minnesota-Twin Cities","Continuing Grant","Yulia Gel","07/31/2025","$98,992.00","Vuk Mandic, Sara Algeri, Michael Coughlin","galin@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","8069","069Z, 1269, 9263","$0.00","Multi-messenger astrophysics leverages multiple modalities of observations, such as gravitational waves, light, neutrinos, and cosmic rays, to observe astrophysical events and objects. Two specific issues arising in multi-messenger astrophysics motivate much of this research project: detecting cross-correlation between gravitational waves and electromagnetic sky maps and constraining the neutron star equation of state. Addressing these issues formally with the data produced from multi-messenger observations presents challenges that require the development of novel statistical and computational methodology. This project aims to develop improved (1) statistical techniques for detecting cross-correlation between a conjectured gravitational wave background and the cosmic microwave background sky map and (2) Bayesian algorithms for analysis of gravitational wave signatures in binary neutron star mergers. The research will provide interdisciplinary opportunities for professional development of the next generation of statisticians and astronomers.<br/> <br/>The project will focus on developing methods for performing goodness-of-fit tests for multidimensional parametric models characterized by high computational complexity, paying particular attention to identifying the sources of mismodelling. Another focus will be developing methods for convergence analysis of Markov chain Monte Carlo methods in both low-dimensional (fixed sizes of state space and observed data) and in high-dimensional (size of observed data and state space increase simultaneously) regimes. Convergence analysis in these regimes should be viewed as complementary; without such convergence analyses, practitioners have limited ability to assess the reliability of their MCMC experiments and hence any subsequent inference. The new methods will be made publicly available through open-source software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2143844","CAREER: Detecting Structured Anomalies in Large-Scale Sequential Decision Problems and Latent Variable Models","DMS","STATISTICS","07/01/2022","06/26/2023","Xiaoou Li","MN","University of Minnesota-Twin Cities","Continuing Grant","Yulia Gel","06/30/2027","$156,144.00","","lixx1766@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","1045","$0.00","This research project will develop new statistical methods and theory for anomaly detection, which is a topic with a long history and wide-ranging applications. Examples include fault detection in manufacturing, disease outbreak detection in public health, spectrum sensing in signal processing, item change detection in educational testing, and fraud detection in e-commerce. While traditional methods mainly focus on identifying data points deviating from their normal states independently, new challenges arise in the big-data era as the anomalies often involve massive data with complex structures. This project will develop novel statistical methods and theoretical results along with computational tools to systematically deal with the detection of structured anomalies in large-scale data. In addition to the technical contribution, the methods developed in this project will positively impact research in other disciplines. For instance, the change detection method developed in this project will aid the monitoring of item pool quality in educational testing to improve the validity and reliability of the tests. This project will also implement an educational plan which includes engaging graduate and undergraduate students in research activities, creating a new curriculum, and outreach to educational institutes. The outcome of the project will be broadly disseminated through journal publications and conferences, and publicly available statistical software will be developed. <br/><br/>Specifically, this project will focus on two classes of problems in large-scale sequential decision-making and latent variable models. The first class of problems involves large-scale streaming data, which have become common in recent years, owing to the rapid development in data acquisition technologies. The project will establish a general compound sequential decision theory framework to quantify the performance of procedures for large-scale online change detection problems and develop efficient sequential decisions under this framework. The second class of problems is on high-dimensional generalized latent factor models with structured outliers. The project will develop efficient model estimation and statistical inference methods and provide theoretical guarantees on their accuracy and reliability. Fundamental issues such as identifiability and estimability of the model will be addressed. Moreover, novel technical tools will be developed to address theoretical and methodological challenges in the above problems. For example, a monotone coupling technique for stochastic processes living on a non-Euclidean space will be developed to enhance the understanding of sequential decisions for multi-stream problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2152999","Collaborative Research: Bayesian Residual Learning and Random Recursive Partitioning Methods for Gaussian Process Modeling","DMS","STATISTICS, MSPA-INTERDISCIPLINARY, CDS&E-MSS","08/01/2022","07/13/2022","Li Ma","NC","Duke University","Standard Grant","Yulia Gel","07/31/2025","$119,999.00","","li.ma@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269, 7454, 8069","1269, 1303, 5294, 9263","$0.00","Rare natural hazards (for example, storm surge and hurricanes) can cause loss of lives and devastating damage to society and the environment. For instance, Hurricane Katrina (2005) caused over 1,500 deaths and total estimated damages of $75 billion in the New Orleans area and along the Mississippi coast as a result of storm surge. Uncertainty quantification (UQ) has been used widely to understand, monitor, and predict these rare natural hazards.  The Gaussian process (GP) modeling framework is one of the most widely used tools to address such UQ applications and has been studied across several areas, including spatial statistics, design and analysis of computer experiments, and machine learning. With the advance of measurement technology and increasing computing power, large numbers of measurements and large-scale numerical simulations at increasing resolutions are routinely collected in modern applications and have given rise to several critical challenges in predicting real-world processes with associated uncertainty. While GP presents a promising route to carrying out UQ tasks for modern emerging applications such as coastal flood hazard studies, existing GP methods are inadequate in addressing several notable issues such as computational bottleneck due to big datasets and spatial heterogeneity due to complex structures in multi-dimensional domains. This project will develop new Bayesian GP methods to allow scalable computation and to capture spatial heterogeneity. The new methods, algorithms, theory, and software are expected to improve GP modeling for addressing data analytical issues across a wide range of fields, including physical science, engineering, medical science, public health, and business science. The project will develop and distribute user-friendly open-source software and provide interdisciplinary research training opportunities for undergraduate and graduate students.<br/><br/>This project aims to develop a new Bayesian multi-scale residual learning framework with strong theoretical support that allows scalable computation and spatial nonstationarity for GP modeling. This framework integrates and extends several powerful techniques respectively arising in the literature on GP and that on multi-scale modeling, including predictive process approximation, blockwise shrinkage, and random recursive partitioning on the domain. This framework decomposes the GP into a cascade of residual processes that characterize the underlying covariance structures at different resolutions and that can be spatially heterogeneous in a variety of ways. The new framework allows for adoption of blockwise shrinkage to infer the covariance of the residual processes and incorporates random partition priors to enable adaptivity to various spatial structures in multi-dimensional domains. New recursive algorithms inspired by wavelet shrinkage and state-space models will be developed to achieve linear computational complexity and linear storage complexity in terms of the number of observations. The resulting GP method will guarantee linear computational complexity in a serial computing environment and also be easily parallelizable. This Bayesian multi-scale residual learning method provides a new approach to addressing GP modeling issues among spatial statistics, design and analysis of computer experiments, machine learning, and nonparametric regression.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2152887","Projecting Flood Frequency Curves Under a Changing Climate Using Spatial Extreme Value Analysis","DMS","Hydrologic Sciences, CDS&E-MSS","06/01/2022","06/14/2023","Brian Reich","NC","North Carolina State University","Continuing Grant","Yulia Gel","05/31/2025","$229,719.00","Sankarasubraman Arumugam, Stacey Archfield, Emily Hector","brian_reich@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1579, 8069","075Z, 079Z, 090Z, 1269, 1303, 5294","$0.00","Climate change is often described in terms of the mean, but it will be felt most acutely in terms of extreme events. In particular, the International Panel of Climate Change?s recent Sixth Assessment warns of an increase in the likelihood and magnitude of extreme flooding events in upcoming decades. Understanding the spatiotemporal variability of these changes is critical to mitigating their impact. However, current methods for spatial extreme value analysis are limited in their modeling flexibility and computational capabilities, and thus methodological work is required to analyze extreme events across the United States. Therefore, in this project, the investigators will develop new methodological and computational tools for spatial extreme value analysis and apply them to forecasting flood risk under a changing climate. The project team is comprised of an interdisciplinary group of statisticians and hydrologists to accomplish these ambitious objectives and ensure that the results are disseminated to the appropriate communities. The analysis combines fifty years of annual maximum streamflow observations at hundreds of gauges provided by the United States Geological Survey with CMIP6 climate model output produced under different climate scenarios. This analysis will provide high-resolution maps of anticipated change in flood risk and local flood frequency curves to inform water infrastructure projects. A highlight of the project is a workshop that will foster synergy between statisticians and hydrologists by encouraging the sharing of ideas, approaches and solutions to flood risk prediction, and aid in the formulation of a common language shared by statisticians and hydrologists for successful transfer of knowledge across disciplines. The overall objective is to improve resiliency to extreme flooding events in the United States.<br/><br/><br/>This project will result in major advances in both spatial extreme value analysis and hydrology. The investigators will pursue two methods that exploit recent developments in distributed computing, machine learning and artificial intelligence, respectively, to improve computation for spatial extreme value analysis. Computation for spatial extremes is challenging because the most common model is the max-stable process, and this model gives an intractable likelihood function and is thus not conducive to direct application of maximum likelihood or Bayesian analysis. To overcome this difficulty, this project will develop a divide-and-conquer method that analyzes data separately by subregion and then combines the results using generalized method of moments techniques. It is shown that this procedure has desirable theoretical properties and gives substantial performance gain over state-of-the-art methods. The project also develops a new method under the Bayesian framework that is preferred for uncertainty quantification. The new method decomposes the intractable likelihood function into a sequence of simpler functions, and uses deep-learning distribution regression to approximate these simpler functions. This approximation can be arbitrarily precise with computational requirements that scale linearly with the number of spatial locations, facilitating analysis of large datasets. The project culminates with the analysis of flood-frequency curves across the US. Compared to current methods, by using spatial extreme value analysis the analysis borrows information across space to improve estimation of small probabilities and estimate the probability of multiple locations simultaneously experiencing an extreme event. This project will produce new software for extreme value analysis and also train two graduate students in theoretical, computational and applied extreme value analysis in hydrology with a strong emphasis on interdisciplinary collaboration.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2134107","RTG: Networks: Foundations in Probability, Optimization, and Data Sciences","DMS","PROBABILITY, STATISTICS, WORKFORCE IN THE MATHEMAT SCI","05/15/2022","04/24/2023","Amarjit Budhiraja","NC","University of North Carolina at Chapel Hill","Continuing Grant","Tomek Bartoszynski","04/30/2027","$904,160.00","Vladas Pipiras, Quoc Tran-Dinh, Sreekalyani Bhamidi, Mariana Olvera-Cravioto","budhiraj@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1263, 1269, 7335","7301","$0.00","This research training group (RTG) project will develop a comprehensive training and mentoring program for undergraduate and graduate students and postdoctoral associates, centered around the theme of theory and applications of networks. Faculty team members bring a broad range of expertise to this effort, including stochastic analysis, random discrete structures, discrete and continuous optimization, time series and mathematical statistics, and machine learning. The training of undergraduates, graduate students, and postdocs will contribute to the readying of the workforce in academia and industry in this high-demand field. The engagement of undergraduates in research will form pathways for these students to pursue graduate studies and careers in research. Educational materials and mentoring mechanisms developed as part of RTG activities will have impact on the overall curriculum and training practices in the department as well as on the pan-campus data science initiative. The research intersects with many other fields, such as engineering, social sciences, business, biological and medical sciences, epidemiology, and ecology, and is expected to have impact in these disciplines.  Research from the RTG activity will be widely disseminated through posters, meetings, workshops, colloquia, conference proceedings and journal articles. <br/><br/>Two key initiatives enabled through this effort are: (1) a set of ten three-week minicourses, taught by international leaders in the field, designed for graduate students and other trainees, which will be broadly disseminated to the community in network science; (2) a weekly ideas seminar that will serve as a central platform to bring undergraduate, graduate, and postdoc trainees together with faculty mentors, and which will serve as a launching pad for undergraduate research projects as well as for identifying topics for Ph.D. dissertations and postdoctoral research. This platform will also provide valuable undergraduate research mentoring opportunities for graduate students and postdocs, and its activities will be instrumental in developing presentation and technical writing abilities of trainees at all levels. Other planned initiatives include (a) a summer boot-camp for incoming graduate trainees; (b) a first year graduate course on research directions in networks that brings together elements of a seminar and an independent reading course; (c) a freshman seminar and a capstone course in networks to form a well-structured pathway for undergraduates, from the freshman year to the senior year, to engage in meaningful and sustained research activity; and (d) a data science lab for organizing undergraduate research activities. The research themes of this RTG will span a broad range of topics, including: development of foundational large network asymptotics using tools from stochastic analysis, percolation theory, and large deviations theory; algorithmic approaches to detection and reconstruction, resource allocation, and computational questions on networks, using tools from applied probability and optimization theory; and approaches to estimation and learning questions using tools from statistics and machine learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210206","Collaborative Research: Use of Random Compression Matrices For Scalable Inference in High Dimensional Structured Regressions","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","06/15/2022","06/14/2022","Aaron Scheffler","CA","University of California-San Francisco","Standard Grant","Yulia Gel","05/31/2025","$110,000.00","","aaron.scheffler@ucsf.edu","1855 FOLSOM ST STE 425","SAN FRANCISCO","CA","941434249","4154762977","MPS","1269, 7454","068Z, 1269","$0.00","As the scientific community moves into a data-driven era, there is an unprecedented opportunity to leverage large scale imaging, genetic and EHR data to better characterize and understand human disease to improve treatment and prognosis. Consequently, analysis of such datasets with flexible statistical models has become an enormously active area of research over the last decade. To this end, this project plans to develop a completely new class of methods, which are based on the idea of fitting statistical models on datasets obtained by compressing big data using a well designed mechanism. The development enables efficient modeling of massive data on an unprecedented scale. While the motivation of the investigators comes primarily from complex modeling and uncertainty quantification of massive biomedical data, the statistical methods are general enough to set important footprints in the related literature of machine learning and environmental sciences. The overarching goal also includes the development of software toolkits to better serve practitioners in related disciplines.  Further, the projects will provide first hand training opportunities for graduate and undergraduate students, including female and students from minority communities, in state-of-the-art statistical methodologies and imaging/genetic/EHR data. By disseminating the outcome of the project among high school students in terminology that they can understand, the project can have far reaching effects to enhance public scientific literacy about statistics.<br/><br/><br/>Two crucial aspects of modern statistical learning approaches in the era of complex and high dimensional data are accuracy and scale in inference. Modern data are increasingly complex and high dimensional, involving a large number of variables and large sample size, with complex relationships between different variables. Developing practically efficient (in terms of storage and analysis) and theoretically ?optimal? Bayesian high dimensional parametric or nonparametric regression methods to draw accurate inference with valid uncertainties from such complex datasets is an extremely important problem. To offer a general solution for this problem, the investigators will develop approaches based on data compression using a small number of random linear transformations. The approach either reduces a large number of records corresponding to each variable using compression, in which case it maintains feature interpretation for adequate inference, or, reduces the dimension of the covariate vector for each sample using compression, in which case the focus is only on prediction of the response. In either case, data compression facilitates drawing storage efficient, scalable and accurate Bayesian inference/prediction in presence of high dimensional data with sufficiently rich parametric and nonparametric regression models. An important goal is to establish precise theoretical results on the convergence behavior of the fitted models with compressed data as a function of the number of predictors, sample size, properties of random linear transformations and features of these models. The approaches will be used to study neurological disorders by combining brain imaging data, genetic data and electronic health records (EHR) data from the UK Biobank database. The project will also contribute on a broader front to advancing the interdisciplinary research training and broadening participation in statistical sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210907","Statistical and Computational Tools for Analyzing High-Dimensional Heterogeneous Data","DMS","STATISTICS","08/01/2022","06/06/2022","Kaizheng Wang","NY","Columbia University","Standard Grant","Yulia Gel","07/31/2025","$179,999.00","","kaizheng.wang@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","1269","$0.00","Modern technologies generate tremendous volumes of data in diverse forms. The high throughput data come inevitably with great heterogeneity and enormous amount of noise. For instance, a large-scale genetic study typically involves people with various different attributes; a social network usually consists of multiple hidden communities with denser internal connections compared to external ones. While the raw features have high ambient dimension (for example, thousands of genes), oftentimes the intrinsic structures exhibit low complexity (for example, latitude and longitude of an individual?s geographic location). Precise extraction of the latent structure paves the way for solving downstream tasks. Faced with the significant challenges in statistics and computation, this project aims to develop efficient methodologies for estimating and inferring latent structures from heterogeneous data. This project will yield cutting-edge tools for scientific study, open-source software for easy implementation, and new mathematical theorems for theoretical analysis. The project will also provide numerous opportunities for statistical education and research training.<br/><br/>The project is structured into three parts. In the first part, the goal is to develop a new flexible methodology for clustering high-dimensional data. This part aims at new algorithms that can identify non-spherical and even non-convex clusters. An in-depth analysis of mixture models brings theoretical insights including tight finite-sample statistical error bounds and finite-iteration convergence guarantees for computation. In the second part, the goal is to study heterogeneous relational data that encode the information of individual objects in their pairwise relations. This part yields reliable methods for estimating and testing latent structures in the realistic scenario where the partially observed data may not be uniformly sampled at random. Finally, the third part focuses on the joint analysis of multiple related datasets, such as social networks with high-dimensional personal attributes. The tools developed in the first two parts of the project will constitute fundamental building blocks to address the research goals of this last thrust. The research finding will provide novel efficient data integration strategies for enhanced statistical accuracy. The proposed research initiatives include dissemination of the new methods and algorithms in a form of publicly available software and an active agenda on enhancing interdisciplinary research training and enhancing diversity in statistical sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2212100","Quality and Productivity Research Conference: Data, Statistics, and Responsibility","DMS","STATISTICS","04/15/2022","04/11/2022","Alexandra Piryatinska","CA","San Francisco State University","Standard Grant","Yong Zeng","03/31/2024","$19,999.00","Tao He","alpiryat@sfsu.edu","1600 HOLLOWAY AVE BUILDING NAD R","SAN FRANCISCO","CA","941321722","4153387090","MPS","1269","7556","$0.00","The 38th Quality and Productivity Research Conference (QPRC) will be hosted by San Francisco State University in San Francisco, California on June 14-16, 2022, preceded by a traditional one-day course on June 13. This conference is the main annual meeting for the Quality and Productivity Section of the American Statistical Association. The theme of the 2022 QPRC is ""Data, statistics, and responsibility."" The conference aims to promote the highest level of scientific integrity, reproducibility, public accountability, and social responsibility in handling big, multidimensional data. It will include a comprehensive discussion of cutting-edge modern methodologies in all aspects of data science and statistics.  Student participation is a vital part of the conference.  NSF funds are used to support the active participation of students in the conference and its associated one-day course. Students from numerous universities and graduate programs across the US, especially those from under-represented groups, will be fully engaged. Students will have the opportunity to present their research, build connections with others in the scientific community, and learn from researchers and practitioners working at the cutting edge of statistics and data science. They will be invited to submit contributed and poster presentations and compete for travel scholarships. <br/><br/>This conference attracts prominent researchers from academia, industry, and government. Participating statisticians, data scientists, quantitative analysts will exchange novel ideas and experience in working with modern big data to discover knowledge and apply it to diverse fields. The Conference will demonstrate the power of knowledge gleaned from data and the importance of scientific integrity and reproducibility. Plenary and invited sessions at the 38th QPRC are scheduled in the following subjects: data science, machine learning, big data analytics, time series and forecasting, statistical process control monitoring, design and analysis of experiments. The conference also includes contributed and poster sessions and a technical tour. It is preceded by a one-day course, ""Empowering the Statistician with Spark, Machine Learning and Deep Learning."" This course will provide an overview of using R and Python for some of the most popular machine learning and deep learning models in real-world data science applications in a cloud environment. The conference details are on its website, https://qprc2022.com/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210002","Collaborative Research: Statistical Inference for Multivariate and Functional Time Series via Sample Splitting","DMS","STATISTICS","07/01/2022","06/13/2022","Xiaofeng Shao","IL","University of Illinois at Urbana-Champaign","Standard Grant","Yulia Gel","06/30/2025","$199,999.00","","xshao@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","1269","$0.00","Multivariate and functional time series are prevalent and routinely collected in many fields. Statistical inference of such time series is a fundamental problem in modern time series analysis and has broad applications in many scientific areas, including bioinformatics, business, climate science, economics, finance, genetics, and signal processing. Compared with existing methodologies, this research project will provide nonparametric inference procedures that can accommodate a wide range of dimensionality and require weak assumptions on the data generating processes. The methodology ensuing from the project will be disseminated to the relevant scientific communities via publications, conference and seminar presentations, and the development of open-source software. The project will involve multiple research mentoring initiatives, including efforts on broadening participation, and will offer advanced topic courses to introduce the state-of-the-art techniques in time series analysis. The project will provide a broad range of interdisciplinary training opportunities at all educational levels and will contribute to the future workforce professional development.<br/><br/>The project will develop a systematic body of methods and theory on inference for both multivariate (including high-dimensional) time series and functional time series based on sample splitting (SS) and self-normalization (SN). Recently, the SN technique has been advanced to the inference of high-dimensional time series, but it requires the use of a trimming parameter. Also, its scope of applicability is limited to high-dimensional time series with weak panel dependence which might be unrealistic in many modern time series applications. In turn, the existing SN for functional time series relies on dimension reduction by functional principal component analysis and, hence, the resulting procedure may be powerless when the alternative is orthogonal to the space spanned by the top principal components used in the procedure. To address these major limitations, this project will develop a new unified framework based on SS-SN, in conjunction with inference for multivariate and functional time series, and investigate its utility in application to analysis of time series of low, medium, high or infinite dimensions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210726","Collaborative Research: Learning Graphical Models for Nonstationary Time Series","DMS","STATISTICS","07/01/2022","06/08/2022","Suhasini Subba Rao","TX","Texas A&M University","Standard Grant","Yulia Gel","06/30/2025","$150,000.00","","suhasini@stat.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","1269","$0.00","In the biological and social sciences, many central questions require one to understand how interactions within a complex system evolve over time. For example, in neurosciences, monitoring time-varying connections among different brain regions is important for studying the progression of neurodegenerative diseases. As another example, risk management and monitoring in interconnected financial markets often require learning how the linkages among different firms evolve over time. These examples demonstrate the need to develop rigorous and scalable statistical methods that are able to learn the evolution of connectivity from large-scale  complex time series data. Such methods can help offer insights into the working of a complex system and guide data-driven policy making.<br/><br/>While graphical models (GM) offer a powerful  framework for data-driven discovery of network architecture,  existing statistical research in this area has focused primarily on modeling time-invariant connections from stationary time series. This project will develop estimation and inference methods for a nonstationary graphical model framework called NonStGM. This framework captures  nonstationary dynamics in a multivariate system in the form of a sparse operator in the Fourier domain, whose structure can in turn be estimated from data using regularized regression methods. Key emphasis will be given on two classes of structured nonstationarity which are prevalent in many applications: (a) local stationarity that allows both abrupt changes and smooth evolution of the temporal dynamics, and (b) periodic stationarity. NonStGM structures learned from large-scale time series data  will be used to build directed graphs with time-varying vector autoregressive (VAR) models. Algorithms developed in this project  will be validated with extensive numerical experiments and real electroencephalogram  (EEG) data sets. All products will be made publicly available in the form of open-source software packages. These products are expected to aid clinical researchers, amongst others, in their understanding of connectome abnormalities in the brains of patients suffering from neurological disorders. Research outcomes will be integrated into educational modules of graduate level courses. The project will provide numerous opportunities to train graduate students in a topical research area of large-scale time series modeling and will actively focus on enhancing diversity and inclusion in statistical sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2211813","Objective Bayes 2022 Methodology Conference","DMS","STATISTICS","09/01/2022","03/08/2022","Paul Parker","CA","University of California-Santa Cruz","Standard Grant","Yong Zeng","08/31/2023","$10,000.00","Zehang Li, Bruno Sanso, Athanasios Kottas","paulparker@ucsc.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1269","7556","$0.00","The conference ""Objective Bayes 2022 Methodology Conference"" will be held at the University of California Santa Cruz, in Santa Cruz, CA, on September 6-10, 2022. Focusing on recent developments in objective Bayes theory, methodology and applications, and related topics, the conference will feature one day of tutorials for new researchers and four days of scientific sessions with discussion and a poster session open to all participants. This conference will bring together experts as well as a group of students and young researchers from a broad set of areas within and related to objective Bayesian methodology. The primary goal is to spark the development of new objective Bayesian techniques, which will serve as invaluable tools in various disciplines that deal with modeling uncertainty such as climatology, economics, and ecology. Emphasis will be placed on the interaction between new and established researchers, and researchers in different methodological areas.<br/><br/>Today, more than ever, Bayesian approaches have become increasingly widespread. Driven in part by the computing revolution and the need for principled uncertainty quantification, Bayesian methods are being applied widely across the sciences and within industry settings. The conference is aimed to respond to these demands, and will encompass many aspects of objective Bayesian methodology, from theoretical, to applied and computational. The conference will also cover recent developments in areas such as high-dimensional data analysis, approximate Bayesian computation, Bayesian nonparametric, machine learning, etc. For more details about the conference, please see the conference homepage: https://obayes.soe.ucsc.edu.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2229410","Collaborative Research: AMPS: Robust Failure Probability Minimization for Grid Operational Planning with Non-Gaussian Uncertainties","DMS","AMPS-Algorithms for Modern Pow, ","12/01/2022","08/09/2022","Sanjay Mehrotra","IL","Northwestern University","Standard Grant","Yulia Gel","11/30/2025","$283,995.00","","mehrotra@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","045Y, W325","1269, 5294, 8396, 9263","$0.00","The electric power industry accounted for the second-largest portion of all carbon emissions across economic sectors in 2020. Renewable energy resources, particularly wind and solar, are critical to decarbonizing the grid and ensuring the nation's future prosperity and welfare. However, because of their inherent and unavoidable intermittency and variability, successful integration of renewable energy resources in the nation's energy mix poses fundamental challenges for day-to-day grid operations. Failure to account for this uncertainty during planning can result in loss of service and grid de-stabilization, thus jeopardizing not only the achievement of decarbonization targets but also system reliability. This project develops the next generation of mathematical methods, computer models, and algorithms for grid operational planning, which accurately and systematically take into account the non-normal and multi-modal nature of renewable uncertainty, as well as the nonlinear and often counter-intuitive physical laws that govern electric power networks. The project's methods and computer implementations shall benefit and inform diverse planning tools, both within the electric power sector as well as the broader energy sector, including those of private companies and vendors who specialize in power systems software. The project further impacts education and the broader society by training undergraduate and graduate STEM students in energy systems optimization and the foundations of electric power grid operations, thereby enabling them to apply their analytical skills to design more environmentally- and economically-efficient future energy systems.<br/><br/>The project contributes a general methodology, including new mathematical models, theory, and algorithms, to systematically account for non-Gaussian error distributions of renewable energy forecasts, in one of the most fundamental power system planning problems called AC Optimal Power Flow. A general treatment of non-Gaussian errors in electric load and renewable energy forecasts has not been considered before in grid planning, despite being exhibited in data. The project rigorously integrates risk and uncertainty in this context by developing a novel methodology for optimization under non-Gaussian probabilistic constraints. This is achieved by exploiting the representability and analyticity of Gaussian mixture models and by designing algorithms that are modular enough to allow current methods which are proven to work well for Gaussian errors to be reusable with only minor modifications. The generality of the approach is expected to spur new algorithms in the broader field of chance-constrained optimization, including nonlinear nonconvex problems whose constraints are affected by Gaussian mixture uncertainties. The project also rigorously accounts for misspecification of the mixture model parameters by designing novel non-Gaussian ambiguity sets, which have not been studied before but have the potential to enable the discovery of robust network operating points with improved out-of-sample performance and reliability. The project uses real utility data to guide model validation and experimentation and also provides a set of practical recommendations for system operators to facilitate the adoption of the developed methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210686","Exploring and Solidifying Functional Calibration of Computer Models","DMS","STATISTICS, EPSCoR Co-Funding","08/15/2022","07/20/2022","Derek Brown","SC","Clemson University","Standard Grant","Yong Zeng","07/31/2025","$150,000.00","","ab7@clemson.edu","201 SIKES HALL","CLEMSON","SC","296340001","8646562424","MPS","1269, 9150","9150","$0.00","Scientists and engineers rely on computer models to study complex physical systems when physical experiments are financially expensive, time-intensive, or potentially harmful to public health or the environment. For example, computer models are used to predict the performance of large wind turbine blades, estimate the time to evacuate burning buildings, and model thermal conductivity for nuclear fuels. Computer model calibration is the process of comparing computer model output to physical data so that the model can be tuned to represent reality as faithfully as possible. This project will explore a version of calibration known as functional calibration, in which the appropriate values of the calibration inputs change with different experimental settings. This research will equip practitioners with well-grounded tools to help them better understand the systems they are studying. With these tools, practitioners can improve computer codes and make more precise predictions. Further, this project will provide opportunities for graduate students and under-represented minorities to participate in innovative research at the intersection of statistics, applied mathematics, and engineering.<br/><br/>The principal investigator (PI) aims to develop novel Bayesian models, theory, and algorithms for functional computer model calibration. The first aim is to use tools from the variable selection literature to distinguish functional parameters from constants and to learn new physics when calibration parameters have physical meaning. The project also seeks to characterize the identifiability of functional parameters more completely in the presence of model bias. The identifiability work will encompass the so-called ?mixed? calibration in which a computer model contains both constant and functional parameters simultaneously, optimal basis function representations, and relevant asymptotics. The PI will study the parameters from the infinite-dimensional perspective rather than with discretization, allowing for the use of the calculus of variations when deriving necessary and sufficient conditions for identifiability, as well as providing insight into properties that would be missed with a finite-dimensional treatment. Further, the PI will wed the Kennedy-O?Hagan model, orthogonal priors, scaled Gaussian processes, and related advents to calibration methodology, with advances in active subspaces to facilitate feasible and meaningful calibration with extremely high-dimensional models. The methods will be illustrated in both simulated settings and models of, for example, plastic deformation of materials and building energy use. The PI will develop software compatible with existing packages, thereby maximizing the accessibility and impact of this work. This project is jointly funded by the Statistics Program and the Established Program to Stimulate Competitive Research (EPSCoR).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2229409","Collaborative Research: AMPS: Robust Failure Probability Minimization for Grid Operational Planning with Non-Gaussian Uncertainties","DMS","AMPS-Algorithms for Modern Pow, ","12/01/2022","08/09/2022","Daniel Maldonado","IL","University of Chicago","Standard Grant","Yulia Gel","11/30/2025","$110,155.00","","maldonadod@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","045Y, W325","1269, 5294, 8396, 9263","$0.00","The electric power industry accounted for the second-largest portion of all carbon emissions across economic sectors in 2020. Renewable energy resources, particularly wind and solar, are critical to decarbonizing the grid and ensuring the nation's future prosperity and welfare. However, because of their inherent and unavoidable intermittency and variability, successful integration of renewable energy resources in the nation's energy mix poses fundamental challenges for day-to-day grid operations. Failure to account for this uncertainty during planning can result in loss of service and grid de-stabilization, thus jeopardizing not only the achievement of decarbonization targets but also system reliability. This project develops the next generation of mathematical methods, computer models, and algorithms for grid operational planning, which accurately and systematically take into account the non-normal and multi-modal nature of renewable uncertainty, as well as the nonlinear and often counter-intuitive physical laws that govern electric power networks. The project's methods and computer implementations shall benefit and inform diverse planning tools, both within the electric power sector as well as the broader energy sector, including those of private companies and vendors who specialize in power systems software. The project further impacts education and the broader society by training undergraduate and graduate STEM students in energy systems optimization and the foundations of electric power grid operations, thereby enabling them to apply their analytical skills to design more environmentally- and economically-efficient future energy systems.<br/><br/>The project contributes a general methodology, including new mathematical models, theory, and algorithms, to systematically account for non-Gaussian error distributions of renewable energy forecasts, in one of the most fundamental power system planning problems called AC Optimal Power Flow. A general treatment of non-Gaussian errors in electric load and renewable energy forecasts has not been considered before in grid planning, despite being exhibited in data. The project rigorously integrates risk and uncertainty in this context by developing a novel methodology for optimization under non-Gaussian probabilistic constraints. This is achieved by exploiting the representability and analyticity of Gaussian mixture models and by designing algorithms that are modular enough to allow current methods which are proven to work well for Gaussian errors to be reusable with only minor modifications. The generality of the approach is expected to spur new algorithms in the broader field of chance-constrained optimization, including nonlinear nonconvex problems whose constraints are affected by Gaussian mixture uncertainties. The project also rigorously accounts for misspecification of the mixture model parameters by designing novel non-Gaussian ambiguity sets, which have not been studied before but have the potential to enable the discovery of robust network operating points with improved out-of-sample performance and reliability. The project uses real utility data to guide model validation and experimentation and also provides a set of practical recommendations for system operators to facilitate the adoption of the developed methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2232812","Foundations of Computational Mathematics Conference ? FoCM 2023","DMS","PROBABILITY, ALGEBRA,NUMBER THEORY,AND COM, GEOMETRIC ANALYSIS, APPLIED MATHEMATICS, STATISTICS, COMPUTATIONAL MATHEMATICS, Combinatorics","12/01/2022","08/04/2022","Peter Binev","SC","University of South Carolina at Columbia","Standard Grant","Stacey Levine","11/30/2023","$49,500.00","","binev@math.sc.edu","1600 HAMPTON ST # 414","COLUMBIA","SC","292083403","8037777093","MPS","1263, 1264, 1265, 1266, 1269, 1271, 7970","7556, 9150, 9263","$0.00","The next triennial international conference in the series Foundations of Computational Mathematics, FoCM 2023, will be hosted at the Sorbonne University in Paris, France, June 12?21, 2023. The Foundations of Computational Mathematics (FoCM) conferences aim to be a main vehicle in the exploration of the broad interface between mathematics and contemporary computation, a subject area of importance both intellectually and for practical applications. It is expected that about 500 researchers will attend the conference from around the world, and about a third of them will be from the US. This project will partially cover travel and lodging costs for a diverse group of US researchers attending the conference. The majority of the funds will be distributed on a competitive basis to junior US participants, including graduate students, postdocs, and tenure track faculty.<br/><br/>The conference will follow the format of former FoCM conferences: plenary lectures in the mornings and theme-centered parallel workshops in the afternoons. Each workshop extends over three days, and the conference will consist of three periods, each period featuring 7 mathematically diverse but interconnected workshops, with 21 workshops in total. These activities combined with poster sessions and informal interactions among participants all synthesize forefront research with mentoring and training opportunities for young participants, helping them develop networks that include the world?s leading researchers. A number of themes connect the diverse workshop topics, such as learning from computation to automate, adapt and optimize the computational process which by itself becomes an analysis tool, ties from homotopy methods used in computational algebraic geometry for systems of algebraic equations and interior point methods in continuous optimization to numerical methods for differential equations and aspects of differential geometry, machine learning and signal processing cast as very large and challenging convex optimization problems, common themes among symbolic algorithms used for the solution of algebraic and differential equations, and relationships between decidability and tractability in real/complex arithmetic computational models and the Turing machine models considered in the context of combinatorics and intractability as well as discrete optimization.  More information about the FoCM 2023 conference can be found at https://focm2023.org<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2210929","Nonparametric Total Variation Regression for Multivariate Process Data","DMS","STATISTICS","09/15/2022","06/03/2022","Michael Pokojovy","TX","University of Texas at El Paso","Standard Grant","Yulia Gel","08/31/2025","$119,853.00","","mpokojovy@utep.edu","500 W UNIVERSITY AVE","EL PASO","TX","799680001","9157475680","MPS","1269","1269","$0.00","Process data of interest frequently occur in engineering, manufacturing, commerce, environmental science and other arenas. For example, water or air contamination levels, configuration of a drilled metal part, chemical composition of a pharmaceutical product, and operational characteristics of computer network, all changing over time, are routinely monitored in real time. Upsets or shifts away from a stable, consistent flow of process data are indicative of special cause intrusion(s).  These special causes can be significantly detrimental to decision making and process understanding in the context of a particular application. Development of reference-free statistical control charts for monitoring multivariate processes for both gradual and abrupt changes in the mean vector has been significantly hampered by a lack of suitable nonparametric regression methodology. In response to this challenge, this project will address the acute need for nonparametric estimators for multivariate process data and will develop new reference-free methods for statistical process monitoring. The outcomes of this project will benefit society through enhanced statistical quality assurance in industrial manufacturing, business, commerce, healthcare, and other domains of societal importance. The results of this project will be implemented in a form of publicly available software. Furthermore, the project will involve multiple research training and career mentoring initiatives at various educational levels and will offer multiple opportunities for interdisciplinary training, with a particular focus on broadening participation in statistical sciences.<br/><br/>The project will advance the frontiers of nonparametric multivariate regression by developing new theory and methodology of statistical process control for individuals multivariate process data. In the context of nonparametric estimation for independent sub-Gaussian processes, the goal is to investigate nonparametric total variation (TV) and taut string (TS) estimators for multivariate process data with piecewise smooth process mean, establish well-posedness for associated optimization problems, prove their equivalence, and investigate asymptotic consistency/convergence rates for TV/TS estimators in various practically relevant topologies. These theoretical results will be applied to develop computationally efficient algorithmic implementations of the TV/TS estimator, investigate convergence and complexity of these algorithms, and showcase their performance based on synthetic and real data. Subsequently, algorithmic implementations of the TV/TS estimator will be used to design a new class of reference-free statistical control charts for nonparametric monitoring of multivariate process mean and compare them to state-of-the-art competitors under a variety of practically relevant scenarios.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."

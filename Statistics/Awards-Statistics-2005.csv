"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"0505025","Haplotype Linkage and Association Mapping of Quantitative Trait Loci","DMS","STATISTICS","06/01/2005","06/09/2006","Ruzong Fan","TX","Texas A&M Research Foundation","Standard Grant","Grace Yang","05/31/2007","$61,147.00","","rf740@georgetown.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","The objectives of the project are to develop statistical methods and algorithms for linkage and association mapping of quantitative trait loci. The research includes: (1) to extend existing methodology to analyze non-temporal genetic data; (2) to create novel approaches and models to analyze temporally longitudinal human genetic data. The projects include the model building and testing, well-designed simulation studies, and applications to empirical datasets. Algorithms and software will be developed based on the techniques developed in the research. For longitudinal human genetic data, multiple measurements of quantitative or qualitative traits are taken for each individual over time, in addition to the genotype information and covariates such as gender, age, and familial income. The theory of stochastic processes will be applied to build models and methods in longitudinal genetic study.<br/><br/>In human genetics, one important issue is to locate and to identify important genetic variants/determinants of complex traits. Complex diseases are familial, but the mode of inheritance is uncertain. Many common diseases are complex disorders, such as asthma, diabetes, Alzheimer's disease, psychiatric disorders, Parkinson's disease, cardiovascular disease, and arthritis. With the development of the Human Genome Project, high resolution micro-satellite and chromosome-wide haplotype maps of human genome, enormous amounts of genetic data on human chromosomes are becoming available. The opportunities for genome-wide scan to map complex disease genes are tremendous. However, it is not yet clear how to extract the most useful information for complex disease gene mapping. To fully utilize the massive genetic data for complex disease gene mappings, novel mathematical and statistical methods are crucial. The investigator develops appropriate models and handy algorithms in linkage and association mapping of complex diseases. This helps to identify important genetic variants/determinants of complex traits. The Broader Impacts are: (1) To advance discovery of common disease genes and to facilitate identification of drug targets for medical sciences and the pharmaceutical industry to benefit society and to enhance public health; (2) to make algorithms and software for disease gene mapping publicly available, and to upgrade and enhance general research and education infrastructure; (3) to help develop an interdisciplinary graduate program in training a new generation of researchers in bioinformatics."
"0504896","A Unified Framework for Statistical Modeling with Multivariate Skewed Distributions and Application to Spatial Selection Models","DMS","STATISTICS","08/15/2005","05/13/2009","Marc Genton","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","07/31/2009","$90,000.00","","genton@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","<br/><br/>The investigator proposes to develop a new unified definition of multivariate <br/>skewed distributions, which are essential to statistical modeling of data. Indeed, skewed distributions provide flexible parametric classes of multivariate <br/>distributions for the statistician's toolkit that allow for modeling key <br/>characteristics such as skewness, heavy tails, and multimodality. They also<br/>contain the multivariate normal distribution as a special case, enjoy pleasant<br/>theoretical properties, and prevent the difficult task of multivariate data <br/>transformation. The investigator proposes to establish links between the new unified definition and existing definitions in the literature, as well as investigate novel distributions emerging from this framework. The investigator also intends to establish the theoretical and inferential properties of the multivariate skewed distributions resulting from the unified definition. This, in turn, provides the necessary basis for the development of selection models for spatial data, and yields a skewed version of the famed kriging procedure, or from another point of view, a spatial version of the celebrated Heckman model. <br/><br/><br/>The primary impact of this project is that the new framework for multivariate<br/>skewed distributions will provide valuable tools to applied statisticians and <br/>practitioners, especially in environmental sciences. Another impact will result from the development and free distribution of implementations of the results emerging from this research in software such as R and Matlab. In order to facilitate the wide dissemination of results from this project, a webpage will be created and maintained. It will contain programs, research reports, references, and links to information about multivariate skewed distributions.<br/>"
"0508349","Fourth International Conference on High Dimensional Probability","DMS","PROBABILITY, STATISTICS","06/15/2005","06/09/2005","Joel Zinn","TX","Texas A&M Research Foundation","Standard Grant","Dean Evasius","05/31/2006","$15,000.00","","jzinn@math.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1263, 1269","0000, OTHR","$0.00","The Fourth International Conference on High Dimensional Probability will be held on June 20-24, 2005 on the St. John's College campus in Santa Fe, New Mexico.  The conference will center on current research topics in Gaussian methods, limit theorems, infinitely divisibility, small deviations, Empirical processes and U-processes, learning theory, non-parametric statistics, and spin glasses."
"0505670","Dimension Reduction for Stochastic Processes","DMS","STATISTICS","07/01/2005","06/22/2005","Randall Eubank","TX","Texas A&M Research Foundation","Continuing Grant","Rong Chen","06/30/2006","$71,558.00","Tailen Hsing","eubank@math.asu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","<br/>Traditional multivariate statistics focuses on the analysis of data that<br/>are vectors of finite length. However, modern data collection methods are<br/>now frequently returning observations that should be viewed as the result<br/>of digitized recording or sampling from stochastic processes. These types of<br/>data occur in the context of functional data analysis (where the<br/>observation process has a one dimensional index set), image analysis and<br/>spatial statistics, for example, and arise from every aspect of the modern<br/>world. Our ability to process and to make use of such data directly<br/>affects the way in which scientific inquiries are conducted and practical<br/>problems are solved. Goals for analyzing this type of high dimensional<br/>data include the detection of structure and dimensionality reduction and<br/>these are the issues that will be addressed in the proposed project.<br/>Specifically, a number of topics will be investigated concerning dimension<br/>reduction for data from stochastic processes including i) canonical<br/>correlations analysis for two or more processes, ii) mixed models methods<br/>for analysis of data from multiple processes, iii) inverse regression and<br/>iv) varying coefficient models. The unifying theme in all this work is the<br/>use of reproducing kernel Hilbert space methods to formulate both the<br/>problems and their proposed solutions.<br/><br/>With fast progressing modern technology in fields such as medicine,<br/>environmental science, and homeland security, the data collected in those<br/>fields today are frequently curved or spatial data, and may be even more<br/>complicated in terms of scope and structure. Traditional statistical<br/>methods were created to primarily deal with<br/>low-dimensional data, and are not suitable for the high-dimensional or<br/>``functional'' nature of the data described above. This research is aimed<br/>at addressing a number of fundamental issues in the emerging branch of<br/>modern statistical data analysis that deals with high-dimensional data.<br/>The results to be obtained will not only potentially impact<br/>high-dimensional data analytic methodology across a myriad of<br/>disciplines, but will also provide a theoretical foundation and<br/>directions for future statistical research.<br/><br/>"
"0505535","Design, Analysis, and Monitoring of Network Characteristics Using Tomography Techniques","DMS","STATISTICS, ERC-Eng Research Centers, MSPA-INTERDISCIPLINARY","07/15/2005","07/02/2008","George Michailidis","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","06/30/2009","$238,334.00","Vijayan Nair","gmichail@ucla.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269, 1480, 7454","0000, 7303, OTHR","$0.00","Abstract <br/><br/>Prop ID:  DMS-0505535  <br/>PI and co-PI:  Michailidis, George  and Nair, Vijay <br/>Institution:  University of Michigan Ann Arbor     <br/>Title:  Design, Analysis, and Monitoring of Network Characteristics Using Tomography Techniques   <br/><br/>This research program is motivated by problems that arise in assessing and monitoring the <br/>quality of service (QoS) characteristics of computer and communication networks. It focuses on<br/>active network tomography where end-to-end traffic measurements are obtained by actively probing the network. The project examines a number of important and interesting statistical issues dealing with the collection, modeling, and analysis of network data using active tomography. Specifically, it introduces a new, flexible class of probing experiments that has advantages over the current schemes. The investigators will study several research issues related to the design of efficient flexicast probing experiments: questions of identifiability, construction of efficient designs, optimal allocation of test probes  (according to some suitable metric), and real-time  monitoring strategies to detect and localize network problems.  A substantial portion of the research deals with estimation and inference for a large-scale inverse problem with missing data. The focus is on fast and scalable algorithms for estimating packet loss rates and delay distributions and their asymptotic properties.  Furthermore, new models for  spatial and temporal dependence that capture the main features of network traffic will be introduced and the behavior of various estimation methods under these models will be analyzed. Finally, an important component of this research program is the validation of the proposed models and techniques on real data. Present day computer networks have evolved into large, complex, decentralized systems. There is a great deal of interest in estimating key network performance parameters and quality of service indicators, such as traffic intensities, link delays and dropped packet rates, and in identifying bottleneck nodes and detecting network failures in the form of connectivity, routing faults  and malicious activity. However, the lack of centralized control makes the quantitative assessment of network performance a rather difficult task, since detailed queueing and traffic models do not capture their complexity and characteristics well. Network tomography is capable of assessing the performance of large-scale networks and in localizing anomalous behavior to individual components and subnetworks. The understanding and insights gained as a result of the proposed research will lead to a core of basic principles and a toolkit of key statistical methods and techniques for the analysis and design of network monitoring schemes. Efficient and scalable network monitoring could significantly benefit network control and provisioning by incorporating the necessary information into admission, flow and routing control protocols. The proposed techniques and algorithms will be integrated into an open source tool, called FLEXICAST. The research program will also be integrated into the educational activities of the Department of Statistics. Planned efforts include training of graduate and undergraduate students, the development of a new course, stimulation of collaborations with students and<br/>faculty in Engineering, and interaction with the networking community to help transfer the research results to industry. <br/><br/><br/>"
"0505265","Gene Tree-Species Tree Relationships Under the Coalescent Process","DMS","STATISTICS","08/01/2005","07/25/2005","Laura Kubatko","NM","University of New Mexico","Standard Grant","Grace Yang","02/28/2007","$74,999.00","","lkubatko@stat.osu.edu","1700 LOMAS BLVD NE STE 2200","ALBUQUERQUE","NM","871063837","5052774186","MPS","1269","0000, 9150, OTHR","$0.00","Abstract <br/><br/>Prop ID:  DMS-0505265  <br/>Prev Awd:  0104290  <br/>PI:  Salter, Laura    <br/>Institution:  University of New Mexico  <br/>Title:  Gene Tree-Species Tree Relationships Under the Coalescent Process   <br/><br/>In this proposal, incongruence in gene trees and species trees is<br/>examined using data from multiple genes in the context of the<br/>coalescent process.  First, the investigator will show<br/>that one currently advocated approach for the analysis of data<br/>from multiple genes, the concatenation approach, can be<br/>statistically inconsistent, even when a consistent method of<br/>phylogenetic tree estimation is used. Second, an algorithm for<br/>maximum likelihood (ML) estimation of species trees from data on<br/>multiple genes under the coalescent model will be developed and<br/>implemented, and will be made freely available via the internet.<br/>The availability of a method for ML species tree estimation will<br/>allow for likelihood-based hypothesis testing of phylogeographic<br/>and population genetic hypotheses.  Further, methods for assessing<br/>uncertainty in the species tree estimates will be developed by<br/>extending traditional bootstrapping methods in phylogenetics to<br/>the case in which data have been collected for multiple genes<br/>sampled randomly throughout the genome.  Finally, tests for the<br/>adequacy of the coalescent model will be developed by examining<br/>whether the observed gene trees are consistent with a given<br/>species tree using several metrics to measure levels of<br/>incongruence.<br/><br/>The inference of the evolutionary history of a collection of organisms <br/>based on the information contained in their DNA sequences is a problem<br/>of fundamental importance in evolutionary biology. The abundance of DNA <br/>sequence data arising from genome sequencing projects has led to <br/>significant challenges in the inference of these phylogenetic <br/>relationships. Among these challenges is the inference of the evolutionary <br/>history of a collection of species based on DNA sequence information<br/>from several distinct genes sampled throughout the genome.  This project <br/>studies the effect of the coalescent process on the inference of species <br/>phylogenies using data from multiple genes.  This work will first <br/>demonstrate that failure to model the coalescent process can lead to <br/>incorrect inferences of species relationships.  The investigator will then <br/>develop methods that can accurately estimate species phylogenies through <br/>explicitly modeling the coalescent process, and will apply these <br/>estimation procedures to construct techniques for hypothesis testing and <br/>for measuring uncertainty in estimated species phylogenies.<br/><br/>--<br/><br/>"
"0505499","Mathematical Methods for Small--Sample Biostatistical Inference","DMS","STATISTICS","07/01/2005","04/15/2005","John Kolassa","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","06/30/2008","$86,998.00","","kolassa@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","The investigator applies various mathematical methods<br/>to extend the range of application of saddlepoint<br/>approximation techniques and exact enumeration techniques<br/>in statistical inference.  These techniques are applied to<br/>multi-dimensional conditional inference, achieved primarily<br/>by developing new approximations to multivariate tail<br/>probabilities for some component of a vector of sufficient<br/>statistics conditional on the remaining components, and<br/>methods for calculating these tail probabilities exactly.<br/>Particular attention is paid to probability models<br/>whose sufficient statistics have lattice distributions,<br/>since standard asymptotic techniques frequently fail<br/>in this context.  These models, including logistic and<br/>Poisson regression and contingency table models, are<br/>very frequently used in applied biostatistical work.<br/>Specifically, this research includes the application<br/>of multidimensional saddlepoint approximations to<br/>order--restricted hypotheses.  It extends existing<br/>Approximations applicable to continuous distributions to<br/>general non-lattice distributions.  It develops guidelines<br/>for tuning approximate conditional inferential methods to<br/>obtain higher power.  Computational algorithms developed<br/>as part of this work are publicly available.<br/><br/>Many current statistical techniques rely on mathematical approximations;<br/>the accuracy of these approximations ranges from very good to inadequate.<br/>This research involves approximations known to be almost always of high<br/>accuracy, and applies them in some statistical contexts that are widely<br/>used by scientists in a variety of disciplines.  This research allows<br/>investigators to draw valid conclusions from smaller data sets,<br/>particularly in cases when important research questions are phrased in terms<br/>of a number of quantities that must be accounted for.  This situation occurs <br/>in a wide range of areas, from finance to political science to medicine.<br/>For example, a new medical therapy might be expected to lead to improvements,<br/>potentially measured in a number of ways.  The investigator wishes to<br/>demonstrate that individuals receiving the new therapy at least as well <br/>according to all of the potential measures, and better on at least one<br/>measure, than do patients on the original therapy.  Standard statistical<br/>methods do not handle such situations in an efficient way; the current<br/>research represents a significant improvement.<br/>"
"0502347","EMSW21-RTG Statistics for Physical and Engineering Sciences: A Plan for the Establishments of a Research Training Group","DMS","STATISTICS, WORKFORCE IN THE MATHEMAT SCI","08/15/2005","07/12/2005","Alicia Carriquiry","IA","Iowa State University","Standard Grant","Christopher Stark","07/31/2009","$1,101,547.00","Huaiqing Wu, Max Morris, William Meeker, Stephen Vardeman","alicia@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269, 7335","0000, 7301, OTHR","$0.00","NATIONAL SCIENCE FOUNDATION<br/>Arlington, Virginia 22230<br/>ABSTRACT<br/>Research Training Groups Program<br/>FY 2005<br/><br/>Prop ID:  DMS-0502347  <br/>PI:  Carriquiri, Alicia L.<br/>NSF Program: WORKFORCE IN THE MATHEMATICAL SCIENCES   <br/>Institution:  Iowa State University  <br/>Title:   EMSW21-RTG statistics for Physical and engineering sciences: A plan for the establishment of a research training group<br/>Panel and ad-hoc Ratings: E, E, G, V, V, V.<br/><br/><br/>Statisticians are playing an increasingly important role in cross-disciplinary research addressing important, challenging scientific problems. The overall objective of this RTG program is to train young statisticians in Statistics for the Physical and Engineering Sciences in a cross-disciplinary, experiential (first-hand exposure) research environment, that brings together scientists from  industry, government, and academia. This program aims to solve problems of national importance through collaboration with external partner scientists, and by educating and training undergraduates and graduate students, in cross-disciplinary research. Activities planned  to achieve the objective include student internships providing an experiential research component and shorter-term visits between partner scientists and ISU faculty members to initiate and extend research collaboration. The activities will also include joint Ph.D. program expansion in the Iowa State Statistics Department, new course offerings, weekly RTG seminars, and two workshops on topics of research in the RTG.  The five initial focus areas for the RTG are: reliability of critical systems, nano-metrology and high throughput technologies, calibration of computer models of complex physical systems, accelerated discovery of useful new chemical compounds and behavior of complicated systems such as large wireless networks. The creation and development of this experiential, cross-disciplinary research environment for educating and training the nation's next-generation statisticians will provide a national model for advancing scientific discovery while promoting education and learning. Expected results have the potential to be implemented widely and to provide solutions in important areas of application, such as complex physical and information networks, nano-technology, and high-performance material discovery. <br/><br/>Statisticians are playing an increasingly important role in cross-disciplinary research addressing important, challenging scientific problems. The overall objective of this RTG program is to train young statisticians in Statistics for the Physical and Engineering Sciences in a cross-disciplinary, experiential (first-hand exposure) research environment, that brings together scientists from  industry, government, and academia. This program aims to solve problems of national importance through collaboration with external partner scientists, and by educating and training undergraduates and graduate students, in cross-disciplinary research. Activities planned  to achieve the objective include student internships providing an experiential research component and shorter-term visits between partner scientists and ISU faculty members to initiate and extend research collaboration. The activities will also include joint Ph.D. program expansion in the Iowa State Statistics Department, new course offerings, weekly RTG seminars, and two workshops on topics of research in the RTG.  The five initial focus areas for the RTG are: reliability of critical systems, nano (very small-scale)-metrology and high throughput technologies, calibration of computer models of complex physical systems, accelerated discovery of useful new chemical compounds and behavior of complicated systems such as large wireless networks. The creation and development of this experiential, cross-disciplinary research environment for educating and training the nation's next-generation statisticians will provide a national model for advancing scientific discovery while promoting education and learning. Expected results have the potential to be implemented widely and to provide solutions in important areas of application, such as complex physical and information networks, nano (very small-scale)-technology, and high-performance material discovery<br/>"
"0505865","Collaborative Research: Graphical and Algebraic Models for Multivariate Categorical Data","DMS","STATISTICS","07/01/2005","04/28/2005","Thomas Richardson","WA","University of Washington","Standard Grant","Gabor Szekely","06/30/2009","$120,000.00","","tsr@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, OTHR","$0.00","The proposed research project develops models for multivariate categorical<br/>data by mimicking Gaussian models with a desired model structure that can<br/>be captured in terms of the non-parametric concept of conditional<br/>independence.  This method has a long history: graphical log-linear models<br/>can be induced in this way by Gaussian models defined by zero constraints<br/>on the inverse covariance matrix. The project seeks to greatly extend the<br/>scope of the approach.  It is proposed to define and study marginal<br/>independence models for contingency tables, discrete-valued time series<br/>with moving average-like dependence structure, seemingly unrelated<br/>regressions with discrete response variables, and discrete graphical<br/>models based on the recently introduced AMP chain graphs and ancestral<br/>graphs.  The main objectives of the study are development of<br/>parameterizations, construction and implementation of efficient algorithms<br/>for maximum likelihood estimation, and investigation of procedures for<br/>model selection. A particular focus of the project will be on employing<br/>modern tools from computational algebra in the analysis of the structure of<br/>parameter spaces and properties of likelihood functions.<br/><br/>Multivariate statistical models seek to describe the complex relationships<br/>between a large set of variables. A particular class of such models,<br/>called graphical models, has found wide-spread application in fields like<br/>artificial intelligence, bio-informatics, biology, epidemiology, and<br/>speech recognition.  The models proposed in the project extend the realm<br/>of graphical models and it is anticipated that they will be applied in<br/>many of these fields.  Moreover, the proposed methodology will provide new<br/>tools for the analysis of data of public interest such as census data.<br/>The researchers also plan to make software tools freely available as part<br/>of a larger open source statistical software package called R.<br/>"
"0503822","Statistical Inverse Problems, Semiparametric Models, and Empirical Processes","DMS","STATISTICS","07/01/2005","05/03/2007","Jon Wellner","WA","University of Washington","Continuing Grant","Gabor Szekely","06/30/2009","$308,998.00","","jaw@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, OTHR","$0.00","PROPOSAL NUMBER.: 0503822<br/>INSTITUTION: University of Washington<br/>NSF PROGRAM: STATISTICS<br/>PRINCIPAL INVESTIGATOR and Co-PI: Wellner, Jon A<br/>PROPOSAL TITLE: Statistical Inverse problems, Semiparametric Models, and Empirical Processes<br/>                                  Abstract  <br/><br/><br/>Jon A. Wellner will carry out research on statistical inverse problems, semi-parametric models, and on empirical process techniques involved in studying these problems. Wellner and graduate students Marloes Maathuis and Leah Jager will carry out research on competing risks with current status data algorithms and theory for monotone function estimation, and a new family of goodness-of-fit tests related to the Berk-Jones statistic and the associated confidence bands, respectively. The first part of this research will involve further study of <br/>nonstandard asymptotics for maximum likelihood and least<br/>squares estimators of monotone, multiply monotone and completely monotone functions.  The investigator plans to develop new penalized likelihood estimators and new likelihood ratio tests and related confidence intervals, as well as introduce new methods of studying the maximum likelihood estimators themselves. The investigator also intends to refine and improve the existing computational algorithms to the point where additional monte-carlo studies of the various estimators can be carried out for likelihood ratio statistics and profile likelihoods, distribution theory for new limiting distributions, and new <br/>distribution theory for point processes.  The investigator will investigate new <br/>computational algorithms and comparisons of various competing algorithms<br/>for several inverse problems.   <br/><br/>Basic empirical process tools and methods will be developed and applied to statistical problems  concerning semiparametric models and inverse problems.   Applications include regression models for panel count data, bivariate interval censored data of several kinds including models for competing risks, regression models for multivariate survival data, and studies of non- and semi-parametric maximum likelihood estimators used in HIV-AIDS research, and two-phase data dependent designs.  Wellner also plans to conduct further research on semiparametric models for panel count data with former Ph.D. student Ying Zhang (now at the University of Iowa, Biostatistics) and further work on longitudinal data involving multiple counting processes with former Ph.D. student Hao Liu (now at the University of California at Davis, Biostatistics).  <br/><br/><br/>"
"0505824","Nonparametric Cluster Analysis","DMS","STATISTICS","06/15/2005","06/23/2008","Werner Stuetzle","WA","University of Washington","Standard Grant","Gabor Szekely","05/31/2009","$168,986.00","","wxs@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, OTHR","$0.00","The investigator studies the problem of finding groups in data<br/>(""clustering""). Most existing clustering methods make implicit or<br/>explicit assumptions about the shapes of the groups, for example that<br/>the groups are roughly spherical or Gaussian, and they will fail if<br/>these assumptions are violated. The goal of the project is to develop<br/>(i) nonparametric clustering methods capable of finding groups of<br/>arbitrary shape; (ii) methods for assessing the statistical validity<br/>of the clusters; (iii) tools for visualizing the results. Clustering<br/>is cast as a statistical rather than a purely algorithmic problem. The<br/>observed data are regarded as a sample from some underlying<br/>population, and the goal is to estimate a well defined target<br/>characteristic of the population - the cluster tree - from the<br/>sample. Adopting a statistical view of clustering has two benefits:<br/>(i) it allows comparing the performance of different methods; (ii) it<br/>gives meaning to the notion of ""cluster validity"". Without a sampling<br/>model and a target characteristic of the population the question<br/>whether clusters are valid or spurious is meaningless. The proposed<br/>method for estimating the population cluster tree is based on<br/>analyzing a graph over the sample but, unlike most other graph-based<br/>clustering methods, it is motivated by the underlying statistical<br/>estimation problem. Assessing cluster validity - determining the<br/>number of distinct groups, with ""one"" as a possible answer - has<br/>proven a vexing problem, especially in the absence of prior knowledge<br/>or assumptions about group shapes. The investigator proposes a novel<br/>approach to this problem based on resampling.<br/><br/>Finding groups in data is a problem that occurs in many areas, from<br/>genomics (identifying groups of genes with similar function based on<br/>gene expression levels measured by DNA microarrays) to information<br/>retrieval (spotting topics in document collections) to marketing<br/>(determining distinct groups of customers with similar<br/>characteristics). Clustering is an exploratory tool, and there<br/>typically is little or no prior information about the shapes or the<br/>number of groups. It is therefore important to have methods that<br/>automatically determine the number of groups and do not rely on<br/>assumptions about their shape, and visualization tools that help in<br/>understanding the shapes of the groups, their arrangement in feature<br/>space, and the influence of parameters of the clustering method on the<br/>results. <br/>"
"0505759","Bayesian Analysis and Prediction of Gaussian Random Fields","DMS","STATISTICS","06/01/2005","03/08/2006","Victor DeOliveira","AR","University of Arkansas","Continuing Grant","Rong Chen","04/30/2007","$122,078.00","","victor.deoliveira@utsa.edu","1125 W MAPLE ST STE 316","FAYETTEVILLE","AR","727013124","4795753845","MPS","1269","0000, 9150, OTHR","$0.00","This project develops methodology for objective Bayesian analysis <br/>of spatial data, both geostatistical and lattice data, that arise in <br/>many of the social and earth sciences, such as economy, epidemiology, <br/>geography, geology and hydrology, as well as computationally efficient <br/>algorithms to perform Bayesian analysis and prediction based on <br/>moderate to large spatial datasets. <br/>On the methodological side, the investigator derives new automatic <br/>prior distributions for the parameters of different kinds of Gaussian <br/>random fields, specified either by their covariance matrices or by <br/>their precision matrices. <br/>The research explores the main statistical properties of Bayesian <br/>inferences based on these automatic priors, such as conditions for <br/>posterior propriety, frequentist properties of parameter and<br/>predictive inferences, and existence of predictive summaries.<br/>A point of particular interest is the study of the pros and cons of <br/>the dependence of these automatic priors on the sampling design.<br/>On the computational side, the investigator derives methods to <br/>approximate these automatic priors distributions, since evaluation <br/>of these priors is in most cases computationally expensive, and <br/>develops new computationally efficient algorithms for Bayesian <br/>inference and prediction of spatial data that would make feasible <br/>Bayesian analysis based on moderate to large spatial datasets.<br/>The methodology proposed in this project serves as an initial step <br/>toward the development of objective Bayesian analysis for spatial <br/>hierarchical models used to describe non-Gaussian data, since most <br/>of these models use Gaussian random fields as building blocks.<br/><br/><br/>The statistical methodology developed during this project has practical<br/>impacts in many social and earth sciences, such as economy, epidemiology, <br/>geography, geology and hydrology, where the collection and analysis of <br/>spatial data have become common tasks.<br/>A paradigm of statistics called the Bayesian approach possesses several <br/>conceptual and methodological advantages when compared to traditional <br/>approaches for the analysis of spatial data, but technical and <br/>computational difficulties that arise during implementation have <br/>hindered its more widespread use among practitioners.<br/>This is particularly so for the analysis of some types large spatial <br/>datasets where current implementations of the Bayesian approach <br/>are too cumbersome or unfeasible to be carried out.<br/>The statistical methodology developed in this project would contribute <br/>to overcome some of these technical and computational hurdles, and<br/>consequently to bridge the gap between methodology and practice for <br/>Bayesian analysis of spatial data.<br/>Graduate students would be engaged in the project, contributing to <br/>their statistical training as well as the enhancement of the Statistics <br/>program at the University of Arkansas.<br/>"
"0505424","Exploiting Special Structures in High-Dimensional Data Classification","DMS","STATISTICS","06/01/2005","05/16/2005","Elizaveta Levina","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","05/31/2009","$101,275.00","","elevina@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","ABSTRACT<br/><br/>Proposed research develops new practical methodology and algorithms as<br/>well as theoretical results for high-dimensional data classification. The<br/>main issue is that when the number of measured variables exceeds by far<br/>the number of observations, estimating the full covariance matrix<br/>accurately is impossible. The investigator has previously shown that<br/>ignoring the dependence completely in such a situation is a better option,<br/>but it means discarding a lot of information.  This problem will be<br/>resolved by developing new sparse covariance estimators which will only<br/>retain important dependence information, and studying their behavior<br/>theoretically in the context of discriminant analysis. New practical,<br/>computationally efficient algorithms for computing these estimators from<br/>data will be developed on the basis of clustering and graph partitioning<br/>methods and compared to existing regularization techniques. Theoretically,<br/>asymptotically optimal or near optimal classification performance is<br/>expected to be demonstrated.  Another way to reduce the size of the<br/>problem and get to the underlying structure is to reduce the data<br/>dimension using recently developed nonlinear manifold projection methods,<br/>which aim to discover a nonlinear low-dimensional embedding preserving<br/>most of the information contained in the data. Using these methods<br/>requires estimating intrinsic data dimension and neighborhood scale on a<br/>manifold, and new rigorous estimators for both are proposed, along with an<br/>analysis of their statistical properties.  Careful estimation of these two<br/>parameters will improve on the current mostly heuristic methods used in<br/>machine learning and increase applicability of manifold projection methods<br/>for high-dimensional data classification.<br/><br/><br/>This proposal addresses the new challenges posed by the massive amounts of<br/>data collected in the modern world by developing new theoretical and<br/>practical tools for dealing with high-dimensional data, particularly with<br/>the situation when the number of measurements taken for one observation is<br/>large relative to the number of observations.  New sparse estimators of<br/>dependence structure in such data are developed, which only contain the<br/>information relevant for data classification.  The new estimators can also<br/>be used in any problem where large covariance matrices need to be<br/>estimated from limited amount of data, and hence will have an impact on a<br/>wide range of modern applications, such as classification and analysis of<br/>gene expression data, analysis of complex chemical and physical<br/>experiments, remote sensing, and medical imaging, among others.<br/><br/>"
"0508671","A Conference on Nonparametric Inference and Probability with Applications to Science","DMS","STATISTICS","07/01/2005","02/16/2005","Robert Keener","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Grace Yang","06/30/2006","$10,000.00","Jiayang Sun","keener@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","This project provides funds to partially support a research conference<br/>focusing on several related important topics in nonparametric<br/>inference and probability, with applications to science. The<br/>conference is planned to be held in Ann Arbor at the University of<br/>Michigan, September 10-11, 2005. It will highlight challenges and<br/>developments in biased sampling and missing data, shape-restricted<br/>inference, contemporary sequential analysis, and modern nonparametric<br/>inference, probability, and statistics in biology and physical sciences.<br/><br/>Statistical inference, the science of learning from data, has always<br/>had an interdisciplinary focus, and as statistics and related areas of<br/>application evolve and move forward, there is an ongoing need to share<br/>developments and new ideas across related disciplines. This conference<br/>at the interface of statistics and the sciences will promote this<br/>transfer of knowledge allowing researchers in these related fields to<br/>share ideas and learn from one another. The conference will also have<br/>broader impacts, allowing young and leading researchers to interact<br/>and exchange ideas, promoting collaboration, broadening the scope of<br/>applications, and contributing to further developments in these areas.<br/>"
"0505432","Flexible Classification and Regression","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","07/01/2005","04/28/2005","Ji Zhu","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","06/30/2008","$115,597.00","Saharon Rosset","jizhu@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269, 1271","0000, 9263, OTHR","$0.00","The research aims to combine statistical and computational considerations <br/>in designing new and useful predictive modeling tools and algorithms.  <br/>Specifically, the research involves the development of: a) new <br/>statistically motivated multi-class boosting algorithms, based on a family <br/>of multi-class loss functions and forward stagewise additive modeling; b) <br/>a family of (loss, penalty) pairs that give piecewise linear solution <br/>paths, and yield modeling tools for both regression and classification <br/>which are robust, adaptable and efficient; c) a general theory and <br/>efficient algorithms for solving an L1 regularized problem in infinite <br/>dimensional predictor space.<br/><br/>With the advent of modern technologies, the needs for predictive <br/>modeling tools have been increasing rapidly.  Consequently, many new ideas <br/>and methods have been finding their way into the statistical community in <br/>recent years.  These are mainly related to the design and analysis of <br/>useful techniques for modeling of high dimensional, noisy data, and these <br/>techniques are now being applied to bioinformatics, high energy physics, <br/>speech recognition, text mining, and a wide range of other important <br/>practical problems.  This research aims to push these developments forward <br/>along the line of regularization in predictive modeling, and is expected <br/>to have broader impacts on the practice and education in the domains of <br/>statistics, machine learning and data mining.<br/>"
"0456571","Committee on Applied and Theoretical Statistics","DMS","STATISTICS","05/15/2005","05/09/2007","Michelle Schwalbe","DC","National Academy of Sciences","Continuing Grant","Gabor Szekely","04/30/2009","$165,000.00","","mschwalbe@nas.edu","2101 CONSTITUTION AVE NW","WASHINGTON","DC","204180007","2023342254","MPS","1269","0000, 7556, OTHR","$0.00","Proposal ID:  DMS-0456571  <br/>Prev Awd:  0119560  <br/>PI:  Weidman, Scott T.    <br/>Institution:  National Academy of Sciences   <br/>Title:  COMMITTEE ON APPLIED AND THEORETICAL STATISTICS   <br/> <br/>  <br/>                                                              ABSTRACT<br/><br/>The Committee on Applied and Theoretical Statistics (CATS) helps to illuminate promising research directions for the statistical sciences by defining and launching workshops and studies of importance to the community.  Recent activities have included broadly cross-disciplinary workshops on Statistical Analysis of Massive Streams of Data (December, 2002) and on Visualization of Uncertain Information (March, 2005).  The former of these led to a special issue of the Journal of Computational and Graphical Statistics (December 2003) and to a workshop proceedings published by the National Academy Press.  Planned activities include a cross-disciplinary workshop on Statistics on Networks, planned for September 2005, and oversight of a study to establish an improved research foundation for fingerprint analysis.<br/>Because of its position within the National Academies, CATS is well situated to strengthen connections between the statistical sciences and other fields.  It has ready access to the leaders in all fields of science, engineering, education, and medicine.  This access gives CATS a unique capability for assembling leaders in a variety of fields, including statistics, to address technical and policy issues in a cross-disciplinary fashion.  In addition, the Committee's position within the National Academies gives it access to leaders in federal agencies and public policy, which enables CATS to identify emerging issues of national importance that require input from the statistical sciences community.   <br/>"
"0505133","Collaborative Research: Analysis of Functional and High-Dimensional Data  with Applications","DMS","STATISTICS","08/01/2005","05/31/2005","Marianna Pensky","FL","The University of Central Florida Board of Trustees","Standard Grant","Gabor Szekely","12/31/2008","$72,000.00","","Marianna.Pensky@ucf.edu","4000 CENTRAL FLORIDA BLVD","ORLANDO","FL","328168005","4078230387","MPS","1269","0000, OTHR","$0.00","This project deals with statistical inference in<br/>phenomena that result in massive, multidimensional  and/or<br/>functional data sets. Prime examples are geophysical, biomedical,<br/>and internet related data. In addition to high dimensionality, such<br/>data are often characterized by self-affinity and require<br/>non-standard (functional) models for their modeling and subsequent<br/>statistical analysis. The methodology to be developed will advance<br/>both the theory and practice of functional data analysis, a very<br/>fast-developing and modern area of statistics. The common and novel<br/>features of the statistical methods proposed here lie in the nature<br/>of analyzed data. The data sets are massive, multidimensional,<br/>functional, and possibly self-affine (fractal or multifractal).<br/>Recent progress in multiscale data representations provide natural<br/>and efficient environments for (i)  developing scale-sensitive<br/>analyzing tools for estimation, testing, classification, and<br/>deconvolution, and (ii) describing, summarizing, and modeling<br/>self-similar data. Bayesian methodology will be used whenever<br/>available prior information can be incorporated or whenever sensible<br/>automatic priors are possible.<br/><br/>Development of new inferential methodologies is critical for the<br/>statistical support of recent scientific initiatives and newly<br/>emerging technologies. The proposed research is application driven,<br/>so the specificities of the application fields influence the design<br/>and focus of the methodology. Techniques suggested in the proposal<br/>deal  with problems of testing of efficiency of new medical<br/>treatments, target  detection and classification as well as<br/>classification of medical images, or more accurate recovery of radar<br/>or satellite data. Hence, the methodologies which result from the<br/>proposal are applicable in such areas of strategic interest as<br/>health and medicine and homeland security. In addition to<br/>methodological impact,  the proposed research has a strong<br/>educational component consisting of training graduate students,<br/>involving undergraduate students in research projects, conducting<br/>inter-departmental seminars, increasing awareness of mathematics<br/>education among the work force, and attracting minority and female<br/>students.<br/><br/>"
"0505552","Unified Robust Multivariate Analysis Including High-Dimensional Testing Based on Scaler Scale Analysis","DMS","STATISTICS","07/01/2005","05/09/2005","Kesar Singh","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","06/30/2008","$75,002.00","","kesar@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00"," <br/>                                                    Abstract<br/><br/>Unified Robust Multivariate Analysis Including High-Dimensional Testing<br/>                                 Based on Scaler Scale Analysis<br/>                                                Kesar Singh<br/><br/> The goal of this proposal is to develop a general framework for multiparameter testing based on the concept of scale curves.  Using a scale curve to measure the scale of a distribution, in comparison with the traditional variance-covariance matrix, has two distinct advantages:  i) it provides a scalar measure and thus easier to grasp its magnitude than the complex matrix measure; and ii) the scale curve is a simple curve on the plane no matter how high the dimension of the data, and this curve can be easily visualized and interpreted. The tests proposed in this proposal are completely nonparametric and robust; and the parameters considered range from the standard ones to high-dimensional parameters pertaining to gene expression, astro-statistical and various functional data. It accomplishes the highly complex testing tasks through easy to read figures of scale bands. This scale curve based methodology can be easily comprehensible and implementable by all practitioners of statistics.<br/><br/> The methodology developed in this proposal will broaden substantially the application of scale curves and classical multivariate statistical analysis, while bringing down the level of technical complexities.  Progress from this proposal should further the statistics discipline as a whole, and facilitate many practical applications in domains such as genetics, and risk management. For the educational purposes, the proposed research activities will allow the student to learn about modern statistics techniques such as robust multivariate analysis, the bootstrap, analysis of gene expression data, etc. Through this project, they can acquire both theoretical research skills and hands-on experience with real life data. Such training is essential for student to become contributing statisticians in the future.<br/> <br/><br/><br/>"
"0457248","Smoothed multiple endpoint procedures.","DMS","STATISTICS","07/01/2005","06/27/2005","Harold Sackrowitz","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","06/30/2008","$160,000.00","Arthur Cohen","sackrowi@rci.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00"," Classical multiple endpoint testing procedures such as Scheffe,<br/>Tukey, Bonferroni etc. are oftentimes too conservative.  This is<br/>particularly true if the number of endpoints is large as in many recent<br/>applications.  In an effort to declare significance of endpoints more<br/>rapidly than classical multiple comparison methods permit, procedures with<br/>stepwise structure were introduced.  Most recent procedures have a<br/>stepwise structure, that is they are either single-step, step-up or<br/>step-down.  To evaluate and compare procedures the proposers have studied<br/>properties of these stepwise procedures.  One very surprising result of<br/>considerable practical value is that the popular step-up procedure is<br/>inadmissible, even for a vector risk function consisting of average size<br/>as one component and average type II error as the other component.  The<br/>step-up and step-down procedures also have a disturbing practical<br/>property; namely a small negative change in one variable accompanied with<br/>reasonably sizeable positive changes in other variables can lead from many<br/>rejections to many acceptances.  The current proposal is to find ""smooth""<br/>procedures that retain the desirable properties of stepwise procedures<br/>while improving on the shortcomings.  By representing the step-up<br/>procedure as a linear combination of products of indicator functions a<br/>smooth competitor to step-up is found and will be evaluated.  Another idea<br/>proposed is to find a parametric empirical Bayes procedure that will<br/>control the average size component of the vector risk used to evaluate<br/>procedures.  A critical aspect of the proposal is to study the amount of<br/>improvement the new methods have over step-up.<br/><br/> The investigators study the problem of how to decide which among<br/>many possible individual entities are significant.  For example, there are<br/>thousands of a person's genes examined in a microarray.  Which genes are<br/>different or expressed differently when one examines cancer patients<br/>compared to non-cancer patients.  To be able to decide such could provide<br/>a valuable diagnostic tool for early detection of cancer.  This example,<br/>is one of many, that illustrates the problem of multiple testing<br/>procedures.  Other areas of application include detection of covert<br/>communications, detection of bioweapons, comparing several treatments with<br/>a control, examination of mutual fund data in an effort to single out<br/>successful funds, examining testing methodologies in education and<br/>psychology settings.  New and efficient statistical methodologies are<br/>introduced and analyzed in this research undertaking.<br/>"
"0534181","Complex Datasets and Inverse Problems:    Tomography, Networks, and Beyond; Rutgers University - New Brunswick, NJ; October 21-22, 2005","DMS","STATISTICS","09/01/2005","07/14/2005","Cun-Hui Zhang","NJ","Rutgers University New Brunswick","Standard Grant","Rong Chen","08/31/2006","$16,000.00","","czhang@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","Abstract <br/>Prop ID:  DMS-0534181  <br/>PI:  Zhang, Cun-Hui  <br/>PO:  Shulamith T. Gross  <br/>Institution:  Rutgers University New Brunswick  <br/>Title:  Complex Datasets and Inverse Problems: Tomography, Networks, and Beyond   <br/> <br/><br/>The conference ``Complex Datasets and Inverse Problems: Tomography, Networks, and Beyond'' will be held October 21-22, 2005 at Rutgers University. The conference will focus on a number of important and emerging interdisciplinary areas of research, including medical tomography, networks, and biased data. Statistical tomography algorithms have been playing crucial roles in the development of medical imaging systems, from CAT, PET, SPECT to MRI. In fast functional MRI, brain functions are studied from data sets composed of multiple time series of incomplete Fourier transformation of the deoxy spin density of the brain. Networks are abundant around us: social, energy, traffic, communication, and computer are just some of the examples. Enormous amount of networks data have been collected in the information age we live in, but few statistical tools have been developed for analyzing them as they are typically governed by time-varying and mutually dependent communication protocols sitting on complicated graph-structured network topologies. Many prototypical applications in these and other important technologies can be viewed as statistical inverse problems with large, high-dimensional, and probably biased/incomplete data, which serve as the unifying ground for the conference. <br/><br/>The conference will advance several important areas in statistics, including models and methodologies for complex datasets, inverse problems, imaging systems, networks, and incomplete and biased data. Cutting-edge developments of statistical models, methods, and algorithms will be discussed. The conference will have direct impact on a broad range of scientific applications outside the immediate realm of statistics. Examples include functional MRI and other medical imaging systems, telecommunication, energy, transportation, and social networks, network security, bioinformatics, epidemiology, and clinical trials. The conference is expected to attract researchers in different areas of applications, in medical imaging, telecommunications, bio-medical engineering, bioinformatics, epidemiology, and more. These will comprise both internationally renowned experts and graduate students or young researchers who wish to embark in these rapidly progressing interdisciplinary areas. Time will be generously allotted for informal discussion and fruitful exchange of ideas. Through these activities, the conference will play an important role in fostering new research partnership between young and senior participants and among researchers in different areas of applications. The conference will promote research activities, education, and participation of new investigators, graduate students, and researchers from under-represented groups. The proceedings of the conference have been arranged to be published as a volume in the Institute of Mathematical Statistics Monograph series. This publication will help disseminate widely the advances covered in the conference, especially among the researchers who are not able to attend the conference. <br/><br/>"
"0505599","Bayesian Methods for Large-Scale Applications","DMS","STATISTICS","07/01/2005","05/22/2007","David Madigan","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","03/31/2008","$149,995.00","","madigan@stat.columbia.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","                                                   ABSTRACT <br/><br/>PROPOSAL NUMBER.: DMS-0505599  <br/>INSTITUTION: Rutgers University New Brunswick  <br/>NSF PROGRAM: STATISTICS<br/>PRINCIPAL INVESTIGATOR: Madigan, David<br/>PROPOSAL TITLE: Bayesian Methods for Large-Scale Applications   <br/><br/>The investigators work on Bayesian statistical methods for large-scale<br/>applications. Three applications provide the backdrop for the work.<br/>""Text categorization"" concerns the automatic assignment of documents<br/>to predefined categories and requires ultra-high dimensional supervised<br/>learning models. ""Authorship attribution"" uses similar methodology<br/>but attempts to identify authors of anonymous documents.<br/>The ""Localization"" problem uses signal characteristics to locate users <br/>in  wireless networks.  The investigators focus on technical challenges <br/>that span these applications including sequential Bayesian analysis,<br/>non-linear optimization, and novelty detection algorithms.<br/><br/>In both the business and scientific realms, computing advances have<br/>drastically altered the role of data analysis.  Historically, analysts<br/>produced data locally to address specific research questions. Now,<br/>ubiquitous computing and cheap storage have decoupled the production<br/>of data from the research questions. Data of all kinds are produced<br/>and deposited in remotely accessible databases with myriad questions<br/>in mind, both foreseen and unforeseen. Statistics has historically<br/>focused on squeezing the maximum amount of information out of limited<br/>data. The investigator's work focuses instead on so-called<br/>Bayesian statistical methods for these  emerging larger-scale applications<br/><br/>"
"0504387","Statistical Methods in Fast Functional MRI","DMS","STATISTICS, , ","07/01/2005","05/22/2007","Larry Shepp","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","06/30/2009","$435,435.00","Cun-Hui Zhang","shepp@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269, T306, T331","0000, 9218, HPCC, OTHR","$0.00","PROPOSAL NUMBER.: 0504387<br/>INSTITUTION: Rutgers University New Brunswick<br/>NSF PROGRAM: STATISTICS<br/>PRINCIPAL INVESTIGATOR and Co-PI: Shepp, Larry and Zhang, C Hui<br/>PROPOSAL TITLE: Statistical Methods in Fast Functional MRI<br/><br/><br/><br/>     Abstract<br/><br/>The proposed research will further advance and use statistical methods developed by the principle investigators and their collaborators to <br/>sharply improve time-resolution for the functional magnetic resonance imaging. The objective remains to improve the time-resolution of <br/>functional magnetic resonance imaging by sampling only a small fraction of the Fourier transform of the spin density, and using a prolate <br/>wavelet filter to approximately obtain an integral representing the total activity of the difference in susceptibility between task and pre-task, over various regions of interest in the brain at successive time-points. The cost for this is a decrease in spatial resolution. A nearly optimal trajectory will be used for sampling a small cube in three-dimensional k-space about the origin. This sampling region is also nearly optimal. The use of a, again nearly optimal, prolate filter will provide a low spatial but high temporal resolution image of the deoxy-hemoglobin density. An aim of the project is to find one or more consistent locations in the brain where oxygen is consumed during <br/>higher level processing by the brain of the image in the primary visual region. This region would then be scanned in a two-dimensional <br/>experiment where a slice plane is chosen to go through the region which would then give convincing demonstration of feasibility of the <br/>proposed methods.<br/>The proposal focuses on developing statistical methods and related theory for fast functional magnetic resonance imaging, to sharply <br/>improve the time-resolution of present techniques.  Fast fMRI is expected to have profound and far-reaching consequences in the <br/>understanding of brain function, a problem of central scientific interest at the present time.<br/><br/>"
"0505902","Eighth North American Meeting of New Researchers in Statistics and Probability","DMS","PROBABILITY, STATISTICS","08/01/2005","02/16/2005","Galin Jones","MN","University of Minnesota-Twin Cities","Standard Grant","Grace Yang","01/31/2006","$20,000.00","","galin@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1263, 1269","0000, OTHR","$0.00","The Eighth North American Meeting of New Researchers in Statistics and <br/>Probability is a meeting of recent Ph.D. recipients in Statistics and <br/>Probability.   Anyone who has received a Ph.D. since 2000 or expects to <br/>receive a Ph.D. by 2006 is eligible to attend.  The purpose of the <br/>conference is to promote the professional development of  new <br/>researchers primarily by introducing them to each other's research in an <br/>informal setting. All participants are expected to give a short, <br/>expository talk or contribute a poster on their research.  Topics will <br/>range across a variety of areas from theory and methods to <br/>applications.  This will provide a unique opportunity for junior <br/>researchers to exchange ideas and initiate contacts amongst themselves <br/>as well as provide them an opportunity to interact with the invited <br/>senior participants.  The meeting is to be held prior to the 2005 Joint <br/>Statistical Meetings in Minneapolis, MN. <br/><br/>The New Researchers conference series is explicitly aimed at training <br/>the future leaders in Statistics and Probability.  At present, <br/>researchers develop contacts with others in their field either through <br/>their own institutions or at meetings of professional societies. It is <br/>often the case that, at their own institutions, junior researchers have <br/>limited contact with senior colleagues.  Also, meetings of professional <br/>societies are rather large and junior researchers often find them <br/>overwhelming. In helping to create networks of new researchers, the <br/>conference will lay the groundwork for future collaboration and the <br/>informal exchange of ideas and knowledge.  It also assists in general <br/>professional development.  Knowing people outside of one's research <br/>specialty area is critical for professional growth, assisting with <br/>making contacts at other universities and industries, and helping with <br/>professional service activities.  In particular, traditionally <br/>underrepresented groups are strongly encouraged to attend.<br/><br/>"
"0504871","Analysis of Absolute Deviation, Inference and Model Selection","DMS","STATISTICS","08/01/2005","07/19/2007","Zhiliang Ying","NY","Columbia University","Continuing Grant","Gabor Szekely","07/31/2009","$159,886.00","","zying@stat.columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","0000, OTHR","$0.00","This project develops a comprehensive approach to estimation,<br/>model selection and inferences for certain regression models, with<br/>applications to health sciences, economics, astronomy as well as<br/>other disciplines of scientific investigations. Various parts of<br/>the project are connected through several key components:<br/>computational aspect is handled by linear programming, inferences<br/>by simulation-based resampling, simultaneous estimation and model<br/>selection by suitably constructed penalty functions, and<br/>asymptotic properties by empirical process theory. The<br/>investigator develops efficient algorithms for parameter<br/>estimation and distributional approximation, establishes small and<br/>large sample properties, and extends the methods and theory to<br/>data with censoring, truncation or other kind of non-random<br/>missingness.<br/><br/>This research project is motivated by and closely related to many<br/>important areas of scientific disciplines, including health and<br/>life sciences, economics, astronomy and sociology. It develops new<br/>tools to assess health risks, drugs and treatments effects and<br/>genetic variations, to analyze policy formation, such as<br/>intervention in unemployment and health insurance, as well as to<br/>understand astronomical phenomena. It provides a platform to<br/>attract and train students with tools for multi-disciplinary<br/>applications.<br/><br/>"
"0505490","Collaborative Research: Analysis of Functional and High-Dimensional Data with Applications","DMS","STATISTICS","08/01/2005","05/31/2005","Brani Vidakovic","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","07/31/2008","$83,998.00","","brani@tamu.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","0000, OTHR","$0.00","  This project deals with statistical inference in<br/>phenomena that result in massive, multidimensional  and/or<br/>functional data sets. Prime examples are geophysical, biomedical,<br/>and internet related data. In addition to high dimensionality, such<br/>data are often characterized by self-affinity and require<br/>non-standard (functional) models for their modeling and subsequent<br/>statistical analysis. The methodology to be developed will advance<br/>both the theory and practice of functional data analysis, a very<br/>fast-developing and modern area of statistics. The common and novel<br/>features of the statistical methods proposed here lie in the nature<br/>of analyzed data. The data sets are massive, multidimensional,<br/>functional, and possibly self-affine (fractal or multifractal).<br/>Recent progress in multiscale data representations provide natural<br/>and efficient environments for (i)  developing scale-sensitive<br/>analyzing tools for estimation, testing, classification, and<br/>deconvolution, and (ii) describing, summarizing, and modeling<br/>self-similar data. Bayesian methodology will be used whenever<br/>available prior information can be incorporated or whenever sensible<br/>automatic priors are possible.<br/><br/><br/><br/>Development of new inferential methodologies is critical for the<br/>statistical support of recent scientific initiatives and newly<br/>emerging technologies. The proposed research is application driven,<br/>so the specificities of the application fields influence the design<br/>and focus of the methodology. Techniques suggested in the proposal<br/>deal  with problems of testing of efficiency of new medical<br/>treatments, target  detection and classification as well as<br/>classification of medical images, or more accurate recovery of radar<br/>or satellite data. Hence, the methodologies which result from the<br/>proposal are applicable in such areas of strategic interest as<br/>health and medicine and homeland security. In addition to<br/>methodological impact,  the proposed research has a strong<br/>educational component consisting of training graduate students,<br/>involving undergraduate students in research projects, conducting<br/>inter-departmental seminars, increasing awareness of mathematics<br/>education among the work force, and attracting minority and female<br/>students.<br/><br/><br/>"
"0504283","New Approaches to Variable Selection","DMS","STATISTICS","07/01/2005","05/12/2005","Dennis Boos","NC","North Carolina State University","Standard Grant","Gabor Szekely","06/30/2009","$300,000.00","Leonard Stefanski","boos@stat.ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","0000, OTHR","$0.00","In this project the investigators will develop two novel, but related,<br/>approaches to regression model variable selection. Both procedures<br/>make use of multiple, Monte Carlo-generated, pseudo data sets to tune<br/>the controlling parameter alpha in a standard variable-selection<br/>routine (e.g., alpha = ""alpha-to-enter"" in forward selection). In the<br/>first approach, white noise is added to the response variable with<br/>variance in controlled multiples, m, of the full-model mean squared<br/>error (FMMSE). The variable selection process is run on the<br/>noise-enhanced data, and the selected model mean squared error (MSE)<br/>is found for each value of the selection process tuning parameter<br/>alpha.  This process is repeated for many bootstrap-type replications<br/>and the average MSE is retained for each value of m. The optimal alpha<br/>is determined as the value that gives, on average, the theoretically<br/>expected mean squared error, FMMSE(1 + m), for the noise-enhanced<br/>data. This approach has wide applicability to variable selection<br/>procedures used with additive-error regression models. In the second<br/>approach phony predictors are added to the data set, and the<br/>proportion of phony variables included in the selected model for<br/>different values of the selection process tuning parameter is<br/>estimated. Then, by averaging over bootstrap-type replications, the<br/>false selection rate (FSR) for the process can be estimated for the<br/>observed data for each value of the process tuning parameter<br/>alpha. The FSR is controlled by choice of alpha. The FSR is a very<br/>understandable and meaningful quantity to control.  This method is not<br/>restricted to additive-error models and thus has wider applicability<br/>than the noise-enhancement above.<br/><br/>Regression modeling is the most widely used statistical procedure.<br/>Statisticians have long known that the choice of predictor variables<br/>to use in a regression model is the most important component of<br/>regression analysis. Yet the identification of important predictor<br/>variables remains one of the least understood and most important open<br/>problems in statistical inference. This is true for small to moderate<br/>data sets with a handful of potential predictor variables, as well as<br/>for the huge data sets with potential predictors numbering in the<br/>thousands that are becoming more prevalent in statistical<br/>applications. In this project the investigators develop methods for<br/>identifying important predictor variables from a larger set of<br/>potential predictors. The impact of the research is as broad as the<br/>application of regression modeling. The new methods will enable<br/>researchers in all application fields to better fit regression models<br/>to data sets, both small and large. The range of applications is<br/>enormous and includes, for example, genetic microarray data, drug<br/>development data, census bureau data, financial data such as credit<br/>card transactions or loan applications, large weather and<br/>environmental data sets, and electric power usage data.<br/>"
"0624239","Dimension Reduction for Stochastic Processes","DMS","STATISTICS","09/01/2005","06/26/2007","Randall Eubank","AZ","Arizona State University","Continuing Grant","Gabor Szekely","06/30/2009","$146,721.00","","eubank@math.asu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1269","0000, OTHR","$0.00","<br/>Traditional multivariate statistics focuses on the analysis of data that<br/>are vectors of finite length. However, modern data collection methods are<br/>now frequently returning observations that should be viewed as the result<br/>of digitized recording or sampling from stochastic processes. These types of<br/>data occur in the context of functional data analysis (where the<br/>observation process has a one dimensional index set), image analysis and<br/>spatial statistics, for example, and arise from every aspect of the modern<br/>world. Our ability to process and to make use of such data directly<br/>affects the way in which scientific inquiries are conducted and practical<br/>problems are solved. Goals for analyzing this type of high dimensional<br/>data include the detection of structure and dimensionality reduction and<br/>these are the issues that will be addressed in the proposed project.<br/>Specifically, a number of topics will be investigated concerning dimension<br/>reduction for data from stochastic processes including i) canonical<br/>correlations analysis for two or more processes, ii) mixed models methods<br/>for analysis of data from multiple processes, iii) inverse regression and<br/>iv) varying coefficient models. The unifying theme in all this work is the<br/>use of reproducing kernel Hilbert space methods to formulate both the<br/>problems and their proposed solutions.<br/><br/>With fast progressing modern technology in fields such as medicine,<br/>environmental science, and homeland security, the data collected in those<br/>fields today are frequently curved or spatial data, and may be even more<br/>complicated in terms of scope and structure. Traditional statistical<br/>methods were created to primarily deal with<br/>low-dimensional data, and are not suitable for the high-dimensional or<br/>``functional'' nature of the data described above. This research is aimed<br/>at addressing a number of fundamental issues in the emerging branch of<br/>modern statistical data analysis that deals with high-dimensional data.<br/>The results to be obtained will not only potentially impact<br/>high-dimensional data analytic methodology across a myriad of<br/>disciplines, but will also provide a theoretical foundation and<br/>directions for future statistical research.<br/><br/>"
"0520687","Sixth International Conference on Forensic Statistics","DMS","STATISTICS, LSS-Law And Social Sciences","04/15/2005","04/04/2005","David Kaye","AZ","Arizona State University","Standard Grant","Grace Yang","03/31/2006","$11,000.00","","k@asu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1269, 1372","0000, 7556, OTHR","$0.00","The International Conference on Forensic Statistics is the only conference that brings statisticians, forensic scientists, social scientists, legal scholars, judges, prosecutors, lawyers, and other experts together for formal presentations, discussion, and debate on the varied uses of probability and statistics in legislative, administrative and judicial proceedings. The sixth conference in this series, sponsored by the Arizona State University Center for the Study of Law, Science and Technology, and the Institute for Mathematical Statistics, will be held in Tempe, Arizona, from March 17 to 19, 2005. Sessions will center on analytical methods and interpretation in forensic science, statistical studies of discrimination and fairness, and experimentation on jury understanding of scientific and statistical evidence. Researchers will describe current work on statistical issues and models in the analysis of fibers, glass fragments, bullet lead, drugs, writing style, and fingerprints; on Bayesian and other methods for interpreting DNA profiles in complex and problematic cases; on social and statistical issues in searching convicted offender DNA databases; and on many other applications of statistical and logical inference in legal studies and litigation.<br/><br/> <br/><br/>The conference is intended to spur the development of suitable statistical and related methods that will improve law enforcement and civil justice.  The logical issues involved in forensic statistics are subtle and entail cross-disciplinary work with experts in diverse fields. DNA evidence, for example, has introduced daunting problems of statistical analysis, with complications such as samples comprising mixtures of DNA from more than one person and the identification of possible offenders through extensive searches of DNA databases. The highly publicized false fingerprint identification in the March 11 Madrid train bombing case and the weaknesses of the statistical aspects of existing bullet lead identification procedures identified in a recent National Academy of Science report commissioned by the FBI underscore the importance of improvements in the analysis and presentation of forensic evidence. These are but a sampling of the issues that will be illuminated by the many presentations from researchers in the fields of computer science, probability and statistics, biology, chemistry, linguistics, psychology, sociology, and law.<br/><br/>"
"0426056","SGER:    Multi-scale Modeling for Homeland Security and Supply-Chain Logistics Efficiency","DMS","STATISTICS, SERVICE ENTERPRISE SYSTEMS","01/01/2005","12/01/2004","Jye-Chyi Lu","GA","Georgia Tech Research Corporation","Standard Grant","Grace Yang","12/31/2005","$75,000.00","Alan Erera, Chelsea White, C. F. Jeff Wu, Paul Kvam","jclu@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269, 1787","0000, 9237, OTHR","$0.00","This Small Grant for Exploratory Research (SGER) formulates multi-scale <br/>spatial modeling procedures for handling data in various details to support <br/>solution of complex logistics optimization problems.  The investigators <br/>develop reliability concepts useful in the logistics field for solving <br/>homeland security related problems.  Finally, this project integrates <br/>probability and statistical models with operations research tools to <br/>improve seaport container-operation efficiency to meet the needs of <br/>security inspections.<br/><br/>Evaluation and design of secure and reliable supply-chain logistics <br/>networks are of critical importance to the modern U.S. economy.  In <br/>particular, balancing security and efficiency objectives in the design and <br/>operation of transportation logistics systems and seaport container <br/>inspections is vitally important, and to do so presents unique mathematical <br/>and statistical modeling challenges.  This interdisciplinary team of <br/>statisticians and logisticians will bring years of experience in research <br/>and education to explore the potential in this field.  The key scientific <br/>impact of this project is that the theory and methods developed in this <br/>project serve as the foundation for advances in multi-scale data modeling <br/>and hierarchical decision-analysis tools.  This project also provides <br/>several broader impacts required by the NSF, e.g., collaboration with <br/>industry for disseminating research results, education of high-quality <br/>students through thesis advising and class teaching (with case studies <br/>explored in this project), and support of summer research experience for <br/>students from underrepresentated groups.<br/>"
"0504323","Multiscale Volatility Analysis for High-Frequency Financial Data","DMS","STATISTICS","07/01/2005","07/19/2007","Yazhen Wang","CT","University of Connecticut","Standard Grant","Gabor Szekely","06/30/2009","$113,998.00","","yzwang@stat.wisc.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","0000, OTHR","$0.00","                                                      ABSTRACT<br/>Proposal Id: DMS-0504323  <br/>Institution: :  University of Connecticut  <br/>Principal Investigator: Wang, Yazhen  <br/>Title: Multiscale Volatility Analysis for High-Frequency Financial Data   <br/>  <br/>      Volatilities of asset returns are pivotal for many issues in<br/>financial economics. They are the key ingredients for the pricing<br/>of financial instruments, portfolio allocation, managerial<br/>decision-making, and financial risk management. Existing financial <br/>modeling mostly employs parametric models like GARCH and stochastic <br/>models. These parametric models are used for modeling volatility <br/>dynamics of low-frequency interdaily data and generally fail to describe <br/>the volatility patterns in high-frequency intradaily data, partly because <br/>high-frequency data have complex structures such as jumps and market <br/>microstructure noise contamination.  The investigator initiates an <br/>innovative research on modeling and analyzing of high-frequency data.<br/>By taking advantage of the fact that jumps, volatility and microstructure <br/>noise tend to behave differently at different scales, he uses wavelets' <br/>multiscale structure as a platform to realize the fact and perform efficient<br/>volatility analysis for high-frequency return data. The proposed nonparametric <br/>multiscale methodology is very flexible and applies to high-frequency data <br/>with (a) microstructure noise, (b) jumps, and (c) leverage effect. It achieves <br/>the following goals: (1) utilize all available data efficiently, (2) test and <br/>detect jumps, (3) construct noise resistant realized volatility for estimating <br/>integrated volatility, and (4) separate the jump variation from the total <br/>quadratic variation.<br/><br/>    Financial data are collected at increasingly shorter time horizons. Over <br/>past decade there has been a radical improvement in the availability of <br/>high-frequency financial data (which is referred to as  intradaily data, <br/>while low-frequency data mean financial data at daily or longer time horizons). <br/>Nowadays high-frequency financial data are available for financial instruments <br/>on markets of all locations and at scales like minute and even individual bids <br/>to buy and sell. One example of high-frequency data is the currency <br/>transaction data that are collected at every minute. High-frequency finance <br/>is an emerging new and challenge research field. The proposed research <br/>invents new statistical techniques to analyze high-frequency financial data <br/>and accurately model and forecast financial markets. It <br/>advances the understanding of high-frequency volatility and provides better <br/>volatility evaluation for practitioners in financial industry, where mistakes <br/>and wrong judgments can lead to catastrophic consequence for the industry and <br/>the rest of society. <br/><br/>"
"0505676","Flexible Statistical Modeling","DMS","STATISTICS","08/01/2005","06/02/2008","Trevor Hastie","CA","Stanford University","Continuing Grant","Gabor Szekely","07/31/2010","$392,271.00","","hastie@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","The investigator studies statistical models in a variety of applied<br/>situations which require innovative modifications of the standard<br/>technology.  Document classification builds models in extremely<br/>high-dimensional feature spaces, as do models for inference and<br/>prediction with gene expression arrays. Species occurrence and<br/>abundance models deal with large numbers of species, often sharing<br/>many characteristics.  In each of these settings, the different<br/>contexts have led the researcher to develop special forms of<br/>regularization that allow one to exploit the structure in the data.<br/><br/><br/>In this advanced technological age, we are faced with analyzing extremely large<br/>volumes of data. Two of the several examples this researcher deals with are gene<br/>expression measurements (40 thousand measurements per human sample), and<br/>online document classification (often the web is the source). Standard<br/>statistical tools do not work well in these situations. This investigator<br/>studies innovative adaptations of these tools designed to defeat the<br/>challenges these problems pose.<br/>"
"0505732","Evolutionary and energy-domain Monte Carlo algorithms and their applications","DMS","STATISTICS","07/01/2005","09/11/2007","Wing Hung Wong","CA","Stanford University","Continuing Grant","Gabor Szekely","06/30/2009","$399,969.00","","whwong@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","Abstract for DMS - 0505732<br/>PI: Professor Wing Hung Wong<br/>INSTITUTION: STANFORD UNIVERSITY<br/>TITLE: Evolution and Energy Domain Monte Carlo algorithms and their applications<br/><br/>This project will develop two new algorithms for Monte Carlo simulation and applied them to several problems in science and technology. The first method, called Evolutionary Monte Carlo, was introduced by the PI's laboratory under prior NSF support. It has been applied with excellent results in challenging problems such as the HP model in protein folding and Cp-based model selection. Here, it is proposed that Evolutionary MC be developed for the computational inference of network structure for directed acylic graphical models (DAG).  The second method, called equi-energy sampling, is in an early stage of development by the PI and his collaborators. The idea is to generate samples from the equi-energy rings each of them having the energy lying within a restricted interval of values. An energy-temperature duality is exploited to allow the estimation of a Boltzmann average (i.e. averages corresponding to a fixed temperature) from estimates of the micro-canonical averages (i.e. averages within equi-energy rings). In addition to giving estimates of Boltzmann averages, this approach also provide estimates for the ""density of states"" function and the partition function. Thus equi-energy sampling can provide information for all thermodynamic quantities in a single run, and for this reason it should be a very attractive algorithm in physical applications such as protein folding. <br/><br/>Recent development of Markov Chain Monte Carlo methods has allowed the application of statistical modeling and inference to more and more application areas. For many important applications such as DNA motif sampling, protein folding, and statistical inference of causal network structures, Monte Carlo computation has become an indispensable tool. The work proposed in this project will result in significant improvement of the performance Monte Carlo algorithms in problems with complex energy landscapes, and therefore will make it feasible to apply this method to a wider spectrum of problems.<br/><br/>"
"0505303","Rigorous Methods for Dimensionality Reduction of High-Dimensional Data","DMS","STATISTICS","07/01/2005","01/23/2009","David Donoho","CA","Stanford University","Continuing Grant","Gabor Szekely","06/30/2010","$799,890.00","David Donoho, Iain Johnstone","donoho@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","A research effort is proposed to create tools for data analysis and inference in high-dimensional settings.  The effort uses tools from random matrix theory (RMT), Banach Space Theory (BST), and differential geometry (DG) to expose new phenomena in high-dimensional statistical inference and data analysis, yielding practical statistical methods with rigorously-established properties under carefully-stated conditions.  The results will impact a wide range of data analysis problems, including the building of linear models, the testing of complex hypotheses about multivariate data, and the detection of subtle nonlinear structures in high-dimensional data. In the research, the investigators build further bridges between RMT, BST, and DG and three problem areas: (a) Sparse Linear Modelling -- How should one build a predictive model choosing relatively few predictors out of many available predictors?; (b) Multivariate Analysis in High Dimensions -- How should one best estimate and test for structure in high-dimensional data, particularly when the number of variables is large and the number of observations is small?; (c) Manifold Learning -- How can one best find nonlinear structure in high-dimensional data and best parametrize that structure?  Each of these areas is of fundamental importance to the analysis of high-dimensional data, and the investigators identify a strategy to use RMT, BST, and DG to make substantial contributions to each.  This strategy builds on the authors' recent research accomplishments using RMT, BST, and DG, which will be extended to show: (a) how to find the best-fitting low-dimensional linear model without spending exponential time searching through model space -- extending previous successes in using Basis Pursuit, LARS and Lasso; (b) how to correctly test a wide range of important hypotheses in multivariate analysis using the Tracy-Widom distribution -- extending previous results in applying the Tracy-Widom distribution to Principal Components Analysis; and (c) how to correctly estimate a nonlinear parametrization of sparsely sampled curved data in high dimensional space -- extending previous successes in developing the Hessian Eigenmap technique of dimensionality reduction.<br/><br/><br/>The motivation for this project lies in the `data deluge' now engulfing every branch of science and technology.  In field after field, new sensors are creating data streams of unparalleled breadth and depth. As a result, today scientific and technological progress depends heavily on the ability to process high-dimensional data and reduce its dimensionality, sometimes drastically, obtaining a good approximation using a few well-chosen combinations of the original measurements. While many methods of dimensionality reduction have already been proposed, much existing research activity in this area is heuristic and speculative; the tools are often of unknown reliability and their properties hold under conditions of unknown generality.  This project develops methods based on careful mathematical analysis to develop methods of dimensionality reduction which are rigorously correct and/or optimal.  These methods give the user the assurance that important features are captured in the dimensions which remain and that little of importance is discarded in the dimensions that are thrown away. The project develops such rigorous methods in three areas: (a) building parsimonious but accurate predictive models out of a database of many possible predictors; (b) testing for hidden structure in what otherwise seems to be high dimensional `noise'; (c) discovering the correct representation for data which are intrinsically nonlinear.  Strong expectations for success of this project can be based on existing solid achievements by the investigators in each of these three areas.<br/>"
"0505748","Computational Tools and Theory of Multivariate Spatial Models","DMS","STATISTICS, EPSCoR Co-Funding","07/15/2005","08/03/2009","Ronald Barry","AK","University of Alaska Fairbanks Campus","Standard Grant","Gabor Szekely","06/30/2010","$190,000.00","Devin Johnson, Julie McIntyre","ffrpb@uaf.edu","2145 N. TANANA LOOP","FAIRBANKS","AK","997750001","9074747301","MPS","1269, 9150","0000, 9150, OTHR","$0.00","ABSTRACT:<br/><br/>PI: Barry, Ronald<br/>Award ID: DMS - 0505748<br/>Institution: University of Alaska, Fairbanks<br/>Program: STATISTICS<br/>Title: Computational Tools and Theory of Multivariate Spatial Models<br/><br/>A research group is developing software and theory for hierarchical and multivariable spatial models.  As such models involve unobserved, spatially-correlated variables, modeling these processes will require the development of flexible variogram models that will conform to a wide variety of random processes.  These are approached through process-convolution models.  The investigators are considering a variety of special cases, including cokriging, generalized (hierarchical) spatial models, spatial compositional data and Poisson process models.  They are exploring several computational approaches, including generalized estimating equations, MCMC and expectation-maximization.  They are also identifying spectral, sparse matrix and other approaches to reduce the computational burden of the analysis.  The software they are developing will allow researchers in many fields to analyze multivariable spatial models.<br/><br/>Many research projects produce data that is associated with locations on a map.  For instance, an epidemiological study might consist of air pollution measurements taken at several fixed locations, along with the locations of households where an occupant was hospitalized with a pollution-related ailment.  In this case the air pollution measurement is a geostatistical variable (it could, in theory, be measured at any location on the map) and the households are a point process.  Other examples might include multiple geostatistical variables:  for instance, soil nitrogen and soil moisture measured at multiple locations in an ecological study.  Multiple geostatistical variables and point process variables are special cases of what are called multivariate or hierarchical spatial models.  The investigators are developing the general theory of these models, and are producing software that can efficiently analyze these models.  Dissemination of the software will allow the development of more sophisticated models for mapped data in many applied fields.<br/>"
"0505682","Quantitating Heterogeneity","DMS","STATISTICS","09/01/2005","06/08/2005","Guenther Walther","CA","Stanford University","Standard Grant","Gabor Szekely","02/28/2010","$180,000.00","","Walther@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","The investigator studies two topics concerning the analysis of mixture distributions which are relevant for the analysis of flow cytometry data. The investigator develops a rigorous <br/>theoretical basis for the inference on the mixture complexity, i.e. the number of subpopulations in the mixture. This understanding allows to develop new methodology that provides finite-sample confidence statements, is more powerful than existing techniques, yet at the same time is more generally applicable.  The second topic addresses the quantitative characterization of <br/>differences in two multivariate distributions, based on samples from these distributions. The research develops methodology to detect regions where the two distributions differ, provides scientifically meaningful and interpretable descriptions of these regions, and provides confidence statements for the corresponding differences. The underlying methodology is also applicable to other problems, such as the detection of spatial clusters of disease cases.<br/><br/>Flow cytometry instruments are among the most widely used biomedical<br/>instruments in the world, but the lack of appropriate statistical methodology<br/>for analyzing the data generated by this instrument is a roadblock<br/>to harnessing its full potential. The investigator studies<br/>two topics that arise in the analysis of flow cytometry data,<br/>but which are also important for other applications.<br/>Those data are comprised of various subpopulations, and the investigator<br/>develops rigorous methodolgy to determine how many such subpopulations<br/>can be distinguished, and how to characterize differences in two<br/>samples that may have different numbers of subpopulations. These<br/>are important tasks for the anlysis of flow cytometry data which to date<br/>have not been satisfactorily resolved. Advances in the statistical methodology<br/>methodology for the above topics have the potential of a significant<br/>impact in biomedical research and in the clinical setting, and have<br/>furthermore important applications in the detection of spatial clusters<br/>in bio-surveillance, ecology, astronomy, and medical imaging.<br/>"
"0505673","Statistical Theory and Methodology","DMS","STATISTICS","08/01/2005","05/31/2007","Bradley Efron","CA","Stanford University","Continuing Grant","Gabor Szekely","07/31/2008","$310,000.00","Persi Diaconis","brad@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","  ABSTRACT<br/><br/>Prop ID:  DMS-050 5673  <br/>PI:  Efron, Bradley and Diaconis, Persi<br/>NSF Program: STATISTICS    <br/>Institution:  Stanford University  <br/>Title:  Statistical Theory and Methodology   <br/><br/><br/>LARGE-SCALE SIMULTANEOUS TESTING (Bradley Efron) <br/><br/><br/>This investigator is studying the analysis of large-scale simultaneous hypothesis<br/>testing situations, for example a microarray experiment searching for genes that<br/>behave differently in HIV positive or negative subjects. A simple<br/>methodology is being developed that requires a minimum of frequentist or<br/>Bayesian modelling assumptions, and provides for both the efficient<br/>selection of the non-null cases, and the estimation <br/>of effect sizes. In classical terminology, both size and power are assessed.<br/>This methodology depends on false discovery rate calculations, <br/>implemented via empirical Bayes techniques. A typical result might report<br/>""there are 200 of the 10,000 genes that can be clearly identified as<br/>differentially expressed between the two groups of subjects, but there are<br/>also about 800 other non-null genes that this experiment was not powerful<br/>enough to detect.""<br/><br/>Classical 20th Century statistical theory was fashioned to handle small<br/>problems, dozens or maybe hundreds of data points, with one or maybe a few<br/>unknown parameters. 21st Century scientific technology now provides massive<br/>data sets, with millions of individual measurements and thousands of<br/>parameters to consider all at once. Microarrays are the prime example, but<br/>similar problems arise from a variety of devices: proteomic chips, time of<br/>flight spectroscopy, flow cytometry, and fMRI scanners. The goal of this<br/>research is an efficient, computationally efficient methodology for analyzing<br/>massive simultaneous testing problems, without the need for extensive<br/>modelling assumptions.<br/><br/><br/><br/>MONTE CARLO METHODS IN PROBABILITY AND STATISTICS (Persi Diaconis)<br/><br/>The main focus of this investigation is on rates of convergence of Monte Carlo<br/>and Markov chain algorithms for statistical computation. One aspect is <br/>phase transitions ( cut-off phenomena), extending Diaconis' recent solution of<br/>the Peres conjecture (joint with Saloff-Coste). The work<br/>includes creating new algorithms using computational tools such as Grobner<br/>bases and combinatorial characterizations such as Tuttes f-factors. This also<br/>contributes to Bayesian methodology studying prior distributions for Markov<br/>chains, non-parametrics and<br/>computational tools. A final focus is the development of group theoretic<br/>character theories in non-standard situations such as<br/>unipotent groups and Hecke algebras.<br/><br/>Probability models underlie many areas of modern scientific study, but they<br/>raise puzzling and important questions concerning the connection of the<br/>model with real world phenomena. Diaconis studies<br/>foundational topics such as `what does it mean to say coin flips are<br/>random'? This recently led to the discovery (joint with Susan Holmes and<br/>Richard Montgomery) that in fact, natural human coin flips show a small but<br/>significant bias (about 51% come up the same as the previous flip).<br/>Careful looks at the assumptions, justification and<br/>validity of `randomness' apply to widely-used simulation methods (how long<br/>should an algorithm be run until its job is done?). Weather forcasting<br/>and air pollution are just two of the areas that require the dependability<br/>of such simulations.<br/>"
"0505696","Model-based Classification of Longitudinal and Functional Data","DMS","STATISTICS","07/01/2005","05/04/2006","Mohsen Pourahmadi","IL","Northern Illinois University","Continuing Grant","Gabor Szekely","06/30/2008","$60,084.00","","mohsen@math.niu.edu","1425 W LINCOLN HWY","DEKALB","IL","601152828","8157531581","MPS","1269","0000, OTHR","$0.00","The idea of classification  permeates many scientific studies and arises in almost every area of human endeavors including the classical problems of numerical taxonomy and market segmentation, and the modern areas of machine learning, experimental spectroscopy and biotechnology.  Fisher's linear classifier which maximizes the separation between the groups in the spirit of analysis of variance requires multivariate normality for each group with a common covariance matrix.  For heterogeneous covariance matrices, the optimal classifier is no longer linear and has poor performance for small samples.  Though there are many heuristic and ad hoc methods to handle the case of unequal covariances, model-based approaches using mixtures of multivariate normal distributions and the spectral decomposition of the covariance matrices have shown great promise for the traditional multivariate data.  The goal of this research is to develop new and flexible classification methods for longitudinal, functional and multivariate time series data using the Cholesky decomposition of covariance matrices instead of their spectral decompositions.  For such data, the Cholesky decomposition is more suitable and its components enjoy both statistical interpretation as certain regression coefficients and geometric interpretation in terms of volumes, shapes and orientations of ellipsoids representing various groups in the data. It is proposed to study the computational, statistical and empirical aspects of using the Cholesky decomposition and compare the results with those obtained using the spectral decomposition.  The methods and tools to be employed include:  generalized linear and mixed models, factor analysis, time series analysis, maximum likelihood and Bayesian estimation of mixture models in the presence of missing values, cross-validation and bootstrap.<br/> The proposed research will extend classical discriminant analysis to longitudinal and functional data.  It is of great practical interest and will provide insight into when a particular classification method can be expected to work well, and may lead to the development of new classification criteria and methods for discriminating between nuclear explosions and earthquakes.  A problem which is of critical importance for monitoring a comprehensive test-ban treaty.  The broader impact of the proposed work can be seen in settings where high-dimensional and large amounts of multivariate data are collected, such as clinical trials, biotechnology, environmental monitoring and global change, epidemiology and financial econometrics.<br/>"
"0504953","Collaborative Research on Bayesian Nonparametric Methods for Spatial and Spatiotemporal Data","DMS","STATISTICS","08/01/2005","09/09/2008","Alan Gelfand","NC","Duke University","Standard Grant","Gabor Szekely","07/31/2009","$67,966.00","","alan@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, OTHR","$0.00","ABSTRACT <br/><br/>Principal Investigators:   Kottas, Athanasios and Gelfand, Alan<br/>Proposal Number: DMS - 0505085 and DMS - 0504953<br/>Proposal Title: Collaborative Research on Bayesian Nonparametric Methods for Spatial and Spatiotemporal Data<br/>Institution: University of California Santa Cruz and Duke University<br/><br/>The investigators develop Bayesian nonparametric methodology for<br/>spatial and spatio-temporal data analysis. Point-referenced spatial<br/>data arises in several fields, including atmospheric science, ecology, <br/>environmental science, and epidemiology. In fact, often such data is <br/>replicated across time say through sampling at monitoring sites. In <br/>certain cases, with appropriate preliminary manipulation, the<br/>replicates may be viewed as independent. More often, the temporal <br/>dependence is retained and, discretizing time, a time series of<br/>spatial processes emerges. In either case, virtually all of the<br/>modeling for the spatial processes is specified parametrically; in fact, <br/>it is almost always a Gaussian process which is most frequently<br/>assumed to be stationary. The investigators study new classes of <br/>nonparametric spatial models to remove these assumptions. These models <br/>are applicable to either of the above replicated settings. In its <br/>simplest form, the investigators use Dirichlet processes to create <br/>random spatial processes, which are non-Gaussian, nonstationary, and <br/>have non-homogeneous variance. These processes are defined through<br/>their finite dimensional distributions, and are referred to as spatial <br/>Dirichlet processes. A spatial Dirichlet process is then convolved<br/>with a pure error process to create an illustrative spatial process<br/>with a nugget component. Such models are hierarchical and can be<br/>fitted through Markov chain Monte Carlo methods. In application, the <br/>investigators use spatial Dirichlet processes to introduce spatial<br/>random effects into the modeling, either directly with independent <br/>replicates or embedded within a dynamic model to handle temporal <br/>dependence. The investigators study an assortment of problems<br/>associated with the use of spatial Dirichlet processes, including<br/>their theoretical global and local properties; their use as mixing<br/>models; their use with semiparametric mixing; their implementation in <br/>dynamic models; their utilization for interpolation at given time<br/>points and for forecasting at future time points; their use with <br/>non-Gaussian first stage specifications for the data; their use in <br/>describing multivariate distributions and, as a special case, for <br/>extended regression modeling; their use in modeling spatial point <br/>process data; and their extension to richer classes of so-called <br/>generalized spatial Dirichlet processes.<br/><br/><br/> <br/>Point-referenced spatial data arises in application areas as diverse <br/>as environmental science, climatology, ecology, epidemiology, and<br/>real estate markets. As researchers collect more and more space and <br/>space-time data, the need for analyses to enhance their understanding <br/>of the complex processes they are sampling grows. This inspires the <br/>need for sufficiently rich models to accommodate a variety of global<br/>and local behaviors. The primary motivation for this research is to <br/>expand the catalog of space-time modeling tools available to such <br/>scientists. This research work suggests the first approach to <br/>nonparametric Bayesian spatial and spatio-temporal data analysis. <br/>Nonparametric Bayesian approaches have witnessed increased utilization <br/>in recent years as a result of their successful application to certain <br/>problems in, for example, engineering and biomedical fields.<br/>Similar success is anticipated by bringing this methodology to <br/>space-time settings. In particular, it is anticipated that, for fields <br/>such as epidemiology, environmental contamination and weather<br/>modeling, researchers will value the flexible modeling framework the <br/>work offers. And, an increase in usage of the methodology is expected <br/>as the computational techniques to fit the models are advanced.<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>"
"0541993","Red Raider Mini-Symposium 2005: Geometry and Statistics and Image Analysis","DMS","STATISTICS","10/01/2005","11/01/2005","Victor Patrangenaru","TX","Texas Tech University","Standard Grant","Grace Yang","09/30/2006","$5,000.00","Robert Paige","vic@stat.fsu.edu","2500 BROADWAY","LUBBOCK","TX","79409","8067423884","MPS","1269","0000, OTHR","$0.00","<br/>Red Raider Mini-Symposium, November 17-19, 2005 : ""Geometry, Statistics <br/>and Image Analysis"" focuses on nonparametric statistics on manifolds,<br/>analyzes on shape spaces, saddlepoint approximations and systems<br/>theory leading to frontiers of science in medical imaging, system<br/>reliability, survival analysis, pattern recognition and<br/>bioinformatics. Principal speakers are Rudy Beran, University of<br/>California Davis, internationally renowned for his contributions<br/>to robust statistics, directional data analysis, and<br/>bootstrapping; Rabindra Bhattacharya, University of Arizona,<br/>internationally renowned for his contributions to large sample<br/>theory, Edgeworth expansions and bootstrapping, statistics on<br/>manifolds and stochastic partial differential equations; Ronald<br/>Butler, Colorado State University, internationally renowned for<br/>his contributions to saddlepoint methods, systems theory and<br/>stochastic networks, special function approximations, robustness<br/>and likelihood inference; John Kent, University of Leeds, United<br/>Kingdom, internationally renowned for his contributions to<br/>multivariate statistics, shape analysis and stochastic models for<br/>protein structure, robustness, spatial statistics, tomography;<br/>Peter Kim, University of Guelph, Canada, internationally renowned<br/>for his contributions to the interface between geometry and<br/>statistics, density estimation on Riemannian manifolds and<br/>applications of statistics to image acquisition, Madan Puri,<br/>Indiana University, internationally renowned for his contributions<br/>to nonparametric statistics, splines, tests of normality, fuzzy<br/>sets and measures, stochastic processes, statistics of directional<br/>data, random sets, and time series and Anuj Srivastava, nationally<br/>renowned for his contributions to ""image understanding"" which aims<br/>to develop automated systems that can match human abilities in<br/>analyzing images.<br/><br/>This symposium is conceived to bring together outstanding researchers<br/>working in the interface of geometry, statistics and image<br/>analysis and consists of a series of lectures by outstanding<br/>scholars in vibrant areas of mathematics and statistics, with<br/>important applications in astronomy and geophysics, homeland<br/>security, medical imaging, computer vision and pattern<br/>recognition. These lectures provide an opportunity to learn about<br/>new research venues for the members of the statistical community,<br/>especially early career researchers and graduate students<br/>including women and under represented minorities. This<br/>mini-symposium provides a perfect medium for interaction and<br/>collaboration between the conference participants and the<br/>distinguished speakers. It will be very helpful for shaping future<br/>directions of statistical research that address problems of high<br/>societal impact."
"0506743","Fifth International Workshop on Objective Bayesian Methodology","DMS","STATISTICS, Methodology, Measuremt & Stats","06/01/2005","02/16/2005","Dongchu Sun","MO","University of Missouri-Columbia","Standard Grant","Grace Yang","05/31/2006","$12,000.00","Paul Speckman","sund@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269, 1333","0000, OTHR","$0.00","The Fifth International Workshop on Objective Bayesian Methodology will<br/>be held in Branson, Missouri, June 4-8, 2005.<br/>Objective Bayesian methodology is, for the most part, oriented towards<br/>the development of prior distributions that can be used automatically,<br/>i.e. that do not require subjective input other than the specific<br/>probabilistic model chosen to describe the data. There are three quite<br/>distinct statistical domains in which this development has taken place:<br/>parametric estimation, model selection, and prediction,<br/>Objective Bayesian methodology is of increasing importance today since<br/>application of Bayesian analysis is rapidly growing among nonspecialists,<br/>most of whom seek automatic or objective Bayesian procedures.<br/>This workshop will emphasize prediction, practical applications<br/>such as spatial-temporal models, multiple comparisons and goodness-of-fit.<br/>Bayesian methods usually are heavily depend on prior distributions.<br/>Although a prior distribution is an important component in the Bayesian<br/>paradigm, it is known that even in the absence of any relevant prior<br/>information, the Bayes method can produce reliable inference to many<br/>challenging problems by employing appropriate objective (noninformative)<br/>Bayesian analysis. The principal objectives are to facilitate the<br/>exchange of recent research developments in objective Bayesian<br/>methodology, to provide opportunities for new researchers, and to<br/>establish new collaborations that will channel efforts into pending<br/>problems and open new directions for investigation. An important<br/>consequence of this meeting will be to further crystallize objective prior<br/>methodology as an established area for statistical research.<br/><br/>Statisticians have traditionally embraced one of two main approaches,<br/>classical or ""frequentist"" methods and Bayesian methods. In recent years,<br/>advances in computing technology and breakthroughs in statistical theory<br/>have made Beyesian methods increasingly attractive, especially for <br/>difficult problems with many variables in fields such as meteorology, <br/>epidemiology, natural resources, etc. With this increased emphasis on <br/>Bayesian statistics, it has become increasingly important to bridge the <br/>differences between the two schools of statistical thinking.  Objective <br/>Bayesians are dedicated to resolving these differences.  The planned <br/>workshop will bring together 80-100 participants including leaders in the <br/>field of objective Bayesian statistics from around the world together with <br/>young researchers and students in a setting especially conducive to <br/>cooperation and exchange of ideas.<br/><br/>"
"0514910","VIII Symposium on Case Studies in Bayesian Statistics","DMS","STATISTICS","07/01/2005","03/22/2005","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Grace Yang","06/30/2006","$14,960.00","","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, 7556, OTHR","$0.00","Case Studies in Bayesian Statistics VIII is the eighth workshop in the<br/>series that was begun in 1991. These workshops aim to advance<br/>statistical practice by examining Bayesian methods in specific applied<br/>contexts. The workshops are held in odd years at Carnegie Mellon<br/>University in early fall. The eighth workshop is planned for September<br/>16--17, 2005.<br/> <br/>The objectives of the workshop, ""Case Studies in Bayesian Statistics<br/>VIII"" is are to explore the interplay of statistical theory and<br/>practice in the context of substantive scientific research; to promote<br/>the continued development of Bayesian statistics by highlighting<br/>problems in the sciences that require non-standard approaches; to<br/>provide an opportunity for scientists and statisticians to present<br/>their work in depth, highlighting both the scientific background and<br/>the analytical approaches; and to encourage dissemination of the<br/>findings presented at the workshop via well-documented and<br/>peer-reviewed case studies.  As it has evolved, this workshop series<br/>has become an important meeting for younger researchers in Bayesian<br/>statistics.  The workshop aims to encourage young researchers,<br/>including graduate students, to present their applied work; provide a<br/>small meeting atmosphere to facilitate the interaction of young<br/>researchers with senior colleagues; expose young researchers to<br/>important challenges and opportunities in collaborative research; and<br/>include as participants women, under-represented minorities and<br/>persons with disabilities who might benefit from the small workshop<br/>environment.<br/><br/>"
"0504972","Multivariate growth charts and robust quantile estimation","DMS","STATISTICS","07/01/2005","05/12/2005","Ying Wei","NY","Columbia University","Standard Grant","Gabor Szekely","06/30/2008","$105,000.00","","yw2148@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","0000, OTHR","$0.00","Growth charts have been widely used in clinics and medical centers to monitor an individual subject's growth or health status in context of population values. Current growth charts consider only one measurement at a time. The investigators propose to develop a framework for multivariate growth charts. More specifically, the objectives of the proposed research include (1) to provide a mathematically formal but clinically sensible definition of multivariate growth charts, and study their properties; (2) to propose methodology for estimating and incorporating time effects and other potential covariate effects (e.g. past growth) into those growth charts; (3) to develop statistical inference and model assessment  tools; (4) to compare the proposed methodology with existing methods and explore alternative devices for multivariate growth chart construction; (5) to propose robust quantile estimators to limit the impact of outliers. <br/><br/>The multivariate growth charts can provide much more informative tools than the currently available univariate growth charts in many areas of public health and biomedical applications. For example, pediatrics will be able to do better screening for obesity with a multivariate approach using longitudinal information on weight and height than with a single body-mass-index (BMI) chart. The methodologies to be developed by the investigators (e.g. estimation of covariate-adjusted multivariate quantile functions) are of general interest to statistical research. The proposed research will be widely disseminated through publications, presentations in domestic and international conferences, and collaborations with clinical researchers. <br/>"
"0503718","Topics in the Theory of Randomization","DMS","STATISTICS","07/01/2005","04/28/2005","William Rosenberger","MD","University of Maryland Baltimore County","Standard Grant","Grace Yang","08/31/2005","$160,000.00","","wrosenbe@gmu.edu","1000 HILLTOP CIR","BALTIMORE","MD","212500001","4104553140","MPS","1269","0000, OTHR","$0.00","The investigator will explore theoretical properties of randomization <br/>procedures with applications to sequential randomized clinical trials. <br/>The first problem the investigator will address is sequential monitoring <br/>of randomization tests following restricted randomization, which will <br/>require extensive use of techniques in sequential analysis and <br/>nonparametric statistical inference.  The second problem the investigator <br/>will address is the development and properties of response-adaptive <br/>randomization procedures, including procedures designed for continuous <br/>responses, and survival responses with censoring.  The third problem the <br/>investigator will address is certain theoretical properties of <br/>covariate-adaptive randomization procedures, particularly <br/>randomization-based inference procedures following covariate-adaptive <br/>randomization.  The ultimate goal of the investigator is to complete a <br/>systmeatic study of randomization, its properties, and its relevance in <br/>clinical trials.<br/><br/>The investigator will explore properties of randomization, the hallmark of <br/>a well-designed clinical trial.  The first problem the investigator will <br/>address is sequential monitoring of randomized clinical trials, which <br/>refers to making early decisions about the effectiveness of new therapies <br/>when compelling evidence is accrued at an interim point in a clinical <br/>trial.  There are currently no sequential monitoring techniques available <br/>that allow the randomization itself to be incorporate into these <br/>decisions, and hence current methodology completely ignores the design of <br/>the trial in the analysis.  The second problem the investigator will <br/>address is response-adaptive randomization, and he will develop new <br/>procedures for clinical trials where the outcome is time-to-event or some <br/>continous measure, e.g. cholesterol level.  These response-adaptive <br/>procedures are attractive from an ethical point of view, because they seek <br/>to maximize power to detect a relevant clinical outcome, while <br/>simultaneously minimizing the expected number of treatment failures <br/>patients will experience while participating in the clinical trial.  The <br/>third problem the investigator will address is proporties of <br/>covariate-adaptive randomization procedures, which is designed to minimize <br/>tial imbalances in important confounding variables that might bias results <br/>of the clinical trial.  Each of these topics is relevant to today's <br/>clinical trials in the drug development phase of new pharmaceuticals.<br/>"
"0448704","CAREER: Asymptotics of random processes and their applications","DMS","STATISTICS","03/01/2005","11/09/2009","Wei Biao Wu","IL","University of Chicago","Continuing Grant","Gabor Szekely","02/28/2011","$400,000.00","","wbwu@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, 1045, OTHR","$0.00","The proposal aims to advance statistical theory for random<br/>processes that exhibit features like long-range dependence and<br/>nonlinearities, and to educate both statisticians and others<br/>scientists in this new exciting area. Compared to the<br/>well-developed theory under the independence assumption, it is<br/>considerably more challenging to establish a limit theory for<br/>processes with such features. The Principal Investigator proposes<br/>a powerful martingale based method and studies spectral<br/>estimation, empirical processes, nonparametric estimation and<br/>other related asymptotic problems for such processes.<br/><br/>Processes with long-range dependence and nonlinearities occur in<br/>various fields, including computer networks, communication,<br/>finance, geology, hydrology, econometrics and atmospheric science<br/>among others. Applications of the research results developed in<br/>the proposal would help test and justify claims made by scientists<br/>in such fields. In particular, the PI develops statistical<br/>methodology to identify trends in temperature and ozone sequences<br/>and provides statistical reasoning for meteorologists' claims on<br/>climate change patterns.<br/>"
"0505612","Collaborative Research: Graphical and Algebraic Models for Multivariate Categorical Data","DMS","STATISTICS","07/01/2005","04/28/2005","Mathias Drton","IL","University of Chicago","Standard Grant","Gabor Szekely","06/30/2008","$88,001.00","Thomas Richardson","md5@uw.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00","The proposed research project develops models for multivariate categorical<br/>data by mimicking Gaussian models with a desired model structure that can<br/>be captured in terms of the non-parametric concept of conditional<br/>independence.  This method has a long history: graphical log-linear models<br/>can be induced in this way by Gaussian models defined by zero constraints<br/>on the inverse covariance matrix. The project seeks to greatly extend the<br/>scope of the approach.  It is proposed to define and study marginal<br/>independence models for contingency tables, discrete-valued time series<br/>with moving average-like dependence structure, seemingly unrelated<br/>regressions with discrete response variables, and discrete graphical<br/>models based on the recently introduced AMP chain graphs and ancestral<br/>graphs.  The main objectives of the study are development of<br/>parameterizations, construction and implementation of efficient algorithms<br/>for maximum likelihood estimation, and investigation of procedures for<br/>model selection. A particular focus of the project will be on employing<br/>modern tools from computational algebra in the analysis of the structure of<br/>parameter spaces and properties of likelihood functions.<br/><br/>Multivariate statistical models seek to describe the complex relationships<br/>between a large set of variables. A particular class of such models,<br/>called graphical models, has found wide-spread application in fields like<br/>artificial intelligence, bio-informatics, biology, epidemiology, and<br/>speech recognition.  The models proposed in the project extend the realm<br/>of graphical models and it is anticipated that they will be applied in<br/>many of these fields.  Moreover, the proposed methodology will provide new<br/>tools for the analysis of data of public interest such as census data.<br/>The researchers also plan to make software tools freely available as part<br/>of a larger open source statistical software package called R.<br/>"
"0505595","Practical Perfect Sampling for Bayesian Computation and Engineering and Financial Applications","DMS","STATISTICS","07/01/2005","06/06/2006","Xiao-Li Meng","MA","Harvard University","Continuing Grant","Gabor Szekely","06/30/2008","$258,511.00","Jose Blanchet","meng@stat.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269","0000, OTHR","$0.00","This is a comprehensive research project aimed at making perfect or exact sampling a more practical tool for common Bayesian modeling, as well as for engineering and financial applications. Key reasons for the current limitation of exact sampling include the non-applicability of available algorithms because they operate under assumptions that are not present (such as monotonicity or compact spaces) or assume elements that are not available (such as suitable ``bounding chains''), and the fact that many proposed perfect sampling algorithms take too long or too much memory to be practical beyond certain ``stylized'' applications. The investigators propose to take full advantage of specific problem structures arising in common applications to enhance the performance of exact simulation algorithms. More precisely, in the context of Bayesian computations, the investigators study the idea of data augmentation and multi-shift/scaling couplers to implement and to speed up perfect sampling algorithms for a number of common Bayesian models. The investigators also propose a new exact simulation algorithm that is suitable for applications in stochastic modeling (as in the contexts of engineering and finance), particularly for distributions that are solutions of fixed point stochastic equations. In addition, the investigators study a general procedure to implement a regeneration-based exact simulation algorithm. Finally, the investigators analyze related methods, such as ``nearly perfect sampling"", which by allowing a known and controlled error term, can potentially provide considerable gain in terms of both speed and applicability.<br/><br/>Markov chain Monte Carlo (MCMC) is a class of very popular methods for scientific computation, and Perfect Sampling is a subclass of MCMC methods that aim to deliver more accurate results. The price one pays for this better accuracy is that the construction of a Perfect Sampling algorithm is typically a difficult task. The main purpose of this proposal is to study practical strategies for reducing such difficulties and thereby to make Perfect Sampling a more practical tool than currently it is. The research activities on perfect sampling described by the investigators focus on widely used models in statistical inference, production and manufacturing systems and financial econometrics. Therefore, the research plan that the investigators propose can have a substantial impact in a great variety of applications in Statistics, Industrial Engineering and Finance. The proposed research activities will also greatly advance the general knowledge and understanding of the applicability of perfect sampling in practice thereby addressing a key problem in the MCMC methodology.  The proposed activities will have broad impact in both statistical computation practice and theory, via both research and associated teaching and advising due to the direct involvement of student research assistants and via seminars and publications. The investigators will also make every effort possible to recruit the best research assistants who at the same time will also enhance diversity in their general research environment.<br/>"
"0503981","Sequential Monte Carlo Methods for Computationally Intensive Problems","DMS","STATISTICS","09/01/2005","08/18/2005","Yuguo Chen","IL","University of Illinois at Urbana-Champaign","Standard Grant","Gabor Szekely","08/31/2008","$85,000.00","","yuguo@uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","0000, OTHR","$0.00","<br/>Sequential Monte Carlo methods provide a versatile and powerful tool for<br/>solving complex statistical inference problems. The objective of this<br/>proposal is to develop sequential Monte Carlo techniques in three<br/>important areas: conditional inference on multiway tables, likelihood<br/>inference in population genetics, and adaptive control of nonlinear<br/>stochastic systems. A common theme in these applications is computational<br/>complexity.  The investigator develops efficient proposal distributions<br/>and resampling techniques to improve current methods in these three<br/>applications.  New theories arising from these applications shed light on<br/>several fundamental issues related to the implementation of sequential<br/>Monte Carlo methods, in particular, the decomposition of a high<br/>dimensional problem into small components so that each component is easy<br/>to handle and the sequential nature of the problem can be utilized, and<br/>the choice of the proposal distribution so that it is easy to sample and<br/>close enough to the true underlying distribution.<br/><br/>The investigator develops innovative sequential Monte Carlo techniques in<br/>three important areas.  The first area is conditional inference on<br/>multiway contingency tables. Such tables arise very often from social and<br/>medical sciences, including large survey data and grouped case-control<br/>data with several risk factors. The second area is likelihood-based<br/>inference in population genetics, which is motivated by the interest in<br/>inferring key biological characteristics of the major pathogenic serotypes<br/>of Cryptococcus neoformans, an agent of serious respiratory disease in<br/>humans. The third area is on-line identification and adaptive control of<br/>nonlinear stochastic systems. The investigator improves the current methods<br/>used in these three applications and develops a systematic theory that<br/>provides insight into general strategies for applying sequential Monte<br/>Carlo methods.  New theories arising from these applications are of<br/>interest across a broad range of areas in statistics, science and beyond.  <br/>The proposed research has significant impact on education through<br/>involvement of Ph.D. students directly in the proposed research and<br/>incorporation of results into undergraduate and graduate courses.<br/>"
"0504851","Bayesian Process Convolutions for Non-stationary Modeling","DMS","STATISTICS","09/01/2005","08/29/2006","Herbert Lee","CA","University of California-Santa Cruz","Continuing Grant","Gabor Szekely","08/31/2009","$120,000.00","Bruno Sanso","herbie@ams.ucsc.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1269","0000, OTHR","$0.00","                                 Abstract<br/>PROPOSAL NO.: 0504851<br/>INSTITUTION: U of Cal Santa Cruz<br/>NSF PROGRAM: STATISTICS<br/>PRINCIPAL INVESTIGATOR: Lee, Herbert <br/>TITLE: Bayesian Process Convolutions for Non-stationary Modeling<br/>RATING: Very Good<br/><br/>The investigators will apply process convolution modeling techniques<br/>to non-stationary data in new ways.  Convolutions are used both for<br/>their theoretical elegance and their computational efficiency.  The<br/>first focus is on the spatio-temporal setting, with exploration of<br/>different approaches to applying convolutions to the problem.  Both<br/>theoretical and implementational aspects will be addressed.  The<br/>second focus is on partitioned processes, starting in the spatial<br/>setting and moving on to spatio-temporal problems.  Partitioning is a<br/>computationally effective method for dealing with non-stationarity,<br/>and combined with the convolution approach allows for practical<br/>modeling of much larger datasets than can traditionally be analyzed<br/>with a non-stationary model.<br/><br/>This work is on new statistical models for processes that vary over<br/>distance and time, possibly varying in an irregular fashion.<br/>Motivating examples come from environmental science (such as<br/>understanding rainfall and other environmental variables, and<br/>validating computer models for climate simulations) and aeronautical<br/>engineering (modeling of computational fluid dynamics simulators, such<br/>as flight simulators).  Such complex processes can be difficult to<br/>model well, as complicated models can be too difficult to use while<br/>simpler models fail to fit well.  This work involves models which are<br/>sufficiently powerful, yet also practical to use for larger datasets.<br/>Success will impact not just the realm of statistical modeling, but<br/>more importantly will help advance research in the fields of the<br/>applications, in particular environmetrics and aeronautical<br/>engineering.  <br/><br/>"
"0505085","Collaborative Research on Bayesian Nonparametric Methods for Spatial and Spatiotemporal Data","DMS","STATISTICS","08/01/2005","07/21/2005","Athanasios Kottas","CA","University of California-Santa Cruz","Standard Grant","Gabor Szekely","07/31/2008","$72,034.00","","thanos@soe.ucsc.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1269","0000, OTHR","$0.00"," ABSTRACT <br/><br/>Principal Investigators:   Kottas, Athanasios and Gelfand, Alan<br/>Proposal Number: DMS - 0505085 and DMS - 0504953<br/>Proposal Title: Collaborative Research on Bayesian Nonparametric Methods for Spatial and Spatiotemporal Data<br/>Institution: University of California Santa Cruz and Duke University<br/><br/>The investigators develop Bayesian nonparametric methodology for<br/>spatial and spatio-temporal data analysis. Point-referenced spatial<br/>data arises in several fields, including atmospheric science, ecology, <br/>environmental science, and epidemiology. In fact, often such data is <br/>replicated across time say through sampling at monitoring sites. In <br/>certain cases, with appropriate preliminary manipulation, the<br/>replicates may be viewed as independent. More often, the temporal <br/>dependence is retained and, discretizing time, a time series of<br/>spatial processes emerges. In either case, virtually all of the<br/>modeling for the spatial processes is specified parametrically; in fact, <br/>it is almost always a Gaussian process which is most frequently<br/>assumed to be stationary. The investigators study new classes of <br/>nonparametric spatial models to remove these assumptions. These models <br/>are applicable to either of the above replicated settings. In its <br/>simplest form, the investigators use Dirichlet processes to create <br/>random spatial processes, which are non-Gaussian, nonstationary, and <br/>have non-homogeneous variance. These processes are defined through<br/>their finite dimensional distributions, and are referred to as spatial <br/>Dirichlet processes. A spatial Dirichlet process is then convolved<br/>with a pure error process to create an illustrative spatial process<br/>with a nugget component. Such models are hierarchical and can be<br/>fitted through Markov chain Monte Carlo methods. In application, the <br/>investigators use spatial Dirichlet processes to introduce spatial<br/>random effects into the modeling, either directly with independent <br/>replicates or embedded within a dynamic model to handle temporal <br/>dependence. The investigators study an assortment of problems<br/>associated with the use of spatial Dirichlet processes, including<br/>their theoretical global and local properties; their use as mixing<br/>models; their use with semiparametric mixing; their implementation in <br/>dynamic models; their utilization for interpolation at given time<br/>points and for forecasting at future time points; their use with <br/>non-Gaussian first stage specifications for the data; their use in <br/>describing multivariate distributions and, as a special case, for <br/>extended regression modeling; their use in modeling spatial point <br/>process data; and their extension to richer classes of so-called <br/>generalized spatial Dirichlet processes.<br/><br/><br/> <br/>Point-referenced spatial data arises in application areas as diverse <br/>as environmental science, climatology, ecology, epidemiology, and<br/>real estate markets. As researchers collect more and more space and <br/>space-time data, the need for analyses to enhance their understanding <br/>of the complex processes they are sampling grows. This inspires the <br/>need for sufficiently rich models to accommodate a variety of global<br/>and local behaviors. The primary motivation for this research is to <br/>expand the catalog of space-time modeling tools available to such <br/>scientists. This research work suggests the first approach to <br/>nonparametric Bayesian spatial and spatio-temporal data analysis. <br/>Nonparametric Bayesian approaches have witnessed increased utilization <br/>in recent years as a result of their successful application to certain <br/>problems in, for example, engineering and biomedical fields.<br/>Similar success is anticipated by bringing this methodology to <br/>space-time settings. In particular, it is anticipated that, for fields <br/>such as epidemiology, environmental contamination and weather<br/>modeling, researchers will value the flexible modeling framework the <br/>work offers. And, an increase in usage of the methodology is expected <br/>as the computational techniques to fit the models are advanced.<br/><br/><br/><br/><br/><br/><br/><br/><br/>"
"0449204","CAREER: Stochastic Modeling and Inference in Biophysics","DMS","STATISTICS","07/01/2005","07/08/2009","Samuel Kou","MA","Harvard University","Continuing Grant","Gabor Szekely","06/30/2011","$400,001.00","","kou@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","0000, 1045, OTHR","$0.00","Prop ID:    DMS-0449204   <br/>P I:     Kou, Samuel  <br/>Organization:   Harvard University <br/>Program:  Career 2005<br/>Title:    CAREER: Stochastic Modeling and Inference in Biophysics  <br/><br/><br/>                                  Abstract<br/><br/> Advances of experimental and computing technology have profoundly <br/>changed the field of biophysics. On the experimental side, recent developments <br/>in nanotechnology have allowed scientists to follow biological processes on <br/>single-molecule basis, providing scientists with powerful means of studying many <br/>biophysical processes that were inaccessible just a decade ago. This new <br/>frontier also raises significant statistical challenges. It calls upon an <br/>urgent need for stochastic modeling, because many classical models derived <br/>from oversimplified assumption are no longer valid for single-molecule <br/>experiments. Parallel to the experimental progress, the rapid advance of <br/>computing resources and Monte Carlo methods also offers great potential <br/>for biophysical studies, because in most biophysical experiments inference<br/>on the underlying stochastic dynamics is complicated by latent processes, <br/>and many biophysical problems are computing intensive. The proposal <br/>consists of three projects: (A)  Constructing stochastic models that account <br/>for the subdiffusion phenomenon in single-molecule biophysics, where the <br/>goal is to provide models that are not only physically meaningful, but<br/>also capable of explaining the subdiffusion phenomenon that eludes classical <br/>diffusion models. (B) Developing data augmentation tools to handle latent <br/>processes in biophysical experiments, where the goal is to use the data <br/>augmentation techniques to augment the hidden processes so as to <br/>efficiently infer from the experimental data the biophysical properties <br/>of interest. (C) Developing more efficient Monte Carlo method for computing<br/>intensive biophysics problems, where the goal is to construct new Monte <br/>Carlo methods capable of sampling complicated distributions, and apply the <br/>new method to study the HP protein folding problem.<br/><br/><br/> Technology advances have profoundly changed the society and science<br/>in the recent decades. The field of biophysics benefits from these <br/>advances in particular, as the technology advances have allowed scientists <br/>to study many biological processes that were inaccessible just a decade <br/>ago. These advances also raise significant challenges for statistics, <br/>because the unprecedented amount of data brought by the new technology <br/>requires critical statistical analysis. While the United States leads the <br/>world in both statistics and biophysics research, future prominence depends<br/>on innovations in both fields and the interdisciplinary collaborations <br/>between them. The research in this proposal aims at developing new <br/>statistical models and inference tools that can be directly applied to <br/>biophysics studies and will lead to new insight about the physics of complex<br/>biological processes. The development of statistics and biophysics not only<br/>presents many interdisciplinary research opportunities, but also attracts <br/>many bright students in mathematical, biological and physical sciences. <br/>The proposed research is integrated with the principal investigator's <br/>educational activities at both the graduate and undergraduate level.<br/>It seeks to meet high academic standards, while providing graduate <br/>training that will prepare students for interdisciplinary research careers.<br/><br/>"
"0505636","Reproducing Kernel Hilbert Space Methods in Statistical Model Building and Data Analysis","DMS","STATISTICS","08/01/2005","06/29/2005","Grace Wahba","WI","University of Wisconsin-Madison","Standard Grant","Grace Yang","07/31/2007","$54,000.00","","wahba@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","0000, OTHR","$0.00","  Abstract <br/>Wahba, Grace G.<br/>Proposal ID: DMS - 0505636<br/><br/>Original Title: Reproducing Kernel Hilbert Space Methods in Statistical<br/>Model Building and Data Analysis<br/><br/>New Title: Positive Definite Kernel Methods in Statistical Model<br/>Building and Data Analysis<br/><br/><br/>Positive definite functions (a.k.a.""kernels"")  play a key role in<br/>statistical model building, classification, clustering and data mining.  Such kernels provide a distance metric for elements in their domain, which may be functions (as in Reproducing Kernel Hilbert Spaces), or, more recently, trees, graphs, images, sounds, DNA and protein <br/>sequences, microarray gene expression data, text messages and other<br/>objects. A reasonable distance metric is a prerequesite for prediction, classification, and clustering, and methods for obtaining such metrics<br/>are an area of active research. The proposer will introduce, develop and study the properties of a new class of nonparametric methods for obtaining kernels in situations where noisy, crude, incomplete information related to dissimilarity between pairs of objects in arbitrary sets is available. The methods are  called <br/>regularized kernel estimation, since they involve a tradeoff between<br/>fitting the crude information available and a penalty or complexity functional on the kernel, analogous to, but not the same as classical<br/>regularization and the bias-variance tradeoff. Optimal tuning and dimensionality reduction procedures will be proposed and their properties studied. The methods proposed are believed to have new and important computational and theoretical advantages, and these will be<br/>demonstrated, by development of efficient computational algorithms, by <br/>simulation, by development of the theory, and by application to <br/>a variety of scientific problems.<br/><br/>This work is motivated by the goal of obtaining better methods for <br/>clustering and classifying objects mentioned above, by obtaining <br/>improved ways to describe the ""distance"" betweein objects. <br/>For example, microarray gene chips may contain information <br/>concerning, e. g. the type of tumor whose DMA is being studied, <br/>and it is anticipated that this research  will provide<br/>improved methods for extracting this information <br/>in cases where it is difficult to identify the type of tumor, <br/>and this will ultimately result in better diagnostic and treatment<br/>outcomes. Similarly, it is of interest to cluster protein sequences into functional classes, with the goal of identifying function by associating sequences that are ""nearby"". <br/>It is anticipated  that the present research will provide a more efficient way of extracting information from crude or incomplete dissimilarity data and, contribute to the long-term technology of <br/>understanding protein function. Other potential applications include<br/>improved classification of weather states, with the goal of clustering<br/>and  classifying local situations that have similar outcomes, signal<br/>detection in large neutrino detectors, classification of astronomical bodies, and classification and clustering problems in a variety of other scientific fields.<br/><br/>"
"0504726","Haplotype-Based Association Modeling for Whole-Genome Scan and Candidate Gene Studies","DMS","STATISTICS","08/15/2005","07/15/2005","Jung-Ying Tzeng","NC","North Carolina State University","Standard Grant","Gabor Szekely","07/31/2009","$75,000.00","","jytzeng@stat.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>Principal Investigators:   Tzeng, Jung-Ying  <br/>Proposal Number: DMS-0504726  <br/>Institution: North Carolina State University  <br/><br/>Proposal Title: Haplotype-based Association Modeling for Whole-Genome Scan and Candidate Gene Studies    <br/><br/>Identifying genes responsible for human diseases can illuminate <br/>significant insight to the detection, treatment and prevention of these <br/>diseases.  In the search for genes underlying human complex diseases, <br/>haplotype-based association analysis has been recognized as a tool with <br/>high resolution and, more importantly, potentially great power for <br/>identifying modest etiological effects of genes.  However, in practice, <br/>its efficacy has not been as successful as expected in theory; one primary <br/>cause is that such analysis requires a large number of parameters in order <br/>to capture the abundant haplotype varieties. While high degrees of freedom <br/>can hinder the power of identifying modest genetic effects on complex <br/>diseases, the need to incorporate covariates of other risk factors further <br/>worsens the degrees-of-freedom problem in mapping genes for complex <br/>diseases. To tackle this issue, the proposed work constructs an efficient <br/>and powerful model-based framework for association analysis at haplotype <br/>level. The central focus of the methods development is on the efficient <br/>use of haplotype information in a model-based framework, and different <br/>strategies of reducing haplotype complexity are considered at different <br/>research stages to optimize efficiency.  For screening-stage analyses, the <br/>PI constructs approaches based on haplotype similarity of pair-wise <br/>comparison in a regression platform to detect chromosomal regions that are <br/>likely to harbor disease genes.  For refinement-stage analyses, the PI <br/>develops evolutionary-based methods of haplotype grouping to identify <br/>specific disease-associated haplotypes, and integrate new <br/>dimension-reduction techniques into existing regression methods.<br/><br/>The proposed work aims not only to provide novel association tools in <br/>complex gene mapping, but also develop a routine method for case-control <br/>studies and offer a methodological foundation for future advancement. With <br/>the availability of a better statistical tool, scientists can advance <br/>their understanding of complex diseases, and design better diagnostic and <br/>therapeutic strategies that improve human health.<br/><br/><br/><br/>"
"0504269","Semiparametric Models, Methodologies and Related Theory for Analysis of Censored Survival Data","DMS","STATISTICS","07/01/2005","04/12/2007","Wenbin Lu","NC","North Carolina State University","Continuing Grant","Gabor Szekely","12/31/2008","$49,828.00","","lu@stat.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","0000, OTHR","$0.00","   This project addresses issues related to semi-parametric regression models, methods and theories with censored survival data. The investigator and his collaborators aim to develop semi-parametric efficient estimators of the regression parameters for the proportional hazards mixture cure model. The approach is extended to another class of semi-parametric cure models. They propose a general class of mixture transformation models, which are common in medical and econometrics literature, and study them via estimating equations as well as some nonparametric smoothing techniques. In addition, they plan to develop estimating equations for multivariate failure time data based on marginal linear transformation models and propose a class of independence tests for multivariate failure time data with the adjustment of covariates. They also derive relatively simple method for analysis of survival data from case-cohort design and discuss more efficient parameter estimation via the projection method.<br/>   The statistical problems studied here are motivated by applications in biomedical sciences, engineering sciences, sociology, economics and genetics. The project develops appropriate statistical models, inferential methods and mathematical theory. The results can be used to facilitate design of clinical trials and epidemiological studies, particularly in studies of cancer, cardiovascular diseases, to analyze engineering reliability and market penetration data, to assess the association among failure times due to unmeasured effects, such as familial genetic effects, after adjusting some environmental factors. <br/><br/>"
"0504957","Distances in Robust Model Selection","DMS","STATISTICS","07/01/2005","05/10/2007","Marianthi Markatou","NY","Columbia University","Continuing Grant","Gabor Szekely","06/30/2009","$250,001.00","","statdistance@gmail.com","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","0000, OTHR","$0.00","Clean data is prerequisite for most statistical analyses. An ideal solution, when questionable data items arise, is to go back to check the source. However, in many cases this is not possible. Contamination therefore is an important problem, and robust techniques that can handle large data sets are needed to cope with this problem. The role of distances in the problem of robust model selection is examined. It is argued that robust model assessment and selection is an important problem that has not received adequate attention in the literature. It is suggested that distances offer potentially valuable tools for addressing various aspects of the problem of modeling, one of which is the aspect of robustness. A new framework is proposed that differs from the classical robustness paradigm in at least two aspects. Most of the developments in classical robustness center around location-scale models and the concepts therefrom. Attempts to extend classical robust procedures to other non location-scale models were met with limited success. The methodology proposed here incorporates easily a wide variety of models, including location-scale models. The starting point of the new proposal is the identification of a goodness-of-fit measure that provides an assessment of whether a given model approximates the mechanism that generated the data. It is then examined in what sense the measure is robust. <br/><br/>     Distances have been used extensively in many scientific fields such as genetics, physics, sociology, anthropology and more recently in the field of machine learning. The significance of this work is two-fold. Within the scientific field of statistics, a very general framework, that can address the problem of robust model assessment and selection is offered, that can handle large data sets and allows to measure the extend to which the model approximates the phenomenon under study. Outside the field of statistics the technology and scientific results can be extended and applied to address important problems in clinical informatics and bioequivalence.<br/>"
"0529861","Regression with Periodic Series","DMS","STATISTICS","03/01/2005","06/25/2005","Robert Lund","SC","Clemson University","Continuing Grant","Grace Yang","06/30/2007","$98,441.00","","rolund@ucsc.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0304407<br/><br/>PI: Robert B. Lund<br/><br/>This research studies regression methods in time series settings with periodic properties. The general goal of this work is to put statistical inference for regression models with periodic error disturbances on the same footing as that for its stationary brethren. The particular issues examined include (1) simple linear regression for periodic series, (2) analysis of variance methods for periodic series, (3) undocumented changepoint detection in periodic series, (4) the adjustment of periodic series for documented and undocumented changepoint times, and (5) trends in monthly extreme temperatures, and winter snow depths. The work will advance and further connect the statistical areas of time series, forecasting, extreme value analysis, and regression modeling.<br/><br/>On a practical level, the research will help resolve climate change issues within the United States and, more generally, help quantify global warming. The statistical methods developed will be used to study trends in United States monthly extreme (maximum and minimum) and average temperatures. Trends in United States snow cover (over seasonal maximums and snow water equivalent) will also be examined<br/>"
"0505201","Hybrid likelihood methods","DMS","STATISTICS","07/01/2005","06/16/2009","Ian McKeague","NY","Columbia University","Standard Grant","Gabor Szekely","12/31/2009","$180,000.00","","im2131@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","0000, OTHR","$0.00","                                                           ABSTRACT<br/><br/>Prop ID:  DMS-0505201  <br/>PI:  McKeague, Ian W.<br/>NSF Program: STATISTICS    <br/>Institution:  Columbia University  <br/>Title:  Hybrid likelihood methods   <br/><br/>Empirical likelihood is being extended in three directions: to allow <br/>for plug-in estimates of nuisance parameters in estimating equations, <br/>slower than root-n  rates of convergence, and settings<br/>in which there are a relatively large number of estimating equations<br/>compared to the sample size.  This work is motivated in part<br/>by the need to more effectively compare survival distributions in <br/>clinical trials and cohort studies.<br/>Methods of finding confidence intervals for split points, as <br/>represented by jumps in  parametric regression models within<br/>broader semiparametric models, are also being investigated.  This <br/>involves developing cube-root asymptotics for hybrid likelihood <br/>methods. A new  confidence set is constructed by inverting a<br/>hybrid likelihood ratio statistic.  This part of the project is <br/>motivated by an application to the development  of a phosphorus <br/>threshold standard for the Everglades. Finally, hybrid  likelihood  <br/>techniques are being  developed for use in HIV vaccine efficacy trials <br/>in which it is important to take into account dependence of the <br/>relative risk of infection  on the divergence of infecting HIV viruses.<br/><br/>Hybrid likelihood provides a unified way of looking at techniques<br/>that adapt a nonparametric likelihood based approach in some way, for <br/>example through the use of plug-in estimates for nuisance parameters, <br/>or by the use of a likelihood derived from a working (but not ""true"") <br/>model. The use of hybrid likelihood has become increasingly<br/>common in recent years and is attractive in many applied areas because <br/>it combines the power of likelihood based methods with a pragmatic <br/>sense of the need to find  tractable and easily implemented<br/>solutions to complex statistical problems.  The investigator is <br/>extending the scope of hybrid likelihood  methods in a variety of <br/>settings.  Methodological work is conducted on three specific topics:<br/>empirical likelihood with applications in survival analysis,<br/>confidence intervals for split points with application to the  <br/>estimation of pollution thresholds, and comparison of mark-specific <br/>relative risks with application to the  detection of<br/>viral divergence in HIV vaccine efficacy trials.<br/><br/>"
"0505528","Quotient Correlation, Nonlinear Dependence, and Extreme Dependence Modeling","DMS","STATISTICS","07/01/2005","05/16/2005","Zhengjun Zhang","MO","Washington University","Continuing Grant","Rong Chen","05/31/2006","$32,727.00","","zjz@stat.wisc.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","MPS","1269","0000, OTHR","$0.00","The proposal consists of five important steps. First, the<br/>investigator develops new dependence measures (quotient correlation)<br/>and analyze the resulting test statistics. It is important to<br/>develop test statistics for tail independence based as much as<br/>possible on appropriate tail observations and less on observations<br/>from the center which may bias the analysis. Clearly, the decision<br/>on ""where does appropriate"" stands has to be carefully analyzed.<br/>Second, a new measure for extreme co-movement is introduced. This<br/>measure allows one to calculate probabilities of occurrences of<br/>certain extreme events in the future given the past history of the<br/>extreme movement of underlying variables. Third, the proposal<br/>includes the development of statistical estimation methods for<br/>max-stable processes. This allows one to efficiently study clustered<br/>spatial-temporal extreme observations. Fourth, procedures of how to<br/>calculate portfolio risk measures -- such as unconditional or<br/>conditional Value at Risk -- are also introduced using combined<br/>Markov process and max-stable process models. Fifth, the statistical<br/>analysis of extreme co-movements is of considerable importance in<br/>practice. For example, the notion of spillover in global financial<br/>markets and credit loss data can be thought of as one area of<br/>application.<br/><br/>The intellectual merit of the proposal in a first instance stems<br/>from a precise testing procedure for extremal dependence. As all<br/>definitions used depend on some limit procedures and often concern<br/>statistical testing for parameters at the edge of a specific<br/>parametric space (hence possibly testing is a non-regular estimation<br/>problem), great care has to be taken to obtain tests with sufficient<br/>power. The proposal is exactly aiming at finding a solution for<br/>this. Several procedures for tackling this problem have been<br/>published, however, up to now, no clear winning approach seems to<br/>exist. The investigator will carefully compare and contrast new<br/>approaches with existing ones using simulated as well as real data.<br/>The real data will come from areas as diverse as insurance, finance,<br/>telecommunications, climatology, seismology, medicine, etc. Beyond<br/>these methodological merits and specific applications, the proposal<br/>also has a considerable broad impact. Throughout applications in<br/>diverse fields (like above), extreme risks play an important<br/>scientific, societal as well as (possibly) political role. The<br/>dissemination of new statistical tools leading to a better<br/>understanding of the occurrence of joint extremes is of great<br/>importance. This can be very well achieved at the level of new<br/>graduate courses, publications in journals aimed at a broaden<br/>audience and in discussion with scientists from other fields.<br/>"
"0505651","Measures of Dependence and Model Selection in Multiple Regression","DMS","STATISTICS","07/01/2005","09/12/2006","Kjell Doksum","WI","University of Wisconsin-Madison","Standard Grant","Grace Yang","06/30/2007","$50,000.00","","doksum@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","0000, OTHR","$0.00"," Measures of Dependence and Model Selection in Multiple Regression <br/><br/>                   Kjell A Doksum,  DMS-0505651<br/> <br/>                            Abstract<br/><br/>The investigators approach the dilemma that estimation of curves and<br/>surfaces such as the conditional mean is virtually impossible with high<br/>dimensional data by focusing instead on the estimation of measures of<br/>dependence between a response and a set of covariates. These measures of<br/>dependence take the form of signal divided by noise. They are used to<br/>select the subset of covariates to include in the model, and to choose<br/>tuning parameters. The signal measures the strength of the dependence of Y<br/>on a set of covariates. The noise is a standard error, that is, an<br/>estimate of the standard deviation of the estimated signal. It will be<br/>small when too many variables are included in the model and when the<br/>tuning parameter sacrifices precision for smaller bias. Choosing variables<br/>and tuning parameters that minimize signal to noise leads to procedures<br/>that converge at the traditional root-n rate. The investigators use<br/>asymptotic and Monte Carlo methods to investigate the properties of such<br/>procedures. They also relate them to traditional model and tuning<br/>parameter selection procedures and compare traditional procedures with the<br/>signal to noise approach.<br/><br/><br/>The last few years have seen the establishment of large databases'<br/>containing a large number of variables that are to be related and<br/>compared. A good example is the data produced by the human genome project<br/>where a great number of genes need to be considered as possible<br/>contributers to a certain disease. Dealing with a large number of<br/>variables is difficult because many of them will contribute variability<br/>(noise) that may drown out potential interesting relationships (signals).<br/>The investigators approach this problem by using general flexible model<br/>equations to represent important relationships between variables. Then<br/>variables and aspects of the equations are selected to minimize the ratio<br/>of signal to noise. This procedure automatically weeds out the variables<br/>that contribute mostly noise and selects an equation that emphasizes the<br/>signals present in the data.<br/>"
"0536938","Winter Workshop on Frontiers of Theoretical Statistics","DMS","STATISTICS","12/01/2005","11/16/2005","A. Khuri","FL","University of Florida","Standard Grant","Grace Yang","11/30/2006","$10,000.00","George Casella, Malay Ghosh, Adao Trindade, Clyde Schoolfield","ufakhuri@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","0000, OTHR","$0.00","Abstract:<br/><br/>The workshop ""Frontiers of Theoretical Statistics"" is dedicated to <br/>exploring the most recent developments in the theory of statistics. <br/>These include saddlepoint approximation, generalized linear mixed <br/>models, bootstrap techniques, Markov chain Monte Carlo methods, Bayesian <br/>    inference, and combinatorial optimization, among others. The <br/>workshop will provide an assessment of the current state of knowledge of <br/>theoretical statistics, identify the most pressing needs for future <br/>developments, and foster cooperation among research workers. The 12 <br/>invited workshop speakers are well-known leaders in theoretical <br/>statistics. They include Jim Berger (Duke University), Peter Bickel <br/>(University of California at Berkeley), Ronald Butler (Colorado State <br/>University), R. Dennis Cook (University of Minnesota), Jim Fill (Johns <br/>Hopkins University), Peter McCullagh (University of Chicago), Susan <br/>Murphy (University of Michigan), Nancy Reid (University of Toronto), <br/>Christian Robert (Ceremade University, Paris, France), Michael Steele <br/>(University of Pennsylvania), Steven Stigler (University of Chicago), <br/>and Bin Yu (University of California at Berkeley). The workshop will be <br/>hosted by the University of Florida Department of Statistics, and will <br/>be held at the J. Wayne Reitz Union of the University of Florida during<br/>the period January 13-14, 2006.<br/><br/>Theoretical statistics is the foundation upon which all statistical <br/>methodologies and applications are based. Many recent advances in <br/>science, such as in biological and medical sciences, physical and <br/>engineering sciences, are intimately connected to advanced statistical <br/>and computational procedures. Some of the goals of the workshop are:<br/>(a) To identify some of the major scientific questions and challenges in <br/>the field of theoretical statistics that can only be studied through <br/>interactive research. Focusing on such problems and trying to answer <br/>them will contribute to science and humanity in a significant way.<br/>(b) To provide support to new researchers by bringing them in contact <br/>with experienced statisticians. The 12 invited senior speakers are <br/>leading authorities on many aspects of theoretical statistics. Their <br/>participation in the workshop will undoubtedly help in inspiring and <br/>motivating the young researchers.<br/>(c) To provide a platform for new collaborations on theoretical <br/>statistics and its many related areas.<br/>(d) Given the growing interest in finding immediate and direct <br/>applications of theoretical statistics, the workshop will provide an <br/>impetus to permeate new ideas in this direction.<br/>"
"0527798","Summer Research Conference on Statistics","DMS","STATISTICS","05/01/2005","04/15/2005","Herman Senter","SC","Clemson University","Standard Grant","Grace Yang","04/30/2006","$10,000.00","Robert Lund","hfsnt@clemson.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","MPS","1269","0000, 7556, 9150, OTHR","$0.00","The Southern Regional Council on Statistics (SRCOS) together with the American Statistical Association is holding a Summer Research Conference (SRC) in Statistics during June 5-8, 2005, on the Clemson University campus in Clemson, South Carolina. The conference objective is to bring together statistics researchers of all academic levels  in a relaxed and stimulating atmosphere. The program includes five technical sessions that focus on major research areas in statistics: Time Series, Spatial Statistics, Survival Analysis and Reliability Theory, Markov Chains, and Bootstrap and Computational Statistics. Two additional sessions deal with issues in Statistics education, one focusing on trends and directions, the other on emerging pedagogical tools and technologies. A poster session is open for graduate students and other attendees to present their research. The sessions of the conference are not concurrent. Those attending the conference will be exposed to a wide range of topics of general and specific interest and to a range of speakers (20) bringing different perspectives to the material.  <br/><br/>The research topic areas are broad, with applications of the methodologies in a spectrum of fields (such as medicine, education, business, etc.) The individual 90-minutes sessions, featuring a senior speaker and two junior ones, are structured to provide in-depth coverage of the topic, from an overview by the senior speaker to specific, current research problems of the junior researchers. Advanced graduate students, postdoctoral fellows, and junior researchers are encouraged to attend and present in the judged poster session. Support to offset travel expenses for students and junior researchers by NSF is gratefully acknowledged."
"0630210","Quotient Correlation, Nonlinear Dependence, and Extreme Dependence Modeling","DMS","STATISTICS","08/31/2005","06/25/2007","Zhengjun Zhang","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","06/30/2008","$73,423.00","","zjz@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","0000, OTHR","$0.00","The proposal consists of five important steps. First, the<br/>investigator develops new dependence measures (quotient correlation)<br/>and analyze the resulting test statistics. It is important to<br/>develop test statistics for tail independence based as much as<br/>possible on appropriate tail observations and less on observations<br/>from the center which may bias the analysis. Clearly, the decision<br/>on ""where does appropriate"" stands has to be carefully analyzed.<br/>Second, a new measure for extreme co-movement is introduced. This<br/>measure allows one to calculate probabilities of occurrences of<br/>certain extreme events in the future given the past history of the<br/>extreme movement of underlying variables. Third, the proposal<br/>includes the development of statistical estimation methods for<br/>max-stable processes. This allows one to efficiently study clustered<br/>spatial-temporal extreme observations. Fourth, procedures of how to<br/>calculate portfolio risk measures -- such as unconditional or<br/>conditional Value at Risk -- are also introduced using combined<br/>Markov process and max-stable process models. Fifth, the statistical<br/>analysis of extreme co-movements is of considerable importance in<br/>practice. For example, the notion of spillover in global financial<br/>markets and credit loss data can be thought of as one area of<br/>application.<br/><br/>The intellectual merit of the proposal in a first instance stems<br/>from a precise testing procedure for extremal dependence. As all<br/>definitions used depend on some limit procedures and often concern<br/>statistical testing for parameters at the edge of a specific<br/>parametric space (hence possibly testing is a non-regular estimation<br/>problem), great care has to be taken to obtain tests with sufficient<br/>power. The proposal is exactly aiming at finding a solution for<br/>this. Several procedures for tackling this problem have been<br/>published, however, up to now, no clear winning approach seems to<br/>exist. The investigator will carefully compare and contrast new<br/>approaches with existing ones using simulated as well as real data.<br/>The real data will come from areas as diverse as insurance, finance,<br/>telecommunications, climatology, seismology, medicine, etc. Beyond<br/>these methodological merits and specific applications, the proposal<br/>also has a considerable broad impact. Throughout applications in<br/>diverse fields (like above), extreme risks play an important<br/>scientific, societal as well as (possibly) political role. The<br/>dissemination of new statistical tools leading to a better<br/>understanding of the occurrence of joint extremes is of great<br/>importance. This can be very well achieved at the level of new<br/>graduate courses, publications in journals aimed at a broaden<br/>audience and in discussion with scientists from other fields.<br/>"
"0505423","New Tools for Sparse Inference in Large-scale Multiple Comparisons","DMS","STATISTICS","07/15/2005","07/05/2005","Jiashun Jin","IN","Purdue University","Standard Grant","Gabor Szekely","06/30/2008","$90,000.00","","jiashun@stat.cmu.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","0000, 1269, OTHR","$0.00","ABSTRACT<br/><br/>Principal Investigators:   Jin, Jiashun  <br/>Proposal Number: DMS - 0505423<br/>Proposal Title: New Tools for Sparse Inference in Large-scale Multiple Comparisons   <br/>Institution: Purdue University  <br/> <br/>A research effort is proposed to create new tools for large-scale multiple comparisons.  Work in this field has been concentrated on idealized models such as the standard Gaussian model, what has not been addressed is the potential of many other models which have more realism and impact. In this proposal, the investigator studies problems in three areas: (a). Formulation of massive data -- Develop models which have <br/>Scientific realism and impact, as well as mathematical simplicity such that careful study is possible. (b).  Development of  new tools --  By exploring a wide variety of models, expose new phenomena and develop tools which are easy-to-implement and theoretically sound. (c). Delicate asymptotic study --  Lay out framework for asymptotic study, carefully compare the existing and newly proposed inference tools, study on the optimality of such tools.<br/><br/>The motivation of this project lies in that,  massive datasets produced in scientific areas such as Genomics, astronomy, and image processing lead to a new field in statistics: large-scale simultaneous hypothesis testing or multiple comparisons. The vision is  advances in this new field will enable the scientists from various scientific fields to quickly extract the information they need from massive datasets, and it is the immediate <br/>interest of the statistics community to develop easy-to-implement tools. This project pushes the boundary of the field by developing new tools and novel theories, as well as exposing new phenomena. The project produces tools which are theoretically sounding and practically feasible for solving problems in areas such as Genomics, astronomy, and image processing.  <br/>"
"0504737","Problems Related to Gaussian Processes","DMS","PROBABILITY, STATISTICS","06/01/2005","03/07/2007","Jan Hannig","CO","Colorado State University","Continuing Grant","Tomek Bartoszynski","12/31/2008","$96,000.00","","jan.hannig@unc.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1263, 1269","0000, OTHR","$0.00","Abstract:<br/><br/>The investigator studies research problems in theoretical probability. In particular he investigates the problem called small deviations. The PI is also working on extreme value theory for Gaussian random fields motivated by an application to nonparametric statistics. The problem of small deviations is related to the behavior of the probability that a stochastic process stays in a small neighborhood of the origin. As the neighborhood shrinks, this probability clearly tends to zero --- the question is at what rate? The PI calculates this rate for various processes, including the storage process that is used to model the amount of water available in a dam. Motivation of the other problem comes from nonparametric statistics. One of the main problem in modern nonparametric smoothing is concerned with finding the best smooth curve approximating the data. Chaudhuri & Marron (1999) proposed a tool for data exploration called SiZer. The visual display of SiZer, can be viewed as a summary of a large number (hundreds) of hypothesis test results. For reasonable statistical inference, care needs to be taken about the multiple comparison issue. The current implementations of SiZer is not addressing the issue of multiple testing adequately. To correct this one needs to study the distribution of the maximum of a particular discrete nonstationary Gaussian random field. <br/><br/>The PI works on two major problems, small deviations for stochastic processes and extreme value theory for a particular random field. The area of small deviations is relatively new with major developments starting in the 1990's. The solution of the small deviation problems helps us understand the nature of certain rare events when the variability of a random process is much less then expected. Applications of this theory are currently pursued by many researchers. The investigator develops a new application of small deviation techniques in storage theory. The proposed extreme value problem is directly motivated by a statistical application. It aims at improving the performance of the data analysis tool SiZer. SiZer is currently widely used by many applied scientists. Successful applications of SiZer include internet traffic modeling, climatology and environment, and biology and genetics. Part of this proposal aims at significantly improving the SiZer tool to make it more reliable for applications. <br/><br/><br/>"
"0505556","Studies in Fractional Factorial Design","DMS","STATISTICS","09/15/2005","05/16/2005","Ching-Shui Cheng","CA","University of California-Berkeley","Standard Grant","Gabor Szekely","08/31/2009","$169,000.00","","cheng@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","The investigator studies several problems in the theory of fractional <br/>factorial design. Some recent results in finite projective geometry <br/>provide powerful tools for characterizing the structures of regular <br/>fractional factorial designs of resolution IV in certain important cases. <br/>One major research activity is to expand these tools and apply them to <br/>develop a comprehensive theory for the determination and construction of <br/>optimal regular fractional factorial designs of resolution IV under the <br/>criterion of minimum aberration. The results are further extended to <br/>nonregular designs and the situation where the experimental units are <br/>divided into more homogeneous blocks to improve precision. The research on <br/>nonregular designs provides new methods for constructing orthogonal arrays <br/>of strength three. Hidden projection properties of multi-level designs are <br/>also investigated. Under the assumption of effect sparsity, a design with <br/>good projections onto small subsets of factors can provide useful <br/>information after the small number of active factors have been identified.<br/><br/>Experimental design is used extensively in a wide range of scientific and <br/>industrial investigations. In industrial experiments, often a large number <br/>of factors have to be studied, but the experiments are expensive to <br/>conduct. In this case, only a small fraction of all the possible <br/>combinations can be observed, and how to choose a good fraction is an <br/>important issue. In recent years, such fractional factorial designs have <br/>received considerable attention, mainly due to the success in applying <br/>them to conduct experiments for improving quality and productivity in <br/>industrial manufacturing. This research is to study the construction of <br/>efficient designs to extract more information. Experimenters will be <br/>benefited by having a rich source of new and good designs, and will be <br/>able to run their experiments more efficiently. Better industrial <br/>experiments can improve the quality of products and reduce production <br/>cost.<br/><br/>"
"0504162","Random Processes: Data Analysis and Theory","DMS","STATISTICS","06/01/2005","04/03/2007","David Brillinger","CA","University of California-Berkeley","Continuing grant","Gabor Szekely","05/31/2009","$270,000.00","","brill@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","Random process data analysis has become a major theme of contemporary science. Motivated by problems from wildlife biology and forestry, techniques for the statistical handling of spatial-temporal point process data and of curved trajectories in the plane are developed as well as statistical methods for random process data generally. In particular probabilists have discovered various theoretical results concerning vector-valued stochastic differential equations, but their statistical and data analytic properties have not been totally developed. Stochastic differential equations are employed to describe the observed trajectories of the animals in the reserve. The equations are unusual in having paths of explanatories included and sometimes time lags. The fact that an animal's motion is bounded by a high fence also affects the analysis. In the work approximations to the random model are needed. The large sample accuracy of the approximations will be studied. The tools of point processes, smoothing and time series analysis are being employed with the wildfire data. Predictors of future risk and possible loss as the fire season proceeds and for future years are being developed.<br/><br/>Two specific problems addressed are of broad impact and of societal importance. The problems are risk estimation for wildfires and the investigation of the effects that humans moving through an animal reserve have on the behavior of the animals. A question in the latter case is with what level of human usage can wild animals and humans share a habitat. There are data available from designed experiments with humans walking, riding horses, bicycling and on all-terrain vehicles traveling in the reserve. Both problems are studied using data sets come from the Forest Service, U. S. Department of Agriculture. This work is of particularly broad impact for there are the possibility of reducing human threat from fire, and of maintaining natural resource values. Predictions of the risk as a function of time and space will allow efficient placement of fire fighting resources.<br/>The intellectual merit of the work includes that quite a variety of interesting analytic problems arise, motivated by these applications, and these problems will be addressed in the research. The data sets are large so it is anticipated that real progress will be made on understanding the applicability of the methods.<br/><br/>"
"0503648","Exact and Approximate Markov Chain Sampling Algorithms","DMS","STATISTICS","06/01/2005","05/04/2009","James Hobert","FL","University of Florida","Standard Grant","Gabor Szekely","05/31/2010","$89,809.00","","jhobert@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","0000, OTHR","$0.00","                             ABSTRACT<br/><br/>Prop ID:  DMS-0503648  <br/>PI:  Hobert, James P.  <br/>NSF Program: STATISTICS    <br/>Institution:  University of Florida  <br/>Title:  Exact and Approximate Markov Chain Sampling Algorithms   <br/><br/>The disadvantages of Markov chain Monte Carlo (MCMC) methods (relative<br/>to classical Monte Carlo techniques) can sometimes be overcome via<br/>perfect sampling, which is a method of converting the underlying<br/>Markov chain into an algorithm that produces independent and<br/>identically distributed samples from its stationary distribution.<br/>Unfortunately, perfect sampling algorithms have not lived up to their<br/>expectations for handling intractable distributions whose support is<br/>unbounded and continuous.  The investigator develops a wide-ranging<br/>perfect sampling algorithm that is particularly well-suited to such<br/>distributions.  The basic idea is to exploit a mixture representation<br/>of the a stationary distribution.  The investigator also studies<br/>techniques for Bayesian analysis of the market model, which is a<br/>multivariate regression model used by financial economists to analyze<br/>data on asset returns.  Empirical evidence and theoretical arguments<br/>suggest that the errors in this model should be from a heavy-tailed,<br/>elliptically symmetric distribution.  However, the Bayesian<br/>methodology that has been developed for making inferences via the<br/>market model is based on the assumption of multivariate normal errors.<br/>This disconnect is presumably due to the fact that the posterior<br/>distributions corresponding to the heavy-tailed alternatives are<br/>highly intractable.  The PI develops and analyzes Monte Carlo and MCMC<br/>methods for exploring such posterior distributions.<br/><br/>Bayesian statistical methods of analyzing data often require the user<br/>to deal with intractable probability distributions.  The investigator<br/>develops sampling algorithms that facilitate the use of such Bayesian<br/>methods.  These algorithms enhance infrastructure for research and<br/>education by providing scientists from many different disciplines with<br/>better techniques for making inferences from their data.<br/>----------------------------------------------------------------------<br/><br/><br/>"
"0505561","Topics in Dimensionality Reduction in Nonparametric Statistical Modelling","DMS","STATISTICS","07/01/2005","06/07/2005","Alexander Samarov","MA","Massachusetts Institute of Technology","Standard Grant","Gabor J. Szekely","06/30/2009","$110,001.00","","samarov@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1269","0000, OTHR","$0.00","The investigator plans to conduct research on identifying structure in multivariate data without imposing rigid structural assumptions and on the development of structural adaptation techniques for dimensionality reduction in nonparametric regression and density estimation.  Building on recent advances in nonparametric and semiparametric estimation, it is proposed to develop iterative algorithms which alternate between identification of the lower dimensional structure, using estimates of average derivative functionals, and model estimation improved by using the current structural information.  Identification and estimation of linear and nonlinear components in partially linear regression models and of independent component analysis model with unspecified component densities will be considered.<br/><br/><br/>With the dramatic increase in large, complex data bases and in computer power, it has become increasingly more desirable and possible to develop nonparametric models, concepts, and procedures that can be used to study relationships between variables and to construct models without relying on rigid parametric assumptions on the structure of mean responses and error distributions. Algorithms developed in this research will provide new statistical learning tools for reducing dimensionality of multivariate data in order to identify and visualize its structure.  After the algorithms are investigated on simulated data and their parameters are fine-tuned, they will be applied to statistical learning problems from bioinformatics, risk management, and other areas. <br/>"
"0504859","Nonparametric Location and Scatter Functionals","DMS","STATISTICS","07/01/2005","04/10/2007","Richard Dudley","MA","Massachusetts Institute of Technology","Continuing grant","Gabor J. Szekely","06/30/2009","$173,000.00","","rmd@math.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1269","0000, OTHR","$0.00","<br/>Abstract: Nonparametric location and scatter functionals<br/><br/>Given n independent observations, each with distribution P, one<br/>can form an empirical measure by taking the average of point masses<br/>at the observations. For a functional T  of P, evaluating T at the<br/>empirical measure gives an estimator of T(P) which will converge<br/>to it almost surely if T is suitably continuous at P. The project <br/>will continue a search for functionals continuous at as many P's as <br/>possible, which may be all P in one dimension, but in higher <br/>dimensions an open dense set. Then if T is also differentiable at <br/>P in a sufficient sense, the estimators will be asymptotically<br/>normal and converge to T(P) at a rate of 1 over square root of n.<br/>Functionals of location include the mean mu when it is finite. <br/>In one dimension, another location functional is the median m,  <br/>and scale functionals include the standard deviation sigma and <br/>median absolute deviation. The classical mu and sigma can be <br/>undefined or infinite at laws decreasing too slowly at infinity. <br/>For some laws P there is an interval of medians, whose midpoint <br/>gives a unique choice of m, but m is discontinuous at such P. <br/>Simultaneous maximum likelihood estimation of location and scale <br/>for t distributions with degrees of freedom nu larger than 1 extends <br/>to location and scale functionals defined and continuous at every <br/>probability distribution on the line, and infinitely differentiable <br/>at distributions having no atom as large as nu over nu plus one.<br/>On multidimensional spaces, the square of a scale parameter<br/>is replaced by a scatter matrix analogous to a covariance<br/>matrix, and the continuity and differentiability will be sought on<br/>a dense open set of distributions, via t functionals and by other<br/>methods.<br/><br/>Location and scale or scatter functionals are some of the most basic <br/>in statistics. But by far the two best known functionals of location, <br/>the mean and median, each have serious drawbacks. Using the mean, one<br/>assumes in effect that all data are correct. The mean can be overly <br/>influenced by outlying, extreme observations such as gross errors. <br/>Using the median, on the other hand, guards against the possibility<br/>that nearly half the data may be incorrect. If a distribution <br/>has more than half its probability in an interval, the median must be <br/>in that interval and may very poorly represent the rest of the distribution. <br/>Functionals combining some of the advantages of the mean and median while<br/>avoiding the worst drawbacks of either can and should be more widely studied <br/>and used. Then one can have kinds of averages that guard against a more<br/>realistic possibility that some small fraction of the data may be<br/>incorrect.<br/>"
"0505537","Nonparametric Methods for Functional Data","DMS","STATISTICS","09/01/2005","06/14/2007","Hans-Georg Mueller","CA","University of California-Davis","Continuing grant","Gabor J. Szekely","08/31/2008","$100,933.00","","hgmueller@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","0000, OTHR","$0.00","<br/>The investigator will combine theoretical analysis and applied tools to<br/>develop innovative models, methodology, algorithms and data analysis for<br/>functional data. A major goal of this proposal is to extend the reach of<br/>functional data analysis methods to sparse, noisy and irregularly sampled<br/>longitudinal data. Functional data and longitudinal data are assumed to be<br/>generated by random trajectories that correspond to realizations of an<br/>underlying stochastic process. These trajectories may be either observable<br/>or hidden. Relevant theory for estimation and inference will be developed.<br/>Current functional data analysis approaches and tools require densely<br/>measured functions or completely observed functions on a common domain,<br/>and abandoning these assumptions requires new tools. Similarly, extensions<br/>of functional data analysis to situations where one has both longitudinal<br/>and survival time components will lead to new practical statistical<br/>methods as well as challenging research problems. The development and<br/>application of new tools for curve warping in cases where a large fraction<br/>of the variability of random curves lies in variation of time scale is<br/>another challenging problem that will be addressed.<br/><br/><br/>The statistical methods that will be developed by the investigator are<br/>motivated by the need to analyze data that include large samples of gene<br/>expression profiles, longitudinal studies in medicine and biology with<br/>multivariate or generalized measurements, and time courses of behavioral<br/>measurements, coupled with age-at-death information. It is expected that<br/>the results of the proposed research will contribute in a significant way<br/>to the analysis and interpretation of such data and will lead to new<br/>insights into underlying mechanisms of gene regulation, complex dynamic<br/>interactions in biological systems, and aging and longevity."
"0505728","Efficient Large Fractional Factorial Designs:   Theory and Construction","DMS","STATISTICS","07/01/2005","04/29/2005","Hongquan Xu","CA","University of California-Los Angeles","Standard Grant","Gabor J. Szekely","06/30/2009","$89,999.00","","hqxu@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","0000, OTHR","$0.00","Fractional factorial designs are among the most widely used experimental<br/>plans in practice.  Most existing researches are limited to small<br/>fractional factorial designs. The objectives of this proposed research are<br/>to develop a general theory and to construct efficient large fractional<br/>factorial designs for practical use.  The concept of moment projection<br/>pattern is introduced to study design isomorphism. Linear programming<br/>technique is used to study design optimality. Efficient algorithms are<br/>proposed for constructing both regular and nonregular designs.  The<br/>concept of minimum moment aberration is utilized to study optimal blocking<br/>schemes for both regular and nonregular designs.  Coding theory is<br/>employed to develop a general theory on minimum aberration blocking<br/>schemes. Catalogs of efficient large fractional factorial designs are<br/>built for practical use.<br/><br/>Experimental design is an effective and commonly used tool in scientific<br/>investigation. Fractional factorial designs are among the most widely used<br/>experimental plans in practice.  Most existing researches are limited to<br/>small fractional factorial designs. Progresses in science and technology<br/>urge the study of efficient fractional factorial designs with both large<br/>run sizes and a large number of factors. Some recent examples are computer<br/>experiments in large scale simulation, high throughput screening in drug<br/>discovery, and microarray experiments in biotechnology.  The objectives of<br/>this proposed research are to develop a general theory and to construct<br/>efficient large fractional factorial designs for practical use. <br/>Addressing the fundamental issue of design construction and selection, the<br/>results of the proposed research can be quickly assimilated into graduate<br/>courses on experimental design.  New algorithms and efficient designs will<br/>be made available online for broad and quick dissemination.  The proposed<br/>research has wide-ranging impact both theoretically and practically, and<br/>will lead to remarkable new advances in design theory and better practice<br/>in experimentation.<br/><br/>"
"0505584","Multivariate Nonparametric Methodology Studies","DMS","STATISTICS","08/01/2005","07/12/2007","David Scott","TX","William Marsh Rice University","Continuing grant","Gabor J. Szekely","07/31/2009","$280,000.00","Dennis Cox","scottdw@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","0000, OTHR","$0.00","The demands on statistical methodology have grown relentlessly<br/>as new technologies for data collection appear.  Many of<br/>these datasets are unusual by statistical standards:<br/>they are massive;  they are highly nonlinear;  they are<br/>contaminated;  they contain data which are in fact functions;<br/>or the data come from a mechanism which is only partially known.<br/>The tasks of estimation, testing, functional testing, pattern<br/>discovery, feature extraction, visualization, and comparison<br/>require the statistician look at each problem anew.<br/>Nonparametric methodology, which has been widely used in one<br/>and two dimensions, is also appropriate in these higher dimensions.<br/>Particular emphasis will be given to multivariate regression and<br/>density estimation problems, and closely related applications such<br/>as clustering, mixture estimation, pattern recognition, robust<br/>estimation, and dimension reduction.  The statistician's view of<br/>the scientific method is a continuously improving process of model<br/>building, data collection, estimation, criticism, and refinement.<br/>However, many practicing statisticians are stymied by an inability<br/>to repair poorly fitting models. Of particular interest in this<br/>research are methods which provide critical diagnostic information<br/>as part of the model estimation task.  A focus of this research is<br/>a relatively new minimum-distance data-based parametric estimation<br/>algorithm, which has been investigated for its robustness properties.<br/>The algorithm can be applied to mixture models and spline fitting.<br/>An incomplete density model may be fitted, a highly unusual<br/>capability that will be explored fully in the context of regression,<br/>image processing, clustering, outlier detection, and density<br/>estimation.  Other novel potential applications include adaptive<br/>wavelet thresholding, solution of the mixture of regression problems,<br/>and application to models which apply to only a subset of the data.<br/>capability that will be explored fully in the context of regression, image <br/>processing, clustering, outlier detection, and density estimation.  Other <br/>novel potential applications include adaptive wavelet thresholding, <br/>solution of the mixture of regression problems, and application to <br/>models which apply to only a subset of the data.<br/><br/>Research in data analysis and statistical modeling provides<br/>intellectual challenges with deep applications in almost every<br/>field of natural and social sciences and engineering. The field of<br/>nonparametric statistics has made a significant contribution to<br/>the success of science with algorithms that are hidden but critical<br/>even in the inner workings of cell phones.  At a recent National<br/>Research Council workshop, numerous scientists identified<br/>critical statistical needs in their work with massive data sets:<br/>new dimension reduction algorithms, specialized visualization tools<br/>for exploring massive data, better clustering algorithms, and<br/>techniques for handling nonstationary data.  Results from this proposed<br/>research directly impact three of these four critical opportunities.<br/>This program represents a comprehensive and long-term attack<br/>on a host of important data analytic problems in multivariate estimation. <br/>Graduate training is significant component of this project.  The results<br/> will be of long-term theoretical interest and will provide short-term <br/>solutions to real-world problems.<br/>"
"0505519","Using the Partitioning Principle to Control the Number of Incorrect Inferences","DMS","STATISTICS","07/01/2005","03/30/2007","Jason Hsu","OH","Ohio State University Research Foundation -DO NOT USE","Continuing grant","Gabor J. Szekely","06/30/2009","$90,992.00","","jch@stat.osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","The investigator develops methods that control the <br/>multiple comparison error rate called generalized <br/>Familywise Error Rate (gFWER).  Whereas control of <br/>the traditional Familywise Error Rate (FWER) <br/>guarantees with a high probability that no <br/>incorrect inference in multiple comparison will be <br/>made, control of gFWER guarantees with a high <br/>probability that the number of incorrect inferences <br/>in multiple comparison will be kept to no more than <br/>a fixed number m.  Thus control of gFWER is less <br/>stringent than control of FWER, if m is greater <br/>than zero.  Controlling gFWER may be more <br/>appropriate in some bioinformatics situations than <br/>controlling the False Discovery Rate (FDR), which <br/>is the expected proportion of false discoveries.  <br/>The investigator uses the generalized Partitioning <br/>Principle to develop modeling-based methods <br/>controlling gFWE.  The generalized Partitioning <br/>Principle makes specific use of the joint <br/>distribution of test statistics to derived methods <br/>more powerful than methods that ignore such <br/>information.<br/><br/>The new methodologies the investigator develops can <br/>have a broad impact in the area of bioinformatics.  <br/>For example, microarrays probing all human genes <br/>can discover which genes are differentially <br/>expressed in a disease situation.  As a follow-up <br/>study, SNP (single nucleotide polymorphism) chips <br/>can be used to confirm which genes have mutations <br/>causing increased susceptibility to the disease.  <br/>In such situations, the cost associated with <br/>falsely rejecting true null hypotheses (of non-<br/>differential expression of genes) is roughly <br/>proportional to the number of incorrectly rejected <br/>true null hypotheses.  Controlling gFWER controls <br/>such costs.  In collaboration with genomic <br/>researchers, the investigator is developing and <br/>testing his gFWER-controlling multiple comparison <br/>methods for microarray data analysis.<br/><br/><br/><br/><br/><br/>"
"0505747","Wavelet estimation of long-range dependent processes","DMS","STATISTICS","06/01/2005","06/14/2005","Murad Taqqu","MA","Trustees of Boston University","Standard Grant","Gabor J. Szekely","05/31/2009","$106,000.00","","murad@math.bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","MPS","1269","0000, OTHR","$0.00","Time series with long-range dependence have been recently the focus of<br/>much attention. They appear in a number of applications, for example,<br/>in the analysis of traffic in computer networks. In finite variance<br/>time series, long-range dependence is characterized by a covariance<br/>function which decreases slowly to 0 as the lag increases. The<br/>decrease is so slow that the corresponding spectral density blows up<br/>at very low frequencies, a phenomenon also known as ''long-range<br/>dependence'', ``long memory'' or ``1/f noise''. There are many<br/>empirical procedures which are graphical in nature but rigorous<br/>estimation procedures are few. The better ones use a semi-parametric<br/>approach because long-range dependence does not involve the short<br/>range dependence structure. The most successful methods have been<br/>Fourier-based. One approach involves a regression on the logarithm of<br/>the periodogram. Another is the local Whittle approach. It is a<br/>pseudo-likelihood approach which uses the periodogram at low<br/>frequencies only. The investigator proposes to use a pseudo-likelihood<br/>approach based not on the Fourier spectrum but on wavelets. The<br/>advantage of wavelets on Fourier is that there is no need to<br/>difference the time series if these are not stationary and, in<br/>general, they are much more robust against departure from<br/>stationarity. Since wavelets are associated with scaling it is,<br/>moreover, natural to attempt to use wavelets in order to estimate the<br/>intensity of long-range dependence. Wavelets have been successfully<br/>used as an alternative to the regression on the logarithm of the<br/>periodogram but wavelets have not been<br/>used as an alternative to the Fourier-based local Whittle approach.<br/>The investigator proposes to do so. This would provide a robust<br/>semi-parametic pseudo-likelihood based method to estimate the<br/>intensity of long-range dependence.<br/><br/>The subject of time series analysis has sparked considerable research<br/>activity over the past several decades. Essentially concerned with<br/>measurements over time of various kinds of phenomena --- from yearly<br/>income, exchange rates, to the level of a river --- the goal is to<br/>develop suitable models and obtain accurate predictions for such<br/>measurements, the core ingredient being the notion of time dependence.<br/>Benoit Mandelbrot in the sixties suggested using models involving<br/>long-range dependence. Long-range dependence, to put it concisely,<br/>affects phenomena in which correlations between the present and the<br/>past decay slowly with time and thus they cannot be easily ignored.<br/>Time series with long-range dependence and their variations have been<br/>used in hydrology, geophysics and biophysics, and more recently, in<br/>finance and in analyzing traffic in computer networks. It is thus<br/>important to be able to estimate effectively the intensity of<br/>long-range dependence. The purpose of this research is to develop a<br/>new methodology to achieve this goal. It is wavelet-based and hence<br/>insensitive to the effects of trends and other deviations from the<br/>model."
"0504233","Asymptotic Equivalence of Nonparametric Experiments with Nuisance Parameters","DMS","STATISTICS","07/01/2005","05/08/2007","Andrew Carter","CA","University of California-Santa Barbara","Continuing grant","Gabor J. Szekely","06/30/2008","$84,985.00","","carter@pstat.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","MPS","1269","0000, OTHR","$0.00","Asymptotic equivalence theory has emerged as a growing new area of <br/>mathematical statistics research. The purpose of this project is to <br/>describe the asymptotic behavior of some nonparametric curve estimation <br/>models, which have distributions that depend on a smooth function f(x). <br/>For each of these nonparametric models, the investigator derives a <br/>limiting model that can be used as an asymptotic approximation, such <br/>that inference in the original experiment can be performed under the <br/>limiting experiment without loss of information. Many regular <br/>nonparametric models have been shown to be approximable in the limit by <br/>a Gaussian white-noise-with-drift experiment which observes a Brownian <br/>motion plus a mean that depends on f(x). In this project, the <br/>investigator considers extensions of the regular nonparametric problem <br/>to include nuisance parameters such as an unknown variance or an unknown <br/>distribution of sample points. The limiting experiments in these cases <br/>contain a secondary component that describes the nuisance parameter, and <br/>this demonstrates that the estimation of f(x) can be separated from <br/>other unknowns in the problems. The investigator also approximates <br/>nonparametric problems with two-dimensional sample spaces by a Brownian <br/>sheet process plus a mean. These asymptotic approximations allow the <br/>non-standard problems to be solved using techniques from the simpler <br/>Gaussian models.<br/><br/>In many areas of technology today, large sets of data that do not follow <br/>a typical pattern are difficult for scientists to analyze. This problem <br/>comes up in tasks such as interpreting medical images and signals, <br/>describing distributions of plant species, and analyzing financial <br/>series. In this project, the investigator transforms the problem into a <br/>form that can be solved by already existing statistical tools. As a <br/>result, existing methods of analysis can be applied to a wider range of <br/>technological problems. Thus the techniques developed by the <br/>investigator may be used to improve the efficiency of medical imaging, <br/>signal interpretation, financial analysis, and other applications."
"0539100","Topics in the Theory of Randomization","DMS","STATISTICS","08/25/2005","07/12/2005","William Rosenberger","VA","George Mason University","Standard Grant","Gabor J. Szekely","06/30/2009","$160,000.00","","wrosenbe@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1269","0000, OTHR","$0.00","The investigator will explore theoretical properties of randomization <br/>procedures with applications to sequential randomized clinical trials. <br/>The first problem the investigator will address is sequential monitoring <br/>of randomization tests following restricted randomization, which will <br/>require extensive use of techniques in sequential analysis and <br/>nonparametric statistical inference. The second problem the investigator <br/>will address is the development and properties of response-adaptive <br/>randomization procedures, including procedures designed for continuous <br/>responses, and survival responses with censoring. The third problem the <br/>investigator will address is certain theoretical properties of <br/>covariate-adaptive randomization procedures, particularly <br/>randomization-based inference procedures following covariate-adaptive <br/>randomization. The ultimate goal of the investigator is to complete a <br/>systmeatic study of randomization, its properties, and its relevance in <br/>clinical trials.<br/><br/>The investigator will explore properties of randomization, the hallmark of <br/>a well-designed clinical trial. The first problem the investigator will <br/>address is sequential monitoring of randomized clinical trials, which <br/>refers to making early decisions about the effectiveness of new therapies <br/>when compelling evidence is accrued at an interim point in a clinical <br/>trial. There are currently no sequential monitoring techniques available <br/>that allow the randomization itself to be incorporate into these <br/>decisions, and hence current methodology completely ignores the design of <br/>the trial in the analysis. The second problem the investigator will <br/>address is response-adaptive randomization, and he will develop new <br/>procedures for clinical trials where the outcome is time-to-event or some <br/>continous measure, e.g. cholesterol level. These response-adaptive <br/>procedures are attractive from an ethical point of view, because they seek <br/>to maximize power to detect a relevant clinical outcome, while <br/>simultaneously minimizing the expected number of treatment failures <br/>patients will experience while participating in the clinical trial. The <br/>third problem the investigator will address is proporties of <br/>covariate-adaptive randomization procedures, which is designed to minimize <br/>tial imbalances in important confounding variables that might bias results <br/>of the clinical trial. Each of these topics is relevant to today's <br/>clinical trials in the drug development phase of new pharmaceuticals."

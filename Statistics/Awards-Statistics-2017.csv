"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1651995","CAREER:   Gaussian Graphical Models: Theory, Computation, and Applications","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2017","06/21/2021","Caroline Uhler","MA","Massachusetts Institute of Technology","Continuing Grant","Yong Zeng","06/30/2023","$400,000.00","","cuhler@mit.edu","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","MPS","1269, 8048","1045","$0.00","Technological advances and the information era allow the collection of massive amounts of data at unprecedented resolution. Making use of this data to gain insight into complex phenomena requires characterizing the relationships among a large number of variables. Graphical models explicitly capture the statistical relationships between the variables of interest in the form of a network. Such a representation, in addition to enhancing interpretability of the model, enables computationally efficient inference. The investigator develops methodology to infer undirected and directed networks between a large number of variables from observational data. This research has broad societal impact, as it affects application domains from weather forecasting to phylogenetics and to personalized medicine. In addition, the PI is one of the initial faculty hires in a new MIT-wide effort in statistics. As such, the PI has major impact on creating new undergraduate and PhD programs in statistics to train the next generation in big data analytics, crucial for taking on challenging roles in this data-rich world.<br/><br/>The goal of this project is to study probabilistic graphical models using an integrated approach that combines ideas from applied algebraic geometry, convex optimization, mathematical statistics, and machine learning, and to apply these models to scientifically important novel problems. The research agenda is structured into three projects. In the first project, the investigator develops methods to infer causal relationships between variables from observational data using the framework of directed Gaussian graphical models combined with tools from optimization and algebraic geometry. The end goal is to apply this new methodology to learn tissue- and person-specific gene regulatory networks from gene expression data such as the Genotype-Tissue Expression (GTEx) project. In the second project, the investigator develops scalable methods for maximum likelihood estimation in Gaussian models with linear constraints on the covariance matrix or its inverse. Such models are important for inference of phylogenetic trees or cellular differentiation trees. The third project is an application of graphical models to weather forecasting; the investigator develops new parametric methods based on Gaussian copulas and also non-parametric methods for the post-processing of numerical weather prediction models that take into account the complicated dependence structure of weather variables in space and time."
"1700494","The 2017 John Barrett Memorial Lectures -- Mathematical Foundations of Data Science","DMS","PROBABILITY, STATISTICS, CDS&E-MSS","04/01/2017","03/23/2017","Vasileios Maroulas","TN","University of Tennessee Knoxville","Standard Grant","robert lund","03/31/2018","$16,000.00","Jan Rosinski, Steven Wise, Clayton Webster","vmaroula@utk.edu","201 ANDY HOLT TOWER","KNOXVILLE","TN","379960001","8659743466","MPS","1263, 1269, 8069","7556","$0.00","The 2017 Barrett Lectures, entitled ""Mathematical Foundations of Data Sciences,"" will be held May 1-3, 2017 at the University of Tennessee in Knoxville, Tennessee. Data science has experienced rapid recent development, largely in response to the challenges associated with massive data sets. The enhanced ability to collect and store data requires a fundamental change in approaches to data analysis. Today's big data challenge presents a unique opportunity to discover new scalable approaches and to expand current theoretical and computational algorithms. The conference features keynote lectures by Andrea Bertozzi and Aad van der Vaart. Additionally, a dynamic group of U.S. and international researchers at different career stages will give invited plenary talks and poster presentations. This project utilizes NSF funding to support travel and other participation costs for approximately forty researchers, giving priority to graduate students, postdoctoral scholars, and early career faculty members. <br/><br/>The 2017 Barrett Lectures will be the 47th installment of a lecture series that began as a tribute to the distinguished scientist John H. Barrett, head of the Mathematics Department at the time of his death in 1969. The Lectures are one of the longest-running mathematical lecture series in the country and are delivered by distinguished speakers. The focus of the 47th Lectures is the application of several mathematical methods in topics of data analysis and the growing penetration of complex physical phenomena and formidable engineering problems related to big data. A dynamic group of prominent researchers will expose several research avenues for integrating aspects of our respective (but seemingly disparate) big data perspectives. These connections will ultimately deepen our understanding of the emerging methodologies, which are vital for solving the scientific and engineering problems of the 21st century. Young researchers, especially, will benefit from exposure to the wide array of approaches presented at the meeting. More information about the conference is available at http://www.math.utk.edu/barrett/"
"1712933","High Dimensional Mediation Analysis with Multi-Omics Data","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","09/01/2017","11/09/2021","Bhramar Mukherjee","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Yong Zeng","08/31/2023","$351,765.00","Xiang Zhou","bhramar@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1253, 1269","096Z, 102Z, 1515","$0.00","Rapid development of various high-throughput biological technologies has revolutionized the field of genomics. Various genomic studies produce molecular-level traits by measuring gene expression levels and characterizing various covalent modifications of DNA and histone proteins. The measured molecular-level traits, including gene expression and methylation levels, are thought to mediate the effects of DNA and/or the environment on many traits and diseases, and hold the key to understanding the genetic and environmental basis of disease susceptibility and phenotypic variation. In particular, these high-dimensional biomarkers absorb and reflect environmental insults to the genome and serve as measures of an individual's internal molecular and cellular environment that change dynamically over the time course of life.  In this project, statistical methods will be developed to perform high-dimensional mediation analysis, in order to further our understanding of the molecular basis of disease susceptibility and phenotypic variation, and facilitate the integrative analysis of various molecular-level traits from omics studies. The proposed statistical methods will be used to study how the inherited DNA environment and the external environment, as measured through environmental toxicants, socioeconomic conditions, neighborhood characteristics, psychosocial stress and other life events, influence omics measures of the internal environment, and in turn lead to adverse health outcomes. <br/><br/>Technically, the molecular-level traits from omics studies will be treated as a multivariate set of high-dimensional mediators for integrative analysis. A novel high-dimensional mediation analysis framework will be developed to handle multiple exposures and multiple mediators simultaneously. The proposed high-dimensional mediation analysis methods will extend existing mediation analysis methods from handling univariate mediator and/or univariate exposure to a high-dimensional setting by making additional modeling assumptions on the effects of mediators and exposures to enable model identifiability. While the problem is formulated in a causal inference framework, inference will be conducted using a Bayesian variable selection framework that identifies important exposures and mediators simultaneously. The research combines ideas from variance component score tests, Bayesian variable selection, and causal inference in a unified manner to lead to new theoretical insights on estimation of direct and indirect effects. Methodological extensions will also be made to conduct mediation analysis based on sharing summary statistics that are becoming increasingly common in genetics studies. The newly developed methods will be applied to large ongoing cohort/case-control studies, and software will be developed for scalable implementation of the proposed methods."
"1654083","CAREER:   Data Assimilation for Massive Spatio-Temporal Systems Using Multi-Resolution Filters","DMS","STATISTICS, Division Co-Funding: CAREER","03/01/2017","05/19/2021","Matthias Katzfuss","TX","Texas A&M University","Continuing Grant","Pena Edsel","02/28/2023","$400,000.00","","katzfuss@gmail.com","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269, 8048","1045, 8012","$0.00","The research supported by this award will produce powerful and scalable open-source software for data assimilation in large spatio-temporal systems with varying degrees of nonlinearity. It will lead to improved inference, forecasts, diagnostics, downscaling, and calibration using data assimilation in many fields of science with direct impact on society, including weather forecasting, climate studies, renewable energy, and pollution monitoring. Despite the great importance and highly statistical nature of data assimilation, there is a lack of statisticians involved in this research area. Thus, the educational component of this project revolves around bridging the gap between the statistics and data-assimilation communities, and getting more statisticians involved in the latter.<br/><br/>The principal investigator will develop approaches for filtering inference on high-dimensional states that can outperform existing methods in linear and nonlinear settings. The novel approaches are based on the multi-resolution approximation, a state-of-the-art method for spatial covariance approximations that employs many adaptive, compactly supported basis functions at multiple resolutions. Algorithmic implementations of the methods are highly scalable and can take full advantage of massively parallel high-performance computing systems. Validation, testing, and comparison of the methods will be carried out using realistic observations simulated from models of varying complexity."
"1712800","Discovering What Matters: Informative and Reproducible Variable Selection with Applications to Genomics","DMS","Genetic Mechanisms, STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2017","06/14/2017","Chiara Sabatti","CA","Stanford University","Standard Grant","Gabor Szekely","08/31/2020","$420,000.00","Emmanuel Candes","sabatti@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1112, 1269, 7454","7465, 8007","$0.00","This project will develop statistical methods to discover which variables, in a large collection, are meaningfully related to an outcome of interest. An example of the problem is the identification of which genetic variants, among the millions we measure, influence disease risk. The methods developed will allow analysis of all variables at the same time, accounting for their interdependence, and leading to the identification of ""actionable"" ones. The approaches put forward come with the guarantee that, on average, a large fraction of the discovered features truly influence the outcome.  The ability to correctly identify important variables will increase knowledge in many domains, and allow experts to devise interventions.  For example, understanding which of the variables recorded on a patient are more relevant with respect to his/her response to therapy, can help develop personalized medical interventions with a higher success rate.<br/><br/>The methods developed will enlarge the tool-box available to statisticians and data scientists as they attempt to extract meaningful information from datasets comprising a very large number of variables. The approach builds on the ""knock-off"" framework, a very flexible and novel approach that does not require specifying a model for the relation between an outcome of interest and possible co-variates. The inferential guarantees provided are on the selected variables, with control of the False Discovery Rate (FDR), where a discovery is considered false if a selected variable is independent of the outcome given the remaining covariates.  This provides assurance on the reproducibility of results, as well as on their interpretability.  The approaches developed will be used to analyze genetics datasets with the goal of obtaining more complete models of how DNA variation influences medically relevant phenotypes.  This project is supported by the Division of Mathematical Sciences and the Division of Molecular and Cellular Biosciences."
"1712580","Collaborative Research: Collaborative Learning for Multimodal Data","DMS","STATISTICS","07/01/2017","05/10/2017","Yunzhang Zhu","OH","Ohio State University","Standard Grant","Gabor Szekely","06/30/2021","$124,963.00","","Zhu.219@osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269","","$0.00","A multimodal paradigm has become increasingly important given today's explosive growth of information, which often arises from, for instance, automatic image categorization and personalized prediction. Multimodal data has a wide spectrum of applications in medical diagnostics, social networking, multimedia, information filtering, personalized advertising, consumers' recommendations, virtually in any electronic commerce and entertainment platform. This research aims to develop statistical theory, methods, and computational tools to integrate multimodal data for prediction and description. The development will lead to the higher accuracy of learning,  which will ultimately enhance information storage, sorting and filtering. Moreover, the research project has an education component to train graduate students in emerging areas. The research products will be disseminated through publications and presentations.<br/><br/>The proposed research aims to develop statistical techniques to utilize conditional dependence structures for integrating multimodal data. It will proceed in the areas of collaborative learning and personalized prediction. In each area, regression, classification, and ranking will be performed collaboratively based on pairwise conditional dependencies between the response components, modeled by a directed graph or an undirected graph. Special efforts will be devoted to the joint learning of data of multiple modalities and extraction of latent structures with an adjustment for covariates. Target applications include image categorization and recommender systems, where the proposed techniques will be applied to understand the content of an image and to predict personalized preference over a large number of items. Furthermore, The research will develop computational tools and design methods that have desirable statistical properties."
"1712990","The Weighted Bootstrap and Berry-Esseen Bounds in High Dimensions","DMS","STATISTICS","07/01/2017","05/03/2017","Mayya Zhilova","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","06/30/2021","$162,447.00","","mzhilova7@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","","$0.00","Resampling methods are widely used for statistical inference in numerous applications. In particular, bootstrapping is known to perform well in situations when the amount of available data is rather small. In many modern applications the data are complex and high-dimensional, motivating development of new approaches in statistical inference, including resampling methods. In this project, the investigator will study weighted bootstrap procedures in a high-dimensional framework with a limited amount of data, for various classes of statistical models. The main goals of this research are to understand essential properties of the weighted bootstrap, such as its limitations and accuracy, and to advance resampling methods in high-dimensional settings.<br/><br/>In this project, the investigator will study the problem of approximation in distribution of a function of a sample average in a high-dimensional non-asymptotic framework, for various classes of functions. Two basic types of approximations, which are closely related to each other, will be studied: an approximation using the weighted bootstrap procedure, and Berry-Esseen type inequalities. In both cases, the investigator aims to establish higher-order approximation bounds that extend classical Gaussian approximation theory and yield considerable improvements in accuracy with respect to both dimension and sample size. The study will be focused on optimality of the resulting bounds, and on explicit form of the error terms. Another important direction of the research in this project is extension of the proposed higher-order approximation methodology to the case of heavy-tailed distributions, which play an important role in many applications in finance and engineering. The work on the project aims to employ and further develop various modern techniques, such as higher-order approximation and comparison inequalities, concentration inequalities for multilinear symmetric forms, geometric properties of Gaussian measures in high dimensions, and methods related to multivariate moment problems."
"1713120","Misspecified Mixed Model Analysis: Theory and Application","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","07/01/2017","07/12/2017","Jiming Jiang","CA","University of California-Davis","Standard Grant","Pena Edsel","06/30/2022","$279,935.00","Debashis Paul, Hongyu Zhao","jiang@wald.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269, 7454","8007","$0.00","This project, a collaboration between statisticians and a statistical geneticist, focuses on the development of statistical theory and methods for the analysis of data from genome-wide association studies (GWAS).  Over the past decade, while GWAS have been very successful in detecting genetic variants that affect complex human traits/diseases, these discoveries have only accounted for a small portion of the genetic factors.  Recently, significant progress has been made using statistical analysis based on a class of statistical models called mixed effects models.  However, there is a gap in understanding why the method works, because, in a way, the statistical model used in the analysis is misspecified.  This project aims to fill the gap by developing new theory and methods, and evaluating the methods through applications to real data.  The project will promote teaching, training and learning, broaden the participation of students from under-represented groups, and build research networks between institutions.  The research will be of great interest to many other areas of science, and the results will be widely disseminated in subject matter domain journals. <br/><br/>In the past decade, more than 24,000 single-nucleotide polymorphisms (SNPs) have been reported to be associated with at least one trait/disease at the genome-wide significance level.  However, these significantly associated SNPs only account for a small portion of the genetic factors underlying complex human traits/diseases, referred to as ""missing heritability"" in the genetics community.  Recently, significant progress has been made in using the restricted maximum likelihood (REML) approach based on linear mixed models (LMM).  While the REML approach appears to provide the right answer to many problems of practical interest, researchers have been puzzled by the fact that the LMM, under which the REML estimators are derived, is misspecified.  In a recently published article, the investigators proved that the REML estimators of some important genetic quantities, such as heritability and the variance of the environmental error, are consistent despite the model misspecification.  While this pioneering work led to a new field called misspecified mixed model analysis (MMMA), many theoretical and practical challenges remain unsolved.  This project seeks to address the following problems: (1) extension of MMMA to correlated SNPs, (2) development of the asymptotic distribution of the REML estimator under misspecified LMM, (3) resampling methods for MMMA, (4) estimation of the number of nonzero random effects, and (5) extensions to multiple random effect factors and discrete traits.  The research will also include software development to implement the methods."
"1712642","Deterministic Sampling through Energy Minimization","DMS","STATISTICS","07/01/2017","05/14/2019","Roshan Joseph","GA","Georgia Tech Research Corporation","Continuing Grant","Gabor Szekely","06/30/2020","$199,999.00","","roshan@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","7433, 8083","$0.00","This project aims at developing optimal deterministic methods for statistical sampling / statistical observations, in contrast to commonly-used random sampling methods such as Monte Carlo (MC) and Markov Chain Monte Carlo (MCMC). The MC/MCMC methods have revolutionized statistics, allowing statisticians to model and solve complex and high-dimensional problems that would have been intractable using conventional techniques. One drawback of these methods is that very many observations or data samples are needed due to the slow convergence rate inherent in random sampling. This becomes an issue when the sampling is expensive. The deterministic method under study in this project attempts to overcome this problem by sampling points more intelligently, so that the same information provided by a random sample can be obtained with fewer deterministic samples. This can significantly cut down the cost of sampling and subsequent computations. The method under development has applications in many fields, such as uncertainty quantification, computer experiments, and machine learning.<br/><br/>The project aims to provide deterministic samples obtained through the minimization of certain energies. The goal is to use carefully developed optimization techniques to reduce the number of expensive evaluations of a probability distribution, thereby reducing the overall computational cost. Furthermore, the deterministic sample provides a much better representative set of points for the distribution, which can further reduce the cost of subsequent computations involving integrals. Compared to the existing Quasi-Monte Carlo methods, which are mostly developed for sampling from the uniform hypercube, the methods under study are much more general and can be used to directly sample from any probability distribution. Two methods for deterministic sampling will be investigated. The first method, known as minimum energy designs, is useful when the probability density is expensive to evaluate. The second method, known as support points, is useful when the integrand is expensive but sampling from the probability density is easy. The minimum energy design possesses an important property: its empirical distribution asymptotically converges to the target distribution. This is a property not shared by some of the competing representative point sets in the literature, such as principal points. On the other hand, support points are obtained by minimizing an energy distance which is used for goodness-of-fit testing. In this light, support points can be viewed as point sets that optimally compact a continuous probability distribution. The project focuses on developing efficient optimization methods for these energy functions using as few function evaluations as possible, and improving the distributional properties of the point sets so that they can be used in problems where MC/MCMC methods are computationally impracticable."
"1712564","Collaborative Research: Collaborative Learning for Multimodal Data","DMS","STATISTICS","07/01/2017","05/10/2017","Xiaotong Shen","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","06/30/2021","$300,000.00","","xshen@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","A multimodal paradigm has become increasingly important given today's explosive growth of information, which often arises from, for instance, automatic image categorization and personalized prediction. Multimodal data has a wide spectrum of applications in medical diagnostics, social networking, multimedia, information filtering, personalized advertising, consumers' recommendations, virtually in any electronic commerce and entertainment platform. This research aims to develop statistical theory, methods, and computational tools to integrate multimodal data for prediction and description. The development will lead to the higher accuracy of learning,  which will ultimately enhance information storage, sorting and filtering. Moreover, the research project has an education component to train graduate students in emerging areas. The research products will be disseminated through publications and presentations.<br/><br/>The proposed research aims to develop statistical techniques to utilize conditional dependence structures for integrating multimodal data. It will proceed in the areas of collaborative learning and personalized prediction. In each area, regression, classification, and ranking will be performed collaboratively based on pairwise conditional dependencies between the response components, modeled by a directed graph or an undirected graph. Special efforts will be devoted to the joint learning of data of multiple modalities and extraction of latent structures with an adjustment for covariates. Target applications include image categorization and recommender systems, where the proposed techniques will be applied to understand the content of an image and to predict personalized preference over a large number of items. Furthermore, The research will develop computational tools and design methods that have desirable statistical properties."
"1756078","Emerging Issues in Modeling Longitudinal Observations with Censoring","DMS","STATISTICS","09/01/2017","08/16/2017","Bin Nan","CA","University of California-Irvine","Continuing Grant","Gabor Szekely","08/31/2018","$106,999.00","","nanb@uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1269","","$0.00","Longitudinal studies--commonly referred to as cohort studies in epidemiology or panel studies in sociology--are of fundamental importance in understanding time related issues within the same group of subjects. In longitudinal studies, the collection of information can be stopped at the end of the study, or at the time of dropout of a study participant, or at the time of the occurrence of a terminal event. Death, the most common terminal event, often occurs in aging cohort studies and fatal disease follow-up studies, e.g., organ failure or cancer studies. If not handled appropriately, the occurrence of terminal event can cause serious bias in statistical inference, leading to incorrect conclusions.  The current literature has primarily focused on modeling the longitudinally measured variables given that the terminal event has not happened yet, hence the observed repeated measures ""terminated"" by a terminal event are implicitly treated as incomplete data. Such a modeling strategy, however, is inappropriate for many studies when some effect of interest is directly related to the terminal event time, such as the medical cost data. In this project, a conditional modeling strategy will be implemented, which treats repeated measures up to a terminal event as complete data and directly models the effect of terminal event time on a response variable. A related problem is some predictor variable subject to limit of detection in regression analysis, which occurs frequently in studies involving assay measures, where measures of certain substance (e.g., hormone, air pollutant, or water contaminant) become unreliable when their concentrations are below certain level due to technology limitation. The current literature has focused on primarily ad hoc imputation or unverifiable model assumptions for the predictor variable subject to limit of detection, but these methods generally yield biased results. This project will tackle this issue using robust statistical methods, which follow similarly the conditional modeling strategy for terminal events, and yield more reliable results than existing approaches.<br/><br/>The project investigates modeling strategies that model the effect of terminal event time directly by treating it as a covariate in longitudinal studies. In such statistical models, the usual relationship of interest between the longitudinally measured response variable and covariates is kept when data collecting time is far from the terminal event time, and the relationship becomes increasingly related to the terminal event time when data collecting time is close to the terminal event. Such models provide much more intuitive and sensible interpretations, and can be applied to recurrent events data with the presence of a terminal event. Both parametric and semiparametric models will be considered. Estimating methods for parameters in the proposed models will be investigated. The asymptotic theory for the case that the terminal event time is subject to right censoring will be a major focus. A closely related set of longitudinal regression problems with censored covariates considered in this project is about the issue of detection limit for covariates. The validity of any estimating approach relies on how reliably one can model and estimate the tail distribution of the covariate subject to limit of detection. Due to the feature of this type of data, parametric models are not verifiable and nonparametric models are not able to gain any useful information about the missing tail distribution. The project investigates semiparametric models, which are able to gain useful information from observed data and are insensitive to model misspecification of the missing tail probabilities."
"1712962","Planes of Change: New Statistical Methods for Complex Non-Standard Systems","DMS","STATISTICS","07/01/2017","08/02/2019","Moulinath Banerjee","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","06/30/2020","$350,000.00","Ya'acov Ritov","moulib@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","8083","$0.00","The project aims to develop new statistical methodologies for analysis of systems in a variety of fields, such as personalized medicine, internet traffic, and economics, in which sharp threshold effects occur. Such sharp effects are typically experienced when a system is subjected to a sudden shock (e.g., the effect of political tension on stock prices, the effect of socio-political upheaval on social-media networks, or the effect of a medical intervention on disease progression). Such sharp changes are of critical interest to practitioners in these different fields as they typically have important implications for future decision-making. Statisticians model such sharp changes in time, for example, through what are called ""change-points;"" when the sharp change happens due to the effect of multiple variables simultaneously, such regions are described in terms of ""change-planes."" This project aims to develop novel methods of identifying such change-points or change-planes in problems where massive amounts of data -- which have now become the norm given advances in storage capabilities as well as collection mechanisms -- are available, and furthermore, the number of variables on which data are recorded is also very large. The performance of such methods will be carefully analyzed using mathematical theory as well as computer-generated simulations, and the methods will also be validated on real data coming from a variety of sources. It is anticipated that the results of the research will have impact in a variety of natural science as well as social science disciplines. <br/><br/>The overarching theme of this project is to develop methodology and inference in a class of problems in which thresholds or boundaries (in one or multiple dimensions) that induce discontinuities arise naturally, either in the statistical model or in the estimation paradigm. The problems are studied both in the setting of massive amounts of data as well as in scenarios where the number of covariates can exceed the number of observations. The boundaries considered in one-dimension are change-points, while those in multiple dimensions are hyper-planes. The studied problems present two different kinds of complexities: (a) massive amounts of available data, and/or (b) large numbers of covariates relative to number of observations. In particular: (i) A number of ideas are developed for sampling intelligently from (retrospectively observed) long time-series to determine the locations of multiple change-points via procedures that require analyzing only a vanishing fraction of the entire series (thereby providing computational benefits), yet produce estimates that match, in precision, the standard estimates that would have been obtained analyzing the entire series. This idea is extended to regression/likelihood based models with covariates in multiple dimensions where the parameters of the regression or the likelihood are different on either side of a hyper-plane in covariate space. (ii) Problems involving hyper-planes, either in the structure of the model or in the criterion function to be optimized, with high-dimensional covariates are studied and new variable selection and estimation methods are investigated. The problems under consideration here are important from the perspective of applications but difficult because the high-dimensional paradigm has to be extended to intrinsically discontinuous settings, outside the (almost) square-root-n rate. Effective solutions to these problems will advance statistical methodology for these important classes of systems."
"1712760","Inference for High Dimensional Quantile Regression","DMS","STATISTICS","09/01/2017","07/30/2019","Huixia Wang","DC","George Washington University","Continuing Grant","Yong Zeng","08/31/2022","$125,000.00","","judywang@gwu.edu","1918 F ST NW","WASHINGTON","DC","200520042","2029940728","MPS","1269","","$0.00","Quantile regression is emerging as an important and active research area driven by diverse applications. Compared with the conventional least squares regression, quantile regression methods are robust against outliers and can capture heterogeneity.  In recent years, significant results related to estimation and variable selection for quantile regression have been obtained for big data applications. However, there exists little work on inferential methods including hypothesis testing and confidence interval construction for quantile regression in the high-dimensional setting.  This project seeks to develop new statistical theory, methodology and algorithms to address inference problems in high-dimensional quantile regression.  The research is motivated by a large clinical trial of diabetes intervention, and the developed methods can also be applied to data from genome-wide association studies, neuroscience, and environmental studies.  <br/><br/>The research will focus on two main directions. First, new testing procedures will be developed to assess the overall significance of high-dimensional covariates on quantiles of the response distribution. Two types of tests are proposed, including a maximum-type statistic based on marginal quantile regression, and a score-type statistic.  Theory and methods for both fixed and diverging dimensions will be studied. Second, focusing on high-dimensional quantile regression without the conventional minimum signal strength condition on the coefficients, the PI will rigorously study the asymptotic theory of penalized estimators and develop valid post-selection inference methods based on both asymptotic theory and bootstrap procedures.  The PI will integrate research and education by developing advanced topics courses, engaging graduate and undergraduate students, especially those from under-represented groups, in the project, and reaching out to K-12 students and developing countries through collaboration and knowledge sharing."
"1712839","Collaborative Research:   Higher-Order Asymptotics and Accurate Inference for Post-Selection","DMS","STATISTICS","08/01/2017","08/02/2017","John Kolassa","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","07/31/2021","$119,995.00","","kolassa@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","Many statistical analyses utilize a model selection procedure. Perhaps the most common model selection problem is that of variable selection in linear regression. Principled motivations for selection include the desire for interpretability, prevention of over-fitting, and concerns about statistical power. A practical motivation arises when the data are high-dimensional, with more explanatory variables than observations. Relevant applications span the entire domain of scientific inquiry, from neuroscience, medicine and physics, to economics, sociology, and psychology. A large catalogue of variable selection procedures is now available, and the statistics community has turned its focus to the question of inference after selection. Standard methods of statistical inference are no longer valid when the same data are used to both select a model and make inferences about that model. It is fundamentally important to have accurate post-selection inference procedures that are also powerful enough to detect observed departures from scientific hypotheses and that avoid the strong distributional assumptions needed for exact inference in finite samples. This research aims to develop post-selection inference methodology that is both accurate and powerful, with particular emphasis on reducing statistical errors that depend on the sample size. <br/><br/>The goal of this project is to further understanding of the asymptotic theory of post-selection inference, particularly selective inference based on the CovTest and truncated Gaussian (TG) statistic, as well as simultaneous inference using the post-selection intervals (PoSI) procedure. The first two procedures can be motivated by selective error control, i.e., error control for the selected model parameters. The PoSI method seeks to control family-wise error rates for all possible sub-model parameters. While these procedures yield valid post-selection inference, without strong assumptions they are particularly vulnerable to the effects of violation of key assumptions in the realistic setting of small to moderate sample sizes, such as overly-conservative or inaccurate confidence intervals, and low power. In this project, asymptotic expansions, saddle-point approximations, the bootstrap, and related techniques from higher-order asymptotics will be employed to improve accuracy and power for these post-selection inference procedures."
"1712872","State Space Models:  A New Look at Smoothing, Parameter Inference, and Model Choice","DMS","STATISTICS","07/01/2017","06/02/2017","Pierre Jacob","MA","Harvard University","Standard Grant","Gabor Szekely","06/30/2021","$202,589.00","","pjacob@fas.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269","","$0.00","Time series are repeated observations of objects over time; for example, hourly prices of a financial asset, daily population sizes of plankton in the ocean, or monthly case counts during a disease outbreak. All of these examples are routinely studied using state space models, under which observations are treated as noisy measurements of an unobserved stochastic process. The underlying process can be modeled using any equation considered relevant for the object at hand, but flexibility comes at a computational cost. For instance, if the process is modeled using nonlinear differential equations, the associated statistical computations might become prohibitive. This is particularly acute for long and high-dimensional time series. This project introduces new methods for three important questions regarding state space models: how to estimate the unobserved process, how to estimate the model parameters, and how to compare multiple models. These historical questions will be revisited in the contexts of parallel computing hardware, of large datasets with vast amounts of missing values, and of limited prior information on the parameters. The toolbox under development will participate in a global effort to quantify uncertainty in scientific models for time series. It will be implemented in efficient and user-friendly software packages, made publicly available during the project.<br/><br/>State space models form a very flexible class of time series models, under which observations are assumed conditionally independent given a latent stochastic process. The project concerns three fundamental questions in these models: latent process estimation, parameter estimation and model choice. For latent process estimation (1), an unbiased estimator of smoothing expectations is developed and investigated. The approach relies on a novel coupling of particle filters, using maximal couplings and ideas from optimal transport. The unbiasedness property allows a complete parallelization of the estimation procedure, and the construction of reliable confidence intervals. For parameter estimation (2), similar couplings of particle filters can be leveraged to drastically reduce the variance in score estimators and in Metropolis acceptance ratios, especially for long time series. For model comparison (3), a novel criterion is investigated following a predictive sequential approach. It is designed for situations where prior information on parameters is limited, contrarily to Bayes factors. The project aims at helping current users of state-space models, for instance in econometrics, genetics, neuroscience, ecology, and marine biogeochemistry, and to assist scientists for whom currently available methods are insufficient. On a more fundamental level, leveraging ideas from coupling and optimal transport for computational methods is of independent interest and paves the way to a wide range of new sampling and optimization methods."
"1712558","New Nonparametric Modeling Methods for High-Dimensional Time Series","DMS","STATISTICS","09/01/2017","08/16/2019","Shujie Ma","CA","University of California-Riverside","Continuing Grant","Gabor Szekely","08/31/2020","$124,970.00","","shujie.ma@ucr.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","MPS","1269","","$0.00","Advances in modern technology have created numerous massive datasets, providing a great amount of information, but also new analysis challenges. The remarkable increase in the amount of data arises not only in the number of observations over time, but also in the number of variables that are simultaneously measured at each time. This results in high-dimensional time series data that are increasingly encountered in many fields, including finance, economics, genomics, social media, biomedical imaging, and so forth. High dimensional time series can be evolutionary, non-normally distributed, and/or heterogeneous. Methods for analyzing these types of data are still in their infancy due to the considerable methodological challenges encountered to describe their complex structure. Because of the intricacies of modern datasets, conventional statistical methods to extract information are often inappropriate. There is an immediate need for efficient and data-driven nonparametric methods to handle these problems. This project seeks to develop new nonparametric modelling methods with theoretical insights for structural change detection, robust estimation, heterogeneity exploration, and dynamic interdependency investigation. The project will help fill methodological gaps by greatly advancing the understanding of the intricacies of high-dimensional time series data. The new flexible methods may can benefit many scientific areas, including public health, medicine, economics, and the social sciences.<br/> <br/>The overall goal of this project is to develop new flexible statistical methods and theories to address the analytical challenges encountered in describing the evolutionary, non-normal, and heterogeneous features of high-dimensional time series data. This will be done via four inter-connected research topics. (1) A novel three-step method with theoretical guarantees will be developed for structural change detection and identification of factor models by exploiting nonparametric local estimation, shrinkage methods, and grid search techniques. The method can automatically detect breaks (if they exist) and identify their locations. (2) A new paradigm, covariate-assisted quantile latent factor models, is proposed for dimension reduction of high-dimensional time series. The method is robust to heavy-tailed distributions. The model assumptions are very general: the factors are unobserved, and both of the factors and their loadings can vary across quantiles. In addition, the method does not require moment conditions on the errors. (3) A concave fusion method is proposed for exploring heterogeneous functional curves driven by unobserved classes. The method permits structural change detection and heterogeneity exploration, which are difficult problems due to latent processes and the high-dimensional and dependence features in the data. (4) A new dimensionality reduction tool will be devised for a time-varying coefficient vector autoregressive model by exploiting non-centered functional principal component analysis. A novel computational estimation algorithm will be developed by combining proximal algorithms and optimization over Stiefel manifolds. The method can illuminate dynamic relationships in high dimensional nonstationary time series."
"1712864","From Functional Data to Random Objects","DMS","STATISTICS","08/01/2017","08/19/2019","Hans-Georg Mueller","CA","University of California-Davis","Continuing Grant","Gabor Szekely","07/31/2020","$150,000.00","","hgmueller@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","Large and complex data that are increasingly collected across the sciences and by companies pose novel challenges for statistical analysis, due to their complexity and size. Specific applications that motivate this research come from brain imaging, genomics, and the social sciences.  To make sense of such data and extract relevant features, statistical methodology that is suitable for the analysis of large samples of complex random objects is needed.  Examples of these objects include networks, distribution functions, and covariance matrices.  A challenge is that common algebraic operations such as sums or differences are not defined for such objects.  In many instances, objects may also be repeatedly observed over time, and the quantification of their time dynamics is then of interest.  In this project, statistical methodology that addresses these basic data analytic needs is developed under minimal assumptions.  These developments also include the theoretical foundations of this methodology and computational implementations.  This methodology is expected to lead to new insights by quantifying phenomena such as changes in mortality or income distributions over calendar years, or changes in brain connectivity networks with aging to allow researchers to distinguish normal and pathological aging processes. Procedures are also developed to test for significant differences between groups of random objects, for example, comparisons between mortality distributions of countries, including the identification of clusters.  The methodology to be developed is based on delicate extensions of basic statistical notions such as population and sample mean, variance, regression and analysis of variance to the case of more complex spaces of random objects.<br/><br/>Over the past decade, there have been rapid advances and substantial developments for functional data, including advanced methods for functional regression. The developments and methodology for Functional Data Analysis are limited to Hilbert space valued random variables, such as square integrable random functions, which limits their applicability.  This research is motivated by the increasing prevalence of examples where random objects are not in a Hilbert space.  Key objects of interest are distributions, networks and covariance matrices, in addition to general metric space valued random objects.  Core concepts that will be applied and appropriately extended to these random objects include Frechet mean, Frechet variance, and Frechet regression. For longitudinally observed random objects, the notion of a general Frechet integral will serve to quantify projections in general spaces.  Such projections will be studied for their use in representing time-varying random objects. The tools that will be developed are based only on distances, and are therefore suitable for general metric space valued objects.  For special classes of objects such as distributions, additional characterizations such as manifold representations and Wasserstein covariance will also be developed and illustrated in applications."
"1713152","Collaborative Research: Theoretical and Methodological Frameworks for Causal Inference of Peer Effects","DMS","STATISTICS, Methodology, Measuremt & Stats","07/01/2017","07/05/2017","Peng Ding","CA","University of California-Berkeley","Standard Grant","Gabor Szekely","06/30/2020","$180,000.00","","pengdingpku@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269, 1333","","$0.00","Understanding how friends or peers interact and affect each other is often of great interest in biomedical studies and the social sciences.  However, it is not entirely clear how to quantify and develop inference for peer effects using a formal statistical framework.  This project will focus on the development of a statistical causal inference framework to address these challenges, with the goal of developing both theoretical and methodological tools for a wide class of questions involving inference of peer effects.  The methods will be applied to investigate peer effects among university students with different academic backgrounds.  The research could provide important guidance for decision and policy makers.<br/><br/>The classical potential outcomes framework for causal inference assumes no interference among experimental units.  In some empirical studies, interference is a nuisance that complicates analysis and should be avoided by careful experimental design.  In many applied fields, however, group or network structures exist and could cause interference among units.  Interference is no longer a nuisance in these applications, because studying the pattern of causal effects with interference is the scientific question of interest with important implications for policy or decision making.  The existing literature discusses external interventions on the units, where the networks, clusters or groups that induce interference are known a priori.  The new framework allows for the development of inferential tools for causal inference with interference from the Fisherian, Neymanian, and Bayesian perspectives. Under the Fisherian view, randomization tests will be used to detect deviations from the sharp null hypothesis without imposing further structural assumptions. Under the Neymanian view, randomization-based point and interval estimators, which serve as the basis for finding optimal treatment assignments will be developed.  Under the Bayesian view, hierarchical models will be developed to accommodate complex structures of real-life data and incorporate information from multiple groups with longitudinal outcomes. The project will also lead to the development of open-source R software. This project is supported by the Division of Mathematical Sciences and the Methodology, Measurement, and Statistics (MMS) Program in the Directorate for Social, Behavioral, and Economic Sciences."
"1712870","Leveraging Structural Information in Regression Tree Ensembles","DMS","STATISTICS","09/01/2017","07/26/2019","Antonio Linero","FL","Florida State University","Continuing Grant","Gabor Szekely","02/29/2020","$100,000.00","","antonio.linero@austin.utexas.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","8083","$0.00","A common task in statistics is prediction; for example, a practitioner may be interested in predicting the presence of a disease given genetic information about an individual. Due to recent advances in data collection, frequently one has access to datasets which contain a massive number of predictors, but with correspondingly few subjects. This setting is generally referred to as the ""big P, small n"" scenario. Drawing meaningful conclusions under such circumstances is generally impossible unless the underlying data satisfy certain structural assumptions. The simplest such structural assumption is that only a small number of the predictors are relevant; in this setting, finding the useful predictors corresponds to finding a so-called ""needle in a haystack."" The goal of this project is to construct procedures which adapt to this, and other, structural assumptions. The project will focus on methods based on decision trees, which are flowchart-like structures in which predictions are based on whether the predictors satisfy various rules. Usually an ensemble of decision trees are constructed, with the predictions for each individual tree averaged. While decision tree ensembles are frequently used with high dimensional data, it is unclear to what extent they adapt to the structural properties of the data. This project will show that, in practice, off-the-shelf decision tree ensembling methods do not adapt to common structural assumptions, and will develop new methods which do. In addition to developing methods with strong theoretical support, this project will support the development of an R package to give practitioners easy access to our methodology.<br/><br/>    <br/>The PI will develop Bayesian methods for incorporating structural information into tree-based ensemble methods, and establish theoretically the benefit of making use of this additional information. This forms a nonparametric counterpart to the parametric approaches used in linear models, such as the lasso, graphical lasso, or group lasso; Bayesian approaches in the parametric setting include the use of variable selection priors, such as spike-and-slab priors and global-local shrinkage priors. Structural information will be incorporated by modifying the commonly used priors on decision tree ensembles so that the prior is concentrated on models which satisfy the desired structure. The PI will first investigate the theoretical properties of a sparsity inducing prior which is designed to eliminate unnecessary predictors. Sparsity here is obtained by applying a sparsity inducing Dirichlet prior to the a priori probability that a given branch is associated to a given predictor. This prior will be extended to allow for grouped variable selection in a similar manner to the group lassoby considering the class of Dirichlet tree priors, and further to accommodate graphical structures in the predictors through sparsity inducing logistic normal priors. Additionally, the PI will develop computationally efficient Markov chain Monte Carlo algorithms to fit the resulting models. Compared to existing methods, these structural priors will be shown to lead to substantial gains in predictive accuracy, and to more accurate scientific discovery."
"1712977","Non-Parametric Methods for Analysis of Time-Varying Network Data","DMS","STATISTICS, MSPA-INTERDISCIPLINARY, Modulation","08/01/2017","06/26/2017","Marianna Pensky","FL","The University of Central Florida Board of Trustees","Standard Grant","Gabor Szekely","07/31/2021","$280,000.00","","Marianna.Pensky@ucf.edu","4000 CENTRAL FLORIDA BLVD","ORLANDO","FL","328168005","4078230387","MPS","1269, 7454, 7714","8007, 8091, 9179","$0.00","Stochastic networks are observed in many domains including biology, sociology, genetics, ecology, information technology, and national security.  While many applications involve temporal network data, the research related to dynamic networks has been relatively limited in scope.  The goal of the project is to fill this gap through the development of statistically sound and computationally viable approaches for studying time-dependent networks.  Although the research is largely methodological, the resulting techniques can be used in a variety of fields including medicine, molecular biology, statistical genetics, national security, and the social sciences.  In particular, the proposed theories and algorithms will be applied to the analysis of brain networks associated with epilepsy disease, in collaboration with the Functional Brain Mapping and Brain Computer Interface Lab at the Florida Hospital for Children.  Since the project presents an integrated approach merging applications and theory, the results will be greatly beneficial for a variety of fields that rely on analysis of dynamic stochastic network data.  Applications include (a) methods for understanding connections between brain regions associated with speech, resulting in safer and more efficient epileptic treatment options; (b) tools for analysis of time-dependent connections between brain regions associated with particular diseases; (c) techniques for analysis of the enzymatic influences between proteins and temporal gene networks; and (d) detection of terrorist or hacker groups on the basis of dynamic social media data.  Educational and training activities include development of Special Topics graduate courses, training of graduate students, and organization of interdisciplinary seminars.  The PI plans to promote diversity through participation in the Women in Science and Engineering (WISE) program.  <br/><br/>The objective of the project is the development of nonparametric techniques for the analysis of temporal networks that require a few simple assumptions on the network, and preserve continuity of the network's structure in time.  Although approaches developed for a time independent network can be applied to a temporal network frame-by-frame, they totally ignore continuity of the network structure and parameters in time.  In addition, the majority of research investigating temporal network models assumes specific mechanisms for changing nodes' memberships as well as parametric forms for the connection probabilities. Modern algebraic techniques will be used to simplify the model, and precision guarantees via oracle inequalities and minimax studies obtained.  The research will substantially advance the fields of non-parametric statistics in general, and the emerging field of network data analysis in particular.  The project will significantly broaden the range of methods applicable to the analysis of time-varying network data by developing techniques for non-parametric estimation and clustering that require few simple nonparametric assumptions, are computationally viable, and have guarantees of high precision."
"1712418","Statistical Inference for Large-Scale Structured Data with Dependence and Non-Stationarity","DMS","STATISTICS","08/01/2017","05/06/2019","Chunming Zhang","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","12/31/2020","$125,000.00","","cmzhang@stat.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","This project aims to address some challenging research problems, with a common theme of exploiting spatial-temporal dependence and non-stationarity in large-scale structured data, emerging from scientific studies in biology, genetics, astronomy, economics, neuroscience, geophysics, and meteorology among others. New tools will be developed for stochastic modeling, computational algorithms, and statistical inference applied to medical imaging data such as functional magnetic resonance imaging, neuron spike trains, genome-wide association studies, and building faster file access systems. It is anticipated that these new developments will allow scientists to efficiently analyze data with significantly increased flexibility and accuracy, and thus will have direct impacts on these applications to science, public health, and information technology. The research will also be integrated with educational practice through development of a seminar course on new statistical methods for analyzing spatially and temporally correlated imaging data. <br/><br/>The project focuses on adequate accommodation of three fundamentally different types of spatial-temporal dependence structures. The research results aim to bridge the gap between the limited theory and methodology currently available and the broad and challenging scientific problems encountered in many applied fields. Motivated by fMRI data analysis, Project 1 explores a new semi-parametric inference procedure applicable to a broad class of ""non-stationary non-Gaussian temporally dependent"" error processes for time-course data collected at a given spatial point. A new test statistic will be developed, and its asymptotic properties will be established. Large-scale multiple testing tasks often exhibit dependence, and accounting for the dependence among individual non-Gaussian test statistics is an important, challenging, but unsolved problem in statistics. Motivated by challenges in detection of activated brain regions from fMRI studies and assessing association between single-nucleotide polymorphisms and a disease from GWAS studies, Project 2 proposes a new multiple testing framework for a diverging number of ""correlated chi-squared test statistics with an arbitrary dependence structure."" Motivated by applications in multiple neural spike train recordings with dependence in both time and space, Project 3 develops new integrative methods for learning the ""sparse network structured dependence"" among nodes underlying a wide class of multivariate point process data with non-stationary event times."
"1654076","CAREER: Nonconvex Optimization and Identifiability with Applications to Medical Imaging","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2017","08/10/2021","Rina Barber","IL","University of Chicago","Continuing Grant","Yong Zeng","06/30/2023","$400,000.00","","rina@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269, 8048","1045","$0.00","Modern large-scale data sets arising in the physical and biological sciences often exhibit complex features that do not fit into the framework of existing methodologies, preventing the information in the gathered data from being fully utilized. In medical imaging, computed tomography (CT) scans and positron emission tomography (PET) scans are often best represented with models that are complex, beyond the scope of most available computational approaches, as existing methodology and theoretical analysis are mostly restricted to simpler classes of optimization problems. Since CT and PET scans come with a cost of a small radiation dose to the patient, better models for the data obtained by these imaging devices would result in a better tradeoff between the risk due to radiation and the benefit of obtaining a precise image for effective diagnosis and treatment. This research project will study a broad framework for complex optimization problems, applicable to medical imaging and across a range of problems in the physical and biological sciences, providing concrete methods and guarantees for many problems arising in these fields. The developed tools will be implemented on specific image reconstruction problems in CT and PET imaging, through collaborations with medical imaging researchers who will provide actual scan data, with the goal of enabling greater diagnostic accuracy for these popular clinical tools. Methods and code developed under this project will all be made publicly available. Throughout, the investigator will mentor students interested in working at the intersection of high-dimensional statistics, optimization, and medical imaging, and will increase interaction and communication across these fields through new courses and new collaborations.<br/><br/>Statistical problems arising in many modern applied fields often exhibit a range of challenging features, including non-convexity and non-differentiability, that pose significant challenges for high dimensional optimization and theoretical analysis. The proposed research will explore complex non-convex optimization and identifiability problems to develop methodology and theory for a broad range of problems facing applied researchers in practice. The research will study and develop algorithms that adapt techniques such as sparse or low-rank optimization, primal/dual methods, and alternating minimization or alternating descent, with the aim of achieving efficient empirical performance and broad theoretical convergence guarantees. The resulting methods will be adapted to address concrete problems in medical imaging, where noisy side information must be incorporated into the reconstructed image, and where the image representation is confounded by additional parameters modeling the imaging device."
"1713011","Reproducing Kernel Hilbert Space Embedding of Measures: Theory and Applications to Statistical Learning","DMS","STATISTICS","07/01/2017","08/02/2019","Bharath Sriperumbudur","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","06/30/2021","$178,251.00","","bks18@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","The statistical estimation and inference questions considered in this project appear in many areas of science and engineering that rely on statistical research. Some of these areas of high societal impact include social and behavioral research, forensic sciences, early detection of covert communications and breaches in data security, early-warning systems for identifying outbreaks of infectious diseases, cognitive development studies in children, and drug discovery, among many others. In many of these areas, commonly-used statistical methods make strong assumptions about the data-generating distribution. Such simplistic approaches may be unjustified; moreover, even if these assumptions make sense, they need to be tested. This research project investigates a powerful alternative to these existing methods and aims to develop a fundamental theoretical understanding of the same, leading to novel statistical applications. Code for algorithms that result from this project will be made publicly available for ready use.<br/><br/>The kernel method is a class of statistical methodology that has gained popularity in statistical learning due to its ability to handle both high-dimensional and non-Euclidean data. The core idea of the method is to map observed data to a function space, called the reproducing kernel Hilbert space (RKHS), which allows capture of non-linear relationships in the data. This project concerns theoretical and methodological research on a generalization of this method by embedding probability measures in an RKHS. This generalization has wide applicability in statistical learning problems such as nonparametric hypothesis testing, density estimation, and regression on distributions, which will be explored in this project. On the theoretical front, the characterization of injectivity of kernel embedding will be considered. While such a characterization is well understood for kernels defined on locally compact Abelian groups and compact non-Abelian groups, this project will investigate the injectivity of the kernel embedding for non-standard spaces such as nuclear spaces, the space of graphs, and the positive definite cone. The injectivity of the embedding is known to be related to the richness of the RKHS in approximating a certain class of functions. The research will investigate the rate of this approximation, which turns out to be critical in analyzing the convergence rates of kernel-based regression and density estimators and separation rates in hypothesis testing. An injective embedding induces a metric, called the kernel distance on the space of probabilities, which is defined as the RKHS distance between the kernel embeddings of two probability measures. The investigator will study the relation of kernel distance to other probability metrics such as the energy distance, distance covariance, f-divergence, and integral probability metrics in order to understand the statistical/computational (dis)advantages associated with these distances. These theoretical studies have an applied counterpart, wherein the RKHS embedding plays a critical role in the problems of regression on probability measures and density estimation in infinite dimensional exponential families. For these problems, the investigator plans to develop computationally efficient estimators with theoretical guarantees. Overall, the project aims to develop a comprehensive theory of RKHS embedding of probability measures with applications to problems in statistical learning."
"1711952","Covariate Balancing in Missing Data and Observational Studies","DMS","STATISTICS","08/15/2017","08/14/2019","Kwun Chuen Gary Chan","WA","University of Washington","Continuing Grant","Gabor Szekely","07/31/2021","$125,000.00","","kcgchan@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","","$0.00","Missing data are ubiquitous in practical data analysis due to the study design or a subject's refusal to participate.  An important related problem is the evaluation of non-randomized interventions, which can also be framed as a missing data problem.  Together, missing data methods have important applications in biomedical sciences, economics, social sciences, and business.  With practical usage in mind, the PIs will develop statistical methodologies that are simple to implement, easy to understand, and yet versatile enough to be applicable to a wide range of practically-motivated problems.<br/><br/>A general covariate balancing framework will be studied, which directly mimics missing completely at random and randomized trials in the case of observational studies, and has a better nonparametric flavor than existing model-based methods.  The investigators will extend the scope of covariate balancing to various estimands and data structures, propose a unified framework for attaining uniform approximate balance in a reproducing kernel Hilbert space, and disseminate the findings to applied statisticians and non-statisticians."
"1654579","CAREER: Utilizing Geometry for Statistical Learning and Inference","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2017","06/23/2021","Lizhen Lin","IN","University of Notre Dame","Continuing Grant","Yong Zeng","06/30/2023","$400,000.00","","lizhen.lin@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","1269, 8048","1045, 8091","$0.00","The  goal of the project is to study the fundamental role of geometry in statistics and utilize it for learning and inference.  More specifically, the investigator proposes to (1) study the role of geometry in statistical inference of complex data,  in particular manifold-valued data that are now routinely collected in many fields of science and engineering; and (2) investigate the role of geometry in high-dimensional data analysis where the data generating process  often centers around some lower-dimensional geometric space.  The central theme of this program is that  geometry is inherently present in the data  with the geometry either known or to be learned,  which should be utilized for efficient and reliable statistical learning and inference. The investigator aims to make fundamentally mathematical, statistical and algorithmic advances in complex data analysis and high-dimensional data analysis. In addition, the investigator proposes a comprehensive and detailed educational and training program for graduates students, undergraduate students as well as high school students that is integrated into the research program.<br/><br/>Modern data of complex nature are routinely being collected in many scientific fields. One example is from diffusion tensor imaging (DTI) of neuroimaging, which obtains local information of neural activity through  3 by 3  positive definite matrices. DTI has clinical applications in the study and treatment of neurological disorders such as schizophrenia, as well as in detecting subtle abnormalities related to a variety of diseases (including stroke, multiple sclerosis, dyslexia). Other examples of complex data include digital images in machine vision, where a digital image can be represented by a set of landmarks, forming certain shapes.  One may also encounter data that are stored in complex forms such as subspaces, surfaces, curves, and networks. The investigator will characterize the structure or geometry of complex data, and incorporate the geometry in developing valid statistical models for inference. In addition to complex data, it is also a common practice  to collect high-dimensional data across many disciplines such as biology, public health and neuroscience. Being able to learn the often lower-dimensional geometry of the  high-dimensional data is essential for accurate statistical inference and decision making in society. The investigator will utilize the geometry in developing valid statistical methods, which will be applied to medical data and neuroscience data, which has potential far-reaching impact in applied fields. In particular, the practical impact of the statistical methodologies from the project will be evaluated in the context of the Alzheimer's Disease Neuroimaging Initiative (ADNI) database, and an Attention Deficit Hyperactivity Disorder (ADHD) data set. By completing the proposed program, the investigator expects to better serve society and advance science by applying the developed models and methods in important fields such as  medical diagnostics by enabling accurate prediction, classification or detection of diseases or brain disorders."
"1712717","Collaborative Research:   Adaptive Testing and Rare-Event Analysis of High-Dimensional Data","DMS","STATISTICS","08/15/2017","08/16/2019","Gongjun Xu","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","07/31/2020","$120,000.00","","gongjun@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","8091","$0.00","This project aims at developing adaptively powerful testing procedures for high-dimensional data with applications in genetics, genomics and neuroimaging.  Due to recent biotechnological advances, large amounts of high-throughput and high-dimensional molecular and imaging data have been collected, resulting in a number of new and challenging statistical questions.  One question is how polygenic testing in genome-wide association studies (GWAS) may be used to answer whether some of the millions of genetic variants are associated with a complex disease like Alzheimer's disease.  The answer to this question is important to uncovering disease-related genes, and thus developing effective prevention and treatment strategies. The focus on rigorous hypothesis testing to avoid false discoveries, while maximizing the chance for true discoveries, is critical to modern genetic, genomic and other omic studies.  The methods will be applied to data related to Alzheimer's disease, for which currently there is no cure, and more powerful analysis methods are urgently needed to unravel the underlying biology. Graduate students will be involved in the conduct of the research and development of the computational tools, and publicly available software packages will be developed for use by other biomedical researchers.<br/><br/>This research will advance the frontiers of modern statistical methodology in hypothesis testing with high-dimensional data and related rare event assessment. Powerful adaptive methods for testing high-dimensional mean parameters in generalized linear models as well as high-dimensional covariance matrix structures will be developed.  The adaptive test statistics are constructed based on high-dimensional high-order von Mises V-statistics and U-statistics, and will provide uniformly high power against sparse, dense, as well as moderately sparse or dense signals for flexible asymptotic regimes. Another thrust of the research deals with the challenging and important rare-event estimation problem in analysis of genome-wide molecular and neuroimaging data, where a high stringent statistical significance level is usually needed. To evaluate such small probabilities, the research will lead to theoretical tail probability approximations as well as efficient Monte Carlo methods using non-standard change-of-measure techniques."
"1713015","Summer Research Conference in Statistics and Biostatistics","DMS","STATISTICS","05/01/2017","04/18/2017","Arnold Stromberg","KY","University of Kentucky Research Foundation","Standard Grant","Nandini Kannan","04/30/2018","$21,315.00","","stromberg@uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","1269","7556, 9150","$0.00","The Summer Research Conference in Statistics and Biostatistics will be held in Jekyll Island, Georgia on June 4-7, 2017. This meeting is the 53rd in a sequence of annual conferences. Its purpose is to encourage the interchange and mutual understanding of current research ideas in statistics and biostatistics, and to give motivation and direction to further research progress. The conference focuses on young researchers, placing them in close proximity to leaders in the field for person-to-person interactions in a manner not possible at most other meetings. Speakers will present formal research talks with adequate time allowed for clarification, amplification, and further informal discussions in small groups. With the travel support provided by this award, students will attend and present their research in posters to be reviewed by more experienced researchers.<br/><br/>The Southern Regional Council on Statistics (SRCOS) is a consortium of statistics and biostatistics programs in the South, with member programs at 45 universities in 16 states in the region. This project will fund student participation in the 2017 Summer Research Conference (SRC) sponsored by SRCOS. The meeting is particularly valuable for students and faculty from smaller schools in the region, affording them the opportunity to participate and interact closely with internationally-known leaders in the field without the cost of travel to distant venues. It will strengthen the research of the statistics and biostatistics community as a whole, and particularly in the relatively underdeveloped southern region. The conference's five session topics are Complex Spatial Environmental Problems, Statistical Education, Functional Data Analysis, Causal Inference, and Integrative Omics. More details can be found at the conference web site http://web1.sph.emory.edu/srcos/."
"1711226","Collaborative Research:  Adaptive Testing and Rare-Event Analysis of High-Dimensional Data","DMS","STATISTICS","08/15/2017","08/16/2019","Wei Pan","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","07/31/2021","$199,987.00","","weip@biostat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","8091","$0.00","This project aims at developing adaptively powerful testing procedures for high-dimensional data with applications in genetics, genomics and neuroimaging.  Due to recent biotechnological advances, large amounts of high-throughput and high-dimensional molecular and imaging data have been collected, resulting in a number of new and challenging statistical questions.  One question is how polygenic testing in genome-wide association studies (GWAS) may be used to answer whether some of the millions of genetic variants are associated with a complex disease like Alzheimer's disease.  The answer to this question is important to uncovering disease-related genes, and thus developing effective prevention and treatment strategies. The focus on rigorous hypothesis testing to avoid false discoveries, while maximizing the chance for true discoveries, is critical to modern genetic, genomic and other omic studies.  The methods will be applied to data related to Alzheimer's disease, for which currently there is no cure, and more powerful analysis methods are urgently needed to unravel the underlying biology. Graduate students will be involved in the conduct of the research and development of the computational tools, and publicly available software packages will be developed for use by other biomedical researchers.<br/><br/>This research will advance the frontiers of modern statistical methodology in hypothesis testing with high-dimensional data and related rare event assessment. Powerful adaptive methods for testing high-dimensional mean parameters in generalized linear models as well as high-dimensional covariance matrix structures will be developed.  The adaptive test statistics are constructed based on high-dimensional high-order von Mises V-statistics and U-statistics, and will provide uniformly high power against sparse, dense, as well as moderately sparse or dense signals for flexible asymptotic regimes. Another thrust of the research deals with the challenging and important rare-event estimation problem in analysis of genome-wide molecular and neuroimaging data, where a high stringent statistical significance level is usually needed. To evaluate such small probabilities, the research will lead to theoretical tail probability approximations as well as efficient Monte Carlo methods using non-standard change-of-measure techniques."
"1748166","CAREER: Flexible Parsimonious Models for Complex Data","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2017","05/10/2021","Jacob Bien","CA","University of Southern California","Continuing Grant","Pena Edsel","06/30/2022","$400,000.00","","jbien@marshall.usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","MPS","1269, 8048","1045","$0.00","Researchers throughout academia, industry, and government are generating data at scales and levels of complexity far beyond what could previously have been imagined.  Complex data demand statistical models that are sufficiently flexible to adapt to meaningful, underlying signals, allowing scientists to discover unexpected patterns.  Yet as society relies more heavily on statistical algorithms to make decisions impacting everyday life, it becomes increasingly important for a method's output to be interpretable by non-experts.  This demands parsimony: that simpler explanations be favored over more complicated ones.  For example, the Internet has led to unprecedented quantities of data in the form of text (such as articles, blogs, webpages, consumer reviews, and many other social media products).  Such text data represent a potential treasure trove of insights into the world -- what people are thinking, how this is changing over time, how this varies by location, etc. The investigator develops new statistical methods for overcoming major technical challenges to gleaning useful information from this data.  This same methodology can be applied to the study of the microbiome, the vast community of microbes living in an environment such as the human gut.  Better statistical methods are needed to identify types of microbes in the gut that play a crucial role in human health and disease.  Another problem that is tackled in this project involves modeling data collected over time (such as wind-speed data and wildlife monitoring).  The methods that are developed allow for more accurate forecasting, which is crucial in many areas including health and medicine and the development of lower cost energy systems.  The last major area in this project is devoted to making the process of statistical research more efficient and its software of higher quality and easier to share across the community of statistical researchers.  Finally, all three research objectives are closely integrated with educational outcomes, including the supervision and teaching of graduate students, outreach to non-statisticians and non-scientists, and the release of undergraduate-accessible mini-papers describing the investigator's new research findings.<br/><br/>This project focuses on the design of new statistical methods that balance two important and often opposing needs: flexibility and parsimony.  (1) Building predictive regression and classification models is difficult when the features are highly sparse.  While many methods focus on the challenge of high dimensionality, relatively few have considered the obstacle posed by features that are rarely nonzero. The investigator develops a new framework for feature selection when the features are highly sparse that succeeds where preexisting methods fail.  This is studied both from theoretical and computational standpoints.  (2) High-dimensional covariance estimation and time series modeling are two rich, but largely distinct, areas in statistics, which the investigator combines to develop new methods for modeling locally stationary time series.  The added flexibility in going from stationarity to local stationarity must be carefully balanced with parsimony.  (3) A series of area-specific software modules will be distributed freely online building on the investigator's new platform for streamlining the process of performing simulation studies.  Each module will implement some of the most common models, methods, and metrics used in a given area of statistics research.  The goal is to facilitate the sharing of high-quality, reproducible simulation code in the statistics research community by creating an easily-adaptable standardized format."
"1712535","Identification and Statistical Inference in Graphical Models with Feedback and Latent Variables","DMS","STATISTICS","08/01/2017","07/30/2019","Mathias Drton","WA","University of Washington","Continuing Grant","Gabor Szekely","07/31/2021","$125,000.00","","md5@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","","$0.00","The last decade has seen great advances in scientific experimentation.  In biology, for instance, it has become routine to collect complex data that simultaneously quantify the levels of expression of many genes or proteins.  This project addresses the development of statistical methodology that allows processing of such data to obtain insights on cause-effect relationships among the units in the studied system.  Specifically, the proposed research addresses two key challenges, namely, how to tackle problems in which some important variables remain unobserved, and how to cope with the presence of causal feedback loops.  The research develops statistical techniques to infer cause-effect relationships and expands our understanding of which conclusions may possibly be reached under imperfect information.<br/><br/>Both feedback and latent variables bring about great challenges in graphical modeling because, in their presence, consideration of conditional independence is no longer sufficient to characterize and compare models.  This project focuses on linear models that allow for refined modeling of feedback loops and/or the effects of latent variables.  The PI will develop criteria for parameter identifiability, which is no longer guaranteed with feedback or latent variables.  The work will also determine conditions for when a model is of expected dimension as given by a parameter count.  Knowledge of dimension is needed for instance when setting degrees of freedom in statistical tests.  Next, the project will lead to a better understanding of constraints other than conditional independence.  Finally, the PI will develop new model selection methods in Gaussian as well non-Gaussian models."
"1712536","An Integrated Toolkit for High-Dimensional Complex and Time Series Data Analysis","DMS","STATISTICS","06/15/2017","05/15/2017","Fang Han","WA","University of Washington","Standard Grant","Gabor Szekely","05/31/2020","$160,000.00","","fanghan@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","","$0.00","Fundamental questions about how the brain's structure evolves with age and how gene activities are controlled by transcription factors may be answered by studies involving high-dimensional data sets.  As massive amounts of medical imaging and genome sequencing data are now being collected, these questions can be investigated by combining neuroscientists' and biologists' expertise with powerful data analysis tools geared towards addressing the subtlety and uncovering hidden patterns in the data. This project aims to develop an integrated toolkit of scalable, robust, and theoretically sound nonparametric and semiparametric solutions for high-throughput estimation of complex biological systems. It addresses the resolution of two major problems.  The first problem considers massive amounts of high-dimensional complex-structured data that possess complex generating distributions and interrelationships, which often cannot be captured by simple linear systems. Such data are usually noisy and contain numerous outliers.  The second problem considers data exhibiting temporal and spatial correlations and a relatively weak signal. Assuming independent and identically distributed data could lead to erroneous estimation and prediction, giving rise to inaccurate interpretation of biological systems. This project aims to provide methods to solve both of these problems.<br/><br/>This research project puts forward new methods for effective analysis of biological systems, handling the aforementioned challenges in a unified fashion. One essential feature is the concept of large-scale robust nonparametric/semiparametric inference. In particular, the project aims to develop an integrated toolkit of methods that are: (1) easily scalable to high-dimensional data with a large sample size; (2) robust to data modeling assumptions and different kinds of data contaminations; (3) built in a nonparametric or semiparametric sense, where the corresponding generative models contain infinite-dimensional components that capture the data information or subtlety as much as possible. To illustrate, the investigator intends to construct, explore, and apply high dimensional generalized regression models, (generalized) partially linear models, shape-constrained regression models, and copula time series models, among others, to unveil hidden patterns in biological systems. The methods under development are designed to be optimal, namely, attaining either a nonparametric minimax or a semiparametric lower efficiency bound."
"1713078","Functional Copula Model for Nonlinear and Non-Gaussian Functional Data Analysis: Graphical Models, Dimension Reduction, and Variable Selection","DMS","STATISTICS","08/15/2017","07/30/2019","Bing Li","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","07/31/2020","$200,000.00","","bing@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","1269","$0.00","Multivariate functional data have become increasingly prevalent with the rise of big data. For example, EEG (Electroencephalography) and fMRI data, GPS tracking data collected by mobile phones, health data collected by smart wearables, some global economic data, and data collected from electric power networks, are all of this type.  The key issue in analyzing such data is to discover interrelations among the different random functions describing them. Currently, this is typically done under the statistical assumption of normal (Gaussian) distributions as this leads to computationally simple and highly interpretable estimation procedures in many applications. The Gaussian assumption, however, is quite strong and is not satisfied in many applications. The main goal of this project is to relax the Gaussian assumption while retaining its computational simplicity and high interpretability.  This is done by developing a notion of a copula Gaussian model -- a way in which multivariate functional data can be transformed to Gaussian distributed data.  Such copula tactics have been extremely versatile in classical low-dimensional settings, combining parsimony aspects of multivariate Gaussian models with the flexibility of nonparametric models.  Extending copula-based ideas to multivariate functional data will benefit statistical graphical models, statistical models for causal relations, nonlinear sufficient dimension reduction, and variable selection, bringing fresh insights and research opportunities to a young and dynamic field.<br/><br/>This project proposes a novel functional copula model for non-Gaussian and nonlinear multivariate functional data analysis. The idea is to apply rank and quantile transformations to the coefficients of the Karhunen-Loeve expansions of the random functions. The functional Gaussian copula model greatly simplifies conditional dependence among different random functions in the expansion, as the conditional distribution is completely determined by the covariance operator among random functions. In particular, smoothing over functional spaces is not needed. At the same time, the model does not require Gaussian assumptions, which can be easily violated by functional data. These properties are very useful for constructing graphical models. The functional elliptical distribution copula model is useful for sufficient dimension reduction, because it is a convenient way to meet linearity requirements posed by many commonly used dimension reduction methods.  Equipped with this parsimonious but flexible framework, the work plans to develop new methodologies for four areas of multivariate functional data analysis: (i) functional graphical models for undirected graphs; (ii) functional graphical models for causal graphs; (iii) functional sufficient dimension reduction; and (iv) variable selection for function on function regression. The work will study asymptotic properties of these new estimators under both the fixed and high dimensional setting, statistical inference procedures, order determination methods, and efficient algorithms to implement these methods."
"1712481","Hypothesis Testing in High Dimensions Without Sparsity","DMS","STATISTICS","09/01/2017","08/23/2019","Jelena Bradic","CA","University of California-San Diego","Continuing Grant","Yong Zeng","08/31/2022","$125,000.00","","jbradic@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","8083","$0.00","The exponential growth of diverse scientific data represents an unprecedented opportunity to make substantial advances in complex science and engineering, such as the discovery of novel materials or drugs. The collection and analysis of information on massive scales has clear benefits for society: it can help businesses optimize online commerce, medical workers address public health issues, and governments interrupt terrorist activities. This project aims at designing rigorous statistical framework and advanced tools for propagating and managing uncertainty in the modeling and design of complex physical and engineering systems. The ultimate goal of this project is to facilitate significant hypothesis generation and accelerate discovery by correlating data across scientific domains in systems with multi-scale and uncertain parameters in extremely high-dimensional spaces, including for example aerospace, engineering, neuroscience, gene-protein disease networks, materials science, climate science, autonomous systems, and cyber-physical systems or self-organized biological systems. This project will enable the design of systems with verifiable properties; reveal how to value the trustworthiness of results in a wide variety of applications; allow conclusions to be probed for their sensitivity to uncertainties and practical experimentation and validation.<br/> <br/>Understanding the complex and increasingly data-intensive world around us relies on the construction of robust empirical models, i.e., representations of real, complex systems that enable decision makers to predict behaviors and answer ""what-if"" questions.  Novel statistical research is needed for dealing with the underlying high dimensionality of the space of uncertain parameters, active multi-physics coupling, and uncertainty in the models themselves.  Besides, there is no fundamental theory for decision making under uncertainty for these large-scale systems. To address these needs, this project intends to develop the following capabilities: new methods for inverse modeling to scale to high-dimensional multi-scale/multi-physics systems; a quantifiable and generalizable understanding of uncertainties and inadequacies in the physical models themselves and entirely new paradigms for decision making that is extremely robust to model misspecification and assumptions. In particular, new decision theory for high-dimensional regression models will be developed that is highly and provably robust to the model misspecification, including missing, non-sparse and large-scale (exploding) data structures.  As such, these methods will be the first attempts at understanding the fundamental limitations of dense high-dimensional models. Moreover, censored analysis models (log-rank, additive hazard, proportional hazard and competing risk) will be studied under the setting where perfect estimation or variable selection is not possible; these studies will be able to bridge the gap between the practice and theory in the high-dimensional setting. Furthermore, a new framework for aggregating heterogeneous and disparate data structures that may be correlated and time-dependent will be developed. The added flexibility in analyzing disparate data going from one stationary source to many moving sources of observations must be carefully balanced with parsimony."
"1646108","RTG: Understanding dynamic big data with complex structure","DMS","STATISTICS, WORKFORCE IN THE MATHEMAT SCI","09/01/2017","08/12/2021","Elizaveta Levina","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Yong Zeng","08/31/2023","$2,500,000.00","Susan Murphy, Ambuj Tewari, Edward Ionides, Xuming He","elevina@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269, 7335","7301, 9251","$0.00","The emerging era of big data has brought with it new unique challenges in both research and training in Statistics. For the new types of statistical problems researchers now aim to solve, the size of available data has grown immensely in many cases, and the nature of the data has changed no less dramatically. Statisticians now work routinely with data that combine many different kinds of observations, from genetic data to brain images to smartphone data. This creates a need for new training approaches and their close integration with current research directions, so that PhD students and postdocs are prepared to take on new challenges as they become independent researchers. It also creates an opportunity for recruiting undergraduates into the field, increasing and diversifying the domestic STEM workforce. This project will train undergraduate and graduate students and postdocs in modern techniques for dynamic big data with complex structures, in modern teaching methods for statistics, and provide mentoring on all aspects of professional development.  <br/><br/>This project brings together three interlinked research streams:  (1) statistical network analysis, (2) inference for dynamic systems, and (3) sequential decision making.   This project will contribute to each of these areas, developing (1) realistic models for network community detection, link prediction and dynamically evolving networks, and tools for utilizing network connections to improve prediction of outcomes of interest on network-linked data; (2) practical algorithms with provably good properties for fitting complex partially observed Markov process models, with an emphasis on scalability;   (3) sequential decision making algorithms based on reinforcement learning, with the goal of achieving excellent prediction performance and discovering interpretable decision variables.   Each research stream will offer a short intensive graduate course and a regular interdisciplinary student workshop.    Equally importantly, the streams will collaborate on topics that cut across these areas, such as inference for dynamically evolving networks or the role of social connections in predicting behavior and their impact on sequential decision making.   Training undergraduates, PhD students, and postdocs in topics at the cutting edge of modern statistics will contribute to supplying much-needed statisticians and data scientists to both academia and industry, increasing and diversifying the STEM workforce.   All three research streams have broad applications to areas beyond Statistics, such as neuroimaging, infectious disease transmission, and mobile health interventions.  The project is thus expected to have wide-ranging impact on how the problems statisticians study are approached by domain scientists."
"1712591","Robust and Distributed Statistical Learning from Big Data","DMS","STATISTICS","07/15/2017","04/28/2021","Jianqing Fan","NJ","Princeton University","Continuing Grant","Yong Zeng","06/30/2023","$600,000.00","","jqfan@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","8083","$0.00","Big Data are ubiquitous in many areas of science, engineering, social sciences, and the humanities, and have significant impact in terms of technological innovation and economic development.    This project seeks to introduce effective methods for robust high-dimensional statistical inference that are insensitive to the potential poor quality of big data, and to develop distributed estimation that is needed for Big Data analysis, computing, and optimization.  The research will address several robust and distributed statistical inference problems for Big Data in genomics, genetics, neuroscience, machine learning, economics, and finance.   The project will advance our understanding of molecular mechanisms, biological processes, genetic associations, brain functions, and economic and financial risk.  Integration of research and education will be achieved through the involvement of undergraduate students, graduate students, and postdoctoral fellows, and the development of publicly available computer code for robust and distributed analysis of Big Data with sound theoretical support.  Working closely with industrial partners, the research will lead to increased collaborations between academia and industry.  <br/><br/>The project will lead to the development of novel statistical theory, methods, and algorithms for robust statistical inference from high-dimensional statistics and Big Data.  The first aim seeks to introduce a simple and widely applicable principle for robust inference via an appropriate shrinkage of observed data or loss functions.  This reduces the influence of outliers and heavy-tailed distributions, and weakens the moment conditions from sub-Gaussian distributions to bounded second moments for regression or fourth moments for covariance estimation.  The research includes plans to systematically develop the theory and methods for robust estimation of high-dimension means, and implementation of these methods to control false discovery rates in large scale inference for gene and transcripts selection, robust regularization of covariance and precision matrices, and their applications to robust principal component analysis, factor analysis and high-dimensional hypothesis testing.  In addition, robust sparse regression, model selection, and low-rank matrix recovery will also be investigated.  The second aim focuses on making the proposed robust procedures applicable to the Big Data environment via the development of distributed estimation and inference.  In particular, divide-and-conquer methods will be used to distribute the computation to node machines and to solve privacy and data ownership issues.  Approaches to reduce the information loss due to the distributed computation for likelihood based models via partial communication of the Hessian matrices will be investigated.   Two important classes of problems, trace regression and principal component analysis, will be used to illustrate the proposed methods."
"1654589","CAREER:  Nonparametric function estimation: shape constraints, adaptation, inference and beyond","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2017","07/06/2021","Adityanand Guntuboyina","CA","University of California-Berkeley","Continuing Grant","Yong Zeng","06/30/2023","$400,000.00","","aditya@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269, 8048","1045","$0.00","Nonparametric statistics is an area of statistics and machine learning that allows one to model and analyze datasets without making strong prior assumptions about the data. Data problems where the techniques of nonparametric statistics are useful come from a wide variety of applied areas including biology, medicine, astronomy, engineering, economics and operations research. In modern complex and large datasets, these methods are especially crucial as they enable the detection of important trends and patterns in the data that may be missed by traditional parametric statistical techniques. However there exist many unresolved issues concerning the theory, methodology and application of nonparametric methods in modern data problems. A systematic study of these issues will be undertaken in this project which will result in (a) an improved understanding (in terms of accuracy and uncertainty quantification) of many existing methods, and (b) novel methods and computational algorithms that will be useful to applied practitioners in the scientific areas mentioned above. Most of the proposed projects are collaborative and involve researchers from a diverse set of universities. The project also contains a well-developed plan of educational activities which will have a major impact on the education and training of undergraduate and graduate students at UC Berkeley in statistical research. In particular, many of the educational activities of the project are aimed towards undergraduate students, a group that is often given less importance at large research universities.<br/><br/>Concretely, a wide range of nonparametric models will be studied, covering both regression and density estimation. In situations where empirically attractive estimators exist, an elaborate theoretical study is proposed focusing on their adaptive risk properties. In other situations, estimators and efficient computational algorithms are proposed together with an analysis of their accuracy. Important practical problems of inference and uncertainty quantification are also addressed. The specific regression problems that are investigated in this project include (a) multivariate convex regression, univariate trend filtering and additive shape constrained regression where adaptive risk properties of the natural estimators will be established, (b) multivariate trend filtering and quasi-convex regression where new estimators are provided along with efficient computational algorithms, and (c) global and pointwise inference in shape constrained estimation where uncertainty quantification will be addressed. In density estimation, the problems investigated include: (a) log-concave and mixture density estimation where maximum likelihood estimators will be studied, (b) distributionally robust optimization and nongaussian component analysis where novel methodology will be proposed based on shape-constrained density estimation, and (c) robust approaches to shape-constrained inference where new procedures will be developed."
"1653404","CAREER:  Bayesian Generalized Shrinkage: An Encompassing Model Approach","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2017","07/01/2021","Anirban Bhattacharya","TX","Texas A&M University","Continuing Grant","Yong Zeng","06/30/2023","$400,000.00","","anirbanb@stat.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269, 8048","1045","$0.00","With ever-increasing complexities of datasets collected in diverse application areas, statisticians are compelled to entertain a plethora of different statistical models with varying degrees of complexity. A primary motivation for this research is to broaden the scope of classical shrinkage approaches to model selection and inferential goals in new statistical problems by providing computationally efficient and statistically accurate solutions. The methodology developed in the project has broad applications ranging from genetic and epidemiological studies to communication networks. The principal investigator is committed to increased interactions and collaborations with the broader scientific community to maximize the impact of the statistical methodology developed through this project. The deliverables of the project include user-friendly software packages that enable users in the scientific community to analyze structured data sets in applications relevant to the project goals.<br/><br/>The principal investigator proposes a class of generalized shrinkage methods for model selection and inference in structured high-dimensional models. Operating in a Bayesian framework, the proposed approach shrinks the parameters of an encompassing model towards a family of sub-models. Given a class of statistical models, the principal investigator defines an encompassing model as one which assigns a positive prior probability to arbitrarily small neighborhoods of any model in the specified class, with the neighborhoods defined in terms of some general statistical divergence measure such as the Kullback-Leibler divergence. This general formulation enlarges the scope of traditional shrinkage mechanisms by employing shrinkage towards potentially non-nested models. Model selection is performed by post-processing the posterior summaries from the encompassing model. The principal investigator develops novel post-processing schemes for variable selection and clustering, tensor decompositions, tree-structured models and network topology identification as various applications of the general methodology. The principal investigator formulates information theoretical techniques for studying non-asymptotic frequentist behavior of the encompassing posterior distributions, with specific emphasis on providing theoretical support for the model selection procedures developed in this project."
"1712943","Scalable Methods for Classification of Heterogeneous High-Dimensional Data","DMS","STATISTICS","07/01/2017","05/11/2017","Irina Gaynanova","TX","Texas A&M University","Standard Grant","Gabor Szekely","06/30/2020","$162,539.00","","irinag@stat.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","8083","$0.00","Recent technological advances have enabled routine collection of large-scale high-dimensional data in the biomedical fields. For example, in cancer research it is common to use multiple high-throughput technology platforms to measure genotype, gene expression levels, and methylation levels. One of the main challenges in the analysis of such data is the identification of key biological measurements that can be used to classify the subject into a known cancer subtype.  While significant progress has been made in the development of computationally efficient classification methods to address this challenge, existing methods do not adequately take into account the heterogeneity across the cancer subtypes and the mixed types of measurements (binary/count/continuous) across technology platforms.  As such, existing methods may fail to identify relevant biological patterns. The goal of this project is to develop new classification methods that explicitly take into account the type and heterogeneity of measurements. While the primary focus is on methodology, high priority will be given to computational considerations and software development to encourage dissemination and ensure ease of use for domain scientists.  <br/><br/>Regularized linear discriminant methods are commonly used for simultaneous classification and variable selection due to their interpretability and computational efficiency. These methods, however, rely on unrealistic assumptions of equality of group-covariance matrices and normality of measurements. This project aims to address the limitations present in current discriminant approaches, and has three objectives: (1) to develop computationally efficient quadratic classification rules that perform variable selection; (2) to generalize the discriminant analysis framework to non-normal measurements; (3) to develop a classification framework for mixed type data coming from multiple technology platforms collected on the same set of subjects. The key methodological innovation is the combination of sparse low-rank singular value decomposition, which enables computational efficiency, with geometric interpretation of linear discriminant analysis, which allows for the construction of nonlinear classification rules by redefining the space for discrimination."
"1748175","Quantitative Analysis of Higher Order Chromatin Interactions","DMS","STATISTICS, MATHEMATICAL BIOLOGY, CDS&E-MSS","09/01/2017","08/14/2017","Xihong Lin","MA","Harvard University","Standard Grant","Nandini Kannan","08/31/2018","$30,000.00","","xlin@hsph.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269, 7334, 8069","7556","$0.00","The Program in Quantitative Genomics (PQG) at Harvard T.H. Chan School of Public Health will host the 2017 conference, ""Quantitative Analysis of Higher Order Chromatin Interactions"", November 2-3, 2017 at the Joseph B. Martin Conference Center at Harvard Medical School in Boston, MA. This is the eleventh in a very successful conference series on emerging statistical and computational issues in genetics and genomics. The impetus for this year's theme comes from the increasing amount of data that provide information on the nuclear organization of the human genome and its applications. A series of ""chromatin confirmation capture"" techniques, such as Hi-C, have been developed for identifying three-dimensional interactions between all pairs of genomic loci at once. Profiling such three-dimensional interactions is critical for a full understanding of gene regulation.  With increasingly refined technology and decreasing sequencing costs, it is now possible to generate high-resolution datasets with close to a billion sequencing reads, on the same order as human whole-genome sequencing.  The scientific community is on the verge of generating massive amounts of Hi-C and related types of data. The conference will deal with key aspects in analyzing and interpreting these large and complex datasets. The conference is open to the whole research community and particularly encourages participation of junior faculty and researchers, postdoctoral fellows, students, and women and minorities. The participants will discuss and critique existing quantitative methods, discuss in-depth emerging statistical and quantitative issues, and identify priorities for future research in the analysis of higher order chromatin interaction data. The research presented will be broadly disseminated in publications in scientific journals and websites.  <br/><br/>The conference will focus on the following three topics of critical importance in quantitative analysis of higher order chromatin interaction data: (1) emerging technologies; (2) computational challenges in high order chromatin data; and (3) applications to basic biology and disease mechanisms.  A key feature of the conference is to provide a timely and interactive platform for cross-disciplinary senior and junior investigators, including statistical geneticists, computational biologists, and biologists, to discuss these analytic challenges. For more information, visit https://www.hsph.harvard.edu/2017-pqg-conference/."
"1916787","CAREER: Scalable methods for discovering multivariate dependencies in high dimensional data.","DMS","STATISTICS, Division Co-Funding: CAREER","08/28/2017","09/14/2022","Balakanapathy Rajaratnam","CA","University of California-Davis","Continuing Grant","Yong Zeng","06/30/2023","$293,153.00","","brajaratnam01@gmail.com","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269, 8048","1045","$0.00","This proposal aims to develop principled methods for discovering multivariate dependencies which cater to ultra high dimensional settings. A common theme that unites the proposed methods is scalability and identification of their limitations. A popular approach to identifying sparse inverse covariance matrices is through penalized likelihood methods. We propose a novel approach for solving the penalized Gaussian log-likelihood that is faster than its competitors by many orders of magnitude. The second research component in the proposal investigates the statistical properties of thresholded matrices in finite samples, with a view to obtaining a positive definite covariance estimation method which is highly scalable. The third research aspect of the project investigates quantifying the variability and uncertainty of estimated graphical network models. A methodology that takes advantage of a convex pseudo-likelihood formulation of the graphical model selection problem is introduced. This allows for the development of a highly scalable uncertainty quantification method with theoretical safeguards. The fourth research aspect of the project examines the use of the methodology proposed in the previous three sub-components to an application in the area of climate change, where high dimensional covariance estimation is required. The proposal also has a significant teaching and outreach component which aims to introduce statistics to aspiring young scientists at various stages of their undergraduate and graduate studies.<br/><br/>The availability of high-throughput data from  various applications, including genomics, environmental sciences and others,  has created an urgent need for methodology and tools for analyzing high dimensional data. Extracting and making sense of the many complex relationships and multivariate dependencies in the data and developing principled inferential procedures is one of the major challenges facing statisticians and data scientists. The theoretical and methodological work proposed in this project is motivated by applications and interdisciplinary collaborations in fields as diverse as the earth and environmental sciences, genomics and cancer research, and the social sciences. In genomics for instance, one is often interested to know how various genes are associated, and how these associations differ between an experimental (diseased) and control group. Gene regulatory networks also serve as important tools to study the evolutions of diseases. In the context of the climate change debate, modeling temperature at different points on the globe requires parsimonious modeling of the way in which these variables are related. Modeling correlations also arises naturally in material sciences and engineering where one is interested in seeing how different atomic particles interact when new materials are produced.  Hence the proposed project for estimating correlations in very high dimensional settings will  have widespread applications, since understanding associations/relationships between many variables is an endeavor that is common to many scientific disciplines. The proposed work, though firmly rooted in the statistical sciences, is very much interdisciplinary, and involves collaborations and partnerships between statisticians/data scientists and biomedical scientists, engineers and earth scientists."
"1712826","Developing New Frontiers in Functional Data Analysis","DMS","STATISTICS","07/01/2017","06/19/2017","Matthew Reimherr","PA","Pennsylvania State Univ University Park","Standard Grant","Gabor Szekely","06/30/2020","$190,347.00","","mreimherr@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","1269","$0.00","This project is concerned with developing statistical tools for several types of data that have become increasingly common during the big data revolution occurring in the sciences.  The first type of data are called manifolds, which are low-dimensional structures in higher-dimensional spaces, and occur frequently in biomedical imaging studies.  The second type of data consists of large numbers of predictors as are commonly found in genetic studies.  The goals of this project are to develop statistical tools to analyze samples of manifolds or functions, as outcomes, alongside large numbers of scalar predictors.  In addition, due to the potential for privacy breaches in such data, the project also aims to develop privacy mechanisms that guarantee the anonymity of subjects when statistical summaries and analyses of these data are released.  This work will have broad impacts in both statistics and bioinformatics.            <br/><br/>The project develops statistical methods for complex high-dimensional data using techniques from functional data analysis (FDA).  The research is divided into three areas: manifold data analysis (MDA), high-dimensional functional regression, and statistical disclosure control (SDC).  MDA, which is the focus here, combines FDA and manifold learning techniques to analyze statistically samples of manifolds.  Methods for high-dimensional functional regression will be developed for handling large numbers of scalar predictors alongside functional/manifold outcomes; the tools will exploit smooth and sparse structures for increased statistical performance.  Lastly, SDC methods for FDA will be developed.  Privacy remains a central concern for researchers and society as a whole, but very little has been done for functional data, which may contain substantial amounts of individual level information.  Functional data require carefully constructed privacy mechanisms to ensure that anonymity is maintained while preserving the scientific import of the objects.  These three areas are motivated by joint anthropological work, ADAPT, analyzing high-resolution 3D facial images, with data consisting of thousands of subjects, thousands of measurements per face, and hundreds of thousands of genetic markers.  The goal is to uncover the genetic architecture of the human face, and to better understand the ancestry of different facial features.  The developed methods will be applied to this example, theoretical results will be derived, and sophisticated software will be provided."
"1712554","Collaborative Research: Statistical Inference Using Random Forests and Related Methods","DMS","POP & COMMUNITY ECOL PROG, STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2017","07/19/2017","Giles Hooker","NY","Cornell University","Standard Grant","Gabor Szekely","08/31/2021","$215,078.00","","ghooker@wharton.upenn.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1182, 1269, 7454","8007, 8083","$0.00","This project seeks to develop methods to quantify uncertainty in machine learning algorithms and <br/>to incorporate machine learning and statistical inference. Machine learning has been enormously <br/>successful at using data to make predictions; it is used in an extensive range of applications <br/>from handwriting recognition to high frequency trading to driverless cars and personalized medicine. <br/>However, while machine learning algorithms make good predictions, they tell humans very little <br/>about how those predictions were arrived at: What were the important factors? How did they affect <br/>the prediction? They also don't distinguish predictions for which there is a lot of information <br/>about the probability of different outcomes (even if that covers a wide range) from those where <br/>very little information is available. For example, a machine learning algorithm may very accurately <br/>predict whether a person is likely to develop diabetes, but provides little if any information <br/>regarding how that person might lower his or her risk. This project will build on initial <br/>mathematical theory to develop methods to explain how Random Forests arrive at their predictions <br/>and how statistically confident those predictions are, and produce ways to link machine learning <br/>methods to other statistical models.<br/><br/>This project seeks to develop methods to quantify uncertainty in machine learning algorithms <br/>and to incorporate machine learning and statistical inference. The project will extend on a <br/>theoretical framework representing Random Forests as U-statistics to produce a practical <br/>implementation of statistical uncertainty quantification in machine learning. In particular, <br/>it will improve on methods to estimate sample variability in Random Forest predictions, develop <br/>computationally efficient screening tools for covariate and interaction selection, and incorporate <br/>ensemble methods as non-parametric terms in partially-linear models while retaining statistical <br/>inference via a modified boosting algorithm. These methods will be demonstrated on a citizen <br/>science data base in ornithology and in various biomedical applications."
"1713129","Collaborative Research:  Statistical Inference for High-Frequency Data","DMS","STATISTICS","07/01/2017","05/24/2017","Per Mykland","IL","University of Chicago","Standard Grant","Pena Edsel","06/30/2022","$204,436.00","","mykland@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","8083","$0.00","To pursue the promise of the big data revolution, the current project is concerned with a particular form of such data, high frequency data (HFD), where series of observations can see data updates in fractions of milliseconds. With technological advances in data collection, HFD occurs in medicine (from neuroscience to patient care), finance and economics, geosciences (such as earthquake data), marine science (fishing and shipping), and other areas. The research focuses on how to extract information from complex big data and how to turn data into knowledge. In particular, the project aims to develop cutting-edge mathematics and statistical methodology to uncover the dependence structure governing a HFD system. The new dependence structure will permit the ""borrowing"" of information from adjacent time periods, and also from other series from a panel of data. It is expected that the results will lead to more efficient estimators and better prediction and that this approach will form a new paradigm for HFD. In addition to developing a general theory, the project is concerned with applications to financial data, including risk management, forecasting, and portfolio management. More precise estimators, with improved margins of error, will be useful in all these areas of finance. The results are expected to be of interest to investors, regulators, and policymakers, and the results are entirely in the public domain. <br/><br/>The goal of this project is to create a unified framework for inference in high frequency data, based on dividing the observations and the parameter process into blocks. The work pursues two paths, both involving the fundamental structure of the data architecture. A ""within block"" approach uses contiguity to make the structure of the observations more accessible in local neighborhoods. The ""between block"" approach sets up a tool for using stochastic calculus to study the relationship between parameters in blocks that are adjacent (in time and space). It also permits the integration of high and low frequency models. This is achieved without altering current models. A final part of the project is devoted to further study of the observed asymptotic variance, in particular work on tuning parameters and inferential interpretation. Both the ""within block"" and ""between block"" approaches are formulated to cover general time varying ""parameters"" that are usually estimated from high frequency data series, not only volatility, but also skewness (leverage effect), regression coefficients, and parameter dynamics (such as volatility of volatility). In both cases, the observed data and also parameter processes may have large dimension (large panel size) in addition to high frequency observation. The within block approach permits contiguity to be stated jointly for the latent underlying processes and the microstructure/observation noise. For the between block approach, the investigators will further develop a new way to look at the dependence relationships between the parameters."
"1712657","Level Crossing of Likelihood Functions in Sequential Decision Problems and Statistical Learning","DMS","STATISTICS","07/01/2017","06/28/2017","Xiaoou Li","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","06/30/2021","$128,988.00","","lixx1766@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","The last decade has witnessed remarkable progress in statistical methods for applications arising from various fields including education, psychology, finance, and engineering. The properties of many of these new methods remain unclear, calling for theoretical insights. This research project aims at (1) studying the theoretical underpinning of effective methods in statistical learning and sequential decision making and (2) developing new methods with provably efficiency and reliability.  This research will not only address a class of fundamental problems in statistics but also have a positive impact on scientific research in other disciplines. One important application will be in the adaptive design of educational testing and personalized learning. <br/><br/>Three types of problems will be considered in the project: information quantification for model selection, measuring the feasibility of classification, and sequential allocation. A common feature of the research problems involves handling the probability that a likelihood function exceeds a high level. The analysis of such a probability is statistically challenging, especially under the asymptotic regime where this probability decays at an exponential rate. Standard numerical evaluation tools, such as Monte Carlo methods, are computationally intensive to achieve a reasonable accuracy level for simulating such a small probability. Novel techniques will be developed to obtain sharp asymptotic approximations as well as provably efficient numerical methods simultaneously."
"1712664","Nonparametric Estimation and Inference: Shape Constraints, Model Selection, and Level Set Estimation","DMS","STATISTICS","07/01/2017","06/28/2017","Charles Doss","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","06/30/2020","$99,842.00","","cdoss@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","Larger and more complex data sets are becoming more and more commonplace.  It is thus both advantageous and necessary to use statistical methods that are very flexible, and which allow the data to ""speak for itself,"" rather than having researchers make strong unjustifiable prior assumptions about the data.  Flexible methods are thus necessary in the modern landscape, but they have the difficulty that the practitioner generally must ""tune"" the methods in order to get reliable results.  This introduces an ad-hoc element to data analysis, leads to a lack of replicability in results, and means (incorrectly tuned) statistical procedures may return incorrect results.  The unifying theme of this project is the development of statistical methods that are both very flexible and also fully automated, meaning they do not depending on user-chosen tuning parameters.  The application areas motivating this project are varied, and include the analysis of vaccine trials, the study of economic data, and the problem of outlier detection (used widely in financial services).<br/><br/>Very flexible nonparametric statistical methods have become necessary tools to handle the complex nature of large data sets.  One difficulty with using nonparametric tools in practice is their general dependence on (potentially many) tuning parameters which must be chosen well to ensure reliable performance.  The focus of this proposal is on developing methods which can be implemented and lead to reliable results without requiring any ad-hoc steps by the end user.  Three main problems will be studied: (a) model selection for shape-constrained estimators, (b) likelihood ratio type tests for shape-constrained estimators, and (c) estimation and inference for complex features of multivariate densities.  In (a) and (b) the focus is on using so-called shape-constrained estimators, which have the benefit of simultaneously being nonparametric but also of automatically selecting optimal tuning parameters.  Furthermore, they arise out of natural or axiomatic prior information (e.g., economic theory or laws of physics) in many settings, and in such cases one should certainly use that information. In (c), the focus is on estimation of complex features of densities (such as level set manifolds, motivated by outlier detection problems).  Both shape-constrained methods and alternative methods will be considered, when shape constraints are not applicable.  There are few or no effective procedures available in many of the problems under consideration, because of the nonstandard nature of the problems."
"1712706","New Methodology and Theory for Optimal Treatment Regimes with Applications to Precision Medicine","DMS","STATISTICS","07/01/2017","01/15/2020","Charles Doss","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","06/30/2021","$176,555.00","","cdoss@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","The problem of finding the optimal treatment regime, or a series of sequential treatment regimes, based on individual characteristics has important applications in areas such as precision medicine, government policies, and active labor market interventions.  Depending on the application, a treatment can represent a drug, a device, a program, a policy, an intervention, or a strategy.  Stimulated by the advancements in fields such as genomics and medical imaging, the last decade has witnessed exciting and remarkable progress in personalized medicine, ranging from treatments for breast cancer to treatments for major depressive disorders.  The success of precision medicine depends on the development of accurate and reliable statistical and machine learning tools for estimating the optimal treatment regime given the data collected from randomized experiments or observational studies.  This project will develop novel methodology, theory, and algorithms with the potential to significantly advance the state of the art in statistical estimation and inference for optimal treatment regimes. <br/> <br/>The proposed research will significantly enhance the availability of statistical methodology and theory for static or dynamic optimal treatment regimes estimation.  A systematic framework for estimating optimal treatment regimes using a new quantile criterion for a variety of scenarios will be developed.    The research will focus on both one-stage (static) treatment regimes and dynamic treatment regimes, the latter allowing for treatments to vary with time.  In addition, the research will address completely observed responses and randomly censored responses (e.g., survival times), randomized trials and observational studies, and doubly robust estimation. The framework will also be extended to alternative criteria such as Gini's mean difference. This project will significantly advance the theoretical foundations of a large class of robust estimators of optimal treatment regimes.  Furthermore, it addresses the challenging and important problem of developing new methodology and algorithms to identity important variables for optimal treatment regime estimation in the high-dimensional setting.  The investigator will develop software packages and make them freely available to the research community.  Students from minority groups will be especially encouraged to participate in the proposed project."
"1745714","Design and Analysis of Optimization Experiments with Internal Noise to Maximize Alignment of Carbon Nanotubes","DMS","STATISTICS","05/01/2017","06/13/2017","Tirthankar Dasgupta","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","08/31/2019","$129,844.00","","tirthankar.dasgupta@rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","7237, 8037","$0.00","Over the past several decades, carbon nanotubes (CNT) have risen to the forefront of scientific research due to their unique electrical, mechanical and optical properties. However, transferring these properties from nanoscale materials to industrial-scale products often requires alignment (orientation in the same direction) of CNT.  One of the important consequences of alignment is improved conductivity, a highly desirable property in electro-chemical water treatment and closely associated with research endeavors to improve quality of drinking water. Therefore, identification of scalable and cost-effective experimental conditions that maximize alignment of CNT is an important research problem. This is addressed in the project.<br/><br/>The proposed research aims to establish statistical methodologies for designing and analyzing efficient experiments that determine conditions for maximizing alignment of CNT, when one or more input factors are prone to internal noise. The proposed research consists of three tasks, with particular focus on addressing the challenges arising from presence of factors with internal noise and complexity of the response surface. (i) Developing a Bayesian approach to response-surface optimization with noisy inputs. Such an approach allows the experimenter to combine data on output, controllable input, and uncontrollable input from different sources; is a natural way of incorporating expert knowledge into the analysis; and provides a natural framework for optimal design of experiments with noisy inputs. (ii) Efficient design of optimization experiments with noisy inputs. The research will focus on developing a comprehensive design strategy, which is a combination of model-free and Bayesian model-based optimal designs. The model-free design will address the challenges arising from internal noise and complex response surface. (iii) Demonstration and validation of the developed methodologies in the co-PI's lab. A series of experiments will be planned to apply the developed statistical methodology in an attempt to identify factors that trigger alignment of CNT and also to identify their optimum levels to maximize alignment. The proposed framework will allow an experimenter to effectively capture the transmission of uncertainty from input variables to output variables by combining data from different sources, and by utilizing a combination of model-free and model-based experimental designs for efficient exploration of complex response surfaces. From a material scientist's perspective, the proposed method will provide a much more accurate quantification of uncertainty, resulting in more reliable predictions about optimal process conditions as determined from laboratory experiments."
"1712996","Point-to-Point Process Models for Spatio-temporal Networks","DMS","STATISTICS","07/15/2017","07/30/2019","James Sharpnack","CA","University of California-Davis","Continuing Grant","Pena Edsel","06/30/2021","$125,000.00","","jsharpna@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","Real-time data collection systems allow researchers to monitor and analyze billions of streaming events.  Transportation data often consists of trips, the streaming events, from one location to another.  This forms a spatio-temporal network, in which the connections occur between locations.  Such spatio-temporal networks are prevalent in econometrics, transportation and infrastructure applications, internet security data, neurology data, and epidemiological networks.  The majority of statistical methodology for modelling networks assumes that a single static network is observed.  In contrast, this setting presumes that each network connection is an event that happens at specific times, so one must adapt network models to accommodate this novel data modality.  Predicting these events is particularly challenging because the event rate can depend on time and the two associated spatial locations, resulting in an explosion of the number of parameters.  One can make assumptions, which will depend on the application, that reduce the effective number of parameters in a data adaptive fashion.  For example, this research will study spatial community structure in transportation planning applications, temporal trends in internet security data, and complex dependencies between events for financial transactions.  This will require a broad probabilistic framework that can accommodate such assumptions, and computationally tractable statistical methodology for predicting connections in spatio-temporal networks.<br/><br/>It is natural to model these edge events in such dynamic networks as point processes with conditional intensities, and the resulting model for the entire system will be called a point-to-point process.  This novel approach has the advantage of modelling the network in continuous time, so that natural likelihood-based procedures can work directly on transactional dataframes where each row corresponds to an edge event.  The research focus is divided into three topics and areas of application.  First, for transportation networks, it is natural to segment spatial locations using low rank methods, the study of which will require adapting the stochastic block model to the point-to-point process framework.  Secondly, detecting and localizing temporal changepoints in dynamic networks can be accomplished by employing group fusion penalties and trend filtering.  Thirdly, for financial transaction networks and epidemiological networks, complex dependencies can be examined by adapting Hawkes models to the point-to-point process framework.  The research will examine the theory of these statistical problems and develop computationally tractable algorithms."
"1712956","Bridging the Gap Between Theory and Applications: Robust and Scalable Statistical Estimation","DMS","STATISTICS","09/01/2017","08/17/2017","Stanislav Minsker","CA","University of Southern California","Standard Grant","Gabor Szekely","08/31/2020","$99,985.00","","minsker@usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","MPS","1269","8083","$0.00","Major challenges in modern data-rich environment require new statistical algorithms that succeed under realistic scenarios and model assumptions, such as estimation in the distributed setting, ability to handle heavy-tailed data, outliers, and missing observations. Research that will be performed by the Principal Investigator (PI) in the course of this project focuses on two important problems faced by contemporary statistical science: scalability and robustness. The goal of the project is to advance our understanding of statistical techniques that involve (a) high-dimension covariance matrix estimation, and (b) distributed statistical estimation protocols. Obtained results will be of interest to scientists working on theory as well as applications.<br/><br/>One part of this project aims at answering open questions related to high-dimensional covariance matrix estimation for the heavy-tailed distributions. Such distributions serve as a viable model for data corrupted with outliers, an almost inevitable scenario in applications. Covariance matrix is one of the most fundamental objects in high-dimensional data analysis: many important statistical tools, such as Principal Component Analysis (PCA) and regression analysis, involve covariance estimation as a crucial step. For instance, PCA has striking connections to nonlinear dimension reduction and manifold learning techniques, genetics, computational biology, among many others. However, the assumptions underlying the theoretical analysis of most existing estimators, such as various modifications of the sample covariance matrix, are often restrictive and do not hold for real-world scenarios. Using tools from the random matrix theory,  the PI will develop a new class of robust estimators that are numerically tractable, show good practical performance and enjoy strong theoretical guarantees under much weaker conditions than currently available alternatives. Specifically, the goal of the project is to design estimators that admit tight concentration around the unknown ""true"" covariance matrix under weak assumptions on the underlying distribution, such as existence of moments of only low order.  Another part of this project is devoted to novel algorithms for scalable estimation that can take advantage of the ""divide and conquer"" approach. Divide and conquer paradigm assumes that data is stored and analyzed in a distributed way by a cluster consisting of several machines: each of the machines in a cluster works on its own sub-sample while communication among different machines is limited, and final results are obtained by piecing the outcomes of these distributed computations together. The PI will develop a class of new divide and conquer strategies supported by strong theoretical evidence. The project will investigate connections between the distributed estimation strategies and robustness of resulting algorithms -- an important characteristic of large distributed systems."
"1748444","Constrained Statistical Estimation and Inference: Theory, Algorithms and Applications","DMS","STATISTICS","07/01/2017","07/26/2017","John Lafferty","CT","Yale University","Standard Grant","Gabor Szekely","07/31/2018","$144,973.00","","john.lafferty@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","","$0.00","This project lies at the boundary of statistics and machine learning. The underlying theme is to exploit constraints that are present in complex scientific data analysis problems, but that have not been thoroughly studied in traditional approaches. The project will explore theory, algorithms, and applications of statistical procedures, with constraints imposed on the storage, runtime, shape, energy or physics of the estimators and applications. The overall goal of the research is to develop theory and tools that can help scientists to conduct more effective data analysis.<br/><br/>Many statistical methods are purely ""data driven"" and only place smoothness or regularity restrictions on the underlying model. In particular, classical statistical theory studies estimators without regard to their computational requirements. In modern data analysis settings, including astronomy, cloud computing, and embedded devices, computational demands are often central. The project will develop minimax theory and algorithms for nonparametric estimation and detection problems under constraints on storage, computation, and energy. Other constraints to be studied include shape restrictions such as convexity and monotonicity for high dimensional data. The project will also investigate the incorporation of physical constraints through the use of PDEs and models of physical dynamics and mechanics, focusing on both algorithms and theoretical bounds."
"1712730","Collaborative Research:  Statistical Estimation with Algebraic Structure","DMS","STATISTICS","07/01/2017","09/13/2019","Alexander Wein","NY","New York University","Continuing Grant","Gabor Szekely","06/30/2021","$279,998.00","","aswein@ucdavis.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1269","","$0.00","Scientific and engineering disciplines ranging from structural biology to computer vision rely on data collection and analysis to guide scientific discovery. Critically, in such applications, the systems under study constrain and govern the structure of information in collected data. The goal of this research project is to develop a family of statistical models that enables a systematic extraction of relevant statistical information from these datasets by bringing together interdisciplinary concepts from statistics and optimization. The approach under development aims to provide a new set of statistical tools that is adapted to this class of problems and that could have a transformative impact on several scientific disciplines.<br/><br/>This project is articulated around a core set of techniques to analyze datasets in the context of a latent algebraic structure, often arising from the physical laws underlying the data collection processes. Unlike more traditional statistical problems where a linear underlying structure is often built into the model, data-driven science generates problems with algebraic but often non-linear structure. The project focuses on problems of central importance in a variety of scientific and engineering disciplines, including signal processing, structural biology, and computer vision, that share a similar feature: the need to leverage algebraic structure in order to extract information from data. The project aims at developing a systematic approach to analyze this family of problems, together with a general procedure to construct computationally efficient algorithms using low-rank tensor decomposition. Importantly, these methods can be proved to be statistically optimal and therefore make the most efficient use of collected data."
"1712940","Collaborative Research:   Higher-Order Asymptotics and Accurate Inference for Post-Selection","DMS","STATISTICS","08/01/2017","08/02/2017","Todd Kuffner","MO","Washington University","Standard Grant","Pena Edsel","07/31/2021","$79,955.00","","kuffner@math.wustl.edu","ONE BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","1269","9150","$0.00","Many statistical analyses utilize a model selection procedure. Perhaps the most common model selection problem is that of variable selection in linear regression. Principled motivations for selection include the desire for interpretability, prevention of over-fitting, and concerns about statistical power. A practical motivation arises when the data are high-dimensional, with more explanatory variables than observations. Relevant applications span the entire domain of scientific inquiry, from neuroscience, medicine and physics, to economics, sociology, and psychology. A large catalogue of variable selection procedures is now available, and the statistics community has turned its focus to the question of inference after selection. Standard methods of statistical inference are no longer valid when the same data are used to both select a model and make inferences about that model. It is fundamentally important to have accurate post-selection inference procedures that are also powerful enough to detect observed departures from scientific hypotheses and that avoid the strong distributional assumptions needed for exact inference in finite samples. This research aims to develop post-selection inference methodology that is both accurate and powerful, with particular emphasis on reducing statistical errors that depend on the sample size. <br/><br/>The goal of this project is to further understanding of the asymptotic theory of post-selection inference, particularly selective inference based on the CovTest and truncated Gaussian (TG) statistic, as well as simultaneous inference using the post-selection intervals (PoSI) procedure. The first two procedures can be motivated by selective error control, i.e., error control for the selected model parameters. The PoSI method seeks to control family-wise error rates for all possible sub-model parameters. While these procedures yield valid post-selection inference, without strong assumptions they are particularly vulnerable to the effects of violation of key assumptions in the realistic setting of small to moderate sample sizes, such as overly-conservative or inaccurate confidence intervals, and low power. In this project, asymptotic expansions, saddle-point approximations, the bootstrap, and related techniques from higher-order asymptotics will be employed to improve accuracy and power for these post-selection inference procedures."
"1832046","Collaborative Research: New Directions in Multidimensional and Multivariate Functional Data Analysis","DMS","STATISTICS","08/01/2017","05/16/2018","Xiaoke Zhang","DC","George Washington University","Standard Grant","Gabor Szekely","06/30/2020","$76,912.00","","xkzhang@gwu.edu","1918 F ST NW","WASHINGTON","DC","200520042","2029940728","MPS","1269","9150","$0.00","Functional data analysis, which deals with a sample of functions or curves, plays an important role in modern data analysis. Nowadays in the era of ""Big Data"", multidimensional and multivariate functional data are becoming increasingly common, especially in biological, medical, and engineering applications.  There are significant challenges posed by the very large dimension and complex structure of these data.  The proposed research will substantially narrow the gap between the increasing demand for handling such data in practice, and the insufficient development of statistical methods and computational tools.  This research has applications to neuroscience, climate science, and engineering.  It will provide scientists, engineers, and doctors with tools to help understand problems in their area, and enhance interdisciplinary collaborations.<br/><br/>This project offers a comprehensive research plan to advance the understanding and applicability of multidimensional and multivariate functional data.  The research will focus on the following three sub-projects: (1) Develop data-adaptive and interpretable representation of the covariance function for multidimensional functional data, (2) Develop a novel model-free procedure to detect dependency between components of multivariate functional data, and (3) Address the modeling and prediction of multivariate functional time series.  The resulting methods will be applied to neuroimaging and climate data.  The integration of these three sub-projects will foster creative directions and strategies for multidimensional and multivariate functional data."
"1713012","Beyond Riemannian Geometry in Inference","DMS","STATISTICS","08/01/2017","08/02/2019","Shayn Mukherjee","NC","Duke University","Continuing Grant","Gabor Szekely","06/30/2021","$220,000.00","","sayan@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","","$0.00","A challenge in modern data science is how to integrate information from 3-dimensional shapes into statistical models. Examples of applications where this challenge is central is associating the shape of a tumor to molecular processes or associating the shape of the roots of a rice plant to crop yield.  In graphics and anatomy, there is the related question of how to warp one object into another, such as the molar of a child to a molar of an adult.  This project seeks to develop methodology to transform these shape data, such as meshes or 3-dimensional images, into representations for which standard statistical models are available. These methods are crucial to advancing data-enabled science in extracting, conceptualizing, interpreting, and visualizing information residing in datasets comprising complex objects such 3D shapes.  Cutting edge ideas in mathematics, specifically from the field of geometry, will be used to address the fundamental problems of (i) modeling structural variation in diverse collections of shapes and (ii) modeling transformations between shapes. Solutions to these problems are crucial to many practical applications and disciplines including biology, medicine, social sciences, and ecology. As such, an important component of the project is validation of methods and tools through applications in radiology and anthropology<br/><br/>Geometric concepts beyond Riemannian geometry and smooth manifolds will be leveraged to develop novel statistical methodology to address complex challenges in data analysis.  Methods and tools, grounded on solid mathematical foundations, will be developed for modeling complex objects (such as shapes and surfaces) and complex relations within data (such as multi-commodity flow or aligning shapes).  The research will address two statistical challenges using geometric tools: (1) representing surfaces and shapes via integral geometry, and (2) learning group actions or transformations between pairs of objects using the geometry of fiber bundles.  Addressing the first challenge results in a framework for parametric and non-parametric statistical models for collections of shapes and surfaces, without the requirement of landmark points and without requiring the shapes to be isomorphic. Addressing the second challenge provides a statistical framework for alignment problems ranging from aligning a collection of shapes such as teeth, to optimization problems on networks such as multi-commodity flow.  The solutions proposed will impact statistics and geometry, and the methods may catalyze transformative advances in practical applications and scientific disciplines as diverse as biology, medicine, social sciences, and ecology."
"1812354","CAREER: New Statistical Methods for Classification and Analysis of High Dimensional and Functional Data","DMS","STATISTICS","08/17/2017","11/16/2017","Yichao Wu","IL","University of Illinois at Chicago","Continuing Grant","Gabor Szekely","08/31/2018","$124,212.00","","yichaowu@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","1045, 1187","$0.00","Recent technology advances have generated data of unprecedented size and complexity across different scientific fields. To analyze such complex data, the principal investigator (PI) aims to develop new statistical methodologies. The PI proposes to study four interrelated research topics. First, the PI focuses on large-margin classification and proposes new large-margin classifiers to deliver competitive classification and conditional class probability estimation. He also proposes to address the question of whether a soft or hard classifier is preferred for a particular classification task and how to incorporate estimated conditional class probability to improve dimension reduction for data with a categorical response. Second, the PI proposes an extension of the least angle regression to deal with generalized linear models and, more generally, a strictly convex optimization problem. The new solution path is piecewise given by systems of ordinary differential equations and can be slightly modified to get the corresponding LASSO regularized solution path. Third, data with a sparse and irregular functional predictor are considered. New response-based dimensional reduction methods are proposed for such data using cumulative slicing and a viable scheme is also proposed to extend large-margin classifiers to analyze such data. Fourth, the PI focuses on the semi-parametric multi-index regression. By noticing that the Hessian operator filters out the effect of the linear component automatically, the PI provides a direct estimation scheme to estimate the space spanned by the multiple indices. The new scheme differs from existing methods in that it does not require estimating the nonparametric link while estimating the space spanned by the multiple indices as in other existing approaches.<br/><br/>The proposed statistical methodology innovations are widely applicable in various fields. For example, the proposed new large-margin classifiers can be applied to analyze genomic data with a categorical response such as cancer type; new ordinary differential equation based solution path algorithms can used to analyze survival or binary genomic data to identify important predictors; while analyzing longitudinal data of aging, the new proposed statistical methods for sparse and irregular functional data will be useful. In order to facilitate the use of the proposed new methods, the PI will implement them in R or Matlab and make new software available to the public along with the corresponding research reports. The success of the proposed research will help to improve public health."
"1713118","Collaborative Research:  Statistical Inference for High-Frequency Data","DMS","STATISTICS","07/01/2017","05/24/2017","Lan Zhang","IL","University of Illinois at Chicago","Standard Grant","Gabor Szekely","06/30/2021","$140,562.00","","lanzhang@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","8083","$0.00","To pursue the promise of the big data revolution, the current project is concerned with a particular form of such data, high frequency data (HFD), where series of observations can see data updates in fractions of milliseconds. With technological advances in data collection, HFD occurs in medicine (from neuroscience to patient care), finance and economics, geosciences (such as earthquake data), marine science (fishing and shipping), and other areas. The research focuses on how to extract information from complex big data and how to turn data into knowledge. In particular, the project aims to develop cutting-edge mathematics and statistical methodology to uncover the dependence structure governing a HFD system. The new dependence structure will permit the ""borrowing"" of information from adjacent time periods, and also from other series from a panel of data. It is expected that the results will lead to more efficient estimators and better prediction and that this approach will form a new paradigm for HFD. In addition to developing a general theory, the project is concerned with applications to financial data, including risk management, forecasting, and portfolio management. More precise estimators, with improved margins of error, will be useful in all these areas of finance. The results are expected to be of interest to investors, regulators, and policymakers, and the results are entirely in the public domain. <br/><br/>The goal of this project is to create a unified framework for inference in high frequency data, based on dividing the observations and the parameter process into blocks. The work pursues two paths, both involving the fundamental structure of the data architecture. A ""within block"" approach uses contiguity to make the structure of the observations more accessible in local neighborhoods. The ""between block"" approach sets up a tool for using stochastic calculus to study the relationship between parameters in blocks that are adjacent (in time and space). It also permits the integration of high and low frequency models. This is achieved without altering current models. A final part of the project is devoted to further study of the observed asymptotic variance, in particular work on tuning parameters and inferential interpretation. Both the ""within block"" and ""between block"" approaches are formulated to cover general time varying ""parameters"" that are usually estimated from high frequency data series, not only volatility, but also skewness (leverage effect), regression coefficients, and parameter dynamics (such as volatility of volatility). In both cases, the observed data and also parameter processes may have large dimension (large panel size) in addition to high frequency observation. The within block approach permits contiguity to be stated jointly for the latent underlying processes and the microstructure/observation noise. For the between block approach, the investigators will further develop a new way to look at the dependence relationships between the parameters."
"1806063","Collaborative Research: New Directions in Multidimensional and Multivariate Functional Data Analysis","DMS","STATISTICS","08/16/2017","05/16/2018","Ka Wai Wong","TX","Texas A&M University","Standard Grant","Gabor Szekely","06/30/2020","$96,811.00","","raywong@tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","","$0.00","Functional data analysis, which deals with a sample of functions or curves, plays an important role in modern data analysis. Nowadays in the era of ""Big Data"", multidimensional and multivariate functional data are becoming increasingly common, especially in biological, medical, and engineering applications.  There are significant challenges posed by the very large dimension and complex structure of these data.  The proposed research will substantially narrow the gap between the increasing demand for handling such data in practice, and the insufficient development of statistical methods and computational tools.  This research has applications to neuroscience, climate science, and engineering.  It will provide scientists, engineers, and doctors with tools to help understand problems in their area, and enhance interdisciplinary collaborations.<br/><br/>This project offers a comprehensive research plan to advance the understanding and applicability of multidimensional and multivariate functional data.  The research will focus on the following three sub-projects: (1) Develop data-adaptive and interpretable representation of the covariance function for multidimensional functional data, (2) Develop a novel model-free procedure to detect dependency between components of multivariate functional data, and (3) Address the modeling and prediction of multivariate functional time series.  The resulting methods will be applied to neuroimaging and climate data.  The integration of these three sub-projects will foster creative directions and strategies for multidimensional and multivariate functional data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1745746","Workshop on Objective Bayes Methodology","DMS","STATISTICS","08/01/2017","08/01/2017","Peter Mueller","TX","University of Texas at Austin","Standard Grant","Gabor Szekely","07/31/2018","$10,000.00","","pmueller@math.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1269","7556","$0.00","This award supports participation in the four-day ""Objective Bayes 2017 Workshop"" held at the University of Texas at Austin, TX on December 10-13, 2017. The workshop will consist of one day of tutorials for graduate students and new researchers, three days of scientific sessions, and a poster session, all centering on major recent developments in statistical inference methods that can be used automatically, that is, methodology that does not require subjective input other than a stylized probabilistic description of how the data arises.<br/><br/>The Objective Bayes workshop (""O'Bayes"") is one of the longest running and preeminent meetings in Bayesian statistics, addressing a wide range of topics including robust, default Bayesian analysis, reproducibility, variable selection, big data, and nonparametric Bayesian methods.  Objective Bayes methods encompass research in very diverse areas, from biostatistics, to machine learning, asymptotics, spatial inference, and computation. The workshop will serve to foster interaction of researchers in these diverse areas with an aim of initiating novel ideas and research directions. For more details about the conference please see the conference homepage https://sites.google.com/site/obayes2017/."
"1713082","Collaborative Research: Inference for Network Models with Covariates: Leveraging Local Information for Statistically and Computationally Efficient Estimation of Global Parameters","DMS","STATISTICS","07/01/2017","04/25/2017","Purnamrita Sarkar","TX","University of Texas at Austin","Standard Grant","Pena Edsel","06/30/2021","$160,001.00","","purna.sarkar@austin.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1269","","$0.00","Large datasets, which are naturally modeled as a network or graph, arise in almost every field of human endeavor. For example, Facebook is a social network, where nodes are users, with edges corresponding to friendships. In gene networks, nodes represent genes with connections corresponding to their co-expression. In ecological networks, the nodes are animal species, with edges determined according to who eats whom.   A major focus of research for network or graph data has been on identifying community membership of the nodes.  However, what is often more important for scientific purposes is examining the nature and evolution of edge and membership probabilities, for instance changes in gene features of individuals as a function of some unknown factor, like a disease.  The focus on using other measured features of nodes and edges could add, in decisive ways, to the information available from observed edges or interactions between nodes.  These could be disease symptoms or test results, or demographic information of users in social networks.  Statistical inference in such models, despite its importance, has only just begun to be studied.  There are both theoretical and computational challenges, due both to the complexity of models fitted, and the size of data sets.   The research will lead to the development of algorithms for fitting models and statistical measures of confidence, with potential applications to many fields. <br/><br/>The research is focused on block models for graphs, when node or edge covariates are present. When formulated, these models are no longer block models, but models whose membership probabilities depend upon covariates and whose connection probabilities depend both on block membership and individual covariates.  Fitting algorithms involve alternating between fitting block and covariate parameters.  Variational (mean field) approaches which effectively lead to semi-parametric model fitting with nK membership ""nuisance"" parameters, with n representing the number of nodes and K the number of communities, are examined.  As these approaches have been found by the PIs to be unstable for large n, the PIs have already begun to investigate the theoretical and practical aspects of divide and conquer algorithms where many subgraphs are independently fit.   The PIs will study the statistical properties, both asymptotically and through simulations, and develop practicable and computationally stable methods for large, relatively sparse graphs."
"1841569","CAREER: An Integrated Inferential Framework for Big Data Research and Education","DMS","STATISTICS, Division Co-Funding: CAREER","09/01/2017","06/17/2022","Han Liu","IL","Northwestern University","Continuing Grant","Yong Zeng","06/30/2023","$347,702.00","","hanliu@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1269, 8048","1045","$0.00","This project addresses several fundamental challenges in modern data analysis and aims to create a new research area named Big Data Inference. Currently available literature regarding Big Data research mainly focuses on developing new estimators for complex data. However, most of these estimators are still in lack of systematic inferential methods for uncertainty assessment. This project hopes to bridge this gap by developing new inferential theory for modern estimators unique to Big Data analysis.  The deliverables of this project include easy-to-use software packages, which directly help scientists to explore and analyze complex datasets. The principal investigator is also actively collaborating with many scientists to ensure the more direct impact of this project to the targeted scientific communities.<br/><br/>This project aims to develop novel inferential methods for assessing uncertainty (e.g., constructing confidence intervals or testing hypotheses) of modern statistical procedures unique to Big Data analysis. In particular, it develops innovative statistical inferential tools for a variety of machine learning methods which have not yet been equipped with inferential power. It also provides necessary inferential tools for the next generation of scientists to be competitive in modern data analysis."
"1710802","Workshop on Applications-Driven Geometric Functional Data Analysis","DMS","STATISTICS","07/01/2017","04/18/2017","Anuj Srivastava","FL","Florida State University","Standard Grant","Nandini Kannan","06/30/2018","$19,999.00","Martin Bauer","anuj@stat.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","7556","$0.00","This award supports participation in a three-day workshop on applications-driven geometric functional data analysis (FDA) held October 8-10, 2017 on the campus of Florida State University in Tallahassee, Florida. The workshop focuses on topics in the intersections of FDA, infinite-dimensional differential geometry, and data-driven applications, and will feature both senior experts and junior researchers from these areas.  The workshop includes a half-day of tutorials providing an introduction to the workshop topics, especially aimed at junior researchers.  The remaining time will be allocated to invited talks, a poster session, and discussion sessions. The main motivation for this workshop is to initiate and facilitate discussions and future collaborations between US scientists and mathematicians with different backgrounds and expertise. The organizers will strive for breadth and diversity by targeting leading experts in the three components areas of the workshop -- geometry, FDA, and big data analysis. The poster session will be used to encourage all participants, especially the junior researchers, to present their research and interact with others. <br/> <br/>A rapid growth of functional data in all disciplines, ranging from natural and social sciences to engineering and technology, has made FDA an important topic of research. To handle the diverse challenges facing FDA, there is a need to exploit low-dimensional patterns and structures present in functional data. Geometry provides tools to: (1) extract and analyze structures in functional data, (2) handle the infinite-dimensionality of function spaces, (3) arrive at viable statistical models and make efficient inferences; and (4) process large datasets associated with many current and future data-centric applications.  The workshop will include both: (1) theoretical topics, such as geometries of infinite-dimensional spaces, analysis under invariant metrics, and statistical models involving functional data; and (2) applications, such as computational anatomy, bioinformatics, biometrics, and neuroscience. This blend of important and complementary topics makes the workshop a unique and worthwhile event.  For more information about the workshop, please refer to https://ani.stat.fsu.edu/GFDW/"
"1764280","Collaborative Research:  Nonparametric Bayesian Aggregation for Massive Data","DMS","STATISTICS","09/01/2017","08/16/2019","Zuofeng Shang","IN","Indiana University","Continuing Grant","Gabor Szekely","12/31/2019","$80,000.00","","zshang@njit.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","MPS","1269","8083","$0.00","Modern massive data appear in increasing volume and high heterogeneity. Examples include internet searches, social networks, mobile devices, satellites, genomics, medical scans, etc. Bayesian approaches are particularly useful in such context since the complex structures in the data can be naturally incorporated in Bayesian hierarchical models. Besides, uncertainty quantification can be easily executed through Bayesian computation. However, due to storage and computational bottlenecks, traditional Bayesian computation implemented in a single machine is no longer applicable to modern massive data. In this project, a set of nonparametric Bayesian aggregation procedures with theoretical justifications are developed based on a standard parallel computing strategy known as Divide-and-Conquer. This research will significantly enhance the availability of Bayesian tools and software for analyzing massive data. The educational plan of the project will be in the form of graduate student advising and offering of special topics courses. <br/><br/>This project consists of three major components. First, the PIs will establish a Gaussian approximation of general nonparametric posterior distributions which serves as a theoretical foundation for general distributed Bayesian algorithms. Second, the PIs will develop a nonparametric Bayesian aggregation procedure with theoretical guarantees that is particularly useful to handle massive data in a parallel fashion. Third, the PIs will develop an efficient parallel Markov Chain Monte Carlo (MCMC) algorithm for nonparametric Bayesian models which will perform as well as traditional MCMC with substantially less computational costs. This research will lead to an emergence of ""Splitotics (Split+Asymptotics) Theory"" providing theoretical guidelines for Bayesian practices. The smoothing spline inference results recently obtained by the PIs will be used as a promising tool for achieving the above goals."
"1703077","Statistical Inference for Biomedical Big Data: Theory, Methods, and Tools","DMS","STATISTICS","04/15/2017","04/11/2017","Faming Liang","FL","University of Florida","Standard Grant","Yong Zeng","03/31/2018","$20,000.00","Somnath Datta, Fei Zou, Peihua Qiu","fmliang@purdue.edu","1523 UNION RD RM 207","GAINESVILLE","FL","326111941","3523923516","MPS","1269","7556","$0.00","This award supports participation in the workshop ""Statistical Inference for Biomedical Big Data: Theory, Methods, and Tools"" to be held on the Gainesville campus of the University of Florida, Gainesville, Florida on April 7-8, 2017. Dramatic improvements in data collection and acquisition technologies in the past two decades have enabled scientists to collect vast amounts of health-related data in biomedical studies. If analyzed properly, these data can expand our knowledge and improve contemporary healthcare services from diagnosis to prevention to personalized treatment, and can also provide insights into reducing healthcare costs. However, biomedical data can be rather big and complex, and are often characterized by some mixture of high dimensionality, heterogeneity, high volume, high velocity, and high variety. Advances in biomedical big data analysis can greatly impact the development of biomedical sciences.<br/><br/>This workshop will bring together some of the most prominent statisticians in biomedical big data and a selected group of local biomedical researchers in a collaborative setting to discuss and foster cutting-edge developments of statistical theory, methods, and tools for biomedical big data analysis. The format of this workshop is designed to foster interactions between statisticians and biomedical researchers. To facilitate communications between graduate students, junior researchers, and senior researchers, a poster session is included in the program. To disseminate the results of the meeting, the program and abstracts will be widely circulated, and modest travel funds for a large geographically diverse group of graduate students, postdocs, and junior faculty are planned. More information about the workshop can be found at http://biostat.ufl.edu/seminars/biostatistics-workshop/."
"1724882","Statistical Analysis of Neuronal Data (SAND8)","DMS","STATISTICS","04/01/2017","03/20/2017","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Nandini Kannan","03/31/2018","$20,000.00","","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","7556, 8091","$0.00","The eighth international workshop on Statistical Analysis of Neural Data (SAND8) will take place May 31-June 2, 2017, in Pittsburgh, PA. Experimental methods for discovering the neural basis of behavior have been advancing rapidly and, as a result, brain data sets are increasing in size and complexity. Methods for better understanding such rich data sets, which can enable design of ever-more informative experiments, are desperately needed. This workshop series is concerned with identifying, discussing, and disseminating many of the most promising approaches to analysis of neural data of all kinds. It also encourages young researchers, including graduate students, to present their work; exposes young researchers to important challenges and opportunities in this interdisciplinary domain, while providing a small meeting atmosphere to facilitate the interaction of young researchers with senior colleagues; and includes as participants women, under-represented minorities and persons with disabilities, who might benefit from the small workshop environment.<br/> <br/>The data discussed in this workshop range from anatomy to electrophysiology, to neuroimaging. Cross-disciplinary communication between experimental neuroscientists and those trained in statistical and computational methods is a priority. SAND8 will bring together neurophysiologists, statisticians, mathematicians, engineers, physicists, and computer scientists who are interested in quantitative analysis of neural data. SAND8 will begin with a morning-long panel discussion on ""Emerging Challenges of Brain Science Data,"" and end with several talks that connect statistical analysis to mathematical modeling. In between there will be keynote talks by senior investigators and shorter presentations by junior investigators, the latter selected on a competitive basis. There will also be a poster session, to which all participants are invited to contribute. Talks and posters may involve new methodology, investigation of existing methods, or application of state-of-the-art analytical techniques. In addition, there will be a lunchtime discussion devoted to opportunities and challenges for women in computational neuroscience. For further information please see  http://sand.stat.cmu.edu."
"1713003","High-dimensional Clustering: Theory and Methods","DMS","STATISTICS","07/01/2017","08/14/2019","Sivaraman Balakrishnan","PA","Carnegie-Mellon University","Continuing Grant","Pena Edsel","06/30/2021","$380,000.00","Larry Wasserman, Alessandro Rinaldo","sbalakri@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","The past two decades have witnessed an explosion in the scale and complexity of data sets that arise in science and engineering. Broadly, clustering methods which discover latent structure in data are our primary tool for navigating, exploring and visualizing massive datasets. These methods have been widely and successfully applied in phylogeny, medicine, psychiatry, archaeology and anthropology, phytosociology, economics and several other fields. Despite its ubiquity, the widespread scientific adoption of clustering methods have been hindered by the lack of flexible clustering methods for high-dimensional datasets and by the dearth of meaningful inferential guarantees in clustering problems. Accordingly, the goal of this research is to develop new and effective methods for clustering complex data-sets, and to further develop an inferential grounding -- which will in turn lead to actionable conclusions -- for these methods. This research will lead to the development of new clustering methods, as well as to a deeper understanding of the fundamental limitations of methods aimed at uncovering latent structure in data. <br/><br/>The research component of this project consists of four aims designed to address related aspects of this high-level goal: (a) analyze and develop new clustering methods for high-dimensional datasets, with a particular focus on practically useful methods like mixture-model based clustering, and minimum volume clustering; (b) develop novel methods for inference in the context of clustering, motivated by scientific applications where it is important not only to cluster the data but also to clearly characterize the sampling variability of the discovered clusters; (c) develop fundamental lower bounds for high-dimensional clustering (d) develop novel methods for clustering functional data with inferential guarantees. These research components are closely coupled with concrete educational initiatives, including the development and broad dissemination of publicly-available software for high-dimensional clustering; tutorials and workshops at Machine Learning conferences and fostering further interactions between the Departments of Statistics and Machine Learning at Carnegie Mellon."
"1712947","Multiscale Generalized Correlation: A Unified Distance-Based Correlation Measure for Dependency Discovery","DMS","STATISTICS","07/01/2017","06/01/2018","Cencheng Shen","MD","Johns Hopkins University","Continuing Grant","Gabor Szekely","03/31/2019","$133,364.00","Carey Priebe, Joshua Vogelstein","shenc@udel.edu","3400 N CHARLES ST","BALTIMORE","MD","212182608","4439971898","MPS","1269","8083","$0.00","Detecting relationships between two data sets has long been one of the most important questions in statistics and is fundamental to scientific discovery in the big-data era. By developing an open-source, robust, efficient, and scalable statistical methodology for testing dependence on modern data, this project aims to advance the understanding and utility of testing dependence, tackle a number of related statistical inference questions, and accelerate a broad range of data-intensive research. The project incorporates fundamental research in mathematics, statistics, and computer science to further develop a multiscale generalized correlation framework to enable discovery and decision-making via analysis of large and complex data. The tools under development will allow scientists to better explore and understand high-dimensional, nonlinear, and multi-modal data in a myriad of applications. The project aims to provide a unified framework for discovery of relationships between observations in an efficient and theoretically-sound manner. <br/><br/>Combining the notion of generalized correlation with the locality principle, multiscale generalized correlation (MGC) is a superior correlation measure that equals the optimal local correlation among all possible local scales. By building upon distance correlation and making use of nearest neighbors, the resulting MGC test statistic is a unique dependence measure that is consistent for testing against all dependencies with finite second moment, and it exhibits better performance than existing state-of-art methods under a wide variety of nonlinear and high-dimensional dependencies. By investigating the theoretical aspects of distance-based correlations, this project aims to further improve the finite-sample performance of MGC-style tests, extend its capability to testing dependence on network and kernel data, and broaden its utility to general inferential questions beyond dependence testing such as two-sample testing, outlier detection, and feature screening, as well as applications to brain activity, networks, and text analysis. Overall, this project intends to establish a unified methodology framework for statistical testing in high-dimensional, noisy, big data, through theoretical advancements, comprehensive simulations, and real data experiments."
"1712041","Collaborative Research:  Statistical Inference Using Random Forests and Related Methods","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2017","07/19/2017","Lucas Mentch","PA","University of Pittsburgh","Standard Grant","Gabor Szekely","08/31/2021","$119,802.00","","lkm31@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269, 7454","8007, 8083","$0.00","This project seeks to develop methods to quantify uncertainty in machine learning algorithms and to incorporate machine learning and statistical inference. Machine learning has been enormously successful at using data to make predictions; it is used in an extensive range of applications from handwriting recognition to high frequency trading to driverless cars and personalized medicine. However, while machine learning algorithms make good predictions, they tell humans very little about how those predictions were arrived at: What were the important factors? How did they affect the prediction? They also don't distinguish predictions for which there is a lot of information about the probability of different outcomes (even if that covers a wide range) from those where very little information is available. For example, a machine learning algorithm may very accurately predict whether a person is likely to develop diabetes, but provides little if any information regarding how that person might lower his or her risk. This project will build on initial mathematical theory to develop methods to explain how Random Forests arrive at their predictions and how statistically confident those predictions are, and produce ways to link machine learning methods to other statistical models.<br/><br/><br/>This project seeks to develop methods to quantify uncertainty in machine learning algorithms and to incorporate machine learning and statistical inference. The project will extend on a theoretical framework representing Random Forests as U-statistics to produce a practical implementation of statistical uncertainty quantification in machine learning. In particular, it will improve on methods to estimate sample variability in Random Forest predictions, develop computationally efficient screening tools for covariate and interaction selection, and incorporate ensemble methods as non-parametric terms in partially-linear models while retaining statistical inference via a modified boosting algorithm. These methods will be demonstrated on a citizen science data base in ornithology and in various biomedical applications."
"1730090","2017 Graybill Conference on Statistical Genomics and Genetics","DMS","STATISTICS","06/01/2017","05/15/2017","Wen Zhou","CO","Colorado State University","Standard Grant","Nandini Kannan","05/31/2018","$15,000.00","Jay Breidt","riczw@stat.colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1269","7556","$0.00","This award supports participation in the 2017 Graybill Conference on Statistical Genomics and Genetics, held June 5-7, 2017 on the campus of Colorado State University, Fort Collins, Colorado. The conference is co-sponsored by the American Statistical Association Section on Statistics in Genomics and Genetics. The program consists of invited talks, a contributed poster session, and a student poster competition. The goal of the conference is to provide an opportunity for quantitative scientists and practitioners in the biological sciences to generate and share ideas for new creative research in statistics and genomics or genetics; to exchange knowledge on frontier statistical methodologies for problems rising from genomics and genetics; to stimulate professional networking opportunities; and to provide young researchers with exposure for their work.<br/><br/>The last decade has witnessed the rapid advancement of high-throughput technologies and the generation and storage of massive data sets. A huge number of new challenges in analyzing, modeling, and interpreting these massive data have arisen, forcing transformative change on research in biological and medical sciences in general, and genomics and genetics in particular. To meet those challenges and provide reliable approaches, statistical genomics and genetics have emerged from traditional statistics and have become indispensable tools for modern research in the biological and medical sciences, also paving the road to many important biological and medical discoveries in the last few years. On the other hand, novel problems arising in genomics and genetics have also motivated many new statistical concepts, models, and methodologies. Statistical genomics and genetics have been growing remarkably fast and apply to an increasing number of topics in both fields. They are now essential parts of modern biological and medical research and also generate important research topics within statistics. Exposure and understanding of the motivating problems, newly-developed models and methods, and working protocols are extremely useful for researchers in the quantitative sciences, including bioinformatics, computer science, and statistics.   For more information, see the conference website at http://graybill.wolpe2.natsci.colostate.edu/"
"1744039","Women in Statistics and Data Science Conference","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","08/01/2017","08/03/2017","Donna LaLonde","VA","American Statistical Association","Standard Grant","Gabor Szekely","06/30/2019","$30,000.00","","donnal@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1260, 1269","7556","$0.00","The Women in Statistics and Data Science (WSDS) Conference will take place from October 19 to 21, 2017 at the Hyatt Regency La Jolla in La Jolla, CA. The theme for the 2017 conference is Share WISDOM (Women In Statistics, Data science, and -OMics). The conference will create an environment that promotes transformation and collaboration by encouraging and supporting women statisticians, biostatisticians, and data scientists. In particular, an ultimate objective of this conference is to increase the proportion of successful women researchers who have the technical skills, capacity and inclination to take on the challenges of complex data oriented research in twenty-first century. <br/><br/>The two and half-a-day conference (starting the afternoon of October 19th) will have multiple technical sessions providing participants with the opportunity to learn about novel approaches and innovations addressing the challenges of working with complex data. The technical sessions will be complemented by professional development sessions for all stages of participants starting from graduate students to senior researchers. The synergy derived from the diverse cohort of presenters and participants will support the intentional integration of formal and informal learning and mentoring. The conference, through multiple invited technical sessions will highlight the research of female rising stars, mid-career and senior researchers and will provide the opportunity to establish new collaborations. In addition, the mentoring and networking sessions of academic, government and industry participants will guide early-career women participants on a successful career trajectory. Conference URL: http://ww2.amstat.org/meetings/wsds/2017/conferenceinfo.cfm"
"1712966","Statistical Models, Inference, and Computation for Multidimensional Time Series Data","DMS","STATISTICS","07/01/2017","08/14/2019","Vladas Pipiras","NC","University of North Carolina at Chapel Hill","Continuing Grant","Gabor Szekely","12/31/2020","$199,997.00","","pipiras@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","It is now commonplace for data to be collected over time across multiple (often many) sources.  Examples include the time signals across multiple brain regions arising from fMRI, ocean wave height series across multiple spatial locations collected from buoys or satellites, and the multiple economic indicators (GPD, unemployment, and so on) gathered over time by government agencies and other parties. Available techniques often either neglect temporal dependencies for such high-dimensional data arising from multiple sources or do not apply to situations when the number of sources is large. This research project aims to develop novel statistical modeling tools that can capture adequately both the temporal features of such data and also their dependencies across multiple sources. Such tools have the potential to greatly enhance knowledge gained from such data. With fMRI data, for example, proper accounting for temporal dependence and large number of brain regions may facilitate better distinction among various clinical categories (ADHD, autism, etc.). Understanding the temporal and spatial dependencies in wave height data can lead to better predictions of storm activity across the oceans, and further insight into economic activity is expected from improved analysis of multiple economic indicators.<br/><br/>The project aims at developing an integrated approach to analyzing large multidimensional time series data, including their statistical models, estimation, computation (algorithms), and practice. The research covers both short-range and long-range dependent multidimensional time series. For short-range dependent series, the focus is on sparse vector autoregressive and related models, dimension reduction, change point detection and some nonlinear models. The problems to be addressed concern regularization techniques, statistical significance, models exhibiting cyclical variations and other issues. Multidimensional long-range dependence is suggested as the important class complementing vector autoregressive and related short-range dependent series, thus gathering the two general classes of models employed in modern time series analysis. The goal is to develop a new methodology for multidimensional long-range dependent series with the so-called general phase, which controls the symmetry properties of multidimensional time series, in both linear and nonlinear settings. The developed methods should be useful across a wide range of areas, including neuroscience, oceanography and environmental sciences, geophysics, economics and finance, and others."
"1712919","Collaborative Research:  Nonparametric Bayesian Aggregation for Massive Data","DMS","STATISTICS","09/01/2017","08/23/2017","Zuofeng Shang","NY","SUNY at Binghamton","Continuing Grant","Yong Zeng","11/30/2017","$30,023.00","","zshang@njit.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139024400","6077776136","MPS","1269","8083","$0.00","Modern massive data appear in increasing volume and high heterogeneity. Examples include internet searches, social networks, mobile devices, satellites, genomics, medical scans, etc. Bayesian approaches are particularly useful in such context since the complex structures in the data can be naturally incorporated in Bayesian hierarchical models. Besides, uncertainty quantification can be easily executed through Bayesian computation. However, due to storage and computational bottlenecks, traditional Bayesian computation implemented in a single machine is no longer applicable to modern massive data. In this project, a set of nonparametric Bayesian aggregation procedures with theoretical justifications are developed based on a standard parallel computing strategy known as Divide-and-Conquer. This research will significantly enhance the availability of Bayesian tools and software for analyzing massive data. The educational plan of the project will be in the form of graduate student advising and offering of special topics courses. <br/><br/>This project consists of three major components. First, the PIs will establish a Gaussian approximation of general nonparametric posterior distributions which serves as a theoretical foundation for general distributed Bayesian algorithms. Second, the PIs will develop a nonparametric Bayesian aggregation procedure with theoretical guarantees that is particularly useful to handle massive data in a parallel fashion. Third, the PIs will develop an efficient parallel Markov Chain Monte Carlo (MCMC) algorithm for nonparametric Bayesian models which will perform as well as traditional MCMC with substantially less computational costs. This research will lead to an emergence of ""Splitotics (Split+Asymptotics) Theory"" providing theoretical guidelines for Bayesian practices. The smoothing spline inference results recently obtained by the PIs will be used as a promising tool for achieving the above goals."
"1712907","Collaborative Research:  Nonparametric Bayesian Aggregation for Massive Data","DMS","STATISTICS","09/01/2017","08/16/2019","Guang Cheng","IN","Purdue University","Continuing Grant","Gabor Szekely","08/31/2020","$140,000.00","","guangcheng@ucla.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1269","8083","$0.00","Modern massive data appear in increasing volume and high heterogeneity. Examples include internet searches, social networks, mobile devices, satellites, genomics, medical scans, etc. Bayesian approaches are particularly useful in such context since the complex structures in the data can be naturally incorporated in Bayesian hierarchical models. Besides, uncertainty quantification can be easily executed through Bayesian computation. However, due to storage and computational bottlenecks, traditional Bayesian computation implemented in a single machine is no longer applicable to modern massive data. In this project, a set of nonparametric Bayesian aggregation procedures with theoretical justifications are developed based on a standard parallel computing strategy known as Divide-and-Conquer. This research will significantly enhance the availability of Bayesian tools and software for analyzing massive data. The educational plan of the project will be in the form of graduate student advising and offering of special topics courses. <br/><br/>This project consists of three major components. First, the PIs will establish a Gaussian approximation of general nonparametric posterior distributions which serves as a theoretical foundation for general distributed Bayesian algorithms. Second, the PIs will develop a nonparametric Bayesian aggregation procedure with theoretical guarantees that is particularly useful to handle massive data in a parallel fashion. Third, the PIs will develop an efficient parallel Markov Chain Monte Carlo (MCMC) algorithm for nonparametric Bayesian models which will perform as well as traditional MCMC with substantially less computational costs. This research will lead to an emergence of ""Splitotics (Split+Asymptotics) Theory"" providing theoretical guidelines for Bayesian practices. The smoothing spline inference results recently obtained by the PIs will be used as a promising tool for achieving the above goals."
"1712714","Collaborative Research: Theoretical and Methodological Frameworks for Causal Inference of Peer Effects","DMS","STATISTICS, Methodology, Measuremt & Stats","07/01/2017","07/05/2017","Jun Liu","MA","Harvard University","Standard Grant","Gabor Szekely","06/30/2021","$238,737.00","","jliu@stat.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269, 1333","","$0.00","Understanding how friends or peers interact and affect each other is often of great interest in biomedical studies and the social sciences.  However, it is not entirely clear how to quantify and develop inference for peer effects using a formal statistical framework.  This project will focus on the development of a statistical causal inference framework to address these challenges, with the goal of developing both theoretical and methodological tools for a wide class of questions involving inference of peer effects.  The methods will be applied to investigate peer effects among university students with different academic backgrounds.  The research could provide important guidance for decision and policy makers.<br/><br/>The classical potential outcomes framework for causal inference assumes no interference among experimental units.  In some empirical studies, interference is a nuisance that complicates analysis and should be avoided by careful experimental design.  In many applied fields, however, group or network structures exist and could cause interference among units.  Interference is no longer a nuisance in these applications, because studying the pattern of causal effects with interference is the scientific question of interest with important implications for policy or decision making.  The existing literature discusses external interventions on the units, where the networks, clusters or groups that induce interference are known a priori.  The new framework allows for the development of inferential tools for causal inference with interference from the Fisherian, Neymanian, and Bayesian perspectives. Under the Fisherian view, randomization tests will be used to detect deviations from the sharp null hypothesis without imposing further structural assumptions. Under the Neymanian view, randomization-based point and interval estimators, which serve as the basis for finding optimal treatment assignments will be developed.  Under the Bayesian view, hierarchical models will be developed to accommodate complex structures of real-life data and incorporate information from multiple groups with longitudinal outcomes. The project will also lead to the development of open-source R software. This project is supported by the Division of Mathematical Sciences and the Methodology, Measurement, and Statistics (MMS) Program in the Directorate for Social, Behavioral, and Economic Sciences."
"1712957","Investigation of Bayes Procedures: Theory, Modeling, and Computation","DMS","STATISTICS","07/01/2017","05/19/2017","Chao Gao","IL","University of Chicago","Standard Grant","Gabor Szekely","06/30/2020","$200,000.00","","chaogao@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","","$0.00","Bayesian analysis is a widely used technique in data science for estimation, prediction, and interpretation. The new era of complex and big data imposes unprecedented challenges to Bayesian statistics. This research project addresses these new challenges from three different perspectives. First, the investigator will study the relation between prior knowledge and scientific conclusion by conducting rigorous mathematical analysis in the framework of Bayesian statistics. Second, the investigator aims to find novel ways of modeling data sets that can take into account new features of modern big data. Finally, the investigator intends to push the boundary of Bayesian computation by inventing new algorithms that are both fast and theoretically sound. The results of the research are expected to have a positive impact in areas that apply Bayesian statistics on a routine basis, including population genetics, astronomy, computer vision, political science, social science, and animal science.<br/><br/>Bayesian analysis is an important statistical framework for both modeling and computation. However, applying Bayesian procedures correctly when encountering a specific problem is non-trivial. The selections of prior, likelihood, and algorithm all influence the final conclusion drawn from a posterior distribution. Despite many successful applications of Bayesian analysis in various scientific areas, solid theoretical foundations on how to perform Bayesian inference are still lacking. The goal of this project is to develop a coherent theory on optimal Bayesian inference. Specifically, the investigator will study: 1) Bayesian theory: optimal posterior contraction in parametric, nonparametric and high-dimensional models; 2) Bayesian modeling: likelihood functions that are free of nuisance parameters and Bayesian edge-exchangeable network analysis; 3) Bayesian computation: algorithmic and statistical properties of variational inference; and 4) applications to single-cell RNA sequencing analysis."
"1712958","Hidden Components in Modern Applications","DMS","STATISTICS","07/01/2017","04/30/2017","Zheng Ke","IL","University of Chicago","Standard Grant","Nandini Kannan","05/31/2019","$200,000.00","","zke@fas.harvard.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","8083","$0.00","In the era of Big Data, researchers often encounter datasets that are large in size and complex in structure where the information of interest is usually contained in ""components"" hidden in the enormous amount of noise.  Examples include communities in large social networks, topics in text documents, and confounding factors in genome-wide association studies (GWAS).  Extracting these hidden components is an interesting but challenging problem. This project will address these challenges and include applications to many scientific areas including social networks, text mining, genomics, and genetics. The project will include (a) collection of large social networks data, (b) development of new models, methods, and theory for extracting hidden components in network analysis, text mining, and genome-wide association studies, and (c) a study of knowledge discovery using academic research data such as co-authorship and citation relationships. The research will have an impact in linguistics, social sciences, cancer research, and knowledge discovery.  <br/><br/>This project aims to develop statistical models, methods, and theory for inferring and utilizing hidden components in complex data, especially matrix data.  The goals of the project include: (1) Development of simple and fast methods for network mixed membership estimation and topic model estimation. These methods, based on nontrivial modifications of Principal Component Analysis (PCA), are easy to implement and can handle very large data. (2) New methods and theory for detecting and estimating rare and weak effects in GWAS.   Problems related to optimal ranking of genes in the presence of complex correlation structures and detection of weak spikes in large covariance matrices will be considered. (3) A study of social network structures of scientific researchers.  The PI and her collaborators will collect meta-information from published articles in representative statistics journals to understand social network structures and other features of the statistics community. (4) Development of new random matrix theory (RMT) for statistical analysis."
"1653017","CAREER: Flexible Parsimonious Models for Complex Data","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2017","03/14/2017","Jacob Bien","NY","Cornell University","Continuing Grant","Gabor Szekely","08/31/2017","$109,228.00","","jbien@marshall.usc.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269, 8048","1045","$0.00","Researchers throughout academia, industry, and government are generating data at scales and levels of complexity far beyond what could previously have been imagined.  Complex data demand statistical models that are sufficiently flexible to adapt to meaningful, underlying signals, allowing scientists to discover unexpected patterns.  Yet as society relies more heavily on statistical algorithms to make decisions impacting everyday life, it becomes increasingly important for a method's output to be interpretable by non-experts.  This demands parsimony: that simpler explanations be favored over more complicated ones.  For example, the Internet has led to unprecedented quantities of data in the form of text (such as articles, blogs, webpages, consumer reviews, and many other social media products).  Such text data represent a potential treasure trove of insights into the world -- what people are thinking, how this is changing over time, how this varies by location, etc. The investigator develops new statistical methods for overcoming major technical challenges to gleaning useful information from this data.  This same methodology can be applied to the study of the microbiome, the vast community of microbes living in an environment such as the human gut.  Better statistical methods are needed to identify types of microbes in the gut that play a crucial role in human health and disease.  Another problem that is tackled in this project involves modeling data collected over time (such as wind-speed data and wildlife monitoring).  The methods that are developed allow for more accurate forecasting, which is crucial in many areas including health and medicine and the development of lower cost energy systems.  The last major area in this project is devoted to making the process of statistical research more efficient and its software of higher quality and easier to share across the community of statistical researchers.  Finally, all three research objectives are closely integrated with educational outcomes, including the supervision and teaching of graduate students, outreach to non-statisticians and non-scientists, and the release of undergraduate-accessible mini-papers describing the investigator's new research findings.<br/><br/>This project focuses on the design of new statistical methods that balance two important and often opposing needs: flexibility and parsimony.  (1) Building predictive regression and classification models is difficult when the features are highly sparse.  While many methods focus on the challenge of high dimensionality, relatively few have considered the obstacle posed by features that are rarely nonzero. The investigator develops a new framework for feature selection when the features are highly sparse that succeeds where preexisting methods fail.  This is studied both from theoretical and computational standpoints.  (2) High-dimensional covariance estimation and time series modeling are two rich, but largely distinct, areas in statistics, which the investigator combines to develop new methods for modeling locally stationary time series.  The added flexibility in going from stationarity to local stationarity must be carefully balanced with parsimony.  (3) A series of area-specific software modules will be distributed freely online building on the investigator's new platform for streamlining the process of performing simulation studies.  Each module will implement some of the most common models, methods, and metrics used in a given area of statistics research.  The goal is to facilitate the sharing of high-quality, reproducible simulation code in the statistics research community by creating an easily-adaptable standardized format."
"1840555","Collaborative Research: Scalable Bayesian Methods for Complex Data with Optimality Guarantees","DMS","STATISTICS","10/10/2017","06/21/2018","Debdeep Pati","TX","Texas A&M University","Standard Grant","Gabor Szekely","06/30/2020","$39,701.00","","debdeep@stat.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1269","7433, 8083","$0.00","Spectacular advances in data acquisition, processing, and storage present the opportunity to analyze datasets of ever-increasing size and complexity in various applications, such as social and biological networks, epidemiology, genomics, and Internet recommender systems. Underlying the massive size and dimension of these data, there is often a parsimonious structure. The Bayesian approach to statistical inference is attractive in this context in terms of incorporating structural assumptions through prior distributions, enabling probabilistic modeling of complex phenomenon, and providing an automatic characterization of uncertainty. This research project aims to advance eliciting and translating prior knowledge regarding the low-dimensional skeleton of big data to provide realistic uncertainty characterizations while maintaining computational efficiency. Bayesian computation poses substantial challenge in high-dimensional and big data problems. The research aims to develop cutting-edge computational strategies and software packages for implementation to be made available publicly. The project involves graduate students in the research.<br/><br/>The research project focuses on theoretical foundations and computational strategies for Bayesian methods in high-dimensional and big data problems motivated by applications in social networks and epidemiology. Techniques for systematically developing and evaluating prior distributions in high-dimensional problems will be investigated with a special emphasis on the trade-off between statistical efficiency and computational scalability. Specific directions include efficient algorithms for posterior sampling with shrinkage priors, a theoretical framework for divide and conquer strategies in big data problems, fast algorithms for clustering nodes in large networks with unknown number of communities, and methods for discovering structure in sparse contingency tables. The algorithms will be motivated by rigorous theoretical understanding of the behavior of the posterior distribution with a particular emphasis on proper quantification of uncertainty in a distributed computing framework. Software will be developed for each application."
"1818674","On Statistical Modeling and Parameter Estimation for High Dimensional Systems","DMS","STATISTICS","09/06/2017","12/14/2017","Faming Liang","IN","Purdue University","Standard Grant","Gabor Szekely","08/31/2019","$120,658.00","","fmliang@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","","$0.00","The dramatic improvements in data collection and acquisition technologies over the last decades have enabled scientists to collect massive amounts of high-dimensional data that allow for monitoring and studying of complex systems.  Due to their intrinsic nature, many of the high-dimensional datasets, such as omics and genome-wide association study (GWAS) data, have a much smaller sample size compared to the dimension (referred to as the small-n-large-P problem). Current research on statistical modeling of small-n-large-P data focuses on linear and generalized linear models. However, these approaches are often not adequate for modeling complex systems, and estimation of the model parameters is challenging.  This project addresses two fundamental problems, statistical modeling and parameter estimation, toward a valid statistical analysis of high-dimensional data.  Successful completion of this project will generate hands-on tools for statistical inference of high-dimensional complex systems, which can benefit researchers in many areas of science and technology.  In particular, the proposed applications to biomedical studies will lead to accurate tools for detecting biomarkers associated with disease processes and tailoring optimal therapy for individual patients with complex diseases. The research results will be disseminated to the statistical and biomedical communities, via collaboration, conference presentations, books, and articles to be published in academic journals. The project will also have significant impact on education through the involvement of graduate students in the project, and incorporation of results into undergraduate and graduate courses.  In addition, the R package developed under this project will provide a valuable tool for statistical analysis of high-dimensional data.<br/><br/>The current approach to modeling small-n-large-P data focuses on linear and generalized linear models, and casts the problem as variable selection by imposing a sparsity constraint on parameter values.  Although these models have many advantages, such as simplicity and computational efficiency, estimation of the parameters is still a challenging problem.  While regularization is often used in these situations, it can perform poorly when the sample size is small and the variables are highly correlated.  Two new methods are proposed to address these concerns, namely, Bayesian neural network (BNN) and blockwise coordinate consistency (BCC).  The BNN method works by first fitting the data with a feed-forward neural network, conducting variable selection through network structure selection under a Bayesian framework, and resolving the associated computational difficulty via parallel computing. Compared to existing methods, BNN can lead to much more precise selection of relevant variables and outcome prediction for high-dimensional nonlinear systems.  The BCC method works by maximizing a new objective function, the expectation of the log-likelihood function, using a cyclic algorithm and iteratively finding consistent estimates for each block of parameters conditional on the current estimates of the other parameters.  The BCC method reduces the high-dimensional parameter estimation problem to a series of low-dimensional parameter estimation problems.  The preliminary results indicate that BCC can provide a drastic improvement in both parameter estimation and variable selection over regularization methods. The validity of the proposed methods will be rigorously studied and applied to biomarker discovery, precision medicine, and joint estimation of the regression coefficients and precision matrix for high-dimensional multivariate regression."
"1713108","Inference for Dynamic Objects","DMS","STATISTICS","08/01/2017","08/14/2019","Wolfgang Polonik","CA","University of California-Davis","Continuing Grant","Gabor Szekely","07/31/2021","$125,000.00","","wpolonik@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","","$0.00","Modern data collection techniques result in a steady stream of complex data. The field of statistics has an obligation to develop tools for their analysis, allowing users to draw meaningful and correct conclusions. In view of the complexity of the collected data, this is a challenging task. This project will tackle these challenges by not only constructing relevant novel methodologies, but by also analyzing these methods in order to establish a thorough understanding of their strengths and weaknesses. This, in turn, will facilitate an honest evaluation and interpretation of practical data analysis results. One instance of a complex data type considered in this project is network data. A relevant real-world example is the world trade network, consisting of trading indices between pairs of countries. Countries can be grouped into trading blocks, and this project will develop methodology allowing the analysis of the dependence structure between the trading blocks. This will help to identify factors that are driving this dependence, and how they change over time. More generally, the outcomes of this project are expected to impact the field of statistics and various fields of application. This will be achieved by widely disseminating statistical insight, methodologies and theory developed in this project through publications in international statistics journals, presentations at national and international conferences, and by developing relevant software/code to be made available to the community. Moreover, this project will directly contribute to the training of both graduate students and undergraduate students in modern fields of statistics. It is expected that there will be mutual benefits and synergies between this project and the ongoing NSF Research Training Grant in Statistics at UC Davis. <br/><br/>This project seeks to develop novel statistical methods for the analysis of dynamic object data, in particular, networks and functional data. More specifically, this project will (i) study dependence structures in hierarchical time-varying block models for stochastic networks, and apply the resulting methodologies to the analysis of economic network data such as trade networks; (ii) develop a class of continuous-time point process models for random networks, allowing for a flexible model and analysis of the corresponding maximum likelihood estimators in these models; (iii) develop empirical likelihood based inference methodology for functional time series. The project aims at developing methodologies that, on the one hand, are flexible enough and computationally feasible to be useful for complex real-world applications, and that, on the other hand, result in methodologies that allow rigorous statistical analyses providing insight into, and understanding of, their behavior."
"1712983","Collaborative Research: Integrative Large-Scale Data Analysis and Statistical Inference","DMS","STATISTICS","09/01/2017","08/16/2019","Wenguang Sun","CA","University of Southern California","Continuing Grant","Gabor Szekely","08/31/2020","$100,000.00","","wenguans@marshall.usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","1269","8083","$0.00","Recent technological advancements in data collection and processing have led to the accumulation of vast amount of digital information with various types of auxiliary information such as prior data, external covariates, domain knowledge and expert insights. However, with few analytical tools available, much of the relevant data and auxiliary information have been severely underexploited in most current studies. The analysis of big data with complex structures poses significant challenges and calls for new theory and methodology for information integration. This collaborative research aims to develop new procedures, computational algorithms and statistical software to provide powerful tools for researchers in various scientific fields who routinely collect and analyze high dimensional data, which would help translate dispersed and heterogeneous data sources into new knowledge effectively.<br/><br/>This NSF project aims to develop new principles, theoretical foundations and methodologies for integrative large-scale data analysis and statistical inference. An important theme is to study how to combine the information from multiple sources in a unified framework. The project focuses on four types of problems: (i) inference of two sparse objects; (ii) structured simultaneous inference; (iii) simultaneous set-wise inference and multi-stage inference; and (iv) applications in genomics and network analysis. The new integrative framework provides a powerful approach for extracting and pooling information from various parts of massive data sets, and can improve conventional methods by delivering more accurate, informative and interpretable results."
"1712735","Collaborative Research: Integrative Large-Scale Data Analysis and Statistical Inference","DMS","STATISTICS","09/01/2017","07/26/2019","T. Tony Cai","PA","University of Pennsylvania","Continuing Grant","Gabor Szekely","08/31/2020","$349,661.00","","tcai@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","8083","$0.00","Recent technological advancements in data collection and processing have led to the accumulation of vast amount of digital information with various types of auxiliary information such as prior data, external covariates, domain knowledge and expert insights. However, with few analytical tools available, much of the relevant data and auxiliary information have been severely underexploited in most current studies. The analysis of big data with complex structures poses significant challenges and calls for new theory and methodology for information integration. This collaborative research aims to develop new procedures, computational algorithms and statistical software to provide powerful tools for researchers in various scientific fields who routinely collect and analyze high dimensional data, which would help translate dispersed and heterogeneous data sources into new knowledge effectively.<br/><br/>This NSF project aims to develop new principles, theoretical foundations and methodologies for integrative large-scale data analysis and statistical inference. An important theme is to study how to combine the information from multiple sources in a unified framework. The project focuses on four types of problems: (i) inference of two sparse objects; (ii) structured simultaneous inference; (iii) simultaneous set-wise inference and multi-stage inference; and (iv) applications in genomics and network analysis. The new integrative framework provides a powerful approach for extracting and pooling information from various parts of massive data sets, and can improve conventional methods by delivering more accurate, informative and interpretable results."
"1743227","Design and Analysis of Experiments","DMS","STATISTICS","07/15/2017","07/12/2017","Hongquan Xu","CA","University of California-Los Angeles","Standard Grant","Gabor Szekely","06/30/2018","$10,000.00","Weng Wong","hqxu@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","7556","$0.00","The Design and Analysis of Experiments Conference (DAE 2017) will be held at UCLA on October 12-14, 2017. The goal of the conference is to bring together leading researchers of international repute, graduate students and researchers from industry and government, and provide a forum for interaction, discussion, and exchange novel research ideas in design and analysis of experiments. Funds will be used to support students and junior researchers to attend the conference. The conference will be widely announced and selected individuals and organizations will be specially contacted to broaden the diversity of the participants.  Of particular note is that junior researchers who participated in earlier DAE conferences continue to speak very highly of the impact of mentorship and funding they had received from DAEs has had on their careers. Many of them have gone on to make significant contributions in the development of the theory and applications of experimental design, including a few who have received NSF career awards and several more who are now recognizable researchers in the field. Organizers of the conference have full confidence that DAE 2017 will follow the tradition in the previous years and have a significant impact on nurturing young researchers in design and analysis of experiments. Organizers will plan for mentoring activities by carefully pairing senior and junior participants in advance, provide some funding for the students and junior researchers and continue to make a serious effort to broaden participation by under-represented groups.  <br/><br/>Design and analysis of experiments are indispensable in the scientific process as they form an integral part of the discovery process in virtually all scientific disciplines. The present era of big data has brought very exciting and new areas of research for design and analysis of experiments. DAE 2017 is timely and will help advance cutting edge research ideas to tackle challenging emerging issues.  Themes of DAE 2017 include internet experiments, designs for big data, computer experiments, uncertainty quantification, optimal designs in the pharmaceutical industry, computational methods in design of experiments, applications of designed studies to solve industrial and engineering problems, and designs for non-normal data.  Additional information is available at the conference web site:  http://www.stat.ucla.edu/~hqxu/dae2017/"
"1712037","Modeling Spin Configurations and Ranking","DMS","STATISTICS","07/01/2017","05/17/2017","Sumit Mukherjee","NY","Columbia University","Standard Grant","Gabor Szekely","06/30/2021","$197,045.00","","sm3949@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","","$0.00","This project considers the modeling of ranking and categorical data that are strongly dependent in nature. Typical examples of dependent ranking data arise when one ranks students in a class based on exam scores, or when people rank movies on a website. Typical examples of dependent categorical data arise when one studies how the behavioral patterns of people in a social network (say smoking or voting preference) are influenced by their friends. Both types of data have become increasingly common. This project studies modeling schemes for such data aimed at capturing its dependence, which is not assumed to be well understood. The research develops a rigorous understanding of the behavior of these models, which will aid in statistical inferences from data sets of these types.<br/><br/>For the ranking work, the project will study the Mallows model of rankings and its generalizations. Most existing research concentrates on one particular such model, namely the Mallows model with Kendall's tau. This project instead puts forth a general framework based on permutation limit theory to understand the behavior of Mallows models and permutation models in general. For modeling dependent or categorical data, the focus is on Ising models, which originated in statistical physics and have received significant recent attention in statistics and machine learning. The main goal of the research is to develop inference for Ising model parameters that are not computationally prohibitive."
"1712709","Statistical Foundations of Model-Based Variable Clustering","DMS","STATISTICS","08/01/2017","06/13/2018","Florentina Bunea","NY","Cornell University","Continuing Grant","Gabor Szekely","07/31/2020","$250,000.00","Marten Wegkamp","fb238@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","8083","$0.00","The problem of variable clustering is a corner stone in a multitude of areas such as genetics, neuroscience, sociology, macroeconomics, to name a few. In neuroscience, it aids in finding new functionally connected areas. In genetics, it helps advance the discovery of genes with under-explored or unknown functions. In macro-economics it can assist with the creation of new economic indices. Despite its wide-spread importance and potential impact, this problem has not received a systematic methodological and theoretical treatment in the literature. Although clustering algorithms abound, and have a very long history, assessing the validity of their input is somewhat arbitrary. A probabilistic, model-based approach is put forward in this project. This will enable the development of a unified framework for principled statistical variable clustering.<br/><br/>Specifically, this project will introduce and investigate classes of latent variable models for overlapping and non-overlapping variable clustering. The focal points are: (I) The introduction of identifiable latent variable models for clustering. This will provide well defined targets for estimation, and will facilitate the scientific interpretation of the clusters. (II) The development of polynomial time algorithms tailored to these models. (III) The creation of a unifying framework for the theoretical analysis of clustering algorithms, with emphasis on minimax  optimality and high dimensional inference. (IV) The study of the impact of model based clustering algorithms on downstream analyses, with emphasis on graphical models, regression and classification."
"1719789","Bayesian Inference in Statistics and Statistical Genetics","DMS","STATISTICS","07/01/2017","05/17/2017","Min Wang","MI","Michigan Technological University","Standard Grant","Nandini Kannan","06/30/2018","$10,000.00","","min.wang3@utsa.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","MPS","1269","7556","$0.00","This award supports the participation of junior researchers in the research conference ""Bayesian Inference in Statistics and Statistical Genetics"" to be held at Michigan Technological University in Houghton, Michigan, August 16-20, 2017. Bayesian inference for high-dimensional data has received considerable attention from researchers in a wide variety of fields, including climatology analysis, genomics and proteomics studies, time series analysis, and neuroimaging. Further, there is an increasing demand for Bayesian inference in other related fields due to the rapid evolution of high-performance computing.<br/><br/>The purpose of this conference is to stimulate research collaboration in the development, analysis, and application of statistical techniques for complex, high-dimensional data arising in real world applications. The conference will be an excellent forum for exchanging cutting-edge research results, highlighting current research trends and open problems, exploring visions of future work, and stimulating cross-disciplinary research in various areas. The conference will feature plenary talks given by internationally recognized specialists, invited presentations by junior researchers, and a contributed poster session.  Additional information can be found at the conference website:<br/>http://kliak.mtu.edu/2017/"
"1712596","Collaborative Research: Statistical Estimation with Algebraic Structure","DMS","STATISTICS","07/01/2017","07/03/2019","Philippe Rigollet","MA","Massachusetts Institute of Technology","Continuing Grant","Gabor Szekely","12/31/2020","$200,000.00","","rigollet@math.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1269","","$0.00","Scientific and engineering disciplines ranging from structural biology to computer vision rely on data collection and analysis to guide scientific discovery. Critically, in such applications, the systems under study constrain and govern the structure of information in collected data. The goal of this research project is to develop a family of statistical models that enables a systematic extraction of relevant statistical information from these datasets by bringing together interdisciplinary concepts from statistics and optimization. The approach under development aims to provide a new set of statistical tools that is adapted to this class of problems and that could have a transformative impact on several scientific disciplines.<br/><br/>This project is articulated around a core set of techniques to analyze datasets in the context of a latent algebraic structure, often arising from the physical laws underlying the data collection processes. Unlike more traditional statistical problems where a linear underlying structure is often built into the model, data-driven science generates problems with algebraic but often non-linear structure. The project focuses on problems of central importance in a variety of scientific and engineering disciplines, including signal processing, structural biology, and computer vision, that share a similar feature: the need to leverage algebraic structure in order to extract information from data. The project aims at developing a systematic approach to analyze this family of problems, together with a general procedure to construct computationally efficient algorithms using low-rank tensor decomposition. Importantly, these methods can be proved to be statistically optimal and therefore make the most efficient use of collected data."
"1713083","Collaborative Research: Inference for Network Models with Covariates: Leveraging Local Information for Statistically and Computationally Efficient Estimation of Global Parameters","DMS","STATISTICS","07/01/2017","04/25/2017","Peter Bickel","CA","University of California-Berkeley","Standard Grant","Gabor Szekely","06/30/2020","$240,000.00","","bickel@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","","$0.00","Large datasets, which are naturally modeled as a network or graph, arise in almost every field of human endeavor. For example, Facebook is a social network, where nodes are users, with edges corresponding to friendships. In gene networks, nodes represent genes with connections corresponding to their co-expression. In ecological networks, the nodes are animal species, with edges determined according to who eats whom.   A major focus of research for network or graph data has been on identifying community membership of the nodes.  However, what is often more important for scientific purposes is examining the nature and evolution of edge and membership probabilities, for instance changes in gene features of individuals as a function of some unknown factor, like a disease.  The focus on using other measured features of nodes and edges could add, in decisive ways, to the information available from observed edges or interactions between nodes.  These could be disease symptoms or test results, or demographic information of users in social networks.  Statistical inference in such models, despite its importance, has only just begun to be studied.  There are both theoretical and computational challenges, due both to the complexity of models fitted, and the size of data sets.   The research will lead to the development of algorithms for fitting models and statistical measures of confidence, with potential applications to many fields. <br/><br/>The research is focused on block models for graphs, when node or edge covariates are present. When formulated, these models are no longer block models, but models whose membership probabilities depend upon covariates and whose connection probabilities depend both on block membership and individual covariates.  Fitting algorithms involve alternating between fitting block and covariate parameters.  Variational (mean field) approaches which effectively lead to semi-parametric model fitting with nK membership ""nuisance"" parameters, with n representing the number of nodes and K the number of communities, are examined.  As these approaches have been found by the PIs to be unstable for large n, the PIs have already begun to investigate the theoretical and practical aspects of divide and conquer algorithms where many subgraphs are independently fit.   The PIs will study the statistical properties, both asymptotically and through simulations, and develop practicable and computationally stable methods for large, relatively sparse graphs."
"1832303","Collaborative Research: Estimation of Large Species/Population Trees Using Tree Space","DMS","STATISTICS","07/01/2017","03/16/2018","Arindam RoyChoudhury","NY","Joan and Sanford I. Weill Medical College of Cornell University","Standard Grant","Gabor Szekely","07/31/2021","$138,594.00","","arr2014@med.cornell.edu","1300 York Avenue","New York","NY","100654805","6469628290","MPS","1269","","$0.00","The estimation of the evolutionary history of a collection of organisms based on the information contained in their DNA sequences is a problem of fundamental importance in evolutionary biology. The abundance of DNA sequence data arising from genome sequencing projects has led to important computational challenges in the estimation of these phylogenetic relationships. Among these challenges is the estimation of the evolutionary history for a group of species based on DNA sequence information from several distinct genes sampled throughout the genome. This research is focused on the development of computationally efficient methods for estimating the evolutionary history when the number of species under consideration is very large (i.e., hundreds to thousands). This is accomplished by considering collections of three species at a time, and using properties of the estimated evolutionary history for groups of three to infer the overall evolutionary history. Properties and performance of the method will be evaluated theoretically as well as with both simulated and empirical data sets. This work has numerous practical applications, such as the study of the evolutionary relationships among human populations.<br/><br/>Though the amount of genomic data available for inferring phylogenetic species trees has increased rapidly within the last 10 years, few methods have been developed to efficiently estimate species trees for data sets consisting of hundreds or thousands of species. A fast approximation to the maximum likelihood estimate (MLE) that retains desirable statistical properties, such as consistency and asymptotic efficiency, is proposed. Results from preliminary work suggest that this approach will be significantly faster than existing likelihood and Bayesian approaches, while also being highly accurate. The method can be applied to a range of data types, including allele frequency data arising under a Brownian motion model along the phylogeny and single nucleotide polymorphism (SNP) data arising from the coalescent model. A software package will be developed to implement the methodology. The project will also support one PhD student, who will contribute to the development and implementation of the methodology."
"1830864","From Approximate to Exact Designs with Applications to Big Data","DMS","STATISTICS","08/01/2017","03/02/2018","Wei Zheng","TN","University of Tennessee Knoxville","Standard Grant","Gabor Szekely","05/31/2020","$101,570.00","","wzheng9@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","MPS","1269","7433, 8083","$0.00","Design of experiments is an integral part of the scientific process in many areas of research with a direct impact on society, such as the biological sciences, the health sciences, the social sciences, engineering, marketing, and education.  A well-chosen design facilitates the collection of data that, at a minimum cost, maximizes the information for the scientific questions of interest.  Many scientific studies allow for repeated use of conceptual units, so that developing tools for optimal design for these problems has great potential impact.  Particularly in the realm of big data, there is much room for improvement of existing methods for design of experiments, and the tools and concepts under development in this research project have potential to lead to significant gain of information without increasing computational cost.  Results from the project will be made available to researchers in other areas through easy-to-use software that implements the algorithms to be developed. Graduate students will be trained to become researchers in design of experiments. <br/><br/>This project aims to result in a major leap forward in understanding and knowledge of optimal design of experiments. Recent work in the field has had a significant impact on the advancement of optimal crossover designs and designs for interference models for arbitrarily given covariance structures and design size configurations. However, these results have for the most part been limited to approximate designs for relatively simple models. While these results are arguably important in their own right, this project will extend methods and tools to achieve the ultimate goal of deriving exact designs for a wider spectrum of practical models. The results will be a much needed addition to our collective design toolbox. Most importantly, this project will go beyond the territory of design and apply the tools and ideas from design of experiments to subsampling problems emerging in big data with both statistical and machine learning methods under consideration. Preliminary results indicate that this is an opportune time to make these challenging but critical steps."
"1712822","Estimation, Computation, and Uncertainty Quantification in Structured Regression Models","DMS","STATISTICS","07/01/2017","04/19/2017","Bodhisattva Sen","NY","Columbia University","Standard Grant","Gabor Szekely","06/30/2020","$239,994.00","","bodhi@stat.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","","$0.00","In statistical modeling, regression is the primary tool to study the relationship between a response variable and a collection of predictors. In this research project, questions related to estimation, computation, and uncertainty quantification in some structured regression models will be investigated.  The imposed ""structure"" refers to known features (domain knowledge) of a system and helps to reduce the complexity of the fitted statistical model/procedure.  Further, imposing such structures yields interpretable (yet flexible) models.  Special emphasis is given to methods applicable to multivariate data, an area that has received relatively less attention, though often necessary in performing effective data analysis.  Some of the methodological development undertaken in this project will address important scientific questions arising from astronomical data. The investigator also plans to continue the tradition of mentoring undergraduate summer interns and to participate in the NYU GSTEM outreach program, a six-week summer program for high school girls.<br/><br/>The three main topics pursued in this project are: (i) incorporating covariate information in multiple hypothesis testing problems; (ii) convexity constrained estimation and inference in regression models; and (iii) statistical methods that are geared towards detecting piecewise constant/affine structure in a (multivariate) regression function. New methodology will be developed to address these topics along with the development of efficient algorithms for computation. Further, a systematic theoretical study of these procedures, focusing on their adaptive (risk) properties, will be undertaken, and the important issues of inference and uncertainty quantification will be addressed. The intended applications of the research are diverse, ranging from estimation of radial velocity distribution of stars in a distant galaxy (astronomy), to developing methodology for detecting interactions between pairs of neurons (neuroscience), to estimating production and utility functions (economics), and to constructing confidence intervals for parameters in a continuous multivariate piecewise affine regression function (engineering)."

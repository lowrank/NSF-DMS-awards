"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"9970902","Some Problems in Nonparametric Regression","DMS","STATISTICS","07/15/1999","11/17/1999","Randall Eubank","TX","Texas A&M Research Foundation","Standard Grant","John Stufken","06/30/2002","$80,335.00","","eubank@math.asu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","This project is concerned with a number of inference problems in<br/>nonparametric regression analysis.  Two of the problems being considered<br/>involve the use of nonparametric smoothing methodology to assess the<br/>lack-of-fit of certain types of parametric models.  The development of such<br/>lack-of-fit tests has been an active research area for about the last 10<br/>years.   However, effective methods for obtaining analytic assessments of<br/>the relative performance of such tests has not, as yet, been determined.<br/>One of the goals of this project is derivation of a framework for studying<br/>the relative asymptotic efficiency of nonparametric smoothing based tests<br/>using an asymptotic intermediate efficiency approach that makes it possible<br/>to extend the concept of asymptotic efficiency into the nonparametric, or<br/>infinite dimensional, alternative setting.  The other lack-of-fit testing<br/>problem being studied concerns nonlinear parametric regression models.<br/>This problem is of particular interest since it provides a case where, in<br/>some instances, the usual smoothing parameter asymptotics obtain under the<br/>null model and thereby allow for the development of asymptotically<br/>distribution free test statistics.  Another collection of problems under<br/>consideration is concerned with the derivation of suitable variance<br/>estimators and associated heteroscedasticity diagnostics in the context of<br/>partially linear models.  Variance estimators are needed here for a number<br/>of reasons which include their use in testing hypotheses about the<br/>parametric components (e.g., treatment effects) of the model.  A final<br/>class of problems under investigation concerns computational methods for<br/>boundary correcting smoothing spline estimators.  These problems have<br/>implications and applications to the problem of interval estimation in<br/>nonparametric regression that are also being explored.<br/><br/>One of the most common approaches to statistical analysis involves the<br/>fitting of data by a parametric model.  Such models are frequently<br/>developed through consideration of the physical nature of a problem under<br/>study which may suggest a mathematical relationship or model for the data.<br/>For example, in Biology there are mathematical models that have been<br/>proposed for relating growth  (of  humans, animals, etc.) to age, while in<br/>Meteorology there are mathematical models deriving from physics that<br/>attempt to predict the development of storms and weather patterns.  This<br/>project is concerned, in part, with the study and development of various<br/>statistical methods for assessing the validity of parametric models.  The<br/>methods being considered use flexible data fitting techniques known as<br/>nonparametric smoothers to evaluate and compare fits obtained from a<br/>proposed or postulated parametric model.  Those smoothes can be used to<br/>obtain statistical tests that can, in turn, be used to assess the accuracy<br/>of a model in question.  This is particularly important because an<br/>incorrectly specified model can have potentially dangerous consequences in<br/>that it can produce incorrect conclusions and predictions about the process<br/>under study.  In addition to the development of new testing methods,<br/>techniques are being developed for the comparison of different tests to<br/>determine which type of test performs the best in different situations that<br/>might be encountered in practice.  Other problems under study include the<br/>development of computationally efficient methods for computing certain<br/>types of data smoothers and methods for conducting statistical inference<br/>when it may not be possible to completely specify a parametric model for a<br/>set of data.  The latter problem arises frequently in practice where it may<br/>be reasonable to assume a particular parametric form for a portion of the<br/>model corresponding, for example, to presence or absence of cancer in a<br/>subject, but there is no obvious choice for a parametric model involving<br/>other influential variables, such as the time of a subject's evaluations."
"9971186","Non- and Semi-parametric Identification and Prediction of   Autoregressive Models, with Applications to Econometrics","DMS","STATISTICS","08/15/1999","08/10/1999","Lijian Yang","MI","Michigan State University","Standard Grant","John Stufken","07/31/2002","$77,508.00","","yang@stt.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","0000, OTHR","$0.00","This research focuses on (1) data-driven estimation, testing, and multi-step prediction for non- and semi- parametric autoregressive conditional heteroscedastic (ARCH) models, in order to combine the exponentially decaying feature of classic ARCH models with nonparametric flexibility; (2) estimation and lag selection using plug-in bandwidths for non- and semi- parametric seasonal autoregressive models, with improved efficiency for the semiparametric models; (3) test of higher order interaction terms and a linearity test that leads to simpler models with easier interpretation, and improved estimation accuracy; (4) lag selection for multivariate time series and volatility functions, which aids in identifying parsimonious models and hidden structures among variables; and (5) nonparametric multi-step prediction for additive and functional coefficient autoregressive models.<br/><br/>A time series consists of numbers observed over time.  Economic indicators such as the daily exchange rate of the Euro against the US Dollar and the monthly rate of unemployment in the United States are important financial time series. In the study of crimes, the number of crimes committed each month over several years are useful for finding patterns of crime victimization.  Time series analysis attempts to discover from the data how future observations relate to past ones, usually through some elementary functions.  Recent research efforts, however, have demonstrated the disadvantages of imposing simplistic structures on the series.  This research develops flexible new methods to understand the dynamic structure of a much broader class of time series data for which the relationship among observations are nonlinear, infinitely dependent and/or seasonal.  For instance, monthly unemployment rate data usually exhibit seasonal patterns that are best described by a semiparametric seasonal model.  This research enables one to predict future observations based on past information much more accurately. Improved forecasting of foreign exchange, stock and other volatile prices will provide crucial information about the future state of the economy.  New insights into the interaction of various economic indicators could lead to a more informed strategy of economic development.  The tools developed through this research have the potential for analyzing non-economic time series data as well.  For instance, seasonal forecasts of future crime rates may help law enforcement agencies to create a more effective crime prevention plan."
"9971755","Model Selection and Tests of Model Fit","DMS","STATISTICS","08/15/1999","08/26/1999","Jeffrey Hart","TX","Texas A&M Research Foundation","Standard Grant","John Stufken","07/31/2003","$80,000.00","","hart@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, 1303, EGCH, OTHR","$0.00","The goals are to develop methods for testing hypotheses depending on infinitely many parameters, and to study tests of fit for time series data.  This part of the research is linked to the first part in that model selection methods play a crucial role in the lack-of-fit tests investigated. <br/><br/>Mathematical and statistical models are an essential part of the process of scientific discovery.  Such models are intellectual constructs that help scientists describe the essence of what they study, from people and animals to physical and mechanical systems process of scientific discovery.  Such models are intellectual constructs that help scientists describe the essence of what they study, from people and animals to physical and mechanical systems. Some models are better than others, and the proof is in the pudding. Upon collecting data, scientists can apply different models to see which ones ""fit"" their data best.  As data become more and more plentiful, models of increasing complexity become more feasible. New and more sophisticated statistical methodology is required to analyze such complex models. This study addresses two important issues in the analysis of statistical models: the selection of a ""good"" model from a large collection of models and testing the suitability of a single model having special scientific significance. An important area of application for this work is EGCH, and in particular global warming. The methodology developed in this study can be used to decide, for various statistical models, if the recent increase in global temperatures is better explained as a chance occurrence or a fundamental change in climate."
"9980229","Conference on Nonlinear Statistical Models: Implementation and Application","DMS","STATISTICS","09/01/1999","08/25/1999","Constance Wood","KY","University of Kentucky Research Foundation","Standard Grant","Joseph M. Rosenblatt","08/31/2000","$4,000.00","Arnold Stromberg, Richard Stockbridge","cwood@pop.uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","1269","0000, OTHR","$0.00","This award provides partial travel support and local expenses for 10 graduate students and/or new researchers to attend the conference and tutorial session on Nonlinear Statistical Models on November 4-6, 1999 at the University of Kentucky, Lexington, Kentucky.  The conference will focus on the development and implementation of nonlinear stochastic models and particularly, the application to problems in the biological sciences and industry.  Four major sessions are proposed: General Linear Models, Pharmacokinetic Models and Generalized Compartment Models, Mixture Models, and Applications of Models and Industry.  Each session will feature a keynote address and approximately three half-hour talks by leading experts in the field.  The conference begins Friday morning, November 5, and runs through Saturday afternoon November 6, 1999.  In addition, a special one-day workshop consisting of two tutorials will be held on Thursday November 4, prior to the conference.  The topics are: An Overview of Compartmental models, taught by Professor David Allen, and Algorithms for Nonlinear Models, by the conference organizers.  Graduate students and new researchers are encouraged to attend the tutorials, and also present their research in a special poster session."
"9971954","Frontiers of Functional Data Analysis","DMS","STATISTICS","06/15/1999","06/07/1999","Kert Viele","KY","University of Kentucky Research Foundation","Standard Grant","John Stufken","05/31/2002","$34,675.00","","viele@ms.uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","1269","0000, OTHR","$0.00","DMS-9971954<br/>PI: Viele<br/><br/>Abstract:<br/>This research focuses on performing functional data analysis, the study of<br/>sets of curves, when the pattern of variation across the population is<br/>complex. Specifically, we focus on three nonstandard patterns. The first is<br/>when some curves have a particular feature while others do not. The second<br/>is when the curves follow a mixture distribution across the population, and<br/>the third is where the curve parameters are not identically distributed,<br/>but depend extensively on covariates.<br/><br/>Functional Data Analysis allows modeling of complex processes in<br/>meteorology, oceanography, medicine, criminology, and other areas.<br/>Realistic modeling of these processes often requires complex descriptions<br/>of variation across the population. For example, if one observes<br/>temperature continuously, one can observe the daily rise in temperature<br/>during the day and the decrease at night. On many days, but not all, this<br/>cycle is partially interrupted by storms. Models for daily temperature must<br/>accommodate this behavior. The proposed research is intended to increase<br/>the flexibility of functional data analysis so that it may provide more<br/>realistic analyses.<br/><br/>"
"9806966","Second International Conference on High Dimensional Probability","DMS","PROBABILITY, STATISTICS","02/01/1999","01/07/1999","Jon Wellner","WA","University of Washington","Standard Grant","K Crank","01/31/2000","$10,000.00","Evarist Gine, Michel Talagrand","jaw@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1263, 1269","0000, OTHR","$0.00","9806966<br/>Wellner<br/><br/>This award provides funds to partially support a research conference in high dimensional probability to be held at the University of Washington, August 1-9, 1999.  The purpose of the conference is to bring together analysts, probabilists, and statisticians with interests in this area of research.  This is currently a very active area of research in probability, which is demonstrating its relevance to applications in statistics, other areas of probability, and to problems in physical science.<br/>"
"9972008","Graphical Markov Models with Interpretable Structure","DMS","STATISTICS","12/15/1999","10/29/2001","Thomas Richardson","WA","University of Washington","Continuing Grant"," Shulamith T. Gross","11/30/2003","$155,000.00","","tsr@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, 1305, 9149, EGCH, MANU, OTHR","$0.00","9972008<br/><br/>One of the most central ideas of multivariate statistics is the modeling of dependencies among a set of stochastic variables. The aim of such an analysis is often to try to provide some insight into the process which generated the data. Graphical Markov models (GMMs) use graphs to represent multivariate statistical dependencies in a parsimonious and computationally efficient manner. Directed acyclic graphs (DAGs) also provide a natural formalism for representing data generation mechanisms.  In the case of observational data it is often not known whether all of the relevant variables have been measured, or whether unmeasured 'confounding' variables are present. Since the class of DAG models is not closed under marginalization, if a subset of the variables involved in a data generating DAG are observed, then the resulting statistical model for the observed margin will not necessarily correspond to a DAG. This project will develop graphical Markov models that can represent the conditional independence relations imposed by a DAG model on an observed marginal.  This involves parametrizing distributions described implicitly via a set of conditional independence constraints; developing techniques for estimating these models, and formulating computationally tractable algorithms for performing model selection.<br/><br/>Among their many applications, GMMs have become prevalent in statistical science for the analysis of categorical data in contingency tables, for the modeling of spatially-dependent processes such as the spread of epidemics in human and animal populations, and for the development of early warning systems for severe weather conditions.  They are used in computer science (as Bayesian networks) for information processing and retrieval, for robotics, computer vision, and pattern recognition, for the debugging of complex programs (such as Windows 95), and for the representation of expert systems for medical diagnosis.  In decision science (as influenced diagrams) as models for information flow and control and for combining the opinions of many decision-makers.  Similar models have long been used infield such as genetics, sociology, econometrics, and cycle metrics.  A crucial feature of all these models is that they represent complex networks of causal dependencies.  In addition, these models allow for fast computational implementation.  These features have led directly to their central role in the development of software that can ""reason"" about real world problems.<br/>"
"9978370","Statistical Innovations to Neurotoxicity Risk Assessment","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","10/01/1999","09/28/2004","Yiliang Zhu","FL","University of South Florida","Standard Grant","Grace Yang","03/31/2005","$204,175.00","Marie Davidian","yzhu@hsc.usf.edu","4202 E FOWLER AVE","TAMPA","FL","336205800","8139742897","MPS","1253, 1269","1386, 9187, EGCH","$0.00","9978370 <br/><br/>Neurotoxic potential of environmental substances is typically evaluated through laboratory experiments, in which a small number of subjects are randomly assigned to 4-5 exposure groups, and multiple outcomes, including continuous, ordinal data, are measured repeatedly.  The factorial designs, however, do not always provide sufficient information for characterizing the underlying trajectories of neurotoxic effects over time.  Often the lowest dose that induces the toxic effects is identified using analysis of variance or multiple comparison, and is used to derive a safety standard, without accounting for the magnitude of the effects, or the underlying dose-response relationship.  Despite the US EPA's recommendation in its latest risk assessment guidelines, the use of dose-response modeling and subsequent estimation of a benchmark dose, a new approach leading to better safety standards have not been produced.  This project proposes to conduct dose-time-response modeling of neurobehavioral toxicity using linear/non-linear and generalized linear mixed-effects models. Models that characterize transient and competing effects are considered.  The use of mixed effects makes it possible to quantify risk for targeted individuals or groups.  The project attempts to develop semi-parametric estimation methods for fitting mixed effects models, exploring the use of GEE-based EM algorithm, for example.  It is hoped that such semi-parametric procedures may not only generate more robust estimates, but also simplify the computational aspects of mixed-effects models.  A second technical focus is to develop semi-parametric, bootstrap-enhanced calibration methods for estimating benchmark doses.  In the context of neurotoxicity risk assessment, benchmark doses lies along a contour line on the dose-time-response surface.  The bootstrap calibration methods can overcome the limitation associated with small sample size, and illustrate sampling uncertainty or variation of the benchmark dose estimates.  A third technical challenge is to develop informative and efficient designs beyond the factorial ones for neurotoxicity screening experiments in terms of dose-time spacing to achieve maximal information recovery on dose-response (e.g. peak effect time), and reliable estimation of benchmark doses.  The project utilizes the database of the IPCS Collaborative Study and the EPA studies on Neurobehavioral Screening Methods.<br/><br/>Neurotoxic effects are recognized as one of the leading workplace disorders.  There are currently more than 65,000 chemical substances that are used in the USA, yet it is not known how many may be neurotoxic in humans.  It is estimated, however, that a large number of the more than 500 registered active pesticide ingredients affect the nervous system of the targeted species to varying degrees.  Of the 588 chemicals listed by the American Conference of Governmental Industrial Hygienists, 167 affected the nervous system or behavior at some exposure level.  As a result of the impact of neurotoxicity on public health, there are increasing scientific interests in neurotoxic risk to humans, as well as increasing regulatory needs for developing better safety standards against environmental exposure.  In summary, this project 1) develops effective design protocols for neurotoxicity screening tests to improve risk characterization; 2) develops and applies better statistical methods to quantify neurobehavioral disorders; and 3) develops and tests better procedures for computing regulatory standards for potential neurotoxicants.  The methods developed in this project are likely to enhance the scientific foundation of risk assessment, and hence to help improve environmental regulatory policies.  This project is supported jointly by the Statistics Program and the Office of Multidisciplinary Activities (OMA) in MPS.<br/><br/>"
"9971791","Three Topics in Statistics with Applications","DMS","STATISTICS, Methodology, Measuremt & Stats","08/01/1999","07/28/1999","Zhiliang Ying","NJ","Rutgers University New Brunswick","Standard Grant","Dean Evasius","07/31/2003","$108,175.00","","zying@stat.columbia.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269, 1333","0000, OTHR","$0.00","9971791 <br/><br/>This project contains three topics: (1) survival analysis with multiple event-time data, (2) growth curve analysis and, (3) computerized adaptive testing.  Topic (1) develops new parametric families for multiple event times that satisfy marginal proportional hazards requirement and proposes semiparametrically efficient estimator for regression parameter in the marginal Cox model. It establishes nonparametric estimation and testing procedures and their large sample properties for gap-time data, which cannot be handled directly by existing methods due to (informative) right censorship.  It also introduces a class of semiparametric transformation models for recurrent event times and develops inference procedures that are easy to implement and valid under general assumptions.  Topic (2) deals with random growth curves and develops nonparametric testing and semiparametric estimation procedures.  A crucial issue being handled there is how to correct bias cause by incompleteness of the growth curves.  Topic (3) develops statistical methods in design and analysis of computerized adaptive tests.  It proposes novel approaches to item selection through suitable stratification to improve efficiency, item exposure rate distribution and test security.<br/><br/>The three topics are motivated directly from scientific and educational applications.  The first two topics are applicable to many biomedical studies including modeling of disease development, testing superiority of a new treatment and estimation of treatment efficacy.  The third topic originates from the recent emergence of computerizing standardized tests, which include such widely available tests as GRE and GMAT.  The new statistical methods provide more accurate and efficient tools for measuring educational achievements.<br/>"
"9972082","Statistical Methods in Fast Functional MRI","DMS","COMPUTATIONAL NEUROSCIENCE, STATISTICS","08/01/1999","05/09/2001","Larry Shepp","NJ","Rutgers University New Brunswick","Continuing Grant","John Stufken","07/31/2002","$152,000.00","Cun-Hui Zhang","shepp@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1162, 1269","0000, 9216, HPCC, OTHR","$0.00","The proposal focuses on developing statistical methods and related theory <br/>for fast functional magnetic resonance imaging (fMRI), to sharply improve <br/>the time-resolution of present techniques.  Our objective is to improve the <br/>time-resolution of functional magnetic resonance imaging by sampling only a <br/>small fraction of the Fourier transform of the spin density, and using a <br/>wavelet filter to approximately obtain, not the usual susceptibility map, <br/>but instead an integral, representing the total activity, of the difference <br/>in susceptibility between task and pre-task, over a prespecified region of <br/>interest in the brain at successive time-points.  This space/time trade-off <br/>thus allows us to obtain, at high time-resolution, the total activity in a <br/>specified region of the brain, believed to process the specific stimulus/task, <br/>to learn or verify where the brain function takes place.  An example of the <br/>use of the technique is to learn where in the brain the ""rolodex"" of memory <br/>is located whereby one recognizes a familiar face in a fraction of a second.  <br/>To try to find where the rolodex is located by a method which requires more <br/>than a second is a doubtful project; better time-resolution is required (but <br/>high spatial resolution is not). Although some researchers claim faster<br/>measurement capabilities they sacrifice signal/noise.<br/><br/>Statistical methods and related theory will be developed to sharply improve <br/>time-resolution for the technique of functional magnetic resonance imaging (fMRI).  Our objective is to improve the time-resolution of fMRI by acquiring data from only a small fraction of the MRI sampling space, to approximately obtain the total activity of the difference in susceptibility between task and pre-task, over a prespecified region of interest in the brain at successive time-points.  For a typical region of the brain, say describing the hippocampus, believed to be involved in memory, our optimal choice of the sampling region should give a ten fold speed-up compared to the usual method of sampling. An example of the use of the technique is to learn where in the brain the ""rolodex"" of memory is located whereby one recognizes a familiar face in a fraction of a second.  This fast fMRI is expected to have profound and far-reaching consequences in the understanding of brain function, a problem of central scientific interest at the present time."
"9971331","A Noninformative Bayesian Approach to some Finite Population Problems when Auxiliary Variables are Present","DMS","STATISTICS","08/15/1999","08/24/1999","Glen Meeden","MN","University of Minnesota-Twin Cities","Standard Grant","John Stufken","07/31/2002","$75,000.00","","glen@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","9971331<br/><br/>In finite population sampling in addition to the characteristic of interest, there are often auxiliary variables present which contain information about the characteristic.  For example, a priori the population means or some of the population quantiles for these auxiliary variables could be known.  Classical theory has developed a variety of methods to exploit this information with different levels of overall effectiveness.  The Polya posterior was developed as a noninformative Bayesian approach to finite population sampling when little or no prior information is available.  The goal of the proposed research is to extend the Polya posterior to problems where auxiliary variables are present.  This will be done by restricting the Polya posterior in a natural way so that given a sample simulated copies of the entire population can be generated which satisfy the constraints induced by the prior knowledge about the auxiliary variables.  Statistical inference can then be carried out using the simulated populations in the usual Bayesian manner. The two main technical problems will be to develop the underlying theory and the techniques for implementing the simulations which are consistent with the prior information at hand. This allows for a coherent approach to problems which standard theory must now consider individually.  Several problems will be identified were certain types of prior information which are often presently ignored can be used in an objective and effective way.  The resulting noninformative Bayesian procedures should have good frequentist properties.<br/><br/>A fundamental problem of statistics is making inferences about a population when information is available only about a sample or subset of the individuals making up the population.  For example, we may want to estimate how many days a typical worker is absent because of illness during a year in a given industry.  In addition to the information in the sample, we may also know the average age or median education level for all the workers in the industry.  Statisticians have developed various methods to handle such problems depending on what kind of information beyond the sample is at hand.  Some methods work much better than others. In this research we will study a general approach which can effectively make use of a variety of types of information. This is done by using the data in the sample and the prior information to construct simulated or random copies of the entire population which are consistent with both the sample data and the prior information.  By considering the variability among these randomly generated copies of the population one can find not only an estimate of the quantity of interest but a measure of uncertainty associated with the estimate. <br/><br/><br/>"
"9971814","Computing Environments for Statistics","DMS","STATISTICS","07/15/1999","04/26/2002","Luke-jon Tierney","MN","University of Minnesota-Twin Cities","Continuing Grant","Marianthi Markatou","06/30/2003","$366,619.00","","luke@stat.uiowa.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","9971814<br/><br/>The objective of the proposed research is to improve the ability of researchers, instructors, and other users of statistical methodology to apply statistical methods in new problem areas.  The proposed research is concerned in particular with the exploration and development of new principles for the design of statistical software to take advantage of modern computing power.  This work explores the use of compilation and multi-threading in statistical computing.  It examines the advantages of different language forms, including graphical languages, for expressing statistical ideas. Principles for developing effective systems for dynamic graphical explorations are also explored.<br/><br/>The objective of the proposed research is to improve the ability of researchers, instructors, and other users of statistical methodology to apply statistical methods in new problem areas.  The proposed research is concerned in particular with the exploration and development of new principles for the design of statistical software to take advantage of modern computing power.  This work examines how to build on new developments in computer science to allow statistical data analysis to take full advantages of the speed and graphics capabilities made available by advances in computer hardware.  The insights and methods developed in this research will lead to improvements in the practice and the teaching of statistical ideas within mathematics, engineering, and other areas of technological education.<br/>"
"9981458","Collaborative Research:  Non-Linear Population Dynamics:    Mathematical Models, Biological Experiments and Data        Analysis","DMS","POPULATION DYNAMICS, POP & COMMUNITY ECOL PROG, APPLIED MATHEMATICS, STATISTICS, COMPUTATIONAL MATHEMATICS","09/01/1999","07/05/2001","Brian Dennis","ID","Regents of the University of Idaho","Continuing Grant","Michael Steuerwalt","08/31/2002","$84,000.00","","brian@uidaho.edu","875 PERIMETER DR MS 3020","MOSCOW","ID","838449803","2088856651","MPS","1174, 1182, 1266, 1269, 1271","9169, 9263, EGCH","$0.00","Dennis<br/>9981458<br/>     Understanding the fluctuations in animal numbers is a<br/>central issue in population biology that has far-reaching impact<br/>on and implications for problems ranging from food production to<br/>the conservation of species.  Nonlinear dynamics opens the way to<br/>a new phase of population research in which theory and<br/>experimentation focus on phenomena such as cycles and<br/>quasi-periodicity, chaos and strange attractors, multiple<br/>attractors and complicated basins of attraction, saddle sets and<br/>their stable manifolds, and so on.  This interdisciplinary<br/>project, in which Robert Costantino, Jim Cushing, Brian Dennis,<br/>Robert Desharnais, and Shandelle Henson collaborate, covers a<br/>spectrum of activities essential to testing nonlinear population<br/>theory: the translation of the biology into the language of<br/>mathematics and back again, the analysis of deterministic and<br/>stochastic models, the development and application of statistical<br/>techniques for the analysis of data, and the design and<br/>implementation of biological experiments.  A series of experiments<br/>with flour beetles of the genus Tribolium provide rigorous<br/>experimental tests of nonlinear phenomena.  Topics include<br/>nonlinearity in the context of stochasticity, chaos and<br/>population control, the impact of periodic environments on animal<br/>abundance, demographic dynamics and natural selection,<br/>nonequilibrium species interactions, and statistical questions<br/>concerning parameterization and validation of models.<br/>     With a sound understanding of the underlying dynamics of<br/>animal populations, ecologists can anticipate the consequences of<br/>environmental degradation and learn better ways to manage natural<br/>populations and control populations of pests.  For example, one of<br/>the experiments suggests that, by taking advantage of the<br/>""sensitivity to initial conditions"" that is the hallmark of a<br/>chaotic system, managers can dramatically decrease pest<br/>population numbers without the use of chemical pesticides by<br/>making small perturbations at critical times.  The marriage of<br/>ecological theory and experiments by the interdisciplinary<br/>research team leads to new techniques for the application of<br/>mathematics to ecological problems.  The project is supported by<br/>the Applied Mathematics, Computational Mathematics, and<br/>Statistics programs in MPS and the Population Biology and Ecology<br/>programs in BIO.<br/>"
"9907259","Contract Services for NSF/EPA Competition","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, Methodology, Measuremt & Stats","03/15/1999","06/08/2000","Claire Razaq-Hines","MD","ALLIED TECHNOLOGY GROUP INC","Contract-BOA/Task Order","Joseph M. Rosenblatt","09/15/1999","$5,707.00","","razaqc@lan.alliedtech.com","46610 EXPEDITION DR STE 101","LEXINGTON PARK","MD","206532112","3013091234","MPS","1253, 1269, 1333","0000, OTHR","$0.00",""
"9982846","Monte Carlo Filters for Nonlinear and Non-Gaussian Dynamic Systems","DMS","STATISTICS, SIGNAL PROCESSING SYS PROGRAM","09/15/1999","09/17/1999","Rong Chen","IL","University of Illinois at Chicago","Standard Grant","John Stufken","08/31/2002","$55,000.00","","rongchen@stat.rutgers.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269, 4720","0000, OTHR","$0.00","9982846<br/><br/>This research is concerned with Monte Carlo methods for nonlinear and non-Gaussian dynamic systems.  The basic idea of the Monte Carlo filter is to sequentially generate random samples from the distribution of the current state of the system.  Compared with traditional filtering methods, simple, flexible, yet powerful Monte Carlo techniques provide effective means to deal with complex dynamic systems.  The investigator studies four specific topics in this research.  Topic (I) studies a special Monte Carlo filter called Mixture Kalman filter for the conditional dynamic linear models, a class of widely encountered systems in practice.  This filter utilizes the `marginalization' operation to improve efficiency.  Topic (II) studies a partial rejection control operation, an operation aimed to generate more efficient random samples in Monte Carlo filters.  Topic (III) studies methods to deal with dynamic systems with unknown system coefficients, an important but difficult problem.  Topic (IV) is devoted to several applications of Monte Carlo filters, including tracking multiple maneuvering targets in a clutter environment; Bayesian receivers in digital wireless communications; on-line estimation of the market volatility in  financial time series analysis; and stochastic system control with non-Gaussian innovations.<br/><br/>Dynamic systems are widely used in almost all fields of applications.  Examples abound in radar or sonar surveillance systems, in mobile telecommunications, in economic and financial data analysis, in computer vision, in medical diagnostics, in speech recognition, in feed-back control and guidance systems, and in Internet traffic monitoring and control, just to name a few.  Most of them are nonlinear and non-Gaussian.  One of the main challenges in dealing with dynamic systems is that they usually require on-line (in real time) estimation and prediction (filtering) of the ever-changing system characteristics, given the continuous flow of observations and information from the system.  The investigator studies and develops Monte Carlo filters for nonlinear and non-Gaussian dynamic systems. The basic idea of the Monte Carlo filter is to sequentially generate random samples to represent the status (distribution) of the current state of the system. Modern computing power has made it feasible to use Monte Carlo methods as filtering methods in application.  Compared with traditional filtering methods, simple, flexible, yet powerful Monte Carlo techniques provide effective means to deal with complex dynamic systems.  This research enriches the toolkit of on-line filtering for nonlinear and non-Gaussian dynamic systems. Faster and more effective filtering techniques will certainly have significant impact on a wide range of important applications in real life and make significant contributions to advancing science and technology.  It also promotes the interdisciplinary research between engineering and statistics.<br/><br/>"
"9971206","Methodology For Analyzing Spatial Data","DMS","STATISTICS","07/01/1999","04/16/2001","Alan Gelfand","CT","University of Connecticut","Continuing Grant","John Stufken","06/30/2003","$157,275.00","","alan@stat.duke.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","0000, 9197, EGCH, OTHR","$0.00","<br/>Recently, considerable interest has begun to focus on <br/>spatial data problems.  The statistician finds an exciting<br/>opportunity to expand the current analytical repertoire, <br/>which is dominated by descriptive summary and ad hoc inference<br/>procedures.  Bringing formal stochastic modeling and the <br/>resultant full inference which becomes available, to the <br/>analysis of spatial data becomes attractive.   Fully model-based<br/>approaches offer challenges both in supplying a sufficiently<br/>flexible framework and fitting models within such a framework.<br/>The proposed research considers three such challenging problems.<br/>They are (i) the handling of misaligned data layers which arises<br/>when there is interest in relating variables which are collected<br/>from different sources and these sources use different areal<br/>partitions of a region, (ii) the handling of bivariate (and, more<br/>generally, multivariate) spatial processes arising for the<br/>observed data or as latent (second stage) processes in a <br/>hierarchical model, and (iii) the handling of large spatial<br/>datasets within the so-called ""geostatistical"" perspective<br/>which, using first or second stage Gaussian specifications,<br/>presents a likelihood whose evaluation requires high-dimensional <br/>matrix inversion.<br/><br/><br/>Spatial data arises in many fields of application.  For instance,<br/>in ecology and evolutionary biology, investigation of the process<br/>of deforestation is an important area.  Such a process inherently<br/>exhibits spatial pattern which evolves over time and hence connects <br/>to federal strategic interest in environmental and global change<br/>(EGCH).  It is of interest to use socioeconomic information in<br/>addition to physical features of the land area to understand this<br/>process.  In financial/real estate applications it is of interest<br/>to index residential propery values.  The familiar maxim, ""location,<br/>location, location"" anticipates spatial association in housing prices.<br/>In epidemiology one seeks to identify spatial patterns/clustering of<br/>disease incidence in order to assess areas of high risk.  Incidence<br/>rates have to be adjusted to reflect differences in population<br/>size and exposure to risk factors.  In all of these applications <br/>simple descriptive summary of the collected data will not be adequate.  <br/>Rather, one needs to make inference, e.g., to assess which factors <br/>explain the deforestation, to predict the price of a home when it <br/>goes on the market, to conclude that a particular area is at a <br/>significantly higher risk for a particular disease.  The research  <br/>under this grant support proposes the use of probabilistic modeling <br/>to address these questions.  Appropriate specification of, fitting <br/>of and drawing inference under such models are the objectives.<br/>"
"9971405","Flexible and Adaptive Statistical Modeling","DMS","STATISTICS","08/01/1999","05/29/2003","Robert Tibshirani","CA","Stanford University","Continuing Grant","Xuming He","07/31/2004","$497,285.00","","tibs@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, 9179, 9216, 9217, HPCC, OTHR, SMET","$0.00","9971405<br/><br/>During the 1980's, researchers who were trying to model learning in the brain developed several novel learning algorithms for multilayer non-linear neural networks.  Somewhat incidentally, these algorithms turn out to be very powerful techniques for adaptive regression and classification, and have proven their usefulness independent of whether or not they are a good model for the brain.  They are now being applied to medical diagnosis, chemical process control, shape recognition and a wide range of other important practical problems.  At the same time, there have been significant advances in adaptive techniques in the field of statistics.  The new statistical methods are more powerful than classical techniques such as linear regression and linear discriminant analysis.  Some of the recently developed procedures include CART (Classification And Regression Trees), generalized additive models, MARS (Multivariate Additive Regression Splines), and sophisticated versions of nearest neighbor algorithms that learn an appropriate metric for the input space.  Although they come from different fields using different terminologies, these methods have much in common.  One item in this proposal is a research monograph that seeks to bring many of these ideas under one umbrella, explaining them in a unified fashion.  Two other items explore some recent new methods for improving classifiers and for adaptive model selection.<br/><br/><br/>This work aims at developing new models and methods for making predictions based on historical data.  These techniques are important in many different fields including medical diagnosis, financial forecasting and industrial process control.  The area of biotechnology is an especially important application for these methods.  Scientists now have techniques for measuring gene expression levels for thousands of genes at the same time, allowing the exciting possibility of determining which human genes are involved in a diseases such as cancer and heart disease.  Sorting through the mass of information is like trying to find a needle in a haystack, and predictive methods like the ones studied here will provide an important tool in this search.<br/><br/>"
"9875598","CAREER:  Statistics for Flow Cytometry and Freshman Seminars","DMS","STATISTICS","09/01/1999","05/14/1999","Guenther Walther","CA","Stanford University","Standard Grant","Gabor Szekely","08/31/2005","$200,664.00","","Walther@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, 1045, OTHR","$0.00","9875598<br/><br/>Flow cytometry is a technology for analyzing and sorting cells that requires the analysis of a large number of high-dimensional data.  In collaboration with one of the pioneering flow cytometry groups the proposal writer will develop the statistical methods necessary to transform this technique from a manually operated apparatus to an internet-accessible digital library.  The research will exploit some surprising properties of log-concave densities to solve various statistical and computational problems in mixture analysis and visualization.  The investigator will also organize summer conferences for junior faculty members to learn the tools necessary to effectively teach freshman seminars, which are gaining a firm foothold in many universities.<br/><br/>Flow cytometry has become a widely used tool to analyze cells.  Until the mid 1980's, the technique was used almost exclusively in the research community.  Recently, it has become a prevalent instrument in the clinical setting: it is the primary method for staging HIV disease, and it is commonly used to detect leukemia and lymphomas. Virtually every modern hospital now has flow cytometer.  The analysis of the resulting data poses considerable statistical challenges, which so far have prevented an effective digital use of the results. The investigator will address these issues with novel statistical techniques. Further, the grant will support summer conferences organized by the investigator to disseminate the tools necessary to effectively teach freshman seminars. These seminars are designed to involve undergraduates in interactive learning and research at an early stage, and are getting increasingly popular at many universities.<br/>"
"9909164","Workshop on Statistics and Information Technology","DMS","STATISTICS","09/01/1999","09/01/1999","Jerome Sacks","NC","National Institute of Statistical Sciences","Standard Grant","Joseph M. Rosenblatt","06/30/2001","$28,000.00","","sacks@niss.org","19 TW ALEXANDER DR","RESEARCH TRIANGLE PARK","NC","277090152","9196859300","MPS","1269","0000, OTHR","$0.00","This award provides travel support and local expenses for 25 participants of a two-day public workshop on Statistics and information technology (IT), to be held on September 16-17, 1999, at the NISS building in Research Triangle Park, North Carolina. The purpose of the workshop is to expose statisticians to researchers and to research problems in information technology.  Several important problem areas in information technology will be presented from the perspectives of both IT and Statistics, with ample opportunity for questions and discussion.  The topic areas to be discussed are: digital government, Internet traffic measurement and analysis, software development, data/model integration, and human-computer interaction.  A workshop report will be prepared and made available over the World Wide Web shortly after the conference."
"9971701","Semiparametric Regression for Multivariate Failure Time Data","DMS","STATISTICS","09/01/1999","04/09/2003","Frits Ruymgaart","TX","Texas Tech University","Standard Grant","Robert J. Serfling","08/31/2003","$90,000.00","","ruymg@math.ttu.edu","2500 BROADWAY","LUBBOCK","TX","79409","8067423884","MPS","1269","0000, OTHR","$0.00","Yang, Song DMS-9971701<br/>ABSTRACT<br/><br/>Multivariate failure time data are frequently encountered in scientific investigations.  Their analysis is much complicated by correlations among components, censoring, and time-dependent covariates.  This project initiates some new approaches in the semiparametric regression analysis of multivariate failure time data.  Various univariate semiparametric regression models are considered for the components of multivariate failure time data.  They include some familiar models, such as the Cox model, as well as some new models.   Inference procedures are proposed from diverse considerations, ranging from  pseudo likelihood to martingale residuals and estimating functions.  The new procedures are applicable to the currently two main modeling approaches: the marginal and the frailty approaches.  The proposed methods much enhance and extend the existing results by considering various semiparametric marginal models, by proposing several classes of general inference procedures, and by eliminating the computational burden that is almost always associated with the existing frailty methods. A key ingredient in the new approaches is the use of some weighted empirical functions.<br/><br/>Applications of multivariate failure time data analysis may be found in industrial life testing, clinical trials and genetic epidemiology, among others. A machine component may break down repeatedly; a patient may experience failures of several organs; phenotypic traits may be collected on human pedigrees that consist of blood relatives and their spouses.  Often, data of interest are not observed completely due to the time frame of the study design or loss of follow-up; also, risk factors, such as treatment indicators, may vary over time.  Adding to the complication are the correlations among observations taken from the same patient or within the same pedigree.  The new approaches deal with these challenges and propose various modeling and analysis tools to the study of multivariate failure time data.  The emphasis is on developing statistical procedures that are flexible, robust, simple to interpret and easy to implement, while maintaining good efficiency properties.<br/>"
"9817969","New Researchers Conference in Conjunction with the          Institute of Mathematical Statistics to be held August 4-7, 1999 in Baltimore, Maryland","DMS","STATISTICS","05/01/1999","04/02/1999","Carey Priebe","MD","Johns Hopkins University","Standard Grant","Joseph M. Rosenblatt","04/30/2000","$12,000.00","","cep@jhu.edu","3400 N CHARLES ST","BALTIMORE","MD","212182608","4439971898","MPS","1269","0000, OTHR","$0.00","9817969<br/>Priebe<br/> This award provides funds to partially support the fourth New <br/>Researchers Conference sponsored by the Institute of Mathematical <br/>Statistics.  The purpose of this conference is the development of <br/>junior researchers in statistics and probability, with special <br/>emphasis on mentoring women and underrepresented minorities.  The <br/>meeting is planned for August 4-7, 1999 at the Johns Hopkins <br/>University Homewood Campus in Baltimore, just prior to the annual <br/>joint statistical meetings to be held in Baltimore.  Support is <br/>offered to 50 junior researchers, within 5 years of degree, who <br/>will present their work and interact with other colleagues.  <br/>Three senior researchers and a panel of representatives of funding <br/>agencies including NSF, NSA and NIH. <br/>"
"9972598","Bayesian Nonparametric Regression and Density Estimation Using CAR Priors","DMS","STATISTICS","08/15/1999","07/12/2001","Dongchu Sun","MO","University of Missouri-Columbia","Continuing Grant","John Stufken","06/30/2003","$127,750.00","Paul Speckman","sund@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269","0000, OTHR","$0.00","9972598<br/><br/>The research deals with several closely related areas of Bayesian hierarchical models and function estimation problems.  The investigators explore the use of Bayesian techniques for function estimation and their frequentist properties and use those properties to help choose appropriate noninformative priors.  One aspect of their study is the use of conditional autoregressive models (CAR) and their generalizations to capture spatial variability and as prior distributions in nonparametric regression and density estimation problems.  Preliminary results suggest good performance for the Bayesian smoothing parameter choice.  Using the fundamental relationship between penalized methods for function estimation and Bayesian estimation with high order CAR models as priors, they use sampling-based techniques (such as Markov chain Monte Carlo) to deal with problems with large amounts of data.  The regression and density estimation problems are special classes of hierarchical generalized linear models.  A common feature is the need to specify prior distributions for hyperparameters.  The investigators develop noninformative priors for these purposes and assess their suitability.  <br/><br/>The methods being developed are motivated by problems in transportation, disease mapping and wildlife management.  The transportation problem consists of modeling the behavior of residents in a city and their choice of activities and destinations.  The disease mapping work estimates disease incidence by location, age and gender strata.  The wildlife management issues involve similar problems in estimating hunting success rates for small areas such as counties.  All of these problems entail a spatial component, large data sets, and statistical inference to small regions.  There is a great deal of current interest in these topics, but many theoretical issues have not been addressed.  Results are being developed to help understand some of the theoretical properties of existing methods and to suggest new ones.  By using objective Bayesian techniques, new methodology is being developed to handle a wide range of practical problems.  These methods are computer intensive but feasible with the latest computer technology.<br/><br/>"
"9971784","Efficient Condensation of Spatial/Temporal Data","DMS","STATISTICS","08/15/1999","05/02/2001","Ian McKeague","FL","Florida State University","Continuing Grant","John Stufken","07/31/2002","$90,000.00","","im2131@columbia.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","0000, OTHR","$0.00","9971784<br/><br/>The proposed research concerns the condensation and analysis of spatial/temporal data for purposes of classification and anomaly detection.  The focus is a new approach to landmark estimation and spatial clustering based on spatial point processes.  Such processes provide a potentially rich class of models to express high-level prior knowledge about curves and shapes.  These models are more suitable than commonly used discrete Markov random field models and template deformation models, especially in situations where there is a lack of training data.  A version of Bayesian nonparametric curve estimation will be developed using the new approach.  A principal objective is to achieve a parsimonious specification of the landmark information (describing the location of the knots in a spline curve for example).  A further objective is improved methods for simultaneous inference from temporal data.<br/><br/>The benefit of an efficient statistical model is data condensation, or the reduction of often unmanageably large data sets to a parsimonious form, without the sacrifice of key statistical information. Another benefit is that it can lead to the discovery of interesting anomalies, given the availability of suitable inferential methods to lend credence to such findings.  Specific applications to be explored include off-line signature recognition and 2D-gel electrophoresis imaging. <br/><br/>"
"9971698","Estimating Parameters in  Spike-convolution Models and Mixture Models","DMS","STATISTICS, Plant Genome Research Project","06/15/1999","06/17/1999","Lei Li","FL","Florida State University","Standard Grant","John Stufken","05/31/2002","$79,866.00","","lilei@usc.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269, 1329","9184, BIOT","$0.00","9971698<br/><br/>This research links the parametric deconvolution problem in the spike-convolution model with the estimation problem in finite mixture models.  It aims to weave together good results on algorithms and asymptotics from both sides, and develop new methodologies, which are implementable in computation and efficient in theory.  The first object of this research, the spike-convolution model, is introduced as part of the models proposed for DNA sequencing by the PI and his collaborators.  The current sequencing scheme named after Sanger combines three techniques: enzymatic reactions, gel or capillary electrophoresis and fluorescence-based detection.  This biochemical procedure produces a four-component vector time series for each DNA fragment.  The task of DNA base-calling is to recover the underlying DNA sequence from the above time series.  Most of the base-calling errors are caused by the diffusion effect of electrophoresis.  It is found that this diffusion effect can be well described by the so-called spike-convolution model.  It arises when a sparse Dirac spike train is convolved with a fixed point spread function, and additive noise or measurement error is superimposed.  In this model, deconvolution is nothing but a standard parameter estimation problem, where the parameters include the number, locations and heights of the underlying spikes, the baseline and the measurement error variance.  The second object of this research, the finite mixture model, is the framework of many statistical analyses like robustness checking, clustering, estimating density functions, etc.  However, the estimation of the parameters in mixture models can be very troublesome, especially when many components are involved.  It is believed that a broad class of finite mixture models is closely related to the spike-convolution model.  No simple solution exists to the estimation problems in these two models because of the complexity.  This research proposes to combine the method of trigonometric moments with a two-stage model selection procedure, Gauss-Newton algorithm, or EM algorithm depending on the situations.  The numerical and statistical aspects of the new methods and their variants are examined and compared with those of existing methods.  The Toeplitz forms constructed from trigonometric moments and their statistical properties play a key role in the proposed methods, and are investigated in full detail.<br/><br/>This research studies the newly proposed spike-convolution model and the long-standing finite mixture models from a unified perspective.  The former is motivated by the large scale and high throughput DNA sequencing, which is one of the most important aspects of the ongoing Human Genome Project and other genome projects.  The lack of satisfactory deconvolution techniques and statistical models has made DNA base-calling---the data analysis part of sequencing---a bottle neck of these projects.  An effective deconvolution technique, a target of this research, is a fundamental prerequisite for rapid and reliable DNA base-calling.  In fact, similar deconvolution problems arise in many other scientific disciplines like geophysics, spectroscopy, and chromatography.  The research results are also expected to enrich the understanding and methodologies of finite mixture models, which have applications to a diversity of fields such as physics, medicine, and biology.<br/>"
"9820015","52nd Session of the International Statistical Institute, Helsinki, Finland, August 10-18, 1999","DMS","STATISTICS","05/01/1999","04/22/1999","Ray Waller","VA","American Statistical Association","Standard Grant","Joseph M. Rosenblatt","04/30/2000","$20,000.00","","rwalter@calpoly.edu","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","0000, OTHR","$0.00","9820015<br/>Waller<br/>  <br/><br/>This award provides travel support for approximately 20 U.S. <br/>participants to the 52nd Biennial Session of the International <br/>Statistical Institute, held in Helsinki, Finland, from <br/>August 10-18, 1999.  This premier statistical international <br/>conference jointly hosts the Bernoulli Society, the <br/>International Association for Official Statistics (IAOS), <br/>the International Association for Statistical Computing (IASC), <br/>the International Association of Survey Statisticians (IASS) <br/>and the International Association for Statistical Education (IASE).  <br/>The grant will provide partial support to defray transportation <br/>costs for individuals selected by a committee appointed by the <br/>ASA president, with emphasis on newer faculty (within 10 years <br/>of degree) and women and minorities.<br/>"
"9972601","Symposium on Case Studies in Bayesian Statistics","DMS","STATISTICS","08/15/1999","08/24/1999","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Javier Rojo","07/31/2000","$10,000.00","","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","9188, EGCH","$0.00","The symposium ``Case Studies in Bayesian Statistics 4'' will be held at Carnegie Mellon University  on Friday September 24 and Saturday September 25, 1999.  <br/><br/>The symposium will include three extended presentations of applications of Bayesian methods. In these applications the statistician has been an integral member of the research team. Two contributed poster sessions will also be held. The objectives of the symposium are to:<br/>(i)   identify and focus attention on specific implementation and theoretical problems that hinder applications of Bayesian methods, and to identify candidate solutions;<br/>(ii) provide a forum in which the interplay between statistical theory and practice will be explored in the context of concrete research projects; <br/>(iii) provide a small-meeting atmosphere within which junior investigators and graduate students can explore substantial Bayesian applications with experienced researchers; and<br/>(iv) produce a volume containing well-documented case studies <br/>and data sets suitable for use by researchers, practitioners, educators and students of  applied statistics  and other quantitative fields.<br/><br/>As increasingly much background information becomes available to scientists undertaking an investigation, it is important to utilize previous knowledge effectively in designing studies and analyzing data. Bayesian statistical methods are tailored to this purpose. There have been many recent advances in Bayesian statistical theory and computation, but scientific meetings rarely spend substantial time discussing applications. The purpose of this symposium is to concentrate attention solely on applications of Bayesian statistics. The goal is to elucidate the interplay between theory and practice and thereby identify successful methods and indicate important directions for future research."
"9971980","Extreme Values, Time Series and Prediction","DMS","STATISTICS","08/01/1999","09/27/1999","Richard Smith","NC","University of North Carolina at Chapel Hill","Standard Grant","John Stufken","07/31/2002","$65,000.00","","rls@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","0000, OTHR","$0.00","The research in this project focuses on a number of theoretical and applied topics concerned with prediction and extreme values in time series. Theoretical research includes: (1) spectral methods for fitting regression models where the residuals form a bivariate time series with unknown spectral density of possibly long-range dependent form; (2) extensions of the methodology to spatial-temporal series; (3) decision-theoretic comparisons of Bayesian and frequentist methods for prediction of extreme values, and (4) extensions of (3) to multivariate processes, including ideas both from hierarchical models and multivariate extreme value theory.  <br/><br/>The applications which have motivated these topics come from two distinct areas, namely (a) climatology, and (b) the evaluation of risk in insurance and finance. On (a), the P.I. has previously <br/>collaborated with two leading climatologists (T. Wigley and B. Santer) on an autocorrelation-based method of detecting climate signals in series of hemispheric temperature averages. The new research, which continues this collaboration, includes more extensive and rigorous techniques for separating signals from background noise in cases where the statistical structure of the background noise is unknown. Extensions will be developed to cases in which the patterns <br/>of temperature over the earth's surface are described, not just through northern and Southern Hemisphere averages, but through much more detailed summaries over spatial grid boxes.  The benefit of this research is improved methods for detecting anthropogenic influence on the earth's climate, which related to the Federal strategic interest in Environment and Global Change (EGCH).  The second topic (b) is concerned with extensions of the idea of ""value at risk"" which has become a widely studied topic in the fields of insurance and finance. Value at risk is a collection of tools for assessing the probability of very large financial losses, but most currently available systems do not use the most advanced statistical techniques available. The new research will assess alternative measures of risk using statistical methods derived from the mathematical theory of extreme value probabilities."
"9971649","Populations of Complex Objects, Visualization and Smoothing","DMS","STATISTICS","09/01/1999","04/16/2001","James Marron","NC","University of North Carolina at Chapel Hill","Continuing Grant","John Stufken","12/31/2002","$208,000.00","","marron@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","0000, OTHR","$0.00","DMS-9971649<br/><br/>The need for understanding the structure of populations of complex objects, including curves, images and surfaces, is becoming increasingly clear, especially as automatic data gathering capabilities are burgeoning in many fields.  Research into functional data analytic methods is proposed to address this need. An important part of this thrust will be in the direction of robust methods, where few of the standard methods are applicable because of the very high dimensionality coupled with small sample sizes.  New visualization methods are needed for functional data analysis, when the objects are images and surfaces.  Creative visualization ideas are proposed in a different direction in the proposed work on smoothing.  The central issue for real data applications of smoothing is which features in the smooth are really there? SiZer was one such methodology, and a variety of improvements are proposed.  New visual paradigms will be developed for the challenging and important cases of dependent data and two dimensional smoothing.  Work is also proposed on studying the mathematical statistical underpinnings of SiZer, and related methods.<br/><br/>The proposed work is motivated by problems in opthalmology, computer science, software engineering, and geology.  The work in ophthalmology and computer science is on finding statistical and visual methods for gaining insight into the structure of populations of complex objects. Possible types of objects are very diverse, with work under way on images representing corneal curvature and on shape representations of corpus collosa.  The methods developed are expected to be useful in many other fields, but an immediate benefit will be improved diagnosis of kerataconus and schizophrenia.  The work in software engineering and geology is a combination of visualization and smoothing methods to find, and verify, unexpected structure in data.  For example one newly developed method, both establishes a link between perfective maintenance and decreased fault rate in software engineering, and also provides the first statistically significant connection between glaciation and volcanism.  Another newly developed method shows how legacy software systems age, and also gives statistical proof of periods of extinction of mollusks, due to climatic changes.<br/>"
"9971964","Estimation from Dynamical Systems and Individual Sequences","DMS","STATISTICS","09/01/1999","09/07/1999","Andrew Nobel","NC","University of North Carolina at Chapel Hill","Standard Grant","John Stufken","08/31/2003","$75,000.00","","nobel@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","0000, OTHR","$0.00","DMS 9971964 <br/>Estimation from Dynamical Systems and Individual Sequences<br/>Andrew B. Nobel, University of North Carolina, Chapel Hill<br/><br/>ABSTRACT:<br/>The advent of modern computing and the recent interest in chaos have focussed increasing attention on deterministic systems that exhibit random behavior.  Statistical analysis of such systems is often complicated by the fact that measurements of their behavior can exhibit very long-range dependence.  The Principal Investigator is studying non-parametric estimation from ergodic processes and individual sequences, with particular emphasis on processes and sequences that arise from measurement of a dynamical system.  He is developing and proving the consistency of schemes for the following problems: (i) estimating the map generating a given discrete time dynamical system; <br/>(ii) estimating the stationary density and Lyapunov exponents of a dynamical system; and (iii) density and regression estimation from individual sequences.  <br/><br/>In addition, the P.I. is seeking to characterize deterministic sequences and to estimate their induced transformations.<br/><br/>The advent of modern computing and recent scientific interest in chaos have focussed increasing attention on physical systems that are governed by deterministic laws, but exhibit erratic or unpredictable behavior that is characteristic of random phenomena.  Dynamical systems of this sort have found application in such diverse areas as medical diagnostics and weather prediction.<br/>The Principal Investigator (P.I.) is developing statistical methods that can be used estimate the underlying properties of a dynamical system from observations of the system as it evolves in time.  This is important if one wishes to predict or control the future behavior of the system.  The P.I. is developing methods to estimate the rule that dictates the evolution of the system over a single unit of time.  He is also developing methods to assess the sensitivity of the system to its initial conditions: if the system is started off in two very similar states, how different will it look later?  The P.I. is seeking methods that will be effective for a wide variety of systems, including those whose measurements exhibit dependence over very long time scales.  <br/>"
"9971738","Statistics and Finance","DMS","STATISTICS","08/15/1999","05/16/2001","Per Mykland","IL","University of Chicago","Continuing Grant","Marianthi Markatou","07/31/2003","$270,000.00","","mykland@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00","9971738<br/><br/>It is proposed to continue the research initiated by the P.I. on how to use confidence and prediction sets for stochastic differential equations to set hedging strategies for derivative securities.  The study will investigate both the statistical methods used to set such sets, and how to trade for a given set.  The plan is to compare sets based purely on nonparametric prediction with those based on confidence statements, and also to look at hybrid methods, involving neighborhoods of parametric models.  The latter will also provide guidance on how to conduct model selection in such circumstances.  Several inference methods will be investigated, ranging from excursion based methods to profile likelihood.  In terms of the interface between sets and trading algorithms, it is proposed to extend the present theory to non-European options.  It is also the plan to investigate the multiple comparison issues that arise if one wishes to revise prediction intervals in the course of a trading strategy.  Finally, it is the intention to look into alternative models to stochastic differential equations, models that can be used for hedging and that are closer to the data than continuous processes.  Derivative securities -- options, futures, and similar instruments -- have emerged as a major feature of the contemporary economic environment.  They are a versatile and efficient mechanism for trading risk, thus lowering its cost.  There is, however, a dark side to derivatives, in that it can be hard to assess the real risks involved.  This is not only the case for `naive' market participants, as evidenced by a number of incidents involving major financial institutions.  These difficulties are, to an important extent, caused by the absence of a connection between statistical methods and the evaluation of financial risk.  There is a substantial literature on the statistical analysis of financial data.  Apart from short-term considerations, however, there is little knowledge on how to use statistical results based on historical data to quantify the risks associated with derivatives.  The aim of this project is to change this situation by providing ways of tying up statistical results and the trading strategies on which the values of derivative securities depend.  This will provide ways of setting contingency reserves and fallback liquidation strategies without interfering in the day-to-day operation of the financial institutions trading in options.<br/>"
"9971127","Statistical Inference for Spatial Data","DMS","STATISTICS","07/01/1999","05/04/2001","Michael Stein","IL","University of Chicago","Continuing Grant","John Stufken","06/30/2003","$252,000.00","","stein@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00","9971127<br/><br/>This proposal focuses on two major areas in spatial statistics:  inference for point processes and prediction and inference for Gaussian random fields.  The work in point processes is largely motivated by questions arising in the analysis of an important cosmological catalog of locations along the lines-of-sight between quasars and the Earth of what are known as heavy-element absorbers.  This catalog provides a means for assessing the clustering of matter in the universe over very large spatial scales and for describing changes in this clustering as the universe has evolved.  The absorber catalog can be most simply modeled as a collection of realizations of a point process observed on a large number of intervals.  The fact that the process is observed over many regions rather than one large contiguous region produces some interesting opportunities and challenges for estimating properties of the point process and obtaining valid standard errors for the estimates.  For example, one can use resampling methods with the regions as sampling units to obtain confidence statements, although the problem is rather difficult if, as in the absorber catalog, the regions are of different sizes.  The proposed work on Gaussian random fields addresses several problems related to prediction when the covariance structure of the random field is partially unknown.  Under the natural asymptotic regime of an increasing number of observations in a fixed and bounded domain, only in some very simple cases is it presently possible to prove that predictions based on an estimated covariance structure are asymptotically optimal.  Recent work by Putter and Young provide a promising approach for proving such results.  This proposal also addresses the design of observation networks for prediction when the covariance structure is unknown.  For prediction, the local behavior of the random field is critical and practical experience has demonstrated that evenly spaced observations are poor designs when trying to infer this local behavior.  This proposal aims to provide theoretical support to this empirical finding.  For example, preliminary work shows that there is tremendous potential for improving the estimation of the fractal dimension of a Gaussian process by taking observations on two grids of very different spacing rather than on a single evenly spaced grid.<br/><br/>Spatial statistics is a rapidly growing area of inquiry with broad applicability to the natural and social sciences.  Despite its present widespread usage, the theoretical properties of many commonly applied procedures in spatial statistics are poorly understood.  The first major topic of this proposal is the analysis of data made up of locations in space.  This work is motivated by a cosmological data set that provides important information about the large-scale structure of the universe.  Estimating this large-scale structure is a critical component in resolving the fundamental cosmological problem of how the universe evolved from its nearly uniform early state to its present highly clumped state.  In addition to the direct application the proposed work has to cosmology, the problems addressed also occur naturally in microscopy.  The second major topic in this proposal is the prediction of quantities such as temperature, pollution concentrations and soil characteristics whose levels vary randomly across space.  Two specific problems include studying the properties of such predictions when there is uncertainty about how the values of this quantity fluctuate in space and selecting the locations of observations to yield accurate predictions.  Spatial prediction is widely used in the atmospheric sciences, pollution monitoring, hydrology and mining.<br/><br/>"
"9817659","Statistics: Modeling of Processes in Science and Industry.  A Conference Marking the 50th Anniversary of the Department of Statistics at Virginia Tech, August 13-14, 1999","DMS","STATISTICS","05/01/1999","04/07/1999","Klaus Hinkelmann","VA","Virginia Polytechnic Institute and State University","Standard Grant","Joseph M. Rosenblatt","01/31/2000","$8,000.00","","","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1269","0000, 9263, OTHR","$0.00","Abstract:  This award provides partial travel support and local expenses for 10 new researchers and 5 of the invited speakers at the conference on ""Statistics:  Modeling of Processes in Science and Industry"" being held in conjunction with the 50th anniversary of the Department of Statistics at Virginia Polytechnic Institute and State University (Virginia Tech).  The theme reflects the research accomplishments and philosophy of the department, which encourages interaction between theory and applications.  The main objective is the search for general principles of statistical modeling as they evolve from specific applications in different subject matter fields.  The conference will provide a forum for discussion of these ideas and establish communication between well established and new researchers.  The timing on the heel of the annual joint statistical meetings in Baltimore should facilitate the travel logistics for potential attendees.  Sessions planned will cover:  Process and Quality Control, Experimental Design, Medical and Biopharmaceutical Statistics, Linear Mixed Models, Bayesian Statistics, Nonparametric Estimation, and Survey Sampling.<br/><br/><br/>"
"9971903","Analysis of High Frequency Time Series Data and Structural Breaks","DMS","STATISTICS","08/01/1999","05/22/2002","Ruey Tsay","IL","University of Chicago","Continuing Grant","Gabor Szekely","07/31/2003","$188,647.00","","ruey.tsay@gsb.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00","9971903<br/><br/>Two research projects are being proposed.  Project (I) is concerned with analysis of high frequency time series data. Its objective is to develop a multivariate framework for analyzing effectively and efficiently high-frequency data observed in many scientific areas such as environmental study, finance and telecommunication.  It focuses on modeling time durations between observations and the associated quantities of interest by using a flexible dynamic model to govern the evolution of conditional distributions of variables involved.  The model also takes into account the effects of exogenous variables and cross dependence between the variables.  The proposed model builds on ideas of generalized linear models by relating conditional means and variances of the variables to the contemporaneous time duration and available information.  The model decomposes discrete-valued variables into ``direction'' and ``size'' of changes and uses a stochastic scale parameter evolving over time to describe the dynamic of time duration.  Consequently, the proposed model combines two sets of dynamic equations to form a multivariate dynamic system for high frequency time series data.  Properties of the proposed multivariate models are investigated, and Markov Chain Monte Carlo methods are used in estimation.  Project (II) is concerned with detecting and modeling structural breaks in multivariate time series analysis.  It generalizes univariate results to the multivariate case in several directions such as to include processes with long-range dependence, to provide a joint detection for several types of structural breaks, and to investigate the relationships between detected breaks among components. <br/><br/>Advance in technology and data collection enables researchers to observe high-frequency data such as traffic patterns in internet, minute-by-minute measurements of ozone concentration and meteorological data via a satellite, and tick-by-tick transactions data in financial markets.  Key features of these data include extremely large sample sizes and in some cases irregularly spaced time intervals between events.  Analysis of such data is important for many reasons, including (a) it can provide an early warning signal, via predicting time interval to the next event and the associated measurements, of potential big surges in internet traffic or air pollution index in a metropolitan area, (b) it can shed light on the micro-structure of the system under study that is critical to understand the dynamic behavior of the data, and (c) it gives a framework on which scientists or data analysts can assess potential risk under different scenarios in a timely manner such as estimating the risk of a position held by a financial institution over a given time horizon. The goal of the proposal is (a) to provide an efficient and effective method for analyzing such  high-frequency data, (b) to bridge the gap between theory and applications of high-frequency data analysis, and (c) to develop methods for detecting structural breaks in a multivariate system that can occur in practice.  The proposal intends to use recent developments in statistical estimation, large-scale data processing, and multivariate time series analysis to achieve its goal.  Results obtained by the proposed research will have applications in many scientific areas beyond statistics such as in environment and global change, in telecommunication, in e-commerce, and in finance. Students will participate in the proposed research. <br/>"
"9978321","Probabilistic Modeling and Computational Methods in Environmental Statistics","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","09/15/1999","09/07/1999","Richard Levine","CA","University of California-Davis","Standard Grant","John Stufken","08/31/2002","$99,600.00","Naoki Saito, David Layton","rlevine@mail.sdsu.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1253, 1269","9197, EGCH","$0.00","9978321<br/><br/>This project considers the development and application of statistical methods in two scientific areas:  1) image analysis of geophysical data and 2) modeling of stated preference ratings data.  The recurring theme is to study the probabilistic model underlying each data structure.  The probabilistic modeling of images is based on the statistical structure of features in order to compress, reconstruct/synthesize, and statistically analyze the images.  The models are derived from the statistical dependencies between the basis coordinates of transformed images.  In particular, the proposed task is to develop a computationally efficient independent component analysis in an attempt to transform images into a set of independent components modeled by Gaussian processes.  If the dependent coordinate systems follow an ICA decomposition, the statistical dependency between basis coordinates will be modeled through Markovian processes.  As part of the study, validation measures are constructed to evaluate the image syntheses and reconstructions based on the probability models.  These measures are developed from the Edgeworth expansion of the Kullback-Leibler information statistic and differential entropy.  The study of stated preference ratings data considers probabilistic models for inferring the probabilistic structure of censored rankings underlying nominal ratings data.  The hierarchical Bayesian models developed will require computationally efficient Markov chain Monte Carlo routines incorporating Gibbs sampling and Metropolis-Hastings steps for fitting the models and drawing appropriate inferences.<br/><br/>The project aims at developing computationally efficient statistical tools for probabilistic modeling of geophysical images and stated preference ratings data.  The image analysis methodologies are motivated by two specific geophysical problems to which the methods developed will be applied:  1) meteorological forecast verification and 2) geological classification.  The goal for forecast verification is to develop automated, computationally efficient routines to compare meteorological forecasts and observations at different geographical scales towards an evaluation of climatological forecast models.  The goal for geological classification is to develop automated, computationally efficient routines to classify sediments and rock formations based on computer learning and extraction of features from training data sets.  Each of these two<br/>Applications will incorporate methods developed by the project in feature extraction, compression, reconstruction/synthesis, and statistical modeling.  Methodologies for handling stated preference ratings data are motivated by two problems in program valuation to which the methods developed will be applied: 1) rating programs for mitigating impacts of global climate change and 2) rating programs for improving fish populations in Washington State.  In each application, the goal is the valuation of environmental programs through the consideration of many underlying attributes such as cost, species impact, and environmental impact, to name a few.  The statistical analysis will incorporate the probabilistic and econometric models and inferential tools developed by the project to study program ratings and preferences elicited from participants in the two studies.  This project is jointly supported by the Statistics Program in the Division of Mathematical Sciences and the Office of Multidisciplinary Activities (OMA) in MPS.<br/><br/><br/>"
"9820870","Conference for Applications of Heavy Tailed Distributions in Economics, Engineering and Statistics, June 3-5, 1999, Washington, DC","DMS","PROBABILITY, STATISTICS","05/01/1999","04/07/1999","John Nolan","DC","American University","Standard Grant","Joseph M. Rosenblatt","10/31/1999","$4,000.00","Ananthram Swami","jpnolan@american.edu","4400 MASSACHUSETTS AVE, NW","WASHINGTON","DC","200168002","2028853440","MPS","1263, 1269","0000, OTHR","$0.00","Abstract:  This award provides partial travel support and local <br/>expenses for the 20 invited speakers at the conference on <br/>""Application of Heavy Tailed Distributions in Economics, Engineering <br/>and Statistics"" on June 3-5, 1999 in the Ward Circle Building at <br/>the American University in Washington, D.C. Although heavy tailed <br/>distributions are increasingly being used to model phenomena in <br/>a wide range of applications in economics, finance, signal <br/>processing, telecommunication network analysis, time series, <br/>physical and biological provblems, progress in this area has been <br/>limited by the lack of efficient and reliable methods of modeling, <br/>analyzing and predicting for such models.  In the past decade, <br/>there has been significant progress in both the theory and methods <br/>of working with non-Gaussian models.  This conference will bring <br/>together many of the top theoreticians and practitioners,<br/>national and interenational, working in the field of heavy-tailed, <br/>non-Gaussian models and those with experience with massive <br/>data sets.  The invited list of speakers includes four women.  <br/>Support for graduate students and young investigators is provided <br/>with reduced registration fees.  <br/>"
"9971586","Algorithms, Approximations, and Valid Statistical Inference","DMS","STATISTICS","08/01/1999","08/03/2000","George Casella","NY","Cornell Univ - State: AWDS MADE PRIOR MAY 2010","Continuing Grant","William B. Smith","07/31/2001","$206,139.00","Martin Wells","casella@stat.ufl.edu","373 Pine Tree Road","Ithica","NY","148502488","6072555014","MPS","1269","0000, OTHR","$0.00","9971586<br/><br/>New solutions to difficult statistical problems have been found through the use of algorithms and accurate approximations.  Sometimes these solutions can be extremely difficult to implement and, moreover, the statistical properties of the solution, and hence the resulting statistical inference, may be difficult to describe and, in the worst cases, even invalid.  The general theme of the proposed research is to develop computing algorithms, accurate approximations, and other numerical techniques for a variety of problems, and do it in such a way that a valid statistical inference results.  Some specific projects are the development of algorithms for hierarchical and missing data models, including extensions of the EM algorithm to estimating equations; using accurate approximations and a synthesis of Bayesian and frequentist approaches to provide valid inferences in nuisance parameter problems; and computation and inference in discrete data problems, with particular attention to numerical alternatives to saddlepoint approximations and novel uses of Monte Carlo methods.<br/><br/>Much current research in statistics is influenced by the availability of exceptionally fast computing, which has led to a reexamination of the approaches to many problems.  These new approaches have provided some solutions to previously unsolved problems; however, their implementation is often not straightforward so the solutions will not be available to more than a few expert users.  Such a situation arose in an epidemiological study to estimate the effect of clinical mastitis, where implementation of the currently available algorithm was not clear.  The type of solution proposed here is straightforward to implement in this case, as well as in other applications such as estimating the location of cancer clusters relative to toxic waste sites.  This new solution will also result in statistically valid conclusions, a property not always enjoyed by computationally intensive procedures.  In addition to these algorithms, research in new statistical simulation methods is proposed which has application in genetics and biotechnology studies (the linkage of locations on a chromosome to particular diseases). <br/><br/>"
"9815344","A Statistics Program at the National Center for Atmospheric Research","DMS","STATISTICS","07/01/1999","08/21/2003","Richard Katz","CO","University Corporation For Atmospheric Res","Cooperative Agreement","K Crank","09/30/2005","$3,000,000.00","","rwk@ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","MPS","1269","0000, 1303, EGCH, OTHR","$0.00","9815344<br/>Katz<br/> The Geophysical Statistics Program (GSP) at the National Center for <br/>Atmospheric Research (NCAR) makes contributions to atmospheric and oceanic <br/>science through the development and application of new statistical methodology. <br/>The project supports a critical mass of statistics visitors at NCAR, trains <br/>young researchers, and involves close collaboration with resident NCAR <br/>geophysical scientists. The continuation of this project expands statistical <br/>research to the entire spectrum of scientific activity at NCAR and in so doing, <br/>establishes a permanent statistics group at this research center. The richness <br/>of geophysical measurements pose substantive research problems in statistical <br/>science and continue to challenge the boundaries of statistics. Some broad areas <br/>of research include modeling and inference for spatial and spatial/temporal <br/>processes, statistical design, inverse problems, statistical computing, and <br/>functional data analysis. These statistical topics have many direct applications <br/>to the geophysical sciences, including the detection of climate change, <br/>improvement of numerical models, prediction of severe weather, and design of <br/>field experiments.<br/> The primary purpose of the Geophysical Statistics Program (GSP) at the <br/>National Center for Atmospheric Research (NCAR) is to promote collaboration <br/>between the statistical and geophysical sciences. NCAR, base-funded by the <br/>National Science Foundation, is a focal point for basic research on the <br/>atmosphere and has strong links to universities throughout North America. The <br/>complexity of problems in the atmospheric and oceanic sciences (e.g., large <br/>numbers of variables and massive data sets) indicates that statistics could make <br/>important contributions. There is also a broader benefit to statistics and <br/>mathematical sciences. New statistical methods, motivated by geophysical <br/>problems, can be transferred to other areas which consider spatiotemporal data <br/>and the output of numerical models.  Besides these benefits to the scientific <br/>fields involved, this collaborative research would result in potential benefits <br/>to society. For instance, it could lead to improved weather and climate <br/>forecasts, as well as to more reliable quantification of uncertainty in global <br/>climate change studies. Such information should be of substantial value to <br/>decision makers and policy analysts.<br/>"
"9971770","Statistical and Computational Methods in Genetic Analysis","DMS","STATISTICS","08/01/1999","06/13/2002","Shili Lin","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","John Stufken","10/31/2002","$50,000.00","","shili@stat.osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","9184, BIOT","$0.00","Large and complex genetic data sets present a great deal of challenges to <br/>standard data analysis techniques. In many cases, standard methods are <br/>infeasible or even impossible for analyzing such data. This research<br/>is to develop statistical and computational methods relevant to the analysis<br/>of complex human pedigree data. The first main focus is to solve problems <br/>that involve large complex pedigrees, multiple polymorphic markers, incomplete<br/>genotypes, and complex disease models. The second main focus is to study <br/>further the Chi-square (CHS) recombination models, to develop new techniques <br/>to incorporate CHS into methods of gene mapping to achieve greater efficiency<br/>of data, and to apply this methodology to study genetic interference in the <br/>human genome. Most of the proposed research in this project is to be carried <br/>out using the Markov chain Monte Carlo (MCMC) methodology. Exploration of MCMC <br/>methods in human pedigree analysis thus far shows that this methodology is <br/>highly suitable for estimating probabilities and likelihoods. However, because  <br/>of special features of models and methods appropriate for modern human <br/>genetics, special modifications to the standard MCMC approach are required<br/>for this technique to be effective in a variety of genetic mapping problems.<br/>This project continues the work of making these modifications and exploring <br/>new applications.<br/><br/>The last decade has seen rapid advancements in the field of human genetics. <br/>Large and complex data sets are accumulating at an incredible rate. Some <br/>disease genes (most of them are for single-gene simple genetic disorders) have  <br/>been identified and mapped. This includes the genes responsible for cystic <br/>fibrosis, Huntington's disease and some breast cancers. This has tremendous <br/>implication in genetic counseling, genetic testing and screening, drug <br/>discovery, and genetic therapy. Identification of genetic factors for complex <br/>diseases is a far more difficult task. Complex diseases may be genetically<br/>heterogeneous caused by different susceptibility genes, or may be caused by<br/>a combination of genes with possible environmental effects. Many common <br/>diseases have complex etiology, and are believed to be at least partially due <br/>to genetic predisposition. Common diseases such as diabetes, alcohol <br/>dependence, and some forms of cancer are examples of complex disorders. <br/>Methods developed in this research can handle complex genetic models and make <br/>use of available genetic data fully, thereby increasing the power to map <br/>genes for complex diseases."
"9972015","Applied Probability and Time Series Modelling","DMS","PROBABILITY, STATISTICS","07/01/1999","06/08/2001","Peter Brockwell","CO","Colorado State University","Continuing Grant","K Crank","06/30/2003","$216,000.00","Richard Davis","pjb2141@columbia.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1263, 1269","0000, OTHR","$0.00","9972015<br/><br/>This research deals with problems in time series analysis related to the theory and application of non-linear models in both discrete and continuous time and to models for integer-valued data.  It is concerned with the development of such models, the study of their properties and the investigation of systematic techniques for fitting them to data.  Also considered are the large-sample properties of the estimated model parameters and the performance of model-based forecasts.  Many of the standard tools of linear time series analysis (e.g. the sample autocovariance function) have substantially different properties when the underlying process is nonlinear and this must be taken into account when they are used in conjunction with nonlinear models.  Nonlinear continuous time ARMA models have been found useful in the modeling of financial data.  Efficient estimation for these processes and for non-Gaussian and multivariate extensions of these are to be investigated to allow for the observed heavy-tailed behavior of financial time series and to permit the study of the dependence between related financial time series.  An alternative promising approach to the analysis of financial time series is to use all-pass linear filters driven by non-Gaussian noise as a possible alternative to the use of nonlinear models.  Integer-valued time series are of wide occurrence, for example the weekly numbers of accidents or diagnosed cases of some disease.  As for the models considered above, the objective is to develop a systematic approach to the modelling and forecasting of such data.<br/><br/>In the last ten years there has been a steadily increasing realization that non-linear time series models provide much better representations of many empirically observed time series than the classical linear models.  Many of the properties of non-linear models are however not yet understood and there is a need for the development of new models accompanied by efficient model-fitting procedures.  At the same time there has been a surge of interest in continuous-time time series models, partly as a result of the very successful application of stochastic differential equation models to problems in finance, exemplified by the derivation of the Black-Scholes option-pricing formula and its generalizations.  This proposal is concerned with the development of non-linear models in both discrete and continuous time with particular emphasis on the application of these models to the representation and forecasting of financial data.  Another class of time series which arises frequently in applications and for which systematic analysis is relatively undeveloped are those in which the observations are integer-valued.  For these series also a systematic approach to model-building and forecasting is proposed.<br/>"
"9970785","Saddlepoint Methods in Statistics","DMS","STATISTICS","08/15/1999","05/30/2001","Ronald Butler","CO","Colorado State University","Continuing Grant","Marianthi Markatou","07/31/2003","$120,000.00","","rbutler@smu.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1269","0000, OTHR","$0.00","9970785<br/><br/>The first project has three parts. (I) Laplace-type approximations are developed for many of the matrix argument special functions including the important Bessel and hypergeometric functions.  These functions arise in MANOVA settings and determine the noncentral distributions for the majority of test statistics as well as the sample eigenvalues.  Often it is the moment generating function (mgf) of the noncentral distribution that is specified in terms of the special function; in this case the mgf can be approximated and then inverted using a saddlepoint approximation. (II) Saddlepoint approximations are used in computing reliabilities and failure rates of complicated stochastic feedback systems.  System applications include various passage times in random walks, queues, multistate survival models, and some stochastic epidemic models.  The methods allow for very accurate determination of the transient behavior of the systems including many important performance characteristics useful in system design. (III) Saddlepoint methods are suggested for the stable laws to approximate all of these distributions and densities.  This development should make it easy to implement Bayesian computation using stable error laws as needed in financial mathematics.<br/><br/>The second project consists of three parts. (I) Very accurate approximations are proposed for some classes of special functions.  These special functions arise in all areas of the physical sciences and were created by mathematicians because they repeatedly arise as the solutions to important scientific problems and also because they are extremely difficult to compute.  In the field of statistics, these functions determine the power and performance of many commonly used statistical tests in multivariate analysis.  Computation of these functions requires immense amounts of computing time even in our modern computing environment.  The proposed approximations are simple, explicit, highly accurate, and should be useful in all the physical and engineering sciences. (II) General methods of approximation are given for the performance characteristics of complicated stochastic systems including various queuing systems, survival models, and stochastic epidemic models.  Such stochastic systems and networks underlie all areas of science and include models for computer system networks, ecosystems, production management methods in manufacturing, and reliability testing.  This second portion of the project develops methods for assessing these performance characteristics which, in turn, allow for the practical consideration of system design. (III) Some methods for statistical inference in financial mathematics are proposed. In this area, certain probability distributions, referred to as the stable distributions, are known to be very useful in modeling financial returns.  Unfortunately these distributions are still extremely difficult to compute and existing routines for their computation are not very accurate.  This proposal suggests some very simple and highly accurate approximations that should make such analysis routine and easy.<br/><br/>"
"9970266","Superefficient Fits to Linear Models","DMS","STATISTICS","08/15/1999","05/04/2001","Rudolph Beran","CA","University of California-Berkeley","Continuing grant","Marianthi Markatou","09/30/2002","$417,550.00","","beran@wald.ucdavis.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00"," Recent theory for shrinkage estimators, techniques from signal-processing, and effective algorithms for computing orthonormal bases now make it possible to exploit the superefficiency loophole in classical information bounds for estimation. In linear regression, if the first few vectors in the regression basis closely approximate the unknown mean vector, then the risk of an estimator that shrinks to zero those regression coefficients associated with the unimportant basis vectors can be much smaller than the risk of the least squares estimator. Such shrinkage estimators, which are particular symmetric linear smoothers, realize the benefits of C. Stein's and M. S. Pinsker's pioneering ideas on estimation of high- or infinite-dimensional parameters. Specific goals of the research are: (a) to construct and interpret confidence sets centered at a superefficient fit; (b) to handle, through multiple shrinkage, cases where the chosen basis is sparse but not well-ordered; (c) to develop within- and between-observation shrinkage techniques to handle the multivariate linear model; (d) to draw on relations with signal-processing techniques that use the discrete cosine basis or wavelet bases.<br/><br/> Regression models fitted by the method of least squares are widely used in scientific research and other disciplines to establish quantitative relationships within sets of data. Studies related to the program on Environment and Global Change and to the program on Manufacturing are examples. The broad goal of the proposed research is to improve the reliability of these fitted relationships by replacing least squares with better adaptive linear smoothers. Recent statistical theory supports the general feasibility of this project. How to realize what is possible in theory is the essence of the work. The author's REACT method, described with references in the proposal, is a practical technique for regression with one response variable that demonstrates real-world<br/>improvements over least squares fits. REACT competes well with current nonparametric regression methods while offering certain advantages, such as built-in diagnostics that indicate the quality of the fit. The proposed research will extend REACT methods to finding relationships among sets of variables and will develop practical methods for assessing the uncertainty of the estimated relationships.  Least squares regression, a standard function in modern statistical packages and spreadsheets, is widely used in data-analysis. This circumstance provides strong motivation for improving on least squares."
"9971301","Topics in Nonparametric Analysis and Model Building","DMS","STATISTICS","09/01/1999","08/14/2002","Kjell Doksum","CA","University of California-Berkeley","Continuing grant"," Shulamith T. Gross","08/31/2004","$164,000.00","","doksum@stat.wisc.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, 9179, OTHR","$0.00","9971301<br/> <br/>This project is the continuation of our research on the question of how intuitive and concise linear model concepts and techniques can be extended to nonparametric settings and on the development of nonparametric techniques for model diagnostics, dimensionality reduction, and assessing adequacy of particular classes of models.  Some of the procedures considered in this research are based on conditional versions of the familiar regression, covariance, and correlation coefficients, where the conditioning is on covariates restricted to neighborhoods.  The size of the neighborhood serves as a resolution scale, and dependence of the response on the covariates is measured and summarized at multiple scales.  Other procedures are curve or surface estimators and estimators of integral functionals of distributions.  Many of the estimators to be considered depend on smoothing parameters needed in estimation of curves and surfaces.  A large part of the research addresses the problem of developing reliable data-based methods for smoothing parameter selection. Properties of estimators are studied using both asymptotic methods and Monte Carlo simulations.<br/><br/>The advent of computer data bases of unprecedented size and complexity, and the dramatic increase in computer power, makes desirable and possible the development of more flexible models, concepts, and procedures.  These can be used to study relationships between variables and to construct models without the need of relying on rigid global assumptions.  Much of the recent work in statistics has addressed this need for more general and flexible methods.  This research further extends this work with a special focus on procedures that are counterparts to many commonly used linear model concepts and that expose important features in the data using intuitive and familiar ideas. The developed procedures are applied to financial, economic, insurance, medical, and other data. <br/><br/>"
"9971309","Point Processes and Time Series: Networks and Wavelets","DMS","STATISTICS","09/01/1999","05/09/2001","David Brillinger","CA","University of California-Berkeley","Continuing grant"," Shulamith T. Gross","08/31/2003","$246,000.00","","brill@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, 9184, BIOT, OTHR","$0.00","The proposed research concerns two basic topics: a) the study of networks with temporal processes at the nodes and b) variants of wavelet analysis involving temporal and spatial point processes. Applications are made to understanding the interconnections of the auditory system of the brain on the basis of recorded spike trains and to creating isoseismal maps of earthquakes in the aftermath of a destructive event. The network effort relates to graphical modeling while the wavelet work may be seen as a competitor to local weighting procedures, for handling nonstationarity.  The topics come together in the problem of assessing whether learning is taking place in a biological neuron network.<br/>In the case of the network study, principal applications are to contemporaneous spike trains obtained from biological neurons at locations, determined post mortem in cats and rats, having in mind the goal of inferring a latent wiring diagram.  In the case of the wavelet research, applications are made to both seismology and neurophysiology seizing on the ability of the wavelet approach to handle nonstationarity. Both the temporal and the spatial cases are considered.<br/><br/>The research proposed is motivated by problems arising in the fields of biology and seismology, for example, and the belief that they may be well-addressed by contemporary research work involving the concept of networks.  Work involving novel mathematical expressions, which allow some change in time or space to take place in a situation, also relates to the research.  Both theoretical development and empirical model fitting and verification will be involved.  There may be applications to the Biotechnology (BIOT) area of Federal strategic interest.  The value of working in fields as diverse as biology and seismology is that analytical techniques thought of for one may be abstracted and applied in the other."
"9971751","Asymptotic Equivalence in Nonparametric Function Problems-Theory and Applications","DMS","STATISTICS","08/01/1999","06/05/2003","Lawrence Brown","PA","University of Pennsylvania","Continuing grant","grace yang","07/31/2004","$347,284.00","","lbrown@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, OTHR","$0.00"," Recent results indicate the possibility of constructing a comprehensive asymptotic theory  of nonparametric function estimation. The major focus of the proposal is on continuing development of such a comprehensive asymptotic equivalence theory. In particular, the emphasis is on constructive, equivalence results. These allow researchers to develop statistical methodologies within the most tractable mathematical formulation and then to readily transfer those methodologies to the possibly less tractable formulation of practical interest. Additionally,  asymptotic equivalence as so far defined is a very stringent property which fails in some important practical settings, particularly those involving higher dimensional problems. One additional aspect of the proposal is to develop a weaker, but still constructive, partial equivalence<br/>theory which can productively be applied when full equivalence fails. Research involving the use of equivalence ideas to further develop currently popular spline and wavelet methodologies is also proposed. This should lead to new, improved adaptive smoothing spline methodologies and to increased understanding of the existing adaptive wavelet methodologies.<br/><br/> Contemporary advances in computational and statistical methodology have made it possible to model and analyze complex patterns arising in two and three dimensional data contexts. Examples abound in medical, geophysical, and astrophysical imaging and in the analysis of the data corresponding to such images. The statistical models accurately corresponding to such images are ""nonparametric"" in the sense that they do not possess the restrictive structure prevalent in statistical models developed in the pre-computer era. The current<br/>proposal is aimed at a broad, but constructive, understanding of such models. In more detail, there are currently several rather different ways to model and analyze such nonparametric data. The main focus of this proposal is on establishing results which constructively demonstrate the operational similarity of all these models. The constructive nature of the proposal should make it feasible to move from one model to the other. For example, this can make it possible to choose one form which is best for efficient computation and then to directly pass to another which is more efficient for general presentation or for further mathematical analysis."
"9971212","Resolution and Minimum Aberration for Nonregular Factorial Designs","DMS","STATISTICS","06/15/1999","06/17/1999","Boxin Tang","TN","University of Memphis","Standard Grant","John Stufken","05/31/2002","$97,417.00","","boxint@sfu.ca","Administration 315","Memphis","TN","381523370","9016783251","MPS","1269","9149, MANU","$0.00","9971212<br/><br/>A regular fractional factorial is uniquely determined by its defining relation and it has a simple aliasing structure in that any two effects are either orthogonal or fully aliased.  When we have little or no knowledge as for what effects are potentially important, it is appropriate to select designs having minimum aberration.  Minimum aberration designs enjoy some desirable model robust properties and therefore can be properly called model robust designs.  Regular fractional factorials are well studied and results are abundant in the literature.  The same cannot be said of nonregular fractional factorials.  Nonregular fractional factorials are given by Plackett-Burman designs,  Hadamard matrices, or more generally by orthogonal arrays. Both regular and nonregular designs permit orthogonal estimation of all the main effects.  When some interactions are potentially important, the two classes of designs have rather different behaviors.  This leads some researchers to study the projection properties of nonregular designs.  Despite this important contribution, there has not been a systematic method for assessing and comparing nonregular designs. The broad objective of this project is to extend the theory and methods in regular designs to nonregular designs.  This is facilitated by the introduction of J-characteristics, which generalizes the concept of defining relation.  The whole project is conducted by focusing on the following research topics: (i) motivate and introduce generalized resolution and minimum aberration criteria, (ii) investigate the hidden projection properties of generalized minimum aberration in terms of estimability and efficiency of designs, (iii) develop a theory of J-characteristics and establish their connections with the projection properties, and (iv) develop, implement, and test efficient computational algorithms for constructing generalized minimum aberration designs.<br/><br/>In many areas of investigations, such as those of Federal Strategic Interest, efficient data collection is one of the key steps for the eventual success of a research project.  Well planned experiments ensure relevant and informative data to be collected.  Factorial designs provide cost-effective experimental plans that allow a large number of variables to be studied simultaneously and efficiently, and are therefore widely used in industrial experiments for improving the quality of manufactured products and the productivity of manufacturing processes.  Factorial designs can be categorized into two classes: regular designs and nonregular designs.  Regular designs are well studied and results are abundant in the literature.  The same cannot be said of nonregular designs.  Two reasons for the scarcity of results on nonregular designs are the lack of general theory and methods, and the associated computational difficulties in data analysis.  The broad objective of this project is to develop general theory and methods for studying and constructing nonregular designs.  This is achieved by introducing an instrumental concept, called J-characteristics, that is capable of capturing the properties of a design when projected onto lower dimensions. Developing and testing a user-friendly computer package is part of the proposed research.  The theory and methods to be developed in the proposed research will shed new light on the study of factorial designs, lead to new economical designs for the experiments in the physical, chemical, and engineering sciences, and promote the application of factorial designs in the areas such as biotechnology and medical research that have huge potential to benefit further from the design methodology.<br/><br/>"
"9971848","Bayesian Inference Estimation in Nonparametric Regression and its Frequentist Properties","DMS","STATISTICS","08/15/1999","08/13/1999","Linda Zhao","PA","University of Pennsylvania","Standard Grant","John Stufken","07/31/2002","$77,770.00","","lzhao@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, OTHR","$0.00","Nonparametric regression analysis is a rapidly developing contemporary statistical methodology.  It avoids some of the restrictive assumptions in more classical regression approaches.  However, to date it has suffered by comparison from an almost total absence of the inferential tools (e.g. tests and confidence intervals) which help make classical regression analysis so useful.  Zhao (1998 preprint) proposed the first proper Bayesian estimator for nonparametric regression which has acceptable performance for every sample size.  The current proposal is to extend this construction in various ways and to study the inferential properties related to such Bayesian formulations.  Its various properties, including the acceptable performance of its Bayes estimator, suggest that Bayesian inference based on this prior distribution and its extensions may be feasible.  They may also present a satisfactory comprehensive set of inferential tools for nonparametric regression analysis and related formulations.<br/><br/>Regression analysis is a classical general statistical tool, which has been widely used in virtually every area of statistical applications.  <br/>Nonparametric regression analysis is a more recent methodology, which avoids some restrictive assumptions in the classical theory.  The general theory includes nonparametric formulations of regression, of density estimation, of signal processing and of spectral density estimation in time series analysis. The theory has already found significant applications in areas such as medical and environmental imaging, economic analysis, computer engineering, and geophysics.  Bayesian analysis is a very powerful general statistical methodology.  This research project proposes a possible way of using Bayesian techniques to solve nonparametric regression problems."
"9971579","Topics in Nonparametric Analysis and Model Building","DMS","STATISTICS","08/15/1999","05/30/2001","Alexander Samarov","MA","Massachusetts Institute of Technology","Continuing grant","John Stufken","07/31/2003","$90,000.00","","samarov@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1269","0000, OTHR","$0.00","9971579<br/><br/>This project is the continuation of our research on the question of how intuitive and concise linear model concepts and techniques can be extended to nonparametric settings and on the development of nonparametric techniques for model diagnostics, dimensionality reduction, and assessing adequacy of particular classes of models.  Some of the procedures considered in this research are based on conditional versions of the familiar regression, covariance, and correlation coefficients, where the conditioning is on covariates restricted to neighborhoods.  The size of the neighborhood serves as a resolution scale, and dependence of the response on the covariates is measured and summarized at multiple scales.  Other procedures are curve or surface estimators and estimators of integral functionals of distributions.  Many of the estimators to be considered depend on smoothing parameters needed in estimation of curves and surfaces.  A large part of the research addresses the problem of developing reliable data-based methods for smoothing parameter selection. Properties of estimators are studied using both asymptotic methods and Monte Carlo simulations.<br/><br/>Computer data bases of unprecedented size and complexity and the dramatic increase in computer power makes possible the development of more flexible models, concepts, and procedures, which can be used to study relationships between variables and to construct models without relying on rigid global assumptions.  Much of the recent work in statistics has addressed this need for more general and flexible methods.  This research further extends this work with a special focus on procedures that are counterparts of many commonly used linear model concepts and that expose important features in the data using intuitive and familiar ideas.  The developed procedures are applied to financial, economic, insurance, medical, and other data.<br/><br/><br/>"
"9971051","Optimal Curve Estimation: from Asymptotic to Small Sample Sizes","DMS","STATISTICS","09/01/1999","08/24/1999","Sam Efromovich","NM","University of New Mexico","Standard Grant","John Stufken","08/31/2002","$50,957.00","","efrom@utdallas.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","MPS","1269","0000, OTHR","$0.00","SAM EFROMOVICH <br/>Optimal curve estimation: from asymptotic to small sample sizes<br/>9971051<br/><br/>The primary focus of this research is to explore the bridges between the sharp asymptotic theory of nonparametric curve estimation and practically important cases of small sample sizes.  Main objectives are as follows: (i) For the case of classical direct statistical settings, like density estimation or heteroscedastic regression, create the theory and methods of finding minimal sample sizes (the degree of the problem) where asymptotically sharp data-driven estimates outperform any other estimates. (ii) For indirect statistical problems, such as nonparametric survival analysis, blurred images or regression<br/>with measurement errors in predictors, create the theory of equivalence between these settings and the corresponding direct ones with a particular emphasis on finding the relation between sample sizes needed for a similar quality of estimation. (iii) For nonparametric curve estimation problems involving time series with heavy tails and long range dependence, which frequently arise in the modern communication systems like the World Wide Web, develop optimal data-driven procedures of nonparametric estimation. <br/><br/>Statistical theory is primarily interested in finding most accurate procedures for large samples while in practically interesting cases sample sizes may be both large and small.  Moreover, the notion of a large or small sample depends on an underlying setting.  The research is devoted to developing universal procedures, which are simultaneously optimal for both large and small sample sizes.  The developed methods will be tested on the following specific practical problems which motivated this proposal: (1) Insulin secretion by the type II diabetes patients; (2) The geomagnetic polarity time-scale and the speed of seafloor spreading using marine magnetic observations, and how these factors affect environment and global change; (3) Drinking water quality including monitoring arsenic and other cancer-causing substances in New Mexico; (4) Learning machines for screening patients in emergency rooms; (5) Engineered systems for the analysis of transformed and noisy images. These applied problems will be done together with Sandia National Laboratories, the UNM Medical Center and Maui High Performance Computing Center.  <br/>"
"9971602","Nonparametric and Semiparametric Modelling for Data Analysis","DMS","STATISTICS","08/15/1999","04/03/2001","Hans-Georg Mueller","CA","University of California-Davis","Continuing grant","Marianthi Markatou","07/31/2002","$119,999.00","","hgmueller@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","0000, OTHR","$0.00","For many scientific problems, nonparametric and semiparametric statistical modelling can contribute to a better understanding and analysis. This includes the application of curve estimation techniques. The P.I. explores models which combine smooth (nonparametric), non-smooth (discontinuous) and parametric components. For high-dimensional data, such as curve data and longitudinal data, a sensible modelling approach includes components which achieve a dimension reduction. These are typically parametric parts of a model, and nonparametric smoothers are then applied in a second step to the dimension reduced data.  The P.I. develops models which are geared towards scientific problems which yield data in the form of samples of curves, high dimensional data or discontinuous data. The techniques used include generalized linear models, estimating equations, change-point analysis, random effects modelling and smoothing (in particular, kernel and local polynomial smoothers).<br/><br/>The proposed models with discontinuous parts are used to address the question whether the growth of children proceeds continuously or discontinuously. A second application is the modelling of DNA sequences here discontinuities in base frequencies enable DNA segmentation techniques. Further applications include edge detection in image analysis and the segmentation of continuously recorded financial data such as stock market indices. In addition, the P.I. constructs models for the efficient analysis of high dimensional data. An example is dose-response analysis in nutrition with the aim of finding optimal vitamin sources. The P.I. also proposes models for data where an entire function is recorded per experimental unit. Such data occur in many fields, notably the life sciences, and require innovative statistical approaches. A pertinent example are data on the individually recorded egg-laying behavior for a large cohort of medflies. A major question in aging research is the relation between reproduction and longevity. It is therefore of particular interest to apply the proposed models to explore this connection."
"9972525","Topics in Multivariate Survival Analysis and Counting Processes","DMS","STATISTICS","08/15/1999","05/04/2001","Dorota Dabrowska","CA","University of California-Los Angeles","Continuing grant","Gabor J. Szekely","07/31/2003","$120,000.00","","dorota@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","0000, OTHR","$0.00","Multivariate survival analysis is a rapidly growing area with a wide range of applications in medicine, genetic epidemiology, econometrics, astronomy and other fields. The purpose of this project is to develop estimation methods in some semiparametric models arising in analyses of multivariate survival data.<br/><br/>Two projects are discussed in this proposal. The first one deals with estimation in partial transformation models, specifically in partial Cox regression and partial accelerated failure  time model. These models are useful in goodness-of-fit testing and analyses of multivariate models involving time measurements recorded on two or more time scales. For estimation purposes we shall use local polynomial fitting.<br/><br/>The second project deals with multivariate frailty models. A common approach towards analysis of these models rests on nonparametric maximum likelihood estimation with the form of the estimates derived from the counting process definition of the frailty models. The latter often fails to apply to multivariate data since they can involve time measurements recorded on several time scales, or nonpredictable censoring and covariate processes.  In this project we consider an ad hoc estimation method designed for analysis of clustered familial data with multivariate failure time processes recorded on age scale, The estimation method allows to incorporate left truncation and accommodate calendar time effects arising in analyses of familial data.<br/><br/>The results of the first project can be applied to any dataset involving censored observations. Results of the second project will be applied towards analysis of datasets provided by the Australian Twin Registry and Demographic Data Base, Umea, Sweden."
"9978318","Fire Hazard Estimation Using Point Process Methods","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","09/01/1999","08/26/1999","Frederic Schoenberg","CA","University of California-Los Angeles","Standard Grant","John Stufken","08/31/2003","$186,701.00","","frederic@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1253, 1269","1386, 9196, EGCH","$0.00","The intent of the proposed research is to investigate the estimation of fire hazard in the Los Angeles basin using techniques from multivariate point process analysis.  The strategy is to model fire occurrence as a marked spatial-temporal point process whose conditional rate depends not only on the record of previous fires but on other covariates as well.  These covariates include local environmental factors such as temperature, altitude, humidity, precipitation, vegetation, and soil characteristics.  An important component of this research is the assessment of various different models via likelihood analysis (based on past data) and the evaluation of forecasting performance (employing data obtained after the models are fitted).  Using the model, which performs best, the investigators intend to construct quantitative predictions of local fire hazard accompanied by estimates of uncertainties in these predictions.  They also plan to integrate these predicted hazards to form detailed, regularly-updated, maps of the risk of fire within the Los Angeles basin and to make these hazard maps and corresponding numerical local hazard estimates available to fellow scientists and to the public by posting the results, with appropriate explanations, on the World Wide Web.  The investigators' methodology is of interest to scientists studying various other fields besides fire hazard in Los Angeles. Of statistical interest is the use of modern techniques in point process theory such as the application of multi-dimensional branching, Markovian, and multi-stage point process models and the use of the multi-dimensional random time change theorem in assessing such models.  In addition, the methods used in the analysis is applicable not only in obtaining estimates of fire hazard in other regions, but also in quantifying similar types of dangerous occurrences, including floods, storms, hurricanes, tornadoes, earthquakes, volcanoes, tsunamis, disease epidemics, etc.<br/><br/>Some of the most devastating natural disasters in the history of the United<br/>States have been caused by wildfires.  However, the threat of fire has been examined with insufficient scientific scrutiny and mathematical precision to date. The immediate practical goal of this proposed research is to obtain and disseminate detailed local estimates of the hazard from wildfires in Los Angeles County. The basic strategy is to exploit local trends in fire occurrence and relationships between the incidence of fires and other environmental factors.  The resulting fire hazard estimates, as well as the relationships discovered between fire incidence and other variables, are needed in urban and in the study of how best to mitigate the potentially disastrous effects of fires in and near urban areas.  Of particular concern is the effect of public policies such as aggressive fire suppression and prescribed burning on fire occurrence rate and overall fire hazard.<br/><br/>This project is jointly supported by the Statistics Program in the Division of Mathematical Sciences and the Office of Multidisciplinary Activities in MPS.<br/>"
"9971797","Multivariate Nonparametric Methodology Studies","DMS","STATISTICS","06/15/1999","05/18/2001","David Scott","TX","William Marsh Rice University","Continuing grant","John Stufken","08/31/2002","$255,000.00","Dennis Cox","scottdw@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","0000, OTHR","$0.00","9971797<br/><br/>This research project focuses on the development of nonparametric density and regression methodology in mid-range dimensions.  Closely related applications such as clustering, mixture estimation, and dimension reduction are examined with a new point of view, relating locally adaptive and spatial estimation and recent extensions of nonparametric criteria to parametric problems.  The new data-based parametric estimation algorithm, based upon integrated squared error, is investigated for its flexibility and robustness.  An investigation of the dual solutions to the bandwidth choice problem for locally adaptive curve estimates continues, with the surprising finding that one solution is asymptotically a large constant.  Three algorithms for finding interesting subspaces will be investigated.  One measures the number of modes; a second finds maximal bias subspaces; and a third is a new least-normal criterion.  Visualization work continues with the acquisition of an ImmersaDesk, which will allow improved implementations of algorithms such as the density grand tour.  A somewhat new methodology is called Variable Clustering Analysis, which assists in semiparametric density estimation, data analysis, and interpretable dimension reduction.  Also in the area of clustering, an algorithm for simplifying complex mixture models fitted by EM is developed, as is a new estimation and testing algorithms for the number of components.  The project continues innovative work on spatial modeling and combining many data surveys into useful data modeling and conditional estimation of factors and their covariates in collaboration with researchers in the Department of Agriculture.<br/><br/>Nonparametric methodology is widely used in one and two dimensions, but less so in higher dimensions.  This research focuses on the mid-range dimensions and provides a deeper understanding of the implications to data modeling of the curse of dimensionality and problems associated with massive data sets.  Particular emphasis is given to multivariate regression and density estimation problems, and closely related applications such as clustering, mixture estimation, and dimension reduction.  Visualization is especially important when dealing with medium-dimensional data and the growing body of massive data sets.  Of special interest is discovering and displaying data in visual clustering and visual discrimination applications.  Rice University has acquired an ImmersaDesk, which will allow the implementation of recently developed algorithms in a virtual reality environment.  At a recent National Research Council workshop, numerous scientists identified critical statistical needs in their work with massive data sets:  alternatives to principal components, specialized visualization tools for exploring massive data, better clustering algorithms, and techniques for handling nonstationary data.  Results from this research directly impact three of these four critical opportunities.  This program represents a comprehensive and long-term attack on a host of important data analytic problems in multivariate estimation.  Graduate training is significant component of this project.  The results will be of long-term theoretical interest and will provide short-term solutions to real-world problems.<br/><br/>"
"9972440","Higher-Order Methods for Statistical Inference","DMS","STATISTICS","07/01/1999","06/09/1999","Thomas DiCiccio","NY","Cornell Univ - State: AWDS MADE PRIOR MAY 2010","Continuing grant","Gabor J. Szekely","06/30/2003","$118,138.00","","tjd9@cornell.edu","373 Pine Tree Road","Ithica","NY","148502488","6072555014","MPS","1269","0000, OTHR","$0.00","DiCiccio<br/>DMS-9972440<br/><br/>ABSTRACT:<br/>The proposed research concerns methodology for accurate inference based<br/>on likelihoods. Inference about a scalar parameter of interest in the<br/>presence of nuisance parameters can be achieved by using the signed root<br/>of the likelihood ratio statistic, which has the standard normal<br/>distribution asymptotically. In situations where the number of nuisance<br/>parameters is large or the sample size is small, the standard normal<br/>approximation to the distribution of the signed root can be inaccurate.<br/>Considerable progress has been made recently in understanding the<br/>statistical properties of the signed root and in constructing analytical<br/>modifications to the signed root that improve the accuracy of the normal<br/>approximation. Unfortunately, the modifications can be challenging to<br/>implement routinely, because they require elusive constructs, such as<br/>ancillary statistics, or they involve complicated analytical<br/>calculations, such as taking expectations of higher-order derivatives of<br/>the log likelihood function. In contrast to the use of analytical<br/>modifications, the proposed research focuses on the development of<br/>simulation methods to refine distributional approximations. These<br/>parametric-bootstrap techniques are attractive for their conceptual<br/>simplicity and very broad applicability. However, for the simulation<br/>methods to be valid, they must offer the same desirable attributes that<br/>the analytical modifications possess, with regard to such key statistical<br/>concepts as ancillarity and large-deviation behavior. A crucial component<br/>of the proposed research is to assess the statistical qualities of the<br/>simulation methods, particularly compared to those of the analytical<br/>modifications. Other aspects of the proposed research involve extending<br/>the simulation methods to models where standard asymptotic behavior<br/>fails, such as models involving random effects, and to circumstances<br/>where genuine likelihoods are unavailable but pseudo-likelihoods can be<br/>devised."
"9902907","Interface '99 - Theme: Models, Predictions and Computing","DMS","STATISTICS","04/01/1999","04/02/1999","Kenneth Berk","VA","Interface Foundation of North America Inc","Standard Grant","Tomek Bartoszynski","03/31/2000","$9,750.00","","","P.O. Box 7460","Fairfax Station","VA","220397460","7039931691","MPS","1269","0000, OTHR","$0.00","9971023<br/>Kurtz<br/> The 26th Conference on Stochastic Processes and their Applications will be held in Beijing, China, from June 14-28, 1999. This conference provides a valuable opportunity for US researchers to interact with prominent researchers from other countries This award will provide travel support for US participants. At least half of the funds will be used to provide support for junior researchers."
"9877107","Dynamic Reliability Models for Systems of Interacting Components","DMS","STATISTICS, OFFICE OF MULTIDISCIPLINARY AC","08/15/1999","09/13/2000","William Padgett","SC","University of South Carolina at Columbia","Standard Grant","John Stufken","07/31/2003","$219,449.00","James Lynch","padgett@stat.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","MPS","1269, 1253","0000, OTHR","$0.00","9877107<br/><br/>Many systems are composed of, or can be viewed as, complex subsystems of simple components whose behavior within the system can be quite complicated due to component interactions and dependencies. These dependencies may occur, for example, because dynamic redistribution of stresses occurs among the remaining components when some components fail. The dynamic approach is to incorporate the mechanism of failure into the model to account for such interactions and dependencies, as well as to use micro-level (component) relationships to determine the macro-level (system) failure. There are several important, but difficult, issues regarding reliability, or failure, models for systems in such dynamic environments. The main objectives of this research are to study the following five issues: (1) appropriate modeling of component and system reliability based on the physics of failure and the assumed environment, (2) incorporating component reliability information into the determination of system reliability in a dynamic setting, (3) identifying critical configurations of components that cause failure, (4) determining approximate reliability models for complex systems that are tractable for statistical inference and data analytic purposes, and (5) incorporating multivariate failure mechanisms into modeling system reliability. While (2)-(5) have been studied extensively for coherent systems of independent components, models incorporating dependencies among components would yield more realistic failure analysis of complex systems. Such models would have direct application to many problems of current importance including, for example, failure of modern complex materials and other structures and of networks of large numbers of components.<br/><br/>The proposed investigations of the mechanisms for failure of complex systems are important, for example, in modeling and understanding the relationship between the micro-level (component) and macro-level (system) size scales of failure. The statistical methodology and probability models to be developed will undoubtedly contribute to a better understanding of the random nature of failure mechanisms at both the component level and system level, and the associated statistical Inferences for failure of general systems in various applications. Particular applications are in prediction of the reliability of structures made from composite materials (such as carbon composites) or ""systems"" such as large computer networks. Also, the new, nonstandard statistical models developed in this study based on cumulative damage concepts are not well understood with respect to sampling and inference from observed failure data. Hence, an important goal of the project is to investigate these aspects of the models. Tactical and competitive advantages can be gained since the theoretical models developed here should lead to better system reliability prediction and its implication to system design and the system maintenance and inspection policies."

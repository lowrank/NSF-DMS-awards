"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"0405330","Monte-Carlo multi-step ahead forecasting for nonlinear time series","DMS","STATISTICS","07/15/2004","04/29/2004","Lijian Yang","MI","Michigan State University","Standard Grant","Grace Yang","06/30/2007","$192,053.00","","yang@stt.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","0000, OTHR","$0.00","ABSTRACT<br/>PI: Lijian Yang<br/>PROPOSAL : 0405330<br/><br/>This research develops multi-step ahead forecasting methods for<br/>nonlinear time series using a Monte-Carlo procedure. The focus is on<br/>taking advantage of three types of simplifying structures in nonparametric<br/>auotoregression time series models: small number of significant lags,<br/>additive model, and additive coefficient model. For time series data<br/>generated according to one of these structures, plug-in type predictors<br/>are developed by estimating the data generating process (DGP) and<br/>then using the estimated DGP to generate realizations. By employing local<br/>polynomial and polynomial spline techniques for the regression structure<br/>and kernel density estimation for the noise distribution, the empirical<br/>distribution of these generated realizations approximates the theoretical<br/>distribution of the time series at rates much faster than that of general<br/>multivariate function smoothing. Hence, the proposed forecasts are much<br/>more accurate than those made with many existing methods. Multi-step ahead<br/>forecasting of time series generated by either multiple index or partially<br/>linear autoregression are also studied, for which the forecasting<br/>accuracy is improved significantly as well. Besides being<br/>theoretically justified, the proposed forecasting methods are expected to<br/>be computationally expedient and should be easily accessible to<br/>practitioners working with time series data.<br/><br/>Time series data appear in many scientific disciplines in the form of<br/>sequences of numbers observed over fixed time period. Economic time series<br/>include leading indicators such as inflation index, oil price, real GNP,<br/>unemployment rate, etc., observed monthly or quarterly. Climatology<br/>studies the trend and variation over time of humidity, precipitation,<br/>temperature, etc, while geographers collect daily measurements of such<br/>variables as leaf area index, soil adjusted vegetation index, soil<br/>moisture index and investigate relationships that exist among them and<br/>with other indices. Of central importance in the analysis of time series<br/>is the understanding of the hidden mechanism that generate the data<br/>sequentially, and the use of such knowledge to predict what the next one<br/>or few numbers will be. These are one- and multi-step ahead forecasting.<br/>The statistical tools developed through this research significantly<br/>enhance the capability to forecast macro-economic time series data several<br/>quarters, even years, ahead. Advanced forecasts of this kind can greatly<br/>help the making of macro-economic decisions. These forecasting methods<br/>have strong impact in multiple disciplines beyond macroeconomics, for<br/>instance, in the study of interaction between climate change and the<br/>various geographic indices.  <br/><br/><br/>"
"0405367","IX Latin American Conference on Probability and Mathematical Statistics","DMS","PROBABILITY, STATISTICS, Catalyzing New Intl Collab","04/01/2004","03/24/2004","Alicia Carriquiry","IA","Iowa State University","Standard Grant","Grace Yang","09/30/2004","$16,500.00","","alicia@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1263, 1269, 7299","0000, 5913, 5926, 5977, OTHR","$0.00","ABSTRACT:<br/> <br/>The IX Latin American Conference on Probability and Mathematical Statistics is held in Punta del Este, Uruguay, from March 23 to March 27, 2004.  Conference organizers structure activities to provide as much opportunity as possible for discussions among conference participants, and to create an environment that is welcoming of young researchers and graduate students who wish to present their work.  Several introductory short courses on topics ranging from stochastic calculus with financial applications, to graphical models, and model selection in a Bayesian framework, are taught.  In addition, special discussion sessions on course topics are organized, to facilitate the interaction between young researchers and their more senior colleagues. <br/><br/><br/>In the past twelve years, collaboration between Latin American and US mathematical scientists has increased many-fold.  One important consequence has been the growth of well-trained students from Latin American institutions who choose to enroll in graduate programs in American universities and who then develop close and enduring ties to the US academic community.  In part, this increased interaction is due to the series of Latin American conferences in probability and statistics, that are generally held every two years in a Latin American country, and that typically include a significant contingent of US participants.  The conferences have grown both in size and importance over time, and have contributed to the creation of a community of applied probabilists and statisticians who are eager to work in interdisciplinary teams to solve complex problems of importance to society.  The National Science Foundation, through its funding of travel awards for US scientists and for scientists from selected Latin American countries excluding Cuba , has been an important sponsor of these workshops.  The funding requested from NSF for the IX Latin American conference will be used to partially cover travel expenses of 10 US participants and of six Latin American conference participants  from countries other than Argentina, Brazil, Chile, Cuba, Mexico, Venezuela.<br/><br/> <br/><br/>"
"0400585","Workshop:  Pathways to the Future Workshop 2004","DMS","STATISTICS","06/01/2004","04/30/2004","Lynne Billard","GA","University of Georgia Research Foundation Inc","Standard Grant","Grace Yang","05/31/2005","$12,000.00","","lynne@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","ABSTRACT<br/><br/>     In August 1988 a small Workshop of approximately twenty women researchers was held immediately prior to the Annual Meeting of the Institute of Mathematical Statistics and its Special Symposium on Probability and its Applications.  Encouraged by the enormous success of this first Workshop, one has been run in subsequent years prior to the major joint annual meeting of American Statistical Association, regions of the International Biometric Society and the Institute of Mathematical Statistics, except that in 1994 it ran in conjunction with the World Congress of the Institute of Mathematical Statistics and the Bernoulli Section of the International Statistical Institute and in 2000 it ran in conjunction with the International Biometric Conference in Berkeley.  The Workshops have been a huge success with laudatory thanks proferred each time.  The need for such avenues for young researchers is still present.  Therefore, this proposal is for a similar Workshop to be held in 2004 to be targeted at newly graduating young researchers who have not as yet had the opportunity to participate.  <br/><br/>     The Workshop is proposed to assist young women researchers in probability and statistics who have received their doctorates in the last five years.  A senior researcher will give a major presentation on what where and how to ensure a successful and productive research career.  Another senior will give a presentation on publishing  - where and what to publish and how to write up research results.  A third senior researcher will give a presentation on grant opportunities and how to pursue them.  Workshop participants would give a brief presentation of their research interests and research environment. The opportunity to meet and interact with each other as well as a few established researchers will be invaluable as the participants learn to promote their own research and to decrease their professional isolation.  As a result of the Workshop, attendees should be more successful in their own research careers, and thus, for example, survive the promotion and tenure process so that these women become role models for the students at all levels (undergraduate and graduate) in their own training programs.<br/>"
"0405748","A Contour Based Monte Carlo Algorithm with Applications to Computational Statistics and Bioinformatics","DMS","STATISTICS","09/01/2004","08/25/2004","Faming Liang","TX","Texas A&M Research Foundation","Standard Grant","Grace Yang","08/31/2007","$90,000.00","","fmliang@purdue.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","Simulation from complex systems, such as proteins, neural networks, and<br/>spin-glasses, is one of the most challenging problems in scientific<br/>computation. The energy landscape of these systems can be characterized<br/>by a multitude of local energy minima separated by high energy <br/>barriers. In simulation from these systems, the conventional Markov<br/>chain Monte Carlo algorithms, such as the Metropolis-Hastings algorithm<br/>and the Gibbs sampler, tend to get trapped in one of local energy<br/>minima indefinitely, rendering the simulation ineffective.<br/>The goal of this research is to develop an effective Monte Carlo<br/>algorithm for simulation from complex systems, and to apply the new<br/>algorithm to some computational problems in statistics and<br/>bioinformatics, including molecular structure prediction, phylogeny<br/>estimation, neural network training, combinatorial optimization,<br/>optimal design,  highest posterior density (HPD) interval construction,<br/>model selection, and others. The preliminary results show that the<br/>contour Monte Carlo algorithm, which is proposed in this research,<br/>will potentially  play a leading role in stochastic optimization in<br/>place of other algorithms, such as simulated annealing and genetic<br/>algorithms.<br/><br/>In this research algorithms are developed that are potentially useful <br/>in many fields such as biology, engineering, and the social sciences. <br/>These algorithms are powerful in identifying best solutions to <br/>optimization problems in applied sciences. Students, researchers, and <br/>users of statistics, such as computational biologists and computer <br/>scientists, will benefit from this research. <br/>   <br/>"
"0413740","SRC  Summer 2004 Conference","DMS","STATISTICS","03/15/2004","03/09/2004","Robert Lund","GA","University of Georgia Research Foundation Inc","Standard Grant","Robert J. Serfling","02/28/2005","$10,000.00","","rolund@ucsc.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","ROBERT B. LUND's PROPOSAL # 0413740 - ABSTRACT<br/><br/>This grant provides travel support for junior researchers and graduate students to attend the 2004 Summer Research Conference (SRC) in Statistics at Blacksburg, Virginia.  The conference is sponsored jointly by the Southern Regional Council on Statistics and the American Statistical Association and will be held June 6 through June 9, 2004.  Invited session topics include Bayesian methods, statistical finance, Markov chains, design of experiments, generalized linear models, functional data analysis, application and case studies, and statistical education.  Participants are exposed to cutting-edge research in statistics as well as future issues in statistical education and training.<br/>The primary impact of the conference will be the dissemination of research information and teaching techniques.  For many junior participants, this conference provides a first exposure to new research topics.  Young statisticians are thus afforded a broad view of current activity within the discipline in a relatively short time frame and small conference setting. The SRC in 2004 emphasizes a broad set of topics that should be of interest to many general statisticians.  Hence, numerous collaborations are expected to be conceived at the conference.  The 2004 SRC is expected to have approximately 100 participants.  This small scale is expected to enhance interaction during the three-day event.<br/><br/><br/>"
"0400584","Statistical Inference for Complex Data","DMS","STATISTICS","05/15/2004","03/04/2008","Lynne Billard","GA","University of Georgia Research Foundation Inc","Standard Grant","Gabor Szekely","04/30/2009","$218,297.00","","lynne@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","The advent of computers is routinely bringing large, <br/>very large datasets for analysis.  How to analyze the<br/>attendant data and/or how to glean useful information <br/>from these data is anything but routine. This proposal <br/>investigates two broad areas, viz., mixture decomposition, <br/>and regression in the presence of taxonomy and hierarchical <br/>variables.  The mixture decomposition work is concerned with <br/>data in which the observation is a distribution function <br/>in the p-dimensional Cartesian product of distributions space,<br/> rather than the single point in p-dimensional space of <br/>classical data.  Such data arise naturally, or after a<br/>ggregation of original data to a more manageable size <br/>yet retaining its inherent information. A goal is to <br/>partition these distributions into coherent classes <br/>and to estimate the relevant class distributions.  <br/>It is proposed to adapt ideas from copula theory for <br/>classical data, to data comprised of distributions.  <br/>Embedded in this process is the need to study parameter <br/>estimation.  Different partitioning techniques will be <br/>explored such as dynamical clustering, different measures <br/>of fit criterion will be considered, e.g., log-likelihood <br/>classification; and different estimation methods explored <br/>for the underlying copulas and the associated distributions <br/>and their parameters, e.g., maximum likelihood, nonparametric <br/>methods such as Parzen's truncated window.  The resulting <br/>methodology will have wide applicability to those datasets <br/>generated in, e.g., meteorology, environmental science, <br/>social sciences, health-care programs, and the like.  <br/>Regression methods when taxonomy variables, and when <br/>hierarchical variables, are present will also be developed.  <br/>This will first be executed for classical data, and then <br/>extended to interval-valued data and to histogram- <br/>(or frequency-) valued data.<br/><br/>With modern computers generating very large datasets, it is imperative<br/>that techniques be developed for analysing such datasets. To date very<br/>few methods exist. The research will develop new methodologies for<br/>analysing these data. A first step is to aggregate the data in some<br/>well-defined but meaningful way. This aggregation will thence produce<br/>data in the form of lists, intervals, or distributions, and these data<br/>will now have some form of internal structure. The research will focus<br/>on such data of two types. One will be where the data now consist of<br/>distributions, and where the goal is to develop methods to identify the<br/>appropraite mixture of distributions that describe these data. Another<br/>deals with taxonomy and hierarical data, with the goal of establishing<br/>regression relationships that will explain the underlying process<br/>governing the variables involved. The resulting methodologies will<br/>allow analysis and interpretation of contemporary datastes where<br/>currently no methods exist.  <br/>"
"0348869","CAREER: Model Selection for Semiparametric Regression Models in High Dimensional Modeling and its Oracle Properties","DMS","STATISTICS","07/01/2004","09/06/2009","Runze Li","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","06/30/2011","$440,000.00","","rzli@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, 1045, 1187, OTHR","$0.00","proposal: DMS 0348869<br/>PI: Runze Li<br/>institution: The Pennsylvania State University<br/><br/>Model Selection for Semiparametric Regression Models in High<br/>Dimensional Modeling and its Oracle Properties<br/><br/>                       Abstract<br/><br/>Model selection is fundamental to high-dimensional data analysis, and<br/>semiparametric regression models are potentially useful for analysis of<br/>high-dimensional data. Model selection for semiparametric regression<br/>models consists of two components: model selection (such as choice of<br/>smoothing parameters) for the nonparametric component, and variable selection<br/>for the parametric portion. Traditional variable selection<br/>schemes, such as the stepwise deletion and the best subset variable<br/>selection, could be extended to semiparametric modeling, but they are<br/>expensive in computation since they require the smoothing parameters to<br/>be selected for each submodel. The objectives of this proposal are to<br/>develop new widely applicable model selection procedures for three<br/>classes of semiparametric models which provide a unified framework for<br/>many existing semiparametric regression models in the literature. In this<br/>proposal, the PI (a) studies the asymptotic behaviors of the proposed<br/>estimators, (b) demonstrates how the rate of convergence of the resulting<br/>estimator depends on the regularization parameter, (c) shows that the proposed<br/>procedures perform as well as the oracle procedure in variable selection<br/>for semiparametric regression models, and (d) addresses issues related to<br/>implementation of the proposed procedures. The PI also examines finite<br/>sample performance via extensive Monte Carlo simulation studies and<br/>applies the proposed procedures to analysis of real data.<br/><br/><br/>With modern data collection devices and vast data storage space, one can<br/>easily collect high-dimensional data, such as biotech data, financial data,<br/>satellite imagery and hyperspectral imagery. Analysis of high-dimensional<br/>data poses many challenges for statisticians and is becoming the most<br/>important research topic in statistics. This proposal (a) lays down<br/>a well-grounded and comprehensive framework for model selection for<br/>semiparametric regression modeling in high-dimensional data analysis,<br/>(b) has significant impact on the future research of high-dimensional<br/>statistical modeling, and (c) enhances significantly the availability<br/>of statistical tools and software for high-dimensional statistical modeling.<br/>The proposed work is incorporated into a new topic course from which<br/>graduate students may directly benefit. The proposed work also<br/>benefits a broad range of scientists and researchers in various fields, including<br/>automotive engineering, medical studies, prevention studies, public health<br/>and social sciences.<br/>"
"0405637","High Dimensional Mixture Models","DMS","STATISTICS","08/01/2004","06/03/2008","Bruce Lindsay","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","07/31/2010","$564,000.00","","bgl@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, OTHR","$0.00","The purpose of this project is the development of theory, statistical <br/>methodology, and computational<br/>methods for use in mixture models in high dimensional data. In the <br/>theoretical portion, the investigator<br/>enhances the potential statistical applications of these models by <br/>examining their topographical structure and their relationship to other <br/>high-dimensional methods such as local linear regression and<br/>hierarchical trees. New kernel densities are being constructed by the use <br/>of the idea of<br/>diffusion processes. New methods to assess the important aspects of <br/>identifiability in these models<br/>are under development. In addition to these basic theoretical developments, <br/>the investigator is creating a set of methods designed to fit diffusion <br/>mixture models, and to assess their fit,  in high dimensions. A key part of <br/>this methodological development is occuring in computational enhancements.<br/><br/>The statistics community is faced with a great challenge by modern science, <br/>and that is to<br/>develop new tools for scientific inference in the aftermath of the data <br/>revolution.  Modern data is potentially high in dimension, and massive in <br/>the number of collected units. The probability models called mixture models <br/>have had a long history of use in describing heterogeneity in data samples. <br/>They are extremely flexible, and provide a compact picture of the key <br/>features of the data structure. Unfortunately, limited theoretical <br/>developments in this difficult area have held back their use in high <br/>dimensional problems. This research project targets a number of the key <br/>difficulties remaining in this area, including the integration of this <br/>methodology with other existing ones, the expansion<br/>of this methodology into new data types, and a better understanding of this <br/>model's structure in<br/>very high dimensions. The methods that arise from these developments are <br/>being turned into computational packages so that they can be used by <br/>scientists.<br/><br/>"
"0349048","CAREER:  Use of Covariate Information in Adaptive Designs","DMS","STATISTICS","09/01/2004","05/15/2008","Feifang Hu","VA","University of Virginia Main Campus","Continuing Grant","Gabor Szekely","08/31/2010","$400,000.00","","feifang@gwu.edu","1001 EMMET ST N","CHARLOTTESVILLE","VA","229034833","4349244270","MPS","1269","0000, 1045, OTHR","$0.00"," Abstract<br/><br/>An adaptive design is a sequential design where the design points are chosen based on both previous design points and the outcomes at those design points. Covariate information is often available and usually very important in statistical inference. Very few adaptive designs described in the literature attempt to incorporate covariate information. This proposal is concerned with using covariate information in adaptive designs. The investigator proposes a new class of adaptive designs under some optimal criteria. The new designs (i) can incorporate covariate information; and (ii) exhibit certain desired properties when there is no covariate information. The investigator then studies the properties of the proposed designs when incorporating covariates. Further he investigates the relationship between efficiency of estimation and the total number of failures in experiments. Based on these results, the investigator develops a procedure to select a suitable adaptive design for a particular problem.<br/><br/>In clinical trials, prognostic factors are often available and usually very important.  How to incorporate prognostic factors (covariates) into trial design is a critical, and conceptually difficult, problem. Adaptive designs use data sequentially as it is collected, making use of it in deciding, for example, how to allocate future subjects between treatment groups or even whether or not future subjects are needed to achieve some accuracy objective. In this proposal, the investigator will thoroughly investigate using covariate information in adaptive designs. Upon completion of this project, a class of adaptive designs, which incorporates covariate information, will be available for clinical trials and other experiments. Applications exist in a wide variety of fields: industrial experiments, bioassay, and clinical trials. The full impact of these designs will be realized through dissemination in the literature, presentations, group working sessions, and interdisciplinary collaborations. The project will stimulate researchers and students to work in the area of adaptive designs, and should lead to numerous graduate and undergraduate level projects and these<br/>"
"0405072","Collaborative Research:     Bayesian ANOVA for Microarrays","DMS","STATISTICS","08/15/2004","08/02/2004","Jonnagadda Rao","OH","Case Western Reserve University","Standard Grant","Gabor Szekely","07/31/2008","$53,880.00","","js-rao@umn.edu","10900 EUCLID AVE","CLEVELAND","OH","441061712","2163684510","MPS","1269","0000, OTHR","$0.00","DNA microarrays can provide insight into genetic changes occurring<br/>during stagewise progression of diseases like cancer.  Accurate<br/>identification of these changes has significant therapeutic and<br/>diagnostic implications.  Statistical analysis of such data is however<br/>challenging due to the sheer volume of information.  With new<br/>microarray technology it is possible to measure expressions on nearly<br/>60,000 transcripts for each sample of tissue analyzed.  To properly<br/>understand the evolution of a progressive disease, expression values<br/>are collected over all possible biological stages, thus the number of<br/>parameters in such problems can be in the hundreds of thousands, or<br/>even millions.  The high dimensionality presents theoretical problems<br/>to standard ANOVA-based extensions of two-sample Z-tests, a popular<br/>method for detecting differentially expressed genes in two groups.<br/>Additionally, standard approaches that focus on controlling false<br/>detection rates primarily apply to simpler experimental designs;<br/>moreover these approaches tend to be conservative and are expected to<br/>be worse in multigroup settings.  This work introduces a new<br/>methodology called Bayesian ANOVA for Microarrays (BAM) for reliably<br/>detecting differentially expressed genes in complex experimental<br/>settings.  The method rests on a high dimensional variable selection<br/>method that exploits a rescaled spike and slab hierarchical model.<br/>BAM is shown to be risk optimal in terms of the total number of<br/>misclassified genes.  The exact mechanisms for this risk optimality<br/>are theoretically delineated as a selective shrinkage effect. Theory<br/>guides development of graphical devices for adaptive optimal gene<br/>selection.  A large multistage colon cancer microarray repository<br/>collected at the Ireland Cancer Center of Case Western Reserve<br/>University serves as a testbed for the methods.  In parallel to this<br/>is the development of JAVA-based software for implementing BAM.<br/>Software uses a menu driven GUI and includes a minimal number of<br/>user-specified tuning parameters, thus making it user friendly for use<br/>by other molecular biology laboratories.<br/><br/><br/><br/><br/>DNA microarrays allow for high throughput analysis of potential<br/>genetic determinants of diseases like cancer.  It is now typical to<br/>have expression on nearly 60,000 transcripts for each sample of tissue<br/>analyzed.  This information can potentially provide information about<br/>which genes are involved in stagewise development of cancer as well as<br/>indicate novel therapeutic and diagnostic targets.  However,<br/>statistical inferences to identify interesting genes is challenging<br/>due to the large number of statistical tests that are run.  Standard<br/>approaches employ ANOVA test statistics and are prone to high false<br/>detections.  False detection rate control methods tend to be overly<br/>conservative and do not extend naturally to more complex multistage<br/>experimental designs.  This work introduces a new methodology called<br/>Bayesian ANOVA for Microarrays (BAM) which reliably detects<br/>differentially expressed genes in multigroup experimental design<br/>settings.  The method employs a special hierarchical model that<br/>imparts an oracle like behaviour for gene selection --- that is,<br/>ultimately, only those truly differentially expressing genes are<br/>selected.  The reasons for this behaviour are theoretically delineated<br/>in this research, and the theory guides the development of novel<br/>graphical devices for adaptively optimal gene selection in real<br/>microarray datasets.  A large multistage colon cancer microarray<br/>repository collected at the Ireland Cancer Center of Case Western<br/>Reserve University serves as a testbed for the methods and also<br/>provides a tremendous opportunity to understand the colon cancer<br/>disease process, a topic which is of great medical importance.  While<br/>colon cancer has a well defined evolution defined by clinical stage,<br/>very little is known about its molecular evolution.  In parallel to<br/>this, is the development of JAVA-based software using a menu driven<br/>GUI having a minimal number of user-specified tuning parameters, thus<br/>making it feasible to port the software to molecular biology<br/>laboratories for active use in analysis of other disease processes and<br/>potentially other high throughput sources of data.<br/>"
"0515990","Adaptive Regression for Dependent Data by Combining Different Procedures","DMS","STATISTICS","12/15/2004","03/28/2005","Yuhong Yang","MN","University of Minnesota-Twin Cities","Continuing Grant","Grace Yang","05/31/2007","$81,226.00","","yyang@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, 1045, 1187, OTHR","$0.00","This proposal concerns research and education on adaptive regression<br/>when the random errors are dependent. Many procedures have been <br/>(and will be) proposed for nonparametric regression based on<br/>different assumptions. In applications, a difficulty a user often<br/>faces is the choice of the best method for the data at<br/>hand. This is specially the case for high-dimensional function<br/>estimation, where to overcome the curse of dimensionality, various <br/>parsimonious models such as projection pursuit, CART, neural nets, <br/>additive models, MARS, etc. are proposed according to different <br/>characterizations of the target function. A main interest in this <br/>research is to construct adaptive estimators by combining a<br/>collection of candidate procedures. The goal for the combined<br/>procedure is to perform automatically as well as (or nearly as well<br/>as) the best original procedure without knowing which one it is.  <br/>The random errors will be assumed to be generally dependent,<br/>including both short- and long-range cases. The effects of<br/>dependence on adaptation capability will be studied. It is<br/>anticipated that theoretically proven and computationally feasible <br/>algorithms will be proposed to combine regression procedures<br/>targeted at various characteristics of the regression function and <br/>different dependence structures for the random errors.<br/><br/>Function estimation is an important statistical tool that tries to<br/>understand accurately the functional relationships between variables based on <br/>data and it has applications in many disciplines for successfully  <br/>addressing scientific questions. In reality, observations are always<br/>subject to random noise (error) from different sources. When the<br/>random errors are dependent on each other, the dependence may<br/>disguise the functional relationship of interest. Long-range<br/>dependence refers to a situation where the errors are still highly<br/>correlated even when they occur at times or locations that are far<br/>away from each other. It is known that such a long-range dependence makes<br/>the estimation of the target function much harder. In applications,<br/>the degree of dependence between the errors is usually unknown,<br/>which makes the function estimation problem even harder. In this<br/>proposal, we intend to develop methods that adaptively handle<br/>different degrees of dependence among the errors so that the<br/>function of interest can be estimated optimally without knowing the<br/>dependence structure of the errors. The research results and related<br/>work by others on long-range dependent data will be brought to<br/>students at various levels in several statistics<br/>courses. Collaborations will be conducted with several professors at<br/>Iowa State University and their students in atmospheric science, <br/>electrical engineering, agronomy and possibly other fields to<br/>appropriately address long-range dependence phenomena, which have<br/>been encountered often and known to cause problems in data analysis <br/>with the existing statistical methods. <br/>"
"0405681","Collaborative Research:     Sufficient Dimension Reduction for High Dimensional Data with Applications in Bioinformatics","DMS","STATISTICS","07/01/2004","05/04/2006","Bing Li","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","06/30/2007","$268,984.00","Hongyuan Zha, Francesca Chiaromonte","bing@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, OTHR","$0.00","Abstract <br/>proposals: 0405360 and 0405681<br/>PIs: Cook & Li<br/><br/>COLLABORATIVE RESEARCH: Dimension Reduction with application to <br/>bioinformatics<br/><br/>As represented in the existing literature, sufficient dimension reduction (SDR) <br/>encompasses model-free methods for linearly reducing the dimension of the <br/>predictor vector in regression and classification problems without loss of <br/>information.  SDR methodology has a brief but striking record of success, <br/>although its inferential foundations are relatively narrow and the restriction <br/>to linear reductions can be limiting in some applications.  The investigators <br/>and their co-authors expand the inferential foundations of SDR through the <br/>development of optimal methods within the context of linear reduction and the <br/>study of new nonlinear reduction methods.  The new optimal reduction methods <br/>permit the investigators to derive model-free tests of conditional independence, <br/>which are roughly data-analytic equivalents of t-tests on coefficients in model-<br/>based linear regression.  They emphasize bioinformatics applications in general <br/>and the analysis of data from high-throughput genomic technologies in <br/>particular.<br/><br/>The computer revolution has produced an unprecedented capacity for data <br/>generation, processing and storage, with the consequence that data reduction is <br/>paramount in many research areas and business applications.  For instance, <br/>genomic technology can produce measurements for thousands of genes across <br/>multiple tissue samples, and WalMart makes over 20 million transactions daily. <br/>The development of diagnostics for breast cancer based on fine needle aspiration <br/>can involve the study of numerous measurements on extracted cells across <br/>hundreds of patients. In response to this proliferation of information, the <br/>investigators and their colleagues study methods for reducing data to <br/>an essential core. Their approach is unique because their overarching goal <br/>is reduction without loss of information on the issues under consideration.  <br/>In the development of diagnostics for breast cancer, this goal translates <br/>into reducing numerous cell measurements to an index that can be used to <br/>classify a breast mass as malignant or benign without loss of information, <br/>allowing the physician to present a more informed recommendation to the patient.<br/>"
"0429091","ISBA 2004 - Bayesian Analyses and Applications; Vina del Mar, Chile","DMS","STATISTICS, Catalyzing New Intl Collab","07/01/2004","06/21/2004","Alicia Carriquiry","IA","Iowa State University","Standard Grant","Grace Yang","12/31/2004","$12,000.00","","alicia@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269, 7299","0000, 5974, 5977, OTHR","$0.00","ISBA 2004, a world meeting of the International Society for Bayesian Analysis, will be held in Vina del Mar, Chile, from May 23 to May 28, 2004. The conference provides a venue for all scientists interested in Bayesian analysis to present their work and interact with colleagues from all over the world.  ISBA world meeting organizers place a very high premium on attracting graduate students and junior researchers, and strongly encourage them to present their work and interact with international colleagues.  Funding from NSF was requested to provide travel grants to graduate students from the United States and from some Latin American countries (except Argentina, Brazil, Chile, Cuba, Mexico and Venezuela).<br/><br/>In the last fifteen years or so there has been an explosion in the development and application of Bayesian ideas.  Research in Bayesian statistics and Bayesian analysis is very active worldwide, and a large proportion of young researchers are now active members of the Bayesian community.  International meetings such as the ISBA world meetings and the Valencia International meetings that focus on Bayesian ideas, have also increased in size, quality and importance over this period.  Encouraging the participation of graduate students and researchers in this community has been an important goal of these conferences.  For the first time, ISBA world meeting organizers have set aside a significant number of sessions at the conference for students. Graduate students worldwide were asked to submit complete proposals for sessions for the consideration of the program committee.  A set of eighteen graduate student-organized sessions (selected from over forty proposals received) is now a part of the program.  Funding from the National Science Foundation was solicited to provide travel grants to graduate students who organized sessions or who will be presenting their work at the conference, and to young researchers who also present their work.<br/>"
"0406085","Collaborative Research: Highly Structured Models and Statistical Computation in High-Energy Astrophysics","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","08/15/2004","08/09/2004","David van Dyk","CA","University of California-Irvine","Standard Grant","Gabor Szekely","07/31/2008","$343,775.00","","dvd@uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1269, 7454","0000, 7303, OTHR","$0.00","Pricipal Investigators: David Van Dyk and Xiao-Li Meng<br/><br/>Institutions: UC Riverside and Harvard University<br/><br/>Collaborative Research: Highly Structured Models and Statistical <br/>Computation in High-Energy Astrophysics<br/><br/>Abstract<br/><br/>The California-Harvard Astrostatistics Collaboration aims to design<br/>and implement fully model-based methods of statistical inference to<br/>solve outstanding data analytic problems in high-energy<br/>astrophysics. The Collaboration's methods explicitly model the<br/>complexities of both astronomical sources and the data generation<br/>mechanisms inherent in new high-tech instruments and fully utilize the<br/>resulting highly structured models in learning about the underlying<br/>astronomical and physical processes. Using these models requires<br/>sophisticated scientific computation, advanced methods for statistical<br/>inference, and careful model checking procedures. The PIs of the<br/>Collaboration (van Dyk and Meng) both have substantial research<br/>experience in developing the methods that the Collaboration is<br/>extending, employing, and publicizing: inferential and efficient<br/>computational methods under highly-structured models that involve<br/>multiple levels of latent variables and incomplete data.  Such models<br/>are ideally suited to account for the many physical and instrumental<br/>filters that compose the data generation mechanism in high-energy<br/>astrophysics. The five consultants on the project (Chiang, Connors,<br/>Kashyap, Karovska, and Siemiginowska) all have expertise on the<br/>instrumentation and science of high-energy astrophysics, and, all have<br/>collaborated with statisticians in efforts to develop appropriate<br/>methods to address scientific questions. There are two primary impacts<br/>of this project: the impact of the development of more reliable<br/>statistical methods on scientific findings in astronomy and the impact<br/>of the new statistical inference and computation methods in a wide<br/>range of scientific fields. As the Collaboration develops methods and<br/>distributes free software for specific inferential tasks, it also<br/>educates the astronomical community as to the benefit of careful use<br/>of sophisticated statistical methods. (The Collaboration organizes one<br/>or two special sessions at meetings of the American Astronomical<br/>Society each year.) It is expected that a fundamental impact of the<br/>proposed research will be a more general acceptance and more prevalent<br/>use of appropriate methods among astronomers. Second, the<br/>Collaboration is an example of a new mode of statistical<br/>inference. Rather than using off-the-shelf models and methods, it is<br/>becoming ever more feasible to develop application specific models<br/>that are designed to account for the particular complexities of a<br/>problem at hand.  The Collaboration develops inferential and<br/>computational methods for handling such multi-level models. As<br/>application specific multi-level models become more prevalent, these<br/>methods will have application throughout the natural, social, and<br/>engineering sciences.<br/><br/>In recent years, there has been an explosion of new data in<br/>observational high-energy astrophysics. Recently launched or<br/>soon-to-be launched space-based telescopes that are designed to detect<br/>and map ultra-violet, X-ray, and gamma-ray electromagnetic emission<br/>are opening a whole new window to study the cosmos. Because the<br/>production of high-energy electromagnetic emission requires<br/>temperatures of millions of degrees and is an indication of the<br/>release of vast quantities of stored energy, these instruments give a<br/>completely new perspective on the hot and turbulent regions of the<br/>universe. The new instrumentation allows for very high resolution<br/>imaging, spectral analysis, and time series analysis. The Chandra<br/>X-ray Observatory, for example, produces images at least thirty times<br/>sharper than any previous X-ray telescope.  The complexity of the<br/>instruments, the complexity of the astronomical sources, and the<br/>complexity of the scientific questions leads to a subtle inference<br/>problem that requires sophisticated statistical tools. For example,<br/>data are subject to non-uniform censoring, errors in measurement, and<br/>background contamination. Astronomical sources exhibit complex and<br/>irregular spatial structure. Scientists wish to draw conclusions as to<br/>the physical environment and structure of the source, the processes<br/>and laws which govern the birth and death of planets, stars, and<br/>galaxies, and ultimately the structure and evolution of the<br/>universe. Nonetheless little effort has been made to bring the<br/>strength of modern statistical methods to bare on these problems. The<br/>California-Harvard Astrostatistics Collaboration develops statistical<br/>methods, computational techniques, and freely available software to<br/>address outstanding inferential problems in high-energy astrophysics.<br/>The methods developed are an example of a new mode of statistical<br/>inference: Rather than using off-the-shelf methods, it is becoming<br/>ever more feasible to develop methods that are application specific<br/>and are designed to account for the particular complexities of a<br/>problem at hand. The inferential and computational methods designed by<br/>the Collaboration for handling such multi-level models have<br/>application throughout the natural, social, and engineering sciences.<br/>"
"0405584","Inference for Restricted Parameters","DMS","STATISTICS","07/01/2004","08/01/2005","Michael Woodroofe","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","06/30/2008","$288,125.00","","michaelw@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","ABSTRACT<br/>PI: Michael Woodroofe <br/>proposal: 0405584<br/><br/>   INFERENCE FOR RESTRICTED PARAMETERS<br/><br/><br/> Problems in which a density or regression function is known to be smooth and to satisfy shape restrictions, like monotonicity or convexity, are investigated.  Shape restricted regression and smoothing splines offer one promising approach to the problem, and the combination of shape restrictions with local polynomial methods another. In large samples the emphasis is on  asymptotic distributions.  With a lot of smoothing the contribution of the shape restrictions to the asymptotic distribution is negligible; with only a little smoothing, the shape restrictions dominate, leading to non-normal asymptotic distributions.  The nature of the transition between a little smoothing a lot is investigated.  In many cases, especially with smoothing splines, an estimator is the solution to a differential equation.  In such cases it is possible to use the Green's function to construct an asymptotically equivalent kernel estimator from which the asymptotic distribution may be found.  In moderate samples, the shape restrictions do affect the distribution of estimators, even in the presence of substantial smoothing.  The size of this effect is investigated both analytically, using techniques of shrinkage estimation, and by simulation.  Problems with known inequalities for parameters also arise in models with only a few parameters, like variance components, and classical confidence intervals can be empty, or degenerate, in such problems.  In current work interest centers on finding Bayesian credible intervals with good frequentist properties.<br/> <br/> The work on shape restricted density estimation and regression is motivated by a project to map the distribution of dark matter in nearby dwarf spheroidal galaxies, like Ursa Minor.  Dark matter is matter that cannot be seen.  Its existence is inferred from gravitational effects, but what it is comprised of is an open question.  It is expected that knowing where the dark matter is may shed some light on what it is.  The work on models with few parameters is motivated by the problem of disentangling signal and background events in both physics and astronomy.  The confidence interval problem is an important part of the search for an elementary particle, the Higgs, in particular, and has been a featured topic at several recent meetings of high-energy physicists.  The investigator is using these two examples and others in an interdisciplinary seminar on Statistics in the physical sciences. <br/>"
"0406229","Adaptive Sampling Designs in Network and Spatial Settings","DMS","STATISTICS, Methodology, Measuremt & Stats, ","08/01/2004","08/11/2005","Steven Thompson","PA","Pennsylvania State Univ University Park","Standard Grant","Gabor Szekely","07/31/2008","$300,000.00","Steven Thompson","skt@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269, 1333, V620","0000, OTHR","$0.00","The purpose of this research project is to develop new adaptive<br/>sampling designs and inference methods for sampling in network and<br/>spatially structured populations.  Adaptive sampling designs are those<br/>in which the procedure for selecting the sample can depend on values<br/>of variables of interest observed during the survey.  In spatial<br/>settings, that can mean adaptively adding new units to the sample in<br/>the vicinity of high or otherwise interesting observed values.  In<br/>network or graph settings, links can be adaptively followed from<br/>interesting sample nodes to add new nodes to the sample.  A variety of<br/>new sampling procedures, together with design and model based<br/>estimation methods, will be investigated in the study.  A new,<br/>flexible and versatile class of adaptive designs, termed ``active set<br/>adaptive sampling,'' was found during the preliminary work toward this<br/>project.  Designs in this class have certain advantages over adaptive<br/>cluster sampling and some of the traditional network sampling designs<br/>in being more flexible, allowing for control of total sample size and<br/>not requiring complete inclusion of connected components.<br/>Design-unbiased estimates are possible with some of these designs,<br/>providing inferences that are robust against assumptions about the<br/>population.  These designs lend themselves toward model-based<br/>inferences as well and can be used in some situations to help ensure<br/>that the assumptions for the model-based inferences are met.  This<br/>project will advance the theory and methodology of adaptive sampling<br/>and in particular will fully investigate and develop several<br/>categories of new adaptive sampling designs within this class and<br/>develop and evaluate design and model based inference methods for use<br/>with adaptive designs of all types.<br/><br/>With adaptive sampling designs, the study design can change in<br/>response to the values and patterns observed during the study.  For<br/>example, in a study of an at-risk hidden human population, social<br/>links from particularly high-risk individuals can be followed to add<br/>more individuals to the sample; in a survey of an unevenly distributed<br/>natural resource, new observations may be adaptively made in<br/>neighborhoods of high observed abundance.  In previous work it has<br/>been established that in many situations the theoretically optimal<br/>sampling strategy is an adaptive one.  Specific adaptive designs, such<br/>as the adaptive cluster sampling designs developed in a previous<br/>project, have been shown to give substantial gains in precision or<br/>efficiency over conventional strategies for certain types of<br/>populations, in particular rare, clustered ones.  The results of the<br/>proposed research will provide research tools for other scientific<br/>fields, including the biological, environmental, health, and social<br/>sciences.  Each of these fields has to deal with populations that are<br/>difficult to sample by conventional means because of their<br/>unpredictably uneven spatial and network structures.  The sampling<br/>methods resulting from this project have applications to many<br/>situations of importance to society, including studies of hidden<br/>populations such as those at risk for HIV/AIDS, environmental<br/>assessment and monitoring, biological surveys, natural resources<br/>explorations and inventories, Internet surveys, rapid response to<br/>natural and induced health threats, studies in human social behavior,<br/>and archaeological studies.<br/><br/><br/>"
"0405833","Collaborative Research ""Tracking Statistics and Inference for Indirect Measurements""","DMS","STATISTICS","07/01/2004","06/08/2005","Regina Liu","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","06/30/2008","$84,000.00","","rliu@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, 9178, 9251, OTHR","$0.00","Classification and clustering are two fundamental data mining tools for discovering useful patterns. Given that there is generally no perfect classification or clustering procedures, it is crucial to correct or account for the classification error in any subsequent inference which is derived from the classification outcomes.  The research in this proposal is developing inference procedures that incorporate the error associated with classification rates, and consequently is improving the robustness of decision-making processes that are based on classification and clustering mechanisms.  A particular example is the development of tracking statistics in process control applications that correct for errors in defect classifications.  Another example is the development of misclassification rate estimates without the usual assumption that a gold standard exists.  The research is enabling a wider use of data mining and knowledge discovery techniques by removing stringent requirements on data quality levels.  <br/><br/>Many decision-making processes use inputs that are the result of statistical analyses of grouping subjects according to their similarities.  Classification and clustering techniques are two important such grouping methods. The validity of the decision-making rests on the accuracy of the grouping outcomes.  Nowadays, classification and clustering algorithms make use of large databases that are often low in data quality, and consequently introduce biases in the classification and clustering outcomes.  The goal of this research is to develop inference methodologies that adjust for inherent noise in the outputs of classification and clustering algorithms, and thereby improve the accuracy of subsequent decision-making. The results of this research should benefit many areas of applications, which include the analysis of micro-array gene expression data, machine learning, information retrieval, risk analysis, computer-aided diagnostics, and pattern recognition.  <br/>"
"0405202","Statistical Models and Methods for Some Applied Problems","DMS","STATISTICS","06/01/2004","02/09/2005","Cun-Hui Zhang","NJ","Rutgers University New Brunswick","Standard Grant","Grace Yang","05/31/2006","$60,000.00","Cun-Hui Zhang","czhang@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","The project focuses on: (1) multivariate methods based on the new concept of L1 data-depth, (2) combining classifiers with applications to biometrics measurements, (3) multivariate ordered event-time data with bias and incompleteness. All are rich areas for statistical applications and theory, where the PI's have made important contributions.  In (1), the new notion of L1 depth was recently to derive a multivariate data-analytic tools, including clustering and robust regression. These tools were shown to be robust against data contamination, asymptotically efficient, and computationally ""friendly"" for high dimensional data.  The projects extend the methods to include depth-based clustering validation, informative visualization, and multivariate linear- and nonlinear-regression based on a new concept of depth-relative-to-a-model.  In (2), classification based on biometrics data is characterized by high dimensionality of objects, large number of classes, and small number of examples per class in the training data. Recent research shows that merging of classifiers can improve correct-classification rate for such problems. The project focuses on the new method of mixed group rank (MGR) combiners which is shown, based on benchmark data, to improve on the constituent as well as on other combination-methods. The project further develops MGR type combiners, establish their theoretical underpinnings, and provide the necessary computational tools to handle large data sets.  In (3), multivariate ordered event-time data are common in observational studies, including epidemiology, clinical studies, behavioral studies, and more. Such data are hard to analyze, as they are typically subject to biased-sampling and censoring. A new multivariate nonparametric framework enables the extension of univariate statistical methods to multidimensional data. New statistical methods will be developed in nonparametric and semiparametric models for a variety of sampling schemes.  Asymptotic (large sample) theory gives theoretical justification for the methodology.<br/><br/>The proposed research advances several important areas in multivariate statistics. New models, methods, and algorithms will be developed for multivariate clustering, robust multivariate regression, combination of classifiers, and the analysis of biased incomplete multivariate event-time data.  The proposed research has direct impact on a broad range of scientific applications outside the immediate realm of statistics.  Examples include genetics studies, biometrics classification methods, epidemiological, clinical, and behavioral studies. <br/>"
"0406169","A Synthesis of Objective Bayesian and Designed Based Methods for Finite Population Sampling","DMS","STATISTICS","07/01/2004","08/31/2005","Glen Meeden","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","06/30/2008","$201,292.00","","glen@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","ABSTRACT<br/>proposal: 0406169<br/>PI: Glen Meeden<br/><br/><br/><br/>     A synthesis of  objective Bayesian and designed based methods <br/>                 for finite population sampling<br/><br/>         Project Abstract <br/><br/>In the frequentist or design approach to finite population sampling prior<br/>information is incorporated in the sampling design. However in some cases <br/>when calculating estimates the design weights need to be readjusted, and <br/>then it can be difficult to find  sensible estimates of variance.<br/>In the Bayesian approach prior information is incorporated  through<br/>a prior distribution. Since the posterior distribution does not depend <br/>on the design it has been  difficult to theoretically reconcile the <br/>two approaches. The Polya posterior is an objective  Bayesian approach <br/>to finite population sampling that is appropriate when little<br/>or no prior information is available. The investigator is developing <br/>a synthesis of this objective Bayesian approach and the design<br/>based approach. It has two main threads. In the first the Polya <br/>posterior is extended to problems where it cannot be applied directly <br/>because of additional prior information contained in  auxiliary<br/>variables. This leads to a constrained or restricted<br/>version of the Polya posterior. In the second Bayesian models are<br/>developed which directly include the design in the specification of the <br/>prior. Such models are a generalization of the Polya <br/>posterior and using them one  can objectively incorporate <br/>into a prior the same kind of information that is encapsulated in a design.<br/>The investigator is developing the underlying theory and methods to<br/>simulate from these  objective posteriors so that  estimators can be <br/>found in practice and their frequentist properties studied. <br/><br/><br/>One of the most basic problems of statistics is making an inference  <br/>about a population based on a sample collected from the population.<br/>When little is known about the population the  mean of the<br/>values in a random sample is used as an estimate of the population<br/>mean. However in addition to the estimate one also needs a <br/>sensible measure of its uncertainty or variance. In most<br/>situations there is prior information available about the population. <br/>This information should be used in deciding what units are to be <br/>included in the sample, the value of the estimate and the appropriate <br/>measure of uncertainty. The investigator is working on a synthesis <br/>of the two standard approaches to these problems.  The results should make <br/>more effective use of available prior information than present methods.<br/>Because of the many surveys done each year by  government and others <br/>improving survey practice is of great practical significance.<br/><br/>"
"0405360","Collaborative Research:     Sufficient Dimension Reduction for High Dimensional Data with Applications in Bioinformatics","DMS","STATISTICS","07/01/2004","03/10/2006","Ralph Cook","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","06/30/2008","$264,275.00","","dennis@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","Abstract <br/>proposals: 0405360 and 0405681<br/>PIs: Cook & Li<br/><br/>COLLABORATIVE RESEARCH: Dimension Reduction with application to <br/>bioinformatics<br/><br/>As represented in the existing literature, sufficient dimension reduction (SDR) <br/>encompasses model-free methods for linearly reducing the dimension of the <br/>predictor vector in regression and classification problems without loss of <br/>information.  SDR methodology has a brief but striking record of success, <br/>although its inferential foundations are relatively narrow and the restriction <br/>to linear reductions can be limiting in some applications.  The investigators <br/>and their co-authors expand the inferential foundations of SDR through the <br/>development of optimal methods within the context of linear reduction and the <br/>study of new nonlinear reduction methods.  The new optimal reduction methods <br/>permit the investigators to derive model-free tests of conditional independence, <br/>which are roughly data-analytic equivalents of t-tests on coefficients in model-<br/>based linear regression.  They emphasize bioinformatics applications in general <br/>and the analysis of data from high-throughput genomic technologies in <br/>particular.<br/><br/>The computer revolution has produced an unprecedented capacity for data <br/>generation, processing and storage, with the consequence that data reduction is <br/>paramount in many research areas and business applications.  For instance, <br/>genomic technology can produce measurements for thousands of genes across <br/>multiple tissue samples, and WalMart makes over 20 million transactions daily. <br/>The development of diagnostics for breast cancer based on fine needle aspiration <br/>can involve the study of numerous measurements on extracted cells across <br/>hundreds of patients. In response to this proliferation of information, the <br/>investigators and their colleagues study methods for reducing data to <br/>an essential core. Their approach is unique because their overarching goal <br/>is reduction without loss of information on the issues under consideration.  <br/>In the development of diagnostics for breast cancer, this goal translates <br/>into reducing numerous cell measurements to an index that can be used to <br/>classify a breast mass as malignant or benign without loss of information, <br/>allowing the physician to present a more informed recommendation to the patient.<br/>"
"0406020","Image Segmentation for cDNA Microarray Data and Jump-Preserving Surface Estimation","DMS","STATISTICS","08/15/2004","07/02/2007","Peihua Qiu","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","07/31/2008","$89,994.00","","pqiu@ufl.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","This project discusses two different but related research problems: (1) image segmentation for analyzing cDNA microarray images, and (2) jump-preserving surface estimation from noisy data. Image segmentation of microarray images is a critical stage in generating gene expression data, which are widely used in pharmaceutical and clinical research for identifying particular diseases.  Several image segmentation procedures have been included in some software packages for handling gene microarray data. In this project, a new image segmentation methodology is proposed based on local linear kernel smoothing. It is expected that this method would possess some good theoretical properties.  Preliminary numerical studies show that it outperforms some existing procedures. For the purposes of de-noising, change of image resolution, data compression, etc., images often need to be reconstructed. Since edges of an image carry critical information about objects, they should be preserved when the image is reconstructed. So edge-preserving image reconstruction is an important research problem. An image can be regarded as a surface of the image intensity function. In this project, the image reconstruction problem is studied from the perspective of surface estimation. A new jump-preserving surface estimation methodology is proposed based on local linear kernel smoothing. Compared to some existing procedures, this method is easy to use and simple to compute. It is also possible to develop profound statistical theory for this method based on existing theory about local linear kernel smoothing.<br/>Gene microarray data are widely used in pharmaceutical and clinical research.  By comparing gene expression in normal and abnormal cells, microarrays can be used for identifying genes involved in particular diseases, and then these genes can be targeted by therapeutic drugs. Most gene expression data are produced by segmentation of microarray images. So image segmentation techniques are related directly to the quality of gene expression data. This project proposes a new image segmentation technique. Based on preliminary numerical studies, it could improve the current segmentation techniques, and consequently, improve the quality of gene expression data and have a positive impact on pharmaceutical and clinical studies involving gene microarrays. This project also suggests a new jump-preserving surface estimation methodology, which can be used directly for restoring true images from their noisy versions.  Compared to some existing procedures, this method is easy to use and simple to compute. It should be helpful for several different sciences and industries using such techniques (e.g., medical sciences, meteorology, oceanography, military, space communication, etc.).<br/><br/>"
"0404594","Flexible and Adaptive Statistical Modeling","DMS","STATISTICS","08/01/2004","09/19/2008","Robert Tibshirani","CA","Stanford University","Continuing Grant","Gabor Szekely","07/31/2009","$336,000.00","","tibs@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","In the past 10-15 years there have been significant developments in the<br/>areas of  applied regression and classification. Much of the impetus<br/>originally came from outside of the field of statistics, from areas<br/>such as computer science, machine learning and neural networks. In some<br/>cases, statisticians have helped to synthesize these innovations, often by<br/>reinterpreting them from the point of view of standard statistical models.<br/>As a result, we now have at our disposal a very powerful collection of<br/>techniques for adaptive regression and classification.  These are now<br/>being applied to medical diagnosis, bioinformatics and genetic modeling,<br/>chemical process control, shape, handwriting, speech and face recognition,<br/>financial modeling, and a wide range of other important practical<br/>problems.  In this proposal,  the investigator develops new methods for<br/> adaptive regression and classification, with particular application<br/>to problems in bioinformatics and  genomics.  One such method is  the<br/>""fused lasso"", a technique which seeks solutions which are both sparse<br/>and  and smooth. It is designed for problems in which the features have<br/>a natural ordering, e.g. in time or space. Protein mass spectometry<br/>represents one such area of application.<br/><br/>In this research, the investigator develops new statistical algorithms<br/>for analyzing data from biological and human  experiments.  This work<br/>is part of the exploding field of ""bioinformatics"", which seeks to make<br/>sense of the huge volume of data produced by new technologies in biology<br/>and medicine. This kind of  work represents the next step after the<br/>sequencing of the human genome, and has enormous potential for societal<br/>impact through disease prevention and treatment.<br/>"
"0406143","Collaborative Research:  Statistical Analysis on Manifolds:  A Nonparametric Approach for Shapes and Images","DMS","STATISTICS","09/01/2004","05/22/2006","Rabindra Bhattacharya","AZ","University of Arizona","Continuing Grant","Gabor Szekely","08/31/2007","$162,358.00","","rabi@math.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1269","0000, OTHR","$0.00"," The shape or image of an object may be recorded digitally by a finite <br/>number k of landmarks or positions on the object, called a k-ad. The space <br/>of orbits under rotation and translation of k-ads is the size-and-shape <br/>space. If one includes scaling with rotation and translation, then the <br/>space of orbits under the resulting group of transformations on the k-ads<br/>is the shape space. These are examples of Riemannian manifolds, some with <br/>singularities. One approach proposed here for inference about a <br/>distribution on a general differentiable or Riemannian manifold M, based <br/>on a random sample from it, is to carry out a multivariate analysis of the <br/>image under the so-called Log map on one or more tangent spaces to M. <br/>Apart from this intrinsic approach, less computation intensive extrinsic <br/>procedures based on embeddings in Euclidean spaces are investigated. The <br/>project explores consistency and asymptotic distribution theory on <br/>manifolds for robust tests and confidence regions from both points of <br/>view--intrinsic and extrinsic.<br/><br/>   One motivation for this study comes from the need to identify <br/>deformations or shape changes for purposes of medical diagnosis and <br/>biomorphology. Immediate applications also arise to machine vision and <br/>pattern recognition. There are significant impacts of this research on <br/>these and many other fields. Another important goal of this project is to <br/>train students in the newly developed methodologies, leading to the <br/>dissemination of knowledge gained through this research and the creation <br/>of a body of technicians and experts to apply it.<br/>"
"0406115","High Dimensional Model Averaging and Model Selection","DMS","STATISTICS","09/01/2004","08/06/2004","Merlise Clyde","NC","Duke University","Standard Grant","Grace Yang","08/31/2007","$144,000.00","Feng Liang","clyde@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, OTHR","$0.00","ABSTRACT<br/>PI: Merlise A Clyde<br/>PROPOSAL: 0406115<br/><br/>The investigators propose new advancements for the problem of variable and model selection, one of the most fundamental and widespread problems in statistics.  Bayesian methods are appealing for this problem, due to the natural probabilistic framework which addresses both model and parameter uncertainty.  The implementation of Bayesian methods, however, becomes challenging as the number of models or variables grows, an all too common problem where massive data sets provide many potential predictors.  The PI and co-PI investigate two major challenges in the implementation of Bayesian model selection and model averaging: prior specification and posterior calculation.  They investigate new families of automatic objective priors that have desirable risk properties, adapt to unknown degree of sparsity and also permit tractable computation for large scale model search.  To implement the new methodology, they develop efficient software for stochastic search and model averaging for high dimensional model spaces.  Applications in industrial and biological problems will be developed using the new methodology.<br/><br/>Finding and using models to describe relationships between variables in massive datasets is a fundamental problem in both statistics and the sciences.  Bayesian methods have been shown to be very successful for this problem, however, the implementation of Bayesian methods becomes challenging in applications where the number of possible models is astronomical.  The PI and co-PI develop innovative new methods and software in statistical computing and modeling for selecting and combining models.  These methodological developments are driven by applications in industrial and biological problems.  The automatic procedures proposed by the investigators for selecting and combining models have applicability to many other important application areas where variable selection is utilized.<br/>"
"0511743","Computational Algebraic Methods for High-dimensional Statistical Applications","DMS","ALGEBRA,NUMBER THEORY,AND COM, STATISTICS","10/01/2004","01/19/2005","Ian DINWOODIE","NC","Duke University","Continuing Grant","Grace Yang","06/30/2005","$44,068.00","","ihd@pdx.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1264, 1269","0000, 9150, OTHR","$0.00","ABSTRACT<br/><br/>Proposal DMS-0200888<br/>PI Ian Dinwoodie<br/>Title ""Computational algebraic methods for high-dimensional statistical applications""<br/><br/>This project has the goal of research in algebraic methods for high-dimensional parametric statistical problems where classical asymptotic methods are not useful. Connected with the research goal is work with existing and future graduate students. The research problems are:<br/>high-dimensional Gibbs-distribution models for network reliability and traffic with incomplete data; optimization for parameter estimation in high-dimensional models with incomplete data and nonconvex log-likelihood functions; fast simulation methods including fiber walks with Markov chains for integer data tables; and multivariate exponential generating functions for computations on lattice points with the hypergeometric distribution. A range of algebraic tools will be used, including Groebner bases in commutative rings and D-modules, elimination theory, polynomial homotopy methods, and Markov Monte Carlo methods. <br/><br/>This project will bring recent developments in computational algebra to new statistical applications where classical methods do not work.  Examples of such new and challenging applications are large-scale network traffic and reliability, and large databases of tabular data such as census information where security and analysis are difficult.<br/>The algebraic tools can help to solve problems of model formulation and model fitting and statistical analysis. Many algebraic techniques have been recently developed to solve computational problems in robotics and differential equations, and these methods are very promising for statistics. The investigators will develop these algebraic methods to solve statistical applications.<br/>"
"0406049","Curve aggregation and classification","DMS","STATISTICS","07/15/2004","03/30/2006","Florentina Bunea","FL","Florida State University","Continuing Grant","Grace Yang","06/30/2007","$161,296.00","Marten Wegkamp","fb238@cornell.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","0000, OTHR","$0.00","This research is devoted to aggregation of estimators in nonparametric<br/>regression models. The motivation is twofold.  One is the existence<br/>of many different methods of estimation, leading to possibly competing<br/>estimators. In this case one may want to combine the estimators, rather<br/>than to select them. The second one arises in meta-analysis type problems,<br/>in which one wishes to aggregate estimators obtained from different<br/>samples. This research proposes a readily implementable method that addresses<br/>these questions, using tools from the theories of empirical processes,  minimax<br/>adaptive estimation and optimization. The proposed procedure yields, via<br/>an oracle inequality, minimax optimal aggregation bounds for the risk of the<br/>estimators.<br/><br/>This proposal has immediate applications in seismology and clinical<br/>psychology and, more widely, to any data set that is comprised of curves.<br/>The methods proposed here are used to develop signature curves for events<br/>of interest. Having a signature for a seismic wave allows quick<br/>discrimination between earthquakes and waves produced by man induced<br/>explosions. An EEG based signature for depression will improve accuracy in<br/>diagnosing the disease and will permit nuanced classification of further<br/>patients, according to the severity of the illness. The software that allows<br/>these analyzes will be made freely available on the world wide web, along<br/>with a step by step description that will facilitate its use by researchers<br/>from  other disciplines.<br/>"
"0355474","A Statistics Program at the National Center for Atmospheric Research","DMS","STATISTICS","07/01/2004","07/13/2006","Douglas Nychka","CO","University Corporation For Atmospheric Res","Continuing Grant","Gabor Szekely","06/30/2008","$1,230,000.00","Richard Katz, Joseph Tribbia, Jeffrey Anderson","nychka@mines.edu","3090 CENTER GREEN DR","BOULDER","CO","803012252","3034971000","MPS","1269","0000, OTHR","$0.00","ABSTRACT<br/>PI: Douglas Nychka<br/>proposal: 0355474<br/><br/><br/>The Geophysical Statistics Project (GSP) at the National Center for<br/>Atmospheric Research (NCAR) provides a unique opportunity to address<br/>important research problems at the interface between the geosciences and<br/>statistics. The statistical research includes contributions to the<br/>analysis of nonstationary spatial and spatio-temporal data, the design and<br/>analysis of computer experiments, filtering, the statistics of extremes,<br/>data mining, and statistical computing. These methods are adapted to large<br/>data sets and publicly available software is available to implement new<br/>methods for spatial data and extremes. The intellectual contributions to<br/>applications in the geosciences is equally broad and includes the<br/>introduction of stochastic modeling for sub-grid scale processes,<br/>estimating the climatology for extreme events, the construction of<br/>spatially coherent weather generators, and statistical improvements to<br/>methods of data assimilation.<br/><br/>A grand scientific challenge for this century is to understand the complex<br/>interrelationships among the atmosphere, ocean, land processes, biosphere<br/>and human activities that define the Earth System. Coupled with this<br/>effort is the need to predict changes to our environment and to translate<br/>such changes into more immediate economic and societal impacts.  There is<br/>a corresponding challenge for statistical science to tackle the large and<br/>complex data sets that are now the norm in many areas of science and<br/>engineering and to leverage the rich structure and prior knowledge<br/>afforded by traditional numerical models.  Through the training of young<br/>researchers, a vigorous visitor program and participation within several<br/>of the NCAR strategic initiatives, this work reaches a wide community<br/>of statisticians and researchers in the geosciences. In this context, some<br/>specific problems are the design and analysis of large computer<br/>experiments that simulate the Earth's climate, extrapolating historical<br/>meteorological data sets to locations where they are not observed,<br/>improving the algorithms used to make numerical weather predictions, and<br/>identifying models that include random processes to represent complicated<br/>geophysical phenomenon. One common element throughout this research is<br/>quantifying the uncertainty in a derived method or result. Uncertainty<br/>estimates are often difficult to obtain without a statistical treatment<br/>but are particularly important when the analysis is subsequently used for<br/>decision making or to assess risks.<br/><br/><br/>"
"0404979","Theory and Methods for Multiple Testing and Inference","DMS","STATISTICS","07/01/2004","06/24/2004","Joseph Romano","CA","Stanford University","Standard Grant","Gabor Szekely","06/30/2008","$89,998.00","","romano@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","Principal Investigator: Joseph P Romano<br/>Proposal Title: Theory and methods for Multiple Testing <br/>Proposal Id: DMS - 0404979<br/>Abstract<br/>The main goal of this research proposal is the development of theory and  methodology for problems in multiple testing and inference.  A classical approach to dealing with multiplicity is to require decision rules that control the familywise error rate (FWER), the probability of rejecting at least one true hypothesis.  But when the number of tests is large, control of the FWER is so stringent that alternative hypotheses have little chance of being detected.  In response, the false discovery rate (FDR) of Benjamini and Hochberg has gained wide use.<br/>Alternative measures, such as the probability of rejecting k or more true hypotheses, or ones based directly of the actual false discovery proportion (FDP) will be considered by the investigator.  For each  measure of error control, it is desired  to construct procedures that exhibit error control under the weakest possible assumptions.  Subject to error control,  the procedures should be efficient in their ability to detect alternative hypotheses.  The main approach used to develop methods that do not rely on unrealistic or unverifiable model assumptions will be the use of  the bootstrap, subsampling, and other computer-intensive methods.  These tools offer viable approaches to obtaining valid distributional approximations while assuming very little about the stochastic mechanism generating the data.     <br/>Just as resampling has been enormously successful in the case of questions of a single inference, its use can  be   extended fruitfully  to questions of multiple inferences.<br/>While such an approach has been used with some success, its full potential is currently unrealized and it is clear that  efficient and more broadly applicable methods will be advanced in the next few years.  The power of the bootstrap and related methods is that the joint dependency structure of the individual test statistics can be captured so that methods need not be overly conservative.  <br/>The pursuit of  such new methodology will be investigated from theoretical, computational and practical points of view.  Notably, the investigator will address the multiple inference problem when the number of hypotheses is large compared with sample size, the open problem of directional errors, as well as the construction of efficient techniques that control the FDR, as well as other measures of error.<br/>Virtually any scientific experiment sets out to answer  questions about the process under investigation, which often can  be translated formally into a set of hypotheses.  It is the exception that a single hypothesis is considered.  Moreover, due to effects of ""data snooping"" (or ""data mining""), other inference questions arise as well.<br/>The statistician is then faced with the challenge of accounting for all possible errors resulting from a complex data analysis, so that any resulting inferences or interesting conclusions can reliably be viewed as real structure rather than artifacts of random data. While the history of statistical methods that deal with  problems of simultaneous inference data back at least half a century, most of the classical techniques typically rely on strong  assumptions, or they are inefficient.  Driven by the advent of computers and the information age, there has  been a growing demand for more reliable and efficient methods for multiple testing.  For example, current methods in biotechnology and genomics generate DNA microarray experiments, where expression levels in cells for thousands of genes must be analyzed simultaneously. Similar problems arise in image processing, such as neuroimaging, and econometrics.  It is now not uncommon to encounter data consisting of megabytes of information.  Thus, the statistician is faced with new challenges of devising techniques that are not based on strong assumptions and can effectively deal with problems of multiplicity in the presence of vast amounts of data.<br/><br/><br/>"
"0405970","Inference for smooth stochastic processes with applications to neuroimaging","DMS","STATISTICS","07/01/2004","05/27/2009","Jonathan Taylor","CA","Stanford University","Standard Grant","Gabor Szekely","06/30/2010","$280,133.00","","jonathan.taylor@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","ABSTRACT<br/>PI: Jonathan Taylor<br/>proposal: 0405970<br/><br/>The focus of this proposal is the study hitting probabilities for smooth vector-valued random fields with independent, identically distributed Gaussian processes as components. These models can be used to build a variety of non-Gaussian real-valued processes, though they are closely related to Gaussian processes. The smoothness of the processes allows many tools from point processes to be used in studying these hitting probabilities. These point processes, based on critical points of the process yield an explicit representation for the hitting probability as well as an accurate approximation, the so-called expected Euler characteristic approximation. The proposal seeks to extend recent work of the investigator and collaborators on real-valued Gaussian processes to these non-Gaussian models. Insight gained from these models should prove useful in studying other non-Gaussian models. <br/><br/>The practical motivation for this proposal is in its application to estimating the Family Wise Error Rate (FWER) in neuroimaging activation studies. This FWER is important in determining which areas in a neuroimaging study are associated with a given experimental task. In these studies, psychologists are able to collect space-time recordings of activation in the human brain (more precisely, they can record something associated with activation known as the BOLD signal). They are then able to study which areas are activated by their task, which might be a visual task, an auditory task, etc. Having collected the data, the psychologists are faced with the task of determining which perceived activations are true activations. The results of this proposal help psychologists in this decision, by allowing the psychologists to only accept results with a prespecified FWER. The proposal builds on earlier results in the literature, and extends them to more complicated models of activation.<br/>"
"0405267","Statistical Analysis of Long-Memory Continuous-Time Processes","DMS","STATISTICS","06/01/2004","02/09/2006","Kung-Sik Chan","IA","University of Iowa","Continuing Grant","Gabor Szekely","05/31/2008","$187,359.00","","kchan@stat.uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1269","0000, OTHR","$0.00","The investigator studies the continuous-time fractionally-integrated <br/>Auto-Regressive Moving-Average processes and their variants, based on<br/>recent advances in stochastic calculus of fractional Brownian motion.<br/>Such models  provide a general framework for analyzing univariate or <br/>multivariate discrete-time data sampled from an underlying <br/>strongly-dependent continuous-time process. In particular, the <br/>investigator develops methods for  studying volatility, fractional <br/>co-integration and  temporal aggregation of long-memory time<br/>series data. <br/> <br/>Time series data are data collected sequentially over time, and <br/>they abound in science and other fields, e.g.,  finance.<br/>The investigator studies new methods for analyzing time series data<br/>with long-memory temporal patterns. The developed methodologies furnish <br/>general tools for analyzing changes in the volatility pattern in the data, <br/>exploring structural relationships within a set of time series data, and  <br/>assessing effects of aggregating the data over longer observational periods.<br/>These methods have applications in various fields, e.g., pricing of financial<br/>derivatives.<br/><br/>"
"0406151","Collaborative Research: Statistical Analysis on Manifolds: A Nonparametric Approach for Shapes and Images","DMS","STATISTICS","09/01/2004","08/01/2006","Victor Patrangenaru","TX","Texas Tech University","Continuing Grant","Rong Chen","12/31/2006","$113,052.00","","vic@stat.fsu.edu","2500 BROADWAY","LUBBOCK","TX","79409","8067423884","MPS","1269","0000, OTHR","$0.00"," The shape or image of an object may be recorded digitally by a finite <br/>number k of landmarks or positions on the object, called a k-ad. The space <br/>of orbits under rotation and translation of k-ads is the size-and-shape <br/>space. If one includes scaling with rotation and translation, then the <br/>space of orbits under the resulting group of transformations on the k-ads<br/>is the shape space. These are examples of Riemannian manifolds, some with <br/>singularities. One approach proposed here for inference about a <br/>distribution on a general differentiable or Riemannian manifold M, based <br/>on a random sample from it, is to carry out a multivariate analysis of the <br/>image under the so-called Log map on one or more tangent spaces to M. <br/>Apart from this intrinsic approach, less computation intensive extrinsic <br/>procedures based on embeddings in Euclidean spaces are investigated. The <br/>project explores consistency and asymptotic distribution theory on <br/>manifolds for robust tests and confidence regions from both points of <br/>view--intrinsic and extrinsic.<br/><br/>   One motivation for this study comes from the need to identify <br/>deformations or shape changes for purposes of medical diagnosis and <br/>biomorphology. Immediate applications also arise to machine vision and <br/>pattern recognition. There are significant impacts of this research on <br/>these and many other fields. Another important goal of this project is to <br/>train students in the newly developed methodologies, leading to the <br/>dissemination of knowledge gained through this research and the creation <br/>of a body of technicians and experts to apply it.<br/>"
"0405038","Time Series Analysis and Applications","DMS","STATISTICS","07/01/2004","10/26/2005","David Stoffer","PA","University of Pittsburgh","Continuing Grant","Gabor Szekely","06/30/2008","$444,935.00","Ori Rosen","stoffer@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","0000, OTHR","$0.00","The focus of this research is on a number of topics relating to applications of time series analysis.  The motivation for the first project is the analysis of DNA sequences.  One important task is to translate the information stored in the protein-coding sequences (CDS) of the DNA.  A common problem in analyzing long DNA sequence data is in identifying CDS that are dispersed throughout the sequence and separated by regions of noncoding.  It is well known that DNA sequences are heterogeneous, and even within short subsequences of DNA, one encounters local behavior.  In this proposal, the interest is in extending the spectral envelope methodology to capture the local behavior of such sequences.  To address this problem of local behavior in categorical-valued time series, local spectral envelope with estimation via mixtures of smoothing splines will be explored.  It is the hope that this methodology will help emphasize any periodic feature that exists in a categorical sequence of virtually any length in a quick and automated fashion.  Projects such as the human genome project have produced large amounts of data.  It is believed the methods will prove to be useful in the analysis of the vast quantities of data being produced by various genome projects.  Another primary objective of this proposal is to explore spatio-temporal modeling by developing models similar to the STARMAX model.  The goal is to develop a general methodology, but the research will be governed by obtaining solutions to difficult problems in biosurveillance, such as monitoring bioterrorism, and in medicine, such as the analysis of concurrent EEG-fMRI recordings.  Although data is being collected in real-time by various organizations such as the CDC, data analytic tools that support both temporal and spatial data analysis and visualization are sorely lacking.  At the present time, most analysis is accomplished by dropping (either by ignoring or by aggregating) either time or space.  EEG has been a key tool in the study of the brain for decades.  However, despite its multiple clinical and research uses, such as in epilepsy, little is yet known about the underlying generators of EEG activity in humans.  Functional MRI (fMRI) recorded in concert with EEG may provide a method for localizing and identifying these sources.  By using the EEG signal as a reference for fMRI maps, concurrent EEG-fMRI opens a new avenue for investigating specific brain function.  <br/><br/>The focus of this research is on a number of topics relating to applications of data collected in time, in space, or in sequence.  The motivation for the first project is the analysis of DNA sequences.  One important task is to translate the information stored in the protein-coding sequences of the DNA.  Projects such as the human genome project have produced large amounts of data.  It is believed the methods will prove to be useful in the analysis of the vast quantities of data being produced by various genome projects.  Another primary objective of this proposal is to explore spatio-temporal modeling by developing new statistical models.  The goal is to develop a general methodology, but the research will be governed by obtaining solutions to difficult problems in biosurveillance, such as monitoring bioterrorism, and in medicine, such as the analysis of concurrent EEG-fMRI recordings.  Although data is being collected in real-time by various organizations such as the CDC, data analytic tools that support both temporal and spatial data analysis and visualization are sorely lacking.  At the present time, most analysis is accomplished by dropping (either by ignoring or by aggregating) either time or space.  EEG has been a key tool in the study of the brain for decades.  However, despite its multiple clinical and research uses, such as in epilepsy, little is yet known about the underlying generators of EEG activity in humans.  Functional MRI (fMRI) recorded in concert with EEG may provide a method for localizing and identifying these sources.  By using the EEG signal as a reference for fMRI maps, concurrent EEG-fMRI opens a new avenue for investigating specific brain function.  <br/>"
"0405914","Collaborative Research:  Tracking Statistics and Inference for Indirect Measurements","DMS","STATISTICS","07/01/2004","03/07/2007","Daniel Jeske","CA","University of California-Riverside","Standard Grant","Gabor Szekely","06/30/2008","$104,989.00","","daniel.jeske@ucr.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","MPS","1269","0000, OTHR","$0.00","Classification and clustering are two fundamental data mining tools for discovering useful patterns.  Given that there is generally no perfect classification or clustering procedures, it is crucial to correct or account for the classification error in any subsequent inference which is derived from the classification outcomes.  The research in this proposal is developing inference procedures that incorporate the error associated with classification rates, and consequently is improving the robustness of decision-making processes that are based on classification and clustering mechanisms.  A particular example is the development of tracking statistics in process control applications that correct for errors in defect classifications.  Another example is the development of misclassification rate estimates without the usual assumption that a gold standard exists.  The research is enabling a wider use of data mining and knowledge discovery techniques by removing stringent requirements on data quality levels.  <br/><br/>Many decision-making processes use inputs that are the result of statistical analyses of grouping subjects according to their similarities.  Classification and clustering techniques are two important such grouping methods. The validity of the decision-making rests on the accuracy of the grouping outcomes.  Nowadays, classification and clustering algorithms make use of large databases that are often low in data quality, and consequently introduce biases in the classification and clustering outcomes.  The goal of this research is to develop inference methodologies that adjust for inherent noise in the outputs of classification and clustering algorithms, and thereby improve the accuracy of subsequent decision-making. The results of this research should benefit many areas of applications, which include the analysis of micro-array gene expression data, machine learning, information retrieval, risk analysis, computer-aided diagnostics, and pattern recognition.  <br/>"
"0439641","Conference on New Development of Statistical Analysis in Wildlife, Fisheries, and Ecological Research","DMS","Population & Community Ecology, STATISTICS","09/01/2004","08/20/2004","Zhuoqiong He","MO","University of Missouri-Columbia","Standard Grant","Grace Yang","08/31/2005","$20,000.00","Lori Thombs, Nancy Flournoy, Christopher Wikle, Dongchu Sun","chong@stat.missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1128, 1269","0000, OTHR","$0.00","As technology has advanced dramatically in recent years, both<br/>statistical research and wildlife, fisheries, and ecological research<br/>have moved into a new era. However, the use of advanced statistical<br/>methods in wildlife, fisheries, and ecological research is in its<br/>infancy. There remain many fundamental statistical and computational<br/>issues. There is a substantial interest among biological<br/>scientists and statisticians in the development of modern statistical<br/>methods with which to answer the relevant scientific questions.<br/>In the recent years, many new statistical methods (such as<br/>spatial-temporal models, multiple testing, stochastic search variable<br/>selection, Bayesian model averaging, Bayesian hierarchical models,<br/>latent variable methods, and Markov chain Monte Carlo methods, to name<br/>a few) are developed to deal with such questions.<br/><br/>The investigators request funding to host a Conference on<br/>New Development of Statistical Analysis in Wildlife, Fisheries,<br/>and Ecological Research at the University of Missouri.<br/>The goal of the conference is to advance the understanding of<br/>statistical problems in wildlife, fisheries, and ecology. In<br/>particular, the goal is to identify future research needs and to<br/>encourage and facilitate collaboration among biological and<br/>statistical scientists and to pursue the interdisciplinary research<br/>programs. The broader impact of this conference will be manifested<br/>through fostering interaction between junior and senior researchers<br/>and promoting communication between researchers from government<br/>and acedamia on research problems, methodology, and theory.<br/><br/>"
"0405675","Collaborative Research:     Bayesian ANOVA for Microarrays","DMS","STATISTICS","08/15/2004","07/30/2007","Hemant Ishwaran","OH","Cleveland Clinic Foundation","Standard Grant","Gabor Szekely","07/31/2008","$118,120.00","","HIshwaran@med.miami.edu","9500 EUCLID AVE","CLEVELAND","OH","441950001","2164456440","MPS","1269","0000, OTHR","$0.00","DNA microarrays can provide insight into genetic changes occurring<br/>during stagewise progression of diseases like cancer.  Accurate<br/>identification of these changes has significant therapeutic and<br/>diagnostic implications.  Statistical analysis of such data is however<br/>challenging due to the sheer volume of information.  With new<br/>microarray technology it is possible to measure expressions on nearly<br/>60,000 transcripts for each sample of tissue analyzed.  To properly<br/>understand the evolution of a progressive disease, expression values<br/>are collected over all possible biological stages, thus the number of<br/>parameters in such problems can be in the hundreds of thousands, or<br/>even millions.  The high dimensionality presents theoretical problems<br/>to standard ANOVA-based extensions of two-sample Z-tests, a popular<br/>method for detecting differentially expressed genes in two groups.<br/>Additionally, standard approaches that focus on controlling false<br/>detection rates primarily apply to simpler experimental designs;<br/>moreover these approaches tend to be conservative and are expected to<br/>be worse in multigroup settings.  This work introduces a new<br/>methodology called Bayesian ANOVA for Microarrays (BAM) for reliably<br/>detecting differentially expressed genes in complex experimental<br/>settings.  The method rests on a high dimensional variable selection<br/>method that exploits a rescaled spike and slab hierarchical model.<br/>BAM is shown to be risk optimal in terms of the total number of<br/>misclassified genes.  The exact mechanisms for this risk optimality<br/>are theoretically delineated as a selective shrinkage effect. Theory<br/>guides development of graphical devices for adaptive optimal gene<br/>selection.  A large multistage colon cancer microarray repository<br/>collected at the Ireland Cancer Center of Case Western Reserve<br/>University serves as a testbed for the methods.  In parallel to this<br/>is the development of JAVA-based software for implementing BAM.<br/>Software uses a menu driven GUI and includes a minimal number of<br/>user-specified tuning parameters, thus making it user friendly for use<br/>by other molecular biology laboratories.<br/><br/>DNA microarrays allow for high throughput analysis of potential<br/>genetic determinants of diseases like cancer.  It is now typical to<br/>have expression on nearly 60,000 transcripts for each sample of tissue<br/>analyzed.  This information can potentially provide information about<br/>which genes are involved in stagewise development of cancer as well as<br/>indicate novel therapeutic and diagnostic targets.  However,<br/>statistical inferences to identify interesting genes is challenging<br/>due to the large number of statistical tests that are run.  Standard<br/>approaches employ ANOVA test statistics and are prone to high false<br/>detections.  False detection rate control methods tend to be overly<br/>conservative and do not extend naturally to more complex multistage<br/>experimental designs.  This work introduces a new methodology called<br/>Bayesian ANOVA for Microarrays (BAM) which reliably detects<br/>differentially expressed genes in multigroup experimental design<br/>settings.  The method employs a special hierarchical model that<br/>imparts an oracle like behaviour for gene selection --- that is,<br/>ultimately, only those truly differentially expressing genes are<br/>selected.  The reasons for this behaviour are theoretically delineated<br/>in this research, and the theory guides the development of novel<br/>graphical devices for adaptively optimal gene selection in real<br/>microarray datasets.  A large multistage colon cancer microarray<br/>repository collected at the Ireland Cancer Center of Case Western<br/>Reserve University serves as a testbed for the methods and also<br/>provides a tremendous opportunity to understand the colon cancer<br/>disease process, a topic which is of great medical importance.  While<br/>colon cancer has a well defined evolution defined by clinical stage,<br/>very little is known about its molecular evolution.  In parallel to<br/>this, is the development of JAVA-based software using a menu driven<br/>GUI having a minimal number of user-specified tuning parameters, thus<br/>making it feasible to port the software to molecular biology<br/>laboratories for active use in analysis of other disease processes and<br/>potentially other high throughput sources of data.<br/>"
"0354223","Collaborative Research: FRG: New development on nonparametric modeling and inferences with biological applications","DMS","STATISTICS","06/01/2004","04/30/2004","Jianqing Fan","NJ","Princeton University","Standard Grant","Gabor Szekely","05/31/2008","$552,000.00","","jqfan@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","0000, 1616, OTHR","$0.00","The objectives of this proposal are to develop new and widely applicable<br/>semiparametric and nonparametric approaches to solve challenging<br/>statistical problems from computational biology. Frontiers of biological<br/>research such as normalization and analysis of microarray and proteomic<br/>data, functional connectivity of brains, covariate effects on longitudinal<br/>and functional data, and prediction of individual response trajectories<br/>have generated a number of outstanding statistical challenges.  Several<br/>new semiparametric and nonparametric models have been introduced to<br/>address the imminent needs for the aforementioned biological applications.  <br/>A number of innovative methods on nonparametric estimation and inferences<br/>are proposed. Their properties will be investigated via both asymptotic<br/>theory and simulations.  Their efficacy in biological applications will be<br/>carefully scrutinized.  This proposal not only introduces a number of<br/>innovative techniques and useful statistical models, but also provides<br/>various new insights into nonparametric inferences.  The research findings<br/>will have significant impact on the future development of statistical<br/>theories and methodologies.<br/><br/>Technological invention and information advancement have revolutionized<br/>scientific research and technological development. Quantitative methods<br/>have been widely employed in scientific communities.  They have played<br/>pivotal roles in knowledge discovery.  This proposal intends to develop<br/>new nonparametric techniques and theories that arise from frontiers of<br/>scientific development. In particular, the investigators will develop<br/>models and cutting-edge technologies for the analysis of microarray,<br/>proteomic, longitudinal and functional data and fMRI brain images. Common<br/>characteristics of these data are their complexity and size, where<br/>nonparametric techniques are particularly powerful and under developed.<br/>The proposed techniques address imminent needs in computational aspects of<br/>molecular biology, neurology, and epidemiology. In addition, they will<br/>integrate new mathematical developments with those in science and<br/>engineering, which empowers new knowledge discoveries and prudent policy<br/>making.  Undergraduate and graduate students, postdoctors and<br/>underrepresented groups will be trained as results of this research.<br/><br/><br/><br/><br/>"
"0432446","Travel Support for the 55th Session of the International Statistical Institute","DMS","STATISTICS","09/01/2004","08/25/2004","William Smith","VA","American Statistical Association","Standard Grant","Grace Yang","08/31/2005","$12,000.00","","bill@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","0000, OTHR","$0.00","Using NSF support, the American Statistical Association (ASA) awards a travel grant to at least 11 United States participants to attend the 55th Session of the International Statistical Institute (ISI) in Sydney, Australia, from April 5 - 12, 2005.  The ISI meeting includes the meetings of the Bernoulli Society, International Association for Official Statistics (IAOS), International Association for Statistical Computing (IASC), International Association of Survey Statisticians (IASS), and the International Association for Statistical Education (IASE).  <br/><br/>The travel grant program administered by ASA is for participation in an umbrella meeting with sessions of interest for thousands of statisticians.  The grant provides partial support to defray transportation costs for approximately eleven (11) individuals selected from institutions and non-profit associations.   Participants will be notified of the availability of the travel grant through Amstat News, a membership publication with over 19,000 circulation and on the ASA Home Page on the Internet.  Also, notices will be sent to university and college statistics and mathematics departments to encourage younger and underrepresented statisticians to apply for travel support.  An ASA committee will do the review and selection of the applications.  Special consideration will be given to statisticians who have recently received their doctorates and to women and minorities in order to broaden research, educational and networking opportunities.  <br/><br/><br/><br/>"
"0305996","Statistical Research in Drug Discovery and Development","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","02/15/2004","02/06/2004","C. F. Jeff Wu","GA","Georgia Tech Research Corporation","Standard Grant","Grace Yang","01/31/2008","$398,828.00","David Stock","jeffwu@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1253, 1269","0000, 1504, OTHR","$0.00","Three broad issues in drug discovery and development are studied. For target identification, use of microarrays for gene expression has become a standard practice. To test the significance of gene expression for thousands of genes, the issue of multiplicity in testing is central. A new graphical procedure is proposed. For drug discovery, the research focuses on high throughput screening (HTS). A central issue in HTS is to be able to identify as many potent drugs as possible and to achieve this within an economic timeframe. To increase the ""hits"", both false positive and false negative errors need to be accurately estimated and the cutoff point be chosen optimally. The major research effort focuses on the accurate estimation of false positive and false negative errors, determination of optimal cutoff points, and estimation strategy.  The results will be pivotal to the planning of validation studies that are used before the screen is put into production. For drug development, it is proposed to apply modern techniques in design and analysis of experiments to pharmaceutical sciences (formulations and stability) and process R&D (chemical and biological scale-up). Two new techniques are considered: estimation of interactions in experiments with complex aliasing, and robust parameter design for process improvement. Since these techniques were developed in the context of manufacturing and hi-tech industries, new features in the pharmaceutical applications should lead to the development of new methods. The research is a jointly effort by the research group of Jeff Wu (PI) at Georgia Institute of Technology and the nonclinical biostatistics and genetics groups headed by David Stock and Kim Zerba at Bristol-Myers Squibb. <br/><br/>Statistical tools have been widely used in drug discovery and development in the pharmaceutical industry. With the increasing competition in the industry and the explosion of disease targets, it is becoming increasingly important to have an efficient system for discovering new compounds and developing them into drugs for clinical trials and scale-up production. There has been a lot of collaborative research between academia and industries on clinical trials. Much less collaboration has been done on preclinical research like drug discovery and development. Successful implementation of this project can serve as a role model for this collaboration. It can have a major societal impact in accelerating the identification of disease target and discovery of compounds for blockbuster drugs, resulting in savings of lives and health care costs. The work should lead to new advances in theory and methodology and the research findings will be presented in professional meetings and published in trade journals. Because of the novelty of applications and the scientific relevance, the methodology and theory developed in this project will be of a broad and generic nature. They will benefit the industrial partner as well as the industry in general. Several Ph.D. students will participate in the project, splitting their time between university and industrial labs. The project will provide a new opportunity for graduate students to be exposed to cutting-edge research in drug discovery and development. It can enrich their educational experience and broaden the prospects for their careers. <br/>"
"0355386","New Researchers Conferences - 2004 and 2005","DMS","STATISTICS","03/15/2004","03/09/2004","Herbert Lee","CA","University of California-Santa Cruz","Standard Grant","Robert J. Serfling","02/28/2005","$20,000.00","","herbie@ams.ucsc.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1269","0000, OTHR","$0.00","Abstract:<br/><br/>The New Researchers Conference is a meeting for new researchers in statistics and probability, where ""new"" is defined as doctoral students within one year of completion of the Ph.D. or post-doctoral statisticians and probabilists up to five years since the completion of their degrees.  The primary purpose of this conference is to foster the professional development of new researchers by providing a much needed venue for interaction among new researchers by allowing these junior researchers to exchange ideas and initiate contacts amongst themselves as well as with the invited senior participants in the context of a smaller meeting, rather than the larger meetings more common in this field.  All participants present their research in a short talk or poster session.  The program also includes several talks by senior researchers and panel discussions with representatives from several statistics journals and representtatives from funding agencies.<br/><br/>The New Researchers Conference is a meeting for recent Ph. D. recipients in statistics and probability (or those within one year of completion).  This conference aims to facilitate the development of new researchers to help them become productive scientific <br/>researchers and educators.  The conference provides a controlled environment for junior researchers to interact by exchanging ideas and initiating contacts amongst themselves.  All participants must present their research during the meeting.  Also included in the program are several talks by senior speakers on topics of professional development and informational panels consisting of representatives from professional journals and funding agencies.<br/>"
"0406361","Analysis of High Dimensional Data Using Subspace Clustering","DMS","STATISTICS, INFORMATION & KNOWLEDGE MANAGE, INFORMATION INTEGRATION","09/15/2004","05/23/2006","Andrew Nobel","NC","University of North Carolina at Chapel Hill","Continuing Grant","Gabor Szekely","08/31/2008","$252,689.00","Wei Wang","nobel@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269, 6855, 7602","0000, OTHR","$0.00","An important and visible trend in empirical science today is<br/>the increasing prevalence of large data sets that contain from thousands to<br/>hundreds of millions of measurements.  Examples include data sets arising<br/>from high throughput measurement techniques such as gene expression arrays,<br/>proteomics and computer network monitoring.  While the analysis of large<br/>data sets is important to scientists, it is often outside the realm of<br/>classical statistical methods, and frequently presents new conceptual and<br/>computational challenges.  The funded research has two principle parts.  In<br/>the first, the investigators are studying the application of a relatively<br/>new development in the field of Data Mining, known as subspace clustering,<br/>to the exploratory statistical analysis of high dimensional data.  In the<br/>second, the investigators are applying ideas from Statistics and Probability<br/>to the development of new subspace clustering methods, and to rigorous<br/>mathematical analyses of their results.  Research is being carried out in<br/>the context of ongoing collaborations with biological scientists, and is<br/>being incorporated in software that will be used by the collaborating<br/>scientists to identify and assess significant sample-variable<br/>associations in a variety of large data sets.<br/><br/><br/>An important and visible trend in empirical science today <br/>is the increasing prominence of large data sets that contain from<br/>thousands to hundreds of millions of<br/>measurements.  Examples include data sets arising from high throughput<br/>measurement techniques such as gene expression arrays, proteomics and<br/>computer network monitoring.  Whereas small to moderate data sets typically<br/>have more samples than measurements, in large data sets it is common to have<br/>more measurements than samples, so-called ``high dimension and low sample<br/>size''.  The investigators are studying the application of data mining<br/>methods known as subspace clustering to the exploratory analysis of high<br/>dimensional data.  Subspace clustering identifies distinguished sample<br/>variable interactions (submatrices) in a given data matrix.  Unlike standard<br/>two-way clustering, the sample and variable sets for different clusters can<br/>overlap.  The investigators are investigating the noise sensitivity of<br/>existing subspace clustering algorithms, and are developing and<br/>implementing new subspace clustering methods for average based selection<br/>criteria that are better suited for applications where noise is present.<br/>As an application of these methods, they are using subspace clusters to<br/>classify high dimensional data.  Using a variety of tools from combinatorial<br/>probability, the investigators are also developing a rigorous theoretical<br/>framework in which multiple testing and the statistical significance of<br/>subspace clusters can be addressed.  The funded research is being carried out<br/>in the context of ongoing collaborations with biologists and computer<br/>scientists.<br/><br/><br/>"
"0402824","Research in Statistics","DMS","STATISTICS","10/01/2004","06/20/2008","Jiming Jiang","CA","University of California-Davis","Standard Grant","Gabor Szekely","09/30/2009","$120,148.00","","jiang@wald.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","0000, OTHR","$0.00","In this project, the principle investigator (PI) proposes<br/>research on four important topics aimed to develop methods<br/>of statistical inference and model diagnostics in cases of<br/>correlated observations. These topics are: (i) partially<br/>observed information and inference about non-Gaussian<br/>linear mixed models; (ii) iterative weighted least squares<br/>procedures for analysis of longitudinal data; (iii) A test<br/>of global maximum for dependent observations; and (iv)<br/>generalized linear mixed model diagnostics. Linear mixed<br/>models are widely used in practice for correlated observations.<br/>A typical assumption regarding these models is that the<br/>observations are normally distributed. However, the normality<br/>assumption is likely to be violated in practice. It is known that<br/>normality based methods such as REML still produce consistent<br/>estimators even if normality fails. However, the estimation<br/>of standard errors of these estimators is complicated, because<br/>for non-Gaussian data the asymptotic covariance matrix of the<br/>Gaussian estimator involves additional unknown parameters.<br/>Project (i) aims to completely solve this long-standing problem<br/>of practical interest. Furthermore, project (ii) proposes an<br/>iterative weighted least squares procedure for computing<br/>efficient estimators of the regression coefficients in linear<br/>models for longitudinal data analysis and studies its properties.<br/>Project (iii) aims to extend a method developed in the i.i.d.<br/>case for checking whether a root to the likelihood equation<br/>corresponds to the global maximum of the likelihood function.<br/>Project (iv) develops goodness-of-fit tests and methods of<br/>informal model checking for generalized linear mixed models.<br/><br/>Correlated responses are often encountered in practice. For<br/>example, in medical studies repeated measures are often collected<br/>from the same individuals over time. It would be reasonable to<br/>assume that correlations exist among the observations from the<br/>same individual. Linear and generalized linear mixed models are<br/>two important classes of statistical models widely used in cases<br/>of correlated observations. For linear mixed models methods of<br/>inference have been developed, but mostly under the assumption<br/>that the observations are normal (or Gaussian). However, the<br/>normality assumption is likely to be violated in practice. The<br/>PI aims to develop a powerful method for inference about<br/>non-Gaussian linear mixed models. For generalized linear mixed<br/>models, the PI proposes to develop methods of model checking<br/>that fills an important gap in the applications of these models.<br/>The methods developed are likely to have impact in other fields<br/>of statistics as well as in fields such as biomedical research,<br/>genetics, biology, economics, education and social science.<br/>"
"0406430","Functional Analysis of Sparse Longitudinal Data","DMS","STATISTICS","07/01/2004","08/18/2006","Jane-Ling Wang","CA","University of California-Davis","Continuing Grant","Gabor Szekely","06/30/2009","$374,522.00","","janelwang@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","0000, OTHR","$0.00","Project Abstract<br/>proposal: 0406430<br/>PI: Jane-Ling Wang<br/><br/>   Functional Regression Analysis of Sparse Longitudinal Data<br/><br/>Recent advances in modern technology have facilitated the collection of<br/>repeated measurements over a period of time on the same subject. Such data<br/>are common in nearly all fields including the biological, medical, neural,<br/>physical and social sciences, but are termed differently, with<br/>""longitudinal data"" being the preferred term in health and social sciences<br/>and ""functional data"" being the preferred term in engineering and physical<br/>sciences. Statistical approaches to analyze such data are also<br/>intrinsically different in the longitudinal and functional data research<br/>communities. Parametric approaches such as GEE and GLMM are<br/>predominantly used methods to analyze longitudinal data, while<br/>nonparametric approaches play the analogous role for functional<br/>data.  Due to the limitations of each approach, semiparametric models<br/>combining longitudinal and functional data analysis methods emerge as a<br/>promising alternative, which can bring out and combine the best aspects of<br/>the two approaches. Longitudinal or functional data are intrinsically<br/>complex owing to irregularity of observational times, dependence of<br/>observations within subjects, sparsity and size of the data.  They pose<br/>challenges both on the computational and theoretical fronts.  This<br/>proposal aims at bringing together methodology from various areas in<br/>statistics, including nonparametric smoothing, multivariate statistics,<br/>random and mixed effects models, dimension reduction and robustness, to<br/>address several challenging issues and to provide flexible modeling and<br/>efficient implementation for longitudinal/functional data.  The proposed<br/>methods range from extension of traditional linear models to new<br/>semiparametric and nonparametric models, and focus particularly on<br/>handling sparse longitudinal data with or without measurement error.  A<br/>new version of functional principal components (PCA) analysis was<br/>developed recently by the PI and collaborators, where the functional<br/>principal component scores are framed as conditional expectations. This<br/>extends the applicability of functional PCA to typical situations in<br/>longitudinal data analysis, where only few repeated measurements are<br/>available per subject. This approach is known as Principal Components<br/>Analysis through Conditional Expectation (PACE) for longitudinal data.<br/>With PACE serving as the backbone, the proposal includes three projects:<br/>(1) Modeling covariate effects, (2) Semi-parametric dimension reduction<br/>approaches, and (3) Robust covariance estimation and functional PCA.<br/><br/>In addition to new methodology, statistical theory will be a major focus<br/>to establish formal inference procedures. So far, theoretical results<br/>for functional data are scattered and this proposal aims to fill the gaps.<br/>The proposed research is motivated by ongoing interdisciplinary research<br/>collaborations of the PI with biologists and physicians.  The new<br/>approaches are applied to data generated from these ongoing and future<br/>collaborations.  The proposed research helps to better understand the<br/>relationship between reproductive activity and longevity and will<br/>contribute to the growing fields of aging research and biodemography. As<br/>the procedures developed will be applicable to general longitudinal or<br/>functional data from other disciplines, they will provide much needed<br/>modern statistical tools to analyze such data, which in turn will<br/>facilitate the advancement of many scientific fields.  Moreover, with the<br/>fast rising trend towards the collection of such large and complex data<br/>sets, there is a shortage of Ph.D. statisticians trained to handle such<br/>data. The research assistantship provides a training<br/>opportunity to address this need. The PI is engaged in<br/>undergraduate research training, and continues this activity in<br/>addition to the dissemination of the new research findings through<br/>teaching, training, and the web.<br/>"
"0405243","Localized Cross Spectral Analysis and Pattern Recognition Methods for Non-Stationary Signals","DMS","STATISTICS","07/01/2004","05/11/2007","Hernando Ombao","IL","University of Illinois at Urbana-Champaign","Standard Grant","Grace Yang","07/31/2008","$271,029.00","","hombao@uci.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","0000, OTHR","$0.00","Abstract<br/>PI: Hernando Ombao<br/>proposal: 0405243<br/><br/>The PI develops a systematic body of methods and models for analyzing massive non-stationary signals.  The basic tool is the SLEX library, a collection of bases, each basis consisting of orthogonal localized Fourier waveforms.  The SLEX methods give results that are easy to interpret because they are time-dependent analogues of the Fourier spectral analysis of stationary signals.  Moreover, the SLEX methods use computationally efficient algorithms, thus, they will be capable of handling massive data sets.  The PI develops a family of multivariate models for non-stationary signals recorded from several subjects.  The model explicitly takes into account the time-evolving inter-connection between the components of the multivariate signals.  In addition, the PI develops an automatic procedure for decomposing the high dimensional multivariate signals into SLEX components using the eigenvalue-eigenvector decomposition of the time-varying SLEX spectral matrix.  The SLEX components are non-stationary and have zero-coherency.  Thus, they contain non-redundant information on the time-varying cross spectra, which will be used as the primary feature for model selection as well as for discrimination and classification.  Finally, the PI develops an automatic and systematic method for extracting time-varying higher order spectral features of non-stationary signals.  The PI develops the SLEX higher order spectra, which can account for the time-evolutionary interaction between different frequency components in the signal.  In this proposal, the SLEX are the foundation on which the body of coherent and systematic methods for non-stationary signals is built.<br/><br/>This proposal is motivated by the statistical problems that confront the neuroscience community.  Major advances in technology now enable neuroscientists to collect complex data sets for investigating the more intricate functioning of the human brain.  There is currently a major interest to study how different brain areas interact with each other in response to a mental stimulus.  There is also a widespread interest in exploring the association between impairment in brain connectivity and various mental disorders.  To study brain connectivity, various types of signals (EEGs, MEGs, fMRI) are recorded.  Analyzing brain signals is quite challenging because the brain is a complex organ.  Moreover, the signals collected are both non-stationary and massive.  The SLEX methods that the PI develops in this proposal address these issues.  The SLEX methods are able to capture the local temporal features of the signals.  Moreover, the methods are able to handle massive data sets, because they use computationally efficient algorithms.  As part of the educational component of this proposal, the PI works closely with graduate and undergraduate students in this research undertaking.<br/>"
"0403443","Statistical Inference Based on Data Tilting","DMS","STATISTICS","06/01/2004","05/12/2004","Liang Peng","GA","Georgia Tech Research Corporation","Standard Grant","Grace Yang","05/31/2007","$85,578.00","","lpeng@gsu.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","0000, OTHR","$0.00","The investigator studies three subjects: extreme value<br/>theory, ARCH/GARCH models, and nonparametric regression with censored<br/>data. The particular issues examined in this proposal include:<br/>1) constructing confidence intervals for high quantiles<br/>in risk management; 2) constructing confidence intervals<br/>for the difference of two means with heavy tails; 3)<br/>constructing confidence intervals for parameters in ARCH/GARCH<br/>models; 4) constructing a confidence interval for the maximal<br/>moment exponent of a GARCH (1,1) sequence; 5) forecasting the<br/>1-step ahead conditional Value-at-Risk based on GARCH models;<br/>6) applying the data tilting method to obtain confidence<br/>intervals for the tail index of a double autoregressive model;<br/>and 7) applying the data tilting method to obtain confidence<br/>intervals for a conditional survival function with censored data.<br/>The proposed activity described above involves<br/>novel applications of data tilting methods. The research<br/>approach is a combination of theoretical asymptotic analysis,<br/>Monte Carlo simulation and real data analysis.<br/><br/>The problems studied in this project arise from real <br/>applications in various fields including<br/>meteorology, hydrology,  insurance, and<br/>finance.  Examples are: in the design of electrical<br/>distribution systems and buildings, the extremes of <br/>wind pressure loading must be accounted for;<br/>insurers who underwrite the financial risk associated with<br/> natural risks like floods, storms and earthquakes must <br/>have good estimates of the size and impact of extreme events in<br/>order to set their premiums at a profitable level.<br/>This project studies the following issues: modeling extreme<br/>events; predicting Value-at-Risk in  risk<br/>management; estimating frontier functions for comparing the <br/>performance of different firms; estimating parameters<br/>of volatility models in financial time series; and estimating the<br/>conditional life time in assessing the influence of risk factors<br/>on survival.  Progress in this project can enhance<br/>the interaction among several areas in statistical science, including<br/>extreme value theory, nonparametric smoothing, time series analysis,<br/>and survival analysis.  The new methods to be developed<br/>can be applied to financial time series, sea level prediction,<br/>internet traffic data, medical data, to name a few. The<br/>theoretical asymptotic results to be developed are expected to have<br/>broader applications as well.<br/>"
"0353941","Collaborative Research: FRG: New development on nonparametric modeling and inferences with biological applications","DMS","STATISTICS","06/01/2004","04/30/2004","Chunming Zhang","WI","University of Wisconsin-Madison","Standard Grant","Gabor Szekely","05/31/2008","$216,000.00","","cmzhang@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","0000, 1616, OTHR","$0.00","The objectives of this proposal are to develop new and widely applicable<br/>semiparametric and nonparametric approaches to solve challenging<br/>statistical problems from computational biology. Frontiers of biological<br/>research such as normalization and analysis of microarray and proteomic<br/>data, functional connectivity of brains, covariate effects on longitudinal<br/>and functional data, and prediction of individual response trajectories<br/>have generated a number of outstanding statistical challenges.  Several<br/>new semiparametric and nonparametric models have been introduced to<br/>address the imminent needs for the aforementioned biological applications.  <br/>A number of innovative methods on nonparametric estimation and inferences<br/>are proposed. Their properties will be investigated via both asymptotic<br/>theory and simulations.  Their efficacy in biological applications will be<br/>carefully scrutinized.  This proposal not only introduces a number of<br/>innovative techniques and useful statistical models, but also provides<br/>various new insights into nonparametric inferences.  The research findings<br/>will have significant impact on the future development of statistical<br/>theories and methodologies.<br/><br/><br/>Technological invention and information advancement have revolutionized<br/>scientific research and technological development. Quantitative methods<br/>have been widely employed in scientific communities.  They have played<br/>pivotal roles in knowledge discovery.  This proposal intends to develop<br/>new nonparametric techniques and theories that arise from frontiers of<br/>scientific development. In particular, the investigators will develop<br/>models and cutting-edge technologies for the analysis of microarray,<br/>proteomic, longitudinal and functional data and fMRI brain images. Common<br/>characteristics of these data are their complexity and size, where<br/>nonparametric techniques are particularly powerful and under developed.<br/>The proposed techniques address imminent needs in computational aspects of<br/>molecular biology, neurology, and epidemiology. In addition, they will<br/>integrate new mathematical developments with those in science and<br/>engineering, which empowers new knowledge discoveries and prudent policy<br/>making.  Undergraduate and graduate students, postdoctors and<br/>underrepresented groups will be trained as results of this research.<br/><br/><br/><br/>"
"0402470","Tree-Structured Methods for Prediction and Data Visualization","DMS","STATISTICS","06/01/2004","05/03/2004","Wei-Yin Loh","WI","University of Wisconsin-Madison","Standard Grant","Grace Yang","01/31/2008","$240,490.00","","loh@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","0000, OTHR","$0.00","Though many recursive partitioning algorithms exist in the literature, most are unsuitable for model interpretation because they tend to select some types of predictor variables more frequently than others. As a result, such tree structures can yield misleading conclusions about the roles and relative importance of the predictor variables. The main thrust of the proposed research is to extend the investigator's GUIDE and QUEST strategies to regression and classification, respectively. This approach effectively solves the problem of selection bias and significantly reduces computation time. The computational savings make it feasible to build tree-structured models that are hitherto impractical to construct. A second objective is to use the methods to model unreplicated and fractionally replicated data from designed experiments. The hierarchical structure of tree-structured models and their variable selection ability make them attractive alternatives to traditional methods. A third objective is extension of the investigator's LOTUS algorithm to fit logistic regression trees to data with multinomial response variables. <br/><br/>Statistical models constructed from high-dimensional data are often difficult or unintuitive to interpret. This applies even to the simplest model, the multiple linear regression model, where interpretation of the parameter estimates is fraught with difficulties caused by nonlinearity, multicollinearity, and interactions in the data. Graphical visualization is perhaps the most effective way to interpret a model. But such techniques are inapplicable to more than two or three dimensions. The proposed research enables the application of visualization techniques to high-dimensional data by using a tree-structured method to partition the data space such that at most one, two, or three predictor variables are needed to model the data in each partition. The result is a graphical model whose broad features are representable by a tree structure and whose finer features are visualizable by two and three-dimensional graphical displays. <br/>"
"0405782","Estimation and Prediction in Spatial Statistics","DMS","STATISTICS","07/15/2004","07/15/2004","Hao Zhang","WA","Washington State University","Standard Grant","Grace Yang","06/30/2007","$89,999.00","","zhanghao@purdue.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","MPS","1269","0000, OTHR","$0.00","ABSTRACT<br/>PI: Hao Zhang <br/>proposal: DMS 0405782<br/><br/><br/>This research concerns two major objectives of geostatistics: estimation of spatial correlation and prediction of values at unsampled sites.  Specific problems to be studied can be categorized into three groups: (i) Infill asymptotics for Gaussian processes.  Recent advances in spatial statistics show that asymptotic results are useful for the analysis of spatial data.  This research establishes infill asymptotic properties of estimators of variogram parameters.  In addition, it also provides, through theoretical and numerical studies, some guidelines about which asymptotics to employ in a finite sample case because there are two distinct asymptotics: the increasing domain asymptotics and infill asymptotics.  Results are quite different under the two asymptotics.  (ii) Univariate model-based Geostatistics.  Spatial generalized linear mixed models (GLMM) are used in model-based geostatistics to model and predict spatial non-Gaussian variables.  This project studies consistency and asymptotic distributions of the maximum likelihood estimators of the parameters in the GLMM.  (iii) Estimation of multivariate covariogram and inferences in multivariate model-based geostatistics.  This project develops and implements explicit algorithms for estimating multivariate covariograms.  It also develops the multivariate spatial GLMM that is a powerful model when one or more spatial variable is non-Gaussian such as binomial counts.  Inferential methods are studied and implemented in S-Plus and R.<br/><br/>Geostatistical data arise in many fields including hydrology, ecology, agriculture, natural resource evaluation, environmental sciences and health studies.  In the midst of the wide applications there is a real need for theoretical work to understand the properties of estimation and prediction.  The results of this research for univariate goestatistics partially meet the need and are readily applicable to the analysis of geostatistical data.  There is also a great need to develop appropriate statistical models for multiple spatial variables.  For example, in environmental and health studies, multiple variables are often observed at different locations.  Due to spatial correlations, modeling these spatial variables is a challenging problem and is understudied.  This research develops methods and algorithms for modeling such multiple spatial variables.  Hence the research results have broad impacts on a variety of scientific disciplines.<br/><br/>"
"0304861","Probabilistic and Statistical Methods in Machine Learning","DMS","PROBABILITY, STATISTICS","01/01/2004","07/10/2003","Vladimir Koltchinskii","NM","University of New Mexico","Standard Grant","Dean Evasius","12/31/2006","$100,818.00","","vlad@math.gatech.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","MPS","1263, 1269","0000, 9150, OTHR","$0.00","The main focus of this research is to take further the theory of data-dependent bounds on generalization error of learning algorithms, especially, in the context of classification problems. One of the main features of modern classification techniques is that they take into account the distribution of the so called classification margins (large margin methods), the quantities that characterize the reliability of classification. However, the margins alone do not give a satisfactory explanation of the superb performance of these methods. To provide such an explanation one has to combine margin type parameters with complexities of the classification rules in rather sophisticated upper confidence bounds. The goal of the research is to use concentration inequalities and various tools from the theory of Gaussian, empirical and Rademacher processes to develop new, much more subtle and powerful bounds of this type, that would lead to a much better understanding of the performance of the existing large margin classification methods and suggest ways to develop statistically optimal large margin procedures. The research includes the study of limit theorems and inequalities for ratio type empirical processes; the study of localized complexities of function classes involved in learning algorithms and the development of new bounds on generalization performance in terms of individual complexities of combined classifiers; the investigation of convergence rates of the empirical margin distribution to the true margin distribution of classifiers in terms of localized and individual complexities; the study of convergence rates of learning algorithms and the development of adaptive classification algorithms with optimal convergence rates; and the investigation of spectral properties of random matrices that play important role in learning theory for kernel machines. <br/> Learning Theory is a rapidly growing area between Computer Science, Mathematics, and Statistics that deals with modeling the process of learning and generalization in both biological and artificial neural networks and other learning machines. The results of the research are likely to facilitate further development of boosting, kernel machines, and other learning techniques and lead to new probabilistic bounds and asymptotic results in the theory of empirical processes with potential applications to many problems in statistical learning theory and other areas of Statistics. Methods of statistical learning theory have been penetrating many important areas of applications ranging from biotechnology to computer security. One of the topics of the proposed research is to develop applications of large margin learning methods in the area of robust control, in particular, to the problem of congestion control in communication networks. The results of the research are likely to impact this and other areas of applications. <br/>"
"0349111","CAREER:    Default Bayesian Methods for Nonparametric Problems","DMS","STATISTICS","06/01/2004","01/11/2008","Subhashis Ghoshal","NC","North Carolina State University","Continuing Grant","Gabor Szekely","05/31/2010","$400,000.00","","subhashis_ghoshal@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","0000, 1045, OTHR","$0.00","DEFAULT BAYESIAN METHODS FOR NONPARAMETRIC PROBLEMS<br/><br/><br/>Statistical models for complex data often contain one or more<br/>infinite-dimensional parameters such as a probability density, a<br/>regression function, or the transition density of a Markov<br/>process. The rapid development of innovative Monte-Carlo schemes<br/>in the last decade makes it possible to compute Bayes<br/>procedures in these complex problems. However, because of the high<br/>dimensionality, it is seldom possible to completely elicit a prior<br/>subjectively from the available information. What is needed is a<br/>general strategy for constructing priors for infinite-dimensional<br/>parameters that incorporates available prior information, such as<br/>smoothness (differentiability) or shape (monotonicity, convexity,<br/>unimodality) of a regression function or density function.<br/>Ideally, the constructed prior should be tested in the given<br/>problem to avoid possible pitfalls in estimation. Large-sample<br/>properties such as consistency and rate of convergence are<br/>well-respected benchmark test criteria. In this research the<br/>investigator constructs prior distributions for select<br/>problems using a default approach, devises suitable algorithms for<br/>computation of the posterior, develops software for computation,<br/>investigates the large sample behavior of the resulting procedures,<br/>supports the theory and methods via simulation studies with<br/>moderately large samples, and applies the new methods to several<br/>interesting data sets. The research provides Bayesian<br/>methodologists with a catalog of priors with known performance<br/>properties, thereby facilitating the application of Bayes methods<br/>in other models with high-dimensional parameters.<br/><br/><br/>Modern statistical models for data in a wide variety of<br/>applications, such as data mining, image analysis, biometrics,<br/>biostatistics, bioinformatics, signal processing, and finance,<br/>often depend on high- or infinite-dimensional parameters such as <br/>survival distributions, probability densities, regression<br/>functions, transition densities of Markov chains, and so on. Successful<br/>analysis of such data presents challenges not found in the<br/>analysis of finite-parameter models, and requires the development<br/>of new statistical theory, methods and software. A non-subjective <br/>Bayesian method retains the advantages of the Bayesian paradigm <br/>without requiring a subjective prior elicitation. In this<br/>research the investigator develops the theory, methods, and<br/>computational algorithms for implementing default Bayesian analyses of<br/>complex statistical models depending on infinite-dimensional<br/>parameters. The research is disseminated through the teaching<br/>of advanced courses and via the usual scientific channels of<br/>publications and seminars. The research provides new<br/>data-analytic tools for solving problems arising in diverse <br/>fields. Useful priors with known performance are cataloged <br/>and user friendly software is developed for ready applications <br/>to diverse fields. Thus the research has a major impact on the <br/>conduct of science in a number of highly-relevant<br/>application areas."
"0405694","Constructing Optimal Factorial Designs for Multiple Groups of Factors: Theory, Methods and Applications","DMS","STATISTICS","07/15/2004","07/07/2004","Yu Michael Zhu","IN","Purdue University","Standard Grant","Grace Yang","06/30/2007","$77,000.00","","yuzhu@stat.purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","0000, OTHR","$0.00","Fractional factorial designs (FFDs) are among the most popularly used<br/>experimental plans in practice. Most existing theory and methods for<br/>FFDs assume that the factors involved in an experiment are symmetrical.<br/>In many applications, however, this assumption does not hold, because<br/>experiments can involve multiple groups (or types) of factors (MGFs).<br/>Different types of factors have different implications for design and<br/>analysis, therefore they need to be treated differently.  Three typical<br/>examples are Taguchi's robust parameter design experiments, Addelman's<br/>compromise plans and experiments with both qualitative and quantitative<br/>factors. This project is intended to develop general theory and methods<br/>for constructing optimal designs with MFGs. Based on the preliminary<br/>results on robust parameter design, various trade-off strategies will<br/>be generalized to designs with MFGs and their theoretical properties<br/>will be studied and characterized. Due to the presence<br/>of different types of factors, the aliasing properties of these designs<br/>are much complicated. The investigator will study the letter patterns<br/>and the coset patterns so as to propose proper criteria for the<br/>construction of optimal designs. The structure function approach developed<br/>by the investigator earlier will be further extended and used in this<br/>research. The theory and methods for constructing nonregular FFDs with<br/>MGFs will also be investigated and developed. Based on the theory and<br/>methods developed in this project, optimal designs with economical run<br/>size will be constructed and tabulated for experimenters in practice.<br/><br/>Statistical design and analysis of experiments are widely used in<br/>scientific investigation and industrial research and development. The<br/>study of experimental design is aimed at constructing optimal experimental<br/>plans that allow experimenters to collect data and discover knowledge in<br/>an economical and efficient way. This project is motivated by the<br/>application of experimental design methodology for quality improvement<br/>in manufacturing industry, especially the robust parameter design<br/>technology. An experiment in robust parameter design usually involves<br/>multiple groups (or types) of factors, which have different implications<br/>in design and analysis. Most existing design theory and<br/>methods assume the symmetry between factors, thus are not directly<br/>applicable for robust parameter design. In general, experiments can<br/>include multiple groups of factors (MGFs), which should be treated<br/>differently in order to generate optimal experimental plans. In this<br/>project, the investigator intends to develop general theory and methods<br/>for constructing optimal factorial designs for experiments with MGFs.<br/>The project consists of three major components. The first component is<br/>to investigate the combinatorial and aliasing properties of fractional<br/>factorial designs with MGFs; the second component is to propose various<br/>optimality criteria for the construction of optimal designs with MGFs;<br/>the third component is to theoretically characterize the optimal designs<br/>and tabulate them for experimenters in practice. The project will advance<br/>the theory and methodology of experimental design as well as enhance<br/>efficient data collection and knowledge discovery in scientific<br/>investigation, quality improvement and other applications.<br/>"
"0412039","Conference:    The Second Erich L. Lehmann Symposium -- Optimality; May 19-22, 2004; Houston, TX","DMS","STATISTICS","04/01/2004","02/24/2004","Javier Rojo","TX","William Marsh Rice University","Standard Grant","Robert J. Serfling","03/31/2005","$16,000.00","","jrojo@iu.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","0000, OTHR","$0.00","The goal of the four-day conference is to examine the role that Optimality can play, or should play, in modern statistics. Due to the advent of high throughput data collection technology and the parallel development of computing power to analyze such data, it often happens that statistical theory gives way to raw computing power. Although most of the new exciting statistical methodologies have provided tools to make headway in many important scientific problems, a need to generalize and systematize this knowledge is now quite evident.<br/>The Symposia will take place in Rice University and will bring together a group of experts to discuss cutting-edge research optimality ideas in the context of modern statistical methodologies. Although much progress has taken place in areas such as data visualization and data mining and knowledge discovery, the subjects are ripe for the development of an optimality paradigm that allows for objective comparisons of methodologies. This new paradigm, although still to be defined, is necessary to push the research frontiers in these important disciplines.<br/><br/><br/>The conference will showcase new developments by leading researchers in an environment conducive to the development of new human resources. The focus will be the development of theoretical ideas to provide strong underpinnings to the many successful efforts in computation intensive areas where statisticians have contributed. This award supports twelve young investigators and eight Ph.D. students to attend the conference. Women and minority young investigators and Ph.D. students will be actively recruited and encouraged to apply. Refereed papers will be published.<br/><br/><br/>"
"0405913","Nonparametric Variable Selection in Smoothing Spline ANOVA Models","DMS","APPLIED MATHEMATICS, STATISTICS","07/15/2004","07/01/2004","Hao Zhang","NC","North Carolina State University","Standard Grant","Gabor Szekely","06/30/2008","$124,936.00","","hzhang@math.arizona.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1266, 1269","0000, OTHR","$0.00","The investigator studies the variable selection problem in nonparametric smoothing and regression models. <br/>In particular, a class of new regularization methods is developed for simultaneous variable selection <br/>and model fitting in the smoothing spline ANOVA models. One such method is the ""cosso"", which applies a <br/>novel soft thresholding type operation to the functional components in a reproducing kernel Hilbert space. <br/>In Gaussian regression, the cosso selects the correct model structure with the probability tending to one <br/>under certain mild conditions. To handle complex heterogeneous datasets with various types of responses, the <br/>investigator further extends the new methods to more complicated statistical models, such as generalized <br/>regression models, support vector machines, and proportional hazards regression models. Theoretical properties <br/>of the estimators like model consistency and the rate of convergence are investigated. Efficient numerical <br/>algorithms and user-friendly software are developed for public use.<br/><br/>Variable selection helps to reduce the dimension of model building, to improve the model accuracy, and to <br/>better understand the underlying mechanism that generates data. This research is motivated by the lack of<br/>theoretical work in nonparametric variable selection and the limits of existing approaches. The investigator <br/>establishes a unified framework for simultaneous variable selection and model estimation in smoothing spline <br/>ANOVA models, and contributes new theories to related variational methods. This work broadens the traditional <br/>understanding of nonparametric smoothing approaches, and eventually will help to generate new methods in <br/>statistical inference. In practice, high dimensional large datasets produced in modern sciences such as in <br/>medicine and biology, often with tens or hundreds of variables, demand more sophisticated tools for dimension <br/>reduction and model estimation. The methodology developed in this work already has successful applications <br/>in some real problems, and it will potentially make a significant impact in various fields.<br/><br/>"
"0405953","Collaborative Research:  Highly Structured Models and Statistical Computation in High-Energy Astrophysics","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","08/15/2004","08/09/2004","Xiao-Li Meng","MA","Harvard University","Standard Grant","Grace Yang","07/31/2007","$249,762.00","","meng@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269, 7454","0000, 7303, OTHR","$0.00","Pricipal Investigators: David Van Dyk and Xiao-Li Meng<br/><br/>Institutions: UC Riverside and Harvard University<br/><br/>Collaborative Research: Highly Structured Models and Statistical <br/>Computation in High-Energy Astrophysics<br/><br/>Abstract<br/><br/>The California-Harvard Astrostatistics Collaboration aims to design<br/>and implement fully model-based methods of statistical inference to<br/>solve outstanding data analytic problems in high-energy<br/>astrophysics. The Collaboration's methods explicitly model the<br/>complexities of both astronomical sources and the data generation<br/>mechanisms inherent in new high-tech instruments and fully utilize the<br/>resulting highly structured models in learning about the underlying<br/>astronomical and physical processes. Using these models requires<br/>sophisticated scientific computation, advanced methods for statistical<br/>inference, and careful model checking procedures. The PIs of the<br/>Collaboration (van Dyk and Meng) both have substantial research<br/>experience in developing the methods that the Collaboration is<br/>extending, employing, and publicizing: inferential and efficient<br/>computational methods under highly-structured models that involve<br/>multiple levels of latent variables and incomplete data.  Such models<br/>are ideally suited to account for the many physical and instrumental<br/>filters that compose the data generation mechanism in high-energy<br/>astrophysics. The five consultants on the project (Chiang, Connors,<br/>Kashyap, Karovska, and Siemiginowska) all have expertise on the<br/>instrumentation and science of high-energy astrophysics, and, all have<br/>collaborated with statisticians in efforts to develop appropriate<br/>methods to address scientific questions. There are two primary impacts<br/>of this project: the impact of the development of more reliable<br/>statistical methods on scientific findings in astronomy and the impact<br/>of the new statistical inference and computation methods in a wide<br/>range of scientific fields. As the Collaboration develops methods and<br/>distributes free software for specific inferential tasks, it also<br/>educates the astronomical community as to the benefit of careful use<br/>of sophisticated statistical methods. (The Collaboration organizes one<br/>or two special sessions at meetings of the American Astronomical<br/>Society each year.) It is expected that a fundamental impact of the<br/>proposed research will be a more general acceptance and more prevalent<br/>use of appropriate methods among astronomers. Second, the<br/>Collaboration is an example of a new mode of statistical<br/>inference. Rather than using off-the-shelf models and methods, it is<br/>becoming ever more feasible to develop application specific models<br/>that are designed to account for the particular complexities of a<br/>problem at hand.  The Collaboration develops inferential and<br/>computational methods for handling such multi-level models. As<br/>application specific multi-level models become more prevalent, these<br/>methods will have application throughout the natural, social, and<br/>engineering sciences.<br/><br/>In recent years, there has been an explosion of new data in<br/>observational high-energy astrophysics. Recently launched or<br/>soon-to-be launched space-based telescopes that are designed to detect<br/>and map ultra-violet, X-ray, and gamma-ray electromagnetic emission<br/>are opening a whole new window to study the cosmos. Because the<br/>production of high-energy electromagnetic emission requires<br/>temperatures of millions of degrees and is an indication of the<br/>release of vast quantities of stored energy, these instruments give a<br/>completely new perspective on the hot and turbulent regions of the<br/>universe. The new instrumentation allows for very high resolution<br/>imaging, spectral analysis, and time series analysis. The Chandra<br/>X-ray Observatory, for example, produces images at least thirty times<br/>sharper than any previous X-ray telescope.  The complexity of the<br/>instruments, the complexity of the astronomical sources, and the<br/>complexity of the scientific questions leads to a subtle inference<br/>problem that requires sophisticated statistical tools. For example,<br/>data are subject to non-uniform censoring, errors in measurement, and<br/>background contamination. Astronomical sources exhibit complex and<br/>irregular spatial structure. Scientists wish to draw conclusions as to<br/>the physical environment and structure of the source, the processes<br/>and laws which govern the birth and death of planets, stars, and<br/>galaxies, and ultimately the structure and evolution of the<br/>universe. Nonetheless little effort has been made to bring the<br/>strength of modern statistical methods to bare on these problems. The<br/>California-Harvard Astrostatistics Collaboration develops statistical<br/>methods, computational techniques, and freely available software to<br/>address outstanding inferential problems in high-energy astrophysics.<br/>The methods developed are an example of a new mode of statistical<br/>inference: Rather than using off-the-shelf methods, it is becoming<br/>ever more feasible to develop methods that are application specific<br/>and are designed to account for the particular complexities of a<br/>problem at hand. The inferential and computational methods designed by<br/>the Collaboration for handling such multi-level models have<br/>application throughout the natural, social, and engineering sciences.<br/>"
"0353029","Estimation, Modeling and Prediction of Nonseparable and Nonstationary Space-Time Processes","DMS","STATISTICS","07/01/2004","01/10/2008","Montserrat Fuentes","NC","North Carolina State University","Standard Grant","Gabor Szekely","12/31/2008","$111,652.00","","mfuentes@vcu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","0000, OTHR","$0.00","M. FUENTES: DMS - 0353029    <br/><br/>ABSTRACT<br/><br/>Classical geostatistics and Fourier spectral methods are powerful<br/>tools to study the spatial temporal structure of stationary and<br/>separable processes.  However, it is widely recognized that in real<br/>applications spatial temporal processes are rarely stationary and<br/>separable.  Thus  an important extension of these spectral methods is to<br/>processes that are nonstationary and nonseparable.  In this work, the<br/>investigator presents some new spectral approaches and tools to<br/>estimate, model, and test for nonstationarity and nonseparability.<br/>The investigator introduces nonparametric approaches and fitting<br/>algorithms to estimate the spatial temporal structure of a<br/>nonstationary and nonseparable spatial process defined on a continuous<br/>space, and studies the asymptotic properties of these estimates. The<br/>methods are based on a spectral approach, using spectral functions<br/>that are space-time dependent.  The most important scientific<br/>contributions of the research proposed here are: the parametric and<br/>nonparametric estimation of the complex spatial temporal dependence of<br/>environmental processes in general situations (nonstationarity,<br/>anisotropy, nonseparability); the introduction of flexible models for<br/>spatial prediction of environmental processes using spectral methods;<br/>and new methodology for spatial prediction and estimation in the<br/>presence of massive data.<br/><br/><br/>Spatial processes are an important modeling tool for many<br/>environmental and scientific problems.  Environmental scientists who<br/>work with spatial temporal data, however, do not typically believe<br/>that real data satisfy the simple model assumptions such as<br/>separability and stationarity that are currently used in practice.<br/>Therefore, it is is imperative for statisticians to develop methods<br/>without using those assumptions, especially for use with massive<br/>spatial-temporal (environmental) data sets.  Through collaborations<br/>with scientists, the new statistical models and methods proposed by<br/>the investigator for estimation and prediction of space-time<br/>processes, will enhance science by improving weather and air quality<br/>mapping.  The investigator will develop applications in collaboration<br/>with atmospheric scientists and oceanographers on data assimilation<br/>problems and on assessment of the performance of weather, ocean, and<br/>air quality numerical models.  The methods proposed here for space<br/>time processes are also applicable to other fields.  Past interactions<br/>of the PI with various scientists at the Environmental Protection<br/>Agency (EPA), the National Oceanic and Atmospheric Administration<br/>(NOAA), and the National Center for Atmospheric Research (NCAR) are<br/>evidence that previous work of the PI has had an impact on various<br/>fields.   At NCSU there is a<br/>high proportion of women, American and African-American students<br/>compared to other Statistics departments.  Five out of the seven PhD<br/>students currently working on their dissertations under the PI's<br/>supervision are women. The PI will continue her efforts to broaden the<br/>participation of minorities and women.<br/><br/><br/><br/><br/> <br/>"
"0413864","Multiscale Stochastic Modeling, Analysis and Computation","DMS","APPLIED MATHEMATICS, STATISTICS, COMPUTATIONAL MATHEMATICS, MSPA-INTERDISCIPLINARY","09/15/2004","09/26/2007","Markos Katsoulakis","MA","University of Massachusetts Amherst","Standard Grant","Junping Wang","08/31/2008","$314,454.00","","markos@math.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","MPS","1266, 1269, 1271, 7454","0000, 1266, 1269, 1271, 7237, 7303, 7454, 9263, OTHR","$0.00","Problems in diverse scientific disciplines ranging from materials science to macromolecular dynamics,   to atmosphere/ocean science and climate modeling involve the nonlinear interaction of  physical processes across many length and time scales ranging from the microscopic to the macroscopic. From a  modeling and  computational perspective, microscopic simulation methods such as Molecular Dynamics (MD) and Monte <br/>Carlo (MC) algorithms   can describe complex, out of equilibrium interactions at  small scales (e.g.  between atoms or molecules). Although there is  substantial progress in improving aspects of these computational methods, they are still limited to short length and time scales, while on the other hand device sizes and morphological features observed in experiments often involve much larger scales; at the same time stochastic <br/>fluctuations--inherited from the microscopics--can be important, for instance  in  self-organization  problems characterized by small coherent structures such as pattern formation in nanotechnology applications. In addition to such challenges posed by the disparity in scales  within the same model, in many instances we are faced with an additional disparity in models: for example  in phenomena with detailed fluid/surface or boundary layer interactions it is necessary to couple microscopic, possibly stochastic models describing the  dynamics of atoms or molecules on a surface, along with continuum PDE for  species, fluid and thermodynamic variables on the overlying to the surface gas phase. It is therefore inevitable that  features of the microscopic model will  essentially enter as a subgrid effect in the coupling with the coarse computational grid of the macroscopic PDE models. In this case,  the proper incorporation and simulation of stochastic effects from the subgrid microscale is a critical element in the modeling and simulations. The proposed projects focus on aspects of the aforementioned   issues by putting forward a combination of interconnected modeling, computational and analysis questions,  roughly divided in two categories: (i) Mathematical strategies for the coarse-graining of microscopic  models and the corresponding<br/> simulators, addressing problems which are currently  intractable with conventional MD/MC  due to scale limitations. Here, it is not   directly attempted to speed up microscopic simulation algorithms; instead, a hierarchy  of new coarse-grained  stochastic models (referred to as Coarse Grained Monte Carlo methods) is derived, ordered by the magnitude of space/time scales. This new set of models involves  a reduced set of observables over the original  microscopic models incorporating microscopic details and noise, as well as<br/>the interaction of the unresolved degrees of freedom. (ii)   Hybrid    stochastic/deterministic systems describing detailed fluid-surface interactions arise in applications that range from deposition process and catalysis to fuel cell design and microreactors, to  biology and  atmosphere and ocean science. Here two such applications are addressed, namely catalytic reactors and stochastic parametrizations of tropical convection.<br/>Due to their inherent complexity it is also necessary to develop simpler,  test bed problems that capture significant features of  the multi-scale nature of the physical models, but  are still amenable to asymptotics, mathematical analysis and  tractable computations.<br/><br/>The modeling and simulation of problems with multiple interrelating length and time scales is one of the preeminent issues in essentially all timely scientific and engineering challenges, ranging from the design of  nanodevices, to biomolecular  dynamics, to the spread of epidemics  and  climate modeling. In spite of  a continuously increasing  computing power many of these problems remain  intractable, at least in realistic conditions, and new modeling and  simulation  strategies  need to be developed. As several paradigms<br/>have recently demonstrated, the critical step in this process is the use of (the limited in number and flexibility) existing multiscale mathematical and statistical tools as well as the development of  new ones, that enable the creation of new algorithms for complex systems. In the proposed work an array of such novel multiscale mathematics and computing methods is developed. The research is motivated by and targeted  to a<br/>number of the aforementioned applications. Two such particular examples  are, (i) the surface processes  in catalytic reactors and fuel cells, and  (ii) the  tropical and open ocean  convection.<br/>"
"0419627","Travel support for the IMS-ISBA international conference","DMS","STATISTICS","04/15/2004","04/15/2004","Montserrat Fuentes","NC","North Carolina State University","Standard Grant","Robert J. Serfling","03/31/2005","$10,000.00","","mfuentes@vcu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","0000, OTHR","$0.00","The principal investigator requested support for the Second international Joint Meeting of the IMS (Institute of Mathematical Statistics) and ISBA (the International Society for Bayesian Analysis), to be held in Bormio, Italy on Wednesday, January 12 to Friday, January 14, 2005.  A central theme of the conference will be Markov chain Monte Carlo (MCMC) and related methods and applications in the 15 years since the publication of Gelfand and Smith (1990, JASA), the paper that introduced these methods to mainstream statisticians. The requested funding is to support the travel expenses of junior statistical investigators from US institutions, i.e., persons pursuing a PhD or DrPH in statistics or a closely related field, or who have received such a degree within the five years preceding the conference. Such investigators are often doing research that is among the most novel, interesting, and important for international dissemination, yet lack the travel funds necessary to attend such a conference, since they have not yet established a publication track record sufficient to attract external travel and other funding for their work.<br/><br/>The benefit of and need for attendance at such meetings by junior statistical researchers is particularly great, since they contribute mightily to their professional development and help ``level the playing field'' with more established senior investigators. The specific anticipated benefits from the conference are:  to provide an opportunity for researchers to discuss an area in some depth, by allowing ample time for presentations on a specific subject; to motivate young researchers to conduct research in the conference's focus areas that provide ample opportunity for interdisciplinary collaboration;  to encourage young researchers and students to present their work, interact with senior colleagues, and make contact with their peers in a friendly, small meeting environment;  to include a large portion of researchers and students from underrepresented groups among participants, and provide the opportunity for one-on-one discussions with colleagues from other institutions; to disseminate materials from the meeting as widely as possible, by posting the presentations on the ISBA web site at www.bayesian.org.<br/>"
"0404535","Imputation for Survey Data with Ignorable or Nonignorable Nonresponse","DMS","STATISTICS, Methodology, Measuremt & Stats, , , ","09/15/2004","09/17/2004","Jun Shao","WI","University of Wisconsin-Madison","Standard Grant","Gabor Szekely","08/31/2008","$111,000.00","","shao@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269, 1333, V720, V732, V804","0000, OTHR","$0.00","The proposed research focuses on imputation and variance<br/>estimation after imputation for survey data with nonresponse.<br/>The investigator will study different models that relate auxiliary variables and <br/>the variable to be imputed (e.g., parametric, non-parametric, <br/>and semi-parametric models); different response mechanisms<br/>(ignorable or non-ignorable); various imputation techniques (e.g., regression,<br/>nearest neighbor, and random imputation);<br/>different types of estimators (e.g., sample mean and<br/>sample quantiles); and different types of data<br/>(e.g., cross-sectional, clustered, or longitudinal data).<br/>The investigator will also study a pseudo empirical likelihood<br/>imputation method that provides more efficient survey estimators<br/>than other imputation methods.<br/>For each imputation method, variance estimation that takes <br/>nonresponse and imputation into account will be studied, using <br/>a direct derivation approach or a replication method (such as <br/>the jackknife, the balanced half samples, and the bootstrap)<br/>that contains a re-imputation component to assess the variability<br/>caused by imputation. <br/><br/>Many statistics and government agencies collect data through surveys. <br/>Most surveys have nonresponse. Item nonresponse occurs when <br/>some sampled units cooperate in the survey but fail to <br/>provide answers to some questions. Imputation techniques, <br/>which insert values for nonrespondents, are commonly <br/>used compensation procedures for item nonresponse. In <br/>some cases, when auxiliary information is properly used, <br/>imputation increases statistical accuracy. An essential <br/>requirement for an imputation method is that one can <br/>obtain unbiased (or approximately unbiased) survey <br/>estimators and their variability estimators <br/>by treating the imputed values as observed <br/>data and using the standard estimation formulas designed <br/>for the case of no nonresponse. This requires developments<br/>on imputation methodology and statistical analysis procedures<br/>to take nonresponse and imputation into account. Since <br/>most of the proposed research topics are motivated by <br/>problems in survey agencies such as the Census Bureau, <br/>the Bureau of Labor Statistics, Westat, and Statistics Canada, <br/>results obtained from the proposed research will have <br/>significant impacts on the imputation and variance estimation <br/>methodology for these survey agencies.<br/>"
"0443048","SGER:     Statistics of Extremes, with Applications in Financial Time Series","DMS","STATISTICS, Economics","07/15/2004","07/15/2004","Zhengjun Zhang","MO","Washington University","Standard Grant","Grace Yang","06/30/2005","$38,574.00","","zjz@stat.wisc.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","MPS","1269, 1320","0000, 9237, OTHR","$0.00","The proposal pursues a series of developments of new test statistics<br/>for tail independence and of new measures for tail dependence.<br/>Particularly, the investigator studies extreme correlation<br/>coefficient, tail dependence measures, gamma test statistics. These<br/>are related to the study of statistics of multivariate extremes. In<br/>financial applications, the proposal will also specify new models<br/>for asset pricing and extreme co-movements for market data, and<br/>develop new portfolio evaluation tools more sensitive to these<br/>co-movements. A new measure for extreme co-movement is introduced.<br/>The proposal includes the development of statistical estimation<br/>methods for max-stable processes. Procedures of how to calculate<br/>portfolio risk measures are also introduced using combined Markov<br/>process and max-stable process models.<br/><br/>As all definitions used for extremal dependence depend on some limit<br/>procedures and often concern statistical testing for parameters at<br/>the edge of a specific parametric space, great care has to be taken<br/>to obtain tests with sufficient power. The proposal is exactly<br/>aiming at finding a solution for this. The investigator will<br/>carefully compare and contrast new approaches with existing ones<br/>using simulated as well as real data. The real data will come from<br/>areas as diverse as insurance, finance, telecommunications,<br/>climatology, seismology, medicine, etc. Throughout applications in<br/>diverse fields (like above), extreme risks play an important<br/>scientific, societal as well as (possibly) political role. The<br/>dissemination of new statistical tools leading to a better<br/>understanding of the occurrence of joint extremes is of great<br/>importance. This can be very well achieved at the level of new<br/>graduate courses, publications in journals aimed at a broaden<br/>audience and in discussion with scientists from other fields.<br/>"
"0405791","Efficient Estimation in Semiparametric Models","DMS","STATISTICS","09/15/2004","09/14/2004","Anton Schick","NY","SUNY at Binghamton","Standard Grant","Grace Yang","08/31/2007","$93,388.00","","anton@math.binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","MPS","1269","0000, OTHR","$0.00","The Principle Investigator proposes to continue his research program <br/>on efficient estimation in semiparametric models with an emphasis <br/>on non- and semiparametric regression models, on bivariate models with <br/>partial knowledge about the marginals, and on time series models.<br/>Special attention will be paid to the possibility of <br/>(efficient) root-n consistent estimation <br/>of curves such as stationary densities and conditional expectations in <br/>time series models with independent innovations.  <br/>Work on regression models will focus on diagnostics and <br/>on imputing when responses are missing at random.<br/>In connection with the latter fully imputed estimators will be studied.<br/>Such estimators impute not only the missing but also the observed responses.<br/>Current work by the Principal Investigator suggests that the <br/>fully imputed estimator is typically better than the partly imputed estimator, <br/>which only imputes the missing responses. <br/>One goal of the research is to prove this conjecture and <br/>to show that fully imputed estimators can even be made to be efficient. <br/>The work on bivariate models will deal with <br/>fine-tuning results for known and equal marginals obtained so far,<br/>and on extending such results to more general models <br/>including those in which the marginal distributions are <br/>linked through a parameter. <br/>Work on two monographs, one on efficient estimation in regression models<br/>and the other on efficient estimation in time series models, is also planned.<br/><br/>The proposed research will advance the theory of efficient estimation in <br/>semiparametric models and will provide more efficient ways of analyzing <br/>data in many concrete problems.  Since semiparametric models are <br/>widespread in many fields that use statistics, the proposed research will <br/>have an impact on all these fields.  For example, results on time series <br/>and Markov chain models have applications in econometrics and mathematical <br/>finance; results on bivariate models have applications in actuarial sciences <br/>and in medical research; results on imputing are useful in medical studies.<br/>The planned monographs are intended to disseminate some of the research <br/>from the proposed research to a wider audience.<br/>"
"0348764","CAREER: Semiparametric and Non-Parametric Models for Correlated Data","DMS","STATISTICS","07/01/2004","05/01/2008","Annie Qu","OR","Oregon State University","Continuing Grant","Gabor Szekely","12/31/2008","$400,000.00","","aqu2@uci.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","MPS","1269","0000, 1045, 1187, OTHR","$0.00","CAREER: Semiparametric and nonparametric models for correlated data<br/><br/>Abstract:<br/><br/>This research project is aimed at developing statistical theory and<br/>practical methodology for complex high dimensional correlated data<br/>where the full parametric likelihood function of the model is difficult<br/>to specify or intractable, and partial data information is not accurate<br/>or is missing. The PI and her collaborators  will develop<br/>efficient and robust estimation procedures by incorporating<br/>correlation structures  into the models where high dimensional<br/>nuisance parameters are present, and develop inference functions<br/>for hypothesis testing with low computational intensity.<br/>Part of research goals for the 5-year plan are:<br/>to provide an explicit maximum number of contaminated<br/>clusters allowed to maintain the consistency of the estimator using<br/>quadratic inference functions; to develop unbiased and efficient<br/>estimating functions if missing responses are missing at random,<br/>and inference functions for testing the model assumption;<br/>to develop an efficient esimator using a nonparametric regression<br/>spline with relatively low demand on computation, and introduce a<br/>goodness-of-fit test with a chi-squared property for testing whether<br/>coefficients in nonparametric regression  are time-varying or time<br/>invariant; and, to develop semi-nonparametric models for cell cycle<br/>microarray data to incorporate both temporal correlation within genes<br/>and correlation between biologically related genes.<br/><br/><br/>This research will have significant impact and many applications in<br/>biomedical research, econometrics, environmental studies, oceanography,<br/>social science and public health where correlated data arise<br/>often. The outlined research projects help to tackle fundamental<br/>questions in statistical science and will stimulate interest  from a<br/>large group of scientists. It also makes connections between<br/>theory and methods developed in econometrics, statistics and<br/>biostatistics. The proposed research will benefit biomedical research<br/>to help combat life threatening diseases such as AIDS and cancer,<br/>and will make contributions to identifying cell cycle regulated genes<br/>more accurately. It will integrate current states of knowledge of<br/>proposed research areas substantially into  educational activities<br/>through development of<br/>new courses on  nonparametric methods and microarray data analysis.<br/>It will advance undergraduate and graduate students' learning and<br/>training in semiparametric and nonparametric methods. Furthermore,<br/>it will broaden opportunities and enable the<br/>participation of all citizens from various disciplines, including<br/>underrepresented minorities and international partnerships.<br/>"
"0404954","Nonlinear and Non-Stationary Time Series Modeling and Its Applications","DMS","STATISTICS","07/15/2004","05/17/2006","Zongwu Cai","NC","University of North Carolina at Charlotte","Continuing grant","grace yang","06/30/2007","$69,973.00","","caiz@ku.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","MPS","1269","0000, OTHR","$0.00","This proposal concerns scientific research and education on a variety of statistical methodological developments and foundational research with particular applications in economics and other applied fields and it consists of two subjects. The first subject is to study a new class of nonlinear seasonal time series models to characterize the seasonal variations in nonlinear and non-stationary time series. The models are decomposed into a common trend function over periods and additive seasonal effect functions that are specific to each season within the periods. Also, the models will be extended to the time-varying coefficient seasonal time series models to include exogenous variables. The nonparametric techniques will be developed to estimate the trend and seasonal effect functions and the asymptotic properties of all the proposed estimators will be established under the strong mixing conditions without any specification of the error distribution. The second subject is to develop the nonparametric modeling methods to analyze nonlinear and non-stationary time series data by considering the following nonparametric or semi-parametric models: the time-varying coefficient time series regression models with nonlinear time trend function and/or integrated regressors and/or correlated errors, the nonparametric or semi-parametric co-integration models, and the nonparametric or  semi-parametric (dynamic) panel models with nonlinear time trend function and/or  integrated regressors and/or correlated errors. The central components of this proposal are the developments of nonparametric modeling techniques to those models and to make those models practically useful. Specifically, the local linear model-fitting scheme will be developed to estimate the nonparametric time trend and the coefficient functions and the nonparametric regression functions. Moreover, the asymptotic properties of all the proposed estimators will be established under some regularity conditions without specifying the error distribution.  Particularly, the attention will be paid on the asymptotic properties for the non-stationary cases due to different rates of convergence (by comparing with the stationary cases). Further, the bandwidth selection issue will be addressed particularly for the non-stationary cases. Finally, to test the misspecification and stationarity, a nonparametric version of the generalized likelihood ratio test together with a simple nonparametric version of bootstrap will be proposed and investigated.<br/><br/> The intellectual merit of this proposal is not only to propose and develop several nonlinear and non-stationary time series regression models but also develop novel nonparametric modeling methodologies to make these models practically applicable. The new statistical methods will possess the advantage of requiring fewer assumptions than previously developed methods. Hence, the conclusions obtained from them should be less dubious. The broader impacts of this proposal are that this proposal represents a comprehensive and long-term  attack on a host of important data analytic problems in applied fields and that the results will be of long-term theoretical and practical interests and will provide near-term solutions to real-world problems. The methods are based on the transparent ideas that can be easily incorporated into the classroom and graduate students are presently involved in the various projects described and more will become involved.<br/>"
"0406431","Shape Statistics and Data Mining","DMS","STATISTICS","07/15/2004","07/15/2004","Wolfgang Polonik","CA","University of California-Davis","Standard Grant","Gabor Szekely","06/30/2008","$198,751.00","","wpolonik@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","0000, OTHR","$0.00","This project is dealing with statistical methods for large and complex data <br/>sets<br/>involving shape aspects. One example for a method relevant to this project<br/>is PRIM (Friedman and  Fisher, 1999), a popular data mining algorithm with<br/>a wide range of possible applications. PRIM has been applied to serval real <br/>world<br/>problems and two modifications have recently been developed<br/>(Becker and Fahrmeir (2001) and LeBlanc et al. (2002)). Nevertheless,<br/>no theoretical foundation of the algorithm exists which might provide a deeper<br/>understanding of the algorithm. To provide such an analysis of PRIM and its <br/>modifications<br/>is part of the proposed project. In fact, a preliminary analysis revealed a <br/>close connections<br/>to much better understood methods based on minimum volume sets. This revelation<br/>also shows the connection to ""shape statistics"". In another subproject the <br/>investigator<br/>will develop a novel projection pursuit type method for dimension reduction <br/>which is<br/>aimed at subsequent classification or mode hunting. This task involves both <br/>algorithmic<br/>and theoretical challenges, and again ""shape"" aspects (modes, antimodes, etc.)<br/>come into play.<br/><br/>In more general terms it can be said that there is an acknowledged lack of<br/>(theoretical) understanding of many statistical methods for large and <br/>complex data<br/>sets, and enhancing this knowledge is considered to be an important task of <br/>statistics<br/>(see Kettenring et al. 2003). This project is aimed at contributing to<br/>this task both directly and indirectly: (a) directly by providing an <br/>analysis of<br/>some of the existing data mining procedures, and (b) indirectly by developing<br/>novel methods for large and complex data sets including supporting<br/>statistical theory.<br/><br/><br/>"
"0405777","Statistical Estimation from Videos of Freeway Traffic","DMS","STATISTICS","09/01/2004","07/17/2006","John Rice","CA","University of California-Berkeley","Continuing grant","Gabor Szekely","08/31/2008","$346,000.00","","rice@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","The proposed research focuses on the estimation of velocity, density, and<br/>flow fields from video recordings of freeway traffic, using data<br/>from the Berkeley Highway laboratory, which has recorded thousands<br/>of hours of traffic flow on freeway I-80.  The investigators are <br/>particularly<br/>concerned with recordings which are not sufficiently well resolved<br/>to allow unambiguous vehicle identification and tracking.<br/>They propose to investigate two broad classes of methods.  The<br/>first, building on an initially encouraging approach, is based<br/>upon local semiparametric statistical modelling of the velocity<br/>field.  The second is based upon simulation using microscopic<br/>models.  Both of these present challenges for the development of<br/>statistical methodology and theory.<br/><br/>Broader impacts include: the development of effective, practical<br/>methods for video surveillance of traffic, which would be of<br/>widespread utility;  the compilation of a large empirical body of<br/>velocity fields, which will be a valuable resource for researchers<br/>in traffic flow theory; extension of the methodology to the more<br/>general problem of estimating optical flow in two dimensions,<br/>which has widespread applications in computer vision; and the<br/>involvement of undergraduates and graduate students in research.<br/><br/><br/><br/>"
"0405716","Prediction for Multi-factor Point Process Models","DMS","STATISTICS","09/01/2004","08/31/2004","Lawrence Brown","PA","University of Pennsylvania","Standard Grant","Gabor Szekely","08/31/2008","$191,861.00","Linda Zhao","lbrown@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, OTHR","$0.00","Abstract<br/>PI: Lawrence D. Brown<br/>Proposal: 0405716<br/><br/> This proposal develops two different classes of models for statistical analysis of temporal point processes with covariates. The temporal structure here has a two way character, since there is day-to-day variation and also intraday variation, and these two types of variation must be modeled separately before being combined. Of interest are accurate predictions and also prediction confidence intervals for future observations of the process. One class of models begins by taking a slightly modified square root of the binned point-process counts and then treats these via variations of customary non-linear Gaussian random-effects models. The second class of models begins from a more primitive perspective by developing new classes of point processes having certain properties related to the usual Gaussian paradigm involving Moving Average and Auto Regressive constructions. These point processes are actually special cases of a more general new construction yielding processes with infinitely divisible finite dimensional marginals, and that are analogs in appropriate senses of classical AR and MA Gaussian processes. This new class of models is being adapted and applied to various straightforward temporal settings. The investigators are also studying how to apply such models in the complex settings mentioned above involving covariates and different temporal dimensions. A further area of study is the examination of the differences between the results of analyses involving the new class of stochastic processes and those using the simpler, but approximate, square root idea.    <br/><br/> Telephone Call Centers are an important and growing component of our modern service-based economy. Proper management of such a center requires estimation of several operational ""primitives"", combined with queuing theory considerations, in order to determine appropriate staffing levels for efficient and economic customer service. Accurate prediction of the level of customer arrivals is the most difficult of the primitives to assess. Predictions as well as confidence bounds for these predictions are needed. In this proposal two new classes of statistical models particularly attuned to special features of this type of data are developed to make such predictions and confidence statements. While these models are particularly tuned to produce the desired result in the telephone context, they are also adaptable to a wide variety of other prediction problems, particularly to an important class of problems in spatial analysis. In addition, variations of the models are useful in developing techniques for internet intrusion detection. Computer intrusion (attacks by hackers) is an increasing impediment to efficient internet communications, and its detection is one vital step in eliminating this burden.<br/><br/>"
"0502454","Winter Workshop on Longitudinal Data Analysis","DMS","STATISTICS","12/01/2004","11/12/2004","Linda Young","FL","University of Florida","Standard Grant","Rong Chen","11/30/2005","$15,000.00","George Casella","LJYoung@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","0000, OTHR","$0.00","Longitudinal data are the result of taking repeated measurements on <br/>experimental units over time.  Because scientific interest often lies in <br/>how a response changes over time, these data arise in both observational <br/>and experimental studies in a wide variety of fields.  Creative <br/>solutions to difficult theoretical issues in longitudinal data analysis <br/>often rely on computationally intensive methods.  A workshop on <br/>longitudinal methods appeals to a wide spectrum of researchers with <br/>diverse statistical interests ranging from clinical trials to social <br/>behavior to environmental assessment.  The challenge of drawing <br/>inferences based on diverse datasets attracts those interested in <br/>theoretical and methodological statistics.  Finally, the excitement of <br/>accepting the challenge of analyzing unorthodox data where existing <br/>statistical methodology is not satisfactory will undoubtedly fascinate <br/>researchers concerned with applications of statistics.   This workshop <br/>will provide a forum for the positive interaction between theoretical <br/>developments and advances in practical applications of longitudinal <br/>methods. <br/><br/>The specific objectives of the workshop on longitudinal data analysis, <br/>and consequently its potential broader impacts, are to generate interest <br/>in this topic among researchers nationwide (particularly young <br/>researchers) and among faculty and graduate students at the University <br/>of Florida and neighboring universities; to encourage future <br/>collaborations among researchers; and to promote advances in both the <br/>theory and application of longitudinal data.<br/>"
"0405543","Cluster Analysis, Predictive Distributions, and Stochastic Search Algorithms","DMS","STATISTICS","06/01/2004","06/23/2008","George Casella","FL","University of Florida","Continuing grant","Gabor Szekely","06/30/2009","$448,554.00","James Booth","casella@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","0000, OTHR","$0.00","Cluster analysis is a widely used exploratory tool for finding patterns in data. The basic goal of a cluster analysis is to separate m distinguishable objects (based on measurements associated with them) into groups, or clusters, such that the objects within each group are ""similar"" while the groups themselves are ""different.""  The number of possible partitions of m objects grows extremely quickly with m, and consequently it is impossible to perform an exhaustive search for the best partition.  Most standard methods such as hierarchical and K-means clustering: (a) sacrifice an extensive search of all possible partitions for speed of implementation; (b) fail to (globally) optimize an objective function; and generally return a single answer, even though there may be many equally good answers that are all relevant to the application.  This investigation will look at:  (i) The improvement attainable in the performance of clustering algorithms  using data smoothing;  (ii)  A model-based approach to simultaneously smooth the data while providing a natural objective function  for ranking partitions;  and (iii) Strategies for conducting a stochastic search with high speed computing and Markov chain Monte Carlo algorithms. The proposed methodology has already been successfully applied in some examples.<br/><br/>Cluster analysis has seen renewed interest of late due, in part, due to its applications in bioinformatics, where it can be used with microarray analysis to identify groups of genes that can be linked to certain diseases.  For example, it could be the case that the presence or absence of certain genes could predispose a person to certain types of cancers, or to indicate greater post-operative risk from certain procedures.  Therefore, the benefits to society of the proposed project include the advances from the better understanding of these relationships that these improved algorithms will yield, and the clearer picture provided of the links between genes and diseases.  Graduate students will also be trained to develop these methods further.  Other researchers, trained in these new methods, will find their own investigations enhanced.<br/>"
"0354448","Collaborative Research: FRG: New Development on Nonparametric Modeling and Inferences with Biological Applications","DMS","STATISTICS","06/01/2004","04/30/2004","Hans-Georg Mueller","CA","University of California-Davis","Standard Grant","Gabor J. Szekely","05/31/2008","$282,000.00","","hgmueller@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","0000, 1616, OTHR","$0.00","The objectives of this proposal are to develop new and widely applicable<br/>semiparametric and nonparametric approaches to solve challenging<br/>statistical problems from computational biology. Frontiers of biological<br/>research such as normalization and analysis of microarray and proteomic<br/>data, functional connectivity of brains, covariate effects on longitudinal<br/>and functional data, and prediction of individual response trajectories<br/>have generated a number of outstanding statistical challenges.  Several<br/>new semiparametric and nonparametric models have been introduced to<br/>address the imminent needs for the aforementioned biological applications.  <br/>A number of innovative methods on nonparametric estimation and inferences<br/>are proposed. Their properties will be investigated via both asymptotic<br/>theory and simulations.  Their efficacy in biological applications will be<br/>carefully scrutinized.  This proposal not only introduces a number of<br/>innovative techniques and useful statistical models, but also provides<br/>various new insights into nonparametric inferences.  The research findings<br/>will have significant impact on the future development of statistical<br/>theories and methodologies.<br/><br/><br/>Technological invention and information advancement have revolutionized<br/>scientific research and technological development. Quantitative methods<br/>have been widely employed in scientific communities.  They have played<br/>pivotal roles in knowledge discovery.  This proposal intends to develop<br/>new nonparametric techniques and theories that arise from frontiers of<br/>scientific development. In particular, the investigators will develop<br/>models and cutting-edge technologies for the analysis of microarray,<br/>proteomic, longitudinal and functional data and fMRI brain images. Common<br/>characteristics of these data are their complexity and size, where<br/>nonparametric techniques are particularly powerful and under developed.<br/>The proposed techniques address imminent needs in computational aspects of<br/>molecular biology, neurology, and epidemiology. In addition, they will<br/>integrate new mathematical developments with those in science and<br/>engineering, which empowers new knowledge discoveries and prudent policy<br/>making.  Undergraduate and graduate students, postdoctors and<br/>underrepresented groups will be trained as results of this research.<br/><br/><br/><br/>"
"0406091","High Dimensional Methods for Complex Data Refining","DMS","STATISTICS","06/01/2004","05/02/2006","Ker-Chau Li","CA","University of California-Los Angeles","Continuing grant","Gabor J. Szekely","05/31/2008","$203,192.00","","kcli@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","0000, OTHR","$0.00","Regression analysis aims at the study of the relationship between input variables X and out variables Y. Difficulties occur  when no parametric model is known, and yet the number of variables is large. Dimension reduction methods for overcoming such difficulties have been investigated by many authors. To embrace many aspects of dimension reduction under one common roof, a new forum called the Z-mediated approach is proposed. In this setting, in addition to the input and out <br/>variables, a third group of variables Z is introduced, which fills the role of mediating the change in the relationship between X and Y. Typically the number of Z variables is much larger than the number of X or Y variables. But only a small portion of Z variables may have a real influence. New methods will be constructed to reduce the dimension of X, Y and Z. <br/><br/>A wave of cutting-edge statistical activities have arrived at a time when there is an explosive demand for processing large data sets in the life sciences, such as those from microarrays and medical imaging. The motivation of this proposal comes from a dynamic perspective about complex gene regulation where two functionally associated genes X and Y may be mediated by a <br/>third unknown gene Z. The challenge is how to identify a short list of candidate gene Z based on microarray data alone. The methodology developed here can be used for elucidating the interplay between disease, genes, and metabolic pathways, thus contributing to drug discovery and benefiting society. The results will be disseminated not only via standard publication, but <br/>also by constructing a website for public access. Interdisciplinary training of students to work in bioinformatics is also provided. <br/> <br/>"
"0428026","Workshop/Conference  for Probability Models and Statistical Analyses for Ranking Data","DMS","STATISTICS","05/15/2004","05/13/2004","Joseph Verducci","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","grace yang","04/30/2005","$7,000.00","","verducci.1@osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","abstract <br/>PI: Joseph Verducci<br/>proposal: 0428026<br/><br/>Workshop/Conference for Probability Models and Statistical Analyses for Ranking Data<br/><br/>The workshop, geared toward young researchers, is comprised of five week-long background courses and a lecture series of current research topics.  The five short courses for ranking data are 1) Probability Models; 2) Experimental Designs and Analysis; 3) Simulation Based Methods; 4) Paired Comparisons; 5) Tests of Independence.  Current research topics include Ranking Methods for Search Engines, Connections with the Theory of Group Representations, and Random Effects in Consumer Preferences.  The conference, geared more toward specialists, consists of invited and contributed papers from diverse areas, including Machine Learning, Algebra, and Utility Theory.  Contributed papers emphasize current applications.<br/><br/>Data in the form of rankings may be found everywhere: in marketing, rankings are the key to studying subtle changes in consumer preferences under changing conditions; in sports, for determining everything from college team seedings to Olympic medals; on the internet, thousands of similar sites must be ranked in different ways to meet the needs of diverse clients.  Accurate analysis of such data requires specialized methods.  Historically these methods were studied in three eras, the pre-computer era, where simple models were constructed, largely based on theories from psychology and economics; the early computer era, where many models were categorized and implemented; and the modern era, where models are being tailored to complex problems.  The workshop/conference is designed to lead young researchers through the diverse literature to the current state of the art.<br/><br/>"
"0406026","Collaborative Research: Methodology for Computer Experiments with Special  Application to Orthopedic  Research","DMS","STATISTICS","08/15/2004","08/12/2004","Thomas Santner","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","grace yang","07/31/2007","$70,000.00","","santner.1@osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","Motivated by problems of biomechanical engineering, this collaborative research project develops and implements statistical tools for the design and analysis of computer experiments that have particular application to prosthesis design.  These tools are used to create ace tabular cups designs that are resistant to pelvic osteolysis in a 3D model finite-element model for bone adaptation in the pelvis.  A second specific biomechanical task is to validate/calibrate the computer codes for polyethylene wear in knee components from mechanical knee simulator studies.  More generally, this work provides a suite of implemented tools that allow cost-effective and rapid engineering development of a wide range of product applications.  It helps merge the statistical and engineering design communities by forcing the former to address fully articulated design problems and the latter to anchor the solution of these problems in rigorous stochastic and statistical methodology. The methodology may also prove useful in the analysis of other large computer models such as the climate, exposure, and pollution models used in environmental science. On the statistical front, exploratory designs are developed for computer experiments that allow better estimation of extreme outcomes, important for assessing the frequency of dangerous high stress-strain situations in a given patient population when a given prosthesis-design is used.  This objective can be thought of as a robustness assessment tool for the given prosthesis design.  Second, sequential designs are devised for constrained optimization problems, which are important in prosthesis design where multiple, competing outcomes occur frequently.  On the analysis front, predictors are developed that give more accurate assessment of optima of predicted surfaces; these are used to produce faster (in the sense of requiring fewer evaluation of time-demanding computer code) solutions of constrained optimization problems.  Finally, prediction methodology is developed for computer codes that use both qualitative and quantitative inputs.   <br/><br/>This project develops and implement statistical tools for the design and analysis of computer experiments that have particular application to prosthesis design. Computer experiments are cost-effective in rapid engineering development of a wide range of product applications. The project develops both statistical and mathematical tools that are useful outside of the current application of prosthesis design.<br/>"
"0404547","Confident Bayes Regularization in Discrete Multi-way Layouts","DMS","STATISTICS","06/01/2004","05/04/2006","Rudolph Beran","CA","University of California-Davis","Continuing grant","Gabor J. Szekely","05/31/2008","$368,532.00","","beran@wald.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","0000, OTHR","$0.00","In a discrete multi-way layout, each of the factors affecting measured response takes on a finite number of levels, which may be either nominal (pure labels) or ordinal (real-values whose order and magnitude bear information). Often, only relatively few noisy observations are made at only some factor-level combinations that form a subset of all theoretically possible combinations of the factor levels. Basic problems in analyzing such sparse regression-type data are to extract efficiently signal from noise at the observed factor-level combinations; to assess intelligibly the uncertainty in the extracted signal; and to extrapolate plausibly the extracted signal to the unobserved factor-level combinations. This project will develop the following data-driven regularization methodology: use a tractable general probability model for the incomplete multi-way layout to motivate classes of candidate Bayes estimators for the means of the multi-way layout; b) estimate the risk of each candidate estimator under this general model; c) define the regularized estimator to be the candidate estimator with smallest estimated risk; d) prove theoretically that the risk of the regularized estimator converges asymptotically to that of the best candidate estimator; e) develop confidence sets centered at the regularized estimator that quantify the uncertainty of that estimator; f) experiment with the regularized estimators on case-study data and on pertinent artificial data.<br/><br/>Important practical instances of the multi-way layout data treated in the project are spatial data, the gene or protein chip data of bioinformatics, and the digital images and videos of medical imaging or of industrial quality control. The project will develop effective, semiautomatic algorithms for separating pattern from unimportant details or noise in such data. Thereby, it will serve to focus human intervention on the subtle questions that remain after effective algorithmic analyses of large multi-way layouts. The project will contribute to solving several core research challenges identified in Section 4 of the 2003 NSF Report ""Statistics: Challenges and Opportunities for the Twenty-First Century"".  Amongst these technical challenges are Bayes and biased estimation, data reduction and compression, and structuring the interaction between statistical theory and computational experiments. Project results will be submitted for journal publication and will be posted on the PI's website.  A unified account of the methodology is planned for a monograph. Portions of the project, including case study applications of the methodology to the data types listed above, will guide Ph.D. thesis research by the PI's students.<br/>"
"0406030","Collaborative Research: Methodology for Computer Experiments with Special Application to Biomedical Mechanics","DMS","STATISTICS","08/15/2004","08/12/2004","Donald Bartel","NY","Cornell University","Standard Grant","grace yang","07/31/2007","$70,000.00","","dlb13@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","0000, OTHR","$0.00","Motivated by problems of biomechanical engineering, this collaborative research project develops and implements statistical tools for the design and analysis of computer experiments that have particular application to prosthesis design.  These tools are used to create ace tabular cups designs that are resistant to pelvic osteolysis in a 3D model finite-element model for bone adaptation in the pelvis.  A second specific biomechanical task is to validate/calibrate the computer codes for polyethylene wear in knee components from mechanical knee simulator studies.  More generally, this work provides a suite of implemented tools that allow cost-effective and rapid engineering development of a wide range of product applications.  It helps merge the statistical and engineering design communities by forcing the former to address fully articulated design problems and the latter to anchor the solution of these problems in rigorous stochastic and statistical methodology. The methodology may also prove useful in the analysis of other large computer models such as the climate, exposure, and pollution models used in environmental science. On the statistical front, exploratory designs are developed for computer experiments that allow better estimation of extreme outcomes, important for assessing the frequency of dangerous high stress-strain situations in a given patient population when a given prosthesis-design is used.  This objective can be thought of as a robustness assessment tool for the given prosthesis design.  Second, sequential designs are devised for constrained optimization problems, which are important in prosthesis design where multiple, competing outcomes occur frequently.  On the analysis front, predictors are developed that give more accurate assessment of optima of predicted surfaces; these are used to produce faster (in the sense of requiring fewer evaluation of time-demanding computer code) solutions of constrained optimization problems.  Finally, prediction methodology is developed for computer codes that use both qualitative and quantitative inputs.   <br/><br/>This project develops and implement statistical tools for the design and analysis of computer experiments that have particular application to prosthesis design. Computer experiments are cost-effective in rapid engineering development of a wide range of product applications. The project develops both statistical and mathematical tools that are useful outside of the current application of prosthesis design.<br/><br/>"

"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"0307057","Probabilistic and Statistical Analysis of Population Evoluation Data","DMS","STATISTICS","06/01/2003","05/28/2003","Anand Vidyashankar","GA","University of Georgia Research Foundation Inc","Standard Grant","Gabor Szekely","05/31/2007","$101,719.00","","anand@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","proposal:  0307057<br/>PI:  Anand Vidyashankar<br/>title:  Probabilistic and Statistical Analysis of Population Evolution Data<br/><br/>Abstract:<br/><br/>This project is aimed at developing probabilistic and statistical theory for population evolution data. These data possess features that can broadly be described as variants of the branching phenomenon. Specific scientific questions that drive this project are motivated towards understanding the PCR (Polymerase Chain Reaction) dynamics. Specifically, this project develops statistical models (linear and generalized linear models) after initially modeling the amplification process as a variant of a general branching process. Due to the non-standard nature of the problem, novel methodologies and techniques are necessary to understand the inferential aspects stemming from these statistical models. These in turn lead to interesting probabilistic questions about large deviations, conditional limit laws, and central limit theorems.<br/><br/>The last two decades have seen an explosion of novel ideas in the areas of biotechnology and information technology. These discoveries are being felt in such diverse areas as finance, e-commerce, biology, and network traffic. The scientific experiments carried out using these modern technologies are yielding data that possess various intricacies and complexities. This research is aimed at providing concrete solutions to some of these complexities. An immediate application of the results of this project would be in accurately quantifying the HIV-1 viral load in HIV-1 infected patients. Other areas of applications include cancer biology, Internet congestion and its impact on e-commerce.<br/><br/>"
"0306574","Higher order accuracy of bootstrap methods for temporal and spatial processes","DMS","STATISTICS","08/01/2003","07/02/2007","Soumendra Lahiri","IA","Iowa State University","Standard Grant","Grace Yang","01/31/2008","$225,588.00","","s.lahiri@wustl.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","0000, OTHR","$0.00","The emphasis of the project is on investigating higher order properties of common resampling methods for time-series and spatial data and on development of new inference methods that improve the accuracy and stability of existing methods.  Specifically, this project concentrates on <br/>(i)  developing Edgeworth expansion theory for Studentized statistics under dependence, <br/>(ii)  investigating higher order accuracy of bootstrap approximations for time series data, <br/>(iii)  developing resampling methods with an aim towards achieving higher order accuracy, <br/>(iv)  a nonparametric method for the selection of optimal block length empirically,<br/>(v)  a pooling method for the bootstrap that yields more stable estimators of population parameters, <br/>(vi)  investigating accuracy of bootstrap approximation in spatial prediction problems, and<br/>(vii)  investigating accuracy of bootstrap approximation for spatial data for irregularly spaced data-points.<br/><br/>Data exhibiting temporal and spatial dependence appear in many areas of sciences, such as Astronomy, Atmospheric Sciences, Economics, Geology, Hydrology, Physics, etc.  Analyses of such data sets using current statistical methodology face some limitations.  This is primarily due to the fact that the existing statistical methodology mostly relies on strong structural (i.e., parametric model) assumptions that are often inadequate to capture all important features of the data.  This project seeks to<br/>(i) develop new methodology (based on what are known as Resampling Methods) that provides valid assessment of uncertainty without strong structural assumptions and <br/>(ii) develop theoretical tools to investigate optimality properties of various resampling methods for time- and space-dependent data.  <br/>"
"0306970","Collaborative proposal:  ISI and TIES conference support program","DMS","STATISTICS","04/01/2003","02/21/2003","Peter Guttorp","WA","University of Washington","Standard Grant"," Shulamith T. Gross","03/31/2004","$10,500.00","","guttorp@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, OTHR","$0.00","Proposals 0304954 and 0306970<br/>PIs  Montserrat Fuentes and Peter Guttorp<br/>Title  Collaborative proposal: ISI and TIES Conference Support Program<br/><br/>Abstract<br/><br/>This collaborative project supports travel of new researchers from US institutions to two international conferences in Statistics during the summer of 2003. The conferences are the International Environmetrics Society (TIES) 2003 Conference, held in August 2003 in Beijing, China, and the International Statistics Institute (ISI) Conference on Environmental Statistics and Health (ISI-ESH), held in July 2003 in Santiago de Compostela, Spain. The conferences provide a forum for these new researchers to present their work and interact with established researchers in statistics who work on environmental and health related scientific issues. <br/>"
"0304661","Crossover Designs for Comparing Test Treatments with a Control Treatment:  Optimality, Efficiency, and Robustness","DMS","STATISTICS, EPSCoR Co-Funding","07/15/2003","07/02/2003","Min Yang","NE","University of Nebraska-Lincoln","Standard Grant","Grace Yang","01/31/2006","$85,163.00","","myang2@uic.edu","2200 VINE ST BOX 830861","LINCOLN","NE","685032427","4024723171","MPS","1269, 9150","0000, 9150, OTHR","$0.00","The goal of this project is to find optimal, efficient, or robust crossover designs for comparing several test treatments with a control treatment. A-optimality and MV-optimality are considered as optimality criteria. Orthogonal matrix theory will be applied to simplify the corresponding information matrix. Permutation techniques will be employed to determine the corresponding achievable lower bounds under the optimality criteria. Five objectives are designed to accomplish this goal. <br/>(1) Identify and construct optimal/efficient designs under the traditional model; <br/>(2) Identify and construct optimal/efficient designs under the self and mixed carryover effects model; <br/>(3) Identify and construct robust designs that perform well under various models; <br/>(4) Propose corresponding algorithms for Objectives (1) through (3) and develop a software package to facilitate the dissemination and wide application of the research results; and<br/>(5) Develop curriculum in the University of Nebraska-Lincoln's advanced experimental design courses.<br/><br/>Crossover designs have been widely used in a variety of fields, especially in clinical trials. Although there exists a great deal of research on identifying and constructing optimal/efficient or robust crossover designs when all treatments are equally important, knowledge on such designs when comparing several test treatments with a control treatment is extremely limited and urgently needed. At present, there is little applicable guidance on how to conduct such experiments. The results of this study, when applied, are expected to significantly reduce the time, money, and the number of patients needed in clinical trials. In addition, it is expected that this research will help the FDA improve its guidelines to crossover designs. Furthermore, the user-friendly software package can help both statisticians and non-statisticians to utilize research results from the project, thus reduce costs and speed up new drug development.<br/>"
"0304407","Regression with Periodic Series","DMS","STATISTICS","07/01/2003","04/05/2004","Robert Lund","GA","University of Georgia Research Foundation Inc","Continuing Grant","Robert J. Serfling","05/31/2005","$121,035.00","","rolund@ucsc.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0304407<br/><br/>PI: Robert B. Lund<br/><br/>This research studies regression methods in time series settings with periodic properties. The general goal of this work is to put statistical inference for regression models with periodic error disturbances on the same footing as that for its stationary brethren. The particular issues examined include (1) simple linear regression for periodic series, (2) analysis of variance methods for periodic series, (3) undocumented changepoint detection in periodic series, (4) the adjustment of periodic series for documented and undocumented changepoint times, and (5) trends in monthly extreme temperatures, and winter snow depths. The work will advance and further connect the statistical areas of time series, forecasting, extreme value analysis, and regression modeling.<br/><br/>On a practical level, the research will help resolve climate change issues within the United States and, more generally, help quantify global warming. The statistical methods developed will be used to study trends in United States monthly extreme (maximum and minimum) and average temperatures. Trends in United States snow cover (over seasonal maximums and snow water equivalent) will also be examined<br/>"
"0306726","Fractional Cointegration, Tapering and Estimation of Misspecified Models in Long Memory Time Series","DMS","STATISTICS","08/15/2003","05/05/2005","Willa Chen","TX","Texas A&M Research Foundation","Continuing Grant","Grace Yang","07/31/2007","$107,483.00","","wchen@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","<br/>This project considers several problems on the most recent topics in long memory time series including fractional cointegration, data tapering and estimation of misspecified long memory models. The first line of research develops and implements new statistical methods for fractionally cointegrated multivariate time series. The focus is on separating the space of cointegrating vectors into subspaces yielding different memory parameters. The second main research topic focuses on data tapers and their applications. A data taper generating algorithm is introduced. The proposed algorithm can easily generate data tapers of any order. This class of new tapers has better variance properties and the resulting periodogram is shift-invariant, a desirable property in estimating parameters of long memory processes. The third line of research addresses the problems of estimating misspecified long memory models and their implications. A study of the frequency domain maximum likelihood estimators of misspecified long memory models suggests that even if the long memory structure of the time series is correctly specified, misspecification of the short memory dynamics may result in parameter estimators which are less than root-n-consistent and non-Gaussian. These nonstandard asymptotic results lead to a study of asymptotically efficient model selection and Efficient Method of Moments (EMM) estimation for long memory processes, because both procedures assume that a misspecified model has been estimated. <br/><br/>This project is part of ongoing development of a new initiative in Social Science and Statistics at Texas A&M University. A major component of this initiative involves interaction between psychology, economics, political science and statistics to study issues where techniques in time series play the key role in advancing science and decision making.<br/>"
"0313274","ITR DMS:  Advancing the State of the Art in Statistical Computing and Simulation in Time Series","DMS","STATISTICS, ITR SMALL GRANTS","08/15/2003","08/06/2003","Jane Harvill","MS","Mississippi State University","Standard Grant","Leland Jameson","07/31/2006","$250,000.00","Ioana Banicescu, John Lestrade","harvill@math.msstate.edu","245 BARR AVE","MISSISSIPPI STATE","MS","39762","6623257404","MPS","1269, 1686","0000, 1686, 9150, OTHR","$0.00","     The work proposed is a blend of developing new methods in statistics and high performance computing (HPC), and their application to gamma-ray burst (GRB) time profiles to extract embedded information that could help with our understanding of these explosive events. In statistics, the proposed work extends functional coefficient autoregressive (FCAR) models to a multivariate framework and develops estimation and inference methods for FCAR models. FCAR analysis is highly computationally intensive, thus requiring the need for high performance computing (HPC) facilities. Since techniques in FCAR analyses are applicable to a wide variety of disciplines, the computational tools for parallel processors developed are flexible enough to be used in other problems in areas such as environmental modeling or econometrics. There are two principal goals for the analysis of the GRB data. First the multivariate FCAR procedures are applied to the GRB time profiles to determine (1) evolution of time scales during the burst, (2) classification schemes for burst profiles, and (3) evidence of relativistic time dilation, a debated effect that is subtle due to the large dynamic ranges in GRB properties. A secondary goal is the application of kernel density estimation to look for, and catalog, features in GRB count spectra. Additionally, the photon spectrum is described in a more general way by applying FCAR modeling techniques. From a computing perspective, these scientific applications are, in general, large, data-parallel, irregular and computationally intensive. The significant contribution of the HPC component is two-fold. First, a competitive methodology for integrating dynamic scheduling into parallel scientific applications is developed, which is flexible enough to adapt to new emerging technologies, and robust enough to address a wide spectrum of performance degradation factors of implementations running in parallel and distributed environments. Then from this methodology, a software infrastructure for statistical computing is built that integrates advanced parallelization techniques and novel dynamic scheduling methods.<br/>     In this work, the principal investigators develop new statistical techniques for analysis of complex data sets which evolve over time. Of prime importance are the relationships between variables hidden in the data, and these cannot often be described by simple mathematical functions. Once discovered, these relationships are used to describe the underlying physical causal phenomena and to predict future observations. The specific application used in this proposal is the study of the brightness time profiles of gamma-ray bursts (GRB), extremely large cosmic explosions that occur daily in outer space. With the advent of quicker and better GRB afterglow observations, astrophysical models are becoming more sophisticated. Consequently, in order to analyze the time profiles, this study develops more sophisticated statistical tools to uncover subtle clues that indicate what causes these explosive events. The results of this work help form the foundation of the next generation of GRB models. The computing challenges play an essential role due to the development of a software infrastructure that uses state-of-the-art tools and techniques and environments that accommodate new emerging technologies.  <br/>"
"0307631","Pathways to the Future Workshop 2003","DMS","STATISTICS","07/01/2003","03/11/2003","Lynne Billard","GA","University of Georgia Research Foundation Inc","Standard Grant","Grace Yang","12/31/2004","$15,000.00","","lynne@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","Pathways to the Future Workshop 2003<br/><br/>PI: Billard Lynne<br/><br/>DMS-0307631<br/><br/>Abstract<br/><br/>This proposal is for a workshop to be held immediately prior to the Joint Statistical Meetings (JSM) in August 2003 in San Francisco. The workshop is targeted at young women researchers in probability and statistics who have received their doctorates in the last five years. A major talk from a senior researchers focuses on how to develop a research career, the pitfalls to be avoided as participants establish their own academic careers, and general advice on how to be as successful as they can in the research academic environment. The talk itself is the springboard for extended discussions among the junior participants and invited senior researchers governing the issues raised. In addition, workshop participants give a brief presentation of their research interests and research environment. The opportunity to meet and interact with each other as well as a few established researchers is invaluable as the participants learn to promote their own research and to decrease their professional isolation. As a result of the workshop, attendees should be more successful in their own researcher careers, and thus, for example, survive the promotion and tenure process so that these women become role models for the students at all levels (undergraduate and graduate) in their own training programs.<br/>"
"0324767","Southern Regional Council of Statistics Research Conference in Statistics","DMS","STATISTICS","05/01/2003","04/18/2003","Rudy Guerra","TX","William Marsh Rice University","Standard Grant","Xuming He","04/30/2004","$9,600.00","","rguerra@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0324767<br/><br/>Title: The 2003 SRCOS/ASA Summer Research Conference<br/><br/>PI: Rudy Guerra<br/><br/>The 2003 Summer research Conference in Statistics, Sponsored jointly by the Southern Regional Council on Statistics and the American Statistical Association, will be held June 8-11, 2003, Jekyll Island, Georgia. The general philosophy of the SRC is to present and promote cutting-edge research and new directions in statistics, as well as to foster discussion of current and future issues in statistical education and training. For many participants this conference provides an entry into new research topics. Young statisticians are thus afforded a broad view of current activity in the discipline in a relatively short time frame and small conference setting. The SRC in 2003 is a special and timely conference with the theme of ""Statistics in Genetics, Molecular Biology and Bioinformatics"". As these areas are presently of high interest across many disciplines, it is expected that many fruitful collaborations will result among participants. Invited sessions will include statistical genetics, microarrays, genomics, gene networks, proteomic networks, as well as case studies and academic training programs. An opening session will provide background to relevant biological and statistical issues. A Contributed Poster session will highlight junior researchers, including graduate students. The conference website is http://www.stat.uga.edu/SRCOS/2003/src. The SRC will be small, approximately 60-80 participants, which will allow much interaction for the three working days of the conference.<br/>"
"0437555","CAREER:  Methodology for Statistical Computing in Massive Datasets: Parallel Approaches to Cluster and MCMC Estimation","DMS","STATISTICS","07/01/2003","05/12/2010","Ranjan Maitra","IA","Iowa State University","Continuing Grant","Gabor Szekely","11/30/2010","$400,017.00","","maitra@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","0000, 1045, OTHR","$0.00","CAREER: Methodology for Statistical Computing in Massive Datasets:  Parallel Approaches to Cluster and MCMC Estimation<br/><br/>DMS 0239734<br/><br/>PI: Ranjan Maitra<br/><br/>This project is aimed at developing practical methodology for statistical analysis and estimation in massively sized databases.  Because of automated data collection methods, there is now<br/>a surfeit of severely multi-dimensional records.  Grouping them into homogeneous clusters to better understand them is a desirable goal in a variety of applications, yet classical statistical methods are computationally infeasible in many cases.  I propose to develop parallel methodology for this context.  Although the methodology and theory developed will be quite general and for potential use in applications ranging from business, medicine, the environment, software quality assessment, I will conduct the research in the context of three scientific collaborations.  The first pertains to the<br/>US Environmental Protection Agency's (EPA) self-reported Toxic Releases Inventory (TRI) databases, where profiling the different facilities in terms of their product, demographic and business information can improve the accuracy of records, as well as better characterize them vis-a-vis their emissions mix.  The second project is to assess the reliability of functional Magnetic Resonance Imaging (fMRI) scans, with a view to understanding the cognitive processes of the brain, as a first step to patient care and therapy.  The third application is in bioinformatics where the goal is to cluster microarray data and also to analyze two-dimensional proteomic gel images.  This will help in isolating genes and understanding their relationship with different disorders.  <br/><br/>Clustering is in general a very difficult problem, with empirical solutions even for very moderately sized datasets.  I propose to develop multi-pass methodologies in several different scenarios.  I also propose multi-scale simulation approaches to estimation in a high-dimensional context.  One of the biggest challenges faced by simulation methods due to high dimensionality is the low mobility around the space to be traversed because of its vastness.  I propose to address this issue by connecting these high-dimensional spaces to lower-dimensional ones  (which are significantly smaller) and by using these lower scales to traverse from one corner of the higher-dimensional space to another.  A final goal of this five-year plan is to investigate the development and estimation in more complex models for proteomic gel data.  Most of the plans proposed will be possible only with a parallel computing interface.  This is increasingly critical in a large number of scientific applications, and I propose to simultaneously provide statistics students with the necessary expertise by designing suitably tailored graduate and undergraduate classes.<br/>"
"0438240","Efficient Computation in Multi-level Models","DMS","STATISTICS","08/01/2003","06/25/2004","David van Dyk","CA","University of California-Irvine","Continuing Grant","Grace Yang","07/31/2005","$152,100.00","","dvd@uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1269","0000, OTHR","$0.00","EFFICIENT COMPUTATION IN MULTI-LEVEL MODELS<br/><br/>In recent years, a new trend has been growing in applied statistics---it is becoming ever more feasible to build application specific models which are designed to account for the structure inherent in any particular data generation mechanism.  Such models have long been advocated on theoretical grounds, but recently the development of new computational tools (e.g., hardware, software, and algorithms) for statistical analysis has begun to bring such model fitting into routine practice.  Of course, much work remains to be done. The flexibility of such methods comes at a cost---they require problem specific coding, long computation times, and present difficulties in ascertaining convergence.  This proposal aims to tackle some of these difficulties using newly developed efficient Monte Carlo techniques.  The PIs plan to study a number of outstanding theoretical questions concerning the behavior and extended application of these efficient methods by developing new algorithms for a number of important models which are prime candidates for these methods.  The PIs are involved in several on-going substantive data analytic projects (e.g., in computational biology and high energy astrophysics) which both help to clarify the relevant theoretical questions and stand to benefit from the new methodology.  The computational goals of this research are by no means an end unto themselves, but rather a means to improved data analysis and statistical inference. As has been so clearly illustrated in recent years improved computational tools can open up whole new areas of statistical application, as well as increase reliability, thus improving statistical inference.<br/><br/>Research will focus on such newly developed Monte Carlo techniques as multi-point Metropolis and the methods of conditional, joint, and marginal data augmentation.  Multi-point Metropolis generalizes the Metropolis-Hastings algorithm by allowing multiple dependent proposals at each iteration. As a consequence the multi-point method is more able to jump further, is less likely to be caught in a local mode, and thus can substantially improve mixing.  The methods of conditional, joint, and marginal augmentation have already substantially improved performance of the EM and Data Augmentation algorithms in a wide range of models (e.g., mixed-effects models, finite mixture models, multivariate t-models, probit generalized linear models and generalized linear mixed model, Poisson image models, etc.). In particular, these new algorithms maintain the stable convergence properties of EM and DA while sometimes reducing the required computation time by over 99%.  These methods, especially in tandem, have the potential to significantly improve and extend Markov Chain Monte Carlo in statistical practice.  This program is being jointly funded by the<br/>Division of Mathematical Sciences and Astronomical Sciences and the Office of Multidisciplinary<br/>Activities from the Directorate of Mathematical and Physical Sciences.<br/>"
"0306235","Likelihood ratio inference in nonparametric monotone function estimation problems","DMS","STATISTICS","06/01/2003","05/05/2003","Moulinath Banerjee","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Grace Yang","05/31/2007","$105,009.00","","moulib@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>PI: M. Banerjee, DMS-0306235<br/><br/>Title: Likelihood ratio inference in nonparametric monotone function estimation problems<br/><br/>The research program primarily concerns statistical inference using likelihood based methods and especially, likelihood ratios in nonparametric monotone function estimation problems. A distinguishing feature of the monotone function models is a slower (cube root of n) pointwise rate of convergence of maximum likelihood estimators of the monotone function of interest, with a non-Gaussian limit distribution; this property is referred to as ``non-regularity''. While some progress in likelihood based inference for these problems has been achieved<br/>over the past few decades, the behavior of likelihood ratios is by and<br/>large unknown. In this project, the P.I. seeks to develop a theory of<br/>likelihood ratio inference for these ``non-regular'' monotone function<br/>models. This is motivated by the wide applicability of likelihood ratio based inference in regular parametric, semiparametric and nonparametric problems. The emergence of a chi-squared distribution as the limit of log-likelihood ratios allows the construction of test procedures and confidence regions for the parameters of interest, based on the known chi-squared distributions and circumvents the need to estimate nuisance<br/>parameters. It is thus natural to ask whether the advantages of the likelihood ratio paradigm carry over to the domain of shape-restricted (and more particularly, monotone) function estimation. The current research program investigates this for various models and applications of interest. More specifically, the main components of the proposed research program are: (i) Investigation of the universality of the limit, D (ii) Studying monotone function models with measured covariates on the individuals, which is typically the case in applications, from both nonparametric and semiparametric angles<br/>(iii) Developing methods of constructing pointwise confidence sets and confidence bands for monotone functions of interest using likelihood based methods and comparison of these procedures to currently existing methods. Also on the agenda are related research issues, like the study of competing likelihood ratio statistics and the computational and analytical characterization of the associated limit distributions.<br/><br/>The study of shape--restricted functions arises in a wide variety of problems. In particular, monotonicity, which is a very natural shape-constraint appears in many different areas of application, such as reliability, renewal theory, survival analysis, epidemiology, biomedical studies and astronomy. Through its use of attractive statistical concepts like likelihood and likelihood ratios, for estimating monotone functions, this project is expected to have a broad impact on the theory and practice of nonparametric statistics. It will lead to significantly improved methods for analyzing data using likelihood ratio based methods in medicine, public health, reliability and numerous other application areas and will trigger the development of analogous methods of statistical inference in related fields. The ideas and results of this project will also be fruitful in the training and development of future statisticians through inclusion in the curriculum of advanced courses.<br/>"
"0306360","Clustering - Visualization, Validation and Response Oriented Methods","DMS","STATISTICS","06/01/2003","05/08/2003","Rebecka Jornsten","NJ","Rutgers University New Brunswick","Standard Grant","Grace Yang","05/31/2007","$73,499.00","","rebecka@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","Project Abstract:<br/>Clustering - Visualization, Validation and Response Oriented Methods<br/><br/>Rebecka Jornsten, PI<br/><br/>This project focuses on two new universally applicable methods for clustering.  A method is proposed for the clustering of explanatory variables in a response oriented fashion, ROVAC (Response Oriented Variable Clustering).  The method uses a response model to generate the variable clustering, via a novel application of bagging.  The clustering does not rely on a distribution assumption for the explanatory variables.  The method is flexible, allowing for the use of different model selection criteria.  It generalizes to a wide variety of response models.  The PI is also investigating extensions of a clustering visualization and validation tool recently developed, the Relative Data Depth (ReD).  Building on the concept of the depth relative to regression the PI develops methods for selecting the number of clusters in a data set and selecting the features that are related to a specific clustering.<br/><br/>This project is largely motivated by interdisciplinary research.  The goal is to provide scientists in related fields with new and flexible clustering tools for analyzing high-dimensional data.  Standard methods for clustering or grouping of features require the definition of a measure of similarity.  This is often a non-trivial and highly subjective task.  In this project the PI focuses on the development of two clustering techniques based on intuitively simple concepts.  The first method uses the knowledge of another measured quantity, a response.  The method groups features together that are similarly related to the response.  The second method uses a concept of depth, a measure of how representative a feature is with respect to its group.  Prototype algorithms are being implemented on real data with examples from, but not limited to, gene expression data.  Preliminary results are competitive with current leading methodologies.<br/>"
"0305858","Robust Methods for Exploring Multivariate Data","DMS","STATISTICS","06/01/2003","03/08/2005","David Tyler","NJ","Rutgers University New Brunswick","Continuing Grant","Grace Yang","08/31/2006","$211,155.00","","david.tyler@rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>PI: David Tyler (DMS-0305858)<br/><br/>Title: Robust Methods for Exploring Multivariate Data<br/><br/>The overall goal of this research project is to develop computationally feasible, conceptually appealing and theoretically defensible robust methods for exploring and making inferences about a multivariate data set. The main class of estimates to be studied is the multivariate redescending M-estimates with auxiliary scale recently introduced by the investigator. This class of estimates is based upon the key idea of partitioning the scatter component into a nuisance ""scale"" component and a structural ""shape"" component. This partitioning method produces a novel interpretation of robust multivariate estimation problems, and enables concepts from univariate robust statistics and from robust regression to be readily extended to the multivariate setting. In particular, it allows for the generalization of the regression MM-estimates to MM-estimates for multivariate data. Given that the regression MM-estimates are the default robust regression estimates in S-plus, theoretical and computational developments for the multivariate MM-estimates are expected to have wide impact as a standard method in the analysis of multivariate data. Aside from the MM-estimates, the redescending M-estimates with auxiliary scale also include the multivariate S-estimates and the multivariate constrained M-estimates. A general unifying study of the robustness properties, including influence functions, relative efficiencies, and maximum bias functions, of these and other multivariate M-estimates with auxiliary scale is to be undertaken. The methods and ideas underlying the robust estimates of multivariate location and scatter are conceptually broad enough to be extended to other settings, such as to multivariate linear models and to structured covariance problems, and such extensions are to be investigated.<br/>Multivariate location and scatter play a central role in many classical statistical procedures, such as principal component analysis, discriminate analysis, and canonical correlation analysis, which are routinely applied in such diverse disciplines as psychology, biology, geology, and other fields. Hence, the further development of robust estimates for multivariate location and scatter can have a substantial impact on data analysis methods in these scientific areas. Aside from the intrinsic importance of robust estimates of multivariate location and scatter, such estimates also serve as an important first step to a deeper analysis of a high dimensional data set. The development of exploratory methods for multivariate data based on the redescending M-estimates with auxiliary scale is another primary goal of this research project. Such exploratory methods for high dimensional data are pertinent to contemporary data problems arising, for example, in areas such as data mining and in image data. For such data problems, the classical model of data arising as signal plus noise is inappropriate and the data is better viewed as arising as signal plus noise embedded within a mass of clutter. Robust multivariate methods are particularly apt for this latter view of data. The investigator has noted important links between this methodology and methodologies developed in other areas such as cluster analysis and computer vision. A deeper investigation into these links will be undertaken.   <br/><br/>"
"0306008","Statistical Mining of Massive Data, Data Depth and Aviation Risk Management","DMS","STATISTICS","07/01/2003","05/05/2005","Regina Liu","NJ","Rutgers University New Brunswick","Continuing Grant","Grace Yang","06/30/2007","$220,000.00","","rliu@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>PI: Regina Liu, Proposal Number: DMS-0306008<br/><br/>Title: Statistical Mining of Massive Data, Data Depth and Aviation Risk Management<br/><br/>This project aims to develop a systematic data mining procedure for exploring some large non-standard data sets by automatic means, with the purpose of discovering meaningful patterns and useful features. The procedure includes four particular research areas: text analysis, risk analysis, data depth and multivariate nonparametric analysis. The PI proposes to introduce and investigate several new data extracting and tracking methodologies. She plans to use two aviation safety report repositories (""Program Tracking Report Subsystem"" from the FAA and ""Aviation Accident Statistics"" from NTSB) to illustrate problem statements as well as applications of the proposed research to aviation risk management. The data mining procedures and methods for constructing and tracking performance measures or risk indicators developed in this project can be a critical component of any effective decision-support systems. Also, included in this project are: a research plan for establishing a general theory of multivariate spacings based on data depth, and some new nonparametric statistical inference methods using the concept of depth-ranking.<br/><br/>The recent advances in computing and data acquisition technologies have made the collection of massive amounts of data a routine practice in many fields. Besides the voluminous size, the types of data are also often less traditional. They may be textual, image, or unstructured high dimensional data. Scientists face increasingly the task of analyzing such massive non-standard data sets. Moreover, with the low cost of implementing automated data collection systems, many data collection systems are often designed to accumulate maximum amounts of data without clearly defined missions. Consequently, the data analysis required of statisticians often includes the new challenge of mining a sea of unstructured data. The goal of this project is to develop a comprehensive statistical mining scheme that should have a broad applicability to many fields. The investigator plans to use some aviation safety report repositories from the NTSB (National Transportation Safety Board) to illustrate problem statements as well as applications of the proposed research to aviation risk management. The data mining procedures and methods for constructing and tracking performance measures or risk indicators developed in this project can be a critical component of any effective decision-support systems.   <br/>"
"0308151","A Study of Random Matrix Problems Related to Statistics","DMS","STATISTICS","07/01/2003","06/12/2003","Tiefeng Jiang","MN","University of Minnesota-Twin Cities","Standard Grant","Grace Yang","06/30/2006","$100,266.00","","jiang040@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","Within the general area of random matrix problems, the PI will especially focus on problems relevant to the following four types of matrices: (i) orthogonal and unitary matrices; (ii) sample correlation matrices; (iii) matrices with matrix-t distribution; (iv) Toeplitz matrices. Based on the PI's and other authors' work on orthogonal matrices, Diaconis has posed an open problem on how a typical random orthogonal matrix can be approximated by a matrix with independent standard normal random variables as entries. This is the motivation to study (i). Part (ii) comes from a statistical hypothesis testing problem when the dimension of a multivariate population distribution and the sample sizes of data from this population are large. By using Principal Component analysis, the maximum eigenvalue of the sample correlation matrix has to be studied. Part (iii) arises from a statistical study on a problem from Image Analysis. The largest entry of matrices with a matrix-t distribution is of central interest. Part (iv) is an unsolved problem in RTM. This type of matrix arises in time series analysis.<br/><br/>The problems studied come from trading markets, engineering and science. The solutions can bring researchers and practitioners from different fields together to exchange ideas: the study helps practitioners by providing new techniques for use and researchers by obtaining motivation and real problems for solution. Matrices are always behind databases. Random matrix theories may give a clean understanding of databases in a certain sense. For example, the largest eigenvalue of a correlation matrix, which is one of the four proposed problems, can tell if multiple quantities depend on each other or not. Further, this work may help graduate students gain a better understanding of this subject. <br/>"
"0306304","Diagnostics for structured data and quality improvement","DMS","STATISTICS","08/01/2003","07/16/2003","Douglas Hawkins","MN","University of Minnesota-Twin Cities","Standard Grant","Grace Yang","07/31/2007","$156,105.00","","doug@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","DMS-0306304<br/>Diagnostics for Structured Data and Quality Improvement<br/>Douglas M. Hawkins, University of Minnesota<br/><br/>Abstract<br/><br/>The investigator and his collaborators and thesis advisees develop diagnostic methods for a variety of problems linked by their use of data having structure beyond that of a univariate random sample.  Examples include the identification of outliers and unexpected parameter changes in multiple regression, multivariate time series data, and large rectangular data arrays.  Application of these methodologies to statistical quality improvement resolves many deficiencies of current approaches and provides high performance charting methods.<br/><br/>Data sets keep growing in size and complexity.  Proteomics and microarray experiments routine provide data sets of a size scarcely imaginable a decade ago.  This creates a need for sturdy statistical methods to extract information without manual checking and in the face of a fraction of erroneous or missing readings.  The research addresses these needs, by developing appropriate statistical methods and effective ways of implementing them.  At the other end of the size spectrum, just-in-time and the ability to customize in manufacturing create a need for statistical quality control tools able to react quickly, and to relatively small departures from specification.  Change-point methodologies provide sharp tools for this detection and diagnosis.<br/>"
"0241160","Statistical and Computational Approaches for Integrated Genomics and Proteomics Analysis and Their Applications to Modeling G1/S Transition During Yeast Cell Cycle","DMS","INFRASTRUCTURE PROGRAM, STATISTICS, MATHEMATICAL BIOLOGY","01/01/2003","06/03/2005","Hongyu Zhao","CT","Yale University","Continuing Grant","Ashwani K. Kapila","12/31/2006","$1,234,850.00","Michael Snyder, Martin Schultz, Mark Gerstein","hongyu.zhao@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1260, 1269, 7334","0000, 4075, 7303, 7454, OTHR","$0.00","Advances in technologies are changing the field of biology to move beyond genomes to transcriptomes, proteomes and metabolomes.  It has become clear that the combination of predictive modeling with systematic experimental verification will be required to gain a deeper insight into living organisms, therapeutic targeting and bioengineering.  Although the importance of integrating various types of biological data to address scientific questions is well recognized and appreciated, the potential information carried in different types of data may not be fully realized without a sound and comprehensive statistical framework to integrate these data.  In addition, close collaborations among statisticians, biologists, bioinformaticians, and computer scientists are essential to ensure that these statistical methods provide a reasonable description of the biological processes studied and the validity of these methods should be rigorously tested through biological experiments.  In this project, a team of researchers with expertise in statistics, genomics and proteomics, bioinformatics, and computer science will develop an integrated approach to reconstructing biological pathways.  Statistical and computational methods will be developed to better identify transcription factor targets, to integrate yeast two-hybrid data, protein complex data, protein localization data, and gene expression data to infer protein interaction networks, and to further integrate DNA- protein binding data to reconstruct transcriptional regulatory networks.  This project focuses on the G1/S transition during the yeast cell cycle to statistically model and experimentally validate inferred regulatory networks.  In addition, parallel computing methods will be developed to overcome the computing bottleneck in the analysis of large-scale networks.  The resources generated from this project, both computer programs and network information will be made available to the scientific community.  It is anticipated that this project will lead to a statistical framework that can be utilized to dissect biological pathways and also will lead to an approach to integrating expertise from diverse disciplines to address important scientific problems in the post-genome era.<br/>With recent progresses in biotechnologies, it has become reality to collect tens of thousands of gene expression and protein expression levels in humans and other organisms.  In addition, scientists now are able to monitor interactions among proteins and interactions between proteins and DNA sequences, to investigate the location that each gene is expressed, and to study the overall effects on the whole organism of individual genes through large collections of mutation strains.  The availability of such data has led to a revolution in biological and biomedical sciences.  Although there is a great potential and an enormous amount of information in these data, the major challenge is how to best integrate, analyze, and interpret these data to understand biological pathways.  In this project, statistical and computational methods will be developed to integrate various types of data in an effort to reconstruct biological pathways with a focus on the understanding of gene regulations in cell cycle.  The statistical models to be developed will be validated with biological experiments.  Computer programs will be developed and distributed to the scientific community after extensive testing to allow biologists and medical researchers to use these tools to study other biological pathways.  This project will also develop high-performance computing approaches to implementing the developed methods and will involve training activities in the general area of computational biology and bioinformatics. This grant is made under the Joint DMS/NIGMS Initiative to Support Research Grants in the Area of Mathematical Biology. This is a joint competition sponsored by the Division of Mathematical Sciences (DMS) at the National Science Foundation and the National Institute of General Medical Sciences (NIGMS) at the National Institutes of Health.  <br/>"
"0304900","Regression and Deconvolution with Heteroscedastic Measurement Error","DMS","STATISTICS, Methodology, Measuremt & Stats","07/15/2003","07/10/2003","Leonard Stefanski","NC","North Carolina State University","Standard Grant","Grace Yang","06/30/2007","$217,652.00","","stefansk@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269, 1333","0000, OTHR","$0.00","In the first component of the research, ""Unbiased Estimation and Corrected-Score Methods for Heteroscedastic Measurement Error with Replicate Measurements,"" the investigator develops a general approach to statistical inference when data are measured with error. Starting with the assumptions that a valid statistical estimation method is known for error-free data, and that replicate measurements are made of the error-prone variate, the investigator shows how to modify the usual estimation method to eliminate bias induced by measurement error. The key technical advances include the accommodation of heteroscedastic measurement errors and replicate measurements, as well as development of a new Monte Carlo method of unbiased estimation of a normal mean. The author applies the general approach to m-estimation and density estimation, thereby incorporating a broad scope of statistical inference problems. In the second component of the research, ""Deconvolution with Auxiliary Data,"" the investigator explores approaches to the deconvolution problem that exploit auxiliary variables correlated to the variable measured with error. The auxiliary variables play a roll akin to that of instrumental variables and are used to reduce variability in the deconvolution estimates.<br/><br/>The astronomer's measurements of distances to galaxies, the epidemiologist's measurements of subjects' blood pressures, the environmental scientist's measurements of daily air pollution levels, and the sociologist's measurements of subjects' behaviors and attitudes share in common the fact that all are less than perfectly accurate. Measurement error is a pervasive problem in the analysis and interpretation of data that crosses disciplinary boundaries. It is a source of uncertainty that can bias estimates derived from data and lead to erroneous inferences. In this project the investigator develops theory and methods for statistical inference when data are measured with error. The research provides a new solution to a long-standing problem in statistical inference, and uses that solution to provide a comprehensive approach to the analysis of data measured with error. The primary benefit is improved statistical inference in the form of less biased and more accurate estimates calculated from scientific data. Because the prevalence of data measured with error is widespread, the impact of the research will be similarly widespread, finding immediate applications not only to the scientific fields mentioned above, but numerous others as well.<br/>"
"0305749","Detection, Estimation and Optimization Problems in Stochastic Systems, and Applications to Genetics, Engineering and Economics","DMS","STATISTICS","07/01/2003","04/12/2007","Tze Lai","CA","Stanford University","Continuing Grant","Gabor Szekely","06/30/2009","$946,785.00","David Siegmund","lait@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","Proposal: DMS-0305749<br/>PI:  Tze L. Lai<br/>Title: Detection, Estimation and Optimization Problems in Stochastic Systems, and Applications to Genetics, Engineering and Economics<br/><br/>ABSTRACT:<br/><br/>The investigators will study a large number of change-point like problems that arise in industrial quality control, automated fault detection of complex engineering systems and gene mapping.  A common feature of these problems involves the probability that a random field exceeds a high threshold.  A unified analytic approach will be developed to evaluate the relevant boundary crossing probabilities.  Importance sampling techniques and sequential Monte Carlo methods will also be developed to supplement the analytic approximations.  For on-line applications, relatively simple parallel, recursive algorithms, which are not too demanding in computational and memory requirements and yet are nearly optimal from a statistical viewpoint, will be developed. Another direction of research is financial time series and stochastic control problems in financial economics. New statistical models, computational algorithms, and data analysis and forecasting methods will be developed to address a variety of sequential decision, portfolio selection, and pricing problems in investments and financial markets.<br/><br/>The project will address problems of (i) industrial quality control, especially control of complex engineering systems, (ii) gene mapping, i.e., the identification of genomic regions containing a gene (or genes) affecting a trait of interest in humans, model experimental organisms, or agriculturally important crops, and (iii) financial economics, especially time series analysis of financial data.  The problems will be studied by using recent developments in probability and statistical theory, and by extensive experiments involving computer simulations. Computational algorithms will be developed to facilitate applications.  The investigators will also develop new undergraduate and graduate courses in statistical genetics and in financial mathematics and write textbooks for these courses.<br/>"
"0244541","Collaborative Research: Advanced Sequential Monte Carlo Methods and Applications","DMS","STATISTICS","09/01/2003","09/04/2007","Rong Chen","IL","University of Illinois at Chicago","Standard Grant","Gabor Szekely","08/31/2008","$243,567.00","","rongchen@stat.rutgers.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","0000, 1616, OTHR","$0.00","Proposal IDs: DMS-0244638, DMS-0244583 and DMS-0244541<br/>PIs:   Jun Liu, Xiaodong Wang, and Rong Chen<br/>Title: FRG Collaborative Research:  Advanced Sequential Monte Carlo Methods<br/><br/>Abstract<br/><br/>Sequential Monte Carlo (SMC) can be loosely defined as a family of techniques that use Monte Carlo simulations to solve on-line estimation and prediction problems in stochastic dynamic systems.  By recursively generating random samples of the state variables, SMC adapts flexibly to the dynamics of underlying stochastic systems. In this FRG project, the investigators and their colleagues will develop a few advanced SMC methods, including novel operations in SMC (pilot exploration, flexible resampling, tempered SMC, etc.) and the nonparametric SMC framework. To demonstrate their wide applicabilities in solving scientific problems, they will apply the developed methodologies to a wide spectrum of problems found in computational biology (e.g., the discovery of cis-regulatory binding motif modules, progressive multiple sequence alignment, the inference for gene relationships, and chain polymer analyses) and wireless information networks (e.g., the design of adaptive nonparametric receivers in various wireless channels, mobility tracking in wireless networks, handoff the design of adaptive nonparametric receivers in various wireless channels, mobility tracking in wireless networks, handoff management and admission control in cellular networks). The proposed research will significantly enrich and advance the statistical modeling methodology and statistical computation theory and is expected to culminate in the formulation of novel modeling, analysis, and computation techniques in computational biology and wireless information networks.<br/><br/><br/>Stochastic modeling is essential in many application fields ranging from computer vision and engineering to molecular biology and statistical physics. But statistical analyses of these models often pose significant challenges to researchers. The sequential Monte Carlo (SMC) methodology recently emerged in the fields of statistics and engineering has shown great promise in solving a large class of highly complex inference and optimization problems regarding stochastic models, opening up new frontiers for cross-fertilization between statistical science and many application areas. The investigators have been working closely in the past on both theoretical developments of SMC and applications of SMC in computational biology and telecommunications, and have made significant impacts. In the meantime, many aspects of SMC theory are yet to be explored; and new challenges arising from applications in these two areas demand novel SMC strategies to be developed. For example, computational biology and wireless communications are two fronts of vital importance in modern science and engineering. With the explosive growth of research and development in these two areas, efficient and accurate statistical methods that can cope with nonlinear, non-Gaussian, and nonstationary features are in urgent need. It is thus important to continue the interdisciplinary research that has been carried out by the investigators to further advance the SMC theory and to bring this powerful statistical paradigm into the most exciting areas of today's scientific and engineering research and development. In this project, the investigators and their colleagues will continue to develop novel SMC methods and to investigate their theoretical properties. They will also apply the developed methods to a wide spectrum of problems found in computational biology and wireless information networks. The proposed research is expected to culminate in the formulation of novel modeling, analysis, and computation techniques in computational biology and wireless information networks.<br/>"
"0234048","Conference on New Directions in Experimental Design (DAE 2003); May 14-17, 2003; Chicago, IL","DMS","STATISTICS","04/01/2003","12/06/2002","Dibyen Majumdar","IL","University of Illinois at Chicago","Standard Grant","Xuming He","03/31/2004","$23,152.00","Kathryn Chaloner, Angela Dean","dibyen@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","0000, OTHR","$0.00","DMS-0234048<br/>PI: Dibyen Majumdar<br/>Co-PI's: Kathryn Chaloner, Angela Dean<br/>Conference on New Directions in Experimental Design <br/>(DAE 2003, Chicago)<br/>May 15-17, 2003<br/><br/>Abstract<br/><br/>The advancement of statistical theory and methodology over the past years has resulted in the development of sophisticated methods capable of analyzing complex experiments. This has given rise to a demand for efficient designs for these experiments. Development of these designs will require new approaches and new techniques. At the same time, a new generation of enthusiastic and talented researchers in design of experiments will be needed in the years to come to carry out original theoretical and applied research in the emerging areas. The junior researchers of today need to be nurtured and encouraged to assume this role.  In order to address these needs it is intended to bring together senior and junior researchers and practicing statisticians from universities and the industry for a three-day conference in Chicago. The emphasis will be on design of experiments for biomedical and pharmaceutical studies. In particular, there will be featured sessions on: Designs for HIV Studies; Bayesian Methods for Designing Clinical Trials; Chemical Compound Selection from Huge Databases; Crossover Designs; Designs for Random Effects, Variance Components and Hierarchical Models.  The conference will also give prominence to recent innovations in the more traditional areas of experimental design that, besides solving existing problems, enrich the researcher's toolkit.<br/><br/>The principal objectives of the conference are threefold: (i) to focus on future new directions of research on design of experiments in the biomedical and pharmaceutical sciences; (ii) to support, encourage and provide stimulus to junior researchers, that is, recent Ph.D.'s and current Ph.D. students, who are working on, or have a strong interest in, design of experiments; (iii) to create an atmosphere for the exchange of ideas and information, between statisticians and researchers in universities and the pharmaceutical industry.  It is estimated that at least half of the participants will be junior researchers, who will be encouraged to give talks and present posters. Other participants will include leading authorities from universities and the pharmaceutical industry. Junior researchers will be provided the opportunity to be matched with a senior mentor. Ample opportunities will be provided for formal and informal discussions.<br/>"
"0306416","Bayesian Analysis of Competing Risks","DMS","STATISTICS","08/01/2003","07/10/2008","Sanjib Basu","IL","Northern Illinois University","Continuing Grant","Gabor Szekely","01/31/2009","$76,726.00","","sbasu@uic.edu","1425 W LINCOLN HWY","DEKALB","IL","601152828","8157531581","MPS","1269","0000, OTHR","$0.00","DMS-0306416<br/>Sanjib Basu<br/>Bayesian analysis of competing risks<br/><br/>Abstract<br/><br/>The investigator develops new statistical methodology and significantly advances currently available methodology for analyzing censored survival data with competing risks where several potential ""risks"" or causes of failure are operating on a subject or a unit.  The failure time data arising in such contexts are often masked where the cause of failure is not exactly known, but can only be narrowed down to a subset of all potential causes. The investigator (a) develops a Bayesian framework for masked competing risks data using the cause-specific model and compares it to the latent approach, (b) develops flexible semiparametric models for such data and compares them to the parametric models, and (c) develops Bayesian model selection methodology for both parametric and semiparametric models and uses it to compare both model fit and predictive power. The investigator studies inferential methods for estimation of overall and cause-specific survival probabilities from the partially masked data; for estimation of the diagnostic probability of failure from a specific cause, given a masked subset of causes; and for incorporating covariate information through regressors in the setting of masked survival data.<br/><br/>Masked competing risks data arise frequently in biomedical settings, clinical trials, and engineering applications. For example, the competing risks acting on a patient diagnosed with prostate cancer could be prostate cancer; other diseases like hypertension cardiovascular disease, and diabetes; or simply old age (since prostate cancer is a slow-growing disease). In an engineering setting, the failure of a system (such as a computer) could be due to failure of a specific component which may not be exactly identified. The goal of this research is to develop new statistical methodology and significantly advance currently available methodology for analyzing such data. The developed computational methods will be made publicly available so that anyone interested in the analysis of such competing risks data can use these methods.<br/>"
"0306306","Factorial Designs, An Indicator Function Approach","DMS","STATISTICS","07/01/2003","04/21/2003","Kenny Ye","NY","SUNY at Stony Brook","Standard Grant","Grace Yang","06/30/2006","$137,150.00","","kye@aecom.yu.edu","W5510 FRANKS MELVILLE MEMORIAL L","STONY BROOK","NY","117940001","6316329949","MPS","1269","0000, OTHR","$0.00","Proposal ID: DMS-0306306<br/>PI:   Kenny Ye<br/>Title:  Factorial Designs, An Indicator Function Approach<br/><br/>Abstract:<br/>Recent researches show that indicator functions are very effective tools for studying theoretical properties of factorial designs. Initially proposed by Fontana, Pistone and Rogantin (2000) and later modified and generalized by the investigator and his collaborators, indicator functions provide a unified representation for all factorial designs, regular or non-regular, two-level, multi-level or mixed-level. The aliasing structure of a factorial design could be clearly revealed by expanding its indicator function with respect to an orthogonal polynomial basis. An important application of this approach is to study geometric structures of orthogonal arrays, which are essential for response surface methodology but have been largely overlooked in the past. The proposed research seeks not only in depth understanding of factorial designs but also discovery of many new efficient designs. The project pursues three main tasks. First, continue development of theoretical foundation of the indicator function approach and apply it to different types of designs including blocked designs and split-plot designs. Second, generalize minimum aberration criteria to all factorial designs. The criteria should be based on the intrinsic structure of a design but not on a priori specified models. Links to design efficiency and estimation capacity will be thoroughly studied, both theoretically and empirically. Third, develop and apply a sequential algorithm to construct a complete catalogue of non-isomorphic orthogonal arrays of small run sizes.<br/><br/>Factorial designs are vital for investigations in both life and physical sciences, as well as in agricultural and industrial studies. Non-regular designs have an advantage over regular designs at the screening and exploration stage of an investigation. Besides many traditional areas that factorial designs are applied, one particular area that will benefit is biomedical research, in which exploration and screening investigations are pervasive, especially with new technologies such as microarrays. Many new, more efficient designs are to be generated in the proposed research and be disseminated through interdisciplinary collaboration with other members of the scientific communities and interactions with industries. An integrated part of the proposed research is to adopt the indicator function approach in the curriculum of standard experimental design courses. Such change from current curriculum will promote the future applications of non-regular designs in practice. The investigator is also committed to involve students in the proposed research, especially those from underrepresented groups. In addition, the proposed research is expected to open interdisciplinary communication between algebraic geometricians and statisticians.<br/>"
"0245166","Fourth International Workshop on Objective Prior Methodology","DMS","STATISTICS","06/01/2003","11/07/2002","Peter Mueller","TX","University of Texas, M.D. Anderson Cancer Center","Standard Grant","Grace Yang","05/31/2004","$14,000.00","","pmueller@math.utexas.edu","1515 HOLCOMBE BLVD","HOUSTON","TX","770304000","7137923220","MPS","1269","0000, OTHR","$0.00","The funding is for the ""Fourth International Workshop on Objective Prior Methodology,"" to be held June 15-20, 2003, in Aussois, France.  The principal objectives of this meeting are to facilitate the exchange of recent research developments in objective prior methodology, to provide opportunities for new researchers, and to establish new collaborations which will channel efforts into pending problems and open new directions for investigation. An important consequence of this meeting will be to further crystallize objective prior methodology as an established area for statistical research.  Most of the funding will go towards expenses of new researchers. The meeting will be organized to actively encourage participation of new researchers and underrepresented groups, and to provide opportunities for the initiation of international collaborations."
"0305226","Computing Environments for Statistics","DMS","STATISTICS","06/01/2003","09/10/2004","Luke-jon Tierney","IA","University of Iowa","Continuing Grant","Grace Yang","05/31/2007","$295,663.00","","luke@stat.uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1269","0000, OTHR","$0.00","Proposal ID DMS-0305226<br/>PI  Luke-jon Tierney<br/>Title  Computing environments for statistics<br/><br/>Abstract<br/><br/>The investigator explores and develops new principles for the design of statistical software to take advantage of modern computing power. Particular emphasis is placed on exploring the effective use of compilation, name space management, and exception handling for statistical languages, and on developing effective tools and frameworks for parallel computing in statistics.  Pilot implementations are incorporated in open source statistical software systems.<br/><br/>Effective statistical methodology made available through statistical software is critical to the ability of researchers to make maximal use of experimental and observational data, and for the ability of instructors to teach good research practices.  The design principles developed by this research lead to software that improves the ability of researchers, instructors, and other users of statistical methodology to apply this methodology more effectively in scientific research and teaching.  These principles also lead to software frameworks that can be used to more rapidly deliver new statistical methodology to end users.<br/>"
"0306243","Modeling Recombination","DMS","Genetic Mechanisms, STATISTICS","07/01/2003","05/13/2003","Elizabeth Housworth","IN","Indiana University","Standard Grant","Grace Yang","06/30/2007","$120,143.00","","ehouswor@indiana.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","MPS","1112, 1269","0000, 1116, OTHR","$0.00","Proposal: DMS-0306243<br/>PI:  Elizabeth A. Housworth<br/>Title: Modeling recombination<br/><br/><br/>Abstract: <br/><br/>Recombination plays a vital role in the proper disjunction of chromosomes in meiosis and is essential to the production of viable gametes. Many organisms use a strategy called interference to regulate the distribution of crossovers among all recombination events that include simple gene conversions (non-crossovers) as well as crossovers.<br/>Recent evidence suggests that the organisms that also use recombination to pair their chromosomes early in meiosis have an additional set of crossovers that is not subject to interference. The investigator develops the statistical theory to test this two-pathway hypothesis by extending previous techniques used to model the distribution of crossovers subject to interference. The investigator analyzes data from a variety of organisms to determine if the two-pathway model provides a better fit than previous models to recombination data from organisms that use recombination to pair their chromosomes during meiosis. Multivariate optimization techniques contribute to the development of tools for determining the design of experiments to test this hypothesis and to obtain sharp estimates of the model parameters. The investigator explores the advantages of using the two-pathway hypothesis in gene mapping methods by comparing its performance with simulated data to that of methods using no-interference and interference-only models of recombination.<br/><br/>A basic understanding of recombination is important for understanding certain causes of infertility, miscarriages, and birth defects in humans. For instance, trisomy is related to certain exchange configurations on the chromosomes during meiosis being susceptible to<br/>missegregation (for example, an egg receives an extra copy of chromosome 21 and a child resulting from the fertilization of this egg has 3 copies, two from its mother and one from its father, leading to Down syndrome). Trisomy is involved in up to 25% of miscarriages in addition to birth defects. Furthermore, mathematical models for the distribution of exchanges between maternal and paternal chromosomes in meiosis provide the fundamental basis for locating genes, including those that are linked to genetic diseases. Including the most appropriate models for these exchanges in gene mapping algorithms improves their efficiency without increasing the error rate they claim. If the proposed model proves to fit data the best, its inclusion in gene mapping algorithms will become increasingly important due to the use of Single Nucleotide Polymorphisms that provide a large number of markers for finding genes associated with complex disease traits. Additionally, the proposed research intimately connects the statistical methods to a current, empirically testable, biological hypothesis about recombination. The results will be disseminated to both the statistical and biological communities and the activities supported by this grant will include training graduate students in this interdisciplinary area.<br/><br/>"
"0306612","Statistical Integration and Approximation","DMS","STATISTICS","07/01/2003","08/27/2004","Art Owen","CA","Stanford University","Continuing Grant","Grace Yang","06/30/2007","$443,349.00","","owen@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>PI:  Art B Owen<br/>proposal number:  0306612<br/><br/>This project centers on numerical investigation of multi-dimensional functions.  The first application is to integration of these functions.  At present digital nets do not exploit much smoothness in the integrand.  New constructions of randomized digital nets are developed to exploit increased smoothness in the integrand.  While digital nets and integration lattices have been randomized, there is little or no work on randomizing Smolyak rules.  This project develops such randomized Smolyak rules.  The quality of the resulting techniques is studied theoretically and in computational investigations.  The second application is approximation and interpretation of multi-dimensional functions by quasi-regression.  The primary application of interest is to determine the relative importance of various inputs and their interactions to provide diagnostics and interpretation of prediction functions fit in machine learning applications.<br/><br/>A multidimensional function is one that depends on more than one input and often on many inputs.  In many applications one wants to average such a function over all possible inputs.  In finance, the function might be the payoff of a security, the inputs are random future prices, and the average determines a fair price.  In computer graphics, images are often generated by averaging the contributions of many sampled paths of light.  Recently, methods that mix deterministic strategic sampling with random sampling have been successfully applied.  This project develops some new methods of this type and investigates how well they work.  The second application is to determine numerically which of perhaps dozens of input variables to a function is most important and whether the variables act individually or synergistically.<br/><br/>"
"0307055","Simultaneous Statistical Modeling of Several Large Covariance Matrices","DMS","STATISTICS","06/01/2003","05/15/2003","Mohsen Pourahmadi","IL","Northern Illinois University","Standard Grant","Grace Yang","05/31/2006","$82,050.00","","mohsen@math.niu.edu","1425 W LINCOLN HWY","DEKALB","IL","601152828","8157531581","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>PI: Mohsen Pourahmadi, DMS-0307055<br/><br/>TITLE: Simultaneous Statistical Modeling of Several Large Covariance Matrices<br/><br/>This research focuses on the development of a three-stage statistical model-fitting process consisting of model formulation, estimation and diagnostics for contemporaneous covariance matrices of large multivariate responses arising, for example, in business and economics, epidemiology, environmental monitoring and global change, biotechnology and manufacturing (quality control). Correlation and covariance matrices and their spectral (eigenvalue) decomposition provide the basis for all classical multivariate techniques. For temporal correlations, however, the Cholesky decomposition lies at the core of most time series techniques. A growing body of work in step-down procedures in multivariate statistics and multivariate stochastic volatility models implicitly rely on the Cholesky decomposition or view each long random vactor as a ""time series"", for a given arrangement of the variables. The proposed work intends to make explicit and reveal the full potential of using Cholesky decomposition or time series techniques in modeling contemporaneous covariance matrices. It includes developing parsimonious models, their estimates and asymptotic properties, their practical implementation and uses, with an eye to guaranteeing the positive-definiteness of the covariance matrices at each stage of an iterative procedure used to compute the maximum likelihood estimates of the parameters. A major difficulty of the implementation for high-dimensional unordered vectors is the large number of possible arrangements of the variables. The methods and tools to be employed include: generalized linear models, factor analysis and random effects models, time-series model fitting process, maximum likelihood and Bayesian estimation and numerical linear algebra. The proposed research has the potential of elevating the Cholesky decomposition as a bona fide tool for modeling temporal and contemporaneous correlations and hence connecting (unifying) disparate areas like time series analysis, factor analysis and linear structural models, graphical models and Bayesian covariance modeling.<br/><br/>Advances in technology and data collection have enabled researchers to record measurements on many characteristics of systems over finer units of time. The research proposed is motivated by statistical problems arising in settings where large amounts of multivariate data are available and the focus is on prediction, control, classification, clustering and data mining. The reliability or error rates of these tasks invariably hinge on the precise estimation of correlations among many variables and better understanding of the dynamics of large covariance matrices. The goal of the proposal is to provide a systematic and efficient method for analyzing high-dimensional data through modeling of the relevant covariance matrices. The proposal intends to use classical and recent statistical estimation and large-scale computing. Results obtained by the proposed research will have applications in diverse areas outlined above, however, particular attention will be paid to their potential use in understanding intelligence in machines and brains. A student will participate in the proposed research during summers.  <br/>"
"0306497","Asymptotic Equivalence of Statistical Experiments","DMS","STATISTICS","06/01/2003","05/07/2007","Michael Nussbaum","NY","Cornell University","Continuing Grant","Gabor Szekely","05/31/2009","$373,735.00","","nussbaum@math.cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0306497<br/><br/>PI: Michael Nussbaum<br/><br/>TITLE: Asymptotic Equivalence of Statistical Experiments<br/><br/>An experiment is a family of probability measures; a distance can be defined between these objects such that the information they contain about an unknown parameter is similar if this distance is small. This basic deficiency pseudo-distance (or Delta-distance) is well known; it generalizes the concept of sufficiency: if experiments are equivalent via sufficiency then their Delta-distance is zero. The idea of data reduction which is at the heart of the concept of sufficient statistics can be combined with limit theorems of probability, resulting in an approximation of general statistical models by Gaussian location or Poisson families. These families allow explicit expressions for risk bounds in many instances, and these risk bounds then become valid in an asymptotic sense in the approximated models. It is proposed to extend the scope of  approximations for nonparametric experiments of independent data from Gaussian to infinitely divisible experiments, including the Poisson case. Furthermore, a class of stochastic process experiments will be studied with regard to asymptotic equivalence. More insight is also sought into the connections to statistical information theory, and in a longer term perspective, to other topics in statistical inference which seem mathematically challenging such as e.g. differential geometry of probability measures and quantum statistics. <br/><br/>Thus the principle is to approximate a given statistical model by another which is better known or more tractable. In the widest sense, this is related to the familiar approximation of the mean of a simple random sample by a normal or Z-distribution (also known as Bell curve), taught in elementary statistics courses, but complex mathematical problems arise when one has to deal with data or parameters of very high or even infinite dimension. Two Ph. D. students will be constantly associated to the project; the aim is to educate young statisticians with distinctly mathematical interests who promise to be future leaders in academic research. This project is anchored in a mathematics department that has a major research tradition in statistics. <br/>"
"0239734","CAREER:  Methodology for Statistical Computing in Massive Datasets: Parallel Approaches to Cluster and MCMC Estimation","DMS","STATISTICS","06/01/2003","04/07/2003","Ranjan Maitra","MD","University of Maryland Baltimore County","Continuing Grant","Grace Yang","08/31/2004","$172,467.00","","maitra@iastate.edu","1000 HILLTOP CIR","BALTIMORE","MD","212500001","4104553140","MPS","1269","0000, 1045, OTHR","$0.00","CAREER: Methodology for Statistical Computing in Massive Datasets:  Parallel Approaches to Cluster and MCMC Estimation<br/><br/>DMS 0239734<br/><br/>PI: Ranjan Maitra<br/><br/>This project is aimed at developing practical methodology for statistical analysis and estimation in massively sized databases.  Because of automated data collection methods, there is now<br/>a surfeit of severely multi-dimensional records.  Grouping them into homogeneous clusters to better understand them is a desirable goal in a variety of applications, yet classical statistical methods are computationally infeasible in many cases.  I propose to develop parallel methodology for this context.  Although the methodology and theory developed will be quite general and for potential use in applications ranging from business, medicine, the environment, software quality assessment, I will conduct the research in the context of three scientific collaborations.  The first pertains to the<br/>US Environmental Protection Agency's (EPA) self-reported Toxic Releases Inventory (TRI) databases, where profiling the different facilities in terms of their product, demographic and business information can improve the accuracy of records, as well as better characterize them vis-a-vis their emissions mix.  The second project is to assess the reliability of functional Magnetic Resonance Imaging (fMRI) scans, with a view to understanding the cognitive processes of the brain, as a first step to patient care and therapy.  The third application is in bioinformatics where the goal is to cluster microarray data and also to analyze two-dimensional proteomic gel images.  This will help in isolating genes and understanding their relationship with different disorders.  <br/><br/>Clustering is in general a very difficult problem, with empirical solutions even for very moderately sized datasets.  I propose to develop multi-pass methodologies in several different scenarios.  I also propose multi-scale simulation approaches to estimation in a high-dimensional context.  One of the biggest challenges faced by simulation methods due to high dimensionality is the low mobility around the space to be traversed because of its vastness.  I propose to address this issue by connecting these high-dimensional spaces to lower-dimensional ones  (which are significantly smaller) and by using these lower scales to traverse from one corner of the higher-dimensional space to another.  A final goal of this five-year plan is to investigate the development and estimation in more complex models for proteomic gel data.  Most of the plans proposed will be possible only with a parallel computing interface.  This is increasingly critical in a large number of scientific applications, and I propose to simultaneously provide statistics students with the necessary expertise by designing suitably tailored graduate and undergraduate classes.<br/>"
"0355179","Inferences for Multivariate Semiparametric and Nonparametric Models with Applications to Risk Management","DMS","STATISTICS","09/01/2003","10/03/2003","Jianqing Fan","NJ","Princeton University","Standard Grant","Grace Yang","07/31/2005","$175,778.00","","jqfan@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>PI: Jianqing Fan<br/>DMS-0204329<br/><br/>The objectives of this proposal are to develop new and widely applicable approaches for semiparametric and nonparametric estimation and inferences, to study theoretical properties of these new approaches, and to evaluate their efficacy in data analyses. This proposal not only introduces a number of innovative techniques, but also provides various new and deep insights into statistical foundation. It will have significant impact on the future research of statistical methodologies, computation and theories. In particular, three inter-related areas are proposed for study. Firstly, a family of flexible semiparametric and nonparametric models is introduced. This allows one to study the extent to which response variables are associated with their covariates. The generalized likelihood ratio statistics is proposed for testing various hypotheses in multivariate semiparametric and nonparametric models. Secondly, new semiparametric and nonparametric models are proposed for understanding interest-rate dynamics, and stock price volatilities. Furthermore, the information on state-domain is incorporated to improve the efficiency of volatility estimation for bonds and to more accurately estimate the market risks of a portofolio. Thirdly, new techniques for variable selection, in the presence of a large number of variables, are proposed via nonconcave penalized likelihood. The innovation is that they estimate parameters and select variables simultaneously. <br/><br/>The above techniques are widely applicable to many scientific and engineering problems. Multivariate nonparametric, semiparametric and large parametric models have been widely used. Statistical questions often arise such as if certain variables or factors are important to public health; if some risk factors contribute significantly to the survival time of patients; and if interest-rate dynamics or stock price processes are time-dependent or follow certain famous hypotheses, among others. Yet, there are no generally applicable tools available to answer these questions in multivariate semiparametric and non-saturated nonparametric models. The techniques proposed here permit one to objectively test scientific hypotheses without restrictive model assumptions. The techniques allow to better price financial derivatives and manage investment risk, to identify important risk variables and their possible interactions in the analysis of large epidemiological studies and to scrutinize famous hypotheses on stock prices <br/>"
"0303688","ISI 2003 Travel Grant","DMS","STATISTICS","03/15/2003","03/12/2003","William Smith","VA","American Statistical Association","Standard Grant","Xuming He","02/29/2004","$12,000.00","","bill@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","0000, OTHR","$0.00","ISI 2003 Travel Grant<br/><br/>PI: W. B. Smith<br/><br/>Proposal Number: DMS-0303688<br/><br/>Abstract<br/><br/>The American statistical Association (ASA) will use this travel grant for 12 United states participants to attend the 54th Session of the International Statistical Institute (ISI) in Berlin, Germany, from august 13-20, 2003. The ISI meeting holds an umbrella meeting of several allied societies with sessions of interest for thousands of statisticians. This travel grant will provide travel awards to partially defray transportation costs for individuals selected from educational institutions and other non-profit organizations. An emphasis of the award is to encourage and provide opportunity for younger statisticians to participate in the meeting.<br/><br/>Participants will be notified of the availability of the travel grant through AMSTAT News, a membership publication with over 16,000 circulation and on the ASA Home Page on the internet. Also, notices will be sent to university and college Statistics departments to encourage younger statisticians to apply for travel support. An ASA committee will do the review and selection of applications. Special consideration will be given to statisticians who have recently received their doctorates and to women and minorities in order to broaden research, educational and networking opportunities. <br/>"
"0343462","Workshop on Opportunities at the Statistics-Operations Research Interface","DMS","STATISTICS, OPERATIONS RESEARCH","11/15/2003","11/12/2003","William Smith","VA","American Statistical Association","Standard Grant","Xuming He","10/31/2004","$50,000.00","Mark Doherty","bill@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269, 5514","0000, OTHR","$0.00","ABSTRACT:  DMS-0343462<br/><br/>Workshop on Opportunities at the Statistics-Operations Research Interface<br/><br/><br/> The American Statistical Association (ASA) and the Institute for Operations Research and Management Science (INFORMS) are conducting a small workshop bringing together research leaders in statistics and operations research to increase the interaction at the interface between these research areas.  This small conference is structured to focus on research activities from these two increasingly divergent communities in order to improve communications between research groups, break down delimiting barriers, share research techniques, and investigate emerging topics.<br/><br/> ASA and INFORMS are two professional societies that represent the interests of most active researchers, practitioners, and educators in statistics and operations research.  Many immediately practical and/or deeply difficult research problems fall at the traditional interface of these disciplines, e.g., impacts and techniques of computer simulation, variational problems in optimization, design and analysis of production data, optimal design in manufacturing processes and experiments, and educational challenges made more difficult by the rapid expansion of the knowledge base in each area.<br/><br/> Thus, these sponsoring professional societies are organizing and conducting this workshop devoted to new research and applications at the interface.  Participants are leading academic researchers, government laboratory researchers, industry research leaders, and influential educational representatives.  Research topics that are fundamental to the two areas are being discussed, applications of these topics to problems of national concern are addressed and a conference report, reporting the results is produced.  It is this latter document that is enabling the discussion to spread well beyond the bounds of the workshop.  This document is produced by the steering committee with contributions from speakers and participants.  The report is made available on the ASA and INFORMS web pages for general discussion by members of the sponsoring societies, and to the general mathematical and engineering sciences communities.  <br/><br/>Thus, the workshop addresses open research problems, discusses the use of novel and/or developing techniques to address these problems and stimulates new collaborations between the researchers in the two disciplines.  Plans are developed to insure that the participants are from diverse backgrounds, with representatives from minority groups and women being included.<br/><br/>"
"0307616","VII Symposium of Case Studies in Bayesian Statistics","DMS","STATISTICS","06/01/2003","02/13/2003","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Xuming He","05/31/2004","$15,000.00","","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, OTHR","$0.00","Proposal ID:  DMS-0307616<br/>PI:  Robert E Kass<br/>Title:  VII Symposium of Case Studies in Bayesian Statistics<br/><br/>Case Studies in Bayesian Statistics VII is the seventh workshop in the series that was begun in 1991. The workshops are held in odd years at Carnegie Mellon University in early fall. The seventh workshop is planned for September 12-13, 2003. The goals of the workshop are to<br/>(i) emphasize the close interplay of statistical theory and applications in the context of substantive scientific research;<br/>(ii) promote the continued development of Bayesian statistics, by highlighting problems in the sciences that require non-standard approaches, thereby requiring theoretical and methodological developments for their solution;<br/>(iii) provide an opportunity for scientists and statisticians to present their work in depth, highlighting both the scientific background and the analytical approaches, for the benefit of the audience;<br/>(iv) encourage young researchers and graduate students to present their work, interact with senior colleagues, learn about the latest developments in Bayesian statistics, and participate in discussions, by providing a small-meeting atmosphere;<br/>(v) highlight the many research opportunities that exist for statisticians who engage in interdisciplinary work;<br/>(vi) include as participants women and under-represented minorities who might benefit from the small workshop environment and the opportunities for one-on-one discussions with colleagues at other institutions;<br/>(vii) disseminate the findings presented at the workshop by publishing a volume or special issue of a journal containing well-documented and peer-reviewed case studies and related workshop presentations, and by posting the same information on the workshop's web pages (www.stat.cmu.edu/bayesworkshop).<br/><br/>"
"0304922","Semiparametric Regression Modeling for Longitudinal Data","DMS","STATISTICS","07/01/2003","06/10/2003","Yanqing Sun","NC","University of North Carolina at Charlotte","Standard Grant","Grace Yang","06/30/2007","$145,334.00","","yasun@uncc.edu","9201 UNIVERSITY CITY BLVD","CHARLOTTE","NC","282230001","7046871888","MPS","1269","0000, OTHR","$0.00","In this proposal, the investigator proposes a semiparametric method for joint modeling of longitudinal responses and measurement times. The semiparametric varying-coefficient regression model postulates that the influences of some covariates vary nonparametrically with time while the effects of the remaining covariates follow certain parametric functions of time. The measurement times of responses are modeled by Cox's proportional model for conditional mean rate. The modeling by Aalen's additive model will also be studied. The investigator employs the approach of Lin and Ying (2001) to estimate the nonparametric regression coefficients. The parametric components are estimated by a weighted least squares method. More efficient smoothing techniques are used for response processes. The investigator proposes a data-driven bandwidth selection criterion based on the sum of squares of residuals. This selection method will be further studied and tested empirically through simulations. Constructions of confidence bands for the parametric components, nonparametric regression coefficient functions and their cumulatives are proposed. Goodness-of-fit test procedures are proposed to check whether some regression coefficient functions follow certain parametric forms by comparing the nonparametric estimators and the corresponding estimators of the cumulative regression coefficient functions under the null hypothesis. A preliminary simulation study demonstrates that the proposed estimation and hypothesis testing methods are promising. The investigator will study the asymptotic properties, such as consistency, weak convergence and consistency of the estimators of the asymptotic variances, for all the proposed estimators. Finite sample properties of the proposed estimation methods and goodness-of-fit tests will be evaluated through extensive numerical simulation studies. The investigator also proposes to study related problems in optimal choice of the weight function, optimal design of measurement times and informative censoring.<br/><br/>The investigator proposes to seek statistical models with a physical or biological basis and biologically interpretable parameters and to develop statistically efficient methods to better understand linear or nonlinear behavior of response process. The proposed semiparametric varying-coefficient regression model allows for additional flexibility to explore how covariate effects change over time and provides a base to test whether simpler models with scientific relevance hold. The proposed research when carried out would help to enrich a collection of statistical tools which have important impact on the analysis of longitudinal data. Giudelines for practical applications will be developed. The methods will be used to analyze longitudinal data in AIDS related medical research to develop more effective treatments and to other real examples in medical studies. The research of the problems proposed here will also generate many research topics at different levels suitable for graduate and undergraduate studies, therefore promotes involvement of students in current scientific research.<br/>"
"0243594","Stochastic Models and Inference for the Reliability of Complex Systems","DMS","STATISTICS, MANFG ENTERPRISE SYSTEMS","08/01/2003","05/01/2005","James Lynch","SC","University South Carolina Research Foundation","Continuing Grant","Grace Yang","01/31/2007","$224,316.00","William Padgett","lynch@stat.sc.edu","915 BULL ST","COLUMBIA","SC","292084009","8037777093","MPS","1269, 1786","0000, OTHR","$0.00","Important aspects of modeling the failure or reliability of complex systems are (i) the modeling of the component interactions and (ii) incorporating information about each component into the system.  Both are needed to relate the system damage caused by component failure to the failure of the entire system.  Here, new approaches to these problems are taken.  The first approach is based on load-sharing systems of components, where the interactions among components are modeled by load-sharing rules.  For examples, for a mechanical system undergoing an increasing load, such as a fibrous composite material under tensile loading where fiber segments are the components, or a routing system under increasing ""traffic"" where the nodes are components, the load-sharing rule describes how the tensile load or traffic is transferred/redistributed from failed components to working components.  The second approach is based on an entropy/information formalism where damage/destruction in the system is quantified in terms of hazard and reverse hazard functions.  These new approaches should lead to more realistic stochastic or probabilistic models for the failure of general systems such as those mentioned.  In addition, many complex systems, or pieces of equipment, degrade over time or under increasing load before they fail, and modeling such degradation for prediction of failure is an important part of this project.  Engineering degradation tests can often be performed at regular intervals to measure the levels of the degradation processes of such systems.  The resulting degradation data, along with any actual failure data, can be used to fit models which provide estimates of the failure distributions or give a basis for prediction of a degradation threshold that causes system failure.  Analogously, the same approaches to degradation modeling can be utilized in the progression of a disease toward a meaningful endpoint in medical or health settings.  Hence, in this research project, development of models for degradation and failure will be undertaken that involve cumulative damage concepts and result in tractable approaches for statistical inference based on known, but perhaps little-used, distributions, such as inverse Gaussian-type or Birnbaum-Saunders-type distributions.  In particular, covariates, or acceleration variables, will be included in the models, and classical inference, as well as Bayesian analysis, will be investigated for these general cases.<br/><br/>Accurate prediction of the failure of pieces of equipment or general systems is essential in decision making concerning maintenance or replacement policies for such systems.  This is an especially important factor in preventing catastrophic failures of key systems or equipment during critical operations.  The overall objective of this research project is to address the above issue by (1) developing new mathematical models for system failure under more realistic conditions and assumptions about the system, taking into account physics-of-failure considerations, and (2) developing new procedures to make inferences about system failure based on either observed failures of such systems or observed levels of the degradation of the system over time, or both.<br/>"
"0426382","Design and Analysis of Experiments for Screening, Optimization and Robustness","DMS","STATISTICS, MANFG ENTERPRISE SYSTEMS","09/01/2003","03/15/2004","C. F. Jeff Wu","GA","Georgia Tech Research Corporation","Continuing Grant","Grace Yang","08/31/2005","$152,098.00","","jeffwu@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269, 1786","0000, 9147, MANU, OTHR","$0.00","Abstract:<br/><br/>The goal of this proposal is to study three important aspects of experimentation: screening, optimization and robustness. Section I proposes a novel approach to factor screening and response surface exploration by using a single design and experiment to achieve both objectives. This differs from the standard response surface methodology, which employs separate designs for factor screening and for response surface exploration.  New concepts, theory and analysis are proposed, which include a two-stage analysis and a projection-efficiency criterion.  Four problems are to be studied: (i) a theory for eligible projections in regular designs, (ii) combinatorial and algorithmic construction of optimal nonregular designs, (iii) connection with the maximum estimation capacity criterion, (iv) sensitivity of response surface exploration to errors in factor screening and a Bayesian alternative to the two-stage analysis. Section II addresses a fundamental and practically important issue of optimal assignment of factors to columns of a design matrix. Existing work can only be applied to regular fractional factorial designs and nonregular designs with two-level factors. By defining a B-contamination criterion and employing the Kronecker calculus, we propose an approach that can handle very general designs.  Three problems are to be studied: (i) Finding expressions for the contamination terms, (ii) characterization in terms of complementary designs, (iii) extensions to blocked designs. Section III addresses the issue of optimal selection of experimental plans for robust parameter design.  When the experimental cost is proportional to the total run size, the cross array format can be quite costly and the single array format becomes an attractive option.  An important question is how to select single arrays optimally and according to what criteria? By using an effect ordering principle, we propose to define new criteria and use them to select optimal single arrays.    <br/><br/>Statistical design and analysis of experiments is an effective and commonly used tool in scientific and engineering investigation. It has made significant impact in many areas of research and development such as manufacturing, electronics, materials, agriculture and energy. It will continue to make important contributions by innovation in methodological and theoretical development and applications in new areas such as biotechnology, drug discovery, and information technology. Potential gains from using the proposed new methods include savings in experimental runs, experimentation time, and discovery of new/better engineering designs and products. The results on factor assignment will provide clear guidelines on the assignment of factors and a substantial improvement over the prevailing practice of making arbitrary and often suboptimal assignment.  Parameter design has become a major tool for variation reduction and product and process improvement. The proposed work will develop new and more economical and efficient techniques for conducting such experiments. <br/><br/> <br/>"
"0306508","Boosting, Support Vector Machines, and Cloud Detection over Ice and Snow","DMS","STATISTICS, DES AUTO FOR MICRO & NANO SYS","08/01/2003","07/17/2003","Bin Yu","CA","University of California-Berkeley","Standard Grant","Grace Yang","07/31/2007","$274,999.00","","binyu@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269, 4710","0000, OTHR","$0.00","DMS-0306508<br/>Bin Yu<br/>Boosting, support vector machines, and cloud detection over ice and snow<br/><br/>Abstract<br/><br/>Classification and regression are two fundamental statistical problems posed by the Information Technology (IT) age and at the same time aided by the computational tools it provides.  Boosting and Support Vector Machines (SVMs) are two revolutionary methodologies from machine learning for regression and classification that meet the needs of massive data sets of our IT age. The investigator and her colleagues study when and why boosting and SVMs work to shed light on the design or tuning of Boosting and SVMs; in particular, understanding on the choice of the base learner in L2Boosting and the effect of the input density distribution on the Reproducing Kernel Hilbert space induced by the kernel in an SVM.  Theoretical understandings on boosting and SVMs are then used, in collaboration with colleagues at Jet Propulsion Laboratory, in the cloud-detection problem over ice/snow based on Multi-angle Imaging SpectroRadiometer (MISR) data.  The investigator and her colleagues also develop a novel cloud detection algorithm based on linear correlations using MISR data and compare this with the boosting and SVM based approaches.<br/><br/>Information Technology (IT) is changing just about every facet of our lives.  At a professional level, emerging innovations in IT areas represent tremendous opportunities for statistical research, providing both fundamental methodological challenges as well as applications with real-world impact.  Advances in data collection and computing technologies have led to the proliferation of massive data sets such as those from remote sensing. Understanding the role of statistics in such data-rich applications forces us to reevaluate and revise traditional procedures and frameworks. The investigator and colleagues study Boosting and Support Vector Machines (SVMs), which are two revolutionary methodologies from machine learning for regression and classification that meet the needs of massive data sets of our IT age. In collaboration with colleagues at <br/>NASA's Jet Propulsion Laboratory, they apply the research results from these studies to the problem of cloud detection, which is a crucial step in any climate prediction or modeling including weather forecasting and global warming monitoring.  Multi-angle Imaging SpectroRadiometer (MISR) was launched in 1999 by NASA to provide 9 angle (4 band) data to retrieve or estimate the cloud height and hence cloud detection. However, cloud detection even with MISR data has been proven very difficult over ice/snow. The investigator and her colleagues develop a novel cloud detection algorithm based on linear correlations using MISR data to work over ice/snow and compare it with boosting and SVM based approaches. <br/>"
"0308331","High Dimension - Low Sample Size Statistical Analysis","DMS","STATISTICS","08/01/2003","07/26/2005","James Marron","NC","University of North Carolina at Chapel Hill","Continuing Grant","Grace Yang","07/31/2007","$185,592.00","","marron@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","0000, OTHR","$0.00","DMS-0308331<br/>James S. Marron<br/>High dimension - low sample size statistical analysis<br/><br/>Abstract<br/><br/>Research is proposed in the areas of medical image analysis and the gene expression micro-array analysis.  These seemingly divergent fields have a common structure.  The over-arching abstraction that contains them both is ""High Dimension - Low Sample Size"" (HDLSS).  HDLSS data are completely outside of the domain of classical statistical multivariate analysis, because the critical first step of ""sphering the data"" can no longer be performed.  This motivates the development of a very large new toolbox of statistical methods, which will be useful in fields far beyond those motivating this research.  An important part of the proposed analytical work is the development of a completely new type of asymptotic analysis, where the sample size is fixed, and the dimension tends to infinity.<br/><br/>The proposed research will be on the development and assessment of new data analytical methods.  These will provide critical statistical infrastructure for the large teams of people that are tackling current major challenges in the field of medicine.  The first challenge is the ongoing war on cancer.  The second challenge is to make the rapidly developing breakthroughs in medical imaging technology much more useful to practicing medical professionals.  Both areas will be substantially benefited by the creative new methods of statistical analysis that will be developed during the course of this research.<br/>"
"0305009","Generalized Linear Models","DMS","STATISTICS","08/01/2003","08/19/2008","Peter McCullagh","IL","University of Chicago","Continuing Grant","Gabor Szekely","07/31/2009","$539,443.00","","pmcc@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00","PI:  Peter McCullagh<br/>Title:  Generalized Linear Models<br/><br/>ABSTRACT<br/><br/>Together with students and colleagues, the PI investigates various topics, all bearing directly or indirectly on parametric statistical models.  The main theme is the development of a framework for constructing logically consistent statistical models, whose hallmark is extendibility or scope.  The PI proposes to develop this notion to a variety of settings, such as cluster-analysis models, where the objects that arise are not vector spaces but partitions in the sense of clusters, or trees in the sense of recursive partitions.  A second application is to spatial variation in agricultural field trials.  Conformal invariance in agriculture is a radical hypothesis with specific testable consequences.  The available evidence to date seems to favor conformal invariance, but spatial parameters are not easy to estimate accurately, so the case is not firmly established.  The PI aims to study the implications of conformal invariance and to test the hypothesis on as wide a range of agricultural data as possible.  Software capable of fitting these models in routine agricultural applications is an important by-product.  Monte Carlo integration is a venerable topic, but the interplay between statistical models and Monte Carlo integration is a relatively new development with promise of substantial payoff for statistical computation.  Standard Monte-Carlo designs for the estimation of integrals achieve the standard rate of convergence, but super-efficient designs can achieve faster rates for specific integrals.  The PI and his colleagues study the phenomenon of super-efficiency, when and how it occurs, to see whether it can be exploited in routine statistical applications.  As an example, eigenvalue processes have the potential to be used as quadrature points for integration in suitable circumstances.<br/><br/>The over-riding goal is to achieve a better understanding of statistical models as processes, initially in the mathematical sense and if possible in a mechanistic or physical sense.  The notion in agricultural field experiments that ""only local spatial properties matter"" is translated into mathematics, emerging as a specific hypothesis with testable consequences.  Conformal invariance is unlikely to have economic or agricultural implications, but if confirmed it demands a re-thinking of current ideas concerning the nature and causes of spatial variation in terrestrial processes.  The work on conformal invariance is aimed specifically at agricultural field trials, but might well be applicable elsewhere, for example processes on the celestial sphere (sky).  As a by-product, statistical methods and software are produced specifically for fitting and testing conformal models.  The last topic concerns Monte-Carlo integration, which has already had a big impact in statistical computation.  The present proposal indicates ways in which those methods might be extended and made more effective.<br/><br/>"
"0311766","An International Workshop on Bayesian Data Analysis","DMS","STATISTICS","05/01/2003","03/20/2003","David Draper","CA","University of California-Santa Cruz","Standard Grant","Grace Yang","04/30/2004","$12,000.00","","draper@ams.ucsc.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1269","0000, OTHR","$0.00","An International Workshop on Bayesian Data Analysis<br/>PI: David Draper<br/>Proposal: DMS 0311766<br/><br/>                                Abstract<br/><br/>Bayesian statistical methods have gained considerably in power, scientific relevance, and applicability over the last 10 years, with the widespread adoption of simulation-based computation methods involving Markov chain Monte Carlo techniques as the engine of progress. Cutting-edge Bayesian modeling is now going on in a wide variety of fields, including bioinformatics, computer science, engineering, epidemiology, machine learning, and statistics. Interdisciplinary transfer of ideas is important but increasingly difficult to achieve, as the literatures in each field burgeon and the use of different terms for similar concepts hampers the ability of search engines to help workers in one field keep up-to-date in other disciplines. Bayesian methodology is important to study in its own right, but for greatest scientific relevance (1) careful attention to modeling and (2) the need for methodology to be appropriate to context are also crucial.<br/><br/>In this project the Statistics Group within the newly-forming Department of Applied Mathematics and Statistics (AMS) at the University of California, Santa Cruz (UCSC), will address these issues by hosting an International Workshop on Bayesian Data Analysis at UCSC from Friday through Sunday 8-10 August 2003, as a kind of satellite meeting to be held right after the Joint Statistical Meetings (JSM) nearby in San Francisco, CA, from August 3--7, 2003. Our objectives are to bring together about 100 people interested in Bayesian modeling and data analysis from a wide variety of disciplines around the world, including about 30 invited participants (leading Bayesian researchers) as speakers and discussants, to promote multidisciplinary discussion of common problems and interdisciplinary transfer of ideas (many of these people will be young researchers). The focus of the Workshop will be Bayesian data analysis: starting with a real problem in science or decision-making, formulating this problem in statistical terms, using Bayesian methods to solve the original problem, and discussing the strengths and weaknesses of the solution both statistically and substantively, with plenty of attention to the interplay between the real-world context and the Bayesian model building, checking, and reformulating. NSF funding will be principally used to provide partial travel and local expenses support for young researchers, minorities, and other under-represented groups.<br/><br/><br/>"
"0306202","Statistical Analysis of Networked Point Processes","DMS","STATISTICS","06/01/2003","10/06/2010","Jiayang Sun","OH","Case Western Reserve University","Continuing Grant","Gabor Szekely","08/31/2011","$257,032.00","","jsun21@gmu.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","MPS","1269","0000, OTHR","$0.00","Motivated by problems arising in the analysis of neural data, the investigator and colleagues will develop new statistical methods for the analysis of functional data, and in particular to study multiple replications of point processes. The main question of interest is understanding the structure of the rate functions, and understanding how the rate functions relate to covariates. Particular attention will be given to the time distortion, or latency problem, where differences in the time scale among different replications are considered. Such time-scale differences are of particular interest in neural data applications, as they represent different speeds at which subjects complete tasks in response to a stimulus. The research will develop new methods, based on tools from extreme value theory, for functional regression and analysis of variance problems.<br/><br/>Nerve cells, or neurons, are fundamental to communication between the brain and other parts of the body. When a subject receives an external stimulus, for example, a visual stimulus may consist of showing an object to the subject, nerve cells react and transmit information about the stimulus to the brain through a series of electrical pulses. The proposed research will advance understanding of this communication process by developing new methods for analyzing the neural signals. The statistical methods developed during the proposed research will study how the response to a stimulus differs among different subjects, and how these differences relate to factors such as species, age and gender. The proposed research has applications in product design. As an example, a warning device such as a traffic signal may be required to generate a stimulus to warn users of potential hazard. Understanding the response of different individuals to a given stimulus, and the different responses to different stimuli, will lead to improved products and safety for a wider variety of users.<br/>"
"0304954","Collaborative Proposal:  ISI and TIES Conference Support Program","DMS","STATISTICS","04/01/2003","02/21/2003","Montserrat Fuentes","NC","North Carolina State University","Standard Grant"," Shulamith T. Gross","03/31/2004","$9,000.00","","mfuentes@vcu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","0000, OTHR","$0.00","Proposals 0304954 and 0306970<br/>PIs  Montserrat Fuentes and Peter Guttorp<br/>Title  Collaborative proposal: ISI and TIES Conference Support Program<br/><br/>Abstract<br/><br/>This collaborative project supports travel of new researchers from US institutions to two international conferences in Statistics during the summer of 2003. The conferences are the International Environmetrics Society (TIES) 2003 Conference, held in August 2003 in Beijing, China, and the International Statistics Institute (ISI) Conference on Environmental Statistics and Health (ISI-ESH), held in July 2003 in Santiago de Compostela, Spain. The conferences provide a forum for these new researchers to present their work and interact with established researchers in statistics who work on environmental and health related scientific issues. <br/><br/>"
"0306366","New Problems in Multiple Hypotheses Testing","DMS","STATISTICS","07/15/2003","08/12/2003","Sanat Sarkar","PA","Temple University","Standard Grant","Grace Yang","08/31/2006","$233,967.00","","sanat@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","MPS","1269","0000, OTHR","$0.00","A sudden upsurge of research in the area of multiple hypotheses testing has been taking place in the recent years resulting in newer ideas and raising a number of important statistical questions. The present proposal intends to address a number of those questions. A major thrust of the research will be to provide further development of the theory of false discovery rate (FDR), a new and powerful concept of error rate in multiple testing, which has been receiving increasing attention in a wide variety of statistical applications. While it has been established by the investigator and other researchers recently that there exist multiple testing procedures which can control the FDR in situations involving independent as well as some types of positively dependent test statistics, no theory has yet been developed toward a statistically meaningful comparisons of these procedures. This proposal considers developing this theory in terms of some new criteria involving false negatives rate (FNR). A second major thrust of the proposed research will be the investigation of some important open problems related to directional error control in stepwise multiple testing procedures. The theory of directional error control in multiple testing has so far been concentrated mostly within the framework of independent test statistics, limiting its scope in real applications where the test statistics are rarely independent. The investigator has recently revisited this theory with independent test statistics and improved it for a step-down procedure. This project aims at developing this theory further accommodating other stepwise procedures and test statistics that are not necessarily independent. <br/><br/>Recent published research and some preliminary investigations by the investigator provide strong motivation and lay the foundation for the proposed research. The results from this research will be of importance to several areas of statistical applications, including microarray experiments where the concept of false discovery rate has been receiving acceptance as a statistical measure of error in detecting differentially expressed genes, and pharmaceutical studies where multiple testing techniques are routinely used in evaluating a drug's efficacy over standard drug or placebo. This research has the potential to generate collaborations with researchers in biopharmaceutical statistics. It would also benefit education through training of graduate as well as undergraduate students and incorporation of the developed methodologies in statistics courses.<br/>"
"0308861","Adaptive Analysis of Sparse Factorial Designs and Related Problems","DMS","STATISTICS","07/15/2003","07/08/2003","Weizhen Wang","OH","Wright State University","Standard Grant","Grace Yang","06/30/2007","$87,061.00","Daniel Voss","weizhen.wang@wright.edu","3640 Colonel Glenn Highway","Dayton","OH","454350001","9377752425","MPS","1269","0000, OTHR","$0.00","DMS-0308861<br/>Adaptive analysis of sparse factorial designs and related problems<br/>Weizhen Wang and Daniel T. Voss<br/><br/>Abstract<br/><br/>The investigators study the statistical theory underlying control of error rates for adaptive analysis of sparse factorial designs.  Considered are orthogonal and nonorthogonal saturated designs, nearly saturated designs, and supersaturated designs.  Methods under consideration rely upon the presence of an adequate but unknown level of effect sparsity, for lack of an adequate variance estimator independent of the effect estimates, and adapt to the level of effect sparsity suggested by the data.  Much of the difficulty and intrigue lies in the apparently circular nature of the required arguments, since relatively large effect estimates are in some sense adaptively set aside for variance estimation, then the resulting variance estimate is used to make inferences about the effects.  Lack of an adequate independent variance estimator gives rise to a variety of problems that are important and technically challenging.  The investigators bring to bear on these problems a geometric perspective and the rigor and perspective of the multiple comparisons field, seeking methods of inference that control error rate strongly, while using the data adaptively and efficiently.<br/><br/>The investigators study the probabilistic foundations of data analysis for sophisticated experiments fundamental to the development and production of high-quality, low-cost products.  Critical characteristics often depend in unknown ways on an unknown subset of a larger collection of variables or factors.  The resulting necessity to study many factors simultaneously in small, economical experiments--essentially to learn a lot from but a little data--presents many statistical challenges.  While great progress has been made in the development of designs or plans for conducting such experiments, fundamental problems concerning the corresponding data analysis remain.  The investigators study the theoretical foundations for the analysis of data from such experiments, and consequently propose innovative methods of data analysis, justifying them mathematically.  This entails seeking solutions to a variety of related problems in probability pertinent to the analysis of data from such factorial experiments. Results have direct applications in engineering product and process design and in statistical process control and improvement, enhancing efforts to achieve competitive economic advantage.<br/>"
"0314302","The Seventh Purdue International Symposium on Statistics","DMS","STATISTICS","04/01/2003","03/21/2003","Mary Bock","IN","Purdue University","Standard Grant","Gabor Szekely","03/31/2004","$15,000.00","Jayanta Ghosh","mbock@stat.purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","0000, OTHR","$0.00","DMS-0314302<br/><br/>PI: Mary Ellen Bock<br/><br/>Abstract<br/><br/><br/>The Seventh Purdue International Symposium on Statistics will be held June 19 to 24, 2003, at Purdue University, Indiana. It consists of a main conference on Statistical Decision Theory and related topics (June 21, 22), a workshop on Bioinformatics (June 19, 20), a workshop on Statistical Consultancy (June 23, 24) and a workshop on Multiple Comparisons and Mixture Models for Large Data Sets (June 23, 24). This series of unique international meetings has long been a major event for presentation and dissemination of Statistical Theory and Methods. In this meeting, as in the previous one, special effort has been made to stress applications to interdisciplinary research and large data sets. In recent years there has been an explosion of new theory and methods that specifically target high dimensional data analysis in interdisciplinary problems of science and industry. These new methods help mine, explore and analyze large data sets. The conference and the workshop will focus on some of these problems and methods. The funding provided by the National Science Foundation will be used to support the main conference and the workshop on Multiple comparisons and Mixture Models for Large Data sets by providing partial or full support of travel with preference for young researchers with no alternative funding and underrepresented groups.    <br/>"
"0244638","Collaborative Research:  Advanced Sequential  Monte Carlo Methods and Applications","DMS","STATISTICS","09/01/2003","09/20/2006","Jun Liu","MA","Harvard University","Standard Grant","Grace Yang","08/31/2007","$244,659.00","","jliu@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","0000, 1616, OTHR","$0.00","Proposal IDs: DMS-0244638, DMS-0244583 and DMS-0244541<br/>PIs:   Jun Liu, Xiaodong Wang, and Rong Chen<br/>Title: FRG Collaborative Research:  Advanced Sequential Monte Carlo Methods<br/><br/>Abstract<br/><br/>Sequential Monte Carlo (SMC) can be loosely defined as a family of techniques that use Monte Carlo simulations to solve on-line estimation and prediction problems in stochastic dynamic systems.  By recursively generating random samples of the state variables, SMC adapts flexibly to the dynamics of underlying stochastic systems. In this FRG project, the investigators and their colleagues will develop a few advanced SMC methods, including novel operations in SMC (pilot exploration, flexible resampling, tempered SMC, etc.) and the nonparametric SMC framework. To demonstrate their wide applicabilities in solving scientific problems, they will apply the developed methodologies to a wide spectrum of problems found in computational biology (e.g., the discovery of cis-regulatory binding motif modules, progressive multiple sequence alignment, the inference for gene relationships, and chain polymer analyses) and wireless information networks (e.g., the design of adaptive nonparametric receivers in various wireless channels, mobility tracking in wireless networks, handoff the design of adaptive nonparametric receivers in various wireless channels, mobility tracking in wireless networks, handoff management and admission control in cellular networks). The proposed research will significantly enrich and advance the statistical modeling methodology and statistical computation theory and is expected to culminate in the formulation of novel modeling, analysis, and computation techniques in computational biology and wireless information networks.<br/><br/><br/>Stochastic modeling is essential in many application fields ranging from computer vision and engineering to molecular biology and statistical physics. But statistical analyses of these models often pose significant challenges to researchers. The sequential Monte Carlo (SMC) methodology recently emerged in the fields of statistics and engineering has shown great promise in solving a large class of highly complex inference and optimization problems regarding stochastic models, opening up new frontiers for cross-fertilization between statistical science and many application areas. The investigators have been working closely in the past on both theoretical developments of SMC and applications of SMC in computational biology and telecommunications, and have made significant impacts. In the meantime, many aspects of SMC theory are yet to be explored; and new challenges arising from applications in these two areas demand novel SMC strategies to be developed. For example, computational biology and wireless communications are two fronts of vital importance in modern science and engineering. With the explosive growth of research and development in these two areas, efficient and accurate statistical methods that can cope with nonlinear, non-Gaussian, and nonstationary features are in urgent need. It is thus important to continue the interdisciplinary research that has been carried out by the investigators to further advance the SMC theory and to bring this powerful statistical paradigm into the most exciting areas of today's scientific and engineering research and development. In this project, the investigators and their colleagues will continue to develop novel SMC methods and to investigate their theoretical properties. They will also apply the developed methods to a wide spectrum of problems found in computational biology and wireless information networks. The proposed research is expected to culminate in the formulation of novel modeling, analysis, and computation techniques in computational biology and wireless information networks.<br/>"
"0244799","Sixth North American Meeting of New Researchers in Statistics and Probability","DMS","PROBABILITY, STATISTICS","03/01/2003","12/19/2002","Richard Levine","CA","San Diego State University Foundation","Standard Grant","Xuming He","12/31/2003","$20,000.00","Juanjuan Fan","rlevine@mail.sdsu.edu","5250 Campanile Drive","San Diego","CA","921822190","6195945731","MPS","1263, 1269","0000, OTHR","$0.00","Abstract<br/><br/>PI: Richard Levine<br/>DMS-0244799<br/><br/>Sixth North American Meeting of New Researchers in Statistics and Probability<br/><br/>This proposal describes plans for the Sixth North American Meeting of New Researchers in Statistics and Probability, a conference sponsored by the Institute of Mathematical Statistics, to be organized by and held for junior researchers. The primary objective is to provide a much needed venue for interaction among new researchers. In contrast with large meetings, this conference will be restricted to 90 participants. Sessions will be followed by discussions and breaks to facilitate interactions. <br/><br/>The proposed conference will take place July 29-August 1, 2003, on the University of California campus at Davis, CA. Housing, meals, and conference facilities will be provided on campus. The Joint Statistical Meetings will be held in San Francisco providing an easy opportunity for conference participants to attend both events. Participants will be statisticians and probabilists whom have received their PhD since 1998 or are expecting to receive their PhD degree by 2004. Each participant will present a talk or poster. Topics will include the gamut of statistical research from theory and methods to applications. There will be four senior speaker sessions in which topics of particular interest to new researchers will be presented. Professor Charles McCulloch from the University of California at San Francisco, Professor Xiao-Li Meng from Harvard University, Professor Jessica Utts from the University of California, Davis, and IMS president Raghu Varadhan from the Courant Institute, have all accepted invitations to speak. Three panel discussions during the conference will cover the topics of journal publications, opportunities in statistics laboratories, and funding.<br/>"
"0308609","Statistics and Applications","DMS","STATISTICS","08/01/2003","07/31/2003","Jiayang Sun","OH","Case Western Reserve University","Standard Grant","Gabor Szekely","07/31/2007","$85,864.00","","jsun21@gmu.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","MPS","1269","0000, OTHR","$0.00","Four research topics are pursued. The first is on inverse problems and measurement error models. New adaptive wavelet denoising for 3-dimensional objects will be studied and new non-Fourier deconvolution methods for measurement error models will allow users to take into account non-homogeneous errors that occur often in astronomical data. The second area is on biased sampling. Nonparametric and semi-parametric methods will be studied for data from multiple surveys and semi-parametric methods will be developed for survival data. The third area is on high dimensional graphics and data mining. Efficient methods and software will be developed for (large) data analysis and modeling. The fourth area is on mixture problems. New tests and model selection procedures are proposed.<br/><br/>The challenges for statistics arising from astronomy, genetics, medical imaging and computer science are multifaceted. Data can be complex due to measurement errors or other unobservable features; or large in size or dimension due to advances in sciences and information technology; or small due to some intrinsic nature of data, for example, halos, ancient stars, are scarce in comparison to other stars. This project develops new theories, practical methods and efficient computational tools to meet these challenges, via the above four major research topics. The interdisciplinary nature of the research will impact our knowledge in the sciences. The research will also benefit education and human development, via training of students, integration with research and mentorship and collaboration with scientists.<br/><br/>"
"0306526","Spatial-temporal Analysis of Earthquake Catalogs using Point Processes","DMS","STATISTICS","08/15/2003","08/07/2003","Frederic Schoenberg","CA","University of California-Los Angeles","Standard Grant","Grace Yang","07/31/2006","$146,258.00","Yan Kagan, David Jackson","frederic@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","0000, OTHR","$0.00","The aim of this project is the development and application of statistical techniques for describing earthquake catalogs, focusing primarily on spatial-temporal marked point process models and closely related objects such as tessellations and branching processes.  The goals of the seismological application are the fitting, assessment, and improvement of models such as the epidemic-type aftershock sequence (ETAS) model, which are useful for describing earthquake catalogs, especially the geometric and spatial-temporal structure of earthquake faults, and the thorough investigation of errors in records of earthquake parameters and their impact on seismological inferences such as estimates of seismic hazard and standard errors for model parameters.  In addition to the development of new useful stochastic models, the main statistical goals are the extension of assessment methods, especially point process residual analyses via rescaling and thinning, to the case of spatial-temporal marked point processes as well as other types of models such as tessellations and branching processes, and the development of tests for stationarity in the residuals that are powerful in the detection of departures from spatial homogeneity and from the assumption of spatial-temporal-magnitude separability.<br/><br/>Major earthquakes are catastrophic events that can cause numerous injuries, loss of lives, and billions of dollars in damages to public and private property.  This project has the potential for very direct societal impact on the preparation for and response toward these potentially devastating occurrences.  Seismic hazard estimation, defined as the quantification of the likelihood of an earthquake of at least a given size occurring in a certain region within a given time interval, is crucial to civil engineering, earthquake preparedness and response systems, and the determination of earthquake premiums which in turn influence governmental policy and property valuation.  The refinement of spatial-temporal models for earthquake patterns described in this project is necessary for more precise estimation of seismic hazard..Further, the analysis of errors in earthquake catalogs and their impact on seismic hazard estimates is critical in order for proper awareness of uncertainties in earthquake forecasts among the public and scientific community.  The statistical tools developed in this project are useful for other applications as well.  In particular, spatial-temporal marked point process models are important for describing the occurrences of other natural phenomena besides earthquakes, such as wildfires, disease epidemics, hurricanes, and many others.<br/>"
"0244583","Collaborative Research: Advanced Sequential Monte Carlo Methods and Applications","DMS","STATISTICS","09/01/2003","07/01/2003","Xiaodong Wang","NY","Columbia University","Standard Grant","Grace Yang","08/31/2007","$246,401.00","","wangx@ee.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","0000, 1616, OTHR","$0.00","Proposal IDs: DMS-0244638, DMS-0244583 and DMS-0244541<br/>PIs:   Jun Liu, Xiaodong Wang, and Rong Chen<br/>Title: FRG Collaborative Research:  Advanced Sequential Monte Carlo Methods<br/><br/>Abstract<br/><br/>Sequential Monte Carlo (SMC) can be loosely defined as a family of techniques that use Monte Carlo simulations to solve on-line estimation and prediction problems in stochastic dynamic systems.  By recursively generating random samples of the state variables, SMC adapts flexibly to the dynamics of underlying stochastic systems. In this FRG project, the investigators and their colleagues will develop a few advanced SMC methods, including novel operations in SMC (pilot exploration, flexible resampling, tempered SMC, etc.) and the nonparametric SMC framework. To demonstrate their wide applicabilities in solving scientific problems, they will apply the developed methodologies to a wide spectrum of problems found in computational biology (e.g., the discovery of cis-regulatory binding motif modules, progressive multiple sequence alignment, the inference for gene relationships, and chain polymer analyses) and wireless information networks (e.g., the design of adaptive nonparametric receivers in various wireless channels, mobility tracking in wireless networks, handoff the design of adaptive nonparametric receivers in various wireless channels, mobility tracking in wireless networks, handoff management and admission control in cellular networks). The proposed research will significantly enrich and advance the statistical modeling methodology and statistical computation theory and is expected to culminate in the formulation of novel modeling, analysis, and computation techniques in computational biology and wireless information networks.<br/><br/><br/>Stochastic modeling is essential in many application fields ranging from computer vision and engineering to molecular biology and statistical physics. But statistical analyses of these models often pose significant challenges to researchers. The sequential Monte Carlo (SMC) methodology recently emerged in the fields of statistics and engineering has shown great promise in solving a large class of highly complex inference and optimization problems regarding stochastic models, opening up new frontiers for cross-fertilization between statistical science and many application areas. The investigators have been working closely in the past on both theoretical developments of SMC and applications of SMC in computational biology and telecommunications, and have made significant impacts. In the meantime, many aspects of SMC theory are yet to be explored; and new challenges arising from applications in these two areas demand novel SMC strategies to be developed. For example, computational biology and wireless communications are two fronts of vital importance in modern science and engineering. With the explosive growth of research and development in these two areas, efficient and accurate statistical methods that can cope with nonlinear, non-Gaussian, and nonstationary features are in urgent need. It is thus important to continue the interdisciplinary research that has been carried out by the investigators to further advance the SMC theory and to bring this powerful statistical paradigm into the most exciting areas of today's scientific and engineering research and development. In this project, the investigators and their colleagues will continue to develop novel SMC methods and to investigate their theoretical properties. They will also apply the developed methods to a wide spectrum of problems found in computational biology and wireless information networks. The proposed research is expected to culminate in the formulation of novel modeling, analysis, and computation techniques in computational biology and wireless information networks.<br/>"
"0239427","CAREER: Statistical and Computational Tools for the Analysis of High Dimensional Genetic Data","DMS","STATISTICS","06/01/2003","07/16/2008","Chiara Sabatti","CA","University of California-Los Angeles","Continuing Grant","Gabor Szekely","11/30/2008","$400,000.00","","sabatti@stanford.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","0000, 1045, OTHR","$0.00","Proposal ID: DMS-0239427<br/>PI:  Chiara Sabatti<br/>Title: CAREER: Statistical and computational tools for the analysis of high dimensional genetic data<br/><br/>Abstract<br/><br/>This project will enable the creation of novel statistical models and computational tools for the analysis of data in high dimensional spaces, as the one generated in the field of genetics.  In particular the investigator and her colleagues will (a) develop models for genomic sequences that aim at establishing the total number of binding sites, their location and their interaction with each other; (b) pursue de-noising of gene array data, modeling of the dependence between the expression of various genes, and the identification of the number of different chemical signals originating change in expression; (c) model the notion of ``haplotype blocks'' and define the procedures to identify them with the purpose of gene mapping, and develop appropriate procedures of correction for multiple comparison in the same context. The project illustrates relations between the topics of model selection, multiple comparison, high-dimensional function estimation and leads to deeper understanding of connections between Bayesian models, minimum description length principle, and false discovery rates. The proposed research will additionally develop a new set of computational tools that are based on Markov Chain Monte Carlo sampling and representation of the objective distribution on a variety of different scales. <br/><br/>The outlined research helps to tackle some fundamental questions regarding the role and the expression of genes, thus leading to improvements of the general welfare, trough the discovery of genes related to diseases, the development of genetic therapies, and the engineering of the over-production of protein of interests on industrial scale. By making the algorithms for genome and gene expression analysis publicly available, and upgrading the computing infrastructure, the project broadens the participation to scientific investigation of under-served community and enhance the general infrastructure for research. The proposed organization of interdisciplinary workshops, research activities and courses assures a broad dissemination of the results to enhance scientific understanding. The organizations of seminars on teaching statistics in interdisciplinary settings for high-school and college instructors goes in the direction of integrating research and education, promoting teaching, training, and learning.<br/>"
"0239053","CAREER:  New Directions in Mixture Models and their Applications","DMS","STATISTICS","06/01/2003","05/15/2006","Ramani Pilla","OH","Case Western Reserve University","Continuing Grant","Grace Yang","05/31/2007","$313,246.00","","pilla@cwru.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","MPS","1269","0000, 1045, 1187, 9229, OTHR","$0.00","<br/>        Mixture models, which can be viewed also as clustering<br/>        techniques, have become widely used statistical tools in the<br/>        analysis of heterogeneous data, aiding researchers in<br/>        interpreting existing data or in classifying new data. This<br/>        project extends the current interests of the PI in mixture<br/>        models to new directions while integrating them into<br/>        education. It is well known that under the normal mixture<br/>        model with unequal variance, the likelihood is unbounded and<br/>        hence the global maximum likelihood estimator (MLE) does not<br/>        exist. High-dimensional data analysis is becoming increasingly<br/>        important in many applied fields, including bioinformatics,<br/>        astronomy and imaging. Moreover, finding the nonparametric MLE<br/>        is widely regarded as computationally intensive, with the<br/>        particular difficulty being locating the mass points. To<br/>        address these issues this project will (1) establish<br/>        spacings-based inferential tools and asymptotic theory for the<br/>        normal mixtures with unequal variances to overcome the<br/>        limitations of the likelihood approach; (2) generalize these<br/>        methods to multivariate normal mixture model; (3) develop<br/>        theory and algorithms for multivariate mixtures via the<br/>        penalized dual method; (4) develop methods for solving<br/>        nonparametric mixture problems; (5) extend the multivariate<br/>        mixture methods to identify features in spatial patterns and<br/>        in turn develop efficient pattern recognition algorithms for<br/>        use in hyperspectral image classification, mammography and<br/>        minefield detection; and (6) integrate research and education<br/>        to advance discovery and understanding of mixtures.<br/><br/><br/>        This project will: (1) promote discovery and understanding of<br/>        the mixture models for modeling univariate and multivariate<br/>        heterogeneous data; (2) broaden and initiate new applications<br/>        to advance mixture models to new frontiers; (3) promote<br/>        collaborative learning and foster critical thinking through<br/>        student involvement in the PI's research projects; (4) build a<br/>        firm foundation for the PI in contributing to a<br/>        well-integrated research and education program in the theory,<br/>        computation and application of mixture models and related<br/>        areas; (5) impart education that will train a new generation<br/>        of scientists and engineers capable of developing mixture<br/>        models tools to solve important problems arising from new<br/>        frontiers of biology, engineering and medicine; (6) result in<br/>        offering an interdisciplinary course in mixture models and<br/>        applications to graduate students, enabling and promoting<br/>        interactions between statistics and allied fields; (7) enhance<br/>        multidisciplinary research experience for students through the<br/>        PI's collaborations and partnerships with the U.S. Navy and<br/>        international scientists.<br/><br/>"
"0308109","Applied Probability and Time Series Modelling","DMS","STATISTICS","06/01/2003","05/09/2005","Peter Brockwell","CO","Colorado State University","Continuing Grant","Grace Yang","05/31/2007","$454,944.00","Richard Davis","pjb2141@columbia.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1269","0000, OTHR","$0.00","DMS-0308109<br/>PI:  Peter J. Brockwell<br/>Title:  Applied probability and Time Series modeling <br/><br/>Abstract:<br/><br/>Properties and applications in finance of Levy-driven linear and non-linear continuous-time autoregressive-moving average (ARMA) processes will be investigated.  Questions concerning existence of stationary solutions for non-linear continuous-time models and relations between discrete-time models and their continuous-time analogues will also be addressed.  In discrete time, the stochastic stability of generalized linear versions of ARMA models will be studied with a view to modeling time series of counts.  Efficient estimation techniques for these models and for all-pass models driven by non-Gaussian noise will be developed.  The results for the latter processes will be applied to the problem of identification and estimation for non-causal and/or non-invertible ARMA models. <br/><br/>In the last decade there has been a widely recognized need for the development of new models and techniques for the analysis of time-series data from scientific, engineering, biomedical and financial applications.  Major features giving rise to this need include non-linearity, complex dependence structures and strong deviations from normality, with discrete-valued data arising in many genetic and biomedical applications.  In financial applications there is a need for continuous-time models exhibiting these characteristics.  The proposal addresses these needs, with the goal of enhancing scientific understanding of the physic and economic processes represented by the models.<br/>"
"0327581","Data Mining and Geographic Information Systems (GIS) to Investigate Public Health Information","DMS","STATISTICS, Methodology, Measuremt & Stats","09/01/2003","10/14/2003","Patricia Cerrito","KY","University of Louisville Research Foundation Inc","Standard Grant","grace yang","08/31/2006","$227,562.00","Arthur Dakan, George Barnes, Robert Forbes, Carol Hanchette","pcerrito@louisville.edu","Atria Support Center","Louisville","KY","402021959","5028523788","MPS","1269, 1333","0000, 9150, 9251, OTHR","$0.00","The purpose of this project is to develop a course that combines GIS (geographic information systems) with data mining and statistical analysis to examine public health data. Two graduate courses that integrate GIS and data mining will be offered in the summer, 2004 term. The courses will be taught by a team of faculty from the geography/geosciences and mathematics departments. The courses are fully integrated into degree programs currently offered at the University of Louisville. In particular, the Department of Mathematics initiated a PhD program in Industrial and Applied Mathematics in the fall, 2002 semester. The degree program includes a mandatory internship and 18 credit hours in an application area. Geography and GIS can easily comprise one application area for the degree. The instruction will include use of ArcView and SAS to examine data from real databases containing information on health outcomes. Students will learn to define association rules between environmental factors and health outcomes. They will also work with clustering techniques, kernel density estimation, and text mining. In geography, students will learn to use ArcView to construct maps using available data, and to manipulate maps to optimize information revealed. Upon completing the course, students will be encouraged to take internship positions that integrate statistics with GIS using real databases. Students will be encouraged to take additional courses in the mathematics and geography departments to further develop their skills."
"0306576","An Adaptation Theory For Nonparametric Function Estimation","DMS","STATISTICS","08/01/2003","05/28/2003","Mark Low","PA","University of Pennsylvania","Standard Grant","grace yang","07/31/2006","$336,821.00","T. Tony Cai","lowm@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, OTHR","$0.00","The investigators propose a comprehensive theory and methodology for adaptive estimation of linear functionals.  A general framework is provided for the construction of both adaptive point estimators and adaptive confidence intervals.  The theory makes precise exactly when adaptation is possible and also the minimum price that must be paid when adaptation is not possible.  A new geometric quantity is shown to be instrumental in building this theory.  This geometric approach also leads to effective adaptive algorithms in optimal recovery problems.  The theory is extended to include simultaneous estimation of many linear functionals.<br/><br/>As data collection and computing power have grown exponentially there has been a trend to fit more complicated models using more flexible statistical tools.  The investigators develop a general theory for the analysis of such models.  A geometric framework is provided and shown to be useful in many engineering and computer science applications.<br/><br/>"
"0337163","NSF Conference in the Mathematical Sciences on Data Mining and Bioinformatics;  January 8-10, 2004; Gainesville, FL","DMS","STATISTICS","08/15/2003","08/04/2003","George Casella","FL","University of Florida","Standard Grant","Xuming He","07/31/2004","$17,500.00","Michael Daniels, Bhramar Mukherjee","casella@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","0000, OTHR","$0.00","DMS-0337163<br/>          PI: George Casella<br/><br/><br/>ABSTRACT<br/><br/>NSF Conference in the Mathematical Sciences on Data Mining and Bioinformatics at the University of Florida, January 8-10, 2004<br/><br/>There is a large demand for statistical tools to help us analyze and understand massive amounts of data.  Traditional statistical approaches often fail to cope with the underlying complexity of such datasets. Some of the potential statistical issues are model selection, including algorithms to search through model spaces, robustness, data quality and sampling, multiplicity issues, inference in high-dimensional, small sample (""large p small n"") problems,  appropriate scaling of data, and inference based on complex datasets from medical images, microarrays or environmental monitoring.  Biological questions inherent in such data include determining the three dimensional structure of proteins based on  DNA sequences and determining the differential expression levels of thousands of genes from data collected on microarrays.  Data mining (DM) is the generic term that encompasses such methods for massive datasets.  Data mining on biological and genomic data is often called Bioinformatics (Bio).  <br/><br/>The topic of Data Mining and Bioinformatics is ideal for a NSF regional conference. It appeals to a wide spectrum of researchers with diverse statistical interests from those interested in internet trafficking to fraud detection to microarrays to protein structure.  The challenge of drawing inferences based on these massive datasets will appeal to those interested in theoretical and methodological statistics.  Finally, the excitement of accepting the fine challenge of analyzing unorthodox data where existing statistical methodology is not satisfactory will undoubtedly fascinate researchers concerned with applications of statistics.<br/><br/>It is hoped that this conference will provide an assessment of the current state of the art in the workings and use of  DM/Bio, bring up open problems, and foster collaboration among research workers in academia, industry and government in an effort to provide solutions to these problems and answer questions of great importance to both science and society.  We expect the conference to generate interest in this topic among researchers nationwide (particularly young researchers), among faculty and graduate students at the University of Florida and neighboring universities, and promote interactions between junior and senior researchers.<br/><br/><br/><br/>"
"0243606","Theory and Applications of Sharp Nonparametric Estimation and Learning","DMS","STATISTICS","06/01/2003","04/30/2003","Sam Efromovich","NM","University of New Mexico","Standard Grant","grace yang","09/30/2006","$167,805.00","","efrom@utdallas.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","MPS","1269","0000, 9150, OTHR","$0.00","abstract<br/><br/>PI:  SAM EFROMOVICH<br/>proposal number:  0243606<br/><br/>The primary focus of this research is to develop general methods of data-driven statistical estimation and learning motivated by and tested on environmental, medical and biological applications.  The main intellectual objectives are threefold:<br/>(A) In the case of settings with known sharp asymptotics (like censored or biased datasets), develop the theory of the onset of the sharp optimality and the equivalence between statistical models for small datasets; <br/>(B) In the case of models with indirect observations and nuisance functions (like error density estimation in heteroscedastic nonparametric regression or recovery of a hidden component in time series), develop the theory of sharp estimation and sampling with fixed accuracy; <br/>(C) In the case of inverse problems with unknown operator, develop data-driven learning machines implying sharp estimation. <br/>Practical problems include statistical modeling of temporal and spatial structures of plants in Sevilleta National Wildlife Refuge, modeling of arsenic concentration in Albuquerque water basin, the study of municipal wastewater treatment plants, statistical modeling of spreading hantavirus, and learning machines for recovery magnetic resonance images.<br/><br/>The primary focus of this research is to develop, in collaboration with Sandia National Laboratories and the UNM Medical School, algorithms and software for adaptive statistical estimation and learning motivated by and tested on the following environmental, medical and biological applications:  Statistical modeling of temporal and spatial structures of plants in Sevilleta National Wildlife Refuge; Modeling of arsenic concentration in Albuquerque water basin; Study of municipal wastewater treatment plants; Statistical modeling of spreading hantavirus; Learning machines for recovery magnetic resonance images.  The broader impact of the research is defined by the well-understood applications that can encourage students to study mathematics and can help a broader audience to understand the importance of statistics.  The impact is based on the following activities:<br/>(i) Developing a new course on adaptive statistical estimation taught via the UNM web-based program;<br/>(ii) Weekly scientific seminars (supported in part by private grants) held for undergraduate and graduate students, and talks during the UNM mathematical awareness weeks for high-school students;<br/>(iii) Regular presentations at outreach seminars conducted by the UNM Valencia campus to broaden participation of under-represented groups;<br/>(iv) Posting the developed software, databases, and practical findings, that can be of interest to a broader audience, on the investigator's webpage;<br/>(iv) Publishing of medical, environmental and biological findings, benefiting the society, in non-technical journals.<br/><br/>"
"0306800","Statistical Methods for Gene Mapping Based on a Confidence Set Approach","DMS","STATISTICS","08/01/2003","04/26/2005","Shili Lin","OH","Ohio State University Research Foundation -DO NOT USE","Continuing grant","grace yang","07/31/2007","$172,684.00","","shili@stat.osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","Multiple testing is an important but difficult statistical issue in many areas of genetic research.  One particular multiple testing problem arises when many markers are screened throughout the genome for their linkage or association with a disease locus, which is the focus of this project, with a broad long-term objective of developing methods applicable in various areas of genetic and genomic research.  The main thrust of the proposed approach lies in its formulation of the hypotheses for linkage.  Traditionally, hypotheses for linkage are usually set up with the null hypotheses being no linkage and the alternative hypothesis being linkage.  In the new formulation, the null and alternative hypotheses are being ``reversed'', with the null hypothesis being tight linkage and the alternative hypothesis being loose linkage or no linkage.  Two of the fundamental advantages with this new paradigm are: first, multiplicity adjustment is unnecessary for the number of tests performed in a genome-scan study, and second, the location of a disease gene can be narrowed down to a small genomic region, even at the stage of a preliminary genome-scan study.  The first specific aim is to develop methods for constructing confidence sets of markers or confidence regions (intervals) of disease gene locations based on parametric tests of the hypotheses.  Single-marker and multiple-marker approaches will be developed for data from general pedigrees.  The second specific aim can be viewed as a non-parametric counterpart of the first aim.  Methods will be developed for constructing confidence sets/regions based on non-parametric tests using allele-sharing statistics.  A wide variety of allele-sharing statistics and data types, ranging from simple structures (sibships, relative pairs) to general pedigrees, will be considered.<br/><br/>With the completion of the Human Genome Project, and the development of high throughput technology for genotyping, it is now a routine matter to search up to thousands of genetic markers distributed throughout the genome to look for disease susceptibility genes.  This project aims at developing statistical methods suitable for probing each of these markers without compromising the power of finding nearby susceptibility locus.  As the number of participating families increases, the rate of falsely implicating a marker not located within a short distance from a disease gene will eventually go down to zero.  This would not only increase the chance of successful identification of disease genes, but would also save tremendous resources by reducing the chance of going after ""ghost"" genes.  Thus, the methods developed can be a valuable tool to the gene mapping community.  In particular, it is expected that the methods developed in this project will be applied to data from projects, on which the investigator is collaborating with medical doctors and other researchers, on a range of autoimmune diseases, including systemic lupus erythematosus and multiple sclerosis.<br/>"
"0306227","Accounting for Measurement Error in the Analysis of Time Series / Longitudinal Data","DMS","STATISTICS","07/15/2003","07/10/2003","John Buonaccorsi","MA","University of Massachusetts Amherst","Standard Grant","grace yang","06/30/2007","$211,033.00","John Staudenmayer","johnpb@math.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","MPS","1269","0000, OTHR","$0.00","This project assesses the effects of measurement error and develops new methods to account for measurement error in the analysis of single or multiple time series. Specifically, the investigators address these problems in the context of: i) fitting a single time series with linear and non-linear dynamic models, ii) assessing the relationship between two variables measured over time on one or more ""units"", with measurement error in one or both variables and serial correlation and/or contemporaneous (among unit) correlation; iii) assessing synchrony correlation among series in different locations. Throughout, this project accommodates a rich class of measurement error models allowing some combination of heteroscedasticity, possible dependence of the measurement error on the true value, correlated measurement errors and a separate estimate of the measurement error variance for each unit. These features arise in many typical applications. A variety of techniques are employed including moment approaches, likelihood methods, and the use of estimating equations.<br/><br/>A wide variety of important problems in ecology, environmental sciences, economics, public health and many other disciplines involve the statistical modeling of one or more variables collected over time and, perhaps, in different locations. Examples include the modeling of population densities, economic indicators, disease rates, pollution levels, temperatures, etc. over time and assessing the relationship among variables (e.g. pollution levels and disease rates or food abundance and population densities) with data collected over time and space. A common problem is that the variable(s) cannot be observed exactly and need to be estimated. In simpler settings it is known that ignoring the estimation step and its accompanying measurement error causes two serious problems. First, the amount of noise in the system is underestimated, and second, the true nature of underlying systematic relationships can be obscured or misinterpreted. In more complicated situations where the data are measured over time or are spatially related, little or nothing is known about the effects of measurement error. Utilizing theoretical developments and computer simulations, this project will provide results to guide investigators as to the impact of the use of error-prone measures, develop methods to correct for the problems caused by measurement error and ensure that our methods are applied to important real world problems."

"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1525692","CAREER: A new and pragmatic framework for modeling and predicting conditional quantiles in data-sparse regions","DMS","STATISTICS","09/30/2014","08/14/2018","Huixia Wang","DC","George Washington University","Continuing Grant","Nandini Kannan","08/31/2018","$298,474.00","","judywang@gwu.edu","1918 F ST NW","WASHINGTON","DC","200520042","2029940728","MPS","1269","1045","$0.00","Quantile regression provides a valuable semiparametric tool for modeling the conditional quantiles of a response variable given predictors. However, making inference for quantile regression is challenging in data-sparse regions such as at low or high quantiles with quantile levels close to 0 or 1. In the proposed research, the Principal Investigator (PI) aims to develop theory and methodology for quantile regression in data-sparse regions, which opens up a significant new direction in quantile regression. For estimating extreme conditional quantiles of the response distribution, the PI plans to develop extrapolation methods based on a novel application of the extreme value theory. In data sparse areas, the formulation of models plays a critical role. The PI will study models with different levels of complexity, which calls for different techniques for quantifying the tail quantiles. New theory and methods for joint quantile estimation and inter-quantile shrinkage will be developed to improve statistical efficiency by sharing information across multiple quantile functions. The PI will also study tail quantile regression for dependent data, where the common understanding about the impact of dependence on statistical inference needs to be re-examined. As a result, new and efficient methods to incorporate tail dependence will be proposed.<br/><br/><br/>An important problem in many fields is the modeling and prediction of events that are rare but have significant consequences. Unexpectedly heavy rainfall, large portfolio loss, and dangerously low birth weight are some examples of rare events. For such events, scientists are particularly interested in modeling and estimating the tail quantiles of the underlying distribution rather than the central summaries such as the mean or median. The proposed methodologies will have broad and valuable applications in studies of rare events in climate sciences, risk management in finance, studies of infant birth weights, and prediction of insurance claims. The PI will integrate research and education by developing advanced topics courses, engaging graduate and undergraduate students, especially those from under-represented groups, in the project, and reaching out to the K-12 education levels by training high school teachers."
"1406760","Collaborative research: A major leap forward: Optimal designs for correlated data, multiple objectives, and multiple covariates","DMS","STATISTICS","07/15/2014","07/07/2014","John Stufken","GA","University of Georgia Research Foundation Inc","Continuing Grant","Gabor Szekely","01/31/2015","$77,236.00","","jstufken@gmu.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","","$0.00","Designed experiments form an integral part of the scientific process in many areas of research, such as the biological sciences, the health sciences, the social sciences, engineering, marketing, education, and others. A well-chosen design facilitates the collection of data that maximizes the information for the scientific questions of interest at a fixed cost, or that minimizes the cost for a desired level of information. Many experiments deal with correlated data, multiple objectives, or multiple covariates, but little is known about the identification of good designs in such settings. This project establishes how to find efficient designs for these types of problems for the most commonly used statistical models. The tools developed in this project have a tremendous potential for impact on society because designed experiments are used so often to further knowledge in many different fields. Results from the project will be made available to researchers in other areas through easy-to-use software that implements algorithms that are developed. Graduate students will be trained to become researchers in design of experiments. <br/><br/>The outcomes of this project constitute a major leap forward in understanding and knowledge of optimal design of experiments. Recent contributions by the principal investigators and others have had a significant impact on the advancement of optimal design of experiments for nonlinear and generalized linear models. However, these results have for the most part been limited to (i) independent data; (ii) use of a single optimality criterion; and (iii) use of a single covariate. While these results are arguably important in their own right, this project will extend methods and tools to problems with correlated data, multiple objectives, and multiple covariates. The latter could consist of a mix of covariates that can be chosen by the experimenter and covariates that, known or unknown at the design stage, cannot be controlled by the experimenter. Preliminary results indicate that this is an opportune time to make these challenging but critical steps. Building a framework for deriving and identifying optimal designs for these types of problems will provide a much needed addition to our collective design toolbox. Current results are very sparse and only for very specialized problems that are mostly motivated by mathematical feasibility. The project develops tools to select efficient designs for models and conditions that are far more realistic than those that have been considered so far."
"1356055","Travel Support for the XIII Latin American Conference on Probability and Mathematical Statistics","DMS","PROBABILITY, STATISTICS","03/01/2014","02/19/2014","Abel Rodriguez","CA","University of California-Santa Cruz","Standard Grant","Tomek Bartoszynski","02/29/2016","$17,000.00","","abelrod@uw.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1263, 1269","7556","$0.00","The Latin American Conference on Probability and Mathematical Statistics (CLAPEM, by its initials in Spanish) is the main event in probability and statistics in the region, having been held roughly every two to three years for almost 30 years. The series of CLAPEMs has greatly contributed to the development of probability and statistics in Latin America, and the NSF has provided support for the conference during the last 15 years.  Accordingly, this award will provide travel support for four senior and at least eight junior U.S.-based researchers who will be participating in the XIII CLAPEM, to be held in Cartagena, Colombia, from September 22nd to September 26th, 2014. The event includes a diverse and high-quality scientific program with short courses (Theory of Statistical Learning, Mathematical Genetics and Bioinformatics, and Quantitative Risk Management), plenary talks (High Dimensional Data Analysis, Stochastic Processes, and Quantum Information Theory), special thematic sessions (Functional Data, Particle Systems, Sampling Methods and Random Segmentation Models, among others), and contributed oral and poster presentations.  The participants include researchers from all over Latin America, Europe and the United States, providing an excellent forum for the development of collaborations and for the recruitment of highly qualified foreign students.<br/><br/>The National Science Foundation (NSF) has long recognized the importance to the United States of the participation of U.S.-based researchers in international scientific events.  In particular, NSF has long supported the participation of U.S.-based researchers in the Latin American Conference on Probability and Mathematical Statistics (CLAPEM, by its initials in Spanish).  This award will support the participation of both junior and senior researchers working at U.S. institution in the upcoming XIII CLAPEM, which is to be held in Cartagena, Colombia, from September 22nd to September 26th, 2014.  The participation of these researchers in the XIII CLAPEM will have a positive impact in the region (by increasing the quality of the conference's scientific program and facilitating collaborations between U.S.-based and Latin American researchers) and in the U.S. (by maintaining our country's international leadership in research on probability and statistics and helping to attract the best and brightest young Latin American minds to graduate programs in the U.S.). The conference website is at http://www.clapem.unal.edu.co/"
"1407665","Collaborative Research: Information Matrix Analysis for Nonparametric Multivariate Problems","DMS","STATISTICS","08/15/2014","08/09/2014","Weixin Yao","KS","Kansas State University","Continuing Grant","Gabor Szekely","10/31/2014","$35,618.00","","weixin.yao@ucr.edu","1601 VATTIER STREET","MANHATTAN","KS","665062504","7855326804","MPS","1269","9150","$0.00","This project develops a new set of tools, called Information Matrix Analysis (IMA), to explore the structure of multivariate data. With modern data gathering devices and vast data storage space, researchers can easily collect high-dimensional data, such as biotech data, financial data, satellite imagery, and hyperspectral imagery. Analysis of such high-dimensional data poses great challenges for statisticians due to the so-called ""curse of dimensionality"". The IMA methodology tackles this problem by finding a smaller number of linear combinations of the original variables that will carry most of the information of the original multivariate data. The IMA is based on the eigenanalysis of the Information Matrix as defined for different problems. The eigenvectors of the proposed information matrices provide the linear combinations of the variables that best summarize the useful features of the data due to their high information content. By combining the new method with the random projection method, one can also apply IMA to ultra high dimensional data. Alternatively, one can do variable selection with IMA. The new data analysis tool under development in this project is timely due to the data revolution, as well as broadly applicable. All of the IMA methodology springs from a common foundation, and so is easily transported to tackle new problems. This project will enhance significantly the availability of statistical tools and software for statistical modeling and exploration for multivariate data. The new method will benefit a broad range of scientists and researchers who want to analyze high-dimensional data in various fields, including medical studies, prevention studies, public health, and the social sciences.<br/><br/>This project will accelerate geometric understanding of Fisherian information matrices. The core of this project consists of the application of IMA to three very different problem areas: building models, assessing models, and comparing populations. Suppose one wants to build a model to analyze the relationship between a response variable and multivariate covariates. The IMA of a defined covariate information matrix can be applied to find a smaller number of linear projections of the original covariates to simplify model building. The new projected variables have the nice explanation of carrying most information, as measured by the defined covariate information, about the relationship between the covariates and the response. The IMA can be also generalized to find linear combinations of the data that can provide the best discrimination between two densities. In the applications, one density will be the true unknown density, to be estimated nonparametrically, and the other density will be some model for the data, which could be parametric or semiparametric. Alternatively, the two densities could represent two distinct populations one wishes to compare. When the two populations are multivariate normal with the same covariance matrix, the IMA provides the same linear projection as the Fisher linear discriminant direction. However, the IMA can move beyond linear discriminant analysis to multiple linear discriminants. The IMA will be further applied to find linear projections of the data that are useful for assessing the fit of a proposed model, whether parametric, semiparametric, or nonparametric. This project will investigate the applications of IMA to popular graphical models and independent component analysis models. It is expected that IMA might have many more application areas. For high dimensional data, the proposal will develop the use of the random projection method to first reduce the dimension of the original variables and restrict the space of possible linear combinations. Then the IMA can be applied to a much lower dimensional data set. Alternatively, one can do variable selection with IMA by imposing the sparsity of the informative projections."
"1404891","Collaborative Research: Second Order Inference for High-Dimensional Time Series and Its Applications","DMS","STATISTICS","08/15/2014","06/02/2016","Xiaohui Chen","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Gabor Szekely","07/31/2019","$151,391.00","","xiaohuic@usc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","","$0.00","During the last decade, analysis of high-dimensional independent data has gained substantial attention. The project involves a variety of estimation and statistical inference problems for high-dimensional multiple time series, a universal type of data seen in a broad spectrum of real applications in spatio-temporal statistics, biomedical engineering, environmental science, finance, and signal processing. As an important research problem, one should extract information from a large number of time series, where the second order structure plays a fundamental role in those applications.<br/><br/>Results developed from this project will provide the theoretical foundation for estimating and inference of the space-time covariance and precision matrix, their related functionals, and time-varying graphs of high-dimensional time series. All of the problems are linked together to characterize the second order properties of the high-dimensional time series with the non-linear and non-stationary time dependent features. We will also study enhanced methods that account for the temporal and spatial dependence structures. Results from this research are useful for understanding the dynamic features of high-dimensional dependent data. In particular, the techniques are applicable to biomedical engineering problems such as modeling brain connectivity networks by using fMRI data."
"1406536","Collaborative Research: Theory and Methods for Massive Nonstationary and Multivariate Spatial Processes","DMS","STATISTICS","08/01/2014","07/29/2014","William Kleiber","CO","University of Colorado at Boulder","Standard Grant","Gabor Szekely","07/31/2018","$307,938.00","","william.kleiber@colorado.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1269","","$0.00","The field of spatial statistics is an expanding subset of statistical science with numerous applications in a wide variety of specialties such as geophysical, environmental, ecological and economic sciences.  Modern datasets in these sciences often involve multiple variables observed at thousands to millions of irregularly spaced geographical locations.  Associated scientific goals include surface estimation, stochastic simulation and statistical modeling to gain insight of underlying phenomena.  Statistical analyses require flexible nonstationary and multivariate constructions, which have heretofore been hampered by a lack of models adequate for datasets of large magnitude.  This project addresses this gap in statistical science, developing a unifying framework for nonstationary and multivariate spatial models capable of modeling complex spatial dependencies.  Additionally, the justification for the use of nonstationary models is generally relegated to empirical results with data and simulation experiments; this research will develop a companion theory for exploring the relative benefit of these more complex spatial models. Using the tools introduced in this project, the final major goal is to develop a gridded data product for the historical climate of the United States based on large, irregularly spaced observational networks with transparent statistical methodology and formal quantification of the uncertainty in such an analysis.  Historical data products such as this are of crucial importance in the fields of atmospheric and climate sciences.<br/><br/>Modern spatial statistics has increased focus on developing methods for massive spatial datasets that involve multiple variables with complex dependency structures.  This research aims to foster a common framework via multiresolution processes for modeling nonstationary and multivariate spatial structures that does not break down in the face of large sample sizes.  Multiresolution processes lend themselves to fast estimation and computation, and also to the linked theoretical questions of asymptotic behavior of spatial estimators.  For example, there is a lack of rigorous theoretical treatment of nonstationary approaches, with current understanding limited to experimental results.  The companion large sample theory of this research is aimed at identifying situations in which nonstationary models provide tangible benefits over simpler stationary cousins.  A linked goal is approximation theory for existing spatial constructions; special multiresolution constructions can approximate existing covariances such as the Matern, allowing for a theoretical treatment of spatial smoothing under these common classes of covariances.  Additionally, the project will generalize the notion of a multiresolution process to the multivariate setting, allowing for feasible and flexible inference-based modeling of massive multivariate spatial datasets."
"1506125","Collaborative research: A major leap forward: Optimal designs for correlated data, multiple objectives, and multiple covariates","DMS","STATISTICS","08/16/2014","09/07/2016","John Stufken","AZ","Arizona State University","Continuing Grant","Gabor Szekely","06/30/2018","$235,314.00","","jstufken@gmu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1269","","$0.00","Designed experiments form an integral part of the scientific process in many areas of research, such as the biological sciences, the health sciences, the social sciences, engineering, marketing, education, and others. A well-chosen design facilitates the collection of data that maximizes the information for the scientific questions of interest at a fixed cost, or that minimizes the cost for a desired level of information. Many experiments deal with correlated data, multiple objectives, or multiple covariates, but little is known about the identification of good designs in such settings. This project establishes how to find efficient designs for these types of problems for the most commonly used statistical models. The tools developed in this project have a tremendous potential for impact on society because designed experiments are used so often to further knowledge in many different fields. Results from the project will be made available to researchers in other areas through easy-to-use software that implements algorithms that are developed. Graduate students will be trained to become researchers in design of experiments. <br/><br/>The outcomes of this project constitute a major leap forward in understanding and knowledge of optimal design of experiments. Recent contributions by the principal investigators and others have had a significant impact on the advancement of optimal design of experiments for nonlinear and generalized linear models. However, these results have for the most part been limited to (i) independent data; (ii) use of a single optimality criterion; and (iii) use of a single covariate. While these results are arguably important in their own right, this project will extend methods and tools to problems with correlated data, multiple objectives, and multiple covariates. The latter could consist of a mix of covariates that can be chosen by the experimenter and covariates that, known or unknown at the design stage, cannot be controlled by the experimenter. Preliminary results indicate that this is an opportune time to make these challenging but critical steps. Building a framework for deriving and identifying optimal designs for these types of problems will provide a much needed addition to our collective design toolbox. Current results are very sparse and only for very specialized problems that are mostly motivated by mathematical feasibility. The project develops tools to select efficient designs for models and conditions that are far more realistic than those that have been considered so far."
"1407530","Random Matrix Approach to High-Dimensional Time Series","DMS","STATISTICS","08/01/2014","05/31/2016","Debashis Paul","CA","University of California-Davis","Continuing Grant","Gabor Szekely","07/31/2018","$329,959.00","Alexander Aue","debpaul@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","High-dimensional time series arise naturally in economics, atmospheric and environmental science, genomics, experimental chemistry, wireless communications, and a multitude of other disciplines. Recent developments in the statistical analysis of data with large number of features have demonstrated the importance for developing new paradigms for qualitative as well as quantitative summaries. Exploration of the behavior of some widely used descriptive statistics has resulted in the discovery of new phenomena, and these theoretical investigations in turn have contributed to the development of sophisticated statistical procedures geared towards analyzing high-dimensional data. This pursuit benefited from the confluence of knowledge from various disciplines such as probability theory, optimization, geometry, and computer science. Random matrix theory has contributed significantly to the aforementioned theoretical developments. The primary goal of this project is to introduce the random matrix perspective to the study of multivariate time series, and utilize the resulting theoretical developments to build statistical methodologies for analyzing large and complex time series data. There are several ways in which this project is expected to have an impact on the scientific community and beyond. This research has potential direct applications to econometrics and finance. The research findings are also expected to influence model building and data analysis techniques in climate studies, environmental science and communications theory. The findings will give wider access to practitioners in various fields to modern statistical tools and concepts for dealing with large volumes of temporally observed data. Students working in this project will be well-versed in a multitude of disciplines through the merger of mathematical, computational and data analytic skills. The training component of this project involves giving exposure to undergraduate and graduate students to modern statistical and mathematical techniques and research problems through short courses and directed individual and group studies. This will facilitate their smooth transition into advanced academic programs and industry jobs specializing in cutting-edge technologies. <br/><br/>In this project, techniques of random matrix theory will be extended to analyze the behavior of sample covariance and symmetrized auto-covariance matrices and spectral density matrices for high-dimensional time series. These statistics are the primary building blocks for modeling and prediction of time-dependent data. A major motivation of this proposal is to the infer nature of dependence in large dimensional time series from the spectral characteristics, such as the empirical distribution of eigenvalues, of the sample covariance and auto-covariance matrices. The theory developed in this proposal extends the frontier of random matrix theory to the domain of dependent data with special structures. Another aim of the project is to develop tools for statistical estimation and prediction for high-dimensional time series and to analyze the performance of these procedures by blending mathematical and computational techniques. This research will also broaden the scope of interface among disciplines such as statistics, applied mathematics, econometrics and engineering."
"1349415","CAREER: An Efficient Framework for Design and Modeling of Complex Computer Experiments","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2014","05/22/2018","Ying Hung","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","06/30/2019","$400,000.00","","yhung@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269, 8048","1045","$0.00","The primary objective of this research is to develop an efficient framework for design and modeling of complex computer experiments, especially those with diverse and high-dimensional inputs and massive outputs. Computer experiments, i.e., experiments using simulation or numerical codes, have been widely used as alternatives to physical experiments, especially for studying complex phenomena. The investigator introduces new classes of designs that can efficiently accommodate large numbers of both quantitative and qualitative factors in computer experiments. The investigator also proposes a new adaptive design that is flexible and robust, yet takes into account uncertainties in the complex systems. Apart from experimental design, a novel sampling/modeling technique is proposed to reduce computational complexity and quantify model uncertainty in the analysis of computer experiments with massive data.<br/><br/>The proposed approaches are readily applicable to a variety of scientific disciplines and will have immediate impact on accelerating discoveries in numerous fields involving complex experiments like biomechanical engineering, systems biology, and environmental science. In particular, the proposed modeling techniques can dramatically enhance the efficiency and prediction accuracy in the analysis of cell adhesion, which plays an important role in tumor metastasis in cancer research. Moreover, the proposed methods can benefit the analysis of massive data for climate change, response to natural disasters, and the spread of pandemic disease. Integrated into the research outlined in this proposal is an education plan that emphasizes interdisciplinary training for a broad body of students and increasing participation from underrepresented groups. Results of the proposed research will be integrated into the Research Experience for Undergraduates program offered by Rutgers. Female and undergraduate students from underrepresented groups will be recruited and actively involved in the PI's research through Rutgers's highly successful Research in Science and Engineering program. Software will be written, which allows graduate and undergraduate students to have hands-on experience to implement the new methods on real examples."
"1407537","Non-gaussian graphical models via additive conditional independence and nonlinear dimension reduction","DMS","STATISTICS","07/15/2014","07/08/2014","Bing Li","PA","Pennsylvania State Univ University Park","Standard Grant","Gabor Szekely","06/30/2017","$210,000.00","","bing@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","Statistical networks and graphical models are two of the most important components of contemporary data analysis. They have important applications in Genomics, sociology, machine learning, study of the internet, and homeland security. Current statistical graphical models require strong assumptions in order to be computationally feasible for estimating large-scale networks but these assumptions also severely limit their applications.  In this project the principal investigator will lay out the groundwork for developing a new class of statistical graphical models that do not rely on these strong assumptions but at the same time retain the computational simplicity of the current models. The new class of models will greatly expand the scope and capability of current methods for analyzing networks that are becoming increasingly prevalent in modern applications.<br/><br/>In this project the principal investigator will develop a class of nonparametric graphical models that avoid the Gaussian or copula Gaussian assumptions. This new class of models can handle intrinsically nonlinear interactions that cannot be captured by a copula Gaussian model. A fully nonparametric approach, however, would involve high-dimensional kernels, which perform poorly due to the ""curse of dimensionality."" This disadvantage is especially noticeable for large-scale  networks. For this reason, the principal investigator will introduce two dimension reduction mechanisms into the nonparametric approach: additive conditional independence and nonlinear sufficient dimension reduction. Additive conditional independence is a new statistical relation that resembles the Gaussian interaction structure without being restricted by the Gaussian (or copula Gaussian) distributional assumption. The graphical models based on additive conditional independence can capture intrinsically nonlinear interactions and at the same time avoid high-dimensional kernels. The second mechanism incorporates ideas and techniques from the most recent advances in nonlinear sufficient dimension in statistics and machine learning into the graphical models to reduce the dimension of the mapping kernels."
"1419219","Conference on Nonparametric Statistics for Big Data, June 4-6, 2014","DMS","STATISTICS","06/01/2014","03/21/2014","Brian Yandell","WI","University of Wisconsin-Madison","Standard Grant","Gabor Szekely","05/31/2015","$10,000.00","Hao Zhang, Xiaotong Shen","byandell@wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","7556","$0.00","The Department of Statistics at the University of Wisconsin at Madison will host a Workshop on Nonparametric Statistics for Big Data (June 4-6, 2014, Madison, WI). Nonparametric statistics is a fundamental area of statistics, at the interface of mathematics, statistics, data mining, engineering, and computer science. The complexity and scale of big data impose tremendous challenges for knowledge discovery; they meanwhile demand more powerful and flexible analysis techniques. In recent years, the field of nonparametric statistics has seen significant development in theory, methods, and computation to address emerging issues in big data analysis. New breakthroughs in nonparametric theory have broadened the horizon of classical large-sample asymptotic inferences to accommodate high and ultra-high dimensional situations. A variety of cutting-edge statistical methodologies and state-of-art computational algorithms have been created for big data visualization, geometric representation, dimension reduction, and modeling and inference. These tools have made significant impacts on sciences, engineering, and industry. A broad range of topics will be covered in the workshop, including sparse nonparametric regression, regularization and feature selection, high-dimensional inference and theory, spatial and environmental statistics, image analysis, functional data analysis, as well as related topics in statistical machine learning such as supervised learning, clustering, network analysis, large-scale optimization, computational biology and bioinformatics.<br/><br/>Due to recent technology advances, Big Data are collected ubiquitously in many scientific investigations, such as in biological, genomic, medical, climate, social, and environmental sciences. Given the complexity and huge range of systems being measured, a wealth of new ""nonparametric"" tools have been emerging that require few assumptions and that adapt to the patterns found in big data. This workshop will bring together broad interdisciplinary expertise from mathematics, statistics, computer science, machine learning, engineering, and biomedical research to highlight cutting-edge research from nationally and internationally renowned scholars and researchers. The workshop will use NSF funding for travel awards to attract graduates students and young researchers, with special attention to women and underrepresented minorites. It will create a unique opportunity for young researchers to interact with leading scientists. Through 30 plenary talks (expository, intermediate, and advanced), open floor discussions, and two poster sections, the workshop will promote new connections and collaborations. Further, this workshop provides an important review that will highlight future research directions nonparametric statistics for big data analysis.<br/><br/>Workshop web site:  http://www.stat.wisc.edu/workshop-npbigdata"
"1542332","Statistical Inference for Functional Data in Time Series and Survey Sampling: Theory and Methods","DMS","STATISTICS","07/01/2014","04/16/2015","Lily Wang","IA","Iowa State University","Standard Grant","Gabor Szekely","07/31/2018","$99,999.00","","lwang41@gmu.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","","$0.00","Sophisticated data collection facilities often produce data which are a set of functions, represented in the form of curves, images or shapes. The development of functional data analysis in theory and methodology has provided us important analytical tools to address challenging problems encountered in many important fields. In the proposed research, the investigator continues to build and enrich the theory and methodology of functional data. This proposal targets the development of powerful statistical tools for analyzing functional data in time series and survey sampling frameworks. Four related research topics are proposed for investigation. For each project, statistical properties of the estimators, statistical inferences governed by the underlying models, and theoretical properties of the inferences will be studied. The proposed methods can be used to estimate global quantities for dependent functional data, quantify and visualize the variability of the estimators, and make global inferences on the shape of the population quantities. <br/><br/>With ""big data"" of complex (such as longitudinal, functional, heterogeneous, or correlated) features becoming increasingly available for public use in many research areas, this proposal is one vehicle to address the challenges of analyzing such types of data. The success of the proposed projects provides effective and practical tools for dealing with large and complex structural data over time and space, representing advances in the theory and methodology of statistical analysis. The benefits to society at large of the proposed research include new methodology and inference tools for big data with complex features. These topics are of interest to statisticians, survey researchers, and indeed, more broadly for researchers in climatology, health, economics, engineering, environmental studies, meteorology, behavioral and social sciences."
"1406747","Nonparametric Likelihood Enhancements for Dependent Data","DMS","STATISTICS","08/15/2014","08/07/2014","Daniel Nordman","IA","Iowa State University","Standard Grant","Gabor Szekely","07/31/2017","$119,997.00","","dnordman@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","9150","$0.00","The project aims to develop accurate statistical methodology for correlated data,  which applies without stringent assumptions about how data may arise.  Current statistical methodology often relies on specifying an adequate model for correlated data, which can be a difficult task, and any inference drawn from a mistaken model can be unreliable.  A direct benefit of this research is to provide alternative, model-free tools for statistical inference that are not susceptible to model choice and can advance data analysis in scientific areas such as environmetrics, economics, astronomy, etc., which encounter different forms of complex dependent data.  Additionally, climate predictions are increasingly relevant for mitigating natural disasters and planning the use of social/economic resources.  Research goals include developing new assessments of regional climate models to understand how scale differences in such models may impact climate forecasts.  <br/><br/>The research targets development of model-free resampling and nonparametric likelihood methods for different types of dependent data structures, temporally and spatially.  Three main research problems are:  (1) Investigation of optimal implementations of empirical likelihood for time series (as performance is linked to tuning parameters in currently unknown ways); (2) Study of spatio-temporal resampling methods to assess the concept of ""scale"" in geophysical and environmental processes, with interest in evaluating regional climate models; (3) Development of new resampling methods for irregularly located spatial observations, based on data-transformations, to advance inference with spatial data.  Such nonparametric methods can provide valid inference and assessments of dependence structures under mild distributional assumptions, and such methodology can also be helpful for informing model selection."
"1407604","Statistical inference for space-time models involving stochastic differential equations","DMS","STATISTICS","08/01/2014","08/05/2016","Peter Craigmile","OH","Ohio State University","Continuing Grant","Gabor Szekely","07/31/2019","$288,343.00","Radu Herbei","pfc@stat.osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269","","$0.00","Many fields have experienced a recent growth in the use of stochastic differential equations (SDEs) to model scientific phenomena over time. Examples include applications in oceanography, ecology, and public health. SDEs can simultaneously capture the known deterministic dynamics of the variables of interest (e.g., ocean flow, the chemical and physical characteristics of a body of water, the presence, absence and spread of a disease), while enabling a modeler to capture the unknown dynamics and measurement processes in a stochastic setting. This proposal develops statistical methodology for building, fitting, and diagnosing the fit of multivariate and spatially-varying SDEs. Such models, which are often inspired by mechanistic modeling, can incorporate the complex dynamics of the variables of interest.<br/> <br/>Although statistical methods for the fitting and analysis of SDEs models using data at a single location are becoming more widely used, accurate statistical methods for multivariate SDEs and SDEs indexed in space are far less developed. This project will derive improved approximate methods of inference for one- and multi-dimensional SDEs that are more accurate than the commonly used, but naive, Euler approximations. Spatially-varying SDE models will be built for modeling spatio-temporal data observed potentially irregularly in space and time. This research will be applied to address problems in applied disciplines. Education of students in statistical methods for SDEs will be an important goal of this project. In addition, a number of outreach programs will be used to educate a broader audience."
"1401793","Statistical tools for climate science","DMS","STATISTICS","07/01/2014","07/03/2014","Peter Guttorp","WA","University of Washington","Standard Grant","Gabor Szekely","06/30/2018","$120,000.00","","guttorp@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","","$0.00","In order to plan for adaptation to climate change, it is important to develop quantitative tools for the uncertainty of climate projections at global, region, and local levels. In this research we develop statistical tools that will let decision-makers get a clearer perspective on how to interpret the projections. For example, we predict (with attendant uncertainty) temperatures at unobserved sites, given that an observed site achieved the network maximum or minimum. <br/><br/>The technical tools used in this research include empirical process models for nonstationary and dependent observations. Data will include global temperature reconstructions, networks of temperature and precipitation stations from Sweden, Norway and the United States, and CMIP5 global model runs as well as regional climate model runs from CORDEX and other repositories.  We will also study nonstationary tools in extreme value theory, including spatial and spatio-temporal situations, and implement visualization methods for comparing output from ensembles of climate models to data."
"1407819","Statistical Inference on Memory Structure of Processes","DMS","STATISTICS","08/15/2014","07/31/2016","Zsolt Talata","KS","University of Kansas Center for Research Inc","Continuing Grant","Gabor Szekely","07/31/2018","$150,000.00","","talata@math.ku.edu","2385 IRVING HILL RD","LAWRENCE","KS","660457552","7858643441","MPS","1269","9150","$0.00","Finite-memory models of time series are used in statistics, information theory, bioinformatics and various other disciplines. Specific areas of applications include lossless data compression, universal prediction of individual sequences, linguistics to compare dialects of languages and genetics for modeling of DNA sequences. The Markov random field model can be regarded as a generalization of the finite-memory property to spatial processes. Markov random fields are special Gibbs fields, therefore they provide essential models in statistical physics for modeling interactive particle systems. They are also used in several other fields, including image processing and pattern recognition. Both models involve information sufficient in many applications but general enough to model large amount of data and to be computed in feasible time.<br/><br/>A discrete-time stochastic process is a Markov chain of order k if the memory depth of the process is k. Estimating the order from a sample, a finite-length observation of the process, is a statistical model selection problem. The project considers the penalized maximum likelihood method and studies statistical inference on the Markov order for stationary ergodic and other processes. A stochastic process on a multidimensional integer lattice is a Markov random field if the distribution at a site depends only on the values of sites from a finite neighborhood, called basic neighborhood. The project addresses the problem of statistical estimation of the basic neighborhood from a sample, a single realization of the process observed in a finite region. Since large sample sizes are typical in applications, asymptotic behavior - such as consistency of the estimation procedures - is studied in addition to error probabilities for finite sample sizes. Computational complexity is a fundamental aspect in model selection problems so this property of the developed statistical procedures is studied as well. The research includes collaborative work with researchers in the area, and it also involves the potential of broadening the applications of the results in collaboration with researchers from the areas of applications, namely, from bioinformatics and engineering. The project provides opportunity for graduate students to collaborate in research."
"1407698","Statistical Methods for Data with Network Structure","DMS","STATISTICS","08/15/2014","08/07/2014","Ji Zhu","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","07/31/2018","$239,595.00","","jizhu@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","8396, 8609","$0.00","Recent advances in computing and measurement technologies have led to an explosion in the amounts of data that are being collected in all areas of application. Much of these data have complex structure, in the form of text, images, video, audio, streaming data, and so on. This project focuses on one important class of problems, viz, data with network structure. Such data are common in diverse engineering and scientific areas, such as biology, computer science, electrical engineering, economics, and sociology. While there has been extensive research on networks (primarily outside the field of Statistics), much of it deals with characterizing and modeling network structures using link information only. The goal of the current research program is to exploit the node features as additional information and develop statistical methods that take into account both link and node information. The research program will make significant contributions in several areas, including Statistics, Biology, Computer Science, Electrical Engineering, Physics, Psychology, and Sociology. The educational program also includes substantial initiatives that will involve undergraduate and graduate students and expose them to state-of-the-art research in the topics related to the project.<br/><br/>The research aims to develop new statistical methodologies and associated theory that exploit the network structure in the data. Such data are becoming increasingly common in various fields. Specifically, the investigator aims to study three different but related problems: a) link prediction for partially observed networks, which deals with the situation where the network we observe is the true network with observation errors; b) community detection in networks with node features, which combines network link information and additional information on the nodes to improve community detection; c) learning network structures, which deals with the situation where one is interested in identifying the underlying network structure from the data."
"1407142","Emerging Issues in Modeling Longitudinal Observations with Censoring","DMS","STATISTICS","09/01/2014","07/01/2016","Bin Nan","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","09/30/2017","$300,000.00","","nanb@uci.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Longitudinal studies--commonly referred to as cohort studies in epidemiology or panel studies in sociology--are of fundamental importance in understanding time related issues within the same group of subjects. In longitudinal studies, the collection of information can be stopped at the end of the study, or at the time of dropout of a study participant, or at the time of the occurrence of a terminal event. Death, the most common terminal event, often occurs in aging cohort studies and fatal disease follow-up studies, e.g., organ failure or cancer studies. If not handled appropriately, the occurrence of terminal event can cause serious bias in statistical inference, leading to incorrect conclusions.  The current literature has primarily focused on modeling the longitudinally measured variables given that the terminal event has not happened yet, hence the observed repeated measures ""terminated"" by a terminal event are implicitly treated as incomplete data. Such a modeling strategy, however, is inappropriate for many studies when some effect of interest is directly related to the terminal event time, such as the medical cost data. In this project, a conditional modeling strategy will be implemented, which treats repeated measures up to a terminal event as complete data and directly models the effect of terminal event time on a response variable. A related problem is some predictor variable subject to limit of detection in regression analysis, which occurs frequently in studies involving assay measures, where measures of certain substance (e.g., hormone, air pollutant, or water contaminant) become unreliable when their concentrations are below certain level due to technology limitation. The current literature has focused on primarily ad hoc imputation or unverifiable model assumptions for the predictor variable subject to limit of detection, but these methods generally yield biased results. This project will tackle this issue using robust statistical methods, which follow similarly the conditional modeling strategy for terminal events, and yield more reliable results than existing approaches.<br/><br/>The project investigates modeling strategies that model the effect of terminal event time directly by treating it as a covariate in longitudinal studies. In such statistical models, the usual relationship of interest between the longitudinally measured response variable and covariates is kept when data collecting time is far from the terminal event time, and the relationship becomes increasingly related to the terminal event time when data collecting time is close to the terminal event. Such models provide much more intuitive and sensible interpretations, and can be applied to recurrent events data with the presence of a terminal event. Both parametric and semiparametric models will be considered. Estimating methods for parameters in the proposed models will be investigated. The asymptotic theory for the case that the terminal event time is subject to right censoring will be a major focus. A closely related set of longitudinal regression problems with censored covariates considered in this project is about the issue of detection limit for covariates. The validity of any estimating approach relies on how reliably one can model and estimate the tail distribution of the covariate subject to limit of detection. Due to the feature of this type of data, parametric models are not verifiable and nonparametric models are not able to gain any useful information about the missing tail distribution. The project investigates semiparametric models, which are able to gain useful information from observed data and are insensitive to model misspecification of the missing tail probabilities."
"1406712","Set based tests for genetic association and gene-environment interaction in longitudinal studies","DMS","STATISTICS","09/01/2014","09/08/2016","Bhramar Mukherjee","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","08/31/2017","$149,998.00","Min Zhang","bhramar@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Most human diseases have a multifactorial etiology, characterizing complex interplay of multiple genes and environmental factors. The effects of these genetic and environmental factors on disease risk are likely to change dynamically over different life stages. Longitudinal studies of risk factors for common and chronic diseases like blood pressure, body mass index, provide a valuable opportunity to explore how genetic variants affect these traits over time. The ability to detect disease susceptibility genes can be improved if we jointly utilize the entire set of longitudinal outcomes. Moreover, since disease risk factors and phenotypes are likely influenced by the joint effect of multiple variants in a gene or in a genomic region, a joint analysis of these variants considering linkage disequilibrium and potential interactions among the variants may help to explain additional heritability. Integrating repeated measures of environmental exposure data into these genetic association models will help to identify specific sub-groups of individuals who may be more susceptible to environmental exposures. Identification of gene-environment interactions may have implications for targeted intervention and prevention. In this project, the investigators will try to utilize the temporally varying outcome-exposure profile available in a longitudinal genetic association study to enhance the power of statistical tests for genetic association and interaction. Using data from a muti-ethnic cohort, the project team will explore time-dependent genetic associations and gene-environment interactions with genes and pathways as the unit of analysis instead of a single marker at a given locus. This approach is biologically more meaningful as genes are the functional units, not the single nucleotide polymorphisms and by joint analysis of rare and common genetic variants in a region one may be closer to capturing functional variation. Several environmental factors measuring an individual's diet, physical activity, psychosocial behavior and perception of the neighborhood they live in will be considered in the planned analysis.<br/><br/>There are several technical challenges that will be addressed in the project. A primary goal of the study team will be to develop simple generalized score tests derived under a random field model involving multiple phenotypes, genes and environmental factors in a longitudinal study. The approach reduces the dimensionality of the inference problem by translating the association testing involving many predictors in terms of a reduced number of parameters and resultant tests with reduced degrees of freedom. The developed methods will use and extend classical spatial random field theory and recent results on multi-marker tests to characterize complex time-dependent associations and interactions. Several essential methodological improvements necessary for handling longitudinal data will be carried out to enhance the robustness to misspecification of within subject correlation structure and to improve computational efficiency. The methods will then be extended to a gene-environment set association test using longitudinal data. Several important dimension reduction techniques to handle correlated environmental exposure data are proposed. The project team also considers treatment of time varying exposure and time varying interaction effects under this set-based framework. There are no multi-marker based tests presently available in the literature that use the richness of longitudinal outcome and exposure data and the current project is expected to fill that gap. To summarize, the project introduces a novel genetic random field framework to formulate this class of multivariable association problems involving disease outcomes, gene, environment, and time to lead to powerful statistical inference."
"1351362","CAREER: Geometric approaches to hierarchical and nonparametric model-based inference","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2014","07/02/2018","Xuanlong Nguyen","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","06/30/2020","$400,000.00","","xuanlong@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269, 8048","1045","$0.00","Hierarchical and nonparametric models present some of the most fundamental and powerful tools in modern statistics. Despite valuable advances made in the past decades, there are several widely recognized and emerging problems. First, even as these models are increasingly applied to large data sets and complex domains, a statistical theory for inferential behaviors of the hierarchy of latent variables present in the models is not yet available. Second, local inference methods based on sampling, although simple to derive, tend to converge too slowly, thereby losing their effectiveness. Third, most existing methods are incapable of handling highly distributed data sources, which are increasingly responsible for the influx of big data. Addressing these challenges requires fundamentally new ideas in theory, modeling and algorithms that must account for the contrast and interplay between the global geometry of an inference problem and the need for decentralization of inference and algorithmic implementation.  This project aims to make fundamental contributions toward advancing hierarchical model-based inference. They include a statistical theory for the latent hierarchy of variables and for analyzing the effects of transfer learning. They also include variational inference algorithms based on the global geometry of latent structures and geometric analyses of the tradeoffs between statistical and computational efficiencies. Both the algorithms and theory are unified by the use of Wasserstein geometry, which arises from the mathematical theory of optimal transportation.  Moreover, scalable hierarchical models will be developed that can exploit highly distributed data sources and decentralized inference architectures.<br/><br/>This research will improve our ability to manage, analyze and make decisions with large-scale, high dimensional and complex data, especially in the research and applications of networks and the environment. The decentralized detection algorithms for highly distributed data sources have the potential of advancing the state of the art technologies that support data-driven and high-performance distributed computing architectures. As such, this research has the potential of extending the capabilities of the real-time detection and tracking devices currently deployed in the health-care and security domains. The optimal transport based theory will deepen our understanding of hierarchical Bayesian inference, a fundamental concept of modern statistics. The algorithms and geometric analyses will provide useful tradeoffs between statistical and computational complexity, an important issue lying in the interface of Statistics and Computer Science. This research will also provide support for broadening the current statistics curriculum at the University of Michigan. The PI will integrate the teaching of statistical and computational tools with modern applications, by developing synthesis courses which interact closely with research topics of the project.  This provides an excellent opportunity to train students with a broad base of knowledge and cross-disciplinary skills in the fields of statistics, probability, machine learning, distributed computation and networked systems."
"1407639","Collaborative Research: Information Matrix Analysis for Nonparametric Multivariate Problems","DMS","STATISTICS","08/15/2014","08/10/2016","Francesca Chiaromonte","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","07/31/2019","$240,000.00","","chiaro@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","This project develops a new set of tools, called Information Matrix Analysis (IMA), to explore the structure of multivariate data. With modern data gathering devices and vast data storage space, researchers can easily collect high-dimensional data, such as biotech data, financial data, satellite imagery, and hyperspectral imagery. Analysis of such high-dimensional data poses great challenges for statisticians due to the so-called ""curse of dimensionality"". The IMA methodology tackles this problem by finding a smaller number of linear combinations of the original variables that will carry most of the information of the original multivariate data. The IMA is based on the eigenanalysis of the Information Matrix as defined for different problems. The eigenvectors of the proposed information matrices provide the linear combinations of the variables that best summarize the useful features of the data due to their high information content. By combining the new method with the random projection method, one can also apply IMA to ultra high dimensional data. Alternatively, one can do variable selection with IMA. The new data analysis tool under development in this project is timely due to the data revolution, as well as broadly applicable. All of the IMA methodology springs from a common foundation, and so is easily transported to tackle new problems. This project will enhance significantly the availability of statistical tools and software for statistical modeling and exploration for multivariate data. The new method will benefit a broad range of scientists and researchers who want to analyze high-dimensional data in various fields, including medical studies, prevention studies, public health, and the social sciences.<br/><br/>This project will accelerate geometric understanding of Fisherian information matrices. The core of this project consists of the application of IMA to three very different problem areas: building models, assessing models, and comparing populations. Suppose one wants to build a model to analyze the relationship between a response variable and multivariate covariates. The IMA of a defined covariate information matrix can be applied to find a smaller number of linear projections of the original covariates to simplify model building. The new projected variables have the nice explanation of carrying most information, as measured by the defined covariate information, about the relationship between the covariates and the response. The IMA can be also generalized to find linear combinations of the data that can provide the best discrimination between two densities. In the applications, one density will be the true unknown density, to be estimated nonparametrically, and the other density will be some model for the data, which could be parametric or semiparametric. Alternatively, the two densities could represent two distinct populations one wishes to compare. When the two populations are multivariate normal with the same covariance matrix, the IMA provides the same linear projection as the Fisher linear discriminant direction. However, the IMA can move beyond linear discriminant analysis to multiple linear discriminants. The IMA will be further applied to find linear projections of the data that are useful for assessing the fit of a proposed model, whether parametric, semiparametric, or nonparametric. This project will investigate the applications of IMA to popular graphical models and independent component analysis models. It is expected that IMA might have many more application areas. For high dimensional data, the proposal will develop the use of the random projection method to first reduce the dimension of the original variables and restrict the space of possible linear combinations. Then the IMA can be applied to a much lower dimensional data set. Alternatively, one can do variable selection with IMA by imposing the sparsity of the informative projections."
"1407475","Solution of Sparse High-Dimensional Linear Inverse problems with  Application to Analysis of Dynamic Contrast Enhanced Imaging  Data","DMS","STATISTICS","09/01/2014","07/31/2016","Marianna Pensky","FL","The University of Central Florida Board of Trustees","Continuing Grant","Gabor Szekely","08/31/2017","$120,000.00","","Marianna.Pensky@ucf.edu","4000 CENTRAL FLORIDA BLVD","ORLANDO","FL","328168005","4078230387","MPS","1269","","$0.00","The project   is motivated by analysis of Dynamic Contrast Enhanced (DCE) imaging data.  DCE imaging  provides a noninvasive measure of tumor angiogenesis and has great potential for cancer detection and characterization. It offers an extremely useful tool for the evaluation and optimization of new therapeutic strategies as well as for long-term evaluation of therapeutic impacts of anti-angiogenic treatments. The current project will be greatly beneficial for <br/>a)  reducing health care costs by replacing expensive and invasive tests by analysis of medical images, and by shortening hospital stays for stroke patients due to better monitoring of drug effectiveness;<br/>b)  the medical practice, since development of novel  path-breaking methodologies for analysis of DCE imaging data will potentially improve clinical outcomes by providing non-invasive tools for cancer detection and characterization and for longitudinal evaluation of therapeutic impact of anti-angiogenic treatments;   <br/>c)  medical research, since the software for analysis of DCE imaging data will be freely available to everyone who carries out examination of such data and, thus, will contribute to design of new methodologies; <br/>d)  other types of medical imaging techniques, since methodologies resulted from the proposal will contribute to understanding of reconstruction of sparse high-dimensional functions in the presence of noise; and<br/>e) various fields of science such as geophysics and astronomy, which rely on solution of noisy inverse problems.<br/> <br/>The current project presents an integral effort of merging applications and theory. Mathematically, the problem reduces to solution of a noisy version of a matrix-variate Laplace  convolution equation based on discrete measurements. We shall  use very modern techniques designed for recovery of sparse representations that led to many successful developments in image and signal analysis and other applications. However, since majority of those methods rely on very stringent assumptions, only few of them have been adopted for solution of inverse ill-posed problems.  In addition, recently, acquisition of new types of data brought to light new types of the linear inverse problems where the function of interest is itself vector or matrix-valued. So far, the new challenge has been addressed by separate recovery of each component of the solution. However, recent developments in the area of sparse matrix estimation allow for much more coherent solution of the problem. Furthermore, in many applications, the operator itself is unknown and is estimated from data. Although often the uncertainty in the operator is ignored, we are planning to account for operator uncertainty. The goal of the present proposal is to extend techniques based on penalized optimization or exponential weights to solution of sparse ill-posed high-dimensional linear inverse problems where the operator may be not be known exactly and the function of interest is matrix-variate. We are planning to put a solid theoretical foundation under the proposed new methodologies, develop computational algorithms for their implementation, and apply the newly constructed algorithms to solution of Laplace convolution equation and, subsequently, to analysis of DCE imaging data obtained in ongoing REMISCAN studies."
"1407751","Robust Estimation for Structured Covariance Models","DMS","STATISTICS","08/15/2014","05/20/2016","David Tyler","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","07/31/2018","$119,999.00","","david.tyler@rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","The need to analyze multivariate data arises in many diverse disciplines, such as computer science and engineering, signal processing, psychology, meteorology, chemometrics, sociology, and biology. Due to the changing methods for collecting data, the number of variables or attributes measured in a single observation or for a single subject are becoming exceptionally large, and they can be considerably larger than the number of observations or subjects themselves. Such data sets are commonly referred to as large sparse data sets. For such data sets, the possibility of recording bad data points or outliers is increasingly likely. Outliers tend to have a disproportionate impact on the interpretation of the data unless one uses robust methods, that is, methods that can accommodate bad data. Developing methods to analyze large sparse data sets has become a major research topic within the field of statistics. There has been, however, relatively little attention given to the development of robust methods for large sparse data sets, which is the primary goal of this research project. The research project aims to produce fundamental results, theoretical approaches and statistical methods applicable to the robust analysis of large sparse data sets, upon which other researchers can build.<br/><br/>Most robust multivariate statistical methods are mainly applicable whenever the sample size is considerably larger than the number of variables, and are not particularly applicable to large sparse data sets. In particular, for sample sizes that are modest relative to the number of variables, robust affine equivariant estimates of multivariate location and scatter are similar in performance to the classical sample mean vector and sample covariance matrix, and consequently do not yield robust results for such data sets. Analyzing relatively sparse multivariate data tends to require either presuming certain covariance structures, such as those arising in graphical models, factor analysis or other reduced rank models, or developing methods which give preference to certain covariance structures via regularization methods. These special covariance structures are usually not considered in most robust multivariate methods. To address this shortcoming, the research project aims to develop robust methods which take into account a presumed covariance structure, and in particular to develop and study direct M-estimation methods and S-estimation methods for structured covariance models, as well as to develop and study penalized M-estimates of the covariance matrix. Addressing robustness issues for structured covariance models and for penalization methods are fundamental problems which is more mathematically and computationally challenging than in the classical setting or in the unrestricted robust estimation setting. Here, some recent work on geodesic convexity within the signal processing community is expected to play an important role in addressing these problems."
"1406279","Collaborative Research: Interface of Probability and Statistics for High-dimensional Inference","DMS","STATISTICS","08/01/2014","08/28/2017","Tiefeng Jiang","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","07/31/2019","$279,998.00","","jiang040@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","With rapid advances in information and technology, big data are now routinely collected in many frontiers of scientific research and technological developments. Various advanced techniques are needed to address the challenges such as computation, noise accumulation, and spurious correlations prominently featured in big data. New theoretical challenges arise in order to address probabilistic problems in high-dimensional statistics. This collaborative research project intends to combine the strength and expertise on probability and statistics of both investigators to address a number of emerging and challenging issues in high-dimensional data analysis. <br/><br/>The project will focus on primarily two types of problems: (i) high-dimensional statistics on spheres and (ii) behaviors of eigenvalues of random matrices and zeroes of polynomials. The two topics are innately related since high-dimensional data are often expressed in terms of matrices. The project would create new theories and methods for high-dimensional statistics, interact and apply them to several different disciplines including mathematics, statistical physics, information theory, and computer science. The project also involves training of top notch mathematical scientists and engineers and to disseminate the research results and products broadly via the online media, conferences and seminar presentations."
"1361869","Conference on Frontiers of Hierarchical Modeling in Observational Studies, Complex Surveys and Big Data, May 29-31, 2014","DMS","STATISTICS, Methodology, Measuremt & Stats","05/15/2014","04/03/2014","Yan Li","MD","University of Maryland, College Park","Standard Grant","Xiaoming Huo","04/30/2015","$10,000.00","Parthasarathi Lahiri","yli@survey.umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1269, 1333","7556","$0.00","Conference title: ""Frontiers of Hierarchical Modeling in Observational Studies, Complex Surveys and Big Data""<br/>Dates: May 29-31, 2014<br/>Location: University of Maryland at College Park <br/>Conference website: http://www.jointprogram.umd.edu/ghosh/ <br/><br/>This conference, titled ""Frontiers of hierarchical modeling in observational studies, complex surveys and big data,"" will cover a range of topics, which include Bayesian methods, random effects, hierarchical models, small area estimation, case-control studies, and large scale data analysis. The conference will promote the role of statistics in both interdisciplinary research and the general science.<br/><br/>The conference is expected to have an impact on the next generation of statistical researchers and educators, through interaction with peers and senior professionals from academia, industries, and government agencies. The conference will have an excellent slate of plenary, invited, contributed, and poster sessions. In addition, a special plenary session will be organized for poster presenters to give three-minute highlights of their papers. During the regular poster sessions, the conference attendees will have ample time to discuss their research. Students will be selected for travel awards based on their submitted papers/posters. The event will provide a forum for junior researchers and students from diverse backgrounds to interact with respected senior leaders in the field and with one another. While selecting the recipients, special consideration will be given to women and minority students."
"1407461","Nonparametric Maximum Likelihood Estimators for Multivariate Distributions and Related Inference Problems with Various Types of Censored Data","DMS","STATISTICS","08/15/2014","08/05/2016","Jian-Jian Ren","MD","University of Maryland, College Park","Continuing Grant","Gabor Szekely","07/31/2018","$120,000.00","","jjren@umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1269","","$0.00","In the analysis of multivariate survival data, we frequently encounter the situation where one or more components of a vector are not completely observable due to censoring. One common type of such data is when the survival time is subject to various types of censoring, and the covariate variables, such as treatments, gender, etc., are completely observable. Data examples of this type have been encountered in important medical research on bone marrow transplant, breast cancer, AIDS research, heart disease, etc. Another common type of censored survival data occurs when both components of the random vector are survival times that are subject to univariate or bivariate right censoring. Data examples of such type have been encountered in medical studies on skin grafts and kidney disease. The objective of this project is to study the empirical likelihood-based nonparametric maximum likelihood estimator (NPMLE) for multivariate distribution function with various types of censored multivariate survival data and to provide solutions and theoretical understanding of several important nonparametric and semi-parametric inference problems in survival analysis. The statistical methodology developed in this project will provide tools for multivariate survival data analysis, which has direct impact to medical research, epidemiology, and social and behavioral sciences.<br/> <br/>It is well-known that nonparametric distribution estimator of that of random vector X based on multivariate survival data is of great importance, because it provides tools to study the relation among the components of X and plays vital roles in modeling and testing, etc. It is also known that to study the effects of covariate Z on survival time T under semi-parametric model assumptions, such as linear models, the Cox model, accelerated life model, etc., the estimators under the model setting often can be expressed as statistical functional of the distribution estimator, thus the asymptotic properties of these estimators can be studied via the differentiability of these statistical functionals and the asymptotic properties of distribution estimators. However, most existing estimators with above mentioned survival data are ad hoc, and are not likelihood-based in the usual sense. Also, most of them either contain negative probability masses, or are kernel and bandwidth dependent. Since Owen (1988), the empirical likelihood function has been generally accepted as the nonparametric likelihood function. The essential idea of empirical likelihood-based NPMLE ensures that it is a proper multivariate distribution function, which is desirable in practice. But, the empirical likelihood-based NPMLE for aforementioned multivariate survival data had not been carefully considered in literature until a recent paper by the PI of this project, in which she discovered many surprisingly nice properties of the bivariate NPMLE with censored survival data, and studied its asymptotic properties for discrete covariate Z. In this proposed empirical likelihood, weighted empirical likelihood, asymptotic methods and simulations will be mainly used, and the issues under consideration include: (a) Derivation of empirical likelihood-based NPMLE for various types of censored multivariate survival data; (b) Computation algorithms and asymptotic properties of the resulting NPMLE; (c) Derivation and asymptotic properties of the statistical functionals under several important semi-parametric survival models. This project will provide a general methodology for constructing the multivariate distribution estimators with various types of censored multivariate survival data, which generally possesses desirable properties, and will provide solutions to several important and challenging statistical inference problems associated with some widely used survival models."
"1407543","Collaborative research:   Statistical and computational efficiency for massive data sets via approximation-regularization","DMS","STATISTICS","09/01/2014","05/22/2017","Darren Homrighausen","CO","Colorado State University","Standard Grant","Gabor Szekely","08/31/2017","$79,996.00","Darren Homrighausen","darrenho@stat.colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1269","","$0.00","This project integrates approximation methodology from computer science with modern statistical theory to improve analysis of large data sets. Modern statistical analysis requires methods that are computationally feasible on large datasets while at the same time preserving statistical efficiency. Frequently, these two concerns are seen as contradictory: approximation methods that enable computation are assumed to degrade statistical performance relative to exact methods. The statistical perspective is that the exact solution is undesirable, and a regularized solution is preferred. Regularization can be thought of as formalizing a trade-off between fidelity to the data and adherence to prior knowledge about the data-generating process such as smoothness or sparsity. The resulting estimator tends to be more useful, interpretable, and suitable as an input to other methods. Conversely, in computer science applications, where much of the current work on approximation methods resides, the inputs are generally considered to be observed exactly. The prevailing philosophy is that while the exact problem is, regrettably, unsolvable, any approximate solution should be as close as possible to the exact one. We make a crucial realization: that the approximation methods themselves naturally lead to regularization, suggesting the intriguing possibility that some computational approximations can simultaneously enable the analysis of massive data while enhancing statistical performance. <br/><br/>Our research develops new methods that leverage this phenomenon, which we have dubbed 'approximation-regularization.' The first method uses a matrix pre-conditioner to stabilize the least-squares criterion. If properly calibrated, this approach provides computational and storage advantages over regularized least squares while providing a statistically superior solution. A second innovation addresses principal components analysis (PCA) for regression on large data sets where PCA is both computationally infeasible and known to be statistically inconsistent. By employing randomized approximations, we can address both of these issues, while improving predictions at the same time. Lastly, we introduce new methods for unsupervised dimension reduction, whereby approximation algorithms that leverage sparsity, and statistical methods that induce it, enable the use of spectral techniques on very large matrices. In each of these cases, approximation-regularization yields both computational and statistical gains relative to existing methodologies. This research recognizes that approximation is regularization and can thereby increase statistical accuracy while enabling computation. It will result in new statistical methods for large datasets, which are computationally and statistically preferable to existing approaches, while also bringing attention to this important area in statistics. Additionally, these methods will permit scientists in other fields, such as astronomy, genetics, text and image processing, climate science, and forecasting, to make ready use of available data."
"1352060","CAREER: Nonparametric Eigenanalysis of High Dimensional Data","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2014","07/06/2018","Zongming Ma","PA","University of Pennsylvania","Continuing Grant","Gabor Szekely","06/30/2020","$400,016.00","","zongming.ma@yale.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","1269, 8048","1045","$0.00","This research project contains a specific research agenda that aims at a general understanding of high dimensional nonparametric eigenanalysis through three fundamental and complementary directions: (1) regularized estimation and variable selection, (2) statistical inference of eigenstructure, including hypothesis testing and confidence statements, and (3) minimax rates and complexity theoretic limits. The first direction will offer flexible regularization and variable selection of eigenstructure through regression based techniques, regularized power methods and other efficient algorithms. The second direction will lead to valid nonparametric testing procedures and confidence sets for eigenstructure with intrinsic sparsity. The third direction will characterize the fundamental limits of inferential accuracy in high dimensional eigenanalysis and its difference from what can be achieved by computationally efficient algorithms. The research will bring in new perspectives from other disciplines such as convex geometry, information theory and theoretical computer science to break new ground in high dimensional statistics. An effort will be made to seek out collaborations with scientists in topics that are related to the methodological and theoretical work.<br/><br/>Advances in science and technology have generated datasets of increasingly high dimensionality in many fields such as medical imaging, climate studies, and bioinformatics. When processing data arising in these applications, eigenanalysis of certain matrix objects, such as the covariance matrix, plays a central role in summarizing and unveiling the underlying structure of data, which often has intrinsic sparsity. This research project spans multiple directions, each aiming at advancing the methodological development and the theoretical understanding of high dimensional eigenanalysis from a different fundamental aspect. Upon completion, it will lead to a comprehensive general understanding of high dimensional eigenanalysis that will significantly enhance the ability to employ it in a wide range of scientific settings. The methodological development will be disseminated to the broader scientific community through user-friendly software packages that will serve as the basis for potential applications. The research output will also be effectively integrated with educational activities such as course development and mentoring at both undergraduate and graduate levels."
"1406456","Variable Selection via Measurement Error Modeling","DMS","STATISTICS","07/01/2014","07/03/2018","Leonard Stefanski","NC","North Carolina State University","Continuing Grant","Gabor Szekely","06/30/2019","$300,000.00","","stefansk@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","","$0.00","Technological advances make it possible to collect and store enormous amounts of data.  The implications for how businesses run (online retailing, precision manufacturing), how science is conducted (environmental science, climate monitoring and modeling, astrophysics), and how governments operate (health care delivery, public safety, homeland security) are comparably enormous.  However, for many particular uses of massive data sets, not all of the available information is relevant; and a key first step in many big-data explorations is the identification of the most relevant subset of information required to address the particular question at hand. For example, when studying certain diseases, it is essential to first identify the most relevant risk factors and precursors. The more information that is available, the more difficult it is to identify the most relevant subset for a particular purpose, akin to the problem of finding a needle in a haystack.  Just as a threshing machine separates the wheat from the chaff, the research in this project will develop statistical methods that separate the relevant information (the wheat) from that information that is not relevant (the chaff), thereby enabling more focused and productive analyses of large data sets.<br/><br/>More specifically, the research in this project will develop methods for identifying the subset of information that is most relevant when the data are used to derive a regression/prediction model or algorithm. In this case the problem of separating the wheat from the chaff is the often-studied problem of variable selection. This project will develop a new approach to variable selection that differs conceptually from existing approaches and promises to offer new insights as well as new methodologies. The new approach is based on the intuitive and universally relevant idea that a non-informative variable can be contaminated with noise without a subsequent loss of predictive power; whereas any amount of contamination to an informative predictor necessarily entails a loss of predictive power. Starting from the noise-contamination idea of variable informativeness, the project shows how the theory, methods, and algorithms from the field of measurement error modeling can be used to develop new methods of variable selection applicable across the full spectrum of model- and algorithmic-based prediction methods. Instances of the general strategy will be studied and refined for several particular prediction methods such as: nonparametric regression (based on splines, or kernels, etc.); classification/regression trees; dimension reduction methods (principle components, partial least squares, SIR, etc.); bagged or model-averaged predictors of any type; and ridge regression."
"1352213","CAREER:  New Developments on Experimental Designs for Pioneering Functional Brain Imaging Technologies","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2014","05/22/2018","Ming-Hung Kao","AZ","Arizona State University","Continuing Grant","Gabor Szekely","06/30/2020","$400,000.00","","mkao3@asu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1269, 8048","1045","$0.00","Understanding functions of the human brain is a very challenging yet crucially important task. To advance knowledge about brain functions, pioneering technologies such as electroencephalography (EEG), functional magnetic resonance imaging (fMRI), and functional near-infrared spectroscopy (fNIRS) have been widely used in neuroimaging experiments. Focusing on such neuroimaging studies, the investigator identifies optimal and efficient experimental designs to allow researchers to collect informative data for rendering precise and valid statistical inference on human brain functions. Specifically, the investigator will develop fundamental theories to provide insights and guidance on selecting optimal designs for fMRI studies. He will also develop efficient computational tools for obtaining designs for modern fMRI experiments with sophisticated experimental settings and/or advanced statistical analysis methods. Various practical situations will be considered. These will include cases where (i) the experimental subject's probabilistic behavior needs to be taken into account at the design stage; (ii) the hemodynamic response function, which models the subject's brain activity in response to a brief mental stimulus, can vary over time; and (iii) flexible statistical approaches such as semi- and non-parametric methods are considered for a better interpretation of the rather complex neuroimaging data. In addition to the popularly used fMRI, the investigator will obtain and study high-quality designs for experiments utilizing some other pioneering brain mapping techniques such as EEG and fNIRS. <br/><br/>The research results will allow researchers to select experimental designs to improve the quality of neuroimaging experiments for studying brain functions. They will benefit society by facilitating the use of pioneering brain mapping technologies in advancing knowledge about some terrifying brain disorders such as Alzheimer's disease, and about how the brains work when we learn, remember and make decisions. To further broaden the impact of the research, the investigator will provide free, user-friendly software packages for researchers and practitioners to easily obtain high-quality neuroimaging experimental designs. In addition, new courses on design of experiments will be developed. These courses will help students to gain practically useful knowledge and skills on design and analysis of modern scientific experiments."
"1407028","A reliable and scalable approach to causal inference for large-scale multivariate data","DMS","STATISTICS","08/15/2014","06/03/2016","Garvesh Raskutti","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","07/31/2017","$119,999.00","","raskutti@cs.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","With masses of large-scale data being generated, a key challenge facing many scientists is to infer relationships amongst variables of interest. In particular, inferring causal or functional relationships amongst genes, proteins, and other biological elements is of fundamental interest to scientists. This project will develop methods for inferring causal or functional relations between genetic, proteomic, and transcriptomic features both for the ENCODE human genome project and data for mice with different susceptibility to obesity and diabetes. For both types of data, this project will develop frameworks that comprise: (1) domain knowledge that informs the choice of model and algorithm; (2) fast, parallelizeable algorithms with provable run-time guarantees; and (3) statistical consistency guarantees for the algorithms developed under assumptions that are likely to be satisfied in practice.<br/><br/>Directed graphical models or Bayesian networks provide a useful framework for representing causal or functional relationships. A number of algorithms have been developed for inferring directed or Bayesian networks from data. However prior approaches are either unreliable as they require assumptions that are rarely satisfied in practice, or do not scale to larger datasets. The proposed project will address this issue by developing algorithms for inferring directed networks with both statistical consistency guarantees and run-time guarantees. The new algorithms will involve exploiting connections between techniques in numerical linear algebra for developing fast solvers of linear systems and concepts in graph theory. Algorithms will be coded in R and will exploit parallel processing. Evaluation will involve both small-scale and large-scale synthetic graphical models with known network structure, real datasets involving yeast data where some of the directions are known, and new biochemistry data in which most of the directions are unknown. Theoretical guarantees on run-time and statistical consistency will be provided using a combination of tools from graph theory, numerical linear algebra, and concentration of measure the PI has used and developed in prior work."
"1440121","Generalized Partially Additive Models For High-Dimensional Data","DMS","STATISTICS","04/01/2014","03/24/2014","Hua Liang","DC","George Washington University","Standard Grant","Gabor Szekely","07/31/2016","$63,270.00","","hliang@gwu.edu","1918 F ST NW","WASHINGTON","DC","200520042","2029940728","MPS","1269","","$0.00","The investigator studies generalized additive partially linear models (GAPLM) with the aim of developing efficient and flexible estimation and inference methods, variable selection procedures,model specification tests, and model structure checks, and studies applications of these methods for biomedical research. Specifically speaking, he (a) is developing a genuine method that is able to select important parametric and nonparametric components that are numerically stable , even when the numbers of the nonparametric and parametric components diverge; (b) is developing model specification tests for GAPLM; (c) studies model structure determination for GAPLM; (d) studies marginal GAPLM for correlated data; and (e) applies the advanced models and proposed methods to analyze gene data for study of the relationship between certain diseases and genes, including the identification of signature gene expression profiles of cancer cells in response to different drug treatments, the prediction of genetic risks through integrating knowledge from genetic variations in the genome and genomic markers, and validation of epigenetic codes from data collected through next generation sequencing platforms.<br/><br/><br/>The proposed models and methods are motivated by the investigator's study of gene and other potentially useful biomarkers in cancer clinical trials. The results of this project can help identify important gene expression profiles and cancer cells and trace the disease progression in cancer research. The theoretic results contribute to the advancement of the statistical theory on variable selections and semi-parametric inference with high-dimensional covariates."
"1407518","Collaborative research: A major leap forward: Optimal designs for correlated data, multiple objectives, and multiple covariates","DMS","STATISTICS","07/15/2014","05/06/2016","Min Yang","IL","University of Illinois at Chicago","Continuing Grant","Gabor Szekely","06/30/2018","$211,019.00","","myang2@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","","$0.00","Designed experiments form an integral part of the scientific process in many areas of research, such as the biological sciences, the health sciences, the social sciences, engineering, marketing, education, and others. A well-chosen design facilitates the collection of data that maximizes the information for the scientific questions of interest at a fixed cost, or that minimizes the cost for a desired level of information. Many experiments deal with correlated data, multiple objectives, or multiple covariates, but little is known about the identification of good designs in such settings. This project establishes how to find efficient designs for these types of problems for the most commonly used statistical models. The tools developed in this project have a tremendous potential for impact on society because designed experiments are used so often to further knowledge in many different fields. Results from the project will be made available to researchers in other areas through easy-to-use software that implements algorithms that are developed. Graduate students will be trained to become researchers in design of experiments. <br/><br/>The outcomes of this project constitute a major leap forward in understanding and knowledge of optimal design of experiments. Recent contributions by the principal investigators and others have had a significant impact on the advancement of optimal design of experiments for nonlinear and generalized linear models. However, these results have for the most part been limited to (i) independent data; (ii) use of a single optimality criterion; and (iii) use of a single covariate. While these results are arguably important in their own right, this project will extend methods and tools to problems with correlated data, multiple objectives, and multiple covariates. The latter could consist of a mix of covariates that can be chosen by the experimenter and covariates that, known or unknown at the design stage, cannot be controlled by the experimenter. Preliminary results indicate that this is an opportune time to make these challenging but critical steps. Building a framework for deriving and identifying optimal designs for these types of problems will provide a much needed addition to our collective design toolbox. Current results are very sparse and only for very specialized problems that are mostly motivated by mathematical feasibility. The project develops tools to select efficient designs for models and conditions that are far more realistic than those that have been considered so far."
"1407820","Collaborative Research: Better efficiency, better forecasting, better accuracy: A new light on the dependence structure in high frequency data","DMS","STATISTICS","08/15/2014","08/10/2014","Lan Zhang","IL","University of Illinois at Chicago","Standard Grant","Gabor Szekely","07/31/2018","$123,851.00","","lanzhang@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","","$0.00","Recent years have seen an explosion in the availability and size of data in many areas of endeavor; the phenomenon is often referred to as big data. This project is concerned with a particular form of such data, namely high frequency data (HFD), where series of observations can see new data to arrive in fractions of milliseconds. HFD occurs in medicine, in finance and economics, in certain recordings relating to the environment, and perhaps in other areas. Research is often concerned with how to turn this data into knowledge, and this is where the current project will help. Specifically, the project has discovered a new way to look at the dependence relationships between the parameters governing the state of the HFD system. The new dependence structure permits the borrowing of information from adjacent time periods, and also from other series if one has a panel of data. The consequences of this new approach are being explored by the project. The research produces transformational improvements in the statistical handling of high frequency data. <br/><br/>The new way to look at dependence involves the representation of series of ordinary integrals with the help of stochastic integrals. This permits the use of high frequency regression techniques to connect the information in adjacent time intervals. It is achieved without altering current models. This has far-reaching consequences, leading to more efficient estimators, better prediction, and, in terms of accuracy, a more systematic treatment of the estimation of standard errors. Model selection will also be greatly facilitated. The methodology does not depend on either time or panel size being large; neither does it depend on assumptions such as stationarity of the data series. All the new dependence relationships can be consistently estimated from high frequency data inside the relevant time periods. Efficiency gains are at the very least close to 50%, and thus existing efficiency bounds will become irrelevant. It is expected that this approach will form a new paradigm for high frequency data. In addition to developing a general theory, the project is concerned with applications to financial data. Applied quantities of interest include realized daily volatility, correlations, leverage effect, volatility risk, fraction of jumps, and so on. We also work on applications to risk management, forecasting, and portfolio management. More precise estimators, with improved standard errors, will be useful in all these areas of finance. The results are of interest to main-street investors, regulators and policymakers, and the results are entirely in the public domain. The dependence structure also has application in other areas of research that have high frequency data, including medicine, neural science, and turbulence."
"1406872","Nonparametric Statistics and Riemannian Geometry in Image Analysis: New Perspectives with Applications in Biology, Medicine, Neuroscience and Machine Vision","DMS","STATISTICS","08/01/2014","05/05/2016","Rabindra Bhattacharya","AZ","University of Arizona","Continuing Grant","Gabor Szekely","07/31/2017","$120,000.00","","rabi@math.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1269","","$0.00","This project aims at (1) precise geometric depictions of digital images arising in biology, medicine, machine vision and other fields of science and engineering and (2) providing their model-independent statistical analysis for purposes of identification, discrimination and diagnostics. One specific application is to discriminate between a normal organ and a diseased one in the human body. Among examples, one may refer to the diagnosis of glaucoma and certain types of schizophrenia based on shape changes. A subject that the project will especially look at and analyze in depth, concerns changes in the geometric structure of the white matter in the brain's cortex brought about by Parkinson's disease, Alzheimers, schizophrenia, autism, etc., and their progression.  Important applications in the fields of graphics, robotics, etc., will be explored as well.<br/><br/>Advancements in imaging technology enable scientists and medical professionals today to view the inner functioning of organs at the cell level and beyond.  For example, in the white matter in the cortex, the coefficients of the 3x3 diffusion matrix of water molecules can be measured.  In the absence of a disease or trauma, these matrices show pronounced anisotropy along well organized neural structures, while perturbations due to a disease lead to a decrease in anisotropy in each such location. This is one aspect of the structural change due to a disease that is visible in the diffusion tensor imaging scans. There are others.  So far there is no statistical methodology that can precisely associate such a decrease in anisotropy with the particular disease that causes it. The present project will represent the main neural structures in the white matter in terms of elements of a Riemannian manifold and their geodesics. As one specific task, the project will choose appropriate metric tensors on the space of alignments of positive definite matrices along neural structures. The broad goal is to provide a nonparametric statistical methodology based on Fre'chet means for discrimination and diagnostics, extending much further and in novel directions the research that was carried out under earlier NSF supports. In a completely different direction, one theoretical objective of the project is to provide broad conditions for uniqueness of the Fre'chet mean under a geodesic distance. Such conditions are required for statistical applications but are unavailable in adequate generality for Riemannian manifolds with positive curvature.  This matter of uniqueness also has surprising implications, for graphics and robotics."
"1406163","The Third  Workshop  on Biostatistics and Bioinformatics, May 9-11, 2014","DMS","STATISTICS","05/15/2014","03/19/2014","Yichuan Zhao","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Gabor Szekely","04/30/2016","$10,000.00","Xin Qi","yichuan@gsu.edu","58 EDGEWOOD AVE NE","ATLANTA","GA","303032921","4044133570","MPS","1269","7556","$0.00","The Third Workshop on Biostatistics and Bioinformatics will be held on the campus of Georgia State University, Atlanta on May 9-11, 2014. The objective of the conference is to reflect new developments in the frontiers of Biostatistics and Bioinformatics.  The conference will cover a broad range of current research topics, including high dimensional data analysis, survival analysis, longitudinal data analysis, and next generation and sequence data analysis.  The invited speakers will include leading experts and outstanding young researchers. The workshop will provide opportunities for exchange new ideas and research collaborations. Graduate students and postdocs will gain valuable research experience by attending the workshop and making poster presentations. <br/><br/>This award supports the participation of approximately 16 people.  The conference organizers are making special efforts to provide travel support for a diverse group of researchers to attend the workshop.  Underrepresented groups are encouraged to attend and apply for travel support through this award. For detailed information, go to website: http://www2.gsu.edu/~matyiz/2014 workshop/."
"1407557","Statistical learning via multivariate density estimation","DMS","STATISTICS","08/01/2014","06/12/2017","Wing Hung Wong","CA","Stanford University","Continuing Grant","Gabor Szekely","07/31/2018","$599,539.00","","whwong@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","The overall goal of the project is to develop methodologies of density estimation in multiple dimensions, and to develop new tools based on this methodology for selected problems in data compression, image analysis and graphical model inference. Density estimation is a fundamental problem in statistics but traditional approaches such as kernel density estimation are not well suited to handle the large multivariate data sets in current applications. The research in this project is centered on the methodology and application of multivariate density estimation. By creating effective methods for this problem, this project will also benefit many other research problems in applied statistics and machine learning where density estimation can be used as a building block for the solution, for example, image segmentation, data compression and network modeling.  <br/>      <br/>Specifically, the project will address the question of how to infer a partition of the sample space that will reveal the structure of the underlying data distribution. The partition will be learned from the observed data based on a Bayesian nonparametric approach which imposes minimal assumptions on the distribution to be estimated. Efficient and scalable algorithms for such inferences will be designed for the analysis of large data sets in multiple dimensions. The theoretical properties of the estimates, such as asymptotic consistency and convergence rates, will also be investigated."
"1407397","Monte Carlo and Quasi-Monte Carlo Methods for Statistics","DMS","STATISTICS","09/01/2014","08/24/2016","Art Owen","CA","Stanford University","Continuing Grant","Gabor Szekely","08/31/2018","$224,991.00","","owen@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","Computer simulations are used in virtually every branch of science and engineering, because some problems are simply beyond closed form mathematical solution, and because computers have become extremely fast. Against that trend, there is the constant feeling, and mathematical support, for the idea that strategically sampled values can give even better results than random ones do. Computer generated imaging is one of the largest users of simulations. Simulations of the behavior of light underly images from the economically significant motion picture and computer game industries, to problems of architectural rendering and scientific visualization. Many of the same ideas that go into choosing input points for simulations are also used to computerize difficult statistical decisions and to design industrial<br/>products.<br/><br/>Quasi-Monte Carlo sampling is a method of producing inputs that are more evenly distributed than random points are. They are said to have low discrepancy. Most of the existing low discrepancy solutions are about placing points inside the unit cube. Some applications in computer rendering require samples from the triangle, simplex or other such spaces.  Plain Monte Carlo sampling of those spaces is straightforward but low discrepancy sampling is quite different. The transformations that work for Monte Carlo may destroy the low discrepancy properties of the resulting points in the triangle. This project will develop  low discrepancy sampling methods for the triangle and for tensor products of triangles. Factor analysis is about hundred years old, yet it remains problematic to even choose the number of factors to use in it. This project will develop cross-validatory methods to select the number of factors."
"1357690","MCQMC 2014 Travel Support","DMS","STATISTICS","01/01/2014","11/08/2013","Art Owen","CA","Stanford University","Standard Grant","Gabor Szekely","12/31/2014","$15,000.00","","owen@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","7556","$0.00","The project funds the travel expenses of US-based researchers to the Eleventh International Conference on Monte Carlo and Quasi-Monte Carlo methods in Scientific Computing, to be held in Leuven, Belgium, April 6--11 2014.  The meeting will consider advanced methods for psuedo-random number generation, construction of low discrepancy point sets, complexity of algorithms, particle methods, rare event simulation, Markov chain Monte Carlo and applications of Monte Carlo methods. There will be tutorial sessions on multilevel Monte Carlo and on global sensitivity indices.<br/><br/>The Monte Carlo (MC) method is a computer based simulation using random number generators. The name was given by atomic researchers in<br/>the 1940s who likened their simulation methods to keeping score in a casino in order to learn some odds. Monte Carlo methods are used in every branch of science and engineering because they allow brute force computer power to be used on problems that are too complicated to solve mathematically. Quasi-Monte Carlo (QMC) methods replace simulated random numbers by strategically chosen ones. By leaving<br/>less to chance, large improvements in accuracy are possible. MC and QMC methods are widely applied in computer graphics to make animated movies and other images, in computational finance to control risks, in statistical inference to separate real findings from chance fluctuations, and in many other areas. Much  of the top QMC work is done in Europe, Asia, Canada and Australia. This conference will bring together leading researchers  from around the world to share results. The project will support travel expenses of US based researchers to participate in this exchange of knowledge.  One of the researchers is a US-based expert giving a major talk. The majority of the funds are for young US-based researchers in the mathematical sciences, including PhD students, postdocs, and junior professors.<br/>Further updates will be made available via the conference website: http://mcqmc2014.cs.kuleuven.be/"
"1407548","Flexible Statistical Modeling","DMS","STATISTICS","08/01/2014","06/09/2017","Trevor Hastie","CA","Stanford University","Continuing Grant","Gabor Szekely","10/31/2019","$499,996.00","","hastie@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","The abundance of data in science, medicine and commerce, and the current state of computing technologies gives us opportunities in statistical modeling never seen before. We are able to build powerful predictive models for the risk of breast cancer, heart disease or stroke, for example, using genomic markers. We can predict the risk of credit-card default or fraudulent insurance claims. Predictive models are able to recommend movies or music to a customer, based on their past behavior and preferences and that of customers like them. Using data on locations of sightings of multiple animal or plant species, we can build distribution maps over a geographical domain. With large amounts of data, it becomes necessary that these models are built in an automatic way; the goal of this project is to ensure that the resulting products remain interpretable.<br/><br/>Generalized additive models are both interpretable and somewhat powerful, but were originally intended for a relatively small set of predictor variables. This project will use methods in convex optimization to automatically build such models using potentially thousands of variables. The method will automatically omit irrelevant variables, as well as select the amount of nonlinearity needed for all those retained. Convex methods will also be used to incorporate side information in matrix completion problems, as well as a variety of multivariate methods where we have traditionally worked with low-rank representations. Ecologists often struggle with combining data from multiple species and different sampling schemes. This project will provide a unified framework using inhomogeneous Poisson process models for combining these data, and producing high-quality distribution."
"1407813","Estimation and testing in low rank multivariate models","DMS","STATISTICS","07/15/2014","08/10/2017","Iain Johnstone","CA","Stanford University","Continuing Grant","Gabor Szekely","06/30/2020","$626,835.00","David Donoho","imj@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","In the big data era, observations are collected on ever larger numbers of variables and cases.  Even after preliminary reductions of the data, subsets of interest may have high dimensionality and appreciable sample size.  The interesting structure in such data is often low dimensional, indeed such models occur in many scientific domains from econometrics to genomics and signal processing and well beyond.  This project will investigate the estimation and testing of a particular class of low dimensional structures, namely low rank perturbations of scaled identity or diagonal matrices.  It will consider high dimensional versions of multivariate statistical methods that have found wide use for traditional data: principal components, multiple response regression, canonical correlations etc., as well as newer applications such as matrix denoising.<br/><br/>The project will study the proportional limit setting in which the number of variables and the sample size are of the same order of magnitude.  It will explore the phase transition phenomenon for a wide class of multivariate methods, using in part the systematic framework developed by A. T. James.  Contiguity properties below the phase transition will be investigated as will Gaussian behavior above the critical point.  A separate low noise approximation will be used to derive long sought power approximations for largest root tests.  In estimation, the project will study optimal shrinkage procedures for the empirical eigenvalues that correspond to the low rank structure, making explicit how the results depend strongly on the particular loss function chosen.  Both scalar non-linearities as well as thresholding techniques will be considered.  The project will build upon preliminary work for covariance estimation and matrix denoising, and also develop results for other multivariate settings such as low rank factor models, canonical correlations and discriminant analysis."
"1352656","CAREER: Scalable methods for discovering multivariate dependencies in high dimensional data.","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2014","09/04/2018","Balakanapathy Rajaratnam","CA","Stanford University","Continuing Grant","Gabor Szekely","03/31/2019","$400,004.00","","brajaratnam01@gmail.com","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269, 8048","1045","$0.00","This proposal aims to develop principled methods for discovering multivariate dependencies which cater to ultra high dimensional settings. A common theme that unites the proposed methods is scalability and identification of their limitations. A popular approach to identifying sparse inverse covariance matrices is through penalized likelihood methods. We propose a novel approach for solving the penalized Gaussian log-likelihood that is faster than its competitors by many orders of magnitude. The second research component in the proposal investigates the statistical properties of thresholded matrices in finite samples, with a view to obtaining a positive definite covariance estimation method which is highly scalable. The third research aspect of the project investigates quantifying the variability and uncertainty of estimated graphical network models. A methodology that takes advantage of a convex pseudo-likelihood formulation of the graphical model selection problem is introduced. This allows for the development of a highly scalable uncertainty quantification method with theoretical safeguards. The fourth research aspect of the project examines the use of the methodology proposed in the previous three sub-components to an application in the area of climate change, where high dimensional covariance estimation is required. The proposal also has a significant teaching and outreach component which aims to introduce statistics to aspiring young scientists at various stages of their undergraduate and graduate studies.<br/><br/>The availability of high-throughput data from  various applications, including genomics, environmental sciences and others,  has created an urgent need for methodology and tools for analyzing high dimensional data. Extracting and making sense of the many complex relationships and multivariate dependencies in the data and developing principled inferential procedures is one of the major challenges facing statisticians and data scientists. The theoretical and methodological work proposed in this project is motivated by applications and interdisciplinary collaborations in fields as diverse as the earth and environmental sciences, genomics and cancer research, and the social sciences. In genomics for instance, one is often interested to know how various genes are associated, and how these associations differ between an experimental (diseased) and control group. Gene regulatory networks also serve as important tools to study the evolutions of diseases. In the context of the climate change debate, modeling temperature at different points on the globe requires parsimonious modeling of the way in which these variables are related. Modeling correlations also arises naturally in material sciences and engineering where one is interested in seeing how different atomic particles interact when new materials are produced.  Hence the proposed project for estimating correlations in very high dimensional settings will  have widespread applications, since understanding associations/relationships between many variables is an endeavor that is common to many scientific disciplines. The proposed work, though firmly rooted in the statistical sciences, is very much interdisciplinary, and involves collaborations and partnerships between statisticians/data scientists and biomedical scientists, engineers and earth scientists."
"1407828","Statistical Methodology and Applications to Engineering and Economics","DMS","STATISTICS","08/15/2014","06/09/2017","Tze Lai","CA","Stanford University","Continuing Grant","Gabor Szekely","07/31/2019","$399,663.00","","lait@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","The past three years witnessed the beginning of a new era in financial markets and in the US health care system, following the health care and financial reform legislation in 2010. A long-term objective of the research is to develop innovative statistical methodologies and to combine them with advances in high-performance computing and communication networks for addressing the challenges in quantitative finance and health care in this new era. The research will lead to advances and innovations in statistical methods in biomedicine, economics and engineering, paving the way for timely applications to clinical and translational medical research, health care, homeland security, environmental change, and risk management. A broader impact of this research is the training of the next generation of scientists in academia, industry, and government, by involving graduate students in all phases of the research, and by developing new course material built around the research and its applications.<br/><br/>The research projects can be broadly divided into four areas. The first is multi-arm bandits with covariates, also called ""contextual bandits"" in machine learning, and their applications to personalized strategies in medicine and electronic business, in particular, to genomic-guided personalized cancer treatments and biomarker- guided treatments for depression in neuroscience. The second area is fault detection and surveillance in network models for manufacturing systems, for systemic risk in financial markets, for homeland security, and for environment or global change. The third area is the development of efficient adaptive particle filters in nonlinear state-space models that have far-ranging applications in engineering and economics, with robotics and stochastic adaptive control as the main focus in this research. The fourth area includes dynamic empirical Bayes modeling of joint default risk for multiple firms in credit markets, of loan loss risk in retail banking and of insurance claims, macroeconomic time series modeling and forecasting, and data analytics for health care cost and preventive management."
"1405685","Nonparametric Inference of Nonstationary Time Series","DMS","STATISTICS","08/15/2014","08/09/2014","Ting Zhang","IA","University of Iowa","Continuing Grant","Gabor Szekely","10/31/2014","$38,949.00","","tingzhang@uga.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1269","9150","$0.00","This research project aims to advance the development of statistical methodology and theory for analyzing nonstationary time series. A nonstationary time series is a sequence of data points collected at successive time points with certain aspects changing over time or being nonstationary. Nonstationary time series appear very frequently in various scientific fields including economics, engineering, environmental science, finance, and medical science among others. Ignoring nonstationarity and dependence, two important features of nonstationary time series, can lead to erroneous conclusions. Therefore, the research project is expected to promote scientific research in not only statistics but also other disciplines that involve the analysis of nonstationary time series.<br/> <br/>In order to capture the temporal dynamics resulting from the nonstationarity, the parameters are allowed to change over time and modeled as deterministic but unknown functions of time, which are intrinsically infinite dimensional. To avoid potential misspecifications of parametric models, nonparametric methods are used to estimate and make inference about the underlying parameter functions. The research to be performed involves parameter estimation, hypothesis testing, and variable selection for regression models with time-varying parameters. By allowing a general class of nonstationary and dependent processes, the methods to be developed and the asymptotic results to be established can be widely applicable, and we are able to understand and quantify the effect of nonstationarity and dependence on the asymptotic behavior of parameter estimators and test statistics of interest."
"1407600","New Horizons in Statistical Decision Theory","DMS","STATISTICS","07/15/2014","06/13/2019","Michael Nussbaum","NY","Cornell University","Continuing Grant","Gabor Szekely","06/30/2020","$370,000.00","","nussbaum@math.cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","","$0.00","The classical metric theory of statistical models (experiments) has been extended towards an asymptotic equivalence paradigm, allowing to classify and relate problems which are essentially infinite dimensional and ill-posed. Asymptotic equivalence theory is emerging as a recognizable research area in statistics. The theoretical possibility to carry over optimal procedures from one model to another allows a better conceptual understanding of asymptotic inference.  This area is still under vigorous development, and new problems arise in the context of asymptotic inference based on high-dimensional data. Modern statistical concepts like these are also being integrated into the emerging field of quantum statistics, which is developing on the background of technological breakthroughs in quantum engineering. The analysis of quantum statistical models points towards an underlying non-commutative statistical decision theory with connections to operator algebra, quantum information and quantum probability.  By focusing on this growing research area at the interface of Statistics and Mathematical Physics, the project will have the side effect of fostering interaction between the different scientific communities. <br/><br/>One of the unexplored topics in asymptotic equivalence theory is the impact of additional observations in nonparametric models. Le Cam showed that in the regular parametric case, additional observations are negligible in an asymptotic information sense if their number is one order of magnitude below the original sample size, but for larger parameter spaces this critical threshold seems to be lower. Reasoning via additional observations has been applied in the past to prove equivalence of spectral density to white noise models, and may prove useful again for problems related to estimation of high-dimensional matrices.  In a related topic, it is of interest to further refine sharp nonparametric risk bounds like the Pinsker bound, which have motivated the development of equivalence theory.  Here the research program aims at confirming a conjecture about a bound of this type for sharp adaptive nonparametric testing, a possible complement to results in adaptive estimation.  In the problem of symmetric quantum hypothesis testing, or discrimination between two quantum states, the program focuses on applications of the quantum Chernoff bound pertaining to the exponential rate of decay of the error probability. In that connection, several new problems appear, such as attainability of the bound by realizable receivers in quantum optics, and variants for discriminating states given by quantum Markov chains. An area of particular interest is local asymptotic normality for quantum statistical models.  Some elements of such a theory have already been put forward in the literature; these notions will be further developed in the spirit of Le Cam's classical theory, focusing at first on the quantum analog of Gaussian stationary sequences."
"1443252","Collaborative Research:  Multidimensional Curve Estimation for Diffusion MRI","DMS","STATISTICS","07/01/2014","05/03/2014","Owen Carmichael","LA","Pennington Biomedical Research Ctr","Standard Grant","Gabor Szekely","07/31/2014","$24,026.00","","owen.carmichael@pbrc.edu","6400 PERKINS RD","BATON ROUGE","LA","708084124","2257632500","MPS","1269","","$0.00","Integral curves are natural models for a variety of biological phenomena, from neuron fibers in brain imaging data to jet streams in atmospheric data. Traditionally they have been modeled as solutions to differential equations defined on fields of direction vectors that are observed with noise in a 3D domain. But advances in imaging technology now provide much more complex directional information--functions defined on the 3D sphere-at each location in the domain. Integral curves traced from this enhanced directional data have the potential to dramatically increase our understanding of biological phenomena such as brain connectivity, but the statistical properties of integral curve estimators for this cutting-edge data are not well understood. Therefore in this project the investigators will provide a solid theoretical foundation for integral curve estimation in 3D fields of complex directional data and apply it to large corpuses of real data sets from ongoing scientific studies. The primary plan will be to model directional data locally using high-order supersymmetric tensors, and pose integral curve estimation in terms of ODEs defined on the field of their pseudo-eigenvectors. The investigators will show that the proposed integral curve estimators enjoy optimal convergence rates in a minimax sense, and prove that balloon estimators of the pseudo-eigenvector fields will lead to improved convergence. Then integral curve estimators will be linked to accompanying random processes to allow construction of uniform confidence bands around point estimates for curves; and adaptive estimation of these confidence bands will be explored to make them practically useful. The investigators will then study whether estimation may be improved further by selecting arbitrary 3D measurement locations, possibly using enhanced imaging techniques. Finally, a test for branching of integral curves will be constructed, for example at locations where axon fibers diverge or cross.<br/><br/><br/>The proposed work has the potential to dramatically increase the usefulness of diffusion magnetic resonance imaging (MRI) data, a technology with tremendous potential to probe the ""wiring diagram"" of the brain-- its connectivity-- in living people. Currently, brain connectivity measurements are widely regarded as brittle, complicated, and difficult to validate. For each individual receiving a diffusion MRI scan, the investigators will estimate curves describing the trajectories of axon fibers, the electrical ""wires"" of the brain. These fibers connect brain regions into distributed networks that give rise to thought; the evolution of this brain wiring in response to normal development, gene expression, aging, disease, drugs, and environmental factors is of primary interest to a broad swath of neuroscience. Simply providing scientific end-users with a sense of whether or not they should believe the estimated fiber trajectories provided to them by computer programs will greatly enhance their ability to make confident decisions about relations between such trajectories and other scientific data. In addition, the proposed methodology is also relevant in meteorology. There, isolines, fronts, jetstreams, and pressure troughs in weather data can be modeled by similar curve trajectories that can be used to enhance existing weather maps. Finally, this proposal has an exciting educational impact. The investigators, a statistician and a computer scientist with neuroscience training, envision building an interdisciplinary team of promising young researchers in statistics and neuroimaging who gain exposure to both the mathematical and neuroscience aspects of curve estimation through joined group meetings, graduate courses, and web resources related to theory and applications. This unique cross-pollination will prepare the trainees to contribute to the broadly interdisciplinary research teams that are ascendant in the sciences."
"1407771","Spectral and principal components analysis in sparse, high-dimensional data","DMS","STATISTICS","08/01/2014","05/04/2016","Jing Lei","PA","Carnegie-Mellon University","Continuing Grant","Gabor Szekely","07/31/2017","$120,000.00","","jinglei@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","With the rapid advances in data collection technology, relational data is becoming increasingly important in modern sciences. Broadly speaking, relational data records interactions and dependences among actors in a population of interest. A typical example is network data, such as social networks, world-wide-web, and terrorist networks, where each actor is represented by a node in the network and an interaction between two actors is represented by the presence of an edge between the corresponding nodes.  Another common form of relational data is covariance and correlation data, which summarizes pairwise dependence among actors, such as gene-gene co-expression, functional correlation in brain imaging, and spatial correlation in atmospheric and oceanographic measurements.  Such data sets often contain important structures that can provide key insights to the population of interest.  For example, the population of actors in a network data may be divided into several communities with different connectivity patterns; the population in a correlation data may contain a few important actors that account for most of the observed variability.  However, the high dimensionality and complex dependence structure in these data sets make it a challenging statistical problem to recover these hidden structures.<br/><br/>This research project aims at advancing the theory and methodology in statistical inference for network and covariance data using spectral and principal components analysis. These two topics are brought together and studied using a novel set of tools recently developed in random matrix theory, spectral analysis, and empirical process theory.  This project will investigate three topics. The first topic is a better understanding and refinement of community recovery in sparse network models using spectral clustering, one of the most popular methods in the literature and in practice. The second topic is network community detection in a statistical minimax framework, including information-theoretic lower bounds quantified by a comprehensive collection of model parameters, and optimal estimation procedures that achieve the lower bounds. The third topic is goodness-of-fit tests for general sparse principal components analysis models, where adaptive procedures will be developed using a detection boundary framework; and the high dimensionality challenge will be tackled by considering regular alternatives such as Sobolev ellipsoids."
"1407622","Advances in Scalable Monte Carlo Algorithms for Bayesian Statistics","DMS","STATISTICS","07/15/2014","07/10/2014","Scott Schmidler","NC","Duke University","Standard Grant","Gabor Szekely","12/31/2017","$297,595.00","","schmidler@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","","$0.00","Statistical analysis of large datasets and complex models is in tremendous demand, with applications throughout the natural and social sciences, engineering, business, and biomedicine.  A major limitation in meeting this demand lies in current computational algorithms, which fail to scale adequately to large problems and data sets.  At the same time, advances in computer processor speeds have slowed dramatically, prompting a shift in the computer hardware industry towards parallelization.  As a result, the demands on Bayesian computational algorithms are increasing rapidly as the platforms underlying them are changing.  This work explores several promising new directions for producing highly efficient and scalable algorithms, and corresponding software tools, suitable for general-purpose Bayesian calculations.<br/><br/>The work involves new directions in Bayesian computation, including (1) true parallelization of general-purpose Markov chain Monte Carlo samplers, a class of algorithms traditionally viewed as ""inherently serial,"" (2) algorithmic and theoretical advances in ""static data"" particle-based sequential Monte Carlo samplers which are highly parallelizable but currently fail on many complex high-dimensional posterior distributions, and (3) new tools for empirical monitoring of convergence of Monte Carlo samplers specifically designed for complex distributions and high-dimensional problem domains.  All facets of the work are directly motivated by applications of Bayesian statistics in chemical kinetics, structural bioinformatics, and systems biology.  This work also has immediate applicability to problems in broader areas of statistical physics, computer science, and molecular simulation."
"1407775","Bayesian Analysis and Interfaces","DMS","STATISTICS","07/15/2014","06/09/2017","James Berger","NC","Duke University","Continuing Grant","Gabor Szekely","06/30/2019","$599,996.00","","berger@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","","$0.00","Many of today's most challenging problems in Big Data involve dealing with the problem of multiple testing, including microarray and other bioinformatic analyses, syndromic surveillance, high-throughput screening, and many others. The challenge when simultaneously conducting thousands or millions of tests is to develop testing methodology that can detect true signals but prevent false discoveries. Crucial contexts for this research include subgroup analysis (searching for a treatment effect in subgroups of the entire population) and multiple endpoint analysis (simultaneously looking for different treatment effects), in interfaces with the pharmaceutical industry and with partial focus on personalized medicine. Development of computer models is crucial in understanding complex processes, as is understanding the interfaces of the computer models with data and uncertainty, often named Uncertainty Quantification. The immediate science applications of the research on Uncertainty Quantification will be to prediction of geophysical hazard probabilities and to models of wind fields. Two of the most significant reasons for the recent concern over reproducibility of science are the failure to control for multiplicities and the common misinterpretation of p-values. In addition to the earlier mentioned multiplicity control, the project will investigate the possibility of converting p-values to more interpretable quantities, such as the odds of the null to the alternative hypothesis.<br/> <br/>The Bayesian approach to multiple testing has the attraction that it is optimally powered for detection, even in the face of highly dependent data or tests statistics, while exerting strong control to prevent false discoveries. The barriers to its implementation are in developing the appropriate prior probability structures and carrying out the computation. While numerous aspects of Uncertainty Quantification will be investigated, the project will particularly focus on the crucially needed development of emulators (approximations) to complex computer models which output massive space-time data fields. Finding situations in which optimal Bayesian and optimal frequentist procedures agree has major benefits, both foundational and practical. Such new agreements typically arise through development of new conditional frequentist procedures. This will be done in the context of two methodologies, study of the odds of correct to false discovery and multiple endpoint testing. This will also be generalized to more general model uncertainty problems, using robust Bayesian analysis."
"1461677","Collaborative Research: Information Matrix Analysis for Nonparametric Multivariate Problems","DMS","STATISTICS","08/15/2014","05/05/2016","Weixin Yao","CA","University of California-Riverside","Continuing Grant","Gabor Szekely","07/31/2018","$120,000.00","","weixin.yao@ucr.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","MPS","1269","9150","$0.00","This project develops a new set of tools, called Information Matrix Analysis (IMA), to explore the structure of multivariate data. With modern data gathering devices and vast data storage space, researchers can easily collect high-dimensional data, such as biotech data, financial data, satellite imagery, and hyperspectral imagery. Analysis of such high-dimensional data poses great challenges for statisticians due to the so-called ""curse of dimensionality"". The IMA methodology tackles this problem by finding a smaller number of linear combinations of the original variables that will carry most of the information of the original multivariate data. The IMA is based on the eigenanalysis of the Information Matrix as defined for different problems. The eigenvectors of the proposed information matrices provide the linear combinations of the variables that best summarize the useful features of the data due to their high information content. By combining the new method with the random projection method, one can also apply IMA to ultra high dimensional data. Alternatively, one can do variable selection with IMA. The new data analysis tool under development in this project is timely due to the data revolution, as well as broadly applicable. All of the IMA methodology springs from a common foundation, and so is easily transported to tackle new problems. This project will enhance significantly the availability of statistical tools and software for statistical modeling and exploration for multivariate data. The new method will benefit a broad range of scientists and researchers who want to analyze high-dimensional data in various fields, including medical studies, prevention studies, public health, and the social sciences.<br/><br/>This project will accelerate geometric understanding of Fisherian information matrices. The core of this project consists of the application of IMA to three very different problem areas: building models, assessing models, and comparing populations. Suppose one wants to build a model to analyze the relationship between a response variable and multivariate covariates. The IMA of a defined covariate information matrix can be applied to find a smaller number of linear projections of the original covariates to simplify model building. The new projected variables have the nice explanation of carrying most information, as measured by the defined covariate information, about the relationship between the covariates and the response. The IMA can be also generalized to find linear combinations of the data that can provide the best discrimination between two densities. In the applications, one density will be the true unknown density, to be estimated nonparametrically, and the other density will be some model for the data, which could be parametric or semiparametric. Alternatively, the two densities could represent two distinct populations one wishes to compare. When the two populations are multivariate normal with the same covariance matrix, the IMA provides the same linear projection as the Fisher linear discriminant direction. However, the IMA can move beyond linear discriminant analysis to multiple linear discriminants. The IMA will be further applied to find linear projections of the data that are useful for assessing the fit of a proposed model, whether parametric, semiparametric, or nonparametric. This project will investigate the applications of IMA to popular graphical models and independent component analysis models. It is expected that IMA might have many more application areas. For high dimensional data, the proposal will develop the use of the random projection method to first reduce the dimension of the original variables and restrict the space of possible linear combinations. Then the IMA can be applied to a much lower dimensional data set. Alternatively, one can do variable selection with IMA by imposing the sparsity of the informative projections."
"1420056","2014 IISA Conference On Research Innovations in Statistics for Health, Education, Technology, and Society, July 11-13, 2014","DMS","STATISTICS","04/01/2014","04/15/2014","Subir Ghosh","CA","University of California-Riverside","Standard Grant","Gabor Szekely","03/31/2015","$10,250.00","","subir.ghosh@ucr.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","MPS","1269","7556","$0.00","The 2014 International Indian Statistical Association (IISA) Conference on Research Innovations in Statistics for Health, Education, Technology, and Society will be held at the Riverside Convention Center in Riverside, California during July 11-13.  The goal of the conference is to bring together a diverse group of researchers, educators, professionals, postdocs, and graduate students to share information on recent advancements in the fields of statistics, biostatistics, probability, and their application areas, including health, education, environment, technology, and society. <br/><br/>This award partially supports the participation of students and young researchers in the conference, which brings together senior researchers, professionals, educators, and students from academia, industries, government, and research institutes to discuss the major issues and challenges in these fields and to share the latest developments.  The conference includes plenary, special invited, young-researcher, and contributed presentations, as well as student presentations selected via competition.<br/><br/>Conference web site:  http://www.intindstat.org/conference2014"
"1352259","CAREER: Theory and Methods for Simultaneous Variable Selection and Rank Reduction","DMS","STATISTICS, Division Co-Funding: CAREER","06/01/2014","05/22/2018","Yiyuan She","FL","Florida State University","Continuing Grant","Gabor Szekely","05/31/2019","$400,000.00","","yshe@stat.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269, 8048","1045","$0.00","The data explosion in all fields of science creates an urgent need for methodologies for analyzing high dimensional multivariate data. The project deepens and broadens existing sparsity and low rank statistical theories and methods by making the following major scientific achievements: (a) an innovative selectable reduced rank methodology through simultaneous variable selection and projection, with guaranteed lower error rate than existing variable selection and rank reduction rates in theory, which paves the way to new frontiers in high dimensional statistics and information theory; (b) fast but simple-to-implement algorithms that can deal with all popular penalty functions (possibly nonconvex) in computation with guaranteed global convergence and local optimality, to ensure the practicality of the proposed approaches in big data applications; (c) a generic extension to non-Gaussian models capable of taking into account the correlation between multivariate responses, with a universal algorithm design based on manifold optimization; (d) a unified robustification scheme that can both identify and accommodate gross outliers occurring frequently in real data, to overcome the non-robustness of many conventional multivariate tools; (e) general-purpose model selection methods serving variable selection and/or rank reduction and achieving the finite-sample optimal prediction error rate with theoretical guarantee. <br/>  <br/>The need to recover low-dimensional signals from high dimensional multivariate noisy data permeates all fields of science and engineering. Hence a project of this nature, designed to develop transformative theory and methods for simultaneous variable selection and rank reduction, finds applications in a wide range of disciplines and areas such as machine learning, signal processing, and biostatistics, among others. By cross-fertilizing ideas from statistics, mathematics, engineering, and computer science, the integrated research and education help students develop critical thinking through cross-disciplinary training, and assist students in becoming life-long learners. The investigator uses the rich topics in this project to inspire the learning and discovery interest of the public and students of all ages. The educational plan consists of course development, student mentoring, outreach, and recruiting underrepresented students."
"1406266","Collaborative Research: Interface of Probability and Statistics for High-dimensional Inference","DMS","STATISTICS","08/01/2014","06/12/2017","Jianqing Fan","NJ","Princeton University","Continuing Grant","Gabor Szekely","07/31/2018","$400,000.00","","jqfan@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","","$0.00","With rapid advances in information and technology, big data are now routinely collected in many frontiers of scientific research and technological developments. Various advanced techniques are needed to address the challenges such as computation, noise accumulation, and spurious correlations prominently featured in big data. New theoretical challenges arise in order to address probabilistic problems in high-dimensional statistics. This collaborative research project intends to combine the strength and expertise on probability and statistics of both investigators to address a number of emerging and challenging issues in high-dimensional data analysis. <br/><br/>The project will focus on primarily two types of problems: (i) high-dimensional statistics on spheres and (ii) behaviors of eigenvalues of random matrices and zeroes of polynomials. The two topics are innately related since high-dimensional data are often expressed in terms of matrices. The project would create new theories and methods for high-dimensional statistics, interact and apply them to several different disciplines including mathematics, statistical physics, information theory, and computer science. The project also involves training of top notch mathematical scientists and engineers and to disseminate the research results and products broadly via the online media, conferences and seminar presentations."
"1407439","Collaborative research: Statistical and computational efficiency for massive datasets via approximation-regularization","DMS","STATISTICS","09/01/2014","08/20/2014","Daniel McDonald","IN","Indiana University","Standard Grant","Gabor Szekely","08/31/2018","$89,911.00","","dajmcdon@gmail.com","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","MPS","1269","","$0.00","This project integrates approximation methodology from computer science with modern statistical theory to improve analysis of large data sets. Modern statistical analysis requires methods that are computationally feasible on large datasets while at the same time preserving statistical efficiency. Frequently, these two concerns are seen as contradictory: approximation methods that enable computation are assumed to degrade statistical performance relative to exact methods. The statistical perspective is that the exact solution is undesirable, and a regularized solution is preferred. Regularization can be thought of as formalizing a trade-off between fidelity to the data and adherence to prior knowledge about the data-generating process such as smoothness or sparsity. The resulting estimator tends to be more useful, interpretable, and suitable as an input to other methods. Conversely, in computer science applications, where much of the current work on approximation methods resides, the inputs are generally considered to be observed exactly. The prevailing philosophy is that while the exact problem is, regrettably, unsolvable, any approximate solution should be as close as possible to the exact one. We make a crucial realization: that the approximation methods themselves naturally lead to regularization, suggesting the intriguing possibility that some computational approximations can simultaneously enable the analysis of massive data while enhancing statistical performance. <br/><br/>Our research develops new methods that leverage this phenomenon, which we have dubbed 'approximation-regularization.' The first method uses a matrix pre-conditioner to stabilize the least-squares criterion. If properly calibrated, this approach provides computational and storage advantages over regularized least squares while providing a statistically superior solution. A second innovation addresses principal components analysis (PCA) for regression on large data sets where PCA is both computationally infeasible and known to be statistically inconsistent. By employing randomized approximations, we can address both of these issues, while improving predictions at the same time. Lastly, we introduce new methods for unsupervised dimension reduction, whereby approximation algorithms that leverage sparsity, and statistical methods that induce it, enable the use of spectral techniques on very large matrices. In each of these cases, approximation-regularization yields both computational and statistical gains relative to existing methodologies. This research recognizes that approximation is regularization and can thereby increase statistical accuracy while enabling computation. It will result in new statistical methods for large datasets, which are computationally and statistically preferable to existing approaches, while also bringing attention to this important area in statistics. Additionally, these methods will permit scientists in other fields, such as astronomy, genetics, text and image processing, climate science, and forecasting, to make ready use of available data."
"1419428","Midwest Women in Math Symposium (Midwest WIMS)","DMS","PROBABILITY, ALGEBRA,NUMBER THEORY,AND COM, GEOMETRIC ANALYSIS, APPLIED MATHEMATICS, TOPOLOGY, FOUNDATIONS, STATISTICS, COMPUTATIONAL MATHEMATICS, ANALYSIS PROGRAM, MATHEMATICAL BIOLOGY, Combinatorics, CDS&E-MSS","02/01/2014","01/31/2014","Amy Buchmann","IN","University of Notre Dame","Standard Grant","Tomek Bartoszynski","01/31/2016","$23,329.00","Julia Knight, Kathleen Ansaldi, Sonja Szekelyhidi, Fang Liu","abuchmann@sandiego.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","1263, 1264, 1265, 1266, 1267, 1268, 1269, 1271, 1281, 7334, 7970, 8069","7556, 9263","$0.00","The Midwest Women in Mathematics Symposium (Midwest WIMS) will be held at Notre Dame on April 5, 2014.  The goals of this symposium are to highlight women's contributions to the mathematical community, strengthen the network of female mathematicians in the Midwest, encourage new collaborations, and facilitate mentoring among graduate students, postdocs, and professors to help women stay active in research. The symposium will include several mathematical talks, including a keynote address given by Lenore Blum of Carnegie Mellon University. There will be parallel sessions in algebra, dynamical systems, geometry and topology, logic, mathematical biology, partial differential equations, and statistics. In addition, there will be a problem session to encourage new collaborations; problems that cut across disciplines are particularly encouraged. The symposium encourages participation of women in various stages of their academic careers, ranging from graduate students to full faculty. <br/><br/>It is not uncommon for women with research talent to drop out of the research community. The talks and the problem session will lead women to new and exciting projects. An extended lunch and other breaks will give participants at various stages in their careers the opportunity discuss mathematical problems, or simply exchange stories. The PI expects that some participants will, in discussions that are not strictly mathematical, learn strategies for staying active in research.<br/><br/>The conference website is at http://www3.nd.edu/~wims/"
"1418791","Statistical Analysis of Neural Data (SAND), May 29-31, 2014","DMS","STATISTICS, MATHEMATICAL BIOLOGY, Organization","09/01/2014","04/24/2014","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","08/31/2015","$20,000.00","","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 7334, 7712","7556","$0.00","The workshop Statistical Analysis of Neural Data (SAND7) will take place May 27-29, 2015, at the Center for the Neural Basis of Cognition, run jointly by the Carnegie Mellon University and the University of Pittsburgh, in Pittsburgh PA. Major advances in neural recording and imaging technologies have put into the hands of investigators wonderful tools for conducting previously unimagined experiments, yielding rich data sources that can shed new light on basic neuroscience and its clinical implications. The data sets are, however, often large and complex, so that novel methods of analysis are needed if the wealth of new information is to be turned into useful knowledge. (SAND7) will bring together euro-physiologists, statisticians, physicists, and computer scientists who are interested in quantitative analysis of neuronal data. The scientific lectures will be given by 6 keynote speakers, and 12 junior investigators. Each keynote lecture will be followed by an invited discussion of the topic. There will also be a poster session. Authors will be encouraged to submit papers to the Journal of Computational Neuroscience. <br/><br/>SAND7 is the seventh workshop in a series that began in 2002. The objectives of the workshop are to define important problems in neuronal data analysis and useful strategies for attacking them; foster communication between experimental neuroscientists and those trained in statistical and computational methods; and provide further dissemination of the findings presented at the workshop via a set of peer-reviewed articles. Other objectives are to encourage young researchers, including graduate students, to present their work; expose young researchers to important challenges and opportunities in this interdisciplinary domain, while providing a small meeting atmosphere to facilitate the interaction of young researchers with senior colleagues; and include as participants women, under-represented minorities and persons with disabilities who might benefit from the small workshop environment. See http://sand.stat.cmu.edu/sand for further information."
"1406016","Spatial-temporal models and methods for big nonstationary multivariate","DMS","STATISTICS","07/15/2014","05/27/2016","Montserrat Fuentes","NC","North Carolina State University","Continuing Grant","Gabor Szekely","01/31/2017","$210,000.00","Lian Xie, Joseph Guinness","mfuentes@vcu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","","$0.00","High dimensional statistical problems are prevalent in the environmental sciences, particularly in soil, atmospheric, and oceanic data applications. In these cases the processes of interest are inherently nonlinear and dynamic. Different sources of information for these systems include spatial observational data as well as physics and chemistry based numerical models. Over the past decade there has been an increase in the amount of available real-time geographic information as well as advances in the sophistication and resolution of deterministic atmospheric and oceanic models. A broad class of spatial-temporal models is developed for multivariate processes on Euclidean spaces and the sphere to explain the variability and the cross-dependency between different variables. This general class of models goes beyond standard assumptions, in particular of stationarity. The properties of the proposed methods, as well as the asymptotic properties of the estimates are studied. Likelihood approximation methods for massive spatial data are presented to efficiently implement the proposed statistical models. The proposed framework and models are used to better model soil pollution, air pollution, and wind fields. These high spatial resolution wind fields are used to predict energy production from windmills, they are also the primary forcing for numerical forecasts of the coastal ocean response to force winds such as the height of the storm surge and the degree of coastal flooding. The goal is to obtain more accurate estimation of wind fields over land and water to improve the quality of storm surge forecasts, and wind energy.<br/><br/>The most important scientific contributions of this research project are: the introduction of flexible spatial models on the sphere for prediction and estimation of environmental spatial processes observed over larger regions on the Earth's surface; methods for likelihood approximation of big spatial temporal lattice data in general situations; general and flexible models for spatial prediction of multivariate environmental processes on spatial lattices, introducing the concept of conditional correlation in spatial lattice models; and advanced methods for spatial prediction and estimation in the presence of massive data from observations and physical and chemistry models. In these cases the processes of interest are inherently nonlinear and dynamic. Different sources of information for these systems include observational data as well as physics-based numerical models. Over the past decade there has been an increase in the amount of available real-time observations as well as advances in the sophistication and resolution of deterministic chemistry, atmospheric and oceanic models. Our methodology  will provide more accurate representation and prediction of the underlying space-time process of interest. Through our collaborative work, we will help the enhancement of science by implementing these methods to hurricane wind fields and to weather and air and soil pollution to improve weather and air/soil quality mapping. The investigators will  disseminate broadly the methods proposed here to enhance mathematical and scientific understanding. The PI will offer  short courses in Spanish in Hispanic countries to broaden the participation of underrepresented geographic and ethnic groups. A course in advanced spatial statistics methods will be taught by the PI, and the new statistical methods proposed here will be introduced to the students.  The investigators will continue their efforts to broaden the participation of minorities and women in Sciences and the PI  through this project will continue  her involvement on K-12 educational efforts, through the Kenan Fellows for Curriculum and Leadership Development Program and the Science House at NCSU."
"1407400","RUI:   Classification, regression, and density estimation with missing variables","DMS","STATISTICS","09/01/2014","07/31/2016","Majid Mojirsheibani","CA","The University Corporation, Northridge","Continuing Grant","Gabor Szekely","08/31/2018","$119,999.00","","majid.mojirsheibani@csun.edu","18111 NORDHOFF ST","NORTHRIDGE","CA","913300001","8186771403","MPS","1269","9229","$0.00","This project develops statistical theory and methods for nonparametric classification and curve estimation in the presence of missing or incomplete data. Many data sets have missing values; these include the data from biomedical studies, remote sensing, as well as social sciences. There are a number of classical approaches for handing the missing data.  Many of the existing results first impute for the missing values and then apply a standard statistical technique to carry out inferences.  However, a study of the theoretical validity of such techniques can become intractable due to the loss of independence assumption in the data; this is particularly true for distribution-free statistical methods. The new results of the Principal Investigator (PI) will answer a number of fundamental questions in statistical classification and pattern recognition with applications to biomedical, remote sensing, and social sciences.  The new results will also solve many important theoretical problems at the intersection of machine learning and statistical classification.<br/><br/>A long-standing problem in classification with missing covariates involves the situation where missing covariates can appear in both the data and in the new unclassified observation. This is fundamentally different from the simpler problem where missing covariates appear in the data only. In the latter case, standard methods based on Horvitz-Thompson inverse weighting can be used to construct asymptotically optimal classifiers. One part of the PI's research project focuses on this challenging case of classification with missing covariates. The PI will develop new asymptotically optimal local-averaging-type classifiers, such as kernel and partitioning rules.  Another part of this project concentrates on the continuation and refinements of the PI's previous efforts on combined classification and estimation, based on recently obtained results in the literature.  The PI is currently developing new methods to combine several individual classifiers in such a way that the asymptotic error of the resulting classifier will be at least as good as that of the best individual classifier.  The PI will also develop methods to combine several regression function estimators in an optimal way. Tools from the empirical process theory will be used to establish the large-sample optimality of the resulting classifiers and estimators. The third part of the project focuses on the weak convergence of various norms of kernel density estimates in the presence of missing data. The PI will study weighted bootstrap approximations of these statistics. Such results will allow someone to construct correct confidence bands for the unknown density in the presence of missing values. The main tools here are the strong approximation theorems that allow one to replace the weighted bootstrapped empirical processes by a sequence of Brownian bridges."
"1407241","Graph-based Learning and Inference for Sparse Regularized Techniques","DMS","STATISTICS","08/15/2014","08/24/2016","Yufeng Liu","NC","University of North Carolina at Chapel Hill","Continuing Grant","Gabor Szekely","07/31/2018","$120,000.00","Shu Lu","yfliu@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","Machine learning is a very active area of interdisciplinary research, closely related to statistics, optimization, and computer science. The goal of this project is to develop several cutting-edge machine learning techniques for solving high dimensional problems. The team plans to develop new techniques for estimating complex graphs and to establish inference procedures for sparse regression methods, using some recently developed tools in optimization. Techniques to be developed in this project have a wide range of applications in many disciplines. Such applications help to promote interdisciplinary research among statistics, operations research, and bioinformatics. Several students will be involved in the research activities.<br/><br/>Many machine learning techniques fit in the regularization framework. This project will develop several new regularized methods. In particular, the team will use sparse regularized tools for complex graphical model estimation. Furthermore, the team will build a new inference tool for sparse regularized regression methods such as the LASSO, by reformulating the LASSO problem as a stochastic variational inequality in optimization. State-of-the-art techniques in optimization will be introduced to the statistical community.  The researchers are committed to establishing both theoretical properties and efficient computational tools for the designed methods. Applications in various disciplines will help to generate new knowledge and inspirations from those disciplines."
"1407655","Advanced Statistical Tools for Ultra-High Dimensional Functional Data with Spatial-Temporal Correlation","DMS","STATISTICS","07/15/2014","04/28/2016","Hongtu Zhu","NC","University of North Carolina at Chapel Hill","Continuing Grant","Gabor Szekely","05/31/2017","$300,000.00","Haipeng Shen","htzhu@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","This project concerns developing innovative advanced statistical tools for the analysis of ultra-high dimensional functional data with spatial-temporal correlation. The primary motivating application is neuroimaging analysis, relevant to the BRAIN Initiative. (However, the developed methods and theory are applicable to a much broader range of fields involving spatial-temporal modeling.) The research program has a strong multidisciplinary collaborative component, with key team members drawn from biostatistics/statistics, computer science, psychiatry, radiology, and psychology. The tools and software under development can have immediate impacts in clinical research, and have wider applications in medical studies of HIV/AIDS, major neuropsychiatric and neurodegenerative disorders, normal brain development, and cancer, among many others. The problems addressed are also of broad interest to general society, since they relate to pressing issues such as health care policies and social security planning.<br/> <br/>With modern imaging techniques, many large-scale studies have been or are being widely conducted to collect a wealthy set of functional data and clinical data. Functional data share four common and important features: (i) extremely high dimensional, (ii) piecewise smooth, (iii) temporally, and (iv) spatially dependent. The analysis of such data and integration of them with clinical data have been hindered by lack of effective statistical tools and theory, underscoring the great need for methodological and theoretical development from a statistical perspective.  The project addresses challenges from three broader perspectives in both time and frequency domains. The first perspective develops spatial-temporal models for adaptive function estimation. The models can effectively extract informative markers from noisy functional data. The second perspective concerns reduced rank models for groups of functional data with spatial-temporal correlation. In addition, the third perspective develops advanced functional mixed effects models for modeling varying association function between repeated functional responses and a set of covariates of interest, while accounting for complex spatial-temporal correlation."
"1405410","Collaborative Research: Second Order Inference for High-Dimensional Time Series and Its Applications","DMS","STATISTICS","08/15/2014","06/27/2019","Wei Biao Wu","IL","University of Chicago","Continuing Grant","Gabor Szekely","07/31/2020","$299,016.00","","wbwu@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","","$0.00","During the last decade, analysis of high-dimensional independent data has gained substantial attention. The project involves a variety of estimation and statistical inference problems for high-dimensional multiple time series, a universal type of data seen in a broad spectrum of real applications in spatio-temporal statistics, biomedical engineering, environmental science, finance, and signal processing. As an important research problem, one should extract information from a large number of time series, where the second order structure plays a fundamental role in those applications.<br/><br/>Results developed from this project will provide the theoretical foundation for estimating and inference of the space-time covariance and precision matrix, their related functionals, and time-varying graphs of high-dimensional time series. All of the problems are linked together to characterize the second order properties of the high-dimensional time series with the non-linear and non-stationary time dependent features. We will also study enhanced methods that account for the temporal and spatial dependence structures. Results from this research are useful for understanding the dynamic features of high-dimensional dependent data. In particular, the techniques are applicable to biomedical engineering problems such as modeling brain connectivity networks by using fMRI data."
"1407732","Support Vector Machines for Censored Data","DMS","STATISTICS","07/01/2014","06/17/2016","Michael Kosorok","NC","University of North Carolina at Chapel Hill","Continuing Grant","Gabor Szekely","06/30/2018","$410,000.00","","kosorok@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","Recent advances in medical research, including those related to the study of the human genome, have led to the development of personalized medicine. Personalized medicine describes medical treatment that is tailored to a patient based on the patient's genetic profile and other personal biomedical information. Developing personalized medical treatment regimens is challenging since it involves learning from high-dimensional patient data. Moreover, data from personalized-medicine clinical studies are typically subject to censoring, i.e., the data are not fully observed, due, for example, to patients dropping out during the course of a study. While statistical analysis of censored data is a well-developed area, most of the existing statistical tools were developed under restrictive assumptions that often do not hold for high-dimensional data settings. It is therefore important to develop an approach for analysis of censored data that can be applied to today's high-dimensional data sets. In this project we will develop novel machine learning techniques that can handle both high-dimensional and censored data. The algorithms that we will develop will be applicable not only in the field of personalized medicine, but also in other disciplines in which high-dimensional censored data are common, such as engineering, economics, and sociology.<br/><br/>In this research, we will extend the framework of support vector machines (SVMs) to censored data. First, we will develop support vector learning techniques for different types of censored data, including right censored data, interval censoring, current status data, and multistage decision problems with censored data. We will then study the theoretical properties of these estimators, including their finite-sample properties and their asymptotic behavior. For this goal we will develop novel methodology, including new finite-sample tools for censored data. Finally, we will apply the tools that we develop to real-world data. We will compare the proposed learning methods with existing methods using theoretical tools, simulation, and analysis of real-world data. Finally, we will develop software for each of the different algorithms that we study. This software will be developed such that it can be integrated into existing machine learning software."
"1407812","Collaborative Research: Better efficiency, better forecasting, better accuracy: A new light on the dependence structure in high frequency data","DMS","STATISTICS","08/15/2014","08/10/2014","Per Mykland","IL","University of Chicago","Standard Grant","Gabor Szekely","07/31/2018","$196,149.00","","mykland@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","","$0.00","Recent years have seen an explosion in the availability and size of data in many areas of endeavor; the phenomenon is often referred to as big data. This project is concerned with a particular form of such data, namely high frequency data (HFD), where series of observations can see new data to arrive in fractions of milliseconds. HFD occurs in medicine, in finance and economics, in certain recordings relating to the environment, and perhaps in other areas. Research is often concerned with how to turn this data into knowledge, and this is where the current project will help. Specifically, the project has discovered a new way to look at the dependence relationships between the parameters governing the state of the HFD system. The new dependence structure permits the borrowing of information from adjacent time periods, and also from other series if one has a panel of data. The consequences of this new approach are being explored by the project. The research produces transformational improvements in the statistical handling of high frequency data.<br/><br/>The new way to look at dependence involves the representation of series of ordinary integrals with the help of stochastic integrals. This permits the use of high frequency regression techniques to connect the information in adjacent time intervals. It is achieved without altering current models. This has far-reaching consequences, leading to more efficient estimators, better prediction, and, in terms of accuracy, a more systematic treatment of the estimation of standard errors. Model selection will also be greatly facilitated. The methodology does not depend on either time or panel size being large; neither does it depend on assumptions such as stationarity of the data series. All the new dependence relationships can be consistently estimated from high frequency data inside the relevant time periods. Efficiency gains are at the very least close to 50%, and thus existing efficiency bounds will become irrelevant. It is expected that this approach will form a new paradigm for high frequency data. In addition to developing a general theory, the project is concerned with applications to financial data. Applied quantities of interest include realized daily volatility, correlations, leverage effect, volatility risk, fraction of jumps, and so on. We also work on applications to risk management, forecasting, and portfolio management. More precise estimators, with improved standard errors, will be useful in all these areas of finance. The results are of interest to main-street investors, regulators and policymakers, and the results are entirely in the public domain. The dependence structure also has application in other areas of research that have high frequency data, including medicine, neural science, and turbulence."
"1405746","High-Dimensional Covariance Estimation via Convex Optimization","DMS","STATISTICS","08/15/2014","05/23/2016","Jacob Bien","NY","Cornell University","Continuing Grant","Gabor Szekely","07/31/2017","$120,001.00","","jbien@marshall.usc.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","","$0.00","Modern technologies allow researchers to measure an unprecedentedly large number of attributes regarding the subjects of their study.  An important question in many applications is how one can infer from such data the underlying relationships between these attributes.  Such an objective can be expressed through a fundamental construct in the field of statistics known as the covariance matrix.  Using classical statistical techniques would require one to collect data on a prohibitively large number of subjects to reliably estimate this matrix.  In this work, novel statistical methods will be developed that allow researchers to make sound inferences by making better use of their data given the limited number of subjects they have available.  The methods developed will be applicable in a wide range of fields.  For example, in biology, one can infer the structures of massive networks of genes based on a small number of samples.  Beyond being an end in itself, the covariance matrix is a key ingredient in many common statistical procedures.  Thus, by developing the ability to reliably estimate it from small numbers of subjects, this work will enable the use of many other methods that would otherwise be unavailable to researchers.  Application areas include disease diagnosis, basic biology, sensor networks, and social networks.<br/><br/>The research program will focus on high-dimensional covariance estimation and capitalize on the strengths of the convex framework to develop novel statistical methodology.  This work will involve developing efficient algorithms, thoroughly investigating the properties of estimators and algorithms through a combination of theory and simulation, and applying methods to real datasets.  The research focus is in two main areas:  (A) In certain applications, the variables have a known ordering.  Such structure suggests the use of a convex penalty not previously applied to covariance estimation.  This work will carefully study using such a penalty to estimate both the covariance matrix and the inverse covariance matrix.  (B) Estimating a covariance matrix as a simultaneously sparse and positive definite matrix is a natural goal, and yet the standard penalized likelihood approach is not convex.  This research will develop convex-optimization-based estimators that still make use of the likelihood.  For all projects, software will be produced, made freely available online, and maintained so that other researchers can benefit from its use."
"1407649","Probability Theory and Statistics in High and Infinite Dimensions: Empirical Processes Theory and Beyond","DMS","PROBABILITY, STATISTICS","05/01/2014","03/13/2014","Vladimir Koltchinskii","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","04/30/2015","$23,000.00","Jon Wellner","vlad@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1263, 1269","7556","$0.00","The conference on ""Probability Theory and Statistics in High and Infinite Dimensions: Empirical Processes Theory and Beyond"" will focus on empirical processes and other methods of high-dimensional probability and their role in a variety of problems related to statistical inference for high-dimensional data as well as to nonparametric inference. The conference will be held on June 23-25, 2014 at the Center for Mathematical Sciences, University of Cambridge, UK. The list of topics includes concentration inequalities, exponential and moment bounds for Gaussian, empirical and other related processes, nonasymptotic bounds for random matrices, complexity penalization methods and oracle inequalities in high-dimensional inference, sparse recovery and low rank matrix recovery, high-dimensional problems in machine learning, nonparametric estimation and hypotheses testing, Bayesian nonparametrics.<br/><br/>High-dimensional probability and statistics have had impact far beyond mathematical sciences. The methods of high-dimensional probability have been crucial in understanding the performance of new machine learning algorithms and in the design of contemporary methods of analysis of big data. Bringing together researchers who have made and are currenly making important contributions to this area would facilitate the exchange of ideas leading to much better understanding of the role of high-dimensional phenomena in statistical inference and machine learning. The conference would be especially beneficial for junior participants entering these research areas.  This award will support the participation of approximately twenty US-based participants in the conference.<br/><br/>Conference web site:  www.statslab.cam.ac.uk/~nickl/Site/2014.html"
"1407037","Statistical Modeling, Adjustment and Inference for Seasonal Time Series","DMS","STATISTICS","09/01/2014","08/22/2014","Xiaofeng Shao","IL","University of Illinois at Urbana-Champaign","Standard Grant","Gabor Szekely","08/31/2017","$220,000.00","","xshao@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","","$0.00","This project studies novel inference procedures and models for seasonal time series. The results of this research will have direct impact on the diagnostics of seasonal adjustment procedures that are currently implemented at the U.S. Census Bureau and other domestic or foreign agencies where seasonal adjustments are routinely published. The ""Visual Significance"" method used at the Census Bureau lacks a rigorous statistical justification and the new spectral peak detection methods will help to quantify type I and II errors in a disciplined fashion for a wide class of processes. Although motivated by research problems at Census, the new methodology and models are expected to be useful in the analysis of time series from various disciplines, including economics, astronomy, environmental science, and atmospheric sciences, among others.<br/><br/>Specifically, the project consists of three interrelated parts. In the first part, the PI will develop two new methods of spectral peak detection, which are intended to provide more principled approaches to the ""Visual Significance"" method used at the U.S. Census Bureau. In the second part, the PI will address the band-limited goodness-of-fit testing using the integral of the square of the normalized periodogram. Instead of assuming the strong Gaussian-like assumption as done in the literature, the PI will use a new Studentizer, so that the limiting distribution of the self-normalization-based test statistic is pivotal under less stringent assumptions. In the third part, the PI will study a new parametric class of spectral density, which can be used in model-based seasonal adjustment to improve the quality of model fitting and seasonal adjustment. The new parametric models and related model-based seasonal adjustment, if successfully developed, may offer a more effective means of modeling and adjusting time series. The research will promote teaching and training through mentoring of undergraduate and graduate students and through the development of related lecture notes."
"1401118","Travel Support for the 12th ISBA World Meeting on Bayesian Statistics","DMS","STATISTICS","06/01/2014","01/17/2014","Bruno Sanso","CA","University of California-Santa Cruz","Standard Grant","Gabor Szekely","05/31/2015","$15,000.00","Raquel Prado, Gabriel Huerta","bruno@soe.ucsc.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1269","7556","$0.00","This project provides travel support for the 12th World Meeting of the International Society for Bayesian Analysis (ISBA), to be held in Cancun, Mexico, July 14-18, 2014. The theme of the conference will be on theory, modeling, and applications of Bayesian statistics. It is anticipated that the conference will be attended by an international group of academics, researchers, professionals, government employees and students, from countries around the world. The main focus of this project is on funding for junior researchers that reside in the U.S. <br/><br/>Bayesian methods provide a coherent framework that facilitates blending information from different sources and communicating findings and conclusions using probabilities.  ISBA-2014 is a scholarly conference where new Bayesian statistics ideas will be exchanged. It is a truly international meeting for the dissemination of state of the art statistical methods that use the Bayesian approach. The meeting will promote the development of statistical theory and its applications to engineering and the sciences.  It will provide participants with the opportunity to interact with colleagues from different countries, working in a variety of fields of application of statistical methods."
"1406455","Statistical Inference on Dynamic Networks","DMS","STATISTICS","07/01/2014","06/20/2014","Yuguo Chen","IL","University of Illinois at Urbana-Champaign","Standard Grant","Gabor Szekely","06/30/2018","$381,439.00","","yuguo@uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","","$0.00","With the fast development of information technology and the emergence of online networks, an increasing number of large scale dynamic network data sets become available. This project develops statistical analysis and modeling of dynamic networks. The model under study has the advantage of providing rich visualization of the dynamics of the network, allowing better understanding of the network structure as well as the behavior of individual nodes. The proposed inference procedure makes it possible to handle large scale dynamic network data, such as gene regulatory networks and disease transmission networks. The model can be used to analyze terrorist networks, study social interaction patterns, model disease transmission, and much more. The research project provides an ideal opportunity for involvement of students with a broad range of background and interests. Additional broader impacts include incorporation of the methods into relevant courses and dissemination of research results to the scientific community.<br/><br/>This project develops statistical models and inference procedures for dynamic networks, including binary networks, weighted networks, and other complex networks. A state space model will be developed which embeds dynamic network data into a latent Euclidean space, allowing each node to have a temporal trajectory in this latent space. A Markov chain Monte Carlo algorithm is proposed to estimate the model parameters and latent positions of the nodes in the network. The model parameters provide insight into the structure of the network, and the visualization provided from the model gives insight into the network dynamics. In addition, the model can handle missing edge data and predict future edges between nodes. The methods will be applied to real network data from natural and social sciences."
"1407838","Bayesian nonparametric methods for spectral analysis of complex brain signals","DMS","STATISTICS","09/01/2014","08/29/2016","Raquel Prado","CA","University of California-Santa Cruz","Continuing Grant","Gabor Szekely","08/31/2018","$120,000.00","Athanasios Kottas","raquel@ams.ucsc.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1269","","$0.00","Complex multiple time series are often recorded in several applied areas of research such as neuroscience, environmetrics, and econometrics. This project develops Bayesian nonparametric methods and related computational tools for frequency-domain analysis of multiple time series. In particular, the statistical approaches that will be developed in this project are motivated by the need to analyze brain signals recorded in clinical and non-clinical studies including electroencephalograms, fMRI data, and magnetoencephalograms.<br/><br/>A novel and flexible mixture modeling framework will be used to represent the spectral characteristics of multiple time series. Computationally efficient algorithms will be implemented, tested and used to analyze complex and large-dimensional brain signals. These algorithms will make use of a variety of computational methods for inference in Bayesian nonparametric models. The models and methods that will be developed have the following key features: (i) they will provide flexible representations of the spectral densities of multiple signals as well as computational feasibility (ii) they will allow researchers to investigate clustering patterns of multiple time series with similar spectral characteristics, and (iii) they will incorporate hierarchical settings that can appropriately accommodate neuroscience data sets involving multiple trials, multiple subjects and/or relevant covariates. The research project has the potential of impacting data-intensive neuroscience research that requires the analysis of several complex brain signals."
"1417056","Advances in Interdisciplinary Statistics and Combinatorics, October 10-12, 2014","DMS","STATISTICS","06/01/2014","04/21/2014","Sat Gupta","NC","University of North Carolina Greensboro","Standard Grant","Gabor Szekely","05/31/2015","$10,000.00","Shan Suthaharan, Jan Rychtar","sngupta@uncg.edu","1000 SPRING GARDEN STREET","GREENSBORO","NC","274125068","3363345878","MPS","1269","7556","$0.00","This award supports the participation of junior researchers in the  international conference ""Advances in Interdisciplinary Statistics and Combinatorics,"" held at The University of North Carolina at Greensboro on October 10-12, 2014.  The conference brings together statisticians and researchers from other disciplines to discuss the use of statistical techniques in research programs in a variety of disciplines including anthropology, biology, computer science, economics, education, education research methodology, environmental science, information systems, medicine, psychology, and public health. A major emphasis is on training young researchers through two workshops, one on Big Data and Machine Learning, and the other on Mathematical Biology and Game Theory. In addition to these two workshops, the conference includes a variety of sessions on topics that highlight the use of statistical methods in other disciplines.  <br/><br/>The main goal of the workshop on Mathematical Biology and Game Theory is to introduce undergraduates and graduate students to the mathematical modeling that is an essential part of any research in applied mathematics, mathematical biology in particular. The workshop focuses on game theoretical models that are used for situations where several entities interact (directly or indirectly) with one another and where each entity acts in its own interest (potentially in conflict with the interests of others).  The workshop on Big Data and Machine Learning, designed for graduate students, will provide a forum for researchers from academia and industry to exchange information and develop new research directions in areas of big data and machine learning focusing on scalability, reliability, and security. These workshops will help to advance the next generation of researchers on some of the most important contemporary topics in statistical science.  The meeting also offers an opportunity to students from underrepresented groups to be part of a high profile conference to broaden their vision of research in mathematical sciences and advance their career goals. More details are available at the conference website http://www.uncg.edu/mat/aisc/2014/index.html"
"1360777","Algebraic Statistics 2014: Conference at IIT, May 19-22, 2014.","DMS","STATISTICS","05/01/2014","02/03/2014","Sonja Petrovic","IL","Illinois Institute of Technology","Standard Grant","Gabor Szekely","10/31/2014","$10,000.00","Seth Sullivant, Ruriko Yoshida, Despina Stasi","spetrov1@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","MPS","1269","7556","$0.00","Building on the success of the 2012 Conference, Algebraic Statistics in the Alleghenies at Pennsylvania State University, a four day conference on Algebraic Statistics at the Illinois Institute of Technology, May 19-22, 2014 will provide a focus for the further development and maturation of this exciting area of research as well as its interconnections.  The unifying theme for the proposed workshop is provided by the common mathematical tool set as well as the increasingly close interaction between algebra and statistics.  The program will allow researchers working in statistics to interact with those working in algebra, algebraic geometry, and discrete mathematics, and make fundamental advances in the development and application of algebraic methods to statistics.  <br/><br/>Algebraic statistics advocates polynomial algebra as a tool for addressing problems in statistics and its applications, and it motivates further developments in theoretical and computational algebraic geometry.  The program will foster international collaborations between U.S.-based participants and their international counterparts. This funding will allow junior researchers to attend the meeting and present their work, and interact with the broader community of researchers in algebraic statistics."
"1360865","FLINT: ONE CITY - ONE HUNDRED YEARS UNDER VARIABILITY","DMS","STATISTICS","04/01/2014","11/15/2013","Boyan Dimitrov","MI","Kettering University","Standard Grant","Gabor Szekely","03/31/2015","$7,500.00","Srinivas Chakravarthy, Leszek Gawarecki","bdimitro@kettering.edu","1700 University Ave","Flint","MI","485046214","8107629677","MPS","1269","7556","$0.00","The proposed conference titled ""Flint: One City - One Hundred Years Under Variability"" is scheduled to be held at Kettering University, Flint, Michigan, June 24-28, 2014. One of the main goals of the project is to bring together experts in the fields of predictive statistics with expertise in statistical modeling, education, and applications.  A specific emphasis is on statistics of historical data. Through this event, the organizers intend to celebrate the International Year of Statistics and the 175th anniversary of the American Statistical Association.<br/> <br/>The practitioners in engineering and applied sciences are now extensively using statistical methods to solve problems arising in real-life situations dealing with local and federal governments, medical services, insurance, actuaries and public media.  While most theoretical aspects and the range of applications of statistical methods have been well established, the organizers of this conference plan to bring together the most active and best researchers, educators, and practitioners to a common setting. The organizers hope that the participants such as students, teachers, and practicing statisticians will take back their experience to enhance their careers as well as to promote the use of statistics in day-to-day activities to benefit society."
"1407852","Modeling Complex Functional Data","DMS","STATISTICS","07/01/2014","06/20/2014","Hans-Georg Mueller","CA","University of California-Davis","Standard Grant","Gabor Szekely","06/30/2018","$337,658.00","","hgmueller@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","","$0.00","There is increasing need for adequate statistical analysis methods for samples of time-dynamic and longitudinal data that include high-dimensional and other complex objects that are recorded over time. Such data arise in many areas of critical concern where adequate methodology to quantify changes of complex patterns over time is relevant to come to the right conclusions. This includes studies of environmental and climate change and the evolution of social networks over time. It also includes biological networks that are thought to operate between ""hubs"" in the brain and that are changing as a person ages. Such data are objects with a structure that defies established linear modeling approaches that have been previously developed in the statistical field of functional data analysis. The investigator and his research group are therefore developing new methods aimed at the analysis of such complex time-dynamic and longitudinal data. These techniques will lead to a better understanding of the underlying dynamics and the changes in the inherent patterns and interactions of the objects over time. Better understanding of time-dynamic phenomena will lead to better informed societal and individual decision making. The research also involves the training of students in upfront methodology and the creation of suitable fast algorithms to implement these new methods. <br/><br/>The established field of Functional Data Analysis has been focused on linear methods such as functional linear principal component analysis. However, the applicability of these established  models for ""next generation"" functional data is quite limited. This is because such data are inherently nonlinear, i.e., they do not live in a linear space, and therefore are far more complex than linear data. Second generation functional data are generated across the sciences and e-commerce and by online and physical tracking devices that continuously record signals and trajectories, in on-line settings, monitoring and sensor systems. Important types of time-varying  ""object data"" are covariance matrices, densities or networks, and new methods will be developed to properly quantify the dynamics of such complex objects. Besides the non-Euclidean nature of these objects, additional challenges arise from the sparsity of available data in many longitudinal studies, due to missing data or irregular sampling schemes. This research addresses these issues and challenges and will lead to techniques to quantify the variability of complex dynamics by developing principled and theoretically supported methodology for the analysis of these more complex data types. This research will benefit the analysis and understanding of the time dynamics of social and biological networks, and of other time-indexed objects, as well as the analysis of high-dimensional functional data. The research includes novel methods for the classification of functional data, which is an increasingly important problem in itself. It also includes methodology for the analysis of short snippets of time-dynamic observations, due to the need to come to quick conclusions under limited observation time, which is a feature of many current longitudinal studies in biomedical fields."
"1342467","Workshop on Dimension Reduction and High-dimensional Inference: Theory and Applications","DMS","STATISTICS","01/01/2014","11/15/2013","Zhihua Su","FL","University of Florida","Standard Grant","Gabor Szekely","12/31/2014","$7,500.00","Malay Ghosh, Hani Doss","zhihuasu@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","7556","$0.00","The workshop on ""Dimension Reduction and High-Dimensional Inference: Theory and Applications"" will be held on January 17-18, 2014, on the campus of the University of Florida, Gainesville, FL, USA. The project will provide travel support for 25 young researchers and two invited speakers. The field of dimension reduction has a long history, but the overarching aim is to reduce the dimension of a family of multivariate random vectors in such a way that the information deemed relevant is preserved. Today, with high-throughput technologies and fast computing, high dimensionality in data is pervasive. Dimension reduction is now a prevalent theme throughout the applied sciences, including genetics, food science, biomedical engineering, economics and computer science. The area of dimension reduction is quickly evolving and expanding to adapt to this new reality. In this workshop, twelve distinguished individuals who work in dimension reduction and high-dimensional inference will review the current state of the field and present their recent work.  A number of young researchers will participate in the workshop and present their work in poster sessions.<br/><br/>Dimension reduction offers an appealing avenue for dealing with high dimensional problems. In effect it transforms a high dimensional data set to a low dimensional one by identifying and combining a small set of important variables which give as much or nearly as much  information as the original large set of variables. Then one can build models and perform estimation or prediction based on the low dimensional data set. Many existing models and approaches, which do not apply to high dimensional data, can be applied to the reduced low dimensional data. In addition, effective reduction in dimension often makes it possible to visualize the data, which can facilitate subsequent model development. Dimension reduction is now an active research area, but many unsolved problems remain. The workshop provides an excellent opportunity for established researchers in the field, as well newcomers, to discuss the significant developments that have taken place recently; to discuss what works and what does not; and to identify important problems and new research directions."
"1418827","16th Meeting of New Researchers in Statistics and Probability, July 31- August 2, 2014","DMS","STATISTICS","04/01/2014","04/16/2014","Edoardo Airoldi","MA","Harvard University","Standard Grant","Gabor Szekely","09/30/2014","$20,000.00","","airoldi@temple.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","7556","$0.00","This award supports participation in the 16th New Researchers Conference, a conference series organized by, and held for, junior researchers under the auspices of the Institute of Mathematical Statistics (IMS). The meeting will take place over three days, July 31 to August 2, 2014, at Harvard University in Cambridge, Massachusetts. The primary objective is to provide a much needed venue for interaction among new researchers in statistics and probability. The meeting will be held immediately preceding the Joint Statistical Meetings (JSM) 2014 in Boston, Massachusetts. Participants will have received their Ph.D. within the past five years or are expecting to receive their degree within the same year. Each participant will present a short expository talk and a poster. Topics will cover a variety of areas in statistics and probability, from theory and methods to applications. Senior speakers will give plenary talks for inspiration and take part in four discussion panels covering topics of importance for young people embarking on an academic/research career: teaching, mentoring, publishing and funding. <br/><br/>This conference series is explicitly aimed at training the future leaders and workers in statistics and probability. In helping to create networks of new researchers, it lays the groundwork for future collaboration and informal exchange of ideas and knowledge. It is also critical for building professional cohesion within the fields of statistics and probability and setting new frontiers for research. Meeting people outside of one's research specialty area, and learning about their research, favors a more comprehensive view of research, which is important when taking editor positions and other professional service activities. The conference attracts participants with different backgrounds and nationalities, and underrepresented groups (women, minorities and people with disabilities) are explicitly encouraged to attend. The conference website is available at http://www.stat.harvard.edu/NRC2014/."
"1407560","Studies in Factorial and Composite Designs with Applications to Drug Combination Experiments","DMS","STATISTICS","08/01/2014","06/07/2016","Hongquan Xu","CA","University of California-Los Angeles","Continuing Grant","Gabor Szekely","07/31/2017","$120,000.00","","hqxu@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","","$0.00","The advance in technology enables researchers to investigate a dozen or more drugs simultaneously in a single experiment in order to identify novel drug combinations with high efficacy and low toxicity. This raises substantial new challenges to experimenters and statisticians, because the number of drug combinations grows exponentially as the number of drugs increases.  Researchers rely more than ever on large efficient experimental designs in order to successfully and efficiently identify a few important drugs and drug interactions among millions or billions possible drug combinations. However, few good designs and results are available in the literature. To address the growing needs from such large-scale drug combination experiments, this project aims at developing new methodology and constructing efficient factorial and composite designs.  This project will help shorten investigation time and reduce experimental cost tremendously in a wide variety of scientific researches. The applications to drug combination experiments will have an immediate impact on discovering novel treatments for various common diseases and hence help advance the national health. <br/><br/>Fractional factorial designs are cost-effective for screening important factors from a large pool of potential variables;  composite designs are indispensable for sequential experimentation, as well as for building response surface models. A systematic method is proposed for constructing multi-level nonregular fractional factorial designs using linear and quadratic functions.  A general theory is developed for obtaining important design properties such as resolution and aberration.  New tools are introduced for studying design structure under level permutations for quantitative factors.  Two classes of composite designs are introduced and studied thoroughly.  The first class consists of a two-level and a three-level factorial design; the second consists of a two-level factorial and a three-level definitive screening design.  These composite designs have many desirable features and are more effective than existing composite designs for factor screening and response surface modeling, especially for large-scale experiments."
"1407670","RUI: A Bayesian Approach to Sequential Change Point Detection","DMS","STATISTICS","07/15/2014","07/03/2014","Eric Ruggieri","MA","College of the Holy Cross","Standard Grant","Gabor Szekely","06/30/2018","$135,490.00","","eruggier@holycross.edu","1 College Street","Worcester","MA","016102395","5087932741","MPS","1269","9229","$0.00","This project will develop an efficient change point algorithm that not only will indicate when change points occur, but also provide uncertainty estimates as to the number and exact timing of these changes.  Applications of this model are widespread and include any field where long sequences of data are collected such as medicine (e.g. EEG readings), economics (e.g. stock market data, coal mining disasters), and climate (e.g. temperature readings, glacial records).  More specifically, a 5 million year record of global ice volume shows at least two distinct changes. The first, around 2.7 million years ago, represents an increase in the amount of ice volume on the Earth as permanent glaciers began to form in the northern hemisphere, whereas a more recent change around 0.8 million years ago represents a gradual change in the frequency of major glacial melting events from every 40,000 to every 100,000 years.  A more prominent example concerns NCDC's global temperature anomalies data set that many have cited as evidence of global warming.  This record indicates three changes in the rate of temperature increases on the Earth over the last 133 years - in 1906, 1945, and either 1963 or 1976.  The algorithm will be able to handle sequential data, giving it the ability to quickly update itself as each new observation is recorded, and will be able to accurately predict where in the data set a change point has occurred.  <br/> <br/>It is well known that long time series are often heterogeneous in nature, any attempt to model these data sets may have to account for parameters that change through time.  The difference can be as simple as a change in the mean, slope, or frequency of the underlying signal.  However, the identification of ?change points? is not always a trivial task as the number of potential solutions grows exponentially with the length of the data set, rendering brute force attempts to solve the problem infeasible.  Previous work on a Bayesian change point algorithm has produced an efficient and exact probabilistic solution to the multiple change point problem by using dynamic programming-like recursions to reduce the computational complexity from exponential to quadratic.  Samples drawn from the joint posterior distribution of the change point locations quantify the uncertainty in both the number and timing of changes in the data set.  In this project, the existing change point model will be modified to handle sequential data.  Once this initial objective is complete, research will turn towards further modifications that include the ability to handle correlated error terms and an approximate algorithm that has linear complexity, bringing the computational complexity down to a point where a time series of any length can be analyzed.  The project fits naturally with undergraduate education and will serve as the basis of summer research projects, senior theses, and a potential seminar course for a new statistics program.  The software developed through this project will be made publicly available so as to make this cutting-edge statistical methodology accessible to researchers in a wide variety of fields."
"1403708","Theory and Methods for Estimation of Nonsmooth Functionals and Detection of Simultaneous Signals","DMS","STATISTICS","07/01/2014","06/20/2014","T. Tony Cai","PA","University of Pennsylvania","Standard Grant","Gabor Szekely","06/30/2017","$485,840.00","Mark Low","tcai@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","","$0.00","The collection and analysis of high dimensional data is now of central importance in a wide range of scientific applications, including genomics, bioinformatics, climate studies, and signal processing. There are many new challenges in the analysis of such high dimensional data and there is much demand for the development of new statistical theory and methodology in this field. Although high dimensional data is often analyzed using complex models in many applications, attention is often focused on testing specific hypotheses that are of low dimension. In such cases new statistical theory and methodology relies on the development of new techniques for estimating non-smooth functionals, the topic of this project.<br/><br/>The focus of this research project is to establish a comprehensive methodology for estimating non-smooth functionals and to show how this methodology can be used in the detection and testing of multiple high-dimensional sparse sequences encountered in genomics. The proposed research will provide scientists with new methodologies to analyze such highly complex data. The statistical procedures developed in this project will be implemented in a high-level programming language and made available on the internet with the related research reports so as to allow comparisons with other approaches."
"1463094","A New Treatment to Dimension Reduction via Semiparametrics","DMS","STATISTICS","09/01/2014","09/18/2014","YANYUAN MA","SC","University of South Carolina at Columbia","Standard Grant","Gabor Szekely","06/30/2016","$282,588.00","","yzm63@psu.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","MPS","1269","","$0.00","The principal investigator (P.I.) will study six topics in dimension reduction problems, develop new methodologies and analyze their properties and performances. Topic One concerns central space estimation. The P.I. will initiate a totally different approach from the current literature, using a semiparametric treatment. The new view point results in a complete class of estimators for the central space which contains all possible estimators. In addition, it relaxes various conditions currently required in the existing methods. Finally, it illustrates the relations between various existing estimators and reveals the underlying reason which enables all these estimators to function. Topic Two concerns central mean space estimation. The P.I. will establish parallel results to Topic One, using the same general idea but via different analytic derivation. Topic Three concerns two common conditions in dimension reduction: the linearity condition and the constant variance condition. The P.I. will reveal an astonishing discovery that these conditions are not only redundant, but also detrimental. They are redundant in that consistent estimation can still be carried out of these conditions are relaxed. They are detrimental in two ways. 1. If these conditions are not met, but are falsely assumed, the classical estimators are biased. 2. If these conditions are met and are used, the classic estimators will have inflated variances  compared to the same estimator without using these conditions. This research will prompt re-evaluation of these popular conditions and divert the current research trend of further exploiting these conditions. Topic Four concerns statistical inference and efficient estimation of the central space. The P.I. will devise a convenient parameterization and proposeto convert the space estimation problem to an equivalent parameter estimation problem which facilitates statistical inference. The P.I. will also establish the semiparametric efficiency bound for the central space estimation and will derive the optimal efficient estimator. She will further provide theoretical proof of the root-$n$ convergence rate and the efficiency. Topic Five concerns inference and efficient estimation for the central mean space. The P.I. will establish parallel results to Topic Four, illustrate the difference between the two problems and highlight their drastically different analytic results. Topic Six concerns deciding the dimension of the reduced space. The P.I. will propose three methods tailored to different estimation procedures and examine their usefulness.<br/><br/>The series of projects in this proposal will shed new light on the dimension reduction problems and resolve some of the most fundamental and outstanding issues in this field. Accompanying the modern development of sciences and technologies, richer and more complex data have become a common phenomenon. Data sets from medical science, genetics, environmental science, social behavioral science including internet behavior studies easily contain a large amount of variables that dimension reduction is unavoidable. The new methodologies will generate wide interest and have important application in these fields. They will also provoke further studies and development in related semiparametric problems and computing methods in statistical sciences itself."
"1407460","New Directions in Envelope Models and Methods with Applications to Public Health and Medical Science","DMS","STATISTICS","09/01/2014","08/19/2016","Zhihua Su","FL","University of Florida","Continuing Grant","Gabor Szekely","08/31/2018","$120,000.00","","zhihuasu@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","","$0.00","Multivariate linear regression (MLR) is an approach to understand relationships between predictors and responses, and it is broadly applied for estimation or prediction in many disciplines.  With the development of modern technology, it is possible to measure more and more characteristics and potential factors for a subject, resulting in very large data sets in many contemporary problems.  In such situations, MLR is inefficient because it fails to distinguish between information that is useful to the scientific goal and a likely overabundance of irrelevant information that can obscure the useful information. The envelope model is a new area in statistics.  By identifying the irrelevant information, the envelope analysis is based on the useful information only and is therefore more efficient.  In this project, new directions will be explored to broaden the applicability of the envelope models, especially the applicability in public health and medical sciences. Students involved in the project will have opportunities to interact with faculty from other disciplines, and gain crucial experience on how to operate across traditional disciplinary boundaries.  Software will be developed to make the new methodologies available to the statistical community.  <br/><br/>This project will develop new models that enrich the area of envelopes, making the envelope method more flexible and adaptive to more practical problems.  For example, prior information is often available from medical history, past experience and other sources, the Bayesian envelope model will enable investigators to incorporate prior information in the analysis. As medical studies often involve missing data, envelope models that handle missing data will be studied.  While most existing variable selection methods are applied to the predictors, sparse envelope model focuses on identifying inactive individual responses, leading to more efficient and interpretable results.  The project connects the envelope model with several branches in statistics, including Bayesian analysis, Markov chain Monte Carlo, variable selection and covariance estimation, which will generate new theory, methods and algorithms in the area of envelope models."
"1461796","Nonparametric Inference of Nonstationary Time Series","DMS","STATISTICS","07/01/2014","05/19/2016","Ting Zhang","MA","Trustees of Boston University","Continuing Grant","Gabor Szekely","07/31/2018","$119,980.00","","tingzhang@uga.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","MPS","1269","9150","$0.00","This research project aims to advance the development of statistical methodology and theory for analyzing nonstationary time series. A nonstationary time series is a sequence of data points collected at successive time points with certain aspects changing over time or being nonstationary. Nonstationary time series appear very frequently in various scientific fields including economics, engineering, environmental science, finance, and medical science among others. Ignoring nonstationarity and dependence, two important features of nonstationary time series, can lead to erroneous conclusions. Therefore, the research project is expected to promote scientific research in not only statistics but also other disciplines that involve the analysis of nonstationary time series.<br/> <br/>In order to capture the temporal dynamics resulting from the nonstationarity, the parameters are allowed to change over time and modeled as deterministic but unknown functions of time, which are intrinsically infinite dimensional. To avoid potential misspecifications of parametric models, nonparametric methods are used to estimate and make inference about the underlying parameter functions. The research to be performed involves parameter estimation, hypothesis testing, and variable selection for regression models with time-varying parameters. By allowing a general class of nonstationary and dependent processes, the methods to be developed and the asymptotic results to be established can be widely applicable, and we are able to understand and quantify the effect of nonstationarity and dependence on the asymptotic behavior of parameter estimators and test statistics of interest."
"1406622","Collaborative Research: Theory and Methods for Massive Nonstationary and Multivariate Spatial Processes","DMS","STATISTICS","08/01/2014","07/29/2014","Soutir Bandyopadhyay","PA","Lehigh University","Standard Grant","Gabor Szekely","10/31/2018","$166,013.00","","sbandyopadhyay@mines.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","MPS","1269","","$0.00","The field of spatial statistics is an expanding subset of statistical science with numerous applications in a wide variety of specialties such as geophysical, environmental, ecological and economic sciences.  Modern datasets in these sciences often involve multiple variables observed at thousands to millions of irregularly spaced geographical locations.  Associated scientific goals include surface estimation, stochastic simulation and statistical modeling to gain insight of underlying phenomena.  Statistical analyses require flexible nonstationary and multivariate constructions, which have heretofore been hampered by a lack of models adequate for datasets of large magnitude.  This project addresses this gap in statistical science, developing a unifying framework for nonstationary and multivariate spatial models capable of modeling complex spatial dependencies.  Additionally, the justification for the use of nonstationary models is generally relegated to empirical results with data and simulation experiments; this research will develop a companion theory for exploring the relative benefit of these more complex spatial models. Using the tools introduced in this project, the final major goal is to develop a gridded data product for the historical climate of the United States based on large, irregularly spaced observational networks with transparent statistical methodology and formal quantification of the uncertainty in such an analysis.  Historical data products such as this are of crucial importance in the fields of atmospheric and climate sciences.<br/><br/>Modern spatial statistics has increased focus on developing methods for massive spatial datasets that involve multiple variables with complex dependency structures.  This research aims to foster a common framework via multiresolution processes for modeling nonstationary and multivariate spatial structures that does not break down in the face of large sample sizes.  Multiresolution processes lend themselves to fast estimation and computation, and also to the linked theoretical questions of asymptotic behavior of spatial estimators.  For example, there is a lack of rigorous theoretical treatment of nonstationary approaches, with current understanding limited to experimental results.  The companion large sample theory of this research is aimed at identifying situations in which nonstationary models provide tangible benefits over simpler stationary cousins.  A linked goal is approximation theory for existing spatial constructions; special multiresolution constructions can approximate existing covariances such as the Matern, allowing for a theoretical treatment of spatial smoothing under these common classes of covariances.  Additionally, the project will generalize the notion of a multiresolution process to the multivariate setting, allowing for feasible and flexible inference-based modeling of massive multivariate spatial datasets."
"1407480","On a General Class of Count Time Series Models","DMS","STATISTICS","08/15/2014","10/30/2017","Robert Lund","SC","Clemson University","Standard Grant","Gabor Szekely","07/31/2019","$150,000.00","","rolund@ucsc.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","MPS","1269","9150","$0.00","This research studies data recorded in time that is count-valued, e.g., the annual number of Californian wildfires, yearly Alaskan polar bear sightings, monthly flu deaths, annual North Atlantic severe hurricanes, etc.  The data may be correlated in time, implying that counts observed today may be influenced by (correlated with) counts occurring in the immediate past.  This work develops time series methods that take into account the particular type of distribution appropriate for the counts, e.g., Poisson, geometric, binomial, etc.. Using the correct distribution facilitates accurate forecasts and inferences. The models developed here allow both positive and negative correlations, a feature absent from current statistical count time series models.  For example, annual Pacific and Atlantic hurricane counts are well described by a Poisson-type distribution, but are negatively correlated: when Atlantic hurricane counts are high, Pacific counts tend to be low (and vice versa).  It is important to account for such correlations to make accurate climate change conclusions.<br/><br/>On technical levels, a discrete-time renewal process is used as a general model for a binary (zero-one) series.  The renewal process is rendered stationary by selecting a special initial renewal life length. Copies of the stationary binary sequence are then superimposed in various ways to build the marginal distribution sought.  It is easy to construct stationary series with Poisson, geometric, and binomial marginal distributions.  The methods easily generate stationary count series with negative correlations and/or long-memory --- aspects not achievable from classical integer autoregressive moving-average count techniques.  Inference issues, covariates, and multivariate series are also considered."
"1405698","New Methods for Sequential Monitoring of Longitudinal Patterns","DMS","STATISTICS","08/15/2014","05/02/2016","Peihua Qiu","FL","University of Florida","Continuing Grant","Gabor Szekely","07/31/2018","$120,001.00","","pqiu@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","","$0.00","In many applications, ranging from disease early detection and prevention, maintenance of airplanes, cars and other durable goods and products, to pollution control and environment monitoring, we need to monitor the longitudinal pattern of certain performance variables of a subject. If the observed values of the performance variables of a given subject are significantly worse than the values of a typical well-functioning subject of the same age, then a signal by a statistical method would be extremely helpful so that some proper adjustments or interventions can be made in a timely manner to avoid any unpleasant consequences. This project aims to develop a new statistical method to handle this problem effectively. If successful, research results from this project will have a profound impact on the applications mentioned above.<br/><br/>In the statistical literature, there are two research areas relevant to the above sequential monitoring of longitudinal pattern (SMLP) problem: longitudinal data analysis (LDA) and statistical process control (SPC). By an LDA method, we can compare a new subject with a group of well-functioning subjects to judge whether the new subject's longitudinal pattern is consistent with the regular pattern in a given time interval. One limitation of the LDA methods is that they cannot make a decision about a subject's longitudinal pattern sequentially and quickly even when all available observations up to the current time point have provided enough evidence to support the decision. For solving the SMLP problem effectively, however, this dynamic decision-making feature is crucial. By a SPC method, we can follow each subject sequentially, and make a decision about its performance by comparing its observations at the current time point with all of its history data. One major limitation of the SPC methods is that they cannot compare different subjects when making decisions about a given subject. Therefore, there are no existing statistical methods that can solve the SMLP problem effectively yet. This project proposes a new method that makes decisions about the longitudinal pattern of a subject by comparing it with other subjects cross-sectionally and by using all its history data as well with a sequential monitoring scheme. The new method combines the major strengths of the LDA and SPC methods and should provide an effective solution to the SMLP problem."
"1405011","The SRCOS Summer Research Conference in Statistics and Biostatistics","DMS","STATISTICS","06/01/2014","01/13/2014","Jane Harvill","TX","Baylor University","Standard Grant","Gabor Szekely","05/31/2015","$13,000.00","","Jane_Harvill@baylor.edu","One Bear Place #97360","Waco","TX","767987360","2547103817","MPS","1269","7556","$0.00","The investigator and her colleagues will organize and conduct a Summer Research Conference (SRC) in statistics and biostatistics at the Hotel Galvez, in Galveston, Texas on June 1-4, 2014.  The 2014 SRC will be the 50th anniversary of that conference.  The SRC is an annual conference sponsored by the Southern Regional Council on Statistics (SRCOS).  Its purpose is to encourage the interchange and mutual understanding of current research ideas in statistics and biostatistics, and to give motivation and direction to further research progress.  The project will focus on young researchers, placing them in close proximity to leaders in the field for person-to-person interactions in a manner not possible at most other meetings.  Speakers will present formal research talks with adequate time allowed for clarification, amplification, and further informal discussions in small groups.  Under the travel support provided by this award students will attend and present their research in posters to be reviewed by more experienced researchers.<br/><br/>The Southern Regional Council on Statistics is a consortium of statistics and biostatistics programs in the South, stretching as far west as Texas and as far north as Maryland.  It currently has member programs at 43 universities in 14 states in the region.  This project will fund student participation in the 2014 Summer Research Conference (SRC) sponsored by SRCOS.  The meeting is particularly valuable for students and faculty from smaller regional schools at drivable distances, affording them the opportunity to participate and interact closely with internationally-known leaders in the field without the cost of travel to distant national or international venues.  It will strengthen the research of the statistics and biostatistics community as a whole, and particularly in the relatively underdeveloped southern region.  For all details, go to the conference link at http://srcos2014.rice.edu."
"1406599","Asymptotically Efficient and Efficiently Computable Bayesian Estimation","DMS","STATISTICS","08/01/2014","06/28/2016","Dawn Woodard","NY","Cornell University","Continuing Grant","Gabor Szekely","07/31/2018","$120,000.00","Pierre Patie","woodard@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","","$0.00","The accuracy of computational methods used in statistics will be investigated and improved.  These methods will be used to perform more efficient allocation of ambulances, and improved management of datacenters, among other applications.  New computational methods have allowed the field of Bayesian statistics to dramatically expand, providing advances in areas from finance to information technology.  Despite this, the biggest challenge for the wider adoption of these approaches is still the limitations of the computational methods: there are very few guarantees on their accuracy, and in practice they can be error-prone. In this research project, computational methods with guarantees on accuracy will be introduced and used to benefit applications including the ones listed above.<br/> <br/>Precisely, the goal of this research and education project is to give Bayes estimators that are also efficiently computable for broad classes of models, meaning that an accurate Bayes estimate can be obtained in time that is a low-degree polynomial of the sample size  and the parameter dimension. A lower level of approximation error than of statistical error is required, meaning that the approximated estimator is also asymptotically efficient.  For motivation and application of the techniques, challenging statistical problems arising in management of commercial and nonprofit operations will be used. For instance, ambulance fleet operations and (distributed computing) datacenter operations will be illustrated as case studies."
"1405174","Modern Statistical Methods for Ecology","DMS","STATISTICS","06/01/2014","04/21/2014","Geof Givens","CO","Colorado State University","Standard Grant","Gabor Szekely","11/30/2015","$10,000.00","Jean Opsomer","geof@lamar.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1269","7556","$0.00","This award supports the participation of junior researchers in the joint 2014 Graybill/ENVR Conference, titled ""Modern Statistical Methods for Ecology"" and co-sponsored by the Department of Statistics at Colorado State University (CSU) and the American Statistical Association Section on Statistics and the Environment (ENVR), occurring  September 7-10, 2014, at CSU in Fort Collins, Colorado.  <br/><br/>Researchers in statistical ecology seek to understand how animals, plants and humans interact with each other and with the environment, using empirical methods that reflect both the evidence and uncertainty inherent in quantitative data. Such research involves both the theoretical development of advanced statistical techniques for data analysis, and the application of such techniques to complex and important real-world datasets. With respect to education, the conference aims to enhance the knowledge base and enthusiasm of graduate students and emerging statisticians, encouraging them to pursue continued study and professional lives in the field of statistics. Scientifically, the continued development of theoretical and applied statistical results in ecology advances the state of knowledge in this discipline, propagates to related statistical research areas, and serves the greater scientific community through collaboration and application. Inclusion of participants from government agencies like NOAA and USGS provides an avenue to immediately impact the scientific foundations supporting national and regional policy making. Many of today's most pressing problems concern human-induced environmental and ecological impacts. Statistical ecology provides the methods and language for interpreting such science and translating it into good policy. By providing a scientific, statistical basis for the study and management of important ecological systems, the field of statistical ecology provides society with insights into what lies ahead for people, communities of all scales, and the world they inhabit.<br/><br/>The meeting includes a short course, keynote and invited talks, and a poster session.  Speaker topics span diverse areas of statistical ecology including: abundance estimation techniques; animal movement models; biodiversity estimation and models; capture-recapture methods; deterministic and stochastic models for ecosystems; distance sampling methods; ecological monitoring; estimation of population trends; individual-based modeling; landscape genomics; multi-scale models; population dynamics modeling; spatial models for ecological problems; species distribution models; and wildlife disease modeling. The goal of the conference is to provide an opportunity for statistical researchers, practitioners and graduate students working in this area to generate and share ideas for new creative research; to receive practical training; to stimulate professional networking opportunities; and to provide young researchers exposure for their work. <br/><br/>Conference website: www.stat.colostate.edu/graybillconference/"
"1431409","Conference Proposal -- StatFest 2013","DMS","STATISTICS","01/01/2014","03/10/2014","Javier Rojo","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Standard Grant","Gabor Szekely","11/30/2014","$21,515.00","","jrojo@iu.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","MPS","1269","7556","$0.00","StaFest conference will be held on October 12, 2013 at Rice University. The StaFest is an annual event partially supported by the American Statistical Association through the Committee on Minorities in Statistics. This year, the International Year of Statistics, this event will be especially significant. The goal of the StatFest is to showcase the work by statisticians in all realms of human endeavors, and by doing so, to encourage undergraduate students -- mostly non-traditional and underrepresented -- to pursue graduate careers in the statistical sciences. In addition, opportunities for graduate fellowships are discussed in the context of opportunities for graduate work in specific institutions. These goals are achieved by gathering a cadre of dynamic speakers who are top researchers in their respective domains (government, industry, academia, etc.) who are able to transmit their enthusiasm for the statistical sciences. In addition, a panel of graduate students discusses more specifically opportunities, challenges, and anecdotes that can be helpful for the soon-to-be graduate students. The conference also offers plenty of one-on-one discussion opportunities between undergraduate students and professional statisticians through communitarian meals (breakfast, lunch, and two coffee breaks) and, for the first time in the history of StatFests, an undergraduate research poster session at the end of the day. Students that will be recruited from across Texas. Traditionally the StatFest has recruited students from institutions in the host city. But this year, due to the International Year of Statistics, it is envisioned that the event will have more of a regional impact rather than a local one.<br/> <br/>The conference will assemble a group of top professional statisticians from academia, industry and government to showcase their activities with the purpose of enticing undergraduate students to pursue graduate careers in the statistical sciences. Students, mostly from underrepresented groups in the statistical sciences, engage in conversations about, and learn about cutting edge problems and solutions in, various realms of scientific inquiry, and most importantly, students come to appreciate the fundamental role that the statistical sciences play in supporting and guiding the fundamental scientific research and in the process get interested in pursuing graduate careers. The StatFest continues to have a major positive impact on the decisions of undergraduate students regarding graduate work. As a substantial percentage of those students attending the StatFest are from underrepresented groups in the statistical sciences, the StatFest -- clearly and compellingly-- provides a launching platform for many underrepresented students. These efforts will have an impact in helping to alleviate the well-documented national critical need to train more students from underrepresented groups in postgraduate careers in the mathematical sciences."
"1406563","Advances for Bayesian Model Selection and Inference","DMS","STATISTICS","08/01/2014","07/23/2014","Edward George","PA","University of Pennsylvania","Standard Grant","Gabor Szekely","07/31/2018","$450,000.00","","edgeorge@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","","$0.00","This project will develop new approaches that will unleash the potential of Bayesian methods for extracting meaningful structure in the ""big data"" that are of increasing interest and importance today.  This will entail the creation of rapidly computable methods that enhance or bypass the much slower simulation methods upon which Bayesian methods have come to rely.   By more efficiently harnessing the power of high-performance computing, our methods will provide a feasible vehicle for uncovering systematic associations in data across a variety of scientific disciplines. Examples include (a) genomics, where identifying the genetic determinants of diseases can substantially enhance targeted therapeutic decisions, (b) neuroimaging, where efficient methods will greatly facilitate understanding the complex brain architecture, and (c) environmental sciences, where identifying the determinants of climate change can better inform policy decisions. <br/><br/> <br/>The main thrust of this research will be the development of a broad framework for fast deterministic search implementations for Bayesian model selection and inference.   Building on new directions recently opened up by EMVS, a fast new alternative to stochastic search, this work will vastly increase the feasibility of general Bayesian implementations for large high-dimensional problems. This will entail the development of algorithms for classes of models that go beyond the canonical normal linear model. The speed of these algorithms will enable the broad application of new dynamic posterior exploration. Properties of the implicit selective shrinkage selection will be studied and asymptotic theory developed. New approaches to posterior reconstruction will provide new avenues for inference.  Effectively, this project will lay the groundwork for a new paradigm for posterior exploration in large high-dimensional problems."
"1519890","CAREER:  New Directions in Spatial Statistics","DMS","STATISTICS, Division Co-Funding: CAREER","09/01/2014","07/03/2017","Debashis Mondal","OR","Oregon State University","Continuing Grant","Gabor Szekely","06/30/2019","$351,845.00","","mondal@wustl.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","MPS","1269, 8048","1045, 1187","$0.00","The  de Wijs process (also known as the  Gaussian free field in statistical physics) is a fundamental spatial process that arises as the scaling limit of lattice based Gaussian Markov random fields and generalizes Brownian motion in two-dimensions. However, at present,  there is a wide gap between the theory of Gaussian free field (including the subsequent theory of random fields) in statistical physics and modern probability, and the current practice of spatial statistics via lattice based Gaussian Markov random fields. Thus, there is great need to bridge this gap to develop a principled framework for statistics and inference of spatial models  and to pursue novel computations that make such inferences feasible.  This  project will consider formulating appropriate functionals of the de Wijs process to construct useful random fields and novel matrix-free computations via conjugate gradient and other methods, and will focus on developing new areas of scientific applications. The proposed research will also shed new light on and allow deeper understanding of theoretical and computational issues discussed by many researchers in spatial statistics in the past decades.  Novel matrix-free computations  will provide further impetus to study parametric bootstrap methods and multi-scale modeling, and to construct a new class of non-Gaussian random fields.  The project will contribute  to obtaining enhanced scientific understanding in studies of environmental bioassays, arsenic contamination of groundwater and distributions of galaxies. <br/><br/>Advances in the field of spatial statistics are important because new statistical methods can be applied to a wide range of scientific questions in fields such as astronomy, agriculture, biomedical imaging, computer vision, climate and environmental studies, epidemiology and geology. The de Wijs process is one fundamental spatial process that generalizes Brownian motion from time to space.  Using the de Wijs process as a fundamental building block, this project will develop novel mathematics and derive fast, efficient and large-scale statistical computations so that various scientific questions can be answered in a practical way. This will lead to new developments  for the analysis of continuum spatial data  and spatial point patterns, and will allow us to obtain enhanced scientific understanding in studies of environmental bioassays, arsenic contamination of groundwater and distributions of galaxies.  The statistics and the computations that will be developed in this project will also be particularly relevant for various research problems that arise in environmental or global change, and in health studies. Finally, this project will integrate research and educational activities through the development of new graduate and undergraduate courses and will also provide valuable training and learning opportunities for students at graduate and undergraduate levels."

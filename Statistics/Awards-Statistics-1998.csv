"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"9730906","International Biometric Conference to be held December      13-18, 1998, in Cape Town, South Africa","DMS","STATISTICS","03/15/1998","03/13/1998","Lynne Billard","GA","University of Georgia Research Foundation Inc","Standard Grant","Joseph M. Rosenblatt","02/28/1999","$10,000.00","","lynne@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","PI: Lynne Billard<br/>Proposal Number: DMS 9730906<br/>Project: International Biometric Conference - Cape Town<br/><br/>Abstract: <br/><br/><br/>The International Biometric Conference is the biennial meeting of the<br/>International Biometric Society, a Society dedicated to the ""advancement<br/>of biological science through the development of quantitative theories and<br/>the application, development and dissemination of effective mathematical<br/>and statistical techniques"". Members and conference participants are<br/>concerned with the cross-disciplinary and interdisciplinary interface of<br/>the theoretical and application oriented real-world problems. This<br/>proposal requests partial support for ten U.S.-based individuals from<br/>academic and nonprofit institutions to participate in the International<br/>Biometric Conference to be held in Cape Town, South Africa, on December<br/>13-18,1998.  About one third of those supported will be those with an <br/>official capacity and two-thirds will be new researchers especially those<br/>traditionally underrepresented groups such as women and minorities.<br/>"
"9803684","Crossover Designs and Orthogonal Arrays","DMS","STATISTICS","07/15/1998","08/22/2000","John Stufken","IA","Iowa State University","Standard Grant","Joseph M. Rosenblatt","06/30/2001","$50,774.00","","jstufken@gmu.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","0000, OTHR","$0.00","-----------------------------------------------------------------------  Proposal Number: DMS 9803684  PI:              John Stufken  Institution:     Iowa State University  Project:         Crossover Designs and Orthogonal Arrays    Abstract:    This research furthers the understanding of two of the most useful  types of statistical designs: crossover designs and orthogonal  arrays. In a crossover design each of several subjects receives  multiple treatments, one at each of several time periods. The design  problem consists of selecting the sequences of treatments to be used,  and of deciding how often to use each of these sequences. While  practical considerations play a major role, answers based on  statistical considerations differ depending on the assumptions about  the statistical model for the measurements. This research involves  finding efficient practical designs under various realistic model  assumptions. We use these results to identify designs with good  properties for most models. Concerning orthogonal arrays, this  research provides new methods for the construction of mixed  orthogonal arrays, as well as an extensive library of such arrays.  In addition, compound orthogonal arrays are studied for their  properties when used in a factorial experiment to detect location   and dispersion effects simultaneously.    Designed experiments are an integral part of scientific research  and of product development and improvement in industry. Crossover  designs and orthogonal arrays play a particularly prominent role.  Crossover designs are best known for their role in medical research  and in clinical trials, where they are used in studies to compare  two or more treatments, possibly including a placebo. Orthogonal  arrays play a crucial role in quality improvement and assessment  studies in all of manufacturing. They are, among others, used in  studies to determine the possible effect that changes in a  manufacturing process have on the quality of the final product.  This research helps to  improve the efficiency of such trials  and studies by identifying better designs and by making such  designs widely available."
"9803720","Statistical Analysis of Longitudinal Studies and Surveys    with Missing Values","DMS","STATISTICS, Methodology, Measuremt & Stats","08/15/1998","07/05/2000","Roderick J.A. Little","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Joseph M. Rosenblatt","07/31/2001","$216,119.00","Trivellore Raghunathan","rlittle@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269, 1333","0000, OTHR","$0.00","-----------------------------------------------------------------------  Proposal Number: DMS 9803720  PI:              Roderick Little  Institution:     University of Michigan  Project:         Statisitcal Analysis of Longitudinal Studies and                    Surveys with Missing Values    Abstract:    Many empirical studies have problems with missing data, where subjects   with missing values differ systematically from subjects with complete   data. Discarding the incomplete cases leads to bias and loss of   efficiency. Model-based statistical methods allow for efficient and   valid inferences based on all the data, but require careful attention  to correct modeling of the data and the missing data mechanism. This  research will develop new Bayesian and maximum likelihood methods for  handling incomplete data in the following problems:  Longitudinal data  where study subjects are measured repeatedly over time, and subjects  are missing at certain times, or drop out of the study prematurely;  and survey data from complex survey designs where subjects have  differential probabilities of selection and nonresponse.     Results from many research studies are hard to interpret because of   missing data. In an opinion poll some of the randomly chosen subjects   refuse to answer some of the questions, and results based on the   respondents may not be representative of the population. In clinical   trials some patients may drop out of the study because they cannot   tolerate a particular treatment, or simply move to a different  location and cannot be traced. Statistical methods for analyzing data  with missing values use partial information on nonrespondents to  provide more precise and more accurate answers than can be obtained  by analyzing only the complete cases."
"9805117","Group Travel Award to Support U.S Participation in Meeting  of International Federation of Classification Societies,    Rome, Italy, July 1998","DMS","STATISTICS","09/15/1998","09/11/1998","Peter Bryant","CO","University of Colorado at Denver-Downtown Campus","Standard Grant","Joseph M. Rosenblatt","08/31/1999","$4,246.00","David Banks, F. James Rohlf","","1380 LAWRENCE ST STE 300","DENVER","CO","802042055","3037240090","MPS","1269","0000, OTHR","$0.00","The 6th conference of the International Federation of Classification<br/>Societies (IFCS) will be held in Rome, Italy, July 21-24, 1998. <br/>IFCS consists of the classification societies of North America,<br/>Great Britain, Italy, France, Germany, Belgium/Netherlands, Portugal,<br/>Poland, Japan, and Korea. While the various member societies differ<br/>in emphasis and philosophies, they share a common interest in problems<br/>of classification, cluster analysis, and data analysis in such areas<br/>as biology, library science, statistics, medicine, social science, <br/>psychology and philosophy. <br/><br/>This proposal will provide grants to help defray the cost of travel <br/>for approximately 5 U. S. citizens or permanent residents to <br/>participate in this conference. Grants under this proposal will be<br/>limited to the approximate cost of airfare. This support will encourage<br/>those who might not otherwise be able to attend IFCS 98, junior faculty<br/>in particular, to become part of the international community of<br/>researchers in the fields of classification, clustering, and data<br/>analysis, and to become aware of the many alternative philosophies,<br/>uses of, and approaches to data analysis and classification-related<br/>problems in particular. <br/>"
"9803273","Messy Data Modeling and Related Topics","DMS","STATISTICS","07/15/1998","07/10/1998","Minge Xie","NJ","Rutgers University New Brunswick","Standard Grant","John Stufken","06/30/2002","$46,786.00","","mxie@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","-----------------------------------------------------------------------  Proposal Number: DMS 9803273  PI:              Minge Xie  Institution:     Rutgers University  Project:         Messy Data Modeling and Related Topics    Abstract:    The main objective of this research is to investigate a number of   problems arising from dealing with messy data sets that occurred in   many disciplines of science. The data studied in this research violate   conventional assumptions, such as independence, homogeneity, among   others, which are otherwise adopted under more standard settings.   Six specific topics are presented; each corresponds to at least one   violation of conventional assumptions, and a messy data set may have   one or more of these types of violations. According to their origins   from two different motivating data sets, two of the six topics address   problems on group testing scheme (Dorfman, 1943) and its variants,   including issues on modeling false negatives and relaxing the   assumption of independence on individuals. The rest of the topics   investigate practical and theoretical issues related to modeling   batch correlated regression data and develop new models and methods   for heterogeneous observations. These developments will not only   solve the specific type of problems, but also stimulate new   researches to develop more general methodologies.    This research develops statistical methodologies, models, and related   theories to address issues arising from modeling and analysis of messy   data, which can be found in many disciplines of the sciences, including  life science, environmental science, social science, industry and   economics. A common feature of these messy data is that they   all violate some conventional model assumptions, which otherwise are   adopted under more standard settings. Accurate modeling of messy data   can eliminate irrelevant information and provide better understanding   of underlying mechanisms; ultimately benefiting prediction and decision   making. Although tremendous progress has been made in development of   both sophisticated statistical methodologies and elegant mathematical   theories in the past half century, many important problems in modeling   and analysis of messy data have yet to be tackled, both from practical   and theoretical viewpoints. This research investigates several such   problems. Although the models and methodologies are tailored to   specific problems in the Pharmaceutical industry and environmental   science, these developments will not only solve the specific type of   problems, but also stimulate new researches to develop more general   methodologies."
"9803226","3D Scanning: Acquiring and Modeling Surface Properties","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS, ADVANCED COMP RESEARCH PROGRAM","08/15/1998","06/08/2000","Werner Stuetzle","WA","University of Washington","Continuing Grant","John Stufken","07/31/2002","$406,473.00","Thomas Duchamp, Brian Curless","wxs@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269, 1271, 4080","0000, 9146, 9216, 9263, HPCC, MANU, OTHR","$0.00","-----------------------------------------------------------------------  Proposal Number: DMS 9803226  PI:              Werner Stuetzle, Brian Curless, Tom Duchamp  Institution:     University of Washington  Project:         3D Scanning: Acquiring and Modeling Surface Properties    Abstract:    3D scanning is the inverse of computer aided manufacturing: given a   physical object, such as a clay model of a car, a sculpture, a chair,   or a house, create an electronic representation capturing its shape  and appearance.  3D scanning is similar in principle to a number of  other important technologies (like digital audio, digital photography,  document scanning, and digital video) that quickly, accurately, and  cheaply record useful aspects of physical reality.  These technologies  have had an enormous impact, primarily because electronic  representations can be used in ways the original physical objects  cannot. For example, they can be stored in, searched for, and  retrieved from databases, transmitted electronically over long  distances, manipulated and edited in software, and used as templates  for making electronic or physical copies.  Research in 3D scanning  promises to have a similar impact on a number of areas, including  the Federal Strategic Areas of high performance communication, civil  infrastructure, and manufacturing, as well as virtual museums,  entertainment, and product marketing.    This research group and others have made significant progress in  capturing, modeling, and displaying geometric shape.  The task of   acquiring and computing surface properties such as appearance,   however, has received far less attention.  The research effort in this   project focuses on three aspects of the problem: (i) texturing, bump   mapping and selective refinement of multiresolution meshes, (ii)   capturing, modeling and displaying radiance (color as a function of   position and direction), and (iii) generally estimating functions over   surfaces."
"9725031","International Meeting on Statistical Climatology","DMS","STATISTICS, Climate & Large-Scale Dynamics","04/01/1998","03/27/1998","Peter Guttorp","WA","University of Washington","Standard Grant","Joseph M. Rosenblatt","03/31/1999","$17,000.00","","guttorp@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269, 5740","0000, 1303, EGCH, OTHR","$0.00","PI: Petter Guttorp  Proposal Number: DMS 9725031  Project: International Conference on Statistical Climatology    Abstract:     This International Conference on Statistical Climatology, the sixth   that has been held since 1979, will be held from May 25 to 29, 1998,  in British Columbia. This series of international conferences has   brought together statisticians and climatologists for both formal   and informal discussions of problems and statistical methods for  addressing them.  There are two invited lectures per day, each   with discussions by both statisticians and climatologists.    This format promotes cross-disciplinary interactions and  collaborations.  Many of the leading issues in climatology are   related to general circulation models.  The major problem of interest  is in comparing model runs to historical data, which have varying   spatial and temporal coverage, varying instrumentation, and very   different precision.  These models are used to assess the effect of  various anthropogenic influences on the climate, such as changing   composition of various greenhouse gasses.  Important topics to be  discussed at this conference are the statistical techniques such as  hiden Markov models, wavelet methods, and neural networks that can   be used in developing useful models.    This award is in support of expenses of participants from the United  States. A large portion of the grant will go towards expenses of   graduate students, new researchers, and members of underrepresented  groups."
"9802885","Inference For High Dimensional Models","DMS","STATISTICS","08/01/1998","07/16/1998","Susan Murphy","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Joseph M. Rosenblatt","07/31/2001","$35,000.00","","samurphy@fas.harvard.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","9802885<br/>Susan A Murphy<br/><br/>This research concerns estimation in high dimensional models.  In the first part the use of high dimensional models   for incomplete data is addressed. Suppose the pattern of incompleteness is related to an high dimensional vector of observations.  Then the dimensionality of the vector precludes straightforward maximum likelihood estimation.  The research addresses two modifications of maximum likelihood.  In the first modification, the dimension of the vector is reduced by a balancing score and in the second modification, a weighted likelihood is used.  The second part of this research focuses on  maximum likelihood estimation for the transformation model, a high dimensional generalization of the linear regression model.  This model stipulates that an unknown transformation of the response follows a linear regression.  In special cases, such as the proportional hazards model, the estimators found by maximum likelihood are well understood.  Yet in general, estimators of parameters in this model, although extremely popular, are rather difficult to analyze.  This research investigates the use of  new techniques, such as empirical processes and  empirical likelihood in order to understand the estimators.<br/><br/>Models in which there are many unknowns or unknown functions (called parameters here) appear throughout the sciences.  Often these models are formulated to address inadequacies of the classical linear regression model.  For example, social scientists use high dimensional models in event history analysis of the causes of variability in the timing of life events such as premarital births, initiation of drug abuse, timing of retirement and duration of poverty spells.  High dimensional models attempt to allow the data to speak for itself and to minimize the addition of spurious information caused by imposing a low-dimensional model on the data.  This research advances the understanding of these high dimensional models. Often high dimensional models are applied without any theoretical understanding of when they may or may not produce quality estimators.  In particular, this research investigates the bias and variability of of estimators found by the use of a common estimation method, maximum likelihood estimation.  This is important as understanding the causes of variability in the timing of life events is crucial to designing appropriate social policies and intervention/prevention programs.<br/>"
"9803281","Statistical Methods for Process Control and Improvement in  Advanced Manufacturing","DMS","STATISTICS","09/01/1998","07/13/2000","Vijayan Nair","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","John Stufken","08/31/2002","$209,017.00","","vnn@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","Proposal Number: DMS 9803281    PI: Vijayan N. Nair, University of Michigan      Mark H. Hansen, Bell Labs, Lucent Technologies    Project:  Statistical Methods for Process Control and Improvement            in Advanced Manufacturing    Abstract:    This research project deals with methods for modeling, monitoring,   diagnosis, and improvement of manufacturing processes with spatial   data.  The scope of the project goes beyond the traditional Shewhart's  paradigm for statistical process control (SPC) which focuses  primarily on process monitoring.  A significant part of the research is  the use of in-process and product quality data to develop failure   diagnostics and to relate these to potential problems for process  improvement. These issues are studied in the context of an integrated  framework for process control and improvement. An overall strategy is  proposed for using the spatial information in defect clustering as the  basis for process improvement.  The methodology consists of several   parts. First, process monitoring methods for routinely monitoring the  spatial data and detecting objects with significant clustering are  developed. A Markov random field model with small-scale clustering is  used to characterize ``in-control'' data. Statistical methods for  failure diagnosis (signatures of spatial patterns) are obtained and  the patterns are then related to process information for improvement.  Various approaches for doing this including parametric models for   large-scale clustering and formal methods based on classification are  studied. Several other related topics, including modeling and analysis  of ordinal data from temporal and spatial processes and the analysis  of spatial data from designed experiments are also studied. These  methods are developed and studied in the specific context of wafer  map data in integrated circuit (IC) fabrication.    Semiconductor manufacturing is one of the key manufacturing   industries in the US, and hence statistical methods for process and    yield improvement are clearly important  from a practical viewpoint. However, the research issues and  methods developed here are quite generic in nature and are of general   interest to many other manufacturing processes with spatial data,   including flat panel displays, printed circuit boards, and the   manufacture and assembly of auto-bodies. These advanced manufacturing  and high-technology industries all share the following features.   Massive amounts of in-process and production data are now being   collected routinely, made possible by advances in computing and data  capture technologies. Much of these data have complex structures,   in the form of spatial objects, images and so on. At the same time,  competitive market pressures are placing a lot of emphasis on reducing  product development cycle time. Moreover, process/product designers  are operating on the boundaries of available subject matter knowledge  of the underlying technology.  Products are being manufactured and  marketed before the technology is well-understood. These manufacturing  processes are often not ``stable'', as is commonly assumed in the  traditional statistical process control (SPC) paradigm. Thus, there is  a critical need for statistical methods that exploit the extensive  information available from in-process and product quality data not  only for process monitoring but also for process improvement. The  results from this research project will significantly advance  methodology for the continuous improvement of advanced manufacturing  processes."
"9870193","High Dimensional Statistical Problems:  Theory and Methods","DMS","STATISTICS","08/01/1998","09/12/2000","Bruce Lindsay","PA","Pennsylvania State Univ University Park","Continuing Grant","John Stufken","07/31/2002","$278,307.00","","bgl@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, OTHR","$0.00","DMS-9870193<br/>Lindsay<br/><br/>The research will be directed toward the broad theme of recovering statistical information about variation of interest in the presence of confounding variation in high dimensions.  The initial work involves developing new tools for the methodology of Generalized Estimating Equations (GEE) based on two new ideas:  first, the investigator will develop a maximum information approach to the estimation of nuisance correlation parameters in GEE,  and second, he will develop a quadratic inference methodology as an alternative to the point estimator/standard error approach.  The first enhancement has been found to increase efficiency under covariance misspecification.  The second allows one to develop ANOVA like model selection techniques for the estimating equation framework while automatically meeting the maximum information criterion.  These new tools will then be used in a wider range of challenging applications involving high dimensional correlated data.  An additional theme of the research is the development of more widely consistent nonparametric estimators for situations in which maximum likelihood fails.<br/><br/>Modern statistics is faced with an explosion of increasing complex and sophisticated scientific data.  One of the key features of such data is that it is high-dimensional.  It is also characterized by having high degrees of interdependence between observations.  The goal of the research is to enhance our ability to separate the effects of this interdependence from the features of the data that we are most interested in.  The initial focus of this research is on a popular method of dealing with longitudinal data, an example of which would be response variables that are measured repeatedly over time on a group of patients.  The structure of the interdependence of observations within a patient is then a nuisance feature which we must adapt to if we wish to learn how the response variables are affected by explanatory variables.  New methods will be developed that are more efficient and reliable for this setting, and they will then be extended to other problems with correlated structures.<br/>"
"9714380","Statistical Methods in Computational Modeling and Virtual   Experiments","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS, THEORY OF COMPUTING","06/01/1998","09/28/1999","John Tucker","DC","National Academy of Sciences","Standard Grant","Joseph M. Rosenblatt","11/30/2000","$22,000.00","","JTucker@NAS.edu","2101 CONSTITUTION AVE NW","WASHINGTON","DC","204180007","2023342254","MPS","1269, 1271, 2860","0000, 9216, 9263, HPCC, OTHR","$0.00","PI: John Tucker  Proposal Number: DMS 9714380  Project: Statistical Methods in Computational Modeling and            Virtual Experiments    Abstract:    The Committee on Applied and Theoretical Statistics (CATS)  will convene a multidisciplinary study panel on statistical  methods in computational modeling and virtual experiments.  The panel will recommend sound methods for identifying and   quantifying uncertainties that arise in simulation experiments.  The panel will develop recommendations for improved and, where  necessary, new statistical methods and data analysis techniques  for use in simulation modeling and virtual experiments.  It will  also recommend appropriate statistical methods to assist  practitioners in their use of experimental data when validating  simulation models or assessing model fit and predictive   performance. These recommendations will address modeling and  simulation limitations associated with ever increasingly complex  systems. The panel's report will provide examples to illustrate  the complexity of evaluating model combinations that arise from  varying assumptions, parameters, and initial conditions within   the models, will point out possible approaches for addressing  those complexities from existing experimental design techniques,  and will highlight areas needing further research. Experts may   be consulted from the communities concerned with computer  simulation, discrete events systems, and dynamic systems  modeling, and any other scientists, researchers, or engineers  with significant technology or methodology to contribute.    A National Research Council panel of experts will examine the   state-of-the-art for statistical methods used in computer  simulation and modeling, and computational experimentation. This  is needed because of an ever-growing dependence upon these kinds  of simulations and ""virtual"" experiments in designing and  evaluating such complicated devices and systems as semiconductors,  automobiles, aircraft, spacecraft, or in very complex  application  areas such as transportation networks, government intelligence, and  battlefield scenarios. The study will address how modeling and  simulation tools can be developed with the robustness and  reliability to handle the demands of emerging intelligence,  domestic, and military technology. It will also look at limitations  on modeling and simulation with respect to these increasingly   complex systems."
"9803622","Diagnostics for Structured Data and Quality Improvement","DMS","STATISTICS","08/15/1998","08/01/2000","Douglas Hawkins","MN","University of Minnesota-Twin Cities","Continuing Grant","John Stufken","07/31/2002","$92,130.00","","doug@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, 9147, 9216, HPCC, MANU, OTHR","$0.00","9803622<br/>Douglas M. Hawkins<br/><br/>Structured data sets are those in which the data are other than independent identically distributed scalar quantities.  Examples are multiple regression and multivariate data sets, and time series.  Diagnostics involves identifying cases that depart from some baseline  (usually Gaussian) model.  This general framework covers finding outliers as a part of data analysis, and is also the basis for statistical process control (SPC) methodologies.  One thread of the present project extends the PI's previous work in outlier identification, particularly in situations where the outliers are numerous or badly placed.  It has recently become apparent that methods that work well on text-book-sized problems are useless in data sets with even a few thousand cases in a few dozen dimensions.  As such data sets are increasingly common, this has reopened a major emphasis of the present project -- the whole question of workable approaches for finding outlying cases in large data sets.  A somewhat distinct problem is detection of persistent changes in time-ordered scalar and multivariate data.  This is the problem addressed by change-point, exponentially weighted moving average, and cumulative sum methodologies.  The program of work includes a major effort in this area also.  The union of the two problem areas leads to the design of statistical process control methodologies that are resistant to isolated outliers.<br/><br/>In large data bases it is impossible to verify the correctness or internal consistency of the entries using current methodology.  Methods that work well on small data sets are computationally unthinkable in data sets up in the megabyte and beyond range leaving the quality of information in data bases hostage to undetected errors.  This project is developing methods to identify ""outliers"" --- atypical entries in large data bases --- with an acceptable, though still large, amount of computational effort.  More processor power alone will not solve the problem, but more powerful processors combined with the improved algorithms developed in this program of work may do so.  The problem is inherently amenable to distributed processing --- previous work showed how outlier identification could be speeded up by using an array of central processors.  Another thread of the work is cumulative sum (cusum) charting, a tool in the statistical process control (SPC) family.  The classic Shewhart Xbar and R control charts are incapable of detecting small but persistent shifts.  Such shifts are found and diagnosed rapidly with cusums.  Used in conjunction with Shewhart charts, cusums can diagnose manufacturing problems, leading to substantial quality improvement.  Cusums are also effective in many other monitoring situations, from online medical monitoring to detecting plumes of pollution in air or water.  Groundwater monitoring around a landfill, for example, aims at exactly this problem of detecting an increased level of pollutants against a highly variable background.  Cusums are already recognized as a powerful tool to use in the detection and diagnosis of leakages, and their extension to handle non-detect chemical data is important in extending their applicability to pollutants like heavy metals that are harmful at low concentrations.<br/>"
"9803596","Investigation and Construction of Statistical Designs for   Multifactorial Experiments","DMS","STATISTICS","07/15/1998","07/17/1998","Sam Hedayat","IL","University of Illinois at Chicago","Standard Grant","Marianthi Markatou","06/30/2002","$88,688.00","","hedayat@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","0000, OTHR","$0.00","-----------------------------------------------------------------------  Proposal Number: DMS 9803596  PI:              Samad Hedayat  Institution:     University of Illinois at Chicago  Project:         Investigation and Construction of Statistical Designs                   for Multifactorial Experiments    Abstract:    The investigator studies statistical problems in the area of design  and analyses of scientific and industrial experiments in which the  outcomes are influenced by multiple factors and noise. The goal will  be to identify the best factor combinations while minimizing and  measuring the noise. The research effort is mainly concentrated in  the construction of designs capable of investigating location and  dispersion effects, compound orthogonal arrays with high strength,  g-designs as core sub-designs ,and orthogonal arrays of strength t  plus.    The success of any serious scientific study depends on the way it is  designed and the related data being collected and analyzed. Noise is  the biggest nuisance while the goal is to relate the outcomes to the  levels of the factors influencing the study. This is true in almost  all social, engineering, environmental, and medical studies. It is  costly and indeed unwise to eliminate noise entirely. Indeed in many  studies it will be enough if one knows the source and the intensity  of noise. Under the latter condition it is possible to explore how the  surrounding factors are impacting the outcomes. The investigator  explores all these interconnected topics and identifies new cost- and  time-effective and improved statistical designs capable of handling  these highly complicated scientific issues."
"9803645","Flexible Statistical Modeling","DMS","STATISTICS","07/15/1998","07/14/2000","Trevor Hastie","CA","Stanford University","Continuing Grant","John Stufken","06/30/2002","$199,074.00","","hastie@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","DMS-9803645<br/>Hastie<br/><br/>There have been significant developments in the areas of applied regression and classification over the past 10-15 years.  Much of the impetus originally came from outside of the field of statistics, from areas such as computer science, machine learning and neural networks.  These disciplines have brought many fresh ideas to the table, a host of new and exciting models such as neural networks, as well as many interesting areas of application.  As the dust settles, we find that these new ideas are best synthesized within a statistical framework, and have a natural place alongside traditional linear and nonlinear models.  A key item in this research program is a research monograph with working title: THE ELEMENTS OF STATISTICAL LEARNING (with Jerome Friedman and Rob Tibshirani).  This book develops a framework for describing and understanding the new regression and classification techniques from a statistical point of view, and for synthesizing them with existing methods.  We strike a natural balance between the classical well tested linear and parametric models, and the more exotic and adaptive techniques appropriate in data rich scenarios.  The research program includes the development of some new techniques for multiclass classification, each of which expand on existing techniques in novel ways.<br/><br/>Many important problems in data analysis and modeling focus on prediction: computer assisted diagnosis of disease (e.g. reading digital mammograms), heart disease risk assessment, automatic reading of handwritten digits (e.g. zip-codes on envelopes), speech recognition, to name a few.  This research program has two arms.  The first is a monograph that synthesizes from the many varied contributions a collection of well-tested techniques, and explains them from a statistical point of view.  The second arm is to develop some new techniques for prediction.  All these new methods exploit the rapid computing facilities we have available, and allow us to develop methods for prediction that would have been infeasible ten years ago.<br/>"
"9802261","Bayesian Analysis, Decision Theory, and Applications","DMS","STATISTICS","07/01/1998","01/24/2000","James Berger","NC","Duke University","Continuing Grant","John Stufken","06/30/2002","$374,471.00","","berger@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, 1308, EGCH, OTHR","$0.00","James O. Berger<br/>DMS-9802261<br/><br/>Five research areas in Bayesian analysis and statistical decision theory are addressed.  The greatest effort is in model selection, in particular the development of automatic Bayesian methodology for choosing between models.  This is based on construction of automatic prior distributions.  Other problems in model selection that are considered include the question of optimal choice of a model when prediction is the goal; study and comparison of large sample approximations in Bayesian model selection; and development of Bayesian measures of criticism of a single proposed model.  The second area of research is conditional frequentist testing, and its unification with Bayesian testing, especially as applied to clinical trials.  Using a modification of the approach to developing automatic priors for model selection that was mentioned above, the mixture modelling problem is also addressed.  Quite general mixture models, involving populations with differing (and unknown) orientation and measurement errors are considered.  A fourth area of research is the development of good prior distributions for covariance matrices and hierarchical models, in part through the concept of decision-theoretic admissibility.  The final area of research is Bayesian analysis involving large data sets.<br/><br/>The advances in model selection methodology are used to study global change problems such as detection of climatic changes due to causes such as greenhouse warming.  The developments in mixture modeling are applied to astronomical problems, in particular the problem of modeling the source of gamma ray bursts.  The work on covariance matrices is used to study detection of patterns, such as El Nino, in climatological fields.  The investigations in Bayesian analysis of large data sets span these problems, finding general tools for analyzing such data sets.  The research benefits education and human development through the training of graduate studentsm and the incorporation of the developed methodology in statistics courses at Duke University and elsewhere.<br/>"
"9803756","Bayes and Empirical Bayes Model Selection","DMS","STATISTICS","09/01/1998","06/01/2000","Edward George","TX","University of Texas at Austin","Continuing Grant","Joseph M. Rosenblatt","08/31/2001","$87,221.00","","edgeorge@wharton.upenn.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1269","0000, OTHR","$0.00","-----------------------------------------------------------------------  Proposal Number: DMS 9803756  PI:              Edward I. George  Institution:     University of Texas   Project:         Bayes and Empirical Bayes Model Selection    Abstract:    Bayes and empirical Bayes approaches for model selection are studied,   developed and enhanced for a variety of different settings.  First of  all, theoretical frequentist risk properties of recently discovered  empirical Bayes selection criteria are established for the canonical  linear model setting.   For graphical model selection, connections  between frequentist and Bayesian methods are investigated and used  to motivate new empirical Bayes methods.  For wavelet representations,  robust empirical Bayes selection procedures are developed which  accommodate heavy tailed noise distributions.  For Bayesian CART  modeling, new families of structured hierarchical priors are developed.  To facilitate and enhance model search, new MCMC algorithms are  developed which move across sets of models rather than single models.  These algorithms use new transition kernel steps, such as  transplantation, which can rapidly traverse the kinds of multimodal   model posterior distributions that arise in this context.  These  algorithms also incorporate stochastic heating and cooling to further  increase movement.    This research ultimately concerns the development of statistical  methods for discovering  systematic structure in large multi-variable  data sets. This general problem is of substantial importance because  of the explosive growth in the technology to collect and analyze such  data.  Indeed, such large data sets occur naturally in federal  strategic areas of national concern including telecommunications,  biotechnology, and climatology.  A standard statistical approach in  such settings is to search for ""promising"" statistical models within  some prespecified, large, flexible class of potential models.  Once  found, the challenge is to estimate the mod el and draw meaningful  inference.  These tasks can be especially difficult when the class of  potential models is huge, as is typically the case the number of  variables is large.  The main thrust of this work will be to develop  new methods to confront these challenges and broaden the scope of the   statistical approach."
"9803143","Likelihood Methods in Statistics","DMS","STATISTICS","07/15/1998","07/16/1998","Thomas Severini","IL","Northwestern University","Standard Grant","John Stufken","06/30/2002","$51,024.00","","severini@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1269","0000, OTHR","$0.00","-----------------------------------------------------------------------  Proposal Number: DMS   PI:              Thomas A.  Severini  Institution:     Northwestern University  Project:         Likelihood Methods in Statistics    Abstract:    This research considers several problems regarding likelihood-based  statistical inference.  These problems naturally fall into two areas.  The first area is inference based on the likelihood ratio statistic.   The research considers the development of an adjustment to the   square-root likelihood ratio statistic that improves the accuracy of  the usual normal approximation and is easily calculated in a wide  range of models.  This leads to improved methods of inference in  models where the accuracy of first-order asymptotic approximations is  questionable. The research also studies analogues of the likelihood  ratio statistic that are based on a quasi-likelihood function. The  second area of research is statistical prediction analysis.   The  specific problems being considered include  the development of   general methods of prediction based on predictive pivots and  conditional inference, and the analysis and comparison of predictive  likelihood methods, including the predictive density based on a  prior distribution.      This research is concerned with the development of accurate and   useful methods for drawing conclusions and making predictions based  on observational data. Many statistical methods are based on  approximations; in many cases, the accuracy of these approximations  is questionable.  One aspect of this research is the development of  more accurate and reliable approximations of this type. Statistical  prediction analysis is concerned with the problem of predicting  future events based on currently available data.  Predictive methods  have applications in a wide array of fields, including medical  diagnosis, environment and global change, manufacturing, and   economic forecasting. This research develops general methods of  constructing pre dictions and  studies the relationships among the  various methods of prediction."
"9802522","Estimation and Computation for Multivariate Classification    and Mixture Problems","DMS","STATISTICS","08/01/1998","01/24/2000","Bernard Flury","IN","Indiana University","Standard Grant","Joseph M. Rosenblatt","07/31/2001","$64,205.00","","","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","MPS","1269","0000, OTHR","$0.00","9802522<br/>Bernhard Flury<br/><br/>This research focuses on methods of classification in multivariate statistics, studying theoretical, practical, and computational aspects.  The investigator explores finite mixture models with an ""improper"" component, i.e., a component in which each data point has the same density value.  This leads to a flexible class of estimators that depend on the setting of a tuning parameter.  For the extreme values of the tuning parameter, the resulting methods of estimation correspond to fully parametric maximum likelihood, and to nonparametric likelihood (empirical distribution function), respectively.  The estimators are useful for contaminated data, and their performance is compared to traditional robust estimators.  The main computational tool is an application of the EM algorithm.  In another problem related to finite mixture analysis the investigator studies the question of dimensionality: if interest focuses on a subset of variables measured, should one use only that particular subsetfor the purpose of estimating the parameters of the mixture, or should one use the remaining variables (""covariates"") as well?  In addition, this research develops the asymptotic distribution theory for maximum likelihood estimators in multivariate models that are usually regarded as untractable by conventional methods (common canonical variates, partial common principal components, the discrimination subspace model, and others), and investigates iterative computational methods needed for estimating their parameters.<br/><br/>Methods of classification play an increasingly important role in areas such as remote sensing, pattern and speech recognition, and taxonomy.  The investigator studies methods of estimation and computation in multivariate situations, i.e., when many variables are measured on the same objects. In particular, the finite mixture model used in this research allows us to improve statistical methodology in situations where the data is distorted by errors and outliers.  Efficient computational methods developed in this research allow us to exploit this powerful methodology, and to make it applicable to problems in many areas, including biotechnoloy and environmental sciences.  Further research topics involve models of allometric growth in biology, improved estimation in unsupervised methods of classification, the development of efficient computational algorithms for recentlycreated multivariate methods of data analysis, and the analysis of periodic phenomena in biology.<br/>"
"9803649","New Monte Carlo Methods for Scientific and Statistical      Computing","DMS","STATISTICS","09/01/1998","12/14/1999","Jun Liu","CA","Stanford University","Continuing Grant","William B. Smith","05/31/2001","$142,255.00","","jliu@stat.harvard.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","-----------------------------------------------------------------------  Proposal Number: DMS 9803649  PI:              Jun S Liu  Institution:     Stanford  University  Project:         New Monte Carlo Methods for Scientific                     and Statistical Computing    Abstract:    Monte Carlo methods have increasingly been recognized by scientists   as indispensable tools  for difficult computational problems. Its   applications  include biochemistry, computational biology, computer  science, engineering, finance,  physics, seismology, traditional   statistics, etc.  The primary objective of this research, as motivated  by various applications encountered by the PI, is on designing  and  analyzing new Markov chain Monte Carlo (MCMC) methods. The  research  plan includes three related  projects, all concentrating on the MCMC   methodology. The first project proposes two new hybrid Monte Carlo   methods. These methods  combine a gradient-type move, or a conjugate   direction-type move, with Metropolis algorithm. The new methods aim at   alleviating slow-mixing behavior (or ``stickiness'') observed in   discretized Langevin diffusion and molecular dynamics, and  can be  applied to simulating  images, predicting protein structures, neural  network training and other continuous-optimization problems. The  second project focuses on generalizing the multigrid Monte Carlo  (MGMC) method used by physicists in computations of lattice field  theories. Our preliminary work reveals a close connection between the  MGMC and the reparameterization method in statistics literature. The  generalized  MGMC proposed in this project unifies these seemingly  different methods and provide a generic approach to the search of  more efficient MCMC schemes. Its use in random effects model and  probit model is currently under investigation and  its applications  in  Bayesian image analysis, such as pattern synthesis and  image  smoothing, will be explored. The third part of the proposal is  concerned with theoretical  analyses of the dynamic weighting method  recently proposed by Wong and Liang (1997). As shown by the authors,  the method has tremendous potential in various  optimization  problems, such as the traveling salesman problem, circuit  partitioning problem, neural network training, and Bayesian model  selection.  A few variations of the dynamic weighting are suggested  and a functional analysis method is outlined for studying   convergence  properties and weight behavior of the DW.    It has long been recognized that many complicated systems, such as   reactions in atomic bombs, fluid dynamics, molecular movements,  etc., can be studied via computer simulation.  One of the most  popular approach of this sort, the Monte Carlo technique,  simulates a complex system described by probability distributions.  The technique was named after the famed gambling resort  because  its procedures incorporate the element of chance. In the recent two  decades, people come to realize that Monte Carlo techniques is also  applicable to a much wider class of problems including global  optimizations, statistical inferences, signal processing, target   tracking, artificial intelligence, financial modeling,  computational biology, data mining, and many others. The PI has  successfully applied Monte Carlo techniques, together with  suitable statistical models, to the understanding of  relationship  between remotely related DNA or protein sequences; to dynamic  system updating (such as target tracking); to neural network  training; and to Bayesian statistical inference.  This project   targets at developing more efficient Monte Carlo methods for  dealing with challenging computational problems, such as  prediction of the 3-D structure of a protein molecule from its  primary sequence. A common feature of all Monte Carlo methods  for simulating complex systems is that they rely on cumulative  evolutions of small, albeit random, local changes.  This often  leads to a very slow-converging algorithm. For example, because   of s uch local moves, with currently available methods one needs  years of super-computer time in order to simulate one tenth of a  second of movement of a protein molecule. The PI outlines a  general method to conduct global moves in Monte Carlo methods;  proposes to incorporate ideas in optimization literature (i.e.,  gradient and conjugate direction methods) to speed up Monte Carlo  simulations; and suggests avenues for the theoretical   understanding of several new and promising Monte Carlo methods.  Success of this  research will benefit scientists and engineers  in coping with their computational challenges."
"9714876","Group Travel Award to the 6th Valencia International        Meeting on Bayesian Statistics to be held in Ibiza, Spain   on June 6-10, 1998","DMS","STATISTICS","05/01/1998","05/01/1998","James Berger","NC","Duke University","Standard Grant","Joseph M. Rosenblatt","04/30/1999","$20,000.00","","berger@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, OTHR","$0.00","PI: James O. Berger  Proposal Number: DMS 9714876  Project: Group Travel Award to the 6th Valencia International Meeting            on Bayesian Statistics     Abstract:     The Sixth International Valencia Meeting on Bayesian Statistics will be   held from May 30 - June 4, 1998, in Las Fuentes, Spain. This series of   international Valencia meetings has long been the major venue for   presentation and dissemination of research on Bayesian statistical   methodology and its applications. The Bayesian approach to statistics   has experienced astonishing growth in recent years, driven in large   part by its ability to solve difficult applied statistical problems in   science, engineering, and society in general.     This group travel award is in support of expenses of participants from  the United States. Participation in this meeting is crucial   for researchers, especially new researchers, as the pace and breadth of   developments in the area is making it increasingly difficult to monitor  the field. By attending the meeting, researchers will be able to acquire   the latest Bayesian techniques, and will have ample opportunity to   obtain feedback on their own work. A large portion of the grant will go   towards expenses of graduate students, new researchers, and members of   underrepresented groups. In addition, special activities for students   and new researchers will be instituted, in part to initiate   international interactions and collaborations."
"9870172","Robust Multivariate Analysis and Outlier Identification in    Massive Data Sets","DMS","STATISTICS","09/01/1998","09/13/2000","David Rocke","CA","University of California-Davis","Continuing Grant","John Stufken","08/31/2002","$105,000.00","David Woodruff","dmrocke@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","1339, 9216, HPCC","$0.00","Rocke<br/>9870172<br/><br/>This research involves new and improved methods for robust estimation of multivariate location and shape and identification of outliers.  Massive data sets, especially in high dimension, form a particular target of the research.  The research focuses on theoretical, methodological, and computational questions in robust multivariate analysis, and also extends the investigators previous work into cluster analysis.<br/><br/>Identification of outliers and clusters of data in high dimension and in massive data sets is an important and difficult problem.  These data sets occur in satellite imagery, in the human genome project, in x-ray astronomy, in geophysics, and in many other areas of science.  They range in size from multiple gigabytes to terabytes, and present significant challenges for analysis.  This project is aimed at methods for uncovering previously unknown structure in these data sets.  The computationally intensive nature of the methods relates this research directly to the High Performance Computing and Communication Federal Strategic Area, and the applications to environmental science data relates it to the Environment and Global Change area.  The project is supported by and associated with the National Partnership for Advanced Computational Infrastructure.<br/><br/><br/>"
"9733013","Model Uncertainty, Model Selection, and Robustness with     Applications in Environmental Sciences","DMS","STATISTICS","07/01/1998","06/23/1998","Merlise Clyde","NC","Duke University","Standard Grant"," Shulamith T. Gross","06/30/2004","$241,920.00","","clyde@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, 1045, 1187, 9188, EGCH, OTHR","$0.00","-----------------------------------------------------------------------  Proposal Number: DMS   PI: Merlise A. Clyde  Institution: Duke University  Project: Mode Uncertainty, Model Selection, and Robustness, with   Applications in Environmental Sciences    Abstract:    Finding models to describe data and relationships among  variables is a fundamental problem in statistics and science. Linear   regression and its generalizations are some of the most commonly used   statistical methods for data analysis.  As scientists collect   increasingly larger data sets, the fear of over-fitting the data by   using all measured covariates leads to the standard approach of  selecting a single ``best'' model based on a subset of the  covariates, and then proceeding with scientific inferences and  predictions as if that were the true model.  This typical analysis  ignores the uncertainty about which variables should be included in  the model, potentially leading to overconfident inferences.  Model  uncertainty often significantly outweighs other sources of  uncertainty in problems, but is generally ignored in standard  statistical practice and teaching.    This research focuses on Bayesian methods for incorporating model  uncertainty into data analysis and decision making.  In Bayesian  model averaging, predictions and inferences are based on a set of  models rather than a single model; each model contributes  proportionally to the support it receives from the observed data.  Novel methods of identifying promising models for use in Bayesian  model averaging are studied.  These can increase the applicability  of Bayesian methods to realistic problems that involve a large  number of variables.  One of the major applications of the  research is to estimate the effect of particulate matter on  mortality in the elderly population and assess the potential  impact of the EPA's new National Ambient Air Quality Standards  for particulates.  This and other case studies will be used to  develop a new course in modern Baye sian statistical methods for  environmental and biological science students at Duke University."
"9803365","Discrete Models for High-Level Image Analysis","DMS","STATISTICS, SIGNAL PROCESSING SYS PROGRAM","08/01/1998","07/17/1998","Valen Johnson","NC","Duke University","Standard Grant","Dean Evasius","07/31/2001","$59,169.00","","vejohnson@mdanderson.org","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269, 4720","0000, OTHR","$0.00","9803365<br/>Valen E. Johnson<br/><br/>This project develops statistical models for automatic image analysis.  The image models developed are defined by arbitrarily placing a large number of points, called facets, in a template image.  The facet points are organized hierarchically, and are arranged either on grids of increasing resolution or determined using more sophisticated criteria, e.g., they may be located at local extrema in the scale-space of the image.  Object shapes are established by connecting facets in a tree-like structure, and then using the shape of the tree in the image template to model expected shapes of facet trees in observed images from the class.  Typically, thirty to forty thousand facets are positioned within a two-dimensional image template, whereas one-quarter to one-half million facets are placed within three-dimensional image sets (e.g., magnetic resonance images or positron emission tomography images).<br/><br/>Models generated under this proposal facilitate computationally efficient high-level analysis of generic images.  These models are applicable to a wide range of image modalities, and require only that a template image or prototype image be available for the group of images under investigation.  Given an image template for the class, derived models can be applied with little or no operator intervention to obtain automatic segmentation, object identification, and volumetric analysis for available image data.<br/>"
"9803063","Discriminant Analysis of Hyperspectral Data for Bio         Species Recognition","DMS","STATISTICS, SIGNAL PROCESSING SYS PROGRAM","07/01/1998","06/26/1998","Bin Yu","CA","University of California-Berkeley","Standard Grant","Joseph M. Rosenblatt","06/30/2001","$75,000.00","","binyu@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269, 4720","0000, 9169, 9216, EGCH, HPCC, OTHR","$0.00","Bin Yu<br/>DMS 9803063<br/><br/>Hyperspectral data consist of intensity readings of hundreds of bands from ground or airborne spectrometers.  The proposed research is to identify forest species by means of  (1) hyperspectral data measured directly from above forest canopies in the field, and (2) hyperspectral data from airborne instruments based on the understanding from part (1).  This research is motivated by the belief that the rich amount of spectral information contained in hyperspectral data should improve the level of discrimination of forest species and by the desire for a relatively simple method that can handle the large number of spectral bands in hyperspectral data while tolerant of spectral noise.  Data reduction will be carried out via nonparametric techniques such as spline fitting and penalized discriminant analysis (PDA) for interpretation and classification purposes. Model selection criteria derived from the Minimum Description Length (MDL) principle will be investigated as both general model selection tools and criteria for spline knots selection and band selection for data reduction based on curve data.  Spatial PDA will be developed to classify hyperspectral images from airborne spectrometers to take into account the spatial dependence of pixels in the image.<br/> <br/>Correct recognition of forest species is important in natural resource management, environmental protection, biodiversity and wildlife studies.  Conventionally reliable methods for tree species recognition depend mainly on inventory in the field or on interpretation of large-scale aerial photographs.  The use of these methods is frequently limited by cost and time and not applicable to large areas.  Digital remote sensing has been used to identify forest species of large areas, but two problems are encountered: different tree species often have similar spectral characteristics partly due to the lack of high spectral resolution and lack of large number of spectral bands; and the same tree species may have distinct spectral properties due to illumination conditions on which optical remote sensing is based.<br/>"
"9803794","Workshops on Nonlinear and Nonstationary Signal Processing","DMS","STATISTICS, SIGNAL PROCESSING SYS PROGRAM","07/01/1998","06/04/1998","Richard Smith","NC","University of North Carolina at Chapel Hill","Standard Grant","Joseph M. Rosenblatt","06/30/1999","$19,000.00","","rls@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269, 4720","0000, OTHR","$0.00","Richard L Smith<br/>DMS-9803794<br/><br/>The classical theory of signal processing is based on models which are stationary, linear and in many cases also Gaussian.  Recent advances in time series and the theory of signal processing have drawn attention to many new models and methods.  Among these are nonlinear autoregressive and nonlinear state-space models, state-space models with time-varying or state-dependent coefficients as models for nonstationary and nonlinear series, linear non-Gaussian processes which pose some specific problems not encountered with Gaussian processes, methods derived from the theory of dynamical systems, and many others.  There has been a parallel growth in the applications of signal processing in many areas such as engineering, finance, the environment, etc.  The current project aims to develop new research in both the theory and applications of this diverse range of topics.<br/><br/>The project will provide funds for US-based researchers to attend a series of workshops taking place within a six-month program on ""Nonlinear and Nonstationary Signal Processing"" at the Isaac Newton Institute for Mathematical Sciences, Cambridge, England.  The overall theme of the program is to bring together researchers in different areas of statistics and signal processing, with the intention of fostering research links between the two groups.  Within this overall structure, a number of workshops on specific themes will be organised.  The themes include Bayesian approaches to signal processing, financial applications, environmental applications, dynamical systems and the analysis of large data sets in industry.  Participants from the USA will receive travel and subsistence support from the grant; some of the money is reserved for women, minorities and new researchers.<br/>"
"9801401","Studies on Foundations of Statistics","DMS","STATISTICS","08/15/1998","08/25/2000","Joseph Kadane","PA","Carnegie-Mellon University","Continuing Grant","John Stufken","01/31/2002","$288,586.00","Teddy Seidenfeld, Mark Schervish","kadane@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, OTHR","$0.00","9801401<br/>Joseph Kadane<br/><br/>The investigators work on three aspects of the foundations of Bayesian<br/>statistics and decision theory:  (i)   They develop measures of incoherence which permit them to quantify the extent to which non-Bayesian procedures violate principles of subjective expected utility.  (ii)  They implement algorithms for assessing partially ordered preferences, according to their representation in terms of sets of agreeing probability/utility pairs.  These are relevant for modeling Pareto consensus of a group of coherent (Bayesian) expert decision makers.  (iii) They explore connections between merely finitely additive probability and valid computations with ""improper"" priors, in the context of so-called ""Marginalization"" paradoxes.<br/><br/>Bayesian statistics aims to put statistics on a sound theoretical footing by modeling a rational statistical decision maker as if he or she were a bookie, announcing probabilities which act as prices at which risky bets can be bought or sold. An important general result is that a bookie either behaves as a Bayesian, or would accept a series of bets that would make the bookie a sure loser.  The investigators explore extensions of this theory to account for non-Bayesian behavior, groups of bookies, and more general interpretations of probability.<br/><br/>"
"9700554","International Workshop on Statistical Modelling to be held  July 27-31, 1998 in New Orleans, Louisiana","DMS","STATISTICS","03/01/1998","04/21/1997","Brian Marx","LA","Louisiana State University","Standard Grant","Joseph M. Rosenblatt","02/28/1999","$9,000.00","","bmarx@lsu.edu","202 HIMES HALL","BATON ROUGE","LA","708030001","2255782760","MPS","1269","0000, OTHR","$0.00","  Brian D. Marx    Abstract  This workshop's friendly and supportive academic atmosphere   brings together scientists from different nationalities, backgrounds   and experiences. The workshop will concentrate on various aspects   of statistical modelling, including theoretical developments,   applications and computational methods, and encourages papers   that are motivated by real practical problems and that make a   novel contribution to the subject. Theoretical contributions   addressing problems of practical importance or related to software   developments are also welcome. The scientific program is   characterized by having one hour invited lectures & tutorials   presenting new developments,contributed papers, posters and   software demonstrations. There are no parallel sessions.   Contributed papers should be suitable for a 30 minute oral   presentation (including discussion) and focus on motivation,   statement of key results and conclusions, and emphasize examples,   wherever possible. Papers and posters will be refereed by the   scientific committee and accepted papers and posters will be   printed in a proceedings volume that will be ready for the   start of the conference.     The first call for papers will be September 1, 1997 with a   closing date of February 10, 1998. Notification of acceptance   will be mailed by March 15, 1998.  Major professors should   encourage their students to attend this workshop. The program   is designed to allow plenty of time for discussion and   interchange between junior and senior scientists.  Additionally one paper session will be devoted entirely to  contributions from students. This session will be judged by   the scientific committee and an award will be given to the   winner of the Best Student Paper at the conference dinner   on Thursday evening (July 30, 1998).    The workshop language is English."
"9803627","Statistical Modelling and Dimension Reduction for Functional Data","DMS","STATISTICS","08/01/1998","05/09/2002","Jane-Ling Wang","CA","University of California-Davis","Standard Grant","John Stufken","07/31/2003","$64,001.00","","janelwang@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","0000, OTHR","$0.00","9803627<br/>Jane-Ling Wang<br/><br/>Curve or functional data are of increasing importance due to enhenced high performance computing facilities and increased awareness that such data require special methods of analysis.  Such data arise for example in longitudinal studies and in aging research.  This research involves the development of new nonparametric methods to analyze such data. The focus is on procedures which involve both multivariate techniques and smoothing methods.  Of particular interest are ANOVA and regression models for curve data.  Emphasis is given on methods viewing the curve data as generalizations of one or several stochastic processes.  Given a Karhunen-Loeve representation of the underlying processes, ANOVA for curve data can be based on the estimated principal components or on the whole curve.  In this context, curve registration is of interest.  New regression methods for curve data is also proposed.  One of the regression methods extends the karhunen-Loeve representation to allow covariates to act on either the mean function or the covariance functions of the representation.  Another regression method extends the sliced inverse regression (SIR) method for multivariate data to curve data.  Such an approach for curve data has the advantage of model flexibility and accomplishes the goal of dimension reduction before the nonparametric model fitting stage.  It is computationally simple and suitable as an exploratory tool for functional data analysis.<br/><br/>This research is motivated by collaborative research of the investigator on biological lifespan and aging.  Recently, there is an accelerated interest in aging research partly due to its financial and policy impact and partly due to its intrinsic aspects.  The elderly population (age 65 or above) will swell as the babyboomers begin turning 65 in the year 2011 so its impact is self-evident.  The investigator has been active in this research area through the analysis of large data sets on the aging of mediterranean fruit flies (medflies).  One of these data sets contains the complete reproduction history, in terms of the number of eggs laid daily, for each of the 1,000 female medflies in the experiment.  The scientific question is how to relate reproductive traits to survival and mortality of the medflies.  Another data set on the nutrition effects to mortality of male and female medflies is studied.  Here the scientific question of interest is to relate nutrition and gender effects to aging.  The procedures developed in this study are applied to these and other medfly data to judge the practicality and scientific merits of the methods.  They also help to address the underlying biological issues about the aging process.<br/>"
"9803112","Imputation and Variance Estimation for Survey Data","DMS","STATISTICS, Methodology, Measuremt & Stats","08/01/1998","08/03/1998","Jun Shao","WI","University of Wisconsin-Madison","Standard Grant","John Stufken","07/31/2002","$59,673.00","","shao@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269, 1333","0000, OTHR","$0.00","9803112 <br/>Jun Shao<br/><br/>This research involves development of imputation techniques and variance estimation methods for survey data with nonrespondents.  The investigator focues on (1) validating and comparing (both theoretically and empirically) the existing imputation techniques and developing better procedures if necessary; and (2) developing correct variance estimators for a given imputation method that produces correct survey estimates.  Special attention will be paid on random hot deck imputation using models, nearest neighbor imputation, cold deck imputation, multivariate imputation, longitudinal imputation, imputation for quantiles, and imputation for non-ignorable response.  Many variance estimation techniques (such as the linearization/Taylor expansion, jackknife, balanced half sample or balanced repeated replication, random groups, and bootstrap) will be studied.  Particular issues that will be addressed in variance estimation include non-negligible sampling fractions, approximation in applying replication methods (such as grouping and collapsing), complex and composite imputation methods (in the sense that a number of different imputation methods are used and/or imputed data are used to impute nonrespondents for other variables), variance estimation for nearest neighbor imputation, variance estimation for sample quantiles, and problems with imputed values that cannot be identified from the data set.<br/><br/>Most surveys have nonrespondents.  Item nonresponse occurs when some sampled units cooperate in the survey but fail to provide answers to some questions. Commonly used compensation procedures for handling item nonresponse are imputation techniques which insert values for nonrespondents.  It is a common practice to treat the imputed values as if they had been observed, and compute survey estimates and assess their varibility using standard formulas designed for the case of no nonresponse.  This, however, could lead to some problems and biases in statistical analysis.  For example, the use of standard formulas to assess varibility in analysis may seriously underestimate the true varibility, because standard formulas do not account for the changes in varibility due to nonresponse and/or imputation.  This research involves development of correct and simple to implement statistical procedures to analyze survey data with nonrespondents and imputation; and will solve some real statistical problems in survey agencies such as the Census Bureau, the Bureau of Labor Statistics, and Westat.<br/><br/>"
"9801130","Sixth Purdue International Symposium on Statistics","DMS","STATISTICS","05/15/1998","05/04/1998","Shanti Gupta","IN","Purdue University","Standard Grant","Joseph M. Rosenblatt","04/30/1999","$17,000.00","Mary Bock","","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","0000, OTHR","$0.00","  PI: Shanti S. Gupta and Mary Ellen Bock, Purdue University  Proposal Number: DMS 9801130  Project: Sixth Purdue International Symposium on Statistics    Abstract:     This international symposium will be held from June 17 to 21, 1998,  at Purdue University.  The symposium consists of a workshop June 17-19  on ""Synthesis and Interfaces of Major Statistical Paradigms"" and a   conference June 19-21 on ""Statistical Decision Theory and Related Topics"".  The workshop will focus on the major approaches such as frequentist,   Bayesian, likelihood, and bootstrap paradigms.  The different paradigms  are not simply separate approaches that should be developed individually,   but are instead pieces of the puzzle of statistical inference that must  be fit together.  One of the primary purposes of the conference on decision  theory is to use the unifying nature of decision theory to enhance   cross-fertilization among apparently disparate areas of statistics.  The  conference is a natural adjunct to the workshop.    Statistical decision theory is increasingly used in modeling and analysis  of decision problems in business, economics, medicine, law, reliability  and quality control, risk assessment, and numerous other fields.  It   attempts to provide a formal approach to decision making and statistical   inference by asking that explicit numerical criteria be given and that  the merits of a statisitcal procedure or decision be assessed by these   criteria.  The broad topics for the conference include classical subjects  within or related to statistical inference and decision theory.  The   interface workshop examines areas where the different paradigms used by  statisticians may be unified in methodology.  The focus is on model  selection and testing, confidence sets and prediction intervals,   nonparametrics and statistical computation.    A large portion of the grant will go towards expenses of   graduate students, new researchers, and members of underrepresented  groups."
"9803352","Computed Tomography and Sampling","DMS","APPLIED MATHEMATICS, STATISTICS","06/15/1998","06/18/2002","Adel Faridani","OR","Oregon State University","Standard Grant","Deborah Lockhart","05/31/2003","$84,893.00","","faridani@math.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","MPS","1266, 1269","9216, HPCC","$0.00","This project involves research in computed tomography, generalized Shannon sampling theory, and applications of sampling theory in tomography. Computed tomography is an imaging technique which  reconstructs a density function from a large number of its line integrals. Ordinary tomography is not local: reconstruction at a point requires integrals over lines far from that point. For a number of applications it is highly desirable if only integrals over lines intersecting some  region of interest need to be used (""region-of-interest tomography""). A special case is local tomography where the region of interest may be arbitrarily small. Local tomographic reconstructions recover some but not all properties of the original density function. Sampling theorems provide exact interpolation formulas for bandlimited functions and play a fundamental role in signal processing. In tomography sampling theorems are used to identify efficient data collection schemes allowing for maximum resolution in the reconstructed image, as well as for error analysis of reconstruction algorithms. If, as is the case in tomography and other applications, sampling theorems are applied to non-bandlimited functions, interpolation is no longer exact and a so-called aliasing error occurs. The usefulness of sampling theorems for such functions depends on the availability of good estimates for this error. <br/><br/>The  goals of this project are development and  numerical analysis of high-resolution reconstruction algorithms and development of a software package for region-of-interest tomography including local tomography; investigation of optimal sampling in three-dimensional local tomography; derivation of new error estimates for the aliasing error; and exploration of a generalization of the sampling concept in the framework of locally compact abelian groups which provides a unified view of  diverse applications such as non-equidistant sampling, numerical integration, multisensor deconvolution, and filter banks.<br/><br/><br/><br/><br/>"
"9803801","Clifford Conference","DMS","STATISTICS","09/01/1998","08/10/1998","Jian-Jian Ren","LA","Tulane University","Standard Grant","Joseph M. Rosenblatt","08/31/1999","$5,000.00","","jjren@umd.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","MPS","1269","0000, OTHR","$0.00","-----------------------------------------------------------------------  Proposal Number: DMS 9803801  PI:              Jian-Jian Ren  Institution:     Tulane University  Project:         The Clifford Conference    Abstract:    The 1998 Clifford Lecture and Conference, which will take place in   October of 1998 at Tulane University.  The 1998 Clifford Lecturer is   Professor Peter Bickel of University of California - Berkeley, who   will give a series of 3 lecturers on ""The Method of Sieves in Non-   and Semiparametric Statistics"".  During the conference, there will   be 10-15 invited speakers giving talks related to Professor Bickel's  main lectures.  These lectures and talks will:      (a) compare and contrast the fields by examples, discuss how   bootstrap methodology fits into this framework, introduce the    method of sieves;      (b) discuss and review various applications of the method of   sieves to function estimation, Haar wavelet based tests, AR sieve    bootstrap, and more general use of orthonormal based or families    of bases or atoms in density estimation and other contexts;      (c) discuss the relation of the method of sieves to model   selection and the great challenge: Selecting Tuning Parameters.    The 1998 Clifford Lecture and Conference will provide a  unique opportunity for researchers to meet and discuss the current   issues in statistical theory and practice. This is particularly   beneficial to junior researchers, graduate students and faculty   from southern universities in or near Louisiana."
"9804739","Participant Support for Newton Institute Program on         Biomolecular Function and Evolution in the Context          of the Genome Project","DMS","COMPUTATIONAL BIOLOGY ACTIVITI, Genetic Mechanisms, PHYLOGENETIC SYSTEMATICS, POPULATION DYNAMICS, PROBABILITY, STATISTICS, COMPUTATIONAL MATHEMATICS","06/15/1998","06/09/1998","Thomas Kurtz","WI","University of Wisconsin-Madison","Standard Grant","K Crank","05/31/1999","$43,500.00","","kurtz@math.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1107, 1112, 1171, 1174, 1263, 1269, 1271","0000, 9263, OTHR","$0.00","9804739  Kurtz   The problems facing molecular biologists in understanding and analyzing the flood   of molecular genetic sequences and structures resulting from the Genome Projects raise a   range of challenging biomathematical research questions. The Newton Institute will host a   program on ""Biomolecular Function and Evolution in the Context of the Genome   Project."" This inter-disciplinary program, on the function, structure and evolution of   molecular sequences, will bring mathematicians and computer scientists working on   subjects such as probabilistic modeling, stochastic processes, geometry, statistical data   analysis, computational complexity, neural networks, genetic algorithms and expert   systems, together with molecular biologists working in medical and biological fields. This   award will support participation by US researchers. US participants will include graduate   students and postdoctoral and senior researchers."
"9817552","Statistical Evaluation of Global Observing Systems","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","10/01/1998","08/06/1998","Ta-Hsin Li","CA","University of California-Santa Barbara","Standard Grant","Dean Evasius","09/30/2001","$200,000.00","","thl@pstat.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","MPS","1253, 1269","0000, OTHR","$0.00","9817552<br/>Ta-Hsin Li<br/><br/>Satellites, particularly low-Earth-orbiting (LEO) asynchronous platforms, have become the primary vehicle for monitoring global environmental behavior from space because of their capability of global coverage. Satellite data, combined with in situ observations, are increasingly important for global climate and environmental assessment and prediction.  The sampling properties of satellites have a great impact on satellite-based data products. Sampling errors due to poor sampling design or improper use of satellite data can cause rapid deterioration in accuracy.  Evaluation of sampling errors thus constitutes an integrated part of the uncertainty assessment of environmental assimilation, estimation, and prediction.<br/><br/>The overall goal of this research is to improve our understanding of sampling errors for global observing systems and to enhance our ability to assess the impact of sampling errors on the estimation of global environmental parameters. Ultimately the proposed research will lead to effective design of sampling schemes and quality assurance of satellite-based data products.<br/><br/>To achieve the overall goal, this research proposes to develop general statistical methodology and procedures for exploration and evaluation of sampling errors in various derived parameters from various observing systems.  The observing systems considered include single polar and nonpolar platforms, intercalibrated multiple polar and nonpolar satellites, and combinations of in situ point-gauge networks with polar/nonpolar satellites.  The derived parameters of interest include the coefficients in the advanced global representations using the spherical harmonics and the empirical orthogonal functions (EOFs). Special attention is paid to small-scale/short-wavelength parameters that are important to the estimation of regional and interseasonal to interannual changes and trends.  For efficient representation of regional activities, the research also proposes to develop statistical theory and algorithms of spherical wavelets and investigate sampling errors in the spherical wavelet representations.<br/><br/>The proposed research is exploratory in nature, conducted both analytically, via theoretical analysis, and numerically, via computer simulations and numerical experiments using physical-statistical models and assimilated data.  Successful completion of the research is expected to improve our understanding of sampling errors for various important global observing systems and our ability to assess the impact of sampling errors on the estimation of important global environmental parameters.  The methodology and procedures developed in the research are expected to be helpful in guiding the data-collection planners to evaluate different sampling designs prior to a data-collecting mission and make optimal choices according to their understanding of the environmental fields to be observed.  The research results are also expected to be useful in the uncertainty assessment and quality assurance of satellite-based data products that are now widely used for environmental studies.<br/><br/>This research project in environmental statistics is jointly supported by the MPS Office of Multidisciplinary Activities (OMA) and the DMS Statistics Program. <br/>"
"9729260","Third International Conference on Monte Carlo and           Quasi-Monte Carlo Methods in Scientific Computing to be     held June 22-26, 1998, in Claremont, California","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","04/15/1998","04/08/1998","Jerome Spanier","CA","Claremont Graduate University","Standard Grant","John C. Strikwerda","09/30/1999","$12,267.00","","jspanier@uci.edu","150 East Tenth Street","Claremont","CA","917115909","9096079296","MPS","1269, 1271","0000, 9263, OTHR","$0.00","Spanier  DMS 9729260      Monte Carlo & Quasi-Monte Carlo methods are very active and pervasively influential areas of applied mathematics in which highly sophisticated uses of mathematical modeling, simulation and probability theory are used to solve a wide variety of real-world problems not amenable to other solution methods. Conventional Monte Carlo methods provide tools for studying such problems on a computer, making use of probability theory and statistical error estimation. Quasi-Monte Carlo methods are essentially deterministic versions of Monte Carlo methods, based not on randomly chosen samples but carefully chosen deterministic nodal sets. Their analysis utilizes number theory, combinatorics, abstract algebra and even algebraic geometry. And in the past several years, exciting new hybrid methods, combining the best features of both stochastic and deterministic models, have been developed for application to many of the most difficult of today's problems.     The Third International Conference on Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing, to be held in Claremont, California June 22-26, 1998, will assess recent progress in Monte Carlo and Quasi-Monte Carlo theory and address the application of these methods to a range of problems challenging scientists, engineers and economists today, problems arising in nuclear physics and engineering, semiconductor and computer modeling, cryptanalysis, oil exploration, and financial engineering - to name just a few important applications."
"9803200","Data-Analytic Modeling for High-Dimensional Data","DMS","STATISTICS","08/01/1998","07/25/2000","Jianqing Fan","CA","University of California-Los Angeles","Continuing grant","Joseph M. Rosenblatt","12/31/2000","$148,984.00","","jqfan@princeton.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","0000, OTHR","$0.00","9803200<br/>Jianqing Fan<br/><br/>This research involves a variety of new data-analytic techniques for processing and modeling high dimensional data that arise from many scientific disciplines.  These range from varying-coefficient models from epidemiology and environmental statistics, time-inhomogeneous diffusion models from finance, curves and images data from ophthalmology, marketing and biostatistics, to wavelet applications in statistics and engineering.  The research enhances significantly the availability of tools and software for analyzing these complicated high-dimensional data.<br/><br/>The research covers a wide array of methodological developments and foundational research.  In particular, four inter-related topics are focussed.  Firstly, a two-step strategy is proposed for efficient estimation in varying-coefficient models.  This is particularly useful when coefficient functions admit different degrees of smoothness.  The proposed methods will be justified by carefully formulated asymptotic theory and simulations.  The proposed methods extends to longitudinal studies and likelihood-based models such as generalized linear models.  Secondly, time-inhomogeneous diffusion models are proposed to model term-structure dynamics and asset pricing.  These nonparametric models aim at addressing inadequacy of over simplified parametric models.   Various data-analytic modeling techniques are proposed and justified by both theoretical studies and empirical research.  Thirdly, wavelet applications to a variety of statistical problems are discussed.  The proposed approaches derive from statistical modeling prospective and are based on penalized likelihood idea.  Our aim is to expand the applicability of wavelets in statistics and to improve efficiency of some existing techniques.  In particular, likelihood based models, robustness, denoising correlated data and additive modeling are proposed for investigation.  Finally, some statistical inference problems arising from processing curves and images data are investigated.<br/>"
"9802071","Extended Linear Modeling with Free Knot Splines","DMS","STATISTICS","07/01/1998","06/07/2000","Charles Stone","CA","University of California-Berkeley","Continuing grant","John Stufken","06/30/2002","$209,135.00","","stone@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","DMS 9802071<br/>Charles J. Stone<br/><br/>In many statistical models of current theoretical and practical interest, the log-likelihood function depends on one or more unknown functions.  The theory and methodology of such models, referred to as extended linear models, are particularly tractable when the models are concave; that is, when the log-likelihood depends concavely on the unknown function or functions.  Among the class of concave extended linear models are regression; logistic, probit, Poisson, and other forms of generalized regression; multiple logistic regression (which is useful in multiple classification); hazard regression for survival analysis; the extension of hazard regression to event history analysis; and models for the estimation of density and conditional density functions.  In the context of such models, polynomial splines and their tensor products are natural building blocks for constructing finite-dimensional estimates of infinite-dimensional main effects and low-order interactions, and the resulting ANOVA decompositions provide an insightful tool for data analysis.  Corresponding nonadaptive theory and adaptive methodology have been extensively developed in recent years.  In this research, in order to reduce the gap between the existing theory and methodology and to improve on the methodology, similar theory and methodology and methodology for free knot splines are being developed, in which knot locations are viewed as free parameters to be determined by the maximum likelihood method along with the other parameters of the model.<br/><br/>The investigator is concurrently working on a monograph on extended linear modeling with splines with four of his collaborators, all former Ph. D. students and Research Assistants on previous NSF Grants of the investigator.  The research on free knot splines will be incorporated into the monograph.  This should contribute to the civil infrastructure by encouraging the inclusion of spline-based methodology in commercial statistical software packages, which would facilitate its use in the analysis of data of scientific and industrial importance.  It should also extend the appeal of this methodology to include researchers in the data mining community.<br/>"
"9802960","Research on Sieve Approximations to Non and Semiparametric  Models, Hidden Markov Models and Comparison of Phylogenetic Tree Biologies","DMS","STATISTICS","07/01/1998","05/05/2000","Peter Bickel","CA","University of California-Berkeley","Continuing grant","John Stufken","06/30/2002","$290,197.00","","bickel@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","-----------------------------------------------------------------------  Proposal Number: DMS-9802261  PI: Peter Bickel  Institution:   Project: Research on sieve approximations to non and semiparametric      models, Hidden Markov models and comparison of phylogenetic tree      biologies.      Abstract:    A theoretical investigation of the ""plug in"" property in the context of  non and semiparametric models.  The intention of the investigators is   to characterize non and semiparametric models in which the outcomes of   appropriate fitting procedures can be safely plugged in for a broad   range of uses, and the study of model selection criteria when the loss   function reflects the goal of fitting some features of the data well   rather than a global fit.    This project includes:    *Further development of a theory for testing parametric or   semiparametric hypotheses in a semi or non parametric context.    *Further development of the theory of inference for Hidden Markov   Models.  The investigators propose extension to state space models.    *Development of new procedures and analysis of existing procedures for   estimating fixed effects and prediction of random effects using   semiparametric models for longitudinal and/or ""pharmacokinetic"" data.    *Further development of the theory and practice of selecting m in the  m out of n bootstrap     *An examination of the sensitivity to choice of stochastic model in the  construction of phylogenetic trees    Tests and diagnostics for semiparametric models such as those the   investigators intend to continue to develop are useful in a number of   areas.  For instance, the Black Scholes option pricing formula is   widely used in finance.  One of the methods the investigators have  already developed show the invalidity of the formula for large data   set and points to plausible more realistic models.  Phylogenetic trees  are used not only for representing evolutionary relationships among  species of animals and plants but also, as in the case we are g oing  to study, important families of proteins.  Studying the types of  models that lead to plausible evolution trees should also lead to  pattern recognition algorithms which will be useful in classifying  protein families and hence to relating new proteins to families whose  properties are known.  This is a major activity in the search for new  bioactive compounds in biotechnology."
"9711623","Hierarchical Modeling for Integrated Environmental          Assessments","DMS","STATISTICS","01/01/1998","07/31/1997","Raymond O'Connor","ME","University of Maine","Standard Grant","K Crank","08/31/1999","$130,000.00","Deirdre Mageean","oconnor@umenfa.maine.edu","5717 Corbett Hall","ORONO","ME","044695717","2075811484","MPS","1269","1317, EGCH","$0.00","9711623  O'Connor   The distribution of biotic resources over large spatial extents is often a function of   climate, of land-use, and of the demographics of the human population but these different   classes of independent variables have different spatial scales for their action. One approach   to the integration of these effects across scales is to use hierarchical models that   incorporate contingencies and constraints in effects. This project seeks to develop such a   modeling paradigm by use of classification and regression trees (CART). The hexagonal   grid of the U.S. Environmental Protection Agency's Environmental Monitoring and   Assessment Program is used as the spatial grid, with about 12000 grid points within the   conterminous U.S. To test the approach, national Breeding Bird Survey data are used as   dependent variables (species richness, abundance of individual species). The independent   variables are derived from climate models, from remote sensing of land use from   Advanced Very High Resolution Radiometry (the Loveland prototype land cover   classification), from National Agricultural  Statistics Service statistics, and from Bureau of   the Census data. The CART modeling then determines the best combination of predictors   for each hexagon with bird data (ca. 1200), and the resulting model is used to predict   values for  all other hexagons. One can then map the regions with unique patterns of   environmental  effects on birds, can determine the location and degree of species losses   associated with the local or regional action of particular stressors,  and assess the location   and total impact of the overall stressor load on the system. Specific questions that are   addressed in the project include the value of different datasets as predictors, the influence   of spatial autocorrelation, improvement of model fit and assessment of prediction   accuracy,  and methods of visualization of the results.   The effects of climate, land-use and human demographics on environmental    resources are particularly difficult to estimate in an integrated manner because some   effects are broad-scale while others are very localized. If one could do so, a national   environmental risk assessment would be feasible in a cost-effective manner. Here a new   approach to the problem is tested, using a hierarchical model that first adjusts for broad-  scale effects and then assesses regional and local effects in turn. As a test, bird data for the   48 conterminous states are modeled against national climate data, remote sensing data on   land use, and agricultural and Census data. The resulting national map shows on a 12,000   point grid the extent to which species are lost to particular or to all stressors  at each   point, in effect a national risk assessment of environmental impacts as experienced by birds   in relation to the stressors tested. The approach can readily be extended to other   environmental response variables and to smaller or larger scales. This project   systematically investigates technical uncertainties that might limit the usefulness of the   approach."
"9803459","Dimension Reduction and Data Visualization","DMS","STATISTICS","08/01/1998","05/04/2000","Ker-Chau Li","CA","University of California-Los Angeles","Continuing grant","John Stufken","07/31/2002","$173,081.00","","kcli@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","0000, OTHR","$0.00","9803459<br/>Ker-Chau Li<br/><br/>Sliced inverse regression (SIR) and principal Hessian directions (PHD) are two newly established dimension reduction techniques.  Their properties have been studied under general regression settings.  This research involves the application of these methods to several problems arising from the following areas:<br/><br/>(1) Monte Carlo and Bayesian computation,<br/>(2) nonlinear time series,<br/>(3) semiparametric modeling,<br/>(4) multivariate outcome and functional data analysis, and<br/>(5) uncertainty  analysis of mathematical/physical/computer models.<br/><br/>These settings are more complicated.  In (1), importance sampling and rejection sampling are considered.  The importance weights can be treated as the outcome values.  The goal becomes the study of finding a simplified  relationship between the weights and the sampled points.  Dimension reduction can help find the modes of posterior distributions.  Proper formulation of the problem is also needed in (2)-(4) before applying SIR/PHD.<br/><br/>Dimensionality is an issue that can arise in every scientific field.  Generally speaking, the difficulty lies on how to visualize a data set involving several variables or the shape of a function with several arguments. SIR/PHD are domain-free dimension reduction tools that can be conveniently applied in areas where mining data for information is crucial.  For this research, the primary examples include ground water modeling, recovering signals from convoluted data sequences in digital communication, and analyzing  nonlinear economic time series data.<br/><br/>"
"9804058","Nonparametric Regression","DMS","STATISTICS","07/01/1998","06/18/1998","David Ruppert","NY","Cornell University","Standard Grant","Joseph M. Rosenblatt","06/30/2001","$85,971.00","","dr24@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","0000, 9146, MANU, OTHR","$0.00","Proposal Number: DMS 9804058  PI: David Ruppert  Institution: Cornell University  Project: Nonparametric Regression     Abstract:     The research focuses on (a) fitting regression splines by penalized   methods, (b) Bayesian nonparametric inference using regression splines,  (c) multivariate regression splines with applications to stochastic   dynamic programming, and (d) bandwidth selection in local regression.    Regression is a statistical technique for discovering relationships   between variables, for example, percentage of saturated fat in one's   diet and the probability of cancer.  Regression is applied thoroughout   the sciences and engineering.  Modern regression methodology, including  this research, focuses on complex nonlinear relationships with large   numbers of variables.  A typical example would be estimation of credit   risk based upon an entire credit history and other financial variables   of a credit applicant.  This research will be on nonparametric   regression meaning that the form of the relationship must be discovered  from the data.  Nonparametric regression techniques are rapidly finding  applications in business under the name ""data mining.""  In a separate   project funded by the SRC, the research will be applied to the   improvement of tool utilization in semiconductor manufacturing."
"9802358","Optimal Judgment Sample Sizes for Distribution-Free Ranked  -Set Sampling Procedures, Effects of Imperfect Judgment     Rankings, & Extensions of K-Sample and Correlation Probl","DMS","STATISTICS","08/01/1998","05/04/2000","Douglas Wolfe","OH","Ohio State University Research Foundation -DO NOT USE","Continuing grant","John Stufken","04/30/2002","$74,865.00","","daw@stat.ohio-state.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","9802358  Douglas A. Wolfe    Ranked-set sampling is an approach to collecting data that has been shown to lead to substantial improvement over simple random sampling in the properties of statistical inference procedures for both the one-and two-sample settings.  The development of nonparametric ranked-set sample procedures has been relatively recent in the literature and there is much yet to be completed.  Such development is vital, however, since often the type of data for which measurement is difficult is also likely to be data which have a distribution that is not Gaussian.  This research involves three major thrusts.  First, the investigators study the all-important optimal choice of ranked-set judgment sample size, concentrating not only on the relative costs of measurements versus judgment orderings but also on the effect of imperfect judgment rankings on the entire process.  In most cases, the optimization criterion is taken to be the Pitman asymptotic relative efficiency of the associated procedures.  A second part of this research deals with the dual question of which order statistics should be measured in the ranked-set samples and  then how best to differentially weight those that are measured.  This portion of the study also investigates the potential gains from collecting more than a single order statistic from each judgment sample.  Finally, the investigators extend nonparametric ranked-set sample procedures to other statistical problems, such as development of ranked-set sample analogs to the k-sample Kruskal-Wallis test and the Spearman and Kendall correlation procedures.    In situations where measurements are costly and/or difficult to obtain but ranking of the potential sample data is relatively easy and accurate, the use of statistical methods based on a ranked-set sampling approach can lead to substantial improvement over analogous methods associated with more standard simple random sampling schemes.  Ranked-set sampling utilizes information gained without formal measurement  to provide more structure in the eventual measured data than is available in a corresponding simple random sample of the same size.  This enables investigators to obtain the same information from fewer observations at less cost.  This is particularly important in areas such as ecological and environmental studies where assessment of collected samples can be quite expensive or in medical studies where utilizing the additional judgment ranking in ranked-set sampling leads to fewer subjects involved in the development and testing of new medications or procedures prior to FDA approval.  There is also tremendous potential for this sampling approach in economic predictions and auditing procedures, where the information of interest is often very time-consuming to collect and ranked-set sampling can help guide the search for those judgment ordered observations (fewer of them) which are best to quantify."
"9803600","Statistical Modeling and Analysis of Circadian Rhythms","DMS","STATISTICS","08/15/1998","05/19/2000","Sreenivasa Jammalamadaka","CA","University of California-Santa Barbara","Continuing grant","Joseph M. Rosenblatt","07/31/2001","$84,056.00","","rao@pstat.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","MPS","1269","0000, 9178, 9251, OTHR, SMET","$0.00","9803600<br/>S. Rao Jammalamadaka<br/><br/>This research is concerned with statistical models and methods for circular data, including inference for wrapped stable family of circular distributions, tests for multimodality, predictive inference for directional data and application of Bayesian methods when appropriate prior information is available. When a circular variable depends on other linear or circular covariates, multifactor designs as well as regression approaches are used to assess such dependence.Correlation and regression for circular variables and related optimal-design issues involve novel statistical problems and form an essential part of this investigation.<br/><br/>One primary motivation for this research comes from the study of biorhythms which control our biological clock and include such important things as the sleep-wake cycle, hormonal pulsatility, reproductive cycles etc. Proper statistical analysis of such rhythms should take account of the fact that cyclical phenomena should be represented as data on a circle of suitable circumference rather than as more traditional ""linear"" data. Appropriate modeling of such rhythmicity and its proper statistical analysis is an intended outcome of this research. The regression models tell us how this rhythmicity might be affected by other covariates such as age, sex, time at which certain treatment is administered, dosage etc. Using these models, one can hope to determine optimal timing for medications or surgery to achieve greatest impact as well as help quantify the benefits of newly emerging approaches such as ""Chronotherapeutics"". Although the presence of circadian-time keeping mechanisms at both the neurophysiologic and molecular genetic levels have been known to scientists, the present research will provide methods for comprehensive statistical validation of such findings and especially in any future efforts to relate the biological clock to covariates like age etc.<br/><br/>This research on circular statistics is also closely related to some current investigations on the Environment and Global Change that utilize the directional spectra of wind and sea currents."

"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"0203215","Bayesian Nonlinear Regression with Multivariate Linear Splines","DMS","STATISTICS","09/01/2002","06/02/2004","Bani Mallick","TX","Texas A&M Research Foundation","Continuing Grant","Grace Yang","08/31/2006","$159,095.00","","bmallick@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","Proposal ID: DMS-0203215<br/>PI: Bani Mallick<br/>Title: Bayesian nonlinear regression with multivariate linear splines<br/><br/>The investigator and his colleagues consider novel, nonparametric modeling of univariate and multivariate non-Gaussian response data. The usual generalized linear models are extended to generalized nonlinear models by modeling the mean function in a flexible way. Data adaptive multivariate smoothing splines are employed to do this, where the number and location of the knot points are treated as random. The posterior model space is explored using a reversible jump Markov chain Monte Carlo (MCMC) sampler. Computational difficulties are partly alleviated by introducing a residual effect in the model that leaves many of the posterior distributions of the model parameters in standard form. The use of the latent residual effect provides a convenient vehicle for modeling correlation in multivariate response data and as such the method can be seen to generalize the seemingly unrelated regression model to non-Gaussian data. In the next part of the project the investigator and his colleagues develop semiparametric Bayesian methods for generalized non-linear models where a predictor is measured with either classical or Berkson error. In the presence of covariate measurement error, estimating usual regression function nonparametrically is extremely difficult, the problem being related to<br/>deconvolution. In the case of generalized linear model it is more difficult. Again combinations of spline regression and MCMC techniques are used to handle the problem.<br/><br/>Function estimation is an important statistical tool that tries to understand accurately the functional relationships between variables based on data and it has applications in many disciplines for successfully addressing scientific questions. Most of the flexible, nonlinear regression problems are developed when the response is a continuous variable. In important applied problems the response may be count or indicator variable and flexible function estimation is much more harder in these situations. In this proposal the investigator intend to develop the methods that adaptively estimate the functional relationships in these more complicated situations. The area of biotechnology is an especially application for these methods. Scientists now have techniques for measuring gene expression levels for thousands of genes at the same time, allowing the exciting possibility of determining which human genes are involved in a disease such as cancer and heart disease. These methods will be useful to explore nonlinear relationship between gene expression levels and the chance of the disease. Other possible applications are to model correlated multivariate disease or accident counts data where the methods being developed here will improve modeling disease or accidents maps (with uncertainties) which will be useful for disease or transportation risk assessments.<br/>"
"0226201","Frontiers of Statistical Research: Forty Years of Statistics at Texas A&M University","DMS","STATISTICS","09/01/2002","07/30/2002","Thomas Wehrly","TX","Texas A&M Research Foundation","Standard Grant","John Stufken","11/30/2002","$11,250.00","Emanuel Parzen, Jeffrey Hart, James Calvin, Fred Speed","twehrly@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","Proposal ID: 0226201<br/>PI: Thomas Wehrly<br/>Title: Frontiers of statistical research: Forty years of statistics at Texas A&M University<br/><br/>Abstract:<br/><br/>This research conference, in recognition of the 40th anniversary of the Department of Statistics at Texas A&M University, brings together leading researchers in four theme areas of great current interest in statistical research: 1) Linear models and their generalizations, 2) bioinformatics and statistical genetics, 3) time series analysis, and 4) smoothing. The meeting features cutting edge presentations in these areas, stimulates cross-fertilization, and pays especial attention to the interaction of new researchers and students with established researchers. The conference offers a number of ways through which new researchers can participate, including poster sessions, roundtable discussions, and discussion sessions following the talks.<br/>"
"0204572","Shape-Restricted Inference","DMS","STATISTICS","08/01/2002","07/31/2002","Mary Meyer","GA","University of Georgia Research Foundation Inc","Standard Grant","Xuming He","07/31/2004","$68,367.00","","mmeyer@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0204572<br/>PI: Mary Meyer<br/><br/>Title: Shape-Restricted Inference<br/><br/>Consider the problem of estimating a function given observations with some random component. The function may be a regression function, a density, or a probability density function such as in bioassay models. Shape-restricted methods allow the practitioner to impose only qualitative restrictions on the class of functions, such as increasing, concave, or sigmoidal. Estimates may be obtained using maximum-likelihood ideas; there are many problems in inference to be solved. Goals for this proposal include developing confidence bounds for regression functions using shape restrictions, developing tests for an ANCOVA type of model with a shape-restricted covariate, time-series analysis with shape-restricted trend function, smooth shape-restricted function estimation, and developing a robust regression estimator using a shape-restricted error density.<br/><br/>Traditional statistical methods for regression, density estimation and bioassay problems include: 1) estimating a function, 2) estimating the quality of fit, perhaps using confidence bounds, and 3) testing hypotheses about the function. The investigator wishes to develop these methods nonparametrically, that is, without imposing a parametric form for the function. Shape-restricted methods in statistics approach these estimation and inference problems with a minimum of assumptions about the functional form. For example, a growth curve may be assumed to be increasing and concave, or a probability curve might be sigmoidal- a more general assumption than the usual logistic model. A density function might be assumed to be symmetric and unimodal, in a situation where stronger assumptions like normality might not be justified. A fit to a function using fewer assumptions will have more fidelity to the data. Perhaps more importantly, these shape-restricted fits may be used to test the validity of the parametric models, or to select from several candidate parametric models.  <br/>"
"0203243","Spline Smoothing and Nonparametric Regression","DMS","STATISTICS","08/01/2002","07/25/2002","Randall Eubank","TX","Texas A&M Research Foundation","Standard Grant","Grace Yang","07/31/2005","$194,889.00","","eubank@math.asu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","Proposal ID: DMS-0203243<br/>PI: Eubank<br/>Title: Spline smoothing and nonparametric regression<br/><br/>ABSTRACT <br/><br/>A number of nonparametric regression type problems are investigated. These problems are connected through the use of spline smoothing in their solution methodology. Specific problems that are studied include: 1) testing the lack-of-fit of a parametric regression model using a spline smoother in a setting where standard smoothing parameter consistency asymptotics do not hold under the null hypothesis, 2) estimation using spline smoothers in varying coefficient models, 3) variance estimation and testing for heteroscedasticity for partially linear models, 4) computational methods for nonlinear spline smoothing problems with both linear and nonlinear parameters, 5) adaptive selection of regularization parameters for spline smoothing of data from ill-posed integral equations and 6) computation and large sample properties of equality constrained local polynomial smoothers with applications to copula density estimation.<br/><br/>The problems that are investigated in this research project concern regression analysis which represents the standard statistical approach to studying relationships between variables. The classical approach to regression analysis assumes that the form of the relationship between a collection of variables is known apart from a few unknown parameters that must be estimated from the data. This project uses more modern techniques that employ flexible or nonparametric curve fitting methods to produce estimators as well as to assess the validity of parametric models. New estimation methodologies are developed for several settings which include time varying coefficient models and partially linear models. Time varying coefficient models provide a generalization of parametric models where the parameters in the regression relationship are allowed to evolve as a function of some other variable such as time. This type of model is useful in a number of settings such as for analyzing data from longitudinal case studies and for prediction of lottery sales as a function of jackpot level. Partially linear models provide a mix of parametric and nonparametric methods where the regression relationships for some of the variables can be modeled parametrically while others must be handled using flexible nonparametric techniques. This latter type of model has been found useful, for example, in modeling yield from agricultural field trials as a function of field fertility and for examining the utility of particular blood enzymes in pregnant women for prediction of future incidences of cancer. <br/>"
"0203884","Some Problems Related to Model Selection","DMS","STATISTICS","06/15/2002","02/14/2006","Gerda Claeskens","TX","Texas A&M Research Foundation","Standard Grant","Rong Chen","11/30/2006","$94,999.00","","gerda@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>PI: Gerda Claeskens<br/><br/>Proposal ID: DMS-0203884<br/><br/><br/>Title: Some problems related to model selection<br/><br/>The first major contribution of this project is a study of nonparametric tests of function fit in the context of both density estimation and regression, hereby generalizing existing results in several directions. The proposed apparatus can be applied to test the adequacy of any parametric family, subject to the usual conditions of regularity, and is also suitable for use in higher dimensions. The second research topic focuses specifically on estimation after model selection and introduces a general approach to frequentist model averaging. A third line of research addresses the problem of generalizing the methods mentioned above to the setting of missing data. This will require the development of new model selection criteria. <br/><br/>The goal of nonparametric testing is to construct an omnibus test, consistent against essentially any alternative model. Proposed tests are applicable to almost any parametric family of functions. There are several reasons why model selection criteria and their asymptotic properties merit further study, one of the most important ones being that only a minority of the past research provides a full integration of model selection into statistical inference. The traditional use of model selection methods in practice is to proceed as if the final selected model had been chosen a priori, without acknowledging the additional uncertainty introduced by model selection. A general framework is developed, in which several modeling and estimation strategies can be included and compared. Aspects of these problems are addressed first in the setting of complete data, and then in the general situation of missing data.   <br/><br/>"
"0322776","Finite Sample Performance of Multivariate Location and Scatter Estimators","DMS","STATISTICS","08/01/2002","03/18/2003","Yijun Zuo","MI","Michigan State University","Continuing Grant","Marianthi Markatou","07/31/2003","$1,947.00","","zuo@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","0000, OTHR","$0.00","TITLE:<br/><br/>Finite Sample Performance of Multivariate Location <br/>and Scatter Estimators<br/><br/>ABSTRACT:<br/><br/>This research develops new methodology and theory in the <br/>performance evaluation of multivariate location and scatter <br/>estimators. The research is to propose finite sample performance<br/>criteria of multivariate location and scatter estimators based on <br/>their ``tail behavior'', to investigate and assess the performance <br/>of existing and new multivariate location and scatter estimators <br/>with respect to the proposed criteria, and to provide guides to <br/>statistical practices in applications. <br/> <br/>Given an estimator of some unknown parameter, a natural question<br/>is -- how good is the estimator? or how should one measure its <br/>performance? These questions are fundamental in statistical <br/>estimation and inference. To answer these questions, various <br/>performance criteria have been proposed and studied. Among the <br/>existing performance criteria, asymptotic approaches including <br/>Fisher consistency and large sample normality are the most prevalent <br/>ones. These approaches have been based on the behavior of the <br/>estimator as the sample size approaches infinity. This, however, <br/>raises some serious concern about the relevancy of the asymptotic <br/>results in practice, where the sample size is always fixed and <br/>finite. The study of the finite sample performance of multivariate <br/>location and scatter estimators thus is not only practically <br/>significant but also theoretically interesting. In sharp contrast <br/>with the asymptotic approaches, in this research the performance <br/>of multivariate location and scatter estimators is studied for <br/>fixed and finite sample size.   <br/><br/>The research develops new methodology and valuable insights <br/>into the comparison of estimator performance particularly from <br/>robustness standpoint. Inherent connections of our finite sample <br/>performance measures with two most crucial and promising notions <br/>in the robust and nonparametric statistical analysis and inference, <br/>the finite sample breakdown point and the data depth (especially<br/>Tukey-Donoho halfspace depth), are to be explored and illuminated. <br/>It is to be shown that the estimators with high breakdown point or <br/>halfspace depth possess remarkable finite sample tail performance. <br/>Findings like this in the research offer new insights into and deepen <br/>our understanding of the notions of breakdown point and halfspace depth, <br/>and establish the important role of the finite sample tail behavior as a <br/>quantitative assessment of robustness of estimators. The research has <br/>profound impact on statistical applications in various disciplines of <br/>sciences such as social, behavior and life sciences, environmental and <br/>biology sciences, industry, and economics. The research suggests, <br/>for example, that location estimators with appealing tail performance <br/>(e.g. depth-based multivariate medians) should be preferred in practice<br/>to traditional least-squares estimators (e.g. the multivariate mean) <br/>for robustness against the influence of extremities of observations in <br/>multivariate data analysis. The research also benefits education through <br/>the training of graduate students and the incorporation of the developed <br/>methodology in statistics courses.<br/>"
"0234078","CAREER:  Statistical Depth Functions and their Applications","DMS","STATISTICS","06/01/2002","05/17/2006","Yijun Zuo","MI","Michigan State University","Continuing Grant","Gabor Szekely","05/31/2008","$320,000.00","","zuo@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","0000, 1045, 1187, OTHR","$0.00","Statistical depth functions have become increasingly pursued as a promising tool in robust and nonparametric multivariate data analysis and inference. This project is to conduct a systematic and thorough study of these functions and their applications. The objectives of this project include: 1)to establish bases and provide specific guidance for selection of depth functions and their induced estimators for practitioners, 2) to introduce depth associated practical inference procedures, 3) to deepen and extend existing depth applications while pushing depth methodology to new frontiers, 4) to develop fast and accurate algorithms and toolkits for the practical computing of depth functions and associated procedures, and 5) to incorporate research activities into the educational process and to engage the participation of underrepresented minorities in the proposed activities. <br/><br/>Simple one-dimensional statistics based on ordering have played such an important role in one-dimensional data analysis and their multi-dimensional analogues have been sought for years, without completely satisfactory results. The extension to higher dimensions of these one-dimensional statistics, such as the median, is difficult because there is no natural and unambiguous method of fully ordering or ranking multi-dimensional observations. Statistical depth functions are proving to be a very promising tool for ordering multi-dimensional observations. The main idea of depth functions is to provide from the ""deepest"" point a ""center-outward"" ordering of multi-dimensional observations. Multi-dimensional data ordering is not the only application of depth functions though. Depth functions have brought us new perspectives towards multidimensional exploratory data analysis and inference, and have been proven to have significant applications in disciplines ranging from industrial engineering to biomedical sciences. Research in depth theory and methodology, however, is still in its preliminary stage and a number of fundamental issues are yet to be addressed including: 1) depth functions have been introduced ad hoc in many areas, and in great variety, without regards as to whether they meet any particular set of criteria and without regard to a general mechanism to construct them, 2) a large number of depth induced estimators exist with little guidance for choosing among them, an obstacle both, to potential users and to those designing software packages, 3) computing depth functions and associated procedures is challenging-- without fast and accurate algorithms further developments of depth methodology can be hampered, 4) depth-associated inference procedures have not been developed in general, and applications of depth functions are yet to be further deepened, broadened, and pushed to new frontiers: and finally, 5) depth methodology has not been integrated into the educational process. The proposal will address all these issues. The project will 1) extend the areas of application and methodological advantages of  one-dimensional statistical procedures and methods based on ordering to the higher-dimensional context, 2) stimulate discovery and understanding within the field of depth theory and methodology, 3) advance nonparametric and robust multivariate exploratory data analysis built on the depth methodology, 4) promote inspired teaching and enthusiastic learning while broadening the participation of underrepresented groups, 5) establish a strong research and education program in statistics involving researchers in the depth community and train undergraduate and graduate students from various disciplines, and 6) build for the PI a firm foundation for a lifetime of integrated contributions to research and education.<br/>"
"0202387","Student Travel Grant for Student Participation in 2002 Summer Research Conference","DMS","STATISTICS","04/15/2002","04/10/2002","Jane Harvill","MS","Mississippi State University","Standard Grant","Marianthi Markatou","01/31/2003","$4,000.00","","harvill@math.msstate.edu","245 BARR AVE","MISSISSIPPI STATE","MS","39762","6623257404","MPS","1269","0000, OTHR","$0.00","<br/>This award provides funds to support the participation of graduate students in a summer research conference organized by the Southern Regional Council on Statistics (SRCOS) and the American Statistical Association. The conference will be held on June 2-5, 2002, in Natchez, Mississippi. The objectives of the conference are to bring together senior researchers, junior researchers and advanced graduate students to focus on several current research areas in statistics and to discuss some current and future issues in statistical education and training. Sessions are organized around the topics of Statistical Education and Biostatistics, Statistics for Dependent Data and Environmental Statistics. Leading researchers will present new developments in an environment that is conducive to the development of new human resources.<br/>"
"0202922","Applied Robust Statistics","DMS","STATISTICS","06/01/2002","03/28/2002","David Olive","IL","Southern Illinois University at Carbondale","Standard Grant","Xuming He","05/31/2004","$43,561.00","","dolive@siu.edu","900 S NORMAL AVE","CARBONDALE","IL","629014302","6184534540","MPS","1269","0000, OTHR","$0.00","This research considers the development of resistant procedures for visualizing regression data and the development of rigorous statistical theory for practical robust estimators. Regression is the study of the conditional distribution of the response variable given a vector of the predictor variables. Many of the most used statistical procedures, including multiple linear regression and generalized linear models, are special cases of regression. Existing methods for regression often make rather strong assumptions on the predictor distribution. If this distribution is skewed or if outliers are present, then regression graphics methodology such as ordinary least squares, sliced inverse regression, and principal Hessian directions may fail to give useful results. Of primary interest in this research is the question of how to visualize regression when the predictor distribution assumptions are violated. Previous research has shown that robust estimators can be useful for visualizing regression. Since robust estimators for which there is rigorous theory are generally impractical to compute, another focus of this research is to develop practical consistent robust estimators.<br/><br/>This research will lead to a better understanding of the increasingly complex high dimensional data sets collected for scientific, social and strategic purposes. Many methods using regression have been developed, and applications include biomedical research, predicting future observations based on previous data, and the analysis of economic and social data. Robust statistics combined with regression graphics has the potential to make simpler but more accurate models and diagnostics. New robust estimators are needed since robust methods that perform well on text book sized problems frequently fail if applied to larger high dimensional data sets that actually occur in practice.  <br/>"
"0204182","Weighted Empirical Likelihood","DMS","STATISTICS","08/01/2002","08/02/2002","Jian-Jian Ren","FL","The University of Central Florida Board of Trustees","Standard Grant","Grace Yang","07/31/2006","$101,334.00","","jjren@umd.edu","4000 CENTRAL FLORIDA BLVD","ORLANDO","FL","328168005","4078230387","MPS","1269","0000, OTHR","$0.00","Proposal ID: DMS-0204182<br/>PI: Jian-Jian Ren<br/>Title: Weighted empirical likelihood<br/><br/>Abstract <br/><br/>The objective of this research is to investigate the applications of a new likelihood function, called weighted empirical likelihood, in constructing confidence sets and tests for various important nonparametric or semiparametric statistical inference problems in survival analysis using different types of incomplete data, including right censored data, doubly censored data and interval censored data. Weighted empirical likelihood function is formulated in a unified form through the probability mass of the nonparametric maximum likelihood estimator for different types of incomplete data. The PI has shown that the high-order expansion of the log-likelihood <br/>ratio for a quite general class of statistics can be obtained in a unified way for various types of censored data aforementioned. Thus, with the help of the adjusted n out of n bootstrap, the weighted empirical likelihood ratio can be used to construct efficient confidence intervals or tests. <br/>The proposed procedure does not require the precise knowledge of the convergence rate of the statistic of interest, which is particularly appealing for interval censored data. All preliminary studies show that the desirable properties of weighted empirical likelihood method include accuracy, efficiency, generality, and independency of the precise knowledge of the convergence rate of the statistic of interest. In this research, the issues under consideration include: (a) Applications of weighted empirical likelihood in statistical inference problems with various types <br/>of incomplete data, including the construction of confidence intervals and tests associated with profile likelihood problems, accelerated life model, proportional hazards model, and logistic regression model, etc.; (b) Coverage accuracy of weighted empirical likelihood ratio confidence intervals; (c) Efficiency of weighted empirical likelihood inferences; (d) Comparison with alternative methods if they exist.<br/>  <br/>Incomplete data are frequently encountered in medical follow-up and reliability studies. Recently, statisticians are paying more attention to some more complicated types of incomplete data, such as doubly censored data, interval censored data, truncated data, etc., as these data occur in <br/>important clinical trials and scientific research. For instance, doubly censored data were encountered in a recent study of primary breast cancer, interval censored data were encountered in AIDS research, and doubly truncated data were encountered in astronomical research. Up to now, the statistical research on these more complicated types of incomplete data still generally lags behind that on right censored data.  Since Owen (1988), the empirical likelihood method has been developed to construct tests and confidence sets based on nonparametric likelihood ratio. Studies have shown that the empirical likelihood ratio inferences are of comparable accuracy to <br/>alternative methods. However, so far the applications of empirical likelihood method to censored data are relatively few and are mostly on right censored data. Examples show that for complicated types of incomplete data, such as interval censored data, the investigation of the limiting <br/>distribution of log-likelihood ratio can be quite difficult. In this context, aiming to provide solutions for various difficult problems that arise in medical and scientific research, this project intends to develop reliable statistical methods based on a new likelihood function, called weighted empirical likelihood function.<br/>"
"0204232","Power, Variability, and Optimality in Adaptive Designs","DMS","STATISTICS","07/15/2002","07/13/2002","Feifang Hu","VA","University of Virginia Main Campus","Standard Grant","Grace Yang","06/30/2006","$205,364.00","William Rosenberger","feifang@gwu.edu","1001 EMMET ST N","CHARLOTTESVILLE","VA","229034833","4349244270","MPS","1269","0000, OTHR","$0.00","Proposal ID: DMS-0204232<br/>PI: Feifang Hu<br/>Title: Power, variability, and optimality in adaptive designs<br/><br/>Abstract:<br/><br/>Adaptive designs use sequentially accruing data in allocation decisions to reach some objective.  In this proposal, the objective is based on an optimization criterion, such as minimizing the cost of an experiment.  The investigators use the power of a hypothesis test as a benchmark for comparisons of adaptive designs.  They explicitly derive the relationship between power and the design in terms of bias of the target allocation from the actual allocation and variation induced by the design.  For four classes of adaptive designs:  urn models, sequential maximum likelihood procedures, doubly adaptive biased coin designs, and treatment effect mappings, the investigators will uniquely unify the theory for easy comparison based on power, optimality, and variability.<br/><br/>Adaptive designs are useful in many scientific disciplines and have application in clinical research, industrial experiments, bioassay, to name a few areas.  The idea is to dynamically use sequentially accruing data in decisions for collecting future data in order to satisfy some objective, which could be minimizing the cost of an experiment, maximizing expected treatment successes in a clinical trial, etc.  The use of adaptive designs can improve efficiency of an experiment by incorporating current knowledge into design decisions.  Heretofore what has been unknown is the relationship of variability of the adaptive designs to efficiency of the experiment.  The investigators will develop guidelines that will allow direct comparison of efficiency of designs by exploring their variability.  The grant will involve both undergraduate and graduate students across two campuses and will lead to increased understanding of how to efficiently design costly or ethically demanding experiments.<br/>"
"0203086","Statisical Methods in Fast Functional MRI","DMS","STATISTICS","07/01/2002","04/09/2004","Larry Shepp","NJ","Rutgers University New Brunswick","Continuing Grant","Grace Yang","06/30/2005","$208,365.00","Cun-Hui Zhang","shepp@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","Statistical methods will be developed to sharply improve time-resolution for the recently proposed technique of functional magnetic resonance imaging (fMRI). The objective of the project is to improve the time-resolution of fMRI by sampling only a small fraction of the Fourier transform of the spin density, and using a prolate wavelet filter to approximately obtain an integral representing the total activity of the difference in susceptibility between task and pre-task, over various regions of interest in the brain at successive time -points. The cost for this is a decrease in spatial resolution. This space/time trade-off allows us to obtain, at high-time resolution, the total activity in specified regions of the brain, believed to process the specific stimulus/task, to learn or verify where the brain function takes place. Furthermore, the proposed methodologies is believed to be applicable to other types of MRI studies, especially magnetic resonance spectroscopy.<br/><br/>The proposal focuses on developing statistical methods and related theory for fast functional magnetic resonance (fMRI), to sharply improve the time resolution of present techniques via 3-dimensional sampling. The principal investigators and their collaborators have conducted a fMRI imaging experiment to answer the feasibility question. The results based on this small experiment are quite encouraging. Further experiments will conducted to confirm the preliminary results and improve upon technology. Fast fMRI is expected to have profound and far-reaching consequences in the understanding of brain function, a problem of central scientific interest at the present time<br/>"
"0204662","New Directions in Dimension Reduction","DMS","STATISTICS","07/01/2002","04/05/2004","Bing Li","PA","Pennsylvania State Univ University Park","Continuing Grant","Grace Yang","01/31/2006","$178,543.00","","bing@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0204662<br/>PI: Bing Li<br/><br/>This research will develop methods in dimension reduction, which aim at increased accuracy and a wider spectrum of applications. Specifically, the work will proceed in three main directions. (1) The classical formulation makes the conditional density in regression the target for dimension reduction. This does not take into consideration that in many applications the primary interest centers in the conditional mean. Moreover, the classical formulation requires homoskedasticity among predictors, which can be too restrictive for some problems. To address these issues the investigator proposes to reformulate the problem as reducing the dimensions of the predictors as they appear in the conditional mean. This will allow further dimension reduction, it will improve accuracy and remove the requirement for homoskedasticity. (2) Within the classical formulation one cannot handle categorical predictors, which occur frequently in practice. This research will broaden the proposed formulation so that it can handle such cases. (3) It is then possible and natural to combine these two new elements to further develop a more focused, and less restricted dimension reduction method for conditional means for regressions involving categorical predictors. <br/><br/>The methods of dimension reduction were introduced originally to provide a comprehensive graphical tool for exploratory data analysis. Recently, active developments are under way due to the rapid growth of computing power; this has dramatically increased the scope and dimensions of the collected data sets. Besides its important role as a graphic method, dimension reduction is particularly useful in problems where interest lies in identifying connections among the variables, such as classification and clustering. It is also useful when the dimension of a data point exceeds the total number of data points, which is typically the case for many scientific data sets, such as gene expression data. But the available dimension reduction methods have several limitations, such as assuming homogeneity between predictors and not be able to handle categorical predictors. This research will tackle these limitations of the current methodology. <br/><br/>"
"0203320","Statistical Inverse Problems and Point Process Methods in Combinatorics","DMS","STATISTICS","07/15/2002","07/12/2002","Jon Wellner","WA","University of Washington","Standard Grant","Grace Yang","06/30/2006","$225,001.00","","jaw@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>PI: Jon Wellner<br/>Proposal Number: DMS-0203320<br/><br/>This proposal deals with research on statistical inverse problems and point process methods in combinatorics. The first part of the proposal will involve work on non-standard asymptotics for likelihood ratio statistics and profile likelihoods, distribution theory for new limiting distributions, and new distribution theory for point processes. New computational algorithms will be investigated and comparisons of various competing algorithms for several inverse problems will be studied. Basic empirical process tools and methods will be developed and applied to statistical problems concerning semiparametric models and inverse problems. Applications include regression models for panel count data, bivariate interval censored data of several kinds, regression models for multivariate survival data, and studies of non- and semi-parametric maximum likelihood estimators used in AIDS research, two-phase data dependent designs, and animal carcinogenesis experiments.<br/><br/>The second part of the research will involve applications of point process methods to combinatorial problems, including study of the longest increasing sequence in a random permutation and related problems concerning random Young tableaux.<br/><br/> <br/>"
"0134264","CAREER:  Self-Similarity: Roadblock or Breakthrough?","DMS","STATISTICS","06/15/2002","11/14/2005","Tilmann Gneiting","WA","University of Washington","Continuing Grant","Grace Yang","05/31/2007","$300,000.00","","tilmann@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, 1045, 1187, OTHR","$0.00","Following Mandelbrot's seminal work, fractal based analyses of time series, profiles, and natural or man-made surfaces have found extensive applications in almost all scientific disciplines. The fractal dimension, D, of an object or data set is a roughness measure and routinely estimated in scientific studies. Long-memory dependence in time series or spatial data plays key roles in the discussion of issues such as global warming. It is associated with power-law correlations and quantified by the Hurst coefficient, H. In principle, fractal dimension, D, and Hurst coefficient, H, are independent of each other: fractal dimension is a local property, and long-memory dependence is a global characteristic. Nevertheless, the two notions have been linked through the celebrated relationship that indicates that the fractal dimension added to the Hurst coefficient equals n+1, for a self-similar surface in n-dimensional space. This relationship is based on the assumption of self-similarity, which has hardly ever been put to test. Recently developed stochastic models allow for any combination of fractal dimension, D, and Hurst coefficient, H, and therefore challenge the relationship and the role of self-similarity. Has the prevalence of self-similar model been a roadblock, keeping scientists from exploring and understanding a wide range of local and global behavior in complex systems? Or is it indeed a breakthrough, which lets scientists focus on models for which the above relationship holds, while other combinations of D and H are physically meaningless? This research develops tests for self-similarity and thereby addresses these problems. Along the way, new insight into the behavior of statistical estimators of fractal dimension and Hurst coefficient, software for the fast and exact simulation of self-similar and related classes of random processes, an analysis of topographic data for Oregon and Washington state, and new theoretical results for stationary random functions will emerge. Visualization tools and Freshman Seminars at the University of Washington will introduce undergraduate students to mathematical and statistical modeling, to fractals, long-memory dependence, and self-similarity, and to the natural beauty and diversity of the Pacific Northwest. <br/><br/>Many natural phenomena such as mountains, clouds, ferns and rocks have features that look alike across scales. A little piece might indeed resemble the whole if the scale is not disclosed. Over the past decades, this notion of self-similarity has emerged as a popular and fruitful theme in a vast number of scientific studies. This research develops statistical tests, which allow scientists to assess whether their observations and data are compatible with classical assumptions of self-similarity. Alternative models allow for distinct behaviors at different scales and might inspire and encourage new theories across disciplines. The research is paired with the development of visualization and simulation tools as well as Freshman Seminars, through which students are introduced to mathematical and statistical modeling, to fractals, long-memory dependence, and self-similarity, and to the natural beauty and diversity of the Pacific Northwest. <br/>"
"0204642","Collaborative Research:  Theory and Methods for Nonparametric Survey Regression Estimation","DMS","STATISTICS, Methodology, Measuremt & Stats","08/15/2002","08/14/2002","Jean Opsomer","IA","Iowa State University","Standard Grant","Grace Yang","07/31/2005","$65,038.00","","jopsomer@stat.colostate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269, 1333","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0204642 & DMS-0204531 (Collaborative)<br/>PIs: Opsomer/Briedt<br/><br/>Title: Collaborative Research: Theory and Methods for Nonparametric Survey Regression Estimation<br/><br/>This research project develops new methods for the efficient use of auxiliary information in complex surveys, based on nonparametric regression techniques. Current practice relies on parametric regression techniques, which have good efficiency if the regression model is well specified, and which have a number of appealing operational features. The nonparametric techniques share these operational features, lose little efficiency when the parametric specification is correct, and gain efficiency when the parametric specification is incorrect. The project increases the scope of applicability of the nonparametric regression estimation approach, by considering complex survey designs, varying types of auxiliary information, and alternative smoothing techniques. Specifically, the project investigates multi-stage surveys with cluster or element-level auxiliary information; multivariate auxiliary information; and alternative smoothing techniques. Parametric and nonparametric techniques are blended using semiparametric additive models to provide a flexible tool for use in complex surveys. <br/><br/>Large-scale surveys are used to collect data in a wide range of fields, from studies of human populations to inventories of natural resources. Information external to the survey, such as administrative records or remote sensing, is often available. This research project makes it possible to incorporate auxiliary information easily and effectively into survey estimates, by using nonparametric regression methods. Nonparametric regression, sometimes referred to as smoothing, is widely used in other areas of statistics, but its use in survey estimation has been limited so far. The investigators show that incorporating auxiliary information into survey estimation through nonparametric regression can improve the precision of the surveys, often at reduced costs. <br/>"
"0208301","International Conference in Reliability and Survival Analysis 2003","DMS","STATISTICS","07/15/2002","07/12/2002","Edsel Pena","SC","University South Carolina Research Foundation","Standard Grant","John Stufken","07/31/2003","$15,000.00","James Lynch, William Padgett","pena@stat.sc.edu","915 BULL ST","COLUMBIA","SC","292084009","8037777093","MPS","1269","0000, 9150, OTHR","$0.00","Proposal ID DMS-0208301<br/>International Conference in<br/>Reliability and Survival Analysis 2003<br/><br/>(PI: Edsel Pena, Ph.D.; Co-PIs: James Lynch, Ph.D., William Padgett, Ph. D.)<br/><br/>Abstract<br/><br/>The Department of Statistics, and its Center for Reliability and Quality Sciences, at the University of South Carolina in Columbia will organize the International Conference on Reliability and Survival Analysis 2003 (ICRSA2003) to be held on the University's historic campus from May 21-24, 2003. The objectives of this conference are to bring together senior and established researchers and young and promising researchers from around the world who are working in the theoretical and applied aspects of reliability, survival analysis, and related topics in order to share recent advances and current research, as well as future research trends, in these areas. Through this conference, it is expected that interaction and/or collaborative work among reliability and survival analysis researchers will be fostered, which will enhance and accelerate developments in both areas. As it currently exists, the reliability and survival analysis disciplines have developed somewhat separately, but there is much to be gained by recognizing similarities between them since both deal with the stochastic modeling and statistical analysis of failure-time data, usually in the presence of incomplete information. This conference, which will be the first since 1998 to be held in the United States focused simultaneously on reliability and survival analysis, will enable many researchers in the United States to participate, especially new researchers and graduate students, as the cost and accessibility for participation is more viable.  Plenary talks and invited talks will be given by leading researchers in reliability and survival analysis, and contributed talks and poster sessions will be solicited from advanced graduate students and new researchers, especially from under-represented groups. The funding provided by the National Science Foundation will enable partial financial support of the plenary speakers, new researchers, and graduate students. New researchers and graduate students from under-represented groups will be particularly encouraged to seek funding from this NSF support.<br/><br/>"
"0140587","Collaborative Research: a Focused Research Group on Multiscale Geometric Analysis -- Theory, Tools, and Applications","DMS","STATISTICS","08/15/2002","08/05/2002","Xiaoming Huo","GA","Georgia Tech Research Corporation","Standard Grant","Grace Yang","07/31/2005","$153,440.00","","xiaoming@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","0000, 1616, OTHR","$0.00","Proposal IDs: DMS-0140698, DMS-0140540, DMS-0140587, DMS-0140623<br/>PIs: Donoho, Candes, Huo and Jones<br/>TITLE: A Focused Research Group on Multiscale Geometric Analysis--Theory,<br/>Tools, Applications<br/><br/>Abstract<br/><br/>An interdisciplinary team, bridging Harmonic Analysis, Statistics, Image Analysis and Astronomy, proposes a united effort to exploit and extend recent breakthroughs in computational harmonic analysis. The team will consolidate several scattered advances into a unified body of theory and methods to be called Multiscale Geometric Analysis, which can probe the fine structure of 2-, 3- and higher- dimensional functions and point clouds, able to isolate and manipulate intermediate-dimensional phenomena. Simple examples include edges in 2-d images, filaments and sheets in 3-d point catalogs. Characterizing such intermediate-dimensional phenomena is essential for fundamental progress in a wide range of problem areas (including 2-d and 3-d imaging) where traditional multiscale methods have now run their course. Our united effort has three main outcomes. 1. Theory. Coherent, comprehensive knowledge, showing what can and cannot be accomplished with computational harmonic analysis. 2. Tools. A wide range of practical MGA algorithms and a unified, publicly available software environment - BeamLab - deploying them. 3. Applications. Our main initial focus will be on the analysis of 3-d point catalogs in astronomy, such as those coming on-line soon from the Sloan Digital Sky Survey. We know a priori that the data contain filaments and sheets, embedded in a scattered background, and MGA provides a decisive set of tools to resolve the structural properties of such catalogs, far more sensitive and more comprehensible than any tools currently available for such analysis.<br/><br/>Many new kinds of massive databases are being created in our era's revolutionary transition to a data-rich society.  Many of these databases contain geometric structures such as surfaces, and filaments, albeit in a sometimes hidden manner. Databases of this sort can include galaxy catalogs in astronomy -- in which the filaments and sheets are predicted by various theories of universe formation -- 3-D scanning of faces and other objects -- in which filaments and surfaces are caused by structures such as skin and hair -- and statistical databases in which curves and surfaces arise from existence of predictable patterns. This Focused Research Group on Multiscale Geometric Analysis is a research effort bringing together mathematicians, statisticians, image analysts and astronomers to create new tools and theories to help extract geometric structure and meaning from such databases.  The participants are skilled at multiscale analysis, which will be a central organizational principle. We expect multiscale methods to have an impact here comparable to what wavelets have had over the last twenty years in many other problems in science and technology.<br/>"
"0203798","Topics in Statistics with Applications","DMS","STATISTICS","08/01/2002","08/15/2002","Zhiliang Ying","NY","Columbia University","Standard Grant","Grace Yang","07/31/2006","$113,841.00","","zying@stat.columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","0000, OTHR","$0.00","Proposal ID: 0203798<br/>PI: Zhiliang Ying<br/>Title: Topics in statistics with applications<br/><br/>Abstract<br/><br/>This project addresses issues related to semiparametric regression models with censored data as well as testing and estimation of item differential functioning arising from IRT-based standardized tests. The investigator and his collaborators develop new algorithms for obtaining parameter estimators for the accelerated failure time regression. These algorithms are based on linear programming which is easy to implement. The estimators are shown to be asymptotically normal, with the limiting variance estimated via a simple resampling method. The approach is extended to censored quantile regression models, which are common in econometrics literature. They study the class of semiparametric transformation models via estimating equations, resulting in a general method for parameter estimation and inference, justified by a large sample theory. They develop a new approach for adaptive standardized tests to detect possible item differential functioning via a generalized logistic regression model with measurement errors. <br/><br/>The statistical problems dealt with here are motivated by applications in biomedical sciences, sociology, economics, marketing, psychology and education. The project develops appropriate statistical models, computer algorithms and mathematical theory. The results can be used to facilitate design of clinical trials and epidemiological studies, particularly in studies of cancer, cardiovascular diseases and AIDS, to analyze unemployment duration data, to quantify consumer responses to marketing strategies and to screen out items for use in large-scale standardized tests that are tilted against certain group or groups to ensure fairness<br/>"
"0204247","Degradation Modeling, Reliability Analysis, and Quality Improvement","DMS","STATISTICS, MANFG ENTERPRISE SYSTEMS","08/01/2002","07/25/2002","Vijayan Nair","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Grace Yang","07/31/2006","$259,477.00","","vnn@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269, 1786","0000, 9147, MANU, OTHR","$0.00","Proposal ID: DMS-0204247<br/>PI: Vijayan Nair<br/>Title: Degradation Modeling, Reliability Analysis, and Quality Improvement<br/><br/>Abstract<br/><br/>Degradation data are a very rich source of reliability information and offer many advantages over the analysis of time-to-failure data. This project will develop a flexible class of models for analyzing degradation data and related reliability inference. These results will be used to obtain efficient methods for the design and analysis of accelerated tests and reliability improvement experiments. In this work, time-to-failure is defined as the level crossing (first-passage time) of a specified degradation threshold. The first part of the project will consider models based on diffusion processes for analyzing degradation data with continuous sample paths. These models can accommodate a variety of degradation rates and shapes. They also lead naturally to a wide class of time-to-failure distributions. The inverse Gaussian distribution plays a central role, similar to the exponential distribution with constant hazard rates. The second part will study a class of degradation-based models for repairable systems data that is quite analogous to non-homogeneous Poisson processes with failure data. Multi-state degradation models will also be considered. This work is an interesting generalization of the formulation in traditional statistical process control. Degradation data allow for more informative accelerated tests and reliability improvement studies. Several research topics on design of accelerated degradation tests, analysis of data from reliability improvement experiments, and robust design studies will be pursued.<br/><br/>There has been tremendous emphasis on quality and reliability improvement in industry, driven by global competition and increasing customer expectations. There is also continued pressure to reduce product development costs and cycle times. Design, development, and manufacturing of highly-reliable products in this environment raise many challenges. The focus within the reliability area has traditionally been on the collection and analysis of time-to-failure data. High reliability implies few failures, so reliability estimation and improvement for product and process design can be extremely difficult. Fortunately, recent advances in sensing and measurement technologies are making it feasible to collect extensive amounts of data on degradation and other performance measures associated with components, systems, and manufacturing processes. However, the lack of flexible models and methods inference has been a major deterrent to the widespread use of degradation data for reliability analysis. This project will develop new models and methods for analyzing reliability data and use them for quality improvement.<br/>"
"0205136","DIMACS Workshop:  Data Depth: Robust Multivariate Analysis, Computational Geometry and Applications","DMS","TOPOLOGY, STATISTICS","07/01/2002","05/30/2002","Fred Roberts","NJ","Rutgers University New Brunswick","Standard Grant"," Shulamith T. Gross","09/30/2003","$18,000.00","","froberts@dimacs.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1267, 1269","0000, OTHR","$0.00","Support is provided for the DIMACS workshop ""Data depth: Robust multivariate analysis, computational geometry and applications"" at Rutgers University. The workshop brings together theoretical and applied statisticians and computer scientists to promote and further the use and development of depth functions in nonparametric multivariate analysis. Data depth has applications in and connections with a number of different research areas, such as aviation safety, data compression, image analysis, data mining and analysis of genetic data.<br/><br/>The workshop addresses significant open conceptual issues and establishes perspective on applications. It also sets primary directions for further research, emphasizing the role of computational geometry in understanding concepts of and developing algorithms for data depth.<br/>"
"0203724","Mixed Model Selection: Theory and Application","DMS","STATISTICS","08/01/2002","08/09/2002","Jonnagadda Rao","OH","Case Western Reserve University","Standard Grant","Grace Yang","07/31/2006","$49,607.00","","js-rao@umn.edu","10900 EUCLID AVE","CLEVELAND","OH","441061712","2163684510","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0203676 & 0203724<br/><br/>PIs: Jiang/Rao<br/><br/>This research involves development of model selection procedures for linear and generalized linear mixed models. The focus of this research will be on studying asymptotic properties and finite sample performance of the procedures, which will include comparisons to commonly used ad hoc methods. The research will also develop applications of the selection methodology to selecting factors in longitudinal data, genetic screening of genes related to quantitative traits of interest, and to model selection in small area estimation problems from surveys. In addition, the methods developed will be made accessible through the development of freely available software. The research will also improve work recently completed by the investigators where the conditions for consistent factor selection in linear mixed models were established. These improvements will include developing a more efficient method for linear mixed model selection, and studying some adaptive procedures. <br/><br/>Linear and generalized linear mixed models are important classes of models which allow relaxation of standard assumptions like independence or homogeneity of variances of observations, and take into account more complicated data structures in quite general ways. Typical applications include repeated measurements made on a patient over time or screening of candidate genes that might be related to a quantitative trait of interest. Selecting which factors should or should not be in the model can be of importance for model inference and predictions, yet little if anything has been developed to formally study this problem. This research attempts to fill this important gap in the field of developing new procedures for correct factor selection from a theoretical perspective, and evaluating real-world performance via extensive simulations. Another important component of this research is to apply it in a variety of real-world settings including longitudinal data, genetic screening, and small area estimation in survey sampling. <br/>"
"0140623","Collaborative Research:  A Focused Research Group on Multiscale Geometric Analysis: Theory, Tools, Applications","DMS","STATISTICS","08/15/2002","08/05/2002","Peter Jones","CT","Yale University","Standard Grant","Grace Yang","07/31/2005","$167,998.00","","jones-peter@math.yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","0000, 1616, OTHR","$0.00","Proposal IDs: DMS-0140698, DMS-0140540, DMS-0140587, DMS-0140623<br/>PIs: Donoho, Candes, Huo and Jones<br/>TITLE: A Focused Research Group on Multiscale Geometric Analysis--Theory,<br/>Tools, Applications<br/><br/>Abstract<br/><br/>An interdisciplinary team, bridging Harmonic Analysis, Statistics, Image Analysis and Astronomy, proposes a united effort to exploit and extend recent breakthroughs in computational harmonic analysis. The team will consolidate several scattered advances into a unified body of theory and methods to be called Multiscale Geometric Analysis, which can probe the fine structure of 2-, 3- and higher- dimensional functions and point clouds, able to isolate and manipulate intermediate-dimensional phenomena. Simple examples include edges in 2-d images, filaments and sheets in 3-d point catalogs. Characterizing such intermediate-dimensional phenomena is essential for fundamental progress in a wide range of problem areas (including 2-d and 3-d imaging) where traditional multiscale methods have now run their course. Our united effort has three main outcomes. 1. Theory. Coherent, comprehensive knowledge, showing what can and cannot be accomplished with computational harmonic analysis. 2. Tools. A wide range of practical MGA algorithms and a unified, publicly available software environment - BeamLab - deploying them. 3. Applications. Our main initial focus will be on the analysis of 3-d point catalogs in astronomy, such as those coming on-line soon from the Sloan Digital Sky Survey. We know a priori that the data contain filaments and sheets, embedded in a scattered background, and MGA provides a decisive set of tools to resolve the structural properties of such catalogs, far more sensitive and more comprehensible than any tools currently available for such analysis.<br/><br/>Many new kinds of massive databases are being created in our era's revolutionary transition to a data-rich society.  Many of these databases contain geometric structures such as surfaces, and filaments, albeit in a sometimes hidden manner. Databases of this sort can include galaxy catalogs in astronomy -- in which the filaments and sheets are predicted by various theories of universe formation -- 3-D scanning of faces and other objects -- in which filaments and surfaces are caused by structures such as skin and hair -- and statistical databases in which curves and surfaces arise from existence of predictable patterns. This Focused Research Group on Multiscale Geometric Analysis is a research effort bringing together mathematicians, statisticians, image analysts and astronomers to create new tools and theories to help extract geometric structure and meaning from such databases.  The participants are skilled at multiscale analysis, which will be a central organizational principle. We expect multiscale methods to have an impact here comparable to what wavelets have had over the last twenty years in many other problems in science and technology.<br/>"
"0134628","CAREER:  Statistical Depth Functions and their Applications","DMS","STATISTICS","06/01/2002","02/21/2002","Yijun Zuo","AZ","Arizona State University","Continuing Grant","Grace Yang","07/31/2002","$120,000.00","","zuo@msu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1269","0000, 1045, 1187, OTHR","$0.00","Statistical depth functions have become increasingly pursued as a promising tool in robust and nonparametric multivariate data analysis and inference. This project is to conduct a systematic and thorough study of these functions and their applications. The objectives of this project include: 1)to establish bases and provide specific guidance for selection of depth functions and their induced estimators for practitioners, 2) to introduce depth associated practical inference procedures, 3) to deepen and extend existing depth applications while pushing depth methodology to new frontiers, 4) to develop fast and accurate algorithms and toolkits for the practical computing of depth functions and associated procedures, and 5) to incorporate research activities into the educational process and to engage the participation of underrepresented minorities in the proposed activities. <br/><br/>Simple one-dimensional statistics based on ordering have played such an important role in one-dimensional data analysis and their multi-dimensional analogues have been sought for years, without completely satisfactory results. The extension to higher dimensions of these one-dimensional statistics, such as the median, is difficult because there is no natural and unambiguous method of fully ordering or ranking multi-dimensional observations. Statistical depth functions are proving to be a very promising tool for ordering multi-dimensional observations. The main idea of depth functions is to provide from the ""deepest"" point a ""center-outward"" ordering of multi-dimensional observations. Multi-dimensional data ordering is not the only application of depth functions though. Depth functions have brought us new perspectives towards multidimensional exploratory data analysis and inference, and have been proven to have significant applications in disciplines ranging from industrial engineering to biomedical sciences. Research in depth theory and methodology, however, is still in its preliminary stage and a number of fundamental issues are yet to be addressed including: 1) depth functions have been introduced ad hoc in many areas, and in great variety, without regards as to whether they meet any particular set of criteria and without regard to a general mechanism to construct them, 2) a large number of depth induced estimators exist with little guidance for choosing among them, an obstacle both, to potential users and to those designing software packages, 3) computing depth functions and associated procedures is challenging-- without fast and accurate algorithms further developments of depth methodology can be hampered, 4) depth-associated inference procedures have not been developed in general, and applications of depth functions are yet to be further deepened, broadened, and pushed to new frontiers: and finally, 5) depth methodology has not been integrated into the educational process. The proposal will address all these issues. The project will 1) extend the areas of application and methodological advantages of  one-dimensional statistical procedures and methods based on ordering to the higher-dimensional context, 2) stimulate discovery and understanding within the field of depth theory and methodology, 3) advance nonparametric and robust multivariate exploratory data analysis built on the depth methodology, 4) promote inspired teaching and enthusiastic learning while broadening the participation of underrepresented groups, 5) establish a strong research and education program in statistics involving researchers in the depth community and train undergraduate and graduate students from various disciplines, and 6) build for the PI a firm foundation for a lifetime of integrated contributions to research and education.<br/>"
"0204297","Robust Statistics for Correlated Data","DMS","STATISTICS","08/01/2002","09/28/2005","Marc Genton","NC","North Carolina State University","Continuing Grant","Rong Chen","07/31/2006","$179,186.00","Dennis Boos, Leonard Stefanski","genton@stat.tamu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0204297<br/>PI: M. Genton<br/><br/>Title: Robust Statistics For Correlated Data<br/><br/>This research focuses on two main new ideas. First, the concept of breakdown point is generalized to correlated observations by defining the critical region of the badness measure implicitly rather than explicitly, thus covering situations with both uncorrelated and correlated observations. Secondly, a new method to robustify likelihoods is introduced that can be applied in any situation where a parametric likelihood is available. This new approach is intimately connected to measurement error models and reduces to well-known contaminated normal likelihoods in location-scale independent, identically distributed settings. This new approach will be used to robustify many of the commonly used methods in spatial data settings.<br/><br/>The applications of this research include real data sources from environmental sciences, namely ozone concentrations and fine particulate matter in the U.S. This work will have impact on both, theoretical statisticians and practitioners of spatio-temporal statistics, and hence will foster collaboration between academia and industry. It aims to provide more reliable analyses of spatio-temporal data sets, and thus contribute to a better understanding of physical processes governing atmospherically-transported pollutants. In particular, it will provide more reliable evaluations of the effectiveness of the legislated emission reductions mandated in the Clean Air Act Amendments of 1990. Moreover, this project will support the teaching and promotion of robust statistics and spatio-temporal statistics at North Carolina State University.<br/>"
"0204612","Flexible Statistical Modelling","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","08/01/2002","08/18/2003","Trevor Hastie","CA","Stanford University","Continuing Grant","Rong Chen","07/31/2005","$238,000.00","","hastie@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1260, 1269","0000, OTHR","$0.00","Proposal ID: DMS-0204612<br/>PI: Trevor Hastie<br/>Title: Flexible statistical modeling<br/><br/>Abstract<br/><br/>The investigator and his students will study the popular support vector machines from the viewpoint of the traditional logistic regression model (with regularization). While the latter exhibits very similar generalization performance to the SVM, it has several advantages: it estimates class probabilities, and generalizes to multiclass problems seamlessly. This and several other even simpler approaches will be developed for the classification of gene expression arrays.<br/><br/>This proposal will thus develop useful models for making class predictions in a variety of high-dimensional situations, in particular for gene expression arrays. While the science journals abound with exotic statistical and computational algorithms for use in this fashionable domain, this investigator and his colleagues firmly believe that some very simple modifications of classical approaches perform as well or better, and are easier to understand.<br/>"
"0204563","Estimating the Structural Dimension of Regressions","DMS","STATISTICS","07/01/2002","08/24/2005","Efstathia Bura","DC","George Washington University","Standard Grant","Grace Yang","12/31/2006","$89,845.00","","ebura@gwu.edu","1918 F ST NW","WASHINGTON","DC","200520042","2029940728","MPS","1269","0000, OTHR","$0.00","Abstract (DMS-0204563)<br/>PI: E. Bura<br/><br/>Title: ESTIMATING THE STRUCTURAL DIMENSION OF REGRESSIONS<br/><br/>Traditional approaches to reducing dimension in a regression context focus on modeling: a sequence of models are fitted and associated diagnostic tools that may lead to a reduction of the dimension of the predictors a posteriori are used in order to select the most reliable yet simplest model. A different approach is taken in this proposal where the goal is to investigate and develop methods of reducing the dimension of the regressor vector at the outset of the analysis without applying any a priori fitting or model selection procedures. The methods proposed in this project use inverse regression as a tool to estimate the structural dimension of a regression, which is defined to be the dimension of the subspace generated by a sufficient number of linear projections of the regressor vector. Furthermore, the case of intrinsically nonlinear relationship between response and regressors will be considered. A ""translation"" of the nonlinear manifold of the regressors to an equivalent linear subspace (equivalent in the sense that all information on the response is retained) is proposed. The research should make a significant contribution to the available inferential tools and software for analyzing high-dimensional data. The goals of the proposed research are (a) to extend and generalize existing parametric-based dimension reduction methodology to nonparametric techniques, (b) to apply the developed methods and devise new ones in order to fully estimate the structural dimension of a regression and not only a lower bound on the full dimension, (c) to generalize the methods so that they apply to nonlinear manifolds of the regressor vector, (d) to develop state-of-the-art implementation computer code, which will be freely available to the research community, and (e) to compare the proposed dimension reduction techniques to the existing ones using simulated and real world data sets.<br/><br/><br/>High-dimensional data have become increasingly common in practically all scientific and business related fields. Modeling such data is challenging, mostly because of our inability to visualize them. Consequently, it is practically imperative to reduce the dimension of the input data prior to any attempts at modeling. Dimension reduction is especially relevant and appealing in the era of cheap and easily available computing power and technology, where huge multivariate data sets are collected or continuously generated and accumulated. The proposed research will extend existing and develop new technology for reducing the complexity of the input data at the outset of the modeling process.<br/><br/><br/>"
"0204532","Efficient and Robust Designs Based on Ordered Units","DMS","STATISTICS","08/01/2002","04/29/2004","Dibyen Majumdar","IL","University of Illinois at Chicago","Continuing Grant","Grace Yang","07/31/2006","$118,327.00","","dibyen@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0204532<br/>PI: Dibyen Majumdar<br/><br/>Title: Efficient and Robust Designs Based on Ordered Units<br/><br/>The goal of this project is to find efficient designs for experiments conducted over time to compare different treatments, varieties of objects or methods. A substantial part of the project deals with crossover experiments, where it often happens that subjects drop out prior to the conclusion of the experiment, thereby reducing the information content of the data, or even jeopardizing the validity of the conclusions. Here the aim is to identify efficient designs that are largely robust to subject dropout. Another part of the project deals with experiments where a trend over time must be accounted for in the model and analysis. Here the aim is to identify efficient designs that are flexible enough to accommodate a wide variety of experimental conditions and models that arise in real applications.<br/><br/>Proper statistical design is a critical aspect of scientific experiments; it ensures that the conclusions, based on observed data, remain valid in the larger context. This project deals with experiments in a variety of fields, including biomedical, agricultural and industrial experiments, that are conducted over time. As the experiment progresses, uncontrollable factors could influence the outcome. The aim of this project is to develop designs that perform well in diverse experimental conditions as well as designs that are robust against the inadvertent loss of experimental subjects before the conclusion of the experiment.<br/> <br/>"
"0218759","ITR:    Bayesian Models Linking Web Site Structure and Usage","DMS","STATISTICS","10/01/2002","01/03/2006","Alan Karr","NC","National Institute of Statistical Sciences","Standard Grant","Rong Chen","12/31/2006","$270,000.00","Ashish Sanil","karr@rti.org","19 TW ALEXANDER DR","RESEARCH TRIANGLE PARK","NC","277090152","9196859300","MPS","1269","0000, 1686, OTHR","$0.00","Abstract<br/><br/>PI: Alan Karr<br/><br/>Proposal Number: ITR-0218759<br/><br/><br/>Despite its ubiquity, the World Wide Web is poorly understood. As a consequence, many sites are difficult to navigate, hard to use and have confusing structure, to the extent that users may be unable to find content and abandon the site. Essential needs are to relate user behavior to Web site structure; to compare site usage at different times, or for different classes of users; segmentation of sessions; quantification of inter-relationships among pages; and prediction of user behavior, including forecasts, for example, of the economic impact of promotional campaigns. The ultimate impact is more efficient Web sites that serve users more effectively. <br/><br/>This research will create a set of four increasingly complex, but scalable, Bayesian models that relate the usage (specifically, user page transitions) of a Web site to its structure. It will apply, validate and refine the models and use real data from four qualitatively different Web sites, an E-commerce site, a site operated by a large financial institution, a content site and an information site. The models are scalable because the destinations from a given page are classes of pages that mirror the tree structure of the site, rather than individual pages. Examples are the parent, children and siblings of a page. All four models assume Dirichlet prior distributions for transitions from each page. The first three employ very aggregated classes of transitions, and differ according to whether the transition distributions and the priors are the same for all pages. The fourth model disaggregates the ""child"" and ""sibling"" destinations. Calculation of posterior distributions varies in difficulty: some are available in closed form, while others require intensive Markov chain Monte Carlo computation. In addition rigorous model assessment will provide insight into what level of aggregation is appropriate to which analyses of Web data.<br/>"
"0140540","Collaborative Research: a Focused Research Group on Multiscale Geometric Analysis -- Theory, Tools, and Applications","DMS","STATISTICS","08/15/2002","08/21/2002","Emmanuel Candes","CA","California Institute of Technology","Continuing Grant","Grace Yang","07/31/2005","$143,518.00","","candes@stanford.edu","1200 E CALIFORNIA BLVD","PASADENA","CA","911250001","6263956219","MPS","1269","0000, 1616, OTHR","$0.00","Proposal IDs: DMS-0140698, DMS-0140540, DMS-0140587, DMS-0140623<br/>PIs: Donoho, Candes, Huo and Jones<br/>TITLE: A Focused Research Group on Multiscale Geometric Analysis--Theory,<br/>Tools, Applications<br/><br/>Abstract<br/><br/>An interdisciplinary team, bridging Harmonic Analysis, Statistics, Image Analysis and Astronomy, proposes a united effort to exploit and extend recent breakthroughs in computational harmonic analysis. The team will consolidate several scattered advances into a unified body of theory and methods to be called Multiscale Geometric Analysis, which can probe the fine structure of 2-, 3- and higher- dimensional functions and point clouds, able to isolate and manipulate intermediate-dimensional phenomena. Simple examples include edges in 2-d images, filaments and sheets in 3-d point catalogs. Characterizing such intermediate-dimensional phenomena is essential for fundamental progress in a wide range of problem areas (including 2-d and 3-d imaging) where traditional multiscale methods have now run their course. Our united effort has three main outcomes. 1. Theory. Coherent, comprehensive knowledge, showing what can and cannot be accomplished with computational harmonic analysis. 2. Tools. A wide range of practical MGA algorithms and a unified, publicly available software environment - BeamLab - deploying them. 3. Applications. Our main initial focus will be on the analysis of 3-d point catalogs in astronomy, such as those coming on-line soon from the Sloan Digital Sky Survey. We know a priori that the data contain filaments and sheets, embedded in a scattered background, and MGA provides a decisive set of tools to resolve the structural properties of such catalogs, far more sensitive and more comprehensible than any tools currently available for such analysis.<br/><br/>Many new kinds of massive databases are being created in our era's revolutionary transition to a data-rich society.  Many of these databases contain geometric structures such as surfaces, and filaments, albeit in a sometimes hidden manner. Databases of this sort can include galaxy catalogs in astronomy -- in which the filaments and sheets are predicted by various theories of universe formation -- 3-D scanning of faces and other objects -- in which filaments and surfaces are caused by structures such as skin and hair -- and statistical databases in which curves and surfaces arise from existence of predictable patterns. This Focused Research Group on Multiscale Geometric Analysis is a research effort bringing together mathematicians, statisticians, image analysts and astronomers to create new tools and theories to help extract geometric structure and meaning from such databases.  The participants are skilled at multiscale analysis, which will be a central organizational principle. We expect multiscale methods to have an impact here comparable to what wavelets have had over the last twenty years in many other problems in science and technology.<br/>"
"0204690","Statistical Models of Biopolymer Sequence and Folding","DMS","STATISTICS, BE: NON-ANNOUNCEMENT RESEARCH","08/01/2002","07/30/2002","Scott Schmidler","NC","Duke University","Standard Grant","Grace Yang","07/31/2005","$165,000.00","","schmidler@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269, 1629","0000, OTHR","$0.00","Proposal ID: 0204690<br/>PI: Scott Schmidler<br/>Title: Statistical models of biopolymer sequence and folding<br/><br/>Abstract:<br/><br/>This research involves development of new probabilistic and Bayesian statistical models for the analysis of biopolymer sequences.  Emphasis is on predictive modeling of proteins and RNA.  Models for sequences of random variables with complex short- and long-range interaction structure are developed and explored.  Particular focus is placed unifying statistical models estimated from data with statistical mechanical models of polymer folding estimated via experimental parameter measurement.  Targeted applications include protein structure prediction, protein folding kinetics, and protein-RNA binding. Statistical methodology development focuses on connecting statistical models for sequence analysis and change-point problems, including graphical Markov models and random fields, to statistical mechanical models of polymer folding, especially on biopolymers (proteins and RNA), to develop predictive theories.  An additional core component of this research program concerns development of computational methodology for probabilistic inference in these models, including novel Markov chain Monte Carlo (MCMC) algorithms for multi-modal distributions and rough energy landscapes.<br/><br/>Modern research in the molecular biosciences and biomedicine relies increasingly on both computational modeling and analysis of large collections of experimental data.  This research concerns development of novel and unified methods for combining these areas.  This work leverages physical models to develop improved methods for statistical data analysis, and uses statistical methodology for improving predictive accuracy of physical models.  The focus is on analysis of protein and nucleic acid sequences and structures being generated by high-throughput whole-genome analyses.  These advances will provide important new statistical methodology for computational biology, as well as provide domain scientists with improved tools for data analysis and predictive modeling.<br/>"
"0203942","Problems in Ill-posed Statistical Inference","DMS","STATISTICS","08/15/2002","08/14/2002","Frits Ruymgaart","TX","Texas Tech University","Standard Grant","Grace Yang","07/31/2006","$115,000.00","","ruymg@math.ttu.edu","2500 BROADWAY","LUBBOCK","TX","79409","8067423884","MPS","1269","0000, OTHR","$0.00","Proposal ID: DMS-0203942<br/>PI: Frits H. Ruymgaart<br/>Title: Problems in ill-posed statistical inference<br/><br/>Abstract<br/><br/>There is a renewed interest from the part of statisticians in solving noisy integral equations.  This kind of problem is ill-posed because the integral operator involved typically has an unbounded inverse and such models boil down to a generalization of traditional curve estimation.  The purpose of this proposal is to pursue the general theory that is being developed into three directions.  The first question is whether asymptotically efficient estimators of sufficiently smooth functionals of the input signal can be constructed despite the presence of the possibly unknown error density as an infinite dimensional nuisance parameter.  The second question to be dealt with is the asymptotic distribution of the integrated squared error centered at its mean.  Such precise asymptotic behavior of a global measure of accuracy has important applications to model checks.  Finally a new method expedient for convolution equations with mathematical irregularities will be developed.  The method is based on expansion in a wavelet basis coupled with inversion of the convolution operator in the time domain.<br/><br/>Experimenters are often faced with the problem of recovering the input of a system when only the output is observable.  Typically the observations will be blurred by measurement error and statistical procedures become pertinent.  Examples include computer tomography employed in medical imaging, and Wicksell's problem in stereology where a transform of the particle size distribution is observed.  Inverse heat conduction requires the recovery of the initial heat distribution (input) when the present one (output) is given.  There are many more examples and related questions.  For instance, can one estimate the total weight of a cable suspended at its endpoints, when only data regarding its shape are available?  The construction of efficient estimators of such functionals of the input is one purpose of this research.  A second question considered is precise information about the frequency distribution of the discrepancy between the estimated input and its expectation.  Results can be useful in checking the validity of certain prior assumptions and could, for instance, be applied in recovering the luminosity distribution of the Milky Way.  Finally a new method of input reconstruction in irregular cases will be developed using wavelets with potential application to image reconstruction.<br/><br/>"
"0206912","International Conference on Current Advances and Trends in Nonparametric Statistics, July 15-19, 2002, Crete, Greece","DMS","STATISTICS","07/01/2002","04/24/2002","Dimitris Politis","CA","University of California-San Diego","Standard Grant","Marianthi Markatou","12/31/2002","$15,000.00","","dpolitis@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","0000, OTHR","$0.00","<br/>The objective of the proposed 5-day international conference is to highlight the major trends in several areas of nonparametric statistics. Among the areas included are: smoothing methods, functional data analysis, data mining, nonparametric model building, nonparametric inference, resampling, spatial statistics, computation statistics, high dimensional data, dependent data, and censored data. The application areas feature medical research, bioinformatics, analysis of microarray data, analysis of evolutionary data, cancer research, AIDS and HIV research. To achieve this goal the members of the organizing committee were selected with the idea of having as complete a representation of the important trends in nonparametric statistics as possible. Indeed the committee consists of some top people in the field. More information can be found in http://www.psu.edu/~npconf/. By bringing together researchers from the USA and Europe, the conference is expected to facilitate the exchange of research ideas, promote collaboration and contribute to further development of the field. New researchers have been encouraged to attend with preference being given to them-as well as members of the underrepresented groups-for support. Six issues (one volume) of the Journal of Nonparametric Statistics have been made available to host papers that will be presented in the conference with the conference organizers serving as guest editors. <br/>"
"0139903","Collaborative Proposal: FRG: Statistical Analysis of Uncertainty in Climate Change","DMS","STATISTICS, Climate & Large-Scale Dynamics","08/01/2002","07/26/2002","Christopher Wikle","MO","University of Missouri-Columbia","Standard Grant","Grace Yang","07/31/2006","$179,999.00","","wiklec@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269, 5740","0000, 1616, 4444, OTHR","$0.00","Proposal Ids: DMS - 0204232; DMS - 0139948; DMS - 0139903<br/>PIs: L. Mark Berliner; Richard A. Levine; Christopher K. Wikle<br/>Title: FRG: Statistical Analysis of Uncertainty in Climate Change<br/><br/>ABSTRACT<br/><br/>There is a growing consensus among scientists that aspects of our planet's climate are changing due to human influences, though the scientific community acknowledges that substantial uncertainty exists regarding the forms, levels, and impacts of change. Quantifying these uncertainties requires new statistical research informed by climate science. Effective solutions to climate change problems will rely on new methods for combining the information content of models and data in a fashion that quantitatively manages uncertainty. The research team will rely extensively on Bayesian hierarchical modeling and analysis strategies. Specific projects will include (1) developing new probabilistic climate change assessments based on an extensive suite of climate simulations; (2) statistical procedures for combining different climate models to produce climate projections; and (3) assessing regional and local impacts of global climate behavior.<br/><br/>Describing the Earth's climate and predicting its responses to human influences are critical problems in science and public policy. The research team of statisticians and climate modeling experts from the National Center for Atmospheric Research will develop new statistical strategies that combine observations with the information present in computer models for the climate system, while managing the uncertainties implicit in both. Assessing potential impacts of climate change on the environment and human activities is also fraught with uncertainty. The research team will develop integrated methods for predicting climate impacts on regional and local phenomena. These methods will be applied in predicting the El Nino-Southern Oscillation and properties of tornado occurrence in the Central United States.<br/>"
"0204029","Topics in Predictive and Descriptive Data Mining","DMS","STATISTICS","07/01/2002","08/18/2005","Jerome Friedman","CA","Stanford University","Continuing Grant","Gabor Szekely","06/30/2008","$420,000.00","","jhf@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>PI: Jerry Friedman<br/>DMS-0204029<br/><br/><br/>This proposal seeks support for research in predictive and descriptive data mining (DM). Decision tree methods are the most popular predictive DM tools. Research under this grant will investigate ways to overcome their most serious limitation: severe over fitting in the presence of categorical (factorial) predictor variables with very large numbers of values (factors). Cluster analysis is often used as a tool in descriptive DM. In most DM applications a large number of variables are measured on each observation. Usually clustering, if it exists, occurs only within (often small) unknown subsets of all the measured variables. Moreover, individual clusters may represent groupings on (possibly overlapping) variable subsets. The goal is to identify the clustered groups as well as the particular variable subsets on which each one preferentially clusters. Traditional clustering algorithms are not well suited for this task. Research under this grant will investigate new approaches for solving this problem, especially in situations where there are a very large number of measured variables.<br/><br/>Data mining is used to discover patterns and relationships in data, with an emphasis on very large data bases. It has had a major impact in business, industry, science, medicine, and most recently homeland security. Data mining activities divide into two types: predictive and descriptive. Predictive DM involves using past observational data from a system to build a mathematical model of that system. The model is used to predict some future unknown property (attribute or variable) of the system, given other properties that will be known in the future. Descriptive DM seeks to construct compact, interpretable summaries of the data in order to understand patterns and relationships, without focusing on the prediction of particular attributes. This research will investigate new methodologies for increasing the power of both descriptive and predictive DM in problems for which they have been traditionally weak. <br/>"
"0140698","Collaborative Research:  A Focused Research Group on Multiscale Geometric Analysis--Theory, Tools, Applications","DMS","APPLIED MATHEMATICS, STATISTICS, COMPUTATIONAL MATHEMATICS","08/15/2002","01/18/2005","David Donoho","CA","Stanford University","Standard Grant","Rong Chen","01/31/2006","$543,002.00","","donoho@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1266, 1269, 1271","0000, 1616, 9263, OTHR","$0.00","Proposal IDs: DMS-0140698, DMS-0140540, DMS-0140587, DMS-0140623<br/>PIs: Donoho, Candes, Huo and Jones<br/>TITLE: A Focused Research Group on Multiscale Geometric Analysis--Theory,<br/>Tools, Applications<br/><br/>Abstract<br/><br/>An interdisciplinary team, bridging Harmonic Analysis, Statistics, Image Analysis and Astronomy, proposes a united effort to exploit and extend recent breakthroughs in computational harmonic analysis. The team will consolidate several scattered advances into a unified body of theory and methods to be called Multiscale Geometric Analysis, which can probe the fine structure of 2-, 3- and higher- dimensional functions and point clouds, able to isolate and manipulate intermediate-dimensional phenomena. Simple examples include edges in 2-d images, filaments and sheets in 3-d point catalogs. Characterizing such intermediate-dimensional phenomena is essential for fundamental progress in a wide range of problem areas (including 2-d and 3-d imaging) where traditional multiscale methods have now run their course. Our united effort has three main outcomes. 1. Theory. Coherent, comprehensive knowledge, showing what can and cannot be accomplished with computational harmonic analysis. 2. Tools. A wide range of practical MGA algorithms and a unified, publicly available software environment - BeamLab - deploying them. 3. Applications. Our main initial focus will be on the analysis of 3-d point catalogs in astronomy, such as those coming on-line soon from the Sloan Digital Sky Survey. We know a priori that the data contain filaments and sheets, embedded in a scattered background, and MGA provides a decisive set of tools to resolve the structural properties of such catalogs, far more sensitive and more comprehensible than any tools currently available for such analysis.<br/><br/>Many new kinds of massive databases are being created in our era's revolutionary transition to a data-rich society.  Many of these databases contain geometric structures such as surfaces, and filaments, albeit in a sometimes hidden manner. Databases of this sort can include galaxy catalogs in astronomy -- in which the filaments and sheets are predicted by various theories of universe formation -- 3-D scanning of faces and other objects -- in which filaments and surfaces are caused by structures such as skin and hair -- and statistical databases in which curves and surfaces arise from existence of predictable patterns. This Focused Research Group on Multiscale Geometric Analysis is a research effort bringing together mathematicians, statisticians, image analysts and astronomers to create new tools and theories to help extract geometric structure and meaning from such databases.  The participants are skilled at multiscale analysis, which will be a central organizational principle. We expect multiscale methods to have an impact here comparable to what wavelets have had over the last twenty years in many other problems in science and technology.<br/>"
"0134431","CAREER: Semiparametric Regression Models for Censored Data","DMS","STATISTICS","07/01/2002","04/26/2007","Zhezhen Jin","NY","Columbia University","Continuing Grant","Gabor Szekely","06/30/2008","$300,000.00","","zj7@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","0000, 1045, 1187, OTHR","$0.00","Semiparametric regression methods for censored data currently used by empirical researchers rarely extend beyond the well-known Cox proportional hazards regression model. The Cox model assumes that covariate-specific hazard functions are proportional, an assumption often not satisfied in practice. The proposed research investigates the accelerated failure time regression and the family of semiparametric linear transformation models, of which the Cox model is a member. Existing methods for these models are complicated by their numerical difficulties and stringent assumptions. To circumvent these complications, the investigator proposes a number of new approaches, including modified least squares method, the M-estimation and a minimization-based rank-type estimation for the accelerated failure time model and a general counting process-based score equations for the transformation models. An important feature of the new approaches is that their inference procedures can be readily implemented with numerically stable algorithms.<br/><br/>Results from the proposed project will be relevant and directly applicable to many scientific disciplines. They will provide methodologic tools for analysis of data across many important fields, including economics, business administration, industrial engineering, medicine, biology and public health. Examples of the scientific research problems that the methods can deal with are as diverse as identification of factors associated with the duration of unemployment, determination of major risk factors for cancer, testing of new drugs to combat AIDS, examination of factors associated with the lifespan of system components, evaluation of potential confounding factors in crime prevention and drug abuse. The research touches many important areas in statistical science: linear regression, survival analysis, semiparametrics, robust statistics, nonparametric statistics and statistical computing. In this connection, it will generate excellent opportunities for graduate students to have exposures to a broad spectrum of modern statistics as well as to learn vital skills in conducting independent researches. A research topic/reading course and a journal club will be developed to facilitate these exposures and enhance the educational experience.<br/>"
"0203762","Sequential Importance Sampling with Resampling and Its Applications","DMS","STATISTICS","09/01/2002","07/31/2002","Yuguo Chen","NC","Duke University","Standard Grant","Grace Yang","08/31/2005","$90,000.00","","yuguo@uiuc.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, OTHR","$0.00","Proposal ID: 0203762<br/>PI: Yuguo Chen<br/>Title: Sequential importance sampling with resampling and its applications<br/><br/>Abstract:<br/><br/>The Monte Carlo method of sequential importance sampling (SIS) provides a versatile and powerful tool for solving complex statistical inference problems. A number of basic issues concerning the method remain to be resolved for it to be more widely applicable:  How should the proposal distribution be chosen to strike a proper balance between computational complexity and statistical efficiency?  What is the role of resampling and what is a good choice for the resampling schedule?  An objective of this proposal is to address these questions through the detailed study of SIS in three important applications. The first area is time series and stochastic dynamic systems.  Research will develop resampling schedules and proposal distributions for SIS to solve some long-standing filtering and smoothing problems in continuous-state hidden Markov models.<br/>Change-point problems, which can be seen as a special case of hidden Markov models, will serve as a test ground for the new methodology. The second area is statistical inference in molecular population genetics.  Research in this area will enhance currently available SIS methodology by developing a new resampling approach, and by combining such resampling strategy with suitably chosen proposal distributions.  The final area of research is conditional inference on contingency and zero-one tables.  New theories arising from these applications will be of interest across a broad range of areas.<br/><br/>The Monte Carlo method of sequential importance sampling has been fruitfully applied to a wide range of scientific problems including simulating molecules, filtering and smoothing time series arising in engineering and economics, and making Bayesian statistical inferences. However, a number of basic issues need to be resolved to make the method more widely applicable and effective.  For example, how should a proper balance be struck between computational complexity and statistical efficiency in implementing the method and what is the role of various enhancements to the method.  This research will address these issues through the development of more efficient sequential importance sampling techniques for three important areas of application.  The first area is filtering and smoothing problems in continuous-state hidden Markov models, which have important applications in communications signal processing.  The second area is statistical inference on genealogical trees.  Recent advances in biotechnology have provided an abundance of data on the genetic variation of DNA within a population.  This data, which often poses computationally challenging statistical inference problems, can shed light on the evolutionary process of a population and yield important information for locating genes that are responsible for genetic diseases.  The third area of application is conditional inference on contingency and zero-one tables, which is motivated by the interest in psychology in testing the Rasch model and in ecology in testing theories about the relationship between evolution and the competition among species.  This research will improve the sequential importance sampling methods used in these three applications and strive to develop a systematic theory that provides insight into general strategies for applying sequential importance samplin<br/>"
"0204688","Bayesian, Empirical Likelihood and Counting Process Methods for Semiparametric Models","DMS","STATISTICS","07/01/2002","06/28/2002","Ian McKeague","FL","Florida State University","Standard Grant","Grace Yang","06/30/2005","$87,600.00","","im2131@columbia.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0204688<br/>PI: Ian McKeague<br/><br/>This project aims to enhance the scope of semiparametric models for use in weather prediction, ocean circulation and biomedical applications. A synthesis of Bayesian, empirical likelihood, counting process and Monte Carlo methods is used to advance statistical methodology in these areas. Five specific topics are investigated: Bayesian single-index models, Bayesian inversion of ocean circulation data, empirical likelihood methods for treatment comparisons, tests for mark-specific hazards and cumulative incidence functions, and covariate selection for semiparametric hazard function regression models. <br/><br/>The initial phase of the project is motivated by a weather prediction problem and introduces Bayesian methodology for single-index models, incorporating some frequentist methods, as well as useful prior information, into the inference machinery. Next, a Bayesian inversion approach for the ocean circulation inverse problem is developed, motivated by the success and popularity of this approach in other ill-posed inverse problems. With a view towards biomedical applications, empirical likelihood based methods for comparing two or more treatments are studied. A new approach for comparing mark-specific hazard functions, which is useful for the analysis of HIV genetic data collected in AIDS clinical trials and the assessment of HIV vaccine efficacy, is introduced. Finally, a model selection procedure for finding the best subset of covariates in a flexible new class of semiparametric hazard function regression model is developed. <br/>"
"0204639","Is Deliberate Misspecification Desirable?    Statistical Study of Financial and Other Time-Dependent Data","DMS","STATISTICS, Methodology, Measuremt & Stats","08/15/2002","05/26/2006","Per Mykland","IL","University of Chicago","Continuing Grant","Grace Yang","07/31/2007","$510,000.00","Lan Zhang","mykland@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269, 1333","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0204639<br/>PI: Per Mykland<br/><br/>Gambling has been one of the main sources for conceptual understanding of statistics and probability, ever since the invention of the latter in the 1600s. Following this tradition, the development of ideas and results in statistics is pursued, on the basis of an application that resembles gambling in many respects. The application is derivative securities such as options. Among the conceptual issues to be investigated, two stand out as recurring themes. One is the need to integrate statistical inference with the needs of the application. The other is that this need often leads to deliberately misspecified models, beyond what is usually the case in ordinary statistics. This proposal is also about the application. Options trading differs from gambling both in its importance to the economy, and (most of the time) in its social value. It is also an industry where mistakes and insufficient understanding can lead to substantial calamity for the rest of society. One of the primary goals of this project is to improve understanding of the (data) processes involved in options trading, and to facilitate government regulation. The dual intentions are conceptual development in statistics and the study of the major sectors of contemporary economy.<br/><br/>The reason why these two aims go together is that statistics relates to three of the most substantial problems facing the derivatives industry today. One is the common failure to incorporate statistical findings into decisions of pricing, trading and regulation. Another is the effect of big discontinuities (jumps, due to various shocks, or simply endogenous) in the prices of securities. And, finally, the frequent lack of transparency in the setting of options values. These are the applied questions to be answered. The three difficulties are of keen interest to regulators and financial institutions alike. Results from the project should be broadly useful to the economy, benefiting both consumers of derivatives and governments trying to regulate a business that is opaque from the outside. The problems are also often mirrored in the setting of corporate governance. On the theoretical side, the proposed project is about statistical inference with a twist, in that the goal is to take the application and integrate it with the process of inference. Since inference will be for longitudinal data, such as diffusions and jump diffusions, the conclusions will also give insights to general inference for such data. A main contention of the project is that the chain from data analysis to actual trading and regulation is best studied by taking apart its different links. These can then be studied separately, often with different methods and deliberately different model specifications, or frequently even useful misspecifications. This permits the isolation of those links, which makes an integrated analysis quite ill posed. And it gives greater transparency to those other links, which are amenable to a reliable analysis. This willingness to use separate tools for separate purposes appears to be quite successful, both from a statistical and a financial perspective. A main example is the analysis of variance (ANOVA) for diffusions. This device leads to trading that can adapt to the markets in more sophisticated ways than by calibration or modeling alone.  <br/>"
"0201814","Fifth International Conference on Forensic Statistics","DMS","STATISTICS","07/01/2002","05/23/2002","Stephen Fienberg","PA","Carnegie-Mellon University","Standard Grant","Marianthi Markatou","06/30/2003","$12,000.00","","fienberg@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, OTHR","$0.00","The Fifth International Conference on Forensic Statistics will be held at Isola di San Servolo, Venice, Italy, on August 30th to September 2nd, 2002. The conference is intended to allow statisticians, forensic scientists, lawyers, and scholars from related disciplines to discuss the many and varied uses of statistics and probability in legislative, administrative and judicial proceedings. Forensic Statistics, the application of statistics and probability to legal matters, is an expanding field. The current debates surrounding the presentation of evidence based on DNA profiling, epidemiological studies on the health effects of various drugs, and racial profiling are just three examples. There is a great need for statisticians to provide guidance to forensic scientists, and lawyers and the courts. By bringing together statisticians, forensic scientists, lawyers and other scholars, this fifth international conference will further the interdisciplinary understanding of Statistics and Probability applied to legal matters. Previous conferences in this series have been held at North Carolina State University, the University of Edinburg, and the Arizona State University. This proposal provides funds to insure that U.S. participants will be able to hear and network with experts on this topic from around the world, and to provide access to the conference for selected new researchers and members of underrepresented groups"
"0203676","Mixed Model Selection: Theory and Application","DMS","STATISTICS","08/01/2002","08/09/2002","Jiming Jiang","CA","University of California-Davis","Standard Grant","Grace Yang","07/31/2006","$68,364.00","","jiang@wald.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0203676 & 0203724<br/><br/>PIs: Jiang/Rao<br/><br/>This research involves development of model selection procedures for linear and generalized linear mixed models. The focus of this research will be on studying asymptotic properties and finite sample performance of the procedures, which will include comparisons to commonly used ad hoc methods. The research will also develop applications of the selection methodology to selecting factors in longitudinal data, genetic screening of genes related to quantitative traits of interest, and to model selection in small area estimation problems from surveys. In addition, the methods developed will be made accessible through the development of freely available software. The research will also improve work recently completed by the investigators where the conditions for consistent factor selection in linear mixed models were established. These improvements will include developing a more efficient method for linear mixed model selection, and studying some adaptive procedures. <br/><br/>Linear and generalized linear mixed models are important classes of models which allow relaxation of standard assumptions like independence or homogeneity of variances of observations, and take into account more complicated data structures in quite general ways. Typical applications include repeated measurements made on a patient over time or screening of candidate genes that might be related to a quantitative trait of interest. Selecting which factors should or should not be in the model can be of importance for model inference and predictions, yet little if anything has been developed to formally study this problem. This research attempts to fill this important gap in the field of developing new procedures for correct factor selection from a theoretical perspective, and evaluating real-world performance via extensive simulations. Another important component of this research is to apply it in a variety of real-world settings including longitudinal data, genetic screening, and small area estimation in survey sampling. <br/>"
"0204531","Collaborative Research: Theory and Methods for Nonparametric Survey Regression Estimation","DMS","STATISTICS, Methodology, Measuremt & Stats","08/15/2002","08/14/2002","Jay Breidt","CO","Colorado State University","Standard Grant","Grace Yang","07/31/2005","$71,166.00","","breidt-jay@norc.org","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1269, 1333","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0204642 & DMS-0204531 (Collaborative)<br/>PIs: Opsomer/Briedt<br/><br/>Title: Collaborative Research: Theory and Methods for Nonparametric Survey Regression Estimation<br/><br/>This research project develops new methods for the efficient use of auxiliary information in complex surveys, based on nonparametric regression techniques. Current practice relies on parametric regression techniques, which have good efficiency if the regression model is well specified, and which have a number of appealing operational features. The nonparametric techniques share these operational features, lose little efficiency when the parametric specification is correct, and gain efficiency when the parametric specification is incorrect. The project increases the scope of applicability of the nonparametric regression estimation approach, by considering complex survey designs, varying types of auxiliary information, and alternative smoothing techniques. Specifically, the project investigates multi-stage surveys with cluster or element-level auxiliary information; multivariate auxiliary information; and alternative smoothing techniques. Parametric and nonparametric techniques are blended using semiparametric additive models to provide a flexible tool for use in complex surveys. <br/><br/>Large-scale surveys are used to collect data in a wide range of fields, from studies of human populations to inventories of natural resources. Information external to the survey, such as administrative records or remote sensing, is often available. This research project makes it possible to incorporate auxiliary information easily and effectively into survey estimates, by using nonparametric regression methods. Nonparametric regression, sometimes referred to as smoothing, is widely used in other areas of statistics, but its use in survey estimation has been limited so far. The investigators show that incorporating auxiliary information into survey estimation through nonparametric regression can improve the precision of the surveys, often at reduced costs. <br/>"
"0204329","Inferences for Multivariate Semiparametric and Nonparametric Models with Applications to Risk Management","DMS","STATISTICS","08/01/2002","07/17/2002","Jianqing Fan","NC","University of North Carolina at Chapel Hill","Standard Grant","Xuming He","11/30/2003","$225,000.00","","jqfan@princeton.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>PI: Jianqing Fan<br/>DMS-0204329<br/><br/>The objectives of this proposal are to develop new and widely applicable approaches for semiparametric and nonparametric estimation and inferences, to study theoretical properties of these new approaches, and to evaluate their efficacy in data analyses. This proposal not only introduces a number of innovative techniques, but also provides various new and deep insights into statistical foundation. It will have significant impact on the future research of statistical methodologies, computation and theories. In particular, three inter-related areas are proposed for study. Firstly, a family of flexible semiparametric and nonparametric models is introduced. This allows one to study the extent to which response variables are associated with their covariates. The generalized likelihood ratio statistics is proposed for testing various hypotheses in multivariate semiparametric and nonparametric models. Secondly, new semiparametric and nonparametric models are proposed for understanding interest-rate dynamics, and stock price volatilities. Furthermore, the information on state-domain is incorporated to improve the efficiency of volatility estimation for bonds and to more accurately estimate the market risks of a portofolio. Thirdly, new techniques for variable selection, in the presence of a large number of variables, are proposed via nonconcave penalized likelihood. The innovation is that they estimate parameters and select variables simultaneously. <br/><br/>The above techniques are widely applicable to many scientific and engineering problems. Multivariate nonparametric, semiparametric and large parametric models have been widely used. Statistical questions often arise such as if certain variables or factors are important to public health; if some risk factors contribute significantly to the survival time of patients; and if interest-rate dynamics or stock price processes are time-dependent or follow certain famous hypotheses, among others. Yet, there are no generally applicable tools available to answer these questions in multivariate semiparametric and non-saturated nonparametric models. The techniques proposed here permit one to objectively test scientific hypotheses without restrictive model assumptions. The techniques allow to better price financial derivatives and manage investment risk, to identify important risk variables and their possible interactions in the analysis of large epidemiological studies and to scrutinize famous hypotheses on stock prices <br/>"
"0233710","Bayesian Methods for Spatio-Temporal, Inverse, and Multi-Resolution Problems","DMS","STATISTICS","09/01/2002","08/01/2002","Herbert Lee","CA","University of California-Santa Cruz","Standard Grant","Grace Yang","08/31/2005","$104,999.00","","herbie@ams.ucsc.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1269","0000, OTHR","$0.00","Proposal ID: 0233710<br/>PI: Herbert Lee<br/>Title: Bayesian methods for spatio-temporal, inverse, and multi-resolution problems<br/><br/>Abstract<br/><br/>The principal investigator and his collaborators study a new class of spatial models derived from the convolution representation of Gaussian process models. By expanding the class of distributions for the underlying process being convolved, the result is a range of flexible spatial models that are especially useful for inverse problems and spatial processes over time. The research specifically examines convolutions of Markov random fields and convolutions of temporally evolving processes. The researchers also explore the use of coupled chains, parallel computing, and surrogate modeling to develop efficient computational methods for this computationally intensive modeling approach.<br/><br/>Spatio-temporal models have a broad spectrum of applications, and the development of more flexible models is a problem of high priority. The motivation for the models developed under this project comes from an inverse problem in hydrology and a space-time problem from meteorology. The research, that addresses both theoretical and methodological aspects in a Bayesian framework, will develop models that are useful for dealing with multiple scales, a phenomenon encountered in physical processes in both hydrology and meteorology.  <br/>"
"0134987","Career: Research and Education of Flexible Methods for Statistical  Modeling and Prediction","DMS","STATISTICS, THEORY OF COMPUTING","03/01/2002","02/28/2006","Yi Lin","WI","University of Wisconsin-Madison","Continuing Grant","Grace Yang","02/28/2007","$300,000.00","","yilin@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269, 2860","0000, 1045, 1187, OTHR","$0.00","In recent years, many novel techniques for regression, classification, and density estimation have been developed, both in statistics and in other related areas such as machine learning and neural networks. Some of these methods have been very successful in practice, but their statistical properties are not fully understood. This hinders the further development of these techniques. The goal of the proposed research is to gain statistical insights into these techniques, and to develop new methodologies and improved algorithms. The specific techniques investigated are the support vector machine, the randomized trees, and the log density functional ANOVA model for continuous and mixed data. Several new techniques are introduced. The support vector machine for multi-category classification with arbitrary cost structures will be further developed. A new framework is proposed that connects the adaptive nearest neighbor estimation and the randomized trees. Through the use of the sparse grid method, a backfitting type algorithm is proposed for fitting the log density functional ANOVA model, with applications to graphical models for continuous and mixed data. These new techniques will be examined through theoretical investigation and empirical evaluation. The investigator will develop a graduate level course on flexible methods for regression, classification, and density estimation, and their applications. Part of the proposed research will be incorporated into the course material.<br/><br/>Regression, classification, and density estimation are the standard problems in statistics. Traditional methods typically employ strong distributional assumptions. With the vast computing power of today, it becomes possible to develop and implement more flexible methods, and a host of new techniques emerged, both in statistics and other related areas. Many of these are computationally intensive, and their statistical properties have not been wellunderstood. A clear understanding of these methods is crucial for their further development and statistical education. The proposed research develops valuable insights into flexible statistical methods of current research interest. The techniques developed in the research provides new and useful tools to efficient data analysis, and can be applied to many problems in medical, social, economical, environmental and biological sciences. An important aspect of the current statistical education is the teaching of flexible statistical methods that take advantage of the computing power we have today, and their application in different scientific and industrial areas. The insights and new techniques developed in the proposed research will be incorporated into graduate level courses, and benefit the training of graduate students.<br/>"
"0225692","Collabrative Research:  Sequential Monte Carlo Methods and Their Applications","DMS","STATISTICS","01/01/2002","08/05/2002","Xiaodong Wang","NY","Columbia University","Standard Grant"," Shulamith T. Gross","08/31/2004","$195,135.00","","wangx@ee.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","0000, 1616, OTHR","$0.00","Collaborative Research: Sequential Monte Carlo Methods and Their Applications<br/><br/>Jun Liu, Harvard University <br/>Rong Chen, Univ. Illinois at Chicago<br/>Xiaodong Wang, Texas A&M University<br/><br/>Abstract (Technical):<br/>Sequential Monte Carlo (SMC) methodology recently emerged in statistics and engineering fields promises to solve a wide class of nonlinear filtering and optimization problems, opening up new frontiers for cross-fertilization between statistics and many areas of applications. A distinctive feature of SMC is its ability to adapt to the dynamics of the underlying stochastic systems via recursive simulation of the variables involved.  Although special forms of SMC date back to 1950s, the general use of the method appeared only recently and its many key properties have yet been well understood.  This research group will focus on three major theoretical issues regarding the design of effective SMC-based computational tools and three important application areas, namely, wireless communications, computational biology, and business data analysis.  In the theory part, they will study approaches of generating better Monte Carlo samples for tracking system dynamics; investigate roles of resampling which is critical to the effectiveness of SMC; and propose system reconfiguration strategies for more efficient SMC algorithms.  In the application part, they plan to design novel signal processing and network control algorithms for wireless multimedia communications; develop better multiple sequence alignment models and SMC-based optimization method for protein structures; and build SMC-based modeling and analysis tools for business data. It is anticipated that the proposed research will culminate in the formulation of novel SMC methodologies and will bring the promise of the SMC paradigm into the practical arena of many emerging applications.<br/><br/>Stochastic dynamic systems are routinely used in many application fields such as automatic control, engineering, and finance.  The statistical analyses of these systems are crucial.  However, except for a few special cases, quantitative analyses of these systems still present major challenges to researchers.  Sequential Monte Carlo (SMC) technique recently emerged in the field of statistics and engineering shows a great promise on solving a wide class of nonlinear filtering, prediction, and optimization problems, providing us with many exciting new research opportunities.  The name ""Monte Carlo"" was coined in 1940s by scientists involved in designing atomic bombs and it refers to a technique in which computer is used to simulate and study a complex stochastic system. The technique was named after the famed gambling resort because its procedures incorporate the element of chance. A distinctive feature of SMC is its ability to sequentially simulate the system by considering one variable at a time.  The general use of SMC appeared recently and its invasion into many fields of science and engineering has just begun. Researchers including people in this research group have demonstrated that SMC can be successfully adapted to solve chemistry, engineering, and statistical problems. Understanding its theoretical properties and extending the use of SMC to other fields are the main focuses of this project. More specifically, this research group will focuse on three major theoretical issues regarding the design of effective SMC-based computational tools and three important application areas including wireless communications, computational biology, and business data analysis. These applications are not only important by their own merits, but also essential as the test ground for the new theories being developed and as the sources of stimulation for new research directions for SMC.  It is anticipated that this research will culminate in the formulation of novel SMC methodologies and will bring the promise of the SMC paradigm into the  practical arena of many emerging applications.  In particular, this research will bear fruits in the following areas: novel designs of signal processing and network control algorithms for wireless multimedia communications; developments of better algorithms analyzing biological sequence and structure data; and a SMC-based tool for business data analysis and prediction.<br/>"
"0204674","Statistical Problems in Hidden Markov Modeling for Biology and Chemistry","DMS","STATISTICS, BE: NON-ANNOUNCEMENT RESEARCH","07/01/2002","06/03/2005","Jun Liu","MA","Harvard University","Continuing Grant","Grace Yang","06/30/2006","$325,771.00","Samuel Kou","jliu@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269, 1629","0000, OTHR","$0.00","Proposal ID: DMS-0204674<br/>PI: Jun S. Liu<br/>Title: Statistical problems in hidden Markov modeling for biology and chemistry<br/><br/>Abstract<br/><br/>With the completion of genomes of many species and the advances of microarray technologies, biological researchers begin to possess a tremendous amount of data --- but these ""raw products"" are still far from usable. One of the most challenging problems of this century is to decipher this huge amount of biological information, turning the data into knowledge. Simultaneously, there also has been a revolution in chemistry research: scientists can now use advanced technology to make observations on single-molecule dynamics, which promises to rewrite some fundamental laws in physics and chemistry derived from traditional ensemble-averaged experiments. As the data concerning molecular movements are inherently noisy, the development of advanced statistical tools for handling such data is a pressing need. The past decade has witnessed the power of formal statistical modeling, especially the use of hidden Markov models, in revolutionizing the field of computational biology. <br/><br/>It is the investigators' belief that using proper statistical models to describe the underlying chemical processes and to derive efficient inference methods can also greatly strengthen the data analysis in single-molecule studies. For the biological information analysis, the investigators describe a few problems related to the statistical models used for finding motifs, whose solutions can deepen the understanding of a few popular Bioinformatics algorithms for sequence analysis. The investigators show that these algorithms are based on special hidden Markov or semi-Markov models and can be generalized to accommodate more detailed biology knowledge. For single-molecule data analysis, the investigators outline an efficient likelihood-based approach for inferring quantities of special interests in single-molecule studies. The form of the observed data naturally calls for a data augmentation framework, which is a promising means for solving the computational difficulty. In single-molecule studies, besides the problem of model inference, model selection is also an important and difficult task, as it is often the case that there are competing models describing one chemical reaction, making it necessary to use the experimental data to choose the appropriate model. The investigators, using a data augmentation approach, propose a few generalized methods for choosing among different chemical models.<br/>"
"0204252","Higher-Order and Exact Methods for Statistical Inference","DMS","STATISTICS","08/01/2002","06/30/2004","Thomas DiCiccio","NY","Cornell Univ - State: AWDS MADE PRIOR MAY 2010","Continuing Grant","Gabor Szekely","07/31/2007","$152,488.00","Martin Wells","tjd9@cornell.edu","373 Pine Tree Road","Ithica","NY","148502488","6072555014","MPS","1269","0000, OTHR","$0.00","DMS-020204252<br/>PI: Thomas J DiCiccio<br/><br/>Title: Higher Order and Exact Methods for Statistical Inference<br/><br/>Abstract<br/><br/>There has been burgeoning interest in the last decade or so in artificial likelihoods for nonparametric problems. The investigators also propose to develop tractable approximations to the usual and adjusted nonparametric likelihood ratio statistics. These approximations are intended both to make bootstrap inference feasible and to yield insight into inaccuracies of the standard asymptotic approximations. Maximizing the adjusted profile nonparametric likelihoods yields adjusted estimators, and the investigators propose to study their properties, particularly as a basis for the popular bootstrap-t method of inference. Small sample inference in discrete data problems, such as contingency tables, log-linear models, and logistic regression, has received much attention. One approach is saddlepoint approximations; another approach is the use of Monte Carlo methods to simulate from the relevant conditional distributions. Important new ideas from computational ring theory may pave the way for the next generation feasible methods in highly discrete problems, just as the network algorithm provided methods that are currently being implemented routinely in practice. The investigators propose to use analytic and computational concepts from the theory of Groebner bases and toric ideals to provide correct and feasible simulation methods in complex discrete problems. <br/><br/>The proposed research concerns feasible methodologies for accurate inference in both nonparametric and parametric settings. In the nonparametric framework, the focus is on approximate inference based on pseudo-likelihoods, while in the parametric context, the focus is on simulation methods for exact inference in discrete models. The inferential methods that are to be addressed in the proposal have broad application in data analysis of numerous areas of modeling scientific phenomena.<br/>"
"0229028","NSF Conference in the Mathematical Sciences on Functional Data Analysis","DMS","STATISTICS","10/01/2002","07/31/2002","George Casella","FL","University of Florida","Standard Grant"," Shulamith T. Gross","09/30/2003","$13,500.00","Adao Trindade, Bhramar Mukherjee","casella@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","0000, OTHR","$0.00","Proposal ID: DMS-0229028<br/>PI: Casella<br/>Title: Conference on Functional Data Analysis<br/><br/>The Department of Statistics at the University of Florida hosts its Fifth Annual Winter Workshop on January 10-11, 2003 in Gainesville, Florida. The workshop focuses on recent developments in functional data analysis (FDA), which is emerging as one of the most important new statistical methodologies with diverse applications in all areas of statistics ranging from image analysis to bioinformatics. The workshop provides a forum for interaction between theoretical developments and advances in applications of FDA methods. Besides the impact on development of human resources, the workshop will generate an impetus for the wider dissemination and use of functional data analysis techniques. In addition to invited sessions, the conference will include a contributed poster session.<br/>"
"0204552","Collaborative Research:  Self-Consistency and Wavelet Regressions with Irregular Designs","DMS","STATISTICS","07/01/2002","04/12/2004","Xiao-Li Meng","MA","Harvard University","Continuing Grant","Grace Yang","06/30/2006","$188,568.00","","meng@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","0000, OTHR","$0.00","Proposal IDs: DMS - 0204552 and DMS - 0203901<br/>PIs: Xiao-Li Meng and Thomas Chun Man Lee<br/>Title: COLLABORATIVE RESEARCH: SELF-CONSISTENCY AND WAVELET REGRESSIONS WITH IRREGULAR DESIGNS<br/><br/>Abstract<br/><br/>This award is for a comprehensive research project for a joint investigation, to be conducted by PI Meng of Harvard University (the lead institution) and PI Lee of Colorado State University, on the use of the self-consistency principle for wavelet regressions with irregular designs. Wavelet estimators enjoy excellent theoretical properties and they are capable of adapting to very complex spatial and frequency inhomogeneities. In addition, their computation is very fast when the regression design points are regular.  However, when the design points are not regular, as is typical in statistical applications, standard wavelet methods are no longer applicable. This collaborative research proposes to attack this problem from a missing-data perspective by viewing an irregular-design problem as a regular-design one but with missing data. This new perspective allows the investigators to apply well-established missing-data methods, guided by the self-consistency principle, to construct efficient irregular-design wavelet estimators, as well as fast algorithms to compute such estimators.<br/><br/>Wavelet regression is a powerful curve and surface fitting method that has attracted enormous attention from researchers across different fields, in particular applied mathematicians, computer scientists, engineers, and statisticians. Self-consistency is a fundamental statistical principle for constructing the most efficient statistical estimators in many incomplete data problems. This collaborate research effort combines these two powerful methods with the aim to make wavelet methods much more applicable to real-life problems, varying from medical imaging to fishery economy to global warming, where irregularities are rules rather than exceptions.<br/>"
"0200888","Computational Algebraic Methods for High-dimensional Statistical Applications","DMS","ALGEBRA,NUMBER THEORY,AND COM, STATISTICS","07/15/2002","07/28/2004","Ian DINWOODIE","LA","Tulane University","Continuing Grant"," Shulamith T. Gross","02/28/2005","$94,046.00","Edward Mosteig","ihd@pdx.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","MPS","1264, 1269","0000, 9150, OTHR","$0.00","ABSTRACT<br/><br/>Proposal DMS-0200888<br/>PI Ian Dinwoodie<br/>Title ""Computational algebraic methods for high-dimensional statistical applications""<br/><br/>This project has the goal of research in algebraic methods for high-dimensional parametric statistical problems where classical asymptotic methods are not useful. Connected with the research goal is work with existing and future graduate students. The research problems are:<br/>high-dimensional Gibbs-distribution models for network reliability and traffic with incomplete data; optimization for parameter estimation in high-dimensional models with incomplete data and nonconvex log-likelihood functions; fast simulation methods including fiber walks with Markov chains for integer data tables; and multivariate exponential generating functions for computations on lattice points with the hypergeometric distribution. A range of algebraic tools will be used, including Groebner bases in commutative rings and D-modules, elimination theory, polynomial homotopy methods, and Markov Monte Carlo methods. <br/><br/>This project will bring recent developments in computational algebra to new statistical applications where classical methods do not work.  Examples of such new and challenging applications are large-scale network traffic and reliability, and large databases of tabular data such as census information where security and analysis are difficult.<br/>The algebraic tools can help to solve problems of model formulation and model fitting and statistical analysis. Many algebraic techniques have been recently developed to solve computational problems in robotics and differential equations, and these methods are very promising for statistics. The investigators will develop these algebraic methods to solve statistical applications.<br/>"
"0203901","Collaborative Research:  Self-Consistency and Wavelet Regressions with Irregular Designs","DMS","STATISTICS","07/01/2002","04/12/2004","Thomas Chun Man Lee","CO","Colorado State University","Continuing Grant","Grace Yang","06/30/2005","$99,000.00","","tcmlee@ucdavis.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1269","0000, OTHR","$0.00","Proposal IDs: DMS - 0204552 and DMS - 0203901<br/>PIs: Xiao-Li Meng and Thomas Chun Man Lee<br/>Title: COLLABORATIVE RESEARCH: SELF-CONSISTENCY AND WAVELET REGRESSIONS WITH IRREGULAR DESIGNS<br/><br/>Abstract<br/><br/>This award is for a comprehensive research project for a joint investigation, to be conducted by PI Meng of Harvard University (the lead institution) and PI Lee of Colorado State University, on the use of the self-consistency principle for wavelet regressions with irregular designs. Wavelet estimators enjoy excellent theoretical properties and they are capable of adapting to very complex spatial and frequency inhomogeneities. In addition, their computation is very fast when the regression design points are regular.  However, when the design points are not regular, as is typical in statistical applications, standard wavelet methods are no longer applicable. This collaborative research proposes to attack this problem from a missing-data perspective by viewing an irregular-design problem as a regular-design one but with missing data. This new perspective allows the investigators to apply well-established missing-data methods, guided by the self-consistency principle, to construct efficient irregular-design wavelet estimators, as well as fast algorithms to compute such estimators.<br/><br/>Wavelet regression is a powerful curve and surface fitting method that has attracted enormous attention from researchers across different fields, in particular applied mathematicians, computer scientists, engineers, and statisticians. Self-consistency is a fundamental statistical principle for constructing the most efficient statistical estimators in many incomplete data problems. This collaborate research effort combines these two powerful methods with the aim to make wavelet methods much more applicable to real-life problems, varying from medical imaging to fishery economy to global warming, where irregularities are rules rather than exceptions.<br/>"
"0202284","Saddlepoint and Bootstrap Methods in Systems Theory and Survival Analysis","DMS","STATISTICS","08/01/2002","08/15/2002","Ronald Butler","CO","Colorado State University","Standard Grant","Grace Yang","07/31/2006","$126,000.00","","rbutler@smu.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>PI: R. W. Butler<br/>DMS-0202284<br/><br/>Title: Saddlepoint and Bootstrap Methods in System Theory and Survival Analysis<br/><br/>Stochastic systems underlie the models applied in most areas of modern science. These concepts have evolved over a long period of time but were heavily influenced more recently by the cybernetics movement spanning the late 40s until the late 70s. Those involved on the electrical engineering side of this movement used flow graphs to represent the systems. Interesting time-dependent system characteristics were naturally characterized in terms of the Laplace transforms that could be associated with the flow graph. Their efforts lead to a general theory of finite state semi-Markov systems that generalized the very restrictive Markov systems that are commonly applied even today. Unfortunately, the inversion of the transforms in this approach proved to be a challenge that ultimately limited the impact of the flow graphs. This proposal addresses these inversions along with other missing tools so that a general systems theory may be fully developed into a complete mathematical discipline. These inversion tools, using saddlepoint methods, allow for the determination of the transient behavior of complex systems. In addition, the bootstrap is introduced for nonparametric statistical inference about the true system based on system observation. The range of applications in the engineering and biomedical sciences include communication and computer networks, queueing theory, multi-state survival models, and right/left and interval censoring in the context of the competing risks associated with such models. More specifically, this proposal addresses the following issues: (1) New techniques are required for inverting the Laplace transforms that characterize the complex behavior of systems; the author proposes several new saddlepoint methods for achieving this by using the method of steepest descents. (2) Transforms describing system characteristics need to be specified in ways that make the saddlepoint methods easy to use. Often Mason's rule is used but, because of its form, it is much too complicated to apply to large systems. The author has proposed several alternative co-factor rules that greatly simplify computations for the saddlepoint inversions. (3) Nonparametric statistical inference is to be developed for such semi-Markov systems using the bootstrap and its relationship to empirical transforms. The single and double bootstrap lead to a practical means for computing confidence bands of survival and hazard functions related to the semi-Markov process. The proposed double bootstrap is implemented through saddlepoint inversions.<br/><br/>This proposal studies the statistical and probabilistic traits of dynamic stochastic systems with feedback. The models for such systems are commonly used to make extrapolations and predictions in most areas of modern science. In engineering, for example, a communication or computer system changes from state to state and the dynamics of these state transitions determine the evolution of the system. This proposal considers the computation and estimation of reliability or performance evaluation for such an evolving dynamic system. The proposed methods have important applications in reliability analysis, electrical engineering, biomedical sciences and manufacturing.   <br/>"
"0203921","Point Processes and time Series: Mutual Information Analyses","DMS","STATISTICS","07/15/2002","06/16/2004","David Brillinger","CA","University of California-Berkeley","Continuing grant","grace yang","06/30/2006","$261,000.00","","brill@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","Proposal: DMS-0203921<br/>PI: David Brillinger<br/>Title: Point processes and time series: mutual information analysis<br/><br/>ABSTRACT<br/>   This research concerns the theoretical and empirical investigation of the mutual information coefficient. It extends the ordinary correlation coefficient to the case of random processes. The processes to be studied include: point processes, time series, spatial-temporal processes and random networks. The mutual information coefficient will become a function of time or space or frequency amongst other quantities. The statistical properties of a variety of types of estimates, including kernel-based, wavelets with shrinkage, recurrence times will be developed. The tools of strong approximations, limit theorems, ergodic theory and nonparametric estimation will be employed.<br/>   Claude Shannon introduced the concepts of entropy and mutual information in 1948. These have both revolutionized and driven the way that our technological society has developed. Mutual information characterizes the strength of dependence between variates and of scientific relationships. It is intended to develop extensions of the idea to random functions and random scatter. The work will be both theory and data driven. The work may have important payoffs for science is about laws and relationships.<br/><br/>"
"0137292","Group Travel to the Seventh International Valencia Meeting on Bayesian Statistics","DMS","STATISTICS","05/01/2002","11/30/2001","Luis Pericchi","PR","University of Puerto Rico-Rio Piedras","Standard Grant","Marianthi Markatou","04/30/2003","$15,000.00","James Berger","luarpr@gmail.com","18 Ave. Universidad, Ste.1801","San Juan","PR","009252512","7877634949","MPS","1269","0000, OTHR","$0.00","PI: Luis  Perrichi<br/>Proposal #: 0137292<br/><br/>This award provides funds to partially support group travel to the seventh international Valencia meeting on Bayesian Statistics. The money, primarily, support junior researchers and graduate students with limited sources of funding. Members of the underrepresented groups will be actively recruited and encouraged to apply. The meeting will be held in Tenerife, Spain, from June 1-6, 2002. It brings together leading experts in the area of Bayesian Statistics and junior researchers, facilitating scientific interactions and providing a stimulating environment for the exchange of ideas. A pre-conference tutorial will be held that is targeted towards junior investigators, graduate students and people with limited exposure to the vast developments in the area of Bayesian statistics.  <br/>"
"0223304","Service Engineering of Human Tele-Queues: Empirically Based Stochastic Analysis of Telephone Call Centers","DMS","STATISTICS, SERVICE ENTERPRISE SYSTEMS","07/15/2002","08/09/2002","Lawrence Brown","PA","University of Pennsylvania","Standard Grant","Xuming He","12/31/2003","$150,000.00","Linda Zhao","lbrown@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269, 1787","0000, OTHR","$0.00","Abstract<br/>PI: Lawrence Brown (Proposal ID: 0223304)<br/>Title: Service Engineering of Human Tele-Queues: Empirically Based Stochastic Analysis of Telephone Call Centers<br/><br/>A call center is a service network in which agents provide <br/>tele-services, here to be interpreted as either telephone-based services or, more generally, any online-service with customers and servers being remote from each other. Sound scientific principles are prerequisites for sustaining the complex socio-technical enterprise of the call center. The overall goal is to contribute to the theory that supports these principles and to the creation of new ones.<br/><br/>The proposed research builds from an analysis of two unique records of call center operations. These contain complete call-by-call data for the operation over approximately one year of two call centers of different size and operational character. This data will be analyzed in order to understand the mathematical queueing and network characteristics of such centers as they actually operate. Special attention will be given to developing an understanding of the role of scale and efficiency in relation to such operations. The operational features of these two call centers will then be adapted into realistic mathematical models for their operations. On the basis of preliminary work already done, it is expected that these mathematical models will differ in important respects from those in current use for analyzing and predicting performance of such large-scale service operations. In particular, it is expected that new theory will be specifically developed to model the queueing characteristics of centers that operate in the high-volume high-efficiency domain.<br/><br/>Call centers can be viewed naturally and usefully as queueing systems. The most widely used queueing model for a call center is the Erlang-C model, also denoted the M/M/S queue. The Erlang-C model is deficient as an accurate model of a call center in several respects. One of these is that it does not accommodate customers' impatience while waiting. <br/><br/>The Erlang-C model also involves other assumptions such as Poisson arrival times and exponential service times. These will be tested against the data, and modified theory will be developed as needed. The statistical analysis will be derived according to the primitive elements of a call-center: system arrivals, queueing behavior, and services. The interactions of these primitives will then be analyzed, and alternate queueing models will be examined.<br/>"
"0204556","Nonparametric and Semiparametric Methods for Longitudinal Data Analysis","DMS","STATISTICS","07/01/2002","07/25/2002","Jianhua Huang","PA","University of Pennsylvania","Standard Grant","grace yang","06/30/2005","$99,000.00","","jianhua@stat.tamu.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0204556<br/>PI: Jianhua Huang<br/><br/>Longitudinal data, which involve variables observed repeatedly over time, are common in biomedicine, epidemiology, economics, sociology, and many other fields. Nonparametric and semiparametric statistical methods provide effective tools for extracting useful information from this type of data. These methods allow scientists, policy makers and researchers to draw conclusions from their data without depending on pre-specified assumptions that may be too restrictive to their settings. The objective of this proposal is to develop systematic, theoretically well-founded, and more efficient such methods. <br/><br/>The focus is on the following five topics: (i) further theoretical and methodological development of the spline-based approach to time-varying coefficient models; (ii) estimation of covariance structures; (iii) extension of time-varying coefficient models to generalized linear models; (iv) extension of time-varying coefficient models to take into account accumulative covariate effects; (v) semiparametric efficient estimation in partly linear models. These projects involve developing novel estimation and inference procedures, providing theoretical justification, and discussing their theoretical and practical importance to the advancement of biomedical and statistical science. The research approach will be a combination of theoretical asymptotic analysis, Monte Carlo simulations and real data analysis. Global smoothing techniques with spline functions will be used.  <br/>"
"0207059","Grants for Students and Junior Scientists to Attend the Conference on Designs for Generalized Linear Models, April 18-20, 2002, Gaithersburg, Maryland","DMS","STATISTICS","02/01/2002","01/18/2002","A. Khuri","FL","University of Florida","Standard Grant","John Stufken","07/31/2002","$8,000.00","Ana Ivelisse Aviles","ufakhuri@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","0000, OTHR","$0.00","This proposal pertains to a conference on Designs for Generalized Linear Models (GLMs) to be organized by the Department of Statistics, University of Florida, and the Statistical Engineering Division of the National Institute of Standards and Technology (NIST). The meeting will be of a three-day duration (April 18-20, 2002) and will be held on the NIST campus in Gaithersburg, MD. The Conference will feature leading active researchers in the field of GLMs. One goal of the conference is to provide a forum for interaction among people working on diverse areas of GLMs. The second goal is to provide a stimulus to young researchers and graduate students working in the design area of GLMs.<br/><br/>Funding from this proposal will provide travel grants for eight doctoral students and junior scientists to attend the Conference. Special efforts will be made to solicit applications from members of under-represented groups (persons with disabilities, minorities, and women). Priority will be given to Ph.D. students who are U.S. citizens or permanent residents. It is hoped that the grant awardees will benefit from the interaction with the invited speakers, who can provide mentoring and role modeling for the awardees.<br/>"
"0204594","General Theory of Minimum Aberration and its Applications","DMS","STATISTICS","08/01/2002","08/09/2002","Boxin Tang","TN","University of Memphis","Standard Grant","grace yang","07/31/2005","$111,823.00","","boxint@sfu.ca","Administration 315","Memphis","TN","381523370","9016783251","MPS","1269","0000, OTHR","$0.00","Proposal ID: DMS-0204594<br/>PI: Boxin Tang<br/>Title: General theory of minimum aberration and its applications<br/><br/>Abstract<br/><br/>The investigator and his colleagues consider the problem of design selection for fractional factorials.  In situations where there is little or no knowledge about effects that are potentially important, it is appropriate to select designs using the minimum aberration criterion. Very often, the experimenter may have reasons to believe that certain two-factor interactions are important. Appropriate designs under such circumstances are those allowing joint estimation of all main effects and these potentially important interactions.<br/>If the effects not in the postulated model, which consists of the main effects and potentially important interactions, cannot be completely ignored, they bias the estimates of the effects in the model. In the proposed research, this problem of design selection is solved by developing a general theory of minimum aberration. The general criterion of aberration to be introduced generalizes the usual criterion of minimum aberration, and it has the robust property that the best designs given by this criterion sequentially minimize the contamination of nonnegligible effects on the estimation of the effects in the postulated model. Developing a general theory of minimum aberration is also motivated by the desire of unifying various versions of minimum aberration in the literature. Is it possible to derive these versions of minimum aberration from a general theory that is based on a sound statistical principle? The general theory of aberration provides a positive answer to this question.  What is more important is that such a general theory enables researchers to derive other versions of minimum aberration that may be more appropriate for given design situations. In the proposal, many research problems are discussed and in some cases, solutions are outlined. <br/><br/>Design of experiments is an area of study in statistics that concerns efficient data collection for industrial experiments and many other areas of scientific investigation. Fractional factorial designs are a class of experimental plans and their importance, both theoretical and practical, cannot be overstated. As exploratory designs, they are directly useful in identifying important factors at the early stage of an investigation. They also form a basis on which other more sophisticated designs can be built. In the proposed research, the investigator and his colleagues study the problem of selecting the best fractional factorial designs when certain prior knowledge regarding the effects of factors is available. The problem is solved by developing a general theory of design selection criteria. In addition to solving the above practical problem, the general theory unifies various existing design selection criteria, and allows researchers to derive other criteria that are more appropriate for their situations.  Involving students in research activities is an important aspect of the proposed research. The proposed research sheds new light on the study of fractional factorial designs, leads to new economical designs for the experiments in the physical, chemical, and engineering sciences, and most importantly, promotes the application of factorial designs in the areas such as biotechnology and medical research that have huge potential to benefit further from the design methodology.<br/>"
"0139948","Collaborative Proposal: FRG: Statistical Analysis of Uncertainty in Climate Change","DMS","STATISTICS, CLIMATE & LARGE-SCALE DYNAMICS","08/01/2002","07/31/2002","Richard Levine","CA","University of California-Davis","Standard Grant","John Stufken","04/30/2003","$55,423.00","","rlevine@mail.sdsu.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269, 5740","0000, 1616, 4444, OTHR","$0.00","Proposal Ids: DMS - 0204232; DMS - 0139948; DMS - 0139903<br/>PIs: L. Mark Berliner; Richard A. Levine; Christopher K. Wikle<br/>Title: FRG: Statistical Analysis of Uncertainty in Climate Change<br/><br/>ABSTRACT<br/><br/>There is a growing consensus among scientists that aspects of our planet's climate are changing due to human influences, though the scientific community acknowledges that substantial uncertainty exists regarding the forms, levels, and impacts of change. Quantifying these uncertainties requires new statistical research informed by climate science. Effective solutions to climate change problems will rely on new methods for combining the information content of models and data in a fashion that quantitatively manages uncertainty. The research team will rely extensively on Bayesian hierarchical modeling and analysis strategies. Specific projects will include (1) developing new probabilistic climate change assessments based on an extensive suite of climate simulations; (2) statistical procedures for combining different climate models to produce climate projections; and (3) assessing regional and local impacts of global climate behavior.<br/><br/>Describing the Earth's climate and predicting its responses to human influences are critical problems in science and public policy. The research team of statisticians and climate modeling experts from the National Center for Atmospheric Research will develop new statistical strategies that combine observations with the information present in computer models for the climate system, while managing the uncertainties implicit in both. Assessing potential impacts of climate change on the environment and human activities is also fraught with uncertainty. The research team will develop integrated methods for predicting climate impacts on regional and local phenomena. These methods will be applied in predicting the El Nino-Southern Oscillation and properties of tornado occurrence in the Central United States.<br/>"
"0328380","Collaborative Proposal: FRG: Statistical Analysis of Uncertainty in Climate Change","DMS","STATISTICS, CLIMATE & LARGE-SCALE DYNAMICS","08/01/2002","03/20/2003","Richard Levine","CA","San Diego State University Foundation","Standard Grant","grace yang","07/31/2006","$55,423.00","","rlevine@mail.sdsu.edu","5250 Campanile Drive","San Diego","CA","921822190","6195945731","MPS","1269, 5740","0000, 1616, 4444, OTHR","$0.00","Proposal Ids: DMS - 0204232; DMS - 0139948; DMS - 0139903<br/>PIs: L. Mark Berliner; Richard A. Levine; Christopher K. Wikle<br/>Title: FRG: Statistical Analysis of Uncertainty in Climate Change<br/><br/>ABSTRACT<br/><br/>There is a growing consensus among scientists that aspects of our planet's climate are changing due to human influences, though the scientific community acknowledges that substantial uncertainty exists regarding the forms, levels, and impacts of change. Quantifying these uncertainties requires new statistical research informed by climate science. Effective solutions to climate change problems will rely on new methods for combining the information content of models and data in a fashion that quantitatively manages uncertainty. The research team will rely extensively on Bayesian hierarchical modeling and analysis strategies. Specific projects will include (1) developing new probabilistic climate change assessments based on an extensive suite of climate simulations; (2) statistical procedures for combining different climate models to produce climate projections; and (3) assessing regional and local impacts of global climate behavior.<br/><br/>Describing the Earth's climate and predicting its responses to human influences are critical problems in science and public policy. The research team of statisticians and climate modeling experts from the National Center for Atmospheric Research will develop new statistical strategies that combine observations with the information present in computer models for the climate system, while managing the uncertainties implicit in both. Assessing potential impacts of climate change on the environment and human activities is also fraught with uncertainty. The research team will develop integrated methods for predicting climate impacts on regional and local phenomena. These methods will be applied in predicting the El Nino-Southern Oscillation and properties of tornado occurrence in the Central United States.<br/>"
"0204869","Nonparametric and Semiparametric Models for High-Dimensional Data","DMS","STATISTICS","08/01/2002","08/15/2002","Hans-Georg Mueller","CA","University of California-Davis","Standard Grant","grace yang","07/31/2005","$158,070.00","","hgmueller@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>DMS-0204869<br/>PI: Hans-Georg Mueller<br/><br/>Title: Nonparametric and Semi-parametric Models for High Dimensional Data<br/><br/>The investigator will focus on statistical models, theory, algorithms and applications geared towards the analysis of high-dimensional and in particular functional data. Semiparametric methods are particularly appropriate for such data since usually little is known about the structure of these data, while at the same time a dimension reduction step is necessary in order to avoid the ""curse of dimension"". Dimension reduction in the form of projections by fitting single index or multiple index models, or by truncating the number of terms included in an expansion of functional data, is therefore a major emphasis of this project. Another emphasis of this project are statistical methods that take into account that curve data often are random curves that are subject to individually varying time scales. This leads to models, theory, methodology and algorithms for time warping of functional data. Curve data are abundant in genetics where dissemination of gene expression profiles is of highest interest and also in the field of aging and mortality. The investigator will develop methods for functional regression, correlation, discriminant and cluster analysis. These methods will provide tools to establish relationships between random functions and allow classification of observed sample curves into distinct categories.<br/><br/>Large and increasingly complex data that are being collected in scientific and other experimental and observational studies are often data that may be viewed as curves or functions. Such data often contain valuable information about the time-dynamics of physical and biological phenomena, and advance statistical techniques are needed to extract it. For example, recordings of repeated cDNA expression data with genetic microarrays may contain valuable information about the dynamics of gene activation patterns and gene regulation. Other examples where such data play a major role concern the relationship between reproduction and aging, the dynamic structure of aging and longevity, or the relationship between various blood proteins that are recorded continuously. The investigator will develop statistical methods and models specifically designed for the analysis and interpretation of such data.<br/> <br/>"
"0204009","Nonregular Designs:    Classification, Optimality and Construction","DMS","STATISTICS","07/15/2002","07/28/2005","Hongquan Xu","CA","University of California-Los Angeles","Standard Grant","grace yang","06/30/2006","$72,704.00","","hqxu@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","0000, OTHR","$0.00","Proposal ID: DMS-0204009<br/>PI: Hongquan Xu<br/>Title: Nonregular designs: Classification, optimality and construction<br/><br/>Nonregular designs, such as Plackett-Burman designs and many other orthogonal arrays, are widely used in practice due to their run size economy and flexibility.  However, many commonly used nonregular designs are not optimal. Unlike regular designs, which have been studied thoroughly in recent years, theory and construction of nonregular designs are still very primitive and urgently needed. The objectives of this proposed research are to classify and construct optimal nonregular designs. In the classification part, general frameworks and novel approaches are proposed for investigating the following important issues: estimation capacity, design efficiency, projection properties, and aliasing structure. In the construction part, novel methods and algorithms are proposed for constructing optimal nonregular designs for a wide variety of situations, including supersaturated designs and block designs.  Coding theory will be employed to develop general results for multi-level and mixed-level nonregular designs.<br/><br/>Experimental design is an effective and commonly used tool in scientific investigation. Over the last century, it has made tremendous impact in many areas of research, including agriculture, biology, manufacturing and high-tech industries, and will continue to do so for the foreseeable future. While most of the existing research has concentrated on the study of regular designs, this proposed research focuses on the study of nonregular designs. Nonregular designs are widely used in practice due to their run size economy and flexibility, for example, Taguchi designs for process improvement and quality control.  However, it is important to note that many commonly used nonregular designs are not optimal. Unlike regular designs, which have been studied thoroughly in recent years, theory and construction of nonregular designs are still very primitive and urgently needed. The objectives of this proposed research are to classify and construct optimal nonregular designs.  This proposal emphasizes an important interdisciplinary connection between error-correcting codes and factorial designs. The proposed work will develop fundamental results for nonregular designs, and will lead to remarkable new advances in design theory and better practice in experimentation.<br/>"
"0204723","Multivariate Nonparametric Methodology Studies","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","08/01/2002","08/14/2003","David Scott","TX","William Marsh Rice University","Continuing grant","grace yang","10/31/2005","$265,148.00","Dennis Cox","scottdw@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1260, 1269","0000, OTHR","$0.00","Proposal ID: DMS-0204723<br/>PI: David Scott<br/>Title: Multivariate nonparametric methodology studies<br/><br/>The investigators will study new nonparametric methodology focusing on the mid-range and high-range dimensions to better understand data modeling, the curse of dimensionality, and problems associated with massive data sets in multivariate regression and density estimation as well as closely related problems in clustering, mixtures, pattern recognition, and dimension reduction. A new data-based parametric estimation algorithm, based upon integrated squared error, will be investigated for its flexibility and robustness. By applying the criterion to the fitting of local polynomials, a new robust nonparametric regression algorithm can be proposed, which will be applied to automatic detection of hundreds of overlapping tracks in subatomic detector experiments. This project will examine semiparametric models for density estimation that can work better than ordinary nonparametric algorithms, extending feasibility by several extra dimensions. Of special interest, this algorithm can be used to fit subsets of a full mixture model. Applications include regression, image processing, clustering, outlier detection, density estimation, and visualization. The project will extend work on spatial modeling and the combination of multiple data surveys into useful data modeling and maps of conditional estimators of factors and their covariates.  Currently, simultaneous mapping of variables is difficult to interpret, due to the availability of data only in discrete spatial areas (e.g. census tracts) and cross-tabulation of the two variables of interest.  By constructing a smooth map of one variable as a second variable varies, a more faithful and accurate understanding of the spatial relationship may be obtained.<br/><br/>Nonparametric methodology is widely used in one and two dimensions, but less so in higher dimensions. This research focuses on the mid-range and high-range dimensions and provides a deeper understanding of the implications to data modeling of the curse of dimensionality and problems associated with massive data sets. Particular emphasis will be given to multivariate regression and density estimation problems, and closely related applications such as clustering, mixture estimation, pattern recognition, and dimension reduction. This proposal examines new points of view, especially related to locally adaptive and spatial estimation, as well as some recent extensions of nonparametric criteria to parametric problems. The new parametric approach has potential for new nonparametric formulations and applications. At a recent National Research Council workshop, numerous scientists identified critical statistical needs in their work with massive data sets:  alternatives to principal components, specialized visualization tools for exploring massive data, better clustering algorithms, and techniques for handling nonstationary data. Results from this research directly impact three of these four critical opportunities. This program represents a comprehensive and long-term attack on a host of important data analytic problems in multivariate estimation. The results will be of long-term theoretical interest and will provide near-term solutions to real-world problems.<br/>"
"0300806","Superefficient Fits to Linear Models","DMS","STATISTICS","08/01/2002","10/31/2002","Rudolph Beran","CA","University of California-Davis","Continuing grant","grace yang","07/31/2004","$200,000.00","","beran@wald.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","0000, OTHR","$0.00"," Recent theory for shrinkage estimators, techniques from signal-processing, and effective algorithms for computing orthonormal bases now make it possible to exploit the superefficiency loophole in classical information bounds for estimation. In linear regression, if the first few vectors in the regression basis closely approximate the unknown mean vector, then the risk of an estimator that shrinks to zero those regression coefficients associated with the unimportant basis vectors can be much smaller than the risk of the least squares estimator. Such shrinkage estimators, which are particular symmetric linear smoothers, realize the benefits of C. Stein's and M. S. Pinsker's pioneering ideas on estimation of high- or infinite-dimensional parameters. Specific goals of the research are: (a) to construct and interpret confidence sets centered at a superefficient fit; (b) to handle, through multiple shrinkage, cases where the chosen basis is sparse but not well-ordered; (c) to develop within- and between-observation shrinkage techniques to handle the multivariate linear model; (d) to draw on relations with signal-processing techniques that use the discrete cosine basis or wavelet bases.<br/><br/> Regression models fitted by the method of least squares are widely used in scientific research and other disciplines to establish quantitative relationships within sets of data. Studies related to the program on Environment and Global Change and to the program on Manufacturing are examples. The broad goal of the proposed research is to improve the reliability of these fitted relationships by replacing least squares with better adaptive linear smoothers. Recent statistical theory supports the general feasibility of this project. How to realize what is possible in theory is the essence of the work. The author's REACT method, described with references in the proposal, is a practical technique for regression with one response variable that demonstrates real-world<br/>improvements over least squares fits. REACT competes well with current nonparametric regression methods while offering certain advantages, such as built-in diagnostics that indicate the quality of the fit. The proposed research will extend REACT methods to finding relationships among sets of variables and will develop practical methods for assessing the uncertainty of the estimated relationships.  Least squares regression, a standard function in modern statistical packages and spreadsheets, is widely used in data-analysis. This circumstance provides strong motivation for improving on least squares."
"0139897","Collaborative Proposal: FRG: Statistical Analysis of Uncertainty in Climate Change","DMS","STATISTICS, CLIMATE & LARGE-SCALE DYNAMICS","08/01/2002","07/26/2002","L. Berliner","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","grace yang","07/31/2006","$190,000.00","","mb@stat.ohio-state.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269, 5740","0000, 1616, 4444, OTHR","$0.00","Proposal Ids: DMS - 0204232; DMS - 0139948; DMS - 0139903<br/>PIs: L. Mark Berliner; Richard A. Levine; Christopher K. Wikle<br/>Title: FRG: Statistical Analysis of Uncertainty in Climate Change<br/><br/>ABSTRACT<br/><br/>There is a growing consensus among scientists that aspects of our planet's climate are changing due to human influences, though the scientific community acknowledges that substantial uncertainty exists regarding the forms, levels, and impacts of change. Quantifying these uncertainties requires new statistical research informed by climate science. Effective solutions to climate change problems will rely on new methods for combining the information content of models and data in a fashion that quantitatively manages uncertainty. The research team will rely extensively on Bayesian hierarchical modeling and analysis strategies. Specific projects will include (1) developing new probabilistic climate change assessments based on an extensive suite of climate simulations; (2) statistical procedures for combining different climate models to produce climate projections; and (3) assessing regional and local impacts of global climate behavior.<br/><br/>Describing the Earth's climate and predicting its responses to human influences are critical problems in science and public policy. The research team of statisticians and climate modeling experts from the National Center for Atmospheric Research will develop new statistical strategies that combine observations with the information present in computer models for the climate system, while managing the uncertainties implicit in both. Assessing potential impacts of climate change on the environment and human activities is also fraught with uncertainty. The research team will develop integrated methods for predicting climate impacts on regional and local phenomena. These methods will be applied in predicting the El Nino-Southern Oscillation and properties of tornado occurrence in the Central United States.<br/>"

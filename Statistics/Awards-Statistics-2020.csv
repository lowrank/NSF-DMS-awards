"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"2015530","Collaborative Research: Statistical Inference for High Dimensional and High Frequency Data","DMS","STATISTICS","07/01/2020","06/26/2020","Lan Zhang","IL","University of Illinois at Chicago","Standard Grant","Yulia Gel","06/30/2024","$149,992.00","","lanzhang@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","","$0.00","To pursue the promise of the big data revolution, the current project will focus on a common form of data, high dimensional high frequency data (HDHFD), where a snapshot of the data involves a large number of variables, and at the same time new data streams in every fraction of milliseconds. With technological advances in data collection, HDHFD occurs in medical applications from neuroscience to patient care; finance and economics; geosciences such as earthquake data; marine science including fishing and shipping; turbulence; internet data; and other areas where data streaming is available. The Principal Investigators' (PIs') research focuses on how to extract information from complex big data and how to turn data into knowledge. In particular, the project seeks to develop cutting-edge mathematics and statistical methodology to uncover the structure governing HDHFD systems. This structure is characterized by a web of dependence across both time and dimension, and the role of analysis is to provide guidance on how to reduce the complexity while retaining the important features of the data architecture. An integral part of this research is also about how to quantify the uncertainty in estimates and forecasts in HDHFD systems. In addition to developing a general theory, the project is concerned with applications to financial data, including risk management, forecasting, and portfolio management. More precise estimators, with improved margins of error, will be useful in all these areas of finance. The results are of interest to main-street investors, regulators and policymakers, and the results are entirely in the public domain. <br/><br/>The purpose of this project is to explore high dimensional high frequency data (HDHFD) from several angles. A fundamental approach is to extend the PIs? contiguity theory. Under a contiguous probability, the structure of the observations is often more accessible (frequently Gaussian) in local neighborhoods, facilitating statistical analysis. This is achieved without altering current models. In a contribution to factor modeling of the HDHFD data, the PIs will explore time-varying matrix decompositions, including the development of a singular value decomposition (SVD) for high frequency data, as a more direct path to a factor model. We plan to compare the new SVD with PCA based methods, as well as L1 type methods such as nonnegative matrix factorization. The PIs have discovered a new way to look at time and cross-dimension dependence, originally developed by the PIs in connection with their observed asymptotic variance (observed AVAR). They will now look into the possibility to ""borrow"" information across time and dimension. This tool will be used for matrix decompositions, as well as to develop volatility matrices for the drift part of a financial process, which will interface with their planned work on matrix decompositions. The PIs will explore a path to an observed AVAR that takes place in continuous time, thereby improving accuracy and simplifying both implementation and theoretical analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2020407","International Indian Statistical Association 2020 Conference: Statistics in the Era of Evidence-Based Inference","DMS","STATISTICS","06/01/2020","04/10/2020","Sanjib Basu","IL","University of Illinois at Chicago","Standard Grant","Yong Zeng","05/31/2023","$34,997.00","Saonli Basu","sbasu@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","7556","$0.00","IISA 2020: Statistics in the Era of Evidence Based Inference, a four-day international conference, will take place at University of Illinois at Chicago (UIC), Illinois, from July 16 to July 19, 2020. The main objective of the conference is to bring together both well established and emerging young researchers, as well as students, who are actively pursuing theoretical and methodological research in statistics, biostatistics and data science and their applications in various scientific  fields. The format of the conference includes student paper competitions and presentations, short courses, plenary talks, special invited talks, invited sessions and contributed posters. The conference strives to maintain a healthy presence of women and minority in all these categories and of young researchers (within five years of their doctoral degrees) in invited sessions. For students, the conference will provide numerous opportunities including student paper competitions, speed sessions as well as short courses. The scientific program will span a wide range of topics ranging from the fundamentals of statistical theory to computationally-intensive methods and scientific application oriented data modeling, through a number of sessions classified as plenary, special invited, invited, student paper competition, posters, and panel discussions. In the tradition of IISA conferences, IISA 2020 will host a vibrant multi-stage student paper competition. The scientific program of IISA2020 will have plenary talks by three eminent speakers, special invited talks by eight leading researchers and about 50 invited sessions, each consisting of three presentations<br/><br/>The conference serves as the official annual meeting of the International Indian Statistical Association (IISA), IISA membership is open to all. IISA?s objectives are to promote education, research and application of statistics, probability and data science, to foster the exchange of information and scholarly activities, to serve the needs of young statisticians and data scientists and to encourage cooperative efforts among members in education, research, industry and business. IISA and University of Illinois at Chicago (UIC) are the primary organizers of the conference. UIC is part of the University of Illinois system and is the largest university in the Chicago area with a strong focus on diversity. IISA 2020 will bring together leaders, emerging researchers and students from diverse areas of statistics, biostatistics, probability and data science in a collaborative setting to discuss and foster the cutting-edge developments of statistical theory, methods and tools for making evidence based statistical inference from complex, noisy, tall and wide data sources. The conference will promote education, research and application of statistics, probability and data science and will foster the exchange of information and scholarly activities among scientific institutions in Chicago area, Midwest and in institutions across the United States.  The most up to date information and announcements regarding the conference is made available at the conference website https://www.intindstat.org/iisaconference2020/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1937229","RTG: Applied Mathematics and Statistics for Data-Driven Discovery","DMS","APPLIED MATHEMATICS, STATISTICS, WORKFORCE IN THE MATHEMAT SCI","04/01/2020","04/28/2023","Kevin Lin","AZ","University of Arizona","Continuing Grant","Pedro Embid","03/31/2025","$1,651,843.00","Hao Zhang, Laura Miller, Michael Chertkov, David Glickenstein, Matthias Morzfeld","klin@math.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1266, 1269, 7335","062Z, 079Z, 7301","$0.00","The simultaneous availability of large datasets, high performance computing, and modern machine learning algorithms holds great promise to enable scientists and engineers to rapidly discover hidden patterns in data, and to utilize these patterns to understand the natural world in order to solve pressing practical problems facing society. Realizing this promise requires addressing many mathematical and computational challenges: in framing scientific and technological problems for solution by data-driven approaches, in interpreting and analyzing data, and in designing efficient and reliable algorithms. There is an urgent need for mathematical scientists who are equally adept at wielding modern applied and computational mathematics on the one hand, and the tools of data-driven modeling, statistical inference, and scientific computing on the other. Furthermore, as interdisciplinary research and development become more common in industry, academia, and government, it is imperative that such mathematical scientists be generalists, able to communicate and work with specialists from diverse fields. This Research Training Group (RTG) addresses this need by increasing the number of mathematical scientists capable of working effectively at the interface of applied mathematics/statistics and modern data science. By focusing on specific applications requiring both mathematical innovation and data-driven modeling and by forming teams of mathematical scientists and domain experts, the RTG will enable trainees to address new challenges in innovative ways using their mastery of relevant mathematics, statistics and data science, and domain knowledge. Recognizing the challenges of advanced studies in STEM fields, the RTG will promote close, small-group mentoring at all levels. The expected outcome is mathematical scientists adept at working at disciplinary boundaries and intellectually equipped to tackle a wide range of scientific and technological challenges. It is expected that some of the trainees will continue in academia, where the proposed training activities can be improved and propagated; others will work in industry and government, applying their knowledge and skills to solve problems of practical significance.<br/><br/>The RTG will support research on applied mathematics and data-driven modeling at the University of Arizona (UA), which is home to a large and vibrant mathematical science community. It is organized around a number of application-centered Working Groups, with foci ranging from analysis of gene regulation data to the modeling and forecasting of power grids. Each research project will impact both fundamental methodology and practical applications. The Working Groups are structured to enable vertically-integrated mentoring of RTG trainees at all levels -- undergraduate, graduate, and postdoctoral, and to enable trainees to work closely with Mathematics faculty and domain experts. Additional training activities include courses on foundational topics, e.g., optimization, machine learning, Monte Carlo methods, as well as practical skills such as software carpentry. By providing research training at the interface between the traditional domains of applied mathematics and the cutting-edge field of data-driven modeling, the RTG will both advance scientific knowledge and increase the number of US citizens and nationals with much-needed scientific and technological expertise.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015568","Collaborative Research: High-Dimensional Decision Making and Inference with Applications for Personalized Medicine","DMS","STATISTICS","06/15/2020","07/15/2022","Zhaoran Wang","IL","Northwestern University","Continuing Grant","Yong Zeng","05/31/2023","$100,000.00","","zhaoranwang@gmail.com","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1269","079Z","$0.00","With the advent of data collection and storage technology, researchers can obtain large-scale and high-dimensional datasets at a low price. Such datasets offer exciting opportunities to make better decisions and reveal new discoveries to improve decision making in various applications, and meanwhile, also raise statistical challenges. Over the past decades, regularization methods such as Lasso, SCAD, and MCP have been proposed to conduct model estimation in the presence of high dimensional covariates. Various numerical algorithms have been developed for these methods, and their theoretical properties are well studied. However, questions of how to efficiently and effectively utilize high-dimensional data to make optimal decisions and conduct inference are relatively less studied, although such problems are of vital practical importance. This project will develop new methods and theories for making optimal decisions and conducting valid inference under high-dimensional settings. The methods have wide applications, for instance, in personalized medicine where the goal is to determine the optimal treatments for a patient based on predictor information, including several thousand genetic markers. The principal investigators will develop and distribute user-friendly open-source software to practitioners and provide training opportunities to students at different levels. <br/><br/>The project has three research aims. The first aim is to study the high-dimensional contextual bandit problem with binary actions, which is an online decision-making problem that finds applications in personalized healthcare and precision medicine. In this problem, the player sequentially chooses one action and observes a reward, where the goal is to maximize the reward. The principal investigators will develop a new algorithm to provide an optimal decision rule, which achieves the minimax optimal regret. The second aim is to study general inference problems that arise from high-dimensional stochastic convex optimization, where the goal is to quantify the uncertainties of the optimal objective value. The third goal is to consider the general stochastic linear bandit problem with a finite and random action space. The principal investigators will develop a new algorithm by using a best-subset-selection type estimator, and the approach achieves a ""dimension-free"" regret and meets existing lower-bound under the low-dimensional setting.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015190","Statistical Modeling and Inference for Network Data in Modern Applications","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","07/01/2020","06/27/2022","Emma Jingfei Zhang","FL","University of Miami","Continuing Grant","Pena Edsel","05/31/2023","$192,500.00","","jingfei.zhang@emory.edu","1320 S DIXIE HWY STE 650","CORAL GABLES","FL","331462919","3052843924","MPS","1269, 7454","068Z","$0.00","In modern data science, networks have emerged as one of the most important and ubiquitous types of non-traditional data. Recently, data sets with a large number of independent network-valued samples have become increasingly available. In such data sets, a network serves as the basic data object, and they are commonly seen in neuroscience, genetic studies, microbiome studies, and social cognitive studies. Such types of data bring statistical challenges that cannot be adequately addressed by existing tools. This project seeks to provide foundational perspectives on the emerging inferential and computational challenges in modeling a population and populations of networks. The theory and methods developed here will allow us to characterize the network connectivity at the population-level, and to monitor how the subject-level connectivity changes as a function of subject characteristics. Quantifying such subject-level differences has become central in studying the human brain, genetics, and medicine in general. Motivated by applications in neuroscience, this research will be beneficial for a variety of fields that study brain development, aging, and disease diagnosis, progression and treatment. Integration of research and education will be achieved through training undergraduate and graduate students, and developing special topics graduate courses.<br/><br/><br/>This project aims to develop a new network response model framework, in which the networks are treated as responses and the network-level covariates as predictors. The framework developed in this project, under appropriate structural constraints, will preserve the intrinsic characteristics of networks, ensure model identifiability, facilitate scalable computation, and allow valid statistical inference. A variety of fundamental and critical computational and inferential challenges will be addressed under this framework, including model identifiability, efficient computation, quantifying computational and statistical errors, and debiased inference. Additionally, the investigator will develop two novel goodness-of-fit tests for a broad class of network models, including those considered in this project. Further, the investigator will investigate modeling with heterogeneity by developing a network mixed-effect model, and a framework for model-based network clustering. Developments in both directions are formulated to take into account the rich information from subject covariates. The theory to be developed under asymptotic regimes allows the network size, the number of network samples, and the model complexity (e.g., rank, sparsity, number of clusters) to increase at reasonable rates.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015544","Collaborative Research: Statistical Inference for High Dimensional and High Frequency Data","DMS","STATISTICS","07/01/2020","06/26/2020","Per Mykland","IL","University of Chicago","Standard Grant","Yulia Gel","06/30/2024","$300,000.00","","mykland@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","","$0.00","To pursue the promise of the big data revolution, the current project will focus on a common form of data, high dimensional high frequency data (HDHFD), where a snapshot of the data involves a large number of variables, and at the same time new data streams in every fraction of milliseconds. With technological advances in data collection, HDHFD occurs in medical applications from neuroscience to patient care; finance and economics; geosciences such as earthquake data; marine science including fishing and shipping; turbulence; internet data; and other areas where data streaming is available. The Principal Investigators' (PIs') research focuses on how to extract information from complex big data and how to turn data into knowledge. In particular, the project seeks to develop cutting-edge mathematics and statistical methodology to uncover the structure governing HDHFD systems. This structure is characterized by a web of dependence across both time and dimension, and the role of analysis is to provide guidance on how to reduce the complexity while retaining the important features of the data architecture. An integral part of this research is also about how to quantify the uncertainty in estimates and forecasts in HDHFD systems. In addition to developing a general theory, the project is concerned with applications to financial data, including risk management, forecasting, and portfolio management. More precise estimators, with improved margins of error, will be useful in all these areas of finance. The results are of interest to main-street investors, regulators and policymakers, and the results are entirely in the public domain.<br/><br/>The purpose of this project is to explore high dimensional high frequency data (HDHFD) from several angles. A fundamental approach is to extend the PIs? contiguity theory. Under a contiguous probability, the structure of the observations is often more accessible (frequently Gaussian) in local neighborhoods, facilitating statistical analysis. This is achieved without altering current models. In a contribution to factor modeling of the HDHFD data, the PIs will explore time-varying matrix decompositions, including the development of a singular value decomposition (SVD) for high frequency data, as a more direct path to a factor model. We plan to compare the new SVD with PCA based methods, as well as L1 type methods such as nonnegative matrix factorization. The PIs have discovered a new way to look at time and cross-dimension dependence, originally developed by the PIs in connection with their observed asymptotic variance (observed AVAR). They will now look into the possibility to ""borrow"" information across time and dimension. This tool will be used for matrix decompositions, as well as to develop volatility matrices for the drift part of a financial process, which will interface with their planned work on matrix decompositions. The PIs will explore a path to an observed AVAR that takes place in continuous time, thereby improving accuracy and simplifying both implementation and theoretical analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2054173","Collaborative Research: Development of New Statistical Methods for Genome-Wide Association Studies","DMS","STATISTICS, MATHEMATICAL BIOLOGY","09/01/2020","10/03/2022","Tieming Ji","VA","Virginia Polytechnic Institute and State University","Standard Grant","Zhilan Feng","06/30/2023","$157,024.00","","jit@missouri.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1269, 7334","9150","$0.00","Advances in high-throughput sequencing technologies now make possible cost-effective analysis of whole genomes. The genomes of any two humans are 99.9% identical, with differences in the remaining 0.1% determining the diversity of human traits. For example, DNA sequence differences account for 80% of the variability in human height. Current technology allows the identification of these sequence polymorphisms between individuals, which can then be correlated to differences in a given trait. When done on a genome wide level with a large population of individuals, such genome wide association studies (GWASes) can be a useful tool for the identification of key genes controlling specific traits. However, a requirement for this approach is the availability of powerful and accurate statistical and computational methods to search through a massive amount of sequencing data to correctly identify DNA differences associated with the phenotypic trait of interest. The outcome of the project will (1) provide statistical methods to understand relationships between DNA sequence differences and the full range of diversity observed in a population, and (2) provide corresponding computational tools suitable for use by biologists and biomedical specialists for their specific population studies. This research project will produce intermediate methodological and theoretical results that lay the foundation for the final output. This project will also apply the developed methods to real, experimental data to demonstrate their utility. In addition to these research outcomes, the project will support the training of students in the field, including women and underrepresented minorities.<br/> <br/>GWAS estimates the correlation between phenotypic traits and sequence polymorphisms to identify genetic variants highly associated with specific traits. Single nucleotide polymorphisms (SNPs) are the most common type of genetic variant, and sequencing technologies allow for large-scale collection of SNP information. The project team will develop new GWAS models and methods to find trait-affecting variants with more power and accuracy. Specifically, the new methods developed in this research project will improve existing approaches by allowing modeling of observed traits from any probabilistic distribution in the exponential family. This extension ensures statistical models are biologically meaningful and interpretable. Second, the new methods will exploit different Bayesian priors, especially contemporary Bayesian priors for ultra-high dimensional model selection, that will share information across the entire genome for stable statistical inferences. Theoretical results of Bayesian priors in these new methods will also be developed. Third, a stochastic search algorithm will be developed to efficiently search through the massively large model space for model selection. This ensures that new methods are practical and useful since analysis can be done within a reasonably short time frame. Meanwhile, this also eliminates the use of subjective thresholds of significance that are now commonly used but an embarrassing practice in GWAS, having no theoretical support. Methods will be implemented into software tools and will be freely available for statisticians, biologists, and biomedical researchers. This project is funded jointly by the Division of Mathematical Sciences Mathematical Biology Program and the Statistics Program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2014221","Efficient Estimation of Treatment Effects via Nonparametric Machine Learning","DMS","STATISTICS","07/01/2020","06/19/2020","Shujie Ma","CA","University of California-Riverside","Standard Grant","Yong Zeng","06/30/2023","$154,390.00","","shujie.ma@ucr.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","MPS","1269","079Z","$0.00","Advances in technology have created numerous large-scale datasets in observational studies, which bring unprecedented opportunities for evaluating the effectiveness of various treatments. The complex nature of large-scale observational data, such as its massive volume and high dimensions in confounders, pose great challenges to the existing conventional methods for causal analysis. The corresponding statistical implication is that even a small amount of bias can easily lead to erroneous conclusions. Thanks to the rapid development of scalable computing techniques, nonparametric machine learning methods have a strong potential for bias reduction via employing data-driven strategies for dimensionality reduction. However, careful consideration must be given in order to realize the potential for studying the underlying causal mechanisms. This project aims to develop cutting-edge statistical methods with theoretical insights for causality analysis using deep neural networks. The new statistical tools meet the immediate needs from various scientific areas for exploring causal relationships from large-scale observational data. The research will facilitate the causality analysis of modern complex data with important applications. This project will integrate research and education through course development, open-source software development, and undergraduate and graduate student training. <br/> <br/>The PI will develop a new unified approach with thorough theoretical justifications for efficient estimation of causal effects using deep neural networks. The method will then be applied to large-scale datasets with binary, multi-valued, or continuous-valued treatment variables. Three interconnected topics will be pursued. Specifically, the PI will offer a new perspective on learning treatment effects through a generalized optimization estimation. As a result, two convenient and efficient estimators of treatment effects will be developed, respectively, in the second and third topics. The estimators involve one nuisance model that will be approximated by deep neural networks, which will be investigated in the first topic. The general optimization framework includes the average, quantile and asymmetric least squares treatment effects as special cases. The methods take full advantage of the large sample size of large-scale data and provide effective protection against model mis-specification bias. The project involves devising new machine learning methods and algorithms for causal studies, establishing the theoretical validity, and developing valid inference procedures. It will promote machine learning methods for causality analysis, and will break new ground in drawing causal inference from large-scale observational datasets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1951474","A Statistical Learning Framework for Phylogenetic Inference: Information, Uncertainty, and Geometry","DMS","STATISTICS, MATHEMATICAL BIOLOGY, MSPA-INTERDISCIPLINARY","08/01/2020","07/20/2020","Vu Dinh","DE","University of Delaware","Standard Grant","Zhilan Feng","07/31/2024","$219,573.00","","vucdinh@udel.edu","220 HULLIHEN HALL","NEWARK","DE","197160099","3028312136","MPS","1269, 7334, 7454","068Z, 9150, 9251","$0.00","Phylogenetics, the study of the evolutionary relationships among individuals or groups of organisms from molecular sequence data, is a dominant theme in biological research. In the last few decades, the explosion in the amount of available data for phylogenetic inference has offered great opportunities to further our understanding of various biological processes. At the same time, they also move phylogenetics to a new learning regime, where traditional theories can no longer guide developments and interpretations of phylogenetic algorithms. The main goals of this research are to improve our understanding of the central concepts of phylogenetics through the viewpoints of statistical learning and information theory and to provide essential tools for phylogenetic analyses in this new learning setting. By providing a framework to design, analyze and improve phylogenetic estimators, the research will greatly extend the set of problems for which reliable analyses can be obtained. Most notably, our research in the setting when the number of species increases is especially amenable to the biology of small evolving units, including studies of viruses and antibody-making B-cells. The education component of this study involves mentoring undergraduate and graduate students with independent research in phylogenetics and will produce various demonstrations, tutorials and statistical packages for phylogenetic inference.<br/><br/>The proposed research lays out the foundation for explicit quantification of phylogenetic information and uncertainty, with a focus on the setting where sequence data are continually being generated and analyzed. This approach enables the use of local phylogenetic methods as a means to analyze likelihood-based methods and helps investigate systematic ways to stabilize uncertainty while retaining essential information. Two analytical tools to construct and analyze phylogenetic methods in non-asymptotic settings will be developed: a new class of concentration inequalities for evolutionary-related random variables and a Taylor-like local-to-global expansion of phylogenetic likelihood on the space of phylogenetic trees. These newly derived tools will be used to study several important inference problems, including species tree/supertree reconstruction and parameter estimation in viral phylogenetics and trait evolution. The research activities will provide important insights into the influences of phylogenetic information and uncertainty on the stability of a phylogenetic estimate.<br/><br/>This award is co-funded with the Statistics program and the Life Science Venture Fund in DMS.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015363","Theoretical Guarantees of Statistical Methodologies Involving Nonconvex Objectives and the Difference-Of-Convex-Functions Algorithms","DMS","STATISTICS","08/01/2020","06/16/2020","Xiaoming Huo","GA","Georgia Tech Research Corporation","Standard Grant","Yong Zeng","07/31/2023","$300,000.00","","xiaoming@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","079Z","$0.00","This project will extend the statistical literature that involves nonconvex optimization to contemporary models. In many contemporary machine learning and/or artificial intelligence applications, deep learning and relevant neural network models are utilized. Extending these theories to other contemporary frameworks can potentially lead to a theoretical foundation for modern techniques such as deep learning. The research project has great potential to make a significant impact on the broad scientific community, who have the needs of performing inferences for their enormous data. Besides scholarly publications and presentations, the research will lead to new teaching modules in statistics and machine learning courses. Ph.D. students will be supported and exposed to asymptotic theory and computational algorithms. New toolboxes will be developed and made available online. Packages are developed so that engineering students (including undergraduates) at Georgia Tech and other universities can use them in their course projects (for example, the undergraduate senior design projects at the School of Industrial and Systems Engineering at Georgia Tech). The PI has organized many influential workshops in the past, including one on the foundation of deep learning, and will continue doing so. <br/><br/>Specific aims include the following. The research work will extend the theory on the statistical properties of potentially fully neural network models to some other neural network models under different structures, such as the convolutional neural networks, to explore the relation between the inferential property and the neural network architecture. The project is to derive the theoretical guarantees of statistical estimators that are based on nonconvex optimization in more general settings. The PI will explore the possibility to carrying out similar analysis in neural network-based models. Statistical model selection can be utilized in identification of partial differential equations. The project is to establish the corresponding statistical theory and uncover the related practical implication.  A set of open-source software products along with related documentation will be generated, to make our work conveniently reproducible. Existing tools (such as GitHub.com or equivalents) will be utilized to disseminate these tools. The applicability and need of the new methods will be explored in a wide spectrum of application domains. Inference techniques with nonconvex objective functions is a fundamental problem in many contemporary techniques, including the neural network based deep learning methodology. This project will contribute to this research. There are evident societal needs for inference from large datasets, and the results of this project can have many applications. The project will contribute to the statistical literature by exploring a new research frontier in statistical sciences. Our work is interdisciplinary and can bridge the communities of optimization and statistics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2014951","Semiparametric Adaptive Designs and Statistical Inference for Both the Mean and the Quantiles","DMS","STATISTICS","09/01/2020","10/25/2022","Hongjian Zhu","TX","The University of Texas Health Science Center at Houston","Standard Grant","Yong Zeng","08/31/2024","$100,000.00","","Hongjian.Zhu@uth.tmc.edu","7000 FANNIN ST","HOUSTON","TX","770305400","7135003999","MPS","1269","","$0.00","Adaptive design is an important and active research area driven by diverse requirements of clinical trials. However, most adaptive randomization designs either do not make good use of the available covariates or depend on unnecessary model assumptions, so the current state of adaptive designs does not match the data-rich environment. This project seeks to develop new theory and methodology for adaptive designs to streamline clinical trials by efficiently incorporating a vast amount of covariate information without model misspecification. The success of the project will allow a large quantity of available data to be utilized in adaptive designs, and trial participants to avoid unnecessary unsafe exposure. The research will have broad impacts on general experimental designs and their applications in fields such as product quality, food industry, energy and architecture, and computer simulation models. The PI will integrate research and education by promoting the adaptive designs among students, researchers, physicians, and project managers through courses and presentations, involving women and underrepresented minority students in research, and making presentations at minority-serving institutions.<br/> <br/>The project will focus on three main research directions. First, the PI plans to develop a new family of semiparametric covariate-adjusted response-adaptive (CARA) designs as well as analysis approaches that can achieve the objectives related to efficiency and ethics and incorporate many covariates without model misspecification. Second, in many fields, scientists are more interested in the tail quantiles than the mean. In addition, quantile inference is often a secondary analysis in clinical trials, allowing researchers and policymakers to detect the points along the distribution that may be the most amenable to the new treatment. The PI plans to develop a new family of semiparametric CARA designs and methods for quantile inference. Third, there is an urgent need to reduce development costs and shorten the time-to-market of new therapies. The PI plans to develop seamless phase II/III CARA designs with sequential monitoring. Both hypothesis testing and estimation will be investigated. Thus, the advantages of adaptive randomization, adaptive seamless design, and sequential monitoring will be combined in a single trial. Both asymptotic and finite-sample properties will be explored, and guidance for clinical trials will be offered.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1952539","FRG: Collaborative Research: Generative Learning on Unstructured Data with Applications to Natural Language Processing and Hyperlink Prediction","DMS","STATISTICS","07/01/2020","04/02/2020","Xiaotong Shen","MN","University of Minnesota-Twin Cities","Standard Grant","Yulia Gel","06/30/2024","$300,000.00","","xshen@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","075Z, 079Z, 1616","$0.00","This project addresses the pressing needs of analyzing ?big? unstructured data and tackles some artificial intelligence questions from the statistical perspective, which requires the focused and synergistic e?orts of a collaborative team. Specifically, the project develops generative models for statistical learning and leverages dependence relations modeled by graphical models in hyperlink prediction, which are applicable to topic sentence generation and protein structure identification. It will lead to a substantial improvement in the accuracy of generative learning based on numerical embeddings, particularly in topic sentence generation and hyperlink prediction. The integrated program of research and education will have significant impacts on machine learning and data science, social and political sciences, and biomedical and genomic research, among others. The project requires extensive algorithm and software development for natural language processing and multimedia data integration. The PIs, their postdocs, and students will develop innovative computational algorithms and software for the analysis of large-scale unstructured complex data. The advanced computational tools will be disseminated to facilitate technology transfer. <br/><br/><br/>The project will address some fundamental issues in two important areas of unstructured data analysis in machine learning and intelligence. In particular, the proposed research will develop a statistical framework for generative learning, which is primarily motivated by applications for unstructured data, namely topic sentence generation and high-order hyperlink prediction. The research will develop powerful generative methods for generating instances or examples to describe and interpret the corresponding learning model.  Moreover, it will develop network models for modeling high-order interactions and relations of units by identifying hidden structures in networks. It will proceed in two areas: (1) instance generation and topic sentence generation; (2) hyperlink prediction for multiway relations in hypergraphs. In the first area, instance generation, particularly sentence generation, will be performed collaboratively with numerical embeddings in categorization and regression. In the second area, hyperlinks will be predicted based on observed pairwise as well as unobserved high-order relations, characterized by graphical models with hidden structures. Special effort will be devoted to inverse learning, the integration of data from multiple sources, and extracting latent structures of networks.  Finally, the research will develop computational tools and design practical methods that have desirable statistical properties.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2013905","Cluster Analysis for High-Dimensional and Multi-Source Data","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","08/01/2020","06/20/2020","Jia Li","PA","Pennsylvania State Univ University Park","Standard Grant","Yong Zeng","07/31/2023","$225,000.00","Lynn Lin","jiali@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269, 7454","068Z, 079Z","$0.00","Rapid technology advances in devices and computer systems continue to grow our capacity to collect and store data. Clustering is often the first stage analysis performed to discover patterns, gain insights, and extract knowledge from massive amount of data routinely faced in science, engineering, and commercial domains. For instance, in biomedical studies, clustering is used to reveal pathological subgroups and help researchers form new hypothesis for in-depth investigation.  It is thus imperative to develop new clustering methods to meet the ever-increasing challenges of data with high complexity, huge volume, and from distributed sources. In this project, novel statistical and optimization-based approaches and software packages will be developed to address these challenges. Graduate students will be trained to conduct research at the forefront of machine learning. The research results will be used to enrich courses and outreach educational materials in data science.<br/> <br/>A prominent statistical paradigm for clustering is based on mixture models, which is objective, parsimonious, not biased for known clusters, and has a probabilistic framework that can be extended and interpreted in standard ways. For high-dimensional large-scale data, existing mixture-model based methods have fundamental limitations. Furthermore, a big data environment can require the integration of clustering results at distributed sites, a problem called multi-source clustering. This research will advance cluster analysis from multiple aspects. First, hidden Markov model on variable blocks (HMM-VB), a special Gaussian mixture model (GMM), is developed to tackle high dimensionality. The estimation of HMM-VB will be enhanced by computationally efficient methods to identify the latent variable block structure and by mixture factor analyzers. Second, leveraging the latent states of HMM-VB, a new variable selection approach will be developed for clustering high-dimensional data. Third, the emerging topic of multi-source clustering will be studied. New methods based on optimal transport and Wasserstein barycenter will be developed for aggregating clustering results from multiple sources. Applications in biomedical areas will be pursued.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015405","Active Sequential Change-Point Analysis of Multi-Stream Data","DMS","STATISTICS","08/15/2020","06/20/2020","Yajun Mei","GA","Georgia Tech Research Corporation","Standard Grant","Yong Zeng","07/31/2024","$210,000.00","","yajun.mei@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","062Z, 079Z","$0.00","This project aims to develop efficient methodologies and algorithms for actively learning from high-dimensional streaming data under the sampling or resource constraints. In many real-world applications, a system consists of many processes that can generate many data streams. At some unknown time, an unusual event could occur to the system, for example, a disease outbreak, a manufacturing defect, or a fraud signal, yielding a set of anomalous processes. Most systems however are operated under resource constraints that prevent the simultaneous use of all resources all the time. Thus, the decision maker must be responsible for actively choosing which processes are prioritized for observation. This will enhance their existing knowledge about the occurring event or anomalous processes while exploring new information and accounting for the penalty of the wrong declaration. The research would have broader impacts in a wide range of real-world applications such as biosurveillance, epidemiology, engineering, homeland security, and finance. The project will integrate research and education by infusing research findings into the curriculum and by training graduate students.<br/><br/>This project seeks to make comprehensive progress on methodology, theory, and application of active sequential change-point analysis of multi-stream data under the sampling or resource constraints.  The specific research aims are: (1) design efficient active change-point detection algorithms with false alarm guarantees, (2) develop an asymptotic theory to characterize statistical performances of the developed methods, (3) post-hoc analysis to apply false discovery rate methods to identify anomalous processes; and (4) applications in sepsis screening with online monitoring data from medical sensors in intensive care units to identify sepsis patients as quickly as possible while avoiding alarm fatigue. Results of the project are expected to significantly advance the state of the art in sequential analysis, change-point detection, multi-armed bandit problems, streaming data analysis, and large-scale inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015481","Correlated Graphical Models for High-Dimensional Heterogeneous Data: Theory, Optimization, and Applications","DMS","STATISTICS","08/01/2020","07/28/2020","Yuping Zhang","CT","University of Connecticut","Standard Grant","Yong Zeng","07/31/2023","$100,000.00","","yuping.zhang@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","","$0.00","This project is motivated by the pressing need for analyzing modern high-dimensional heterogeneous data from multiple sources. Driven by high-throughput biotechnologies, it is increasingly common to have multiple types of measurements on the same set of subjects. Integration of heterogeneous data types is the key to gaining fundamental knowledge on biological processes. Complex characteristics of modern biological data also result in challenges for data analysis and statistical modeling. This project will contribute to theoretical and methodological development through novel graphical models suitable for fusing correlated and mixed data from static and dynamic conditions. These approaches have great potential to translate rich data into meaningful knowledge. The new statistical methods and theories will also advance modern statistical science. The resulting products of this project will provide valuable software tools to scientific communities. This research will promote curriculum development, student training, and educational outreach.<br/><br/>Multimodal data from different experimental platforms have different properties and characteristics. In many systems, including biological processes, regulation is modularized and temporally dynamic in nature. Common regulatory principles exist in certain related biological conditions. This project will focus on the development of new data integration methods and theories through novel correlated graphical models from a frequentist inference perspective. The methods will be based on exponential Markov random fields beyond the traditional Gaussian assumption. The new methods will allow for network discovery for high-dimensional heterogeneous data from multiple static and dynamic conditions. This project also involves the development of efficient algorithms for complex optimization problems incorporating the structure-inducing regularization mechanism.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015561","Variational Inference for Complex Networks","DMS","STATISTICS","07/01/2020","06/14/2020","Yuguo Chen","IL","University of Illinois at Urbana-Champaign","Standard Grant","Yulia Gel","06/30/2024","$150,000.00","","yuguo@uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","","$0.00","Large-scale complex networks are becoming increasingly common in a variety of scientific disciplines, including social sciences, biological sciences, and physical sciences. Such complex networks challenge the computational limit of classical methods, making it infeasible to carry out statistical network inference within a reasonable amount of time. This project will develop efficient algorithms that are computationally feasible for large-scale complex networks and have provable statistical guarantees on performance. The proposed methods will be applied to social and biological network data, including brain networks, and will be used for the study of disorders associated with hearing loss, such as tinnitus. The proposed research is highly interdisciplinary and provides an opportunity for involvement of graduate and undergraduate students with a broad range of backgrounds and interests. The proposed methods will be incorporated into relevant courses. Research results will be disseminated to the scientific communities and all software developed in this research will be freely distributed as open-source to the public.<br/><br/>The project will develop variational methods for complex networks, including dynamic, multi-layer, and heterogeneous networks, and investigate theoretical properties of the variational methods on these networks to provide provable statistical guarantees on performance. The network models the PI studies include latent space models for dynamic networks and dynamic multi-layer networks, stochastic block models for multi-layer networks, various models for heterogeneous networks, and other models for complex networks. The proposed variational inference procedure makes it possible to handle large scale complex network data. The theoretical properties the PI will investigate include consistency of parameter estimation and community detection for variational methods. The proposed methods will be applied to real network data from social and natural sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015540","Accurate and Interpretable Machine Learning for Prediction and Precision Medicine","DMS","STATISTICS","09/01/2020","06/23/2020","David Benkeser","GA","Emory University","Standard Grant","Yong Zeng","08/31/2024","$219,995.00","","benkeser@emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1269","075Z, 079Z","$0.00","Many machine learning technologies are built using black-box approaches, which can make it difficult to scrutinize the technology's decision-making process. This lack of interpretability represents a fundamental barrier to the adoption of machine learning technologies in some areas, such as health care, where transparency is key. Researchers have long relied on decision trees as a means of interpretable machine learning. In this approach, one develops a series of yes/no questions that eventually lead to a particular action being taken. While appealing in their simplicity, researchers have generally accepted that this approach will perform more poorly than more complex (but less interpretable) approaches. This project will establish that this need not be the case. A new approach to creating decision trees will be developed, which has a strong theoretical basis and performance competitive with less interpretable algorithms that are considered state-of-the-art. Software to implement the new approach will be developed and made freely available. The developed methods will be applied to ongoing studies of preventive vaccines and are poised to have broad impacts by providing personalized recommendations for vaccination. The project will also support graduate students and develop pedagogical material pertaining to ethical issues arising in machine learning in public health and clinical care. <br/><br/>The typical process for building a decision tree involves recursively partitioning the feature space using a greedy search. While this approach conveniently yields a decision tree, it is generally accepted to have worse performance when compared to other tree-based strategies, such as random forests or boosted trees. The project seeks to develop an alternative approach, where the feature space is partitioned using a method based on penalization called the highly adaptive lasso. The resultant prediction function enjoys desirable theoretical properties, but is not immediately representable as a decision tree, as it relies on non-recursive partitioning of the feature space. The first aim of the project will develop strategies for optimally representing a given partitioning via a recursive partitioning, thereby allowing a decision tree representation. A novel application of deep learning will be used to learn an optimal strategy for this representation, leveraging this archetypal uninterpretable algorithm to generate interpretable machine learning. In the second aim of the project, the approach will be extended to the context of personalized medicine and optimal decision trees for assigning treatments will be developed. The developments will have broad impacts on the theory of causal inference and robust machine learning. In the final aim, the developed methods will be applied to several contemporary trials of preventive vaccines to develop personalized vaccination recommendations. In particular, the methods will be applied to help determine optimal dosing strategies for a preventive malaria vaccine in children, which could have broad impacts on informing future vaccination strategies in Sub-Saharan Africa.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2013486","Structural Learning and Statistical Inference for Large-Scale Data","DMS","STATISTICS","08/15/2020","07/28/2020","Chunming Zhang","WI","University of Wisconsin-Madison","Standard Grant","Yong Zeng","07/31/2023","$119,998.00","","cmzhang@stat.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","079Z","$0.00","This project aims at developing new structure learning and statistical inference procedures for capturing some essential structures of large-scale data, emerging from scientific studies in genetics, biology, neuroscience, finance, and meteorology, among others. New tools for stochastic modeling, computational algorithms, and statistical inference applied to multi-channel brain EEG recordings, multi-subject fMRI and multiple neuron spike trains in neuroscience research, and identifying structural changes in climate data and copy number variation in genetics, will be developed. Outcomes of this study will help scientists to efficiently analyze large-scale imaging, temporal and spatial data, and thus will have broader impacts on our society through their direct impacts on these applications to science, public health, and information technology. Dissemination of these developments will enhance new knowledge discoveries, and strengthen interdisciplinary collaborations. The research will also be integrated with educational practice through designing either regular, seminar or short courses on new statistical approaches for analyzing complex data as well as benefitting the training and learning of undergraduate, graduate students and underrepresented minorities.<br/><br/>This research work focuses on statistical learning of fundamentally distinct types of structures, with the ultimate goal of better understanding of complex systems. Motivated from inferring neural connectivity from the ensemble neural spike train data, Project 1 will learn the directed acyclic graph structure in a large Poisson network, underlying a wide array of multivariate point process data. The related probabilistic mechanism will provide new insights into understanding statistical properties of the estimators for graph parameters relevant to mining the causal relation among neurons. Inspired by feature extraction and source separation from multi-channel brain EEG recordings and non-linear temporal signal processing, Project 2 will develop a class of non-linear non-smooth combinations of structured component analysis (SCA) to extract hidden component signals from observed mixed signals. The SCA developed will be more broadly applicable in scientific studies. Motivated from identifying and understanding structural changes in climate trends, and structural variation in gene copy numbers associated with genetic diseases, Project 3 will develop a novel two-step adaptive procedure of jump detection, for simultaneously selecting the unknown number of jump points and detecting their locations in the flexible non-parametric regression model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015374","Collaborative Research: Shape-Based Imputation and Estimation of Fragmented, Noisy Curves with Application to the Reconstruction of Fossil Bovid Teeth","DMS","STATISTICS","09/01/2020","06/16/2020","Gregory Matthews","IL","Loyola University of Chicago","Standard Grant","Yong Zeng","08/31/2024","$100,000.00","Karthik Bharath","gmatthews1@luc.edu","820 N MICHIGAN AVE","CHICAGO","IL","606112147","7735082471","MPS","1269","","$0.00","Statistical analysis of shape data is relevant to a wide array of fields including biology, anthropology, chemistry, and medicine, to name just a few. Current methods for analyzing shape data generally focus on shapes that are fully observed. This project will develop methods for analyzing shapes that are only partially observed. Traditional missing data techniques are not applicable in the shape setting when 1) shapes are defined by functions and 2) shape-preserving transformations (e.g. translation, rotation, scaling, re-parameterization, etc.) must be accounted for. This project will formalize and harness this perspective for data obtained from fragmentary fossil tooth images of the Family Bovidae (antelopes and buffaloes) as partially observed curves, representing the outlines of imaged objects, possibly with measurement error. The resulting taxonomic identifications will generate more robust estimates of the ecologically sensitive bovids. These improved estimates, in turn, afford novel insight into the paleoenvironmental context of early human evolution.<br/> <br/>The main goal of this project is to develop statistical methods for the analysis of partially observed shapes (i.e., fragmented curves). This goal will be accomplished via two different approaches: 1) developing computational methods for imputing a fragmented curve by matching and completing it based on a template or donor curve obtained from a sample of fully observed curves, and 2) developing Bayesian model-based methods for estimation and classification of the shape of a noisy, fragmented curve using an empirical prior on the overall shape of the curve. Both approaches will be built using Riemannian geometric tools for shape analysis that ensure proper invariance to shape preserving transformations including translation, scaling (when appropriate), rotation, and re-parameterization. These methods will be developed with the motivating application of the analysis and taxonomic classification of fractured and complete fossil teeth from the Family Bovidae. By generating a proxy for paleoenvironmental conditions, the project seeks to advance our understanding of the relationship between environmental change and hominin evolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015236","Collaborative Research: Shape-Based Imputation and Estimation of Fragmented, Noisy Curves with Application to the Reconstruction of Fossil Bovid Teeth","DMS","STATISTICS","09/01/2020","06/16/2020","Juliet Brophy","LA","Louisiana State University","Standard Grant","Yong Zeng","08/31/2024","$28,512.00","","jbrophy@lsu.edu","202 HIMES HALL","BATON ROUGE","LA","708030001","2255782760","MPS","1269","","$0.00","Statistical analysis of shape data is relevant to a wide array of fields including biology, anthropology, chemistry, and medicine, to name just a few. Current methods for analyzing shape data generally focus on shapes that are fully observed. This project will develop methods for analyzing shapes that are only partially observed. Traditional missing data techniques are not applicable in the shape setting when 1) shapes are defined by functions and 2) shape-preserving transformations (e.g. translation, rotation, scaling, re-parameterization, etc.) must be accounted for. This project will formalize and harness this perspective for data obtained from fragmentary fossil tooth images of the Family Bovidae (antelopes and buffaloes) as partially observed curves, representing the outlines of imaged objects, possibly with measurement error. The resulting taxonomic identifications will generate more robust estimates of the ecologically sensitive bovids. These improved estimates, in turn, afford novel insight into the paleoenvironmental context of early human evolution.<br/> <br/>The main goal of this project is to develop statistical methods for the analysis of partially observed shapes (i.e., fragmented curves). This goal will be accomplished via two different approaches: 1) developing computational methods for imputing a fragmented curve by matching and completing it based on a template or donor curve obtained from a sample of fully observed curves, and 2) developing Bayesian model-based methods for estimation and classification of the shape of a noisy, fragmented curve using an empirical prior on the overall shape of the curve. Both approaches will be built using Riemannian geometric tools for shape analysis that ensure proper invariance to shape preserving transformations including translation, scaling (when appropriate), rotation, and re-parameterization. These methods will be developed with the motivating application of the analysis and taxonomic classification of fractured and complete fossil teeth from the Family Bovidae. By generating a proxy for paleoenvironmental conditions, the project seeks to advance our understanding of the relationship between environmental change and hominin evolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015226","Collaborative Research: Shape-Based Imputation and Estimation of Fragmented, Noisy Curves with Application to the Reconstruction of Fossil Bovid Teeth","DMS","STATISTICS","09/01/2020","06/16/2020","Sebastian Kurtek","OH","Ohio State University","Standard Grant","Yong Zeng","08/31/2024","$100,000.00","","kurtek.1@osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269","","$0.00","Statistical analysis of shape data is relevant to a wide array of fields including biology, anthropology, chemistry, and medicine, to name just a few. Current methods for analyzing shape data generally focus on shapes that are fully observed. This project will develop methods for analyzing shapes that are only partially observed. Traditional missing data techniques are not applicable in the shape setting when 1) shapes are defined by functions and 2) shape-preserving transformations (e.g. translation, rotation, scaling, re-parameterization, etc.) must be accounted for. This project will formalize and harness this perspective for data obtained from fragmentary fossil tooth images of the Family Bovidae (antelopes and buffaloes) as partially observed curves, representing the outlines of imaged objects, possibly with measurement error. The resulting taxonomic identifications will generate more robust estimates of the ecologically sensitive bovids. These improved estimates, in turn, afford novel insight into the paleoenvironmental context of early human evolution.<br/> <br/>The main goal of this project is to develop statistical methods for the analysis of partially observed shapes (i.e., fragmented curves). This goal will be accomplished via two different approaches: 1) developing computational methods for imputing a fragmented curve by matching and completing it based on a template or donor curve obtained from a sample of fully observed curves, and 2) developing Bayesian model-based methods for estimation and classification of the shape of a noisy, fragmented curve using an empirical prior on the overall shape of the curve. Both approaches will be built using Riemannian geometric tools for shape analysis that ensure proper invariance to shape preserving transformations including translation, scaling (when appropriate), rotation, and re-parameterization. These methods will be developed with the motivating application of the analysis and taxonomic classification of fractured and complete fossil teeth from the Family Bovidae. By generating a proxy for paleoenvironmental conditions, the project seeks to advance our understanding of the relationship between environmental change and hominin evolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015552","Robust Bayesian Semiparametric Inference of Heterogeneous Causal Effects in Observational Studies","DMS","STATISTICS","07/01/2020","06/09/2020","Xinyi Xu","OH","Ohio State University","Standard Grant","Yong Zeng","06/30/2024","$250,000.00","Bo Lu, Steven MacEachern","xinyi@stat.osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269","","$0.00","A scientific mission of critical importance is to transform massive data into actionable knowledge, which largely centers on understanding causal relationships.  Causal inference has become one of three main tasks in data science, in addition to descriptive and predictive analyses.  This research project aims to close existing gaps in estimation of heterogeneous causal effects and will make more statistical tools available for analyzing massive observational data.  It will blend the conventional statistical approaches to causal inference with the fast-growing machine learning techniques and provide researchers and policy makers with powerful methodological tools to better evaluate the impact of interventions and thus to optimize decision making.  Doctoral students in Statistics and Biostatistics will be involved in the development and implementation of the methods.<br/><br/>This project concerns the development of a stream of innovative Bayesian semiparametric methods for efficient and robust causal inference in the presence of effect heterogeneity in large observational datasets.  Conventional statistical approaches have a strong tie to randomized experiments, which enjoy easy causal interpretation but may suffer in terms of efficiency.  Moreover, recently developed nonparametric regression and machine learning methods focus primarily on outcome modelling and prediction, which may encounter troubles from confounding and are often more difficult to interpret.  Furthermore, hidden bias from unmeasured confounding is a major concern in observational studies.  The status quo sensitivity analysis for assessing hidden bias does not accommodate complex data structures.  The PIs will develop a robust Bayesian semiparametric framework for incorporating the treatment assignment process into the outcome modelling.  The framework can easily accommodate complex heterogenous effects or hierarchical structures in massive observational data, adequately take advantage of experts? knowledge and existing causal theory on how the intervention might work, and effectively assess the impact due to potential unmeasured confounders.  Propensity scores will be incorporated in potential outcome models via Gaussian process priors and connections with the conventional matching estimators will be established.  Moreover, the impact of unmeasured confounding will be assessed through Bayesian sensitivity analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1945396","CAREER: Statistical Learning, Inference and Approximation with Reproducing Kernels","DMS","STATISTICS","06/01/2020","06/08/2023","Bharath Sriperumbudur","PA","Pennsylvania State Univ University Park","Continuing Grant","Yong Zeng","05/31/2025","$317,178.00","","bks18@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","1045","$0.00","Many modern scientific fields, such as astrophysics, bio-informatics, finance, forensics, social science, and others generate massive amounts of data that are both high-dimensional and non-standard. For example, the data may have structure such as graphs, functions, strings, and sets, but is not Euclidean. To analyze these data sets and address various statistical applications arising in these fields, efficient learning and inference procedures that can handle high-dimensional and non-standard data are needed. The functional analytic paradigm involving reproducing kernels, also known as the kernel method, provides a unified framework to handle such data and has been applied to a variety of non-parametric statistical problems with great empirical success by the machine learning community. However, its theoretical understanding in terms of statistical optimality has been limited, and computationally it scales poorly to large data. The key focus of this project is to explore various foundational research questions associated with the kernel method to achieve a statistically optimal and computationally efficient paradigm that can handle high-dimensional non-standard data. This research will significantly impact scientific development in all areas of science and engineering that intersect with statistics, and will be integrated with the PI's  educational activities of mentoring students, developing new courses and forging new collaborations. Methods and code developed under this project will be made publicly available for ready use. <br/><br/>The core idea behind the kernel method is to map the observed data (could be high-dimensional and non-standard) to exotic function space, called the reproducing kernel Hilbert space (RKHS) and apply the standard methods developed for Euclidean data on the mapped data. Ironically, the RKHS is usually higher dimensional (even infinite-dimensional) than the dimensionality of the observed data, and is characterized by a kernel function called the reproducing kernel. The main advantage of the kernel method is its ability to explore nonlinear relationships in data by simply exploring linear relationships between the mapped elements in the RKHS through the kernel function. Despite its superior empirical performance, the statistical theory of learning algorithms based on the kernel method is not well understood except in a few cases such as classification, non-parametric least square regression, principal component analysis and goodness-of-fit testing. In tise project, the PI will explore various foundational research questions associated with the kernel method and associated learning algorithms to address this gap. The project consists of four related research themes that overall seek to deepen the mathematical understanding of the kernel method so as to exploit its full power in constructing inference procedures that can efficiently handle non-standard data. The project will also shed light on the advantages and limitations of the kernel method over other non-parametric methods in the literature. The aims are to (i) Develop statistical optimality results for kernel-based hypothesis tests and non-linear canonical correlation analysis, (ii) Develop computational vs. statistical trade-off analysis for various kernel learning procedures using approximation schemes such as Nystrom method, random features and their variations, that speed up these procedures, (iii) Develop new methodologies with concrete mathematical guarantees using the kernel method for learning and inference on functions and probability distributions, with applications in functional data analysis, and (iv) Generalize the kernel method using multi-scale kernels to obtain wavelet-like representations and investigate its statistical and computational behaviors in various learning procedures. Overall, the project will develop a comprehensive mathematical theory for computationally efficient kernel-based learning algorithms with applications in statistical learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015378","Statistical Properties of Privacy-Preserving Algorithms: Optimality, Adaptivity, and Stability","DMS","STATISTICS","07/01/2020","06/27/2022","Linjun Zhang","NJ","Rutgers University New Brunswick","Continuing Grant","Yong Zeng","06/30/2023","$100,000.00","","linjun.zhang@rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","079Z","$0.00","The increasing popularity of large-scale data analysis raises privacy concerns. The tremendous amount of data collected by data curators such as search engines, social network platforms, and medical institutions contain potentially sensitive information about individuals. With the rapid emergence of data-driven technologies, it has been increasingly important to respect the privacy of individuals. A central question is: how to build privacy-preserving algorithms to protect individual privacy without sacrificing the utility in a large degree? This project aims to develop rigorous tools and methodologies to analyze privacy-preserving algorithms.<br/> <br/>The research objective of this project is to develop statistical theories and applications of privacy-preserving algorithms. In particular, the technical goals include (1) the statistical optimality of privacy-preserving algorithms in parametric models; (2) the statistical optimality and adaptivity of privacy-preserving algorithms in nonparametric regression, with focus on random forests algorithms, and; (3) the stability of privacy-preserving algorithms with applications to post-selection inference and adversarial robustness of deep neural networks. The new theoretical understandings will not only shed light on current privacy-preserving methodologies but also lead to new methodological developments of stable and adversarially robust algorithms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1952406","FRG: Collaborative Research: Generative Learning on Unstructured Data with Applications to Natural Language Processing and Hyperlink Prediction","DMS","STATISTICS","07/01/2020","04/02/2020","Annie Qu","CA","University of California-Irvine","Standard Grant","Yulia Gel","06/30/2024","$250,000.00","","aqu2@uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1269","075Z, 079Z, 1616","$0.00","This project addresses the pressing needs of analyzing ?big? unstructured data and tackles some artificial intelligence questions from the statistical perspective, which requires the focused and synergistic e?orts of a collaborative team. Specifically, the project develops generative models for statistical learning and leverages dependence relations modeled by graphical models in hyperlink prediction, which are applicable to topic sentence generation and protein structure identification. It will lead to a substantial improvement in the accuracy of generative learning based on numerical embeddings, particularly in topic sentence generation and hyperlink prediction. The integrated program of research and education will have significant impacts on machine learning and data science, social and political sciences, and biomedical and genomic research, among others. The project requires extensive algorithm and software development for natural language processing and multimedia data integration. The PIs, their postdocs, and students will develop innovative computational algorithms and software for the analysis of large-scale unstructured complex data. The advanced computational tools will be disseminated to facilitate technology transfer. <br/><br/><br/>The project will address some fundamental issues in two important areas of unstructured data analysis in machine learning and intelligence. In particular, the proposed research will develop a statistical framework for generative learning, which is primarily motivated by applications for unstructured data, namely topic sentence generation and high-order hyperlink prediction. The research will develop powerful generative methods for generating instances or examples to describe and interpret the corresponding learning model.  Moreover, it will develop network models for modeling high-order interactions and relations of units by identifying hidden structures in networks. It will proceed in two areas: (1) instance generation and topic sentence generation; (2) hyperlink prediction for multiway relations in hypergraphs. In the first area, instance generation, particularly sentence generation, will be performed collaboratively with numerical embeddings in categorization and regression. In the second area, hyperlinks will be predicted based on observed pairwise as well as unobserved high-order relations, characterized by graphical models with hidden structures. Special effort will be devoted to inverse learning, the integration of data from multiple sources, and extracting latent structures of networks.  Finally, the research will develop computational tools and design practical methods that have desirable statistical properties.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015379","Collaborative Research: Extremes in High Dimensions: Causality, Sparsity, Classification, Clustering, Learning","DMS","STATISTICS","07/01/2020","06/18/2020","Richard Davis","NY","Columbia University","Standard Grant","Yong Zeng","06/30/2023","$299,999.00","Marco Avella Medina","rdavis@stat.columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","","$0.00","In recent years, through news reports and first-hand experience, the general public has become keenly aware of extreme events, in particular, of extreme weather conditions such as extended heat waves, periods of extreme cold, an increase in the number and intensity of tornadoes and hurricanes, or periods of record precipitation resulting in unprecedented floods.  Just in the past few years, the insurance claims from extreme climatic events have been staggering, which include the Missouri River flood in April 2019 ($10.8B), Hurricane Michael in October 2018  ($25B), the California wildfires in December 2017 ($18.7B), the US drought/heatwave in 2012 ($33.9B), and Hurricane Sandy in October 2012 ($73.4B).  This list does not include non-climatic extreme events such as the financial crisis from 2008 nor the current covid-19 pandemic. Many of the extreme events experienced today that are weather, environmental, industrial, epidemiological, economic, or social media related are occurring at a more frequent rate, which often result in huge losses to our society in a variety of ways from financial to human life to our way of life. While the occurrence of extreme events is reasonably well understood in steady state situations, it has become clear that the preponderance of extremes events suggest that the steady-state assumption is no longer valid.  The key objective of this research is to try to understand causal impacts of various factors from a potentially large array of variables including changing environmental conditions, demographic movements within the US, changing landscapes, and changing economic conditions, on the frequency and magnitude of extreme events.  From many variables, we hope to produce methodology to extract the important features in the data that have a direct impact on describing and predicting extremes.  This research is potentially of use to policymakers who need to anticipate and plan for extreme events leading to sensible strategies for mitigating their impact on society. The graduate student support will be used for interdisciplinary research.<br/><br/>The principal goal of this research project is to design new tools for analyzing and modeling extremes in a myriad of situations that go well beyond the boundaries of classical extreme value theory. These include detection of often nonlinear sets of much smaller dimension that can provide an adequate description of extremes in high dimensions, for which we hope to apply the powerful modern learning techniques (such as graph-based learning methods) that allow us to determine this extremal support from the data. In general, detecting sparsity in the exponent measure describing high-dimensional extremes, i.e., locating (often numerous) low-dimensional regions which carry most of the support of exponent measure will be a key focus of this research. A second main thrust of this research centers on the issue of causality in both small and large dimensional problems. In the most basic form, a set of variables X is said to be tail causal to a dependent vector Y if certain changes in X (sometimes themselves extreme but not always so) impact the tail behavior of Y. An important setting of this type is the potential outcomes framework for causality of extreme events, which will be a major focus in this project's research agenda.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2014626","Models for Complex Functional and Object Data","DMS","STATISTICS","07/01/2020","06/09/2020","Hans-Georg Mueller","CA","University of California-Davis","Standard Grant","Yong Zeng","06/30/2024","$300,000.00","","hgmueller@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","079Z","$0.00","Big data are increasingly encountered across society and all of the sciences and pose novel challenges for statistical analysis, due to their complexity and size. Challenging data analysis tasks that motivate this research originate in brain imaging, genomics, the social sciences and many other areas of current interest. To make sense of such data and extract relevant information requires principled statistical methodology that is suitable for the analysis of large samples of complex data. Examples include networks or age-at-death distributions for which common algebraic operations such as sums or differences are not defined. In many instances such data objects may also be repeatedly observed over time, and the quantification of their time dynamics is then of great interest. For example, one might be interested to determine whether sudden changes occur and where these are located in time.  Statistical methodology will be developed that addresses these data analytic needs, along with theory and efficient computational implementations. This new methodology is expected to lead to substantial new insights. For example, it will be possible to quantify phenomena such as changes in temperature, mortality or income distributions over calendar years, or changes in brain connectivity networks as a function of age, which will aid in distinguishing normal and pathological brain aging. The new methodology will also make it possible to detect differences between groups of complex data, for example between the mortality distribution of countries, including the identification of clusters. The project also provides research training opportunities for undergraduate and graduate students. <br/><br/>The focus of this research is the development of statistical methods and theory for random objects, i.e., metric space valued random variables, including object-valued functional and longitudinal data. Due to the lack of Euclidean structure, existing methods from high-dimensional and functional data analysis are generally not applicable for metric-space valued random objects. This motivates the development of novel approaches that address the challenge of a lack of Euclidean structure.  Major lines of inquiry will be regression and change-point models for random objects on one hand and methods for trajectories of random objects including complex functional data on the other. New regression and change-point models to be studied include distributions as predictors; regression models for point processes; inference and single index modeling for Frechet regression; and change-point analysis for sequences of object data under various scenarios. For object-valued functional data, an emphasis will be the development of time warping models for random objects and of models for longitudinal random objects in various spaces, including the case where the data are only sparsely and irregularly observed in time. Tools and theory for principled statistical analysis of random objects to be developed will rely on empirical process theory for M estimators in metric spaces, U statistics and related approaches. These developments will lead to the creation of a toolbox suitable for data analysis of object data and associated freely available software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2016175","Diversity Workshop and Mentoring Program","DMS","STATISTICS","05/01/2020","04/29/2020","Donna LaLonde","VA","American Statistical Association","Standard Grant","Yulia Gel","04/30/2023","$49,900.00","Brian Millen, Jesse Chittams, Dionne Swift","donnal@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","7556","$0.00","This project supports the 2020 through 2022 offerings of the Joint Statistical Meetings (JSM) Diversity Workshop and Mentoring Program / JSM Diversity Mentoring Program.  These annual programs serve statisticians from underrepresented groups at early- to mid-career levels. Participants are connected to mentors and a network through a four-day experience at the JSM. Additionally, engagement and connections are encouraged beyond the structured experience at the JSM. <br/><br/>Mentoring relationships are critically important for individuals at all career stages. These relationships result in increased awareness of opportunities, increased insight into organizational culture and politics, and increased access to opportunities or leaders, among other benefits. Establishing such relationships has the ability to impact other objectives of the program as well. Mentors can help motivate students to pursue graduate study or careers in statistics and encourage their proteges to become involved in the American Statistical Association or other professional organizations and can often make connections that increase the access of the workshop participants to these opportunities. The net societal result is a diverse workforce of well-trained, influential leaders in the field of statistics. The website for this project is https://community.amstat.org/cmis/events/dwmp/dmp2020<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015120","Novel Inference Procedures for Non-Standard High-Dimensional Regression Models","DMS","STATISTICS","07/01/2020","06/20/2020","Hui Zou","MN","University of Minnesota-Twin Cities","Standard Grant","Yong Zeng","06/30/2023","$149,999.00","","hzou@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","Statistical theory of hypothesis testing plays a fundamental role in virtually all scientific studies. In the era of big data, high-dimensional data are ubiquitous in many scientific fields such as natural sciences, social sciences, medicine, and public health. Therefore, modern applications often involve hypothesis testing under high dimensions, which calls for new statistical inference theory.  As regression is the most popular statistical analysis tool in applications, some recent work has been focused on hypothesis testing in high dimensional least squares regression. However, it is well-known that the standard least squares regression model has severe limitations in real applications. This research aims to develop new statistical inference theory for more flexible high dimensional regression models. <br/><br/>This research focuses on the development of inference theory for several important non-standard regression models under ultra-high dimensions. Specifically, the PI will develop tests for testing linear hypotheses under three models: high dimensional expectile regression, high dimensional heteroscedastic regression, and robust high dimensional regression. Asymptotic distributions of the test statistics will be established rigorously. The theoretical study will fill important gaps in the high-dimensional statistics literature.  A unified efficient algorithm will be developed to tackle the computational challenges. The research will provide principled tools for studying expectile functions, for examining the heterogeneity in high dimensional data and for performing robust inference. Research training opportunities for graduate students will be provided.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1952373","FRG: Collaborative Research: Quantile-Based Modeling for Large-Scale Heterogeneous Data","DMS","STATISTICS","06/01/2020","03/27/2020","Lan Wang","FL","University of Miami","Standard Grant","Yong Zeng","05/31/2024","$200,000.00","","lanwang@mbs.miami.edu","1320 S DIXIE HWY STE 650","CORAL GABLES","FL","331462919","3052843924","MPS","1269","079Z, 1616","$0.00","The rapid development of technology has led to the tremendous growth of large-scale heterogeneous data in science, economics, engineering, healthcare, and many other disciplines. For example, in a modern health information system, electronic health records routinely collect a large amount of information on many patients from heterogeneous populations across different disease categories. Such data provide unique opportunities to understand the association between features and outcomes across different subpopulations. Existing approaches have not fully addressed the formidable computational and statistical challenges. To tap into the true potential of information-rich data, this project will develop a new computational and statistical paradigm and solid theoretical foundation for analyzing large-scale heterogeneous data. In addition the project will also provide research training opportunities for graduate students.<br/> <br/>The project will build a unified, quantile-modeling based framework with an overarching goal of achieving effectiveness and reliability in analyzing heterogeneous data, especially when both the number of potential explanatory variables and the sample size are large. The specific goals are (1) to develop resampling-based inference for large-scale heterogeneous data; (2) to develop Bayesian algorithms and scalable and interpretable structure-aware approach for better inference; (3) to develop quantile-optimal decision rule estimation and inference with many covariates; (4) to develop novel estimation and inference procedure for large-scale quantile regression under censoring. The project will address some of the key barriers in scalability to data size and dimensionality, exploration of heterogeneity and structures, need for robustness, and the ability to make use of incomplete observations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015298","Collaborative Research: Inference for Networks: Bridging the Gap between Metric Spaces and Graphs","DMS","STATISTICS","07/01/2020","06/27/2022","Tianxi Li","VA","University of Virginia Main Campus","Continuing Grant","Yong Zeng","06/30/2023","$100,000.00","","tianxili@umn.edu","1001 EMMET ST N","CHARLOTTESVILLE","VA","229034833","4349244270","MPS","1269","","$0.00","Network data, representing interactions and relationships between units, has become ubiquitous in many science disciplines and technology areas. Analyzing such complex and structurally novel data requires new ideas and tools beyond the scope of classical statistics. A sequence of methods will be developed for several common statistical analyses involving network data, motivated by various applied problems in cyber-security, social behavior studies, genetics, and medical imaging. These methods can be used to identify the risk factors for the reliability of a complex system, to infer social and peer effects on health-related behaviors, to flexibly model the differential networks between genes, and to infer neuron functionality from brain images. The results will be disseminated through publications and presentations, but will also be incorporated in teaching. The research will include projects suitable for student participation at various levels, and undergraduate research training will be emphasized. The codes will be provided through statistical packages implemented in the programming language R for broader use. <br/><br/>The broad theme of the research is developing versatile and flexible network analysis tools by connecting and extending mature statistical methods in metric space to network data. Overall, the technical challenges in developing these tools range from the lack of clear definitions for sampling units and sample sizes, to the discrete and noisy nature of network observations. Addressing such challenges requires extensions and combinations of tools from different research areas, including random matrix theory, optimization algorithms, and statistical inference. Collaborations between the PI and researchers in computer science, social science, and medical sciences will provide opportunities to apply the developed methods to real-world problems in these domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2016307","Statistical Methods for Intra-Tumor Heterogeneity Studies Using Sequencing Data","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","07/01/2020","06/26/2020","Mengjie Chen","IL","University of Chicago","Standard Grant","Yong Zeng","06/30/2023","$146,298.00","","mengjiechen@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269, 7454","068Z","$0.00","Tumor heterogeneity refers to the observation that tumor cells can display distinct phenotypic and morphological characteristics, such as gene expression, metabolism, cellular morphology proliferation, motility, and metastatic potential. This phenomenon occurs both between tumors (inter-tumor heterogeneity) and within tumors (intra-tumor heterogeneity). Understanding tumor heterogeneity is a prerequisite for personalized tumor diagnosis and treatment. The objective of this project is to develop statistical methods to study the intra-tumor heterogeneity using next generation sequencing data. These methods will address important statistical, computational, and biological challenges that arise from recent cancer genomics studies. The applications will further our understanding of mechanisms underlying tumor heterogeneity as well as its clinical consequences. The research will provide opportunities to attract and nurture diverse future scientists to work at the frontiers of computational cancer genomics. The project will also provide research training opportunities for undergraduate and graduate students. User-friendly open-source software implementing the research methods will be developed, distributed, and supported to benefit the genomics and statistics community.<br/><br/>This project will develop clonality analysis methods with a firm statistical footing and tailored for the characteristics of data from different sequencing technologies. A new stochastic process will be developed to model clonal expansion. Using likelihood-based approaches, the PI will construct clonal history empowered by several novel strategies: 1) incorporating phase information bridges cancer genomics studies with rich germline variant resources profiled by consortium such as the 1000 Genome Project; (2) integrating single cell RNA sequencing and bulk tissue DNA sequencing data; (3) adjusting for characteristics of local sequences. These analyses can also potentially open up new opportunities for joint analysis of tumor heterogeneity and a rich list of epigenomic features such as DNA accessibility and methylation. Successful achievement of all aims will dramatically increase the power of subclone identification in cancer genomics studies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015134","Collaborative Research: Inference for Networks: Bridging the Gap between Metric Spaces and Graphs","DMS","STATISTICS","07/01/2020","08/16/2022","Can Le","CA","University of California-Davis","Continuing Grant","Yong Zeng","06/30/2023","$125,000.00","","canle@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","Network data, representing interactions and relationships between units, has become ubiquitous in many science disciplines and technology areas. Analyzing such complex and structurally novel data requires new ideas and tools beyond the scope of classical statistics. A sequence of methods will be developed for several common statistical analyses involving network data, motivated by various applied problems in cyber-security, social behavior studies, genetics, and medical imaging. These methods can be used to identify the risk factors for the reliability of a complex system, to infer social and peer effects on health-related behaviors, to flexibly model the differential networks between genes, and to infer neuron functionality from brain images. The results will be disseminated through publications and presentations, but will also be incorporated in teaching. The research will include projects suitable for student participation at various levels, and undergraduate research training will be emphasized. The codes will be provided through statistical packages implemented in the programming language R for broader use.<br/><br/>The broad theme of the research is developing versatile and flexible network analysis tools by connecting and extending mature statistical methods in metric space to network data. Overall, the technical challenges in developing these tools range from the lack of clear definitions for sampling units and sample sizes, to the discrete and noisy nature of network observations. Addressing such challenges requires extensions and combinations of tools from different research areas, including random matrix theory, optimization algorithms, and statistical inference. Collaborations between the PI and researchers in computer science, social science, and medical sciences will provide opportunities to apply the developed methods to real-world problems in these domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015273","Statistical Approaches for Spatio-Temporal Stochastic Population Models","DMS","STATISTICS","08/01/2020","08/03/2020","Ephraim Hanks","PA","Pennsylvania State Univ University Park","Standard Grant","Yong Zeng","07/31/2024","$210,000.00","","hanks@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","Mathematical models are used to describe and explore complex processes in many fields, including the study of disease dynamics in a population and the study of the wind-born spread of pollutants from power plants.  Continuing advances in remote sensing and data collection have made it possible to collect data on similar processes at resolutions that were impossible a generation ago.  Statistics and Data Science are fields focused on the analysis of data to inform decision making and scientific inquiry, but the most common methods used, such as linear regression or machine learning methods, cannot easily use mathematical models in their analysis approach.  In this work, the PI will develop methods that make it easier to analyze data from systems where mathematical models are useful to describe the process in question.  The methods developed include statistical approaches for modeling data in common forms, such as yearly averaged pollution concentrations over space, or the current number of individuals hospitalized with a disease.  These methods will improve our ability to understand and predict complex behavior in spatial epidemiology, disease modeling, and ecology.  Potential results include better estimates of epidemiological parameters such as the rate at which individuals contract a disease but do not show symptoms, which is critical for predicting the future of an epidemic. The project will provide research training opportunities for graduate students. <br/><br/>The use of mechanistic process models, like ODEs, SDEs, and PDEs, is central to the mathematical analysis of ecological and epidemiological processes.  However, their use in statistical inference is relatively limited.  In this work, the PI will develop statistical methods useful for analyzing data when the governing process is scientifically known to follow a mechanistic process model.  As part of this work, the PI will develop methods for joint inference of individual-level data (like individual animal movement data) with population-level data (like population-level counts of animal abundance) with formal links between these two data streams and a single process model.  In addition, the PI will develop methods for modeling data that come from an assumed stochastic process, like a diffusion model or a spatial disease spread model, but are collected as either a snapshot in time or an average over time (i.e., yearly average pollutant concentration).  Together these projects will provide increased ability to specify and fit mechanistic statistical models to data common in a wide variety of scientific disciplines. This work will advance the ability of scientists to model data obtained in a variety of common formats using mechanistic models with interpretable parameters.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015373","Repro Sampling Method: A Transformative Artificial-Sample-Based Inferential Framework with Applications to Discrete Parameter, High-Dimensional Data, and Rare Events Inferences","DMS","STATISTICS","07/01/2020","06/26/2020","Minge Xie","NJ","Rutgers University New Brunswick","Standard Grant","Yong Zeng","06/30/2023","$258,589.00","Zijian Guo","mxie@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","In the era of data science, statistical inference is the cornerstone of extracting useful information from complex data sets. Despite significant progress made in statistics, there remain many challenges in uncertainty quantification in confronting the complex and high-dimensional data. For instance, inherently discrete parameters and model structures are routinely encountered in data science and machine learning problems. For these intrinsically discrete structure problems, conventional statistical inference approaches do not apply. This project aims to develop a new inferential framework addressing the statistical inference questions for those difficult problems in high-dimensional and also rare events data analyses. The development of the framework will be transformative, since it will greatly expand the reach of statistical inference and uncertainty quantification and greatly improve our thinking and approach of making inference for many data science problems. The PIs will actively use the project to recruit and train students, especially underrepresented students, and also integrate the research output into teaching through developing topic courses to senior undergraduate students and graduate students at their home university. The obtained results will be disseminated in journal publications and conferences to enhance the understanding of the results in different communities. R packages for the proposed methods will also be released to the public.The graduate student support will be used on interdisciplinary research and writing codes.<br/> <br/>Inherently discrete parameters and structures are prevalent in data science, for example, model indices in model selection problems, number of clusters and membership in classifications, number of layers and structure in deep neural network models, connectivity, membership and structure questions in network data, etc. Making inference for discrete parameters and structures is known to be a difficult task. A major challenge is that the large sample central limit theorem (CLT) no longer holds, and a Bayesian analysis is very sensitive and heavily impacted by the prior choice on the discrete model structure. This research project is aimed to develop a novel and general artificial-sample-based inferential framework, termed as, repro sampling. The idea of repro sampling is to create and study the performance of artificial samples that are generated by mimicking the sampling mechanism of the observed data; the artificial samples are then used to help quantify the uncertainty in estimation of model and parameters. The repro-sampling will guarantee the coverage property in finite sample and also can be extended to large sample. The proposed approaches are expected to be broadly applicable, efficient and computationally feasible. The main research goal is to fully develop the novel inferential framework of repro sampling. Three specific topics tailored to important and difficult inferential problems in data science will also be investigated: (A) Model selection and inference in high dimensional regression, nonparametric and deep learning models; (B) Predictive inference for high dimensional regression and data science; (C) Finite sample inference and fusion learning for rare events data. The research work will significantly advance the statistical methodology for the important yet challenging inference problems for discrete parameters, and broaden the applicability of uncertainty quantification to advanced machine learning methods.  In addition, the research projects involve real databases and are ideally suited for engaging and training students and new researchers.<br/><br/><br/><br/>________________________________________<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2013736","Flexible Statistical Modeling","DMS","STATISTICS","07/01/2020","06/16/2020","Trevor Hastie","CA","Stanford University","Standard Grant","Yong Zeng","06/30/2024","$250,000.00","","hastie@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","079Z","$0.00","This project develops new statistical methodology for solving important applied problems in medicine and science. In personalized medicine, the PI will focus on the prediction of whether particular treatments are suitable for a patient based on their demographics and clinical history, using vast troves of past patient experiences. Biologists seek to understand the folding patterns of chromosomes in cells, a key ingredient in understanding their function.  In a second project, the PI will develop novel curve-fitting methods to learn these folding patterns from indirect and noisy measurements of this three dimensional structure. Ecologists try to learn the characteristics of environments that attract certain species, as well as shared aspects of species that coinhabit environments, as a critical component in species survival, pest control and disease prevention.  In a third project, the PI will develop methods that can scale to extremely large species populations (such as bacteria and insects) based on site-specific surveys.  The project also provides research training opportunities for graduate students.<br/> <br/>The project develops validation methods to select from a collection of models for estimating heterogenous treatment effects, despite the fact that in observational data there are no direct measurements of the treatment effect. The project develops adaptive nearest-neighbor matching techniques to construct a comparison set for each validation point.  With high-dimensional chromosomal contact maps, the PI plans to draw on his early work on principal curves to model the three-dimensional folding structure of chromosomes. This amounts to metric scaling with side information on the local structure of the three-dimensional solution. Generalized linear latent-variable models are popular for modeling species distributions (usually Poisson models for counts, and binomial models for presence/absence), but they grind<br/>to a halt when the number of species and/or locations is very large. The PI plans to adapt earlier work on matrix completion to develop alternating maximum-likelihood fitting algorithms to scale these methods to extremely large populations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015259","Collaborative Research: Transfer Learning for Large-Scale Inference: General Framework and Data-Driven Algorithms","DMS","STATISTICS","07/01/2020","06/18/2020","T. Tony Cai","PA","University of Pennsylvania","Standard Grant","Yong Zeng","06/30/2023","$249,999.00","","tcai@wharton.upenn.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","1269","079Z","$0.00","Transfer learning provides crucial techniques for utilizing data from related studies that are conducted under different contexts or on diverse populations. It is an important topic with a wide range of applications in integrative genomics, neuroimaging, computer vision and signal processing. This research work will provide new tools to scientific researchers who routinely collect and analyze high dimensional and complex data across different sources and platforms. This project aims to develop new analytical tools to improve conventional methods by delivering more informative and interpretable scientific findings. The developed transfer learning algorithms, which can reliably extract and combine knowledge from diverse data types and across different studies, will help address important issues from genomics applications. User-friendly software packages will be developed and made publicly available. Scientific researchers can use the tools to translate dispersed and heterogeneous data sources into new knowledge and medical benefits. This will help improve the understanding of the role of various genetic factors in complex diseases, and accelerate the development of new medicines and treatments in a cost-effective way. <br/><br/>Transfer learning for large-scale inference aims to extract and transfer the knowledge learned from related source domains to assist the simultaneous inference of thousands or even millions of parameters in the target domain. We aim to develop a general framework to gain understanding of the benefits and caveats of transfer learning in a wide range of large-scale inference problems including sparse estimation, false discovery rate analysis, sparse linear discriminant analysis and high-dimensional regression. Our research addresses two key issues in transfer learning: (a) What should be transferred? (b) How to transfer and prevent negative learning? We aim to pursue three major research goals. The first is to develop a class of computationally efficient and robust transfer learning algorithms for high-dimensional sparse inference. The general strategy is to first learning the local sparsity structure of the high-dimensional object through auxiliary data and then apply the structural knowledge to the target domain by adaptively placing differential weights or setting varied thresholds on corresponding coordinates. The second is to formalize a decision-theoretic framework for high-dimensional transfer learning that is applicable across the sparse and non-sparse regimes. Along this direction, we aim to develop a class of kernelized nonparametric empirical Bayes methods for data-sharing shrinkage estimation and multiple testing. The third is to address the urgent needs and new challenges arising from important genomics applications using the newly developed methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015400","Black-Box Science:  Ideas and Insights for Learning-Based Statistical Inference","DMS","STATISTICS","07/01/2020","06/27/2022","Lucas Mentch","PA","University of Pittsburgh","Continuing Grant","Yong Zeng","06/30/2024","$160,000.00","","lkm31@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","079Z","$0.00","This project seeks to develop essential tools that will allow scientists to better harness the full power of machine learning in practical scientific settings. In the current era of big data, machine learning algorithms have set themselves apart as excellent, accurate tools for modeling complex systems and predicting future outcomes. Determining why those particular algorithms actually work and how those predictions are generated has proven to be a much greater challenge, yet understanding these aspects is crucial for practical scientific use. For example, if an algorithm predicts that you are at risk for a particular disease, you will instinctively care less about the exact percentage chance you have of getting it and much more about why you are more likely to get it and whether there is something you could do to prevent getting it. This work will develop tools that allow scientists to more easily determine which variables most affect an algorithm's performance and whether some other collection of variables might offer an alternative but equally accurate explanation for the outcomes predicted. Various components of these algorithms will also be explored mathematically to determine whether some of them can be borrowed and inserted into simpler models in order to obtain predictions that are not only more accurate, but are also more easily explainable.<br/><br/>This project seeks to develop efficient means of statistical inference within a machine learning context with an emphasis on random forests in particular. Specifically, a computationally efficient hypothesis test will be developed that allows for p-values for feature importance to be calculated with similar effort to the original algorithm. In addition to these tests, a framework for characterizing the uncertainty in the model selection process itself will be developed to provide insights into not just the optimal model obtained, but also to illustrate how many alternative models may exist with similar predictive power. Finally, an in-depth study on the fundamental role of randomness in supervised learning ensembles will be undertaken. Lessons learned about the helpful effects of such randomness will be utilized to boost performance of more traditional models in appropriate settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015323","Optimal Nonparametric Methods for Ito Processes Based on High-Frequency Data","DMS","STATISTICS","07/01/2020","06/23/2020","Jose Figueroa-Lopez","MO","Washington University","Standard Grant","Yong Zeng","12/31/2023","$149,999.00","","figueroa@math.wustl.edu","ONE BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","1269","9150","$0.00","The project will develop new statistical methods to analyze stochastic phenomena that inherently evolves in time. The methods will be based on high-frequency monitoring of the phenomena under study, which is of widening use in many fields such as finance, meteorology, biology, neuroscience, turbulence, statistical physics, seismology, and telecommunication. Thus, the research will foster more interaction between applied scientists and statisticians. The research and educational elements of the project will also serve as a training tool for both undergraduate and graduate students by forming synergistic research groups involving all levels. <br/><br/>Nonparametric methods are powerful statistical tools to reduce the model misspecification error, and high-frequency-based statistical analysis is a natural route to take when estimating the fine statistical features of continuous-time stochastic processes. Though the literature combining these two approaches has grown significantly during the last two decades, comparatively little work has been done to analyze and correct the sensitivity of methods to tuning parameters. The project will address these needs by (i) developing a unified approach for optimal kernel estimation of the spot volatility of an Ito process in the presence of leverage and microstructure noise; (ii) devising of optimal jump detection and integrated variance estimation methods, under the presence of stochastic volatility and infinite jump activity, via thresholding or shrinkage of the process' increments or wavelet coefficients; (iii) establishing theoretical guarantees for the data-driven plugging implementation methods resulting from the optimal schemes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015498","Scalable Algorithms for Bayesian On-Line Learning with Large-Scale Dynamic Data","DMS","STATISTICS","08/01/2020","06/20/2020","Faming Liang","IN","Purdue University","Standard Grant","Yong Zeng","07/31/2024","$250,000.00","","fmliang@purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1269","059Z, 079Z","$0.00","Bayesian methods provide a principled way for assessing model uncertainty in machine learning of big data, which is critical to the development of trustworthy artificial intelligence (AI). However, the lack of efficient Monte Carlo algorithms has drastically hindered applications of Bayesian methods in the big data era. Compared to frequentist methods, Bayesian methods are often much slower. To tackle this difficulty, a variety of scalable Monte Carlo algorithms have been developed in the recent literature. However, these algorithms can only be applied to static data; none of them can be directly applied to dynamic data.  Many of the problems centering data science, such as natural language processing, autonomous car driving and weather forecasting, are facing challenges of dynamic data. The traditional particle filters or sequential Monte Carlo algorithms lack the scalability necessary for dealing with large-scale dynamic data. By reformulating the ensemble Kalman filter (EnKF) under the framework of Langevin dynamics, this project proposes Langevinized EnKF as a general and scalable stochastic gradient sequential Monte Carlo algorithm for Bayesian on-line learning with large-scale dynamic data. The Langevinized EnKF improves uncertainty quantification for a wide class of data assimilation problems, advancing the development of trustworthy AI. Successful completion of this project will generate a set of scalable and theoretically rigorous algorithms for Bayesian on-line learning, which can provide significant benefits to the development of data driven technologies. The research results will be disseminated to communities of interest via collaborations, publications, and conference presentations. The project will also have significant impacts on education through direct involvement of graduate students and incorporation of the research results into undergraduate and graduate courses. <br/><br/>Although the EnKF has been extremely successful in dealing with complex dynamic data encountered in  oceanography, reservoir modeling and weather forecasting, it does not converge to the right filtering distribution except for linear systems in the large ensemble limit. The Langevinized EnKF resolves this issue; it converges to the right filtering distribution in data assimilation and is thus able to quantify uncertainty of the underlying dynamic system. The Langevinized EnKF can also be used for Bayesian learning with large-scale statistic data by reformulating the Bayesian inverse problem as a state-space model with Langevin dynamics and the subsampling technique. Different variants of the Langevinized EnKF will be developed to extend its applications to non-Gaussian data and incomplete data. As the whole, this project will provide a complete treatment for Bayesian analysis of big data. The Langevinized EnKF can be applied to big data problems in various data scenarios: dynamic data and static data, Gaussian data and non-Gaussian data, and complete data and incomplete data,  provided the data is classified in different ways. Statistical theory underlying the Langevinized EnKF will be rigorously studied. Exciting scientific applications, including language modeling and dynamic network analysis, will be conducted.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015390","Composite Resampling Inference for Dependent Data","DMS","STATISTICS","08/01/2020","08/03/2020","Daniel Nordman","IA","Iowa State University","Standard Grant","Yong Zeng","07/31/2023","$149,961.00","","dnordman@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","","$0.00","Current statistical methodology for dependent data analysis often relies on specifying an adequate model, which can be difficult in practice.  A potential consequence is that conclusions drawn from an inappropriate or mistaken model may be unreliable or misleading.  The project seeks to develop efficient and accurate statistical methods that are ""model-free"" or apply without restrictive assumptions about the dependence in data.  A direct benefit of this research will be to provide alternative tools for statistical inference that are not susceptible to model choice or model misspecification.  Therefore, the research will benefit data-based inference in scientific areas such as environmetrics, economics, geology, and astronomy, which encounter different forms of dependent data and where model-free methods can play an important role in data analysis.  The project will also support the professional development of students through graduate student mentoring as well as outreach activities with undergraduate students at local colleges and universities for promoting recruitment and education in statistics and data science.<br/><br/>This project particularly aims to produce composite, or hybrid-type, resampling methods that combine strategies for re-using dependent data.  By merging philosophically different resampling techniques (subsampling and bootstrap), the PI will investigate convolved subsampling for nonparametric inference. This new resampling method has wide applicability and favorable performance under mild conditions. For important types of strongly or long-range dependent time series, statistical inference depends heavily on an unknown process index. The PI will study resampling under long-memory and develop a first-ever estimator of this index via a composition of resampling ideas.  Additionally, the PI will develop new empirical likelihood methods for time series and spatial data by combining different resampling devices (data transformations and data blocking) for inference over a variety of dependence structures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015492","Theory and Methods for Large-Scale Multi-Modal Matrix Data","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","07/01/2020","06/14/2020","Jing Lei","PA","Carnegie-Mellon University","Standard Grant","Yong Zeng","06/30/2024","$200,000.00","","jinglei@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 7454","068Z","$0.00","Modern data acquisition technology produces new types of data that carry rich information but also poses new challenges for analysis. In many modern datasets, the basic unit of measurement can be a matrix or even higher order array recording the interactions among one or multiple groups of individuals.  For example, a gene co-expression network measures the average strength of correlation between each pair of genes in a particular organ tissue. With gene co-expression networks collected at different developmental stages, it is possible to understand how groups of genes change their behavior in a coherent way. As another example, next generation sequencing techniques are able to produce gene expression data at different scales: Tissue sample data consists of gene expressions in bulk tissue samples, whereas single cell RNA sequencing data contains expressions of the same genes for individual cells.  Motivated by the these examples, this research work aims at developing novel probability tools and statistical inference methods for complex matrix valued datasets, which will enable scientists to uncover salient structures in such datasets in a coherent and efficient way. The project also provides research training opportunities for graduate students. <br/><br/>This project consists of two parts. In the first part, the PI studies multiple layer networks with a shared latent structure across layers and develops methods to efficiently combine the information across different layers to recover the latent structure, which would be impossible if only a single layer were available. The expected results will provide new probability theorems describing the behavior of random noises in matrix forms, as well as their linear combinations and higher order functions. In the second part, the PI studies a series of inference problems related to tissue and single cell RNA-seq data, starting from dimensionality reduction and variable selection in a computationally efficient manner, followed by downstream inference problems such as cell type deconvolution in tissue RNA-seq data.  The expected results will provide an important addition to the sparse principal components analysis literature, by developing a projection-free, gradient-based algorithm with provable global convergence properties. The cell type deconvolution problem will be an interesting application combining techniques from variable selection, nonnegative matrix factorization, and optimization.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2014166","Global Non-Gaussian Stochastic Partial Differential Equation Models for Assessing Future Health of Ecohydrologic Systems","DMS","STATISTICS","07/01/2020","06/27/2022","Stefano Castruccio","IN","University of Notre Dame","Continuing Grant","Yong Zeng","06/30/2024","$150,010.00","Diogo Bolster","scastruc@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","1269","","$0.00","In recent decades, the dramatic increase of computational power, coupled with technological advances in portable and remote sensing devices has exponentially increased the volume, variety and velocity of data, facilitating new scientific and engineering breakthroughs. This project focuses on global spatio-temporal data, a data type highly affected by this Big Data revolution and aims to develop a new global dynamical model for processes monitored at high resolution in time (daily or hourly). The application will focus on the occurrence and intensity of rainfall, and the model will be applied to assess risks faced by diverse hydrologic systems, including lakes, wetlands, and the surface/groundwater interaction zone. The proposed global statistical model will be flexible enough to explain floods and drought events governed by large scale atmospheric/oceanic patterns (for example, the El Nio Southern Oscillation) that a local model could miss. This application will focus on four regions in the continental USA known to be sensitive to precipitation events. Outreach activities at different levels, from lectures to high school students to events for the local community, are planned to increase awareness on the value of healthy hydrological systems, and a computer program will allow users to explore which areas in the United States are at higher risk of floods and droughts. The graduate student support will be used on interdisciplinary research and writing codes.<br/> <br/> <br/>Models for global data represent a theoretical challenge, as there are restrictions in defining valid processes over the sphere and time. Practical and computational challenges also exist as these models must be both flexible enough to capture non-trivial data structure across the globe, and be able to fit the extremely large size of modern data sets (billions of points). A latent Gaussian model for global spatio-temporal data is proposed, which will control the spatial dependence by a Stochastic Partial Differential Equation with an operator able to capture non-stationarity with a local tensor deformation, and changing behavior across land and ocean to allow for a smooth transition across the two domains. The model will be solved with a finite volume approach which will guarantee sparsity of the precision matrix in the latent process, thus allowing scalability for extremely large data sets. The application will address the critical issue in hydrology of the assessment of the uncertainty in future health of ecohydrological systems. Global simulations of daily precipitation and a mass conservation equation will provide estimates of the future risk to droughts and floods in four regions in the United States.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1943902","CAREER: Learning Probabilistic Factor Models","DMS","STATISTICS","07/01/2020","07/31/2023","Zheng Ke","MA","Harvard University","Continuing Grant","Yong Zeng","06/30/2025","$315,637.00","","zke@fas.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269","1045","$0.00","A large amount of text and social network data is emerging in scientific research as well as everyday life. This project will develop statistical methods for analyzing data resulting in new scientific, sociological, and biomedical discoveries. The research has several fundamental challenges due to the features of the data: (1) large scale, which requires advanced strategies on storage, computation, and quality control; (2) a complicated structure, which makes careful statistical modeling a critical need; and (3) strong noise, which requires sophisticated de-noising techniques. To address these challenges, the PI proposes a universal probabilistic factor modeling approach. The research will provide an array of statistical tools for social network analysis, natural language processing, RNA-sequencing data analysis, and electronic health records analysis.  This project will also help train graduate and undergraduate students on data collection, data cleaning, statistical methodology and theory. In addition, this project will release new software and data sets for network and text analysis providing useful resources for both education and research.  <br/><br/>Probabilistic factor models refer to factor models whose factors or factor loadings are connected to probability mass functions. Examples include the topic models in text mining and mixed membership models in social networks. Due to the nonnegative constraints and the dependent and heteroscedastic noise in these models, statistical estimation and inference are extremely challenging. This project will tackle these challenges and apply the proposed methods to different applications. The first thrust aims to develop a novel framework for exploring sparsity in topic models. It proposes a new notion of ""sparsity"" on the vocabulary, which is different from the conventional notion of sparsity in high-dimensional statistics. The framework will provide a theoretical foundation for dimension reduction in text mining, as well as new word screening methods and new spectral methods for topic weight estimation. The second thrust aims to study the fundamental statistical limits for network mixed membership estimation. It will lead to a new optimality theory of mixed membership estimation, especially for network models with a large degree of heterogeneity, and new random matrix theory for empirical eigenvectors. It will also produce data sets about the networks among academic researchers in statistics-related fields and generate discoveries about the trend and patterns in academic research. The third thrust aims to adapt the above technical tools to biomedical data, including bulk and single-cell RNA-sequencing data and electronic health care data. It will result in new mixture models and statistical inference tools for biomedical data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015361","Parameter Estimation Theory and Algorithms under Latent Variable Models and Model Misspecification","DMS","STATISTICS","07/01/2020","06/17/2020","Xuanlong Nguyen","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yong Zeng","06/30/2023","$200,000.00","","xuanlong@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","079Z","$0.00","Latent variables models have become one of the most powerful tools in modern statistics and data science. They are indispensable in the core data-driven technologies responsible for advancing a vast array of domains of engineering and sciences. While these tools represent remarkable achievements in which statisticians have played fundamental and decisive roles, there are urgent and formidable challenges lying ahead. As these tools are increasingly applied to ever bigger data sets and systems, there are deep concerns that they may no longer be understood, nor is their construction and deployment reliable or robust.  When treated as merely black-box modeling devices for fitting densities and curves, latent variable models are difficult to interpret and can be hard to detect or fix when something goes wrong, either when the model is severely misspecified or the learning algorithms simply break down. This project  aims to address the theoretical and computational issues that arise in modern latent variable models, and the learning efficiency and interpretability of such statistical models when they are misspecified.<br/><br/><br/>The goals of this project are to develop new methods, algorithms and theory for latent variable models. There are three major aims: (1) a statistical theory for parameter estimation that arises in latent variable models;  (2) scalable parameter learning algorithms which account for the geometry of the latent structures, as well as the geometry of the data representation arising from specific application domains; and (3) impacts of model misspecification on parameter estimation motivating the development of new methods. These three broadly described aims are partly motivated by the PI's collaborative efforts with scientists and engineers in several data-driven domains, namely intelligent transportation, astrophysics and topic modeling for information extraction. In all these domains, latent variable models are favored as an effective approximation device, but practitioners are interested in not only predictive performance but also interpretability. In terms of methods and tools, this research draws from and contributes to several related areas including statistical learning, nonparametric Bayesian statistics and non-convex optimization. In terms of broader impacts, the development of scalable geometric and variational inference algorithms for latent variable models will help to expand the statistical and computational tool box that are indispensable in the analysis of complex and big data. The investigation into the geometry of singularity structures and the role of optimal transport based theory in the analysis of models and the development of algorithms will help to accelerate the cross-fertilization between statistics and mathematics, computer science and operations research. In terms of education and training, the interdisciplinary nature of this project provides an exciting opportunity to attract and train a generation of researchers and students in variational methods and optimization, statistics and mathematics, as well as machine learning and intelligent infrastructure.  The materials developed in this project will be integrated into an undergraduate honor course and a summer school for statistical science and big data analytics developed at the University of Michigan.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2014053","Collaborative Research: Segmentation of Time Series via Self-Normalization","DMS","STATISTICS","07/15/2020","06/27/2022","Zifeng Zhao","IN","University of Notre Dame","Continuing Grant","Yong Zeng","06/30/2023","$99,999.00","","zzhao2@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","1269","","$0.00","This project aims to develop new statistical methodology and theory for change-point analysis of time series data. Change-point models have wide applications in many scientific areas, including modeling the daily volatility of the U.S. financial market, and the weekly growth rate of an infectious disease such as coronavirus, among others. Compared with existing methodologies, this research will provide inference for a flexible range of change point models, which will remain valid under complex dependence relationships exhibited by real datasets. The methodologies ensuing from the project will be disseminated to the relevant scientific communities via publications, conference and seminar presentations, and the development of open-source software. The Principal Investigators (PIs) will jointly mentor a Ph.D. student and involve undergraduate students in the research, and offer advanced topic courses to introduce the state-of-the-art techniques in time series analysis.<br/><br/>Time series segmentation, also known as change-point estimation, is one of the fundamental problems in statistics, where a time series is partitioned into piecewise homogeneous segments such that each piece shares the same behavior. There is a vast body of literature devoted to change-point estimation in independent observations; however, robust methodology and rigorous theory that can accommodate temporal dependence is still scarce. Motivated by the recent success of the self-normalization (SN) method, which was developed by one of the PIs for structural break testing and other inference problems in time series, the PIs will advance the self-normalization technique to time series segmentation. Specifically, the PIs will develop a systematic and unified SN-based change-point estimation methodology and the associated theory for (i) segmenting a piecewise stationary time series into homogeneous pieces so within each piece a finite dimensional parameter is constant; (ii) segmenting a linear trend model with stationary and weakly dependent errors into periods with constant slope. The segmentation algorithms to be developed are broadly applicable to fixed-dimensional time series data and can be further extended to cover high-dimensional and locally stationary time series with proper modification of the self-normalized test statistics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015272","Manifold Coordinates with Physical Meaning","DMS","STATISTICS","08/15/2020","08/04/2020","Marina Meila","WA","University of Washington","Standard Grant","Yong Zeng","07/31/2023","$150,000.00","","mmp@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","","$0.00","Finding low dimensional meaningful descriptors for high-dimensional phenomena has been one of the motors of scientific discovery. For instance, the ""genetic maps"" introduced by Cavalli-Sforza map the variation of the human genomes into 2-dimensional geographic locations, charting prehistorical migrations. In this example, the scientists' intuition guided the mapping of the genomes on the spatial dimensions. This project will develop a general statistical framework to expand and automate this process. A  scientist provides a list of descriptors with scientific meaning, that could be used to unfold the data in low dimensions. This list is called a ""dictionary"". The dictionary mediates between expert knowledge, expressed with the concepts of the scientific domain, on one hand, and the lower level representations of the data used by learning algorithms, on the other. This project will facilitate the tranfer of knowledge between scientist and machine. A statistical learning algorithm will replace the task of manually checking individual descriptors for correlation with the data variation; the algorithm will perform this task on the whole dictionary at once. The output is a small set of descriptors from the dictionary, which together capture most of the variation in the data; we call them Interpretable Embedding Coordinates (IEC). Unlike Principal Components or Principal Directions, which are abstract, these coordinates are always meaningful and interpretable, because they are selected from the dictionary of descriptors supplied by the scientist.<br/><br/>In this project it is assumed that the data lie on or near a smooth low-dimensional manifold; the dictionary consists of smooth functions on the manifold. The new method finds coordinates in the manifold among the interpretable, meaningful functions in the dictionary.  Interpretable Embedding Coordinates (IEC) will be formulated as a non-parametric, non-linear sparse functional regression problem. The main idea is to tranform this problem into a linear sparse regression in the space of function gradients. This allows one to apply the well-developed aresenal of sparse recovery methods to IEC, without sacrificing the original non-linearity of the problem. Statistical and geometric guarantees for recovery will be given. The new methods will be integrated into the big data unsupervised learning platform megaman, distributed and maintained by Meila's group. PI Meila, with support from the UW eScience Institute, will disseminate the ideas and methods in an on-line Active Training Lab on Unsupervised Learning. This project is part of Meila's current research program ""Unsupervised Validation for Unsupervised Learning"" to design mathematically founded methods to interpret, verify and validate the output of machine learning algorithms for scientific data and scientific discovery.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2012448","Collaborative Proposal: Models and Methods for High Quantiles in Risk Quantification and Management","DMS","STATISTICS","08/01/2020","07/27/2020","Liang Peng","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Yong Zeng","07/31/2023","$80,000.00","","lpeng@gsu.edu","58 EDGEWOOD AVE NE","ATLANTA","GA","303032921","4044133570","MPS","1269","","$0.00","In recent years, vulnerabilities in financial markets, economies, and public health have posed increasingly severe risks to society. For monitoring natural disasters and forecasting epidemics, financial institutions and governmental organizations must invest in risk intelligence to clearly define, understand, measure, quantify, and manage their tolerance for and exposure to risk. By employing rigorous and robust analytics to measure, quantify, and forecast risk, business leaders and regulators can rely less on intuition and more on systematic methodologies to manage risk well and make sound policy decisions. This project will develop improved and powerful analytic tools for applied researchers, regulators, and practitioners to conduct risk assessment. These tools and techniques will have broad impacts in wide-ranging fields such as economics, finance, and insurance. The project also intends to provide training opportunities for graduate students and broaden the participation of underrepresented groups in statistics and actuarial science. <br/><br/>This research project focuses on the uncertainty quantification, back-test, and sensitivity analysis for both conditional and unconditional risk measures computed from mathematical models. This project develops a computationally efficient two-step inference for an ARMA-GARCH model and fits parametric and semi-parametric distribution family to residuals. The investigators will study semi-supervised learning for risk analysis when other variables with a large sample size are available. They also plan to validate residual-based bootstrap methods for quantifying risk uncertainty and develop efficient ways for risk forecasts and back-tests. The new methodologies combine some modern statistical techniques such as extreme value theory for forecasting catastrophic risk, weighted estimation for handling both infinite variance and persistent volatility, and empirical likelihood method for efficient hypothesis testing. These techniques are robust and applicable to various problems in risk management and other research fields requiring uncertainty quantification.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015485","Advancing High-Dimensional Bayesian Asymptotics and Computation","DMS","STATISTICS","07/01/2020","06/26/2020","Yves Atchade","MA","Trustees of Boston University","Standard Grant","Yong Zeng","06/30/2024","$160,000.00","","atchade@bu.edu","1 SILBER WAY","BOSTON","MA","022151703","6173534365","MPS","1269","","$0.00","The big data revolution has turned statistics and machine learning into highly active and fast-pace research areas that have seen great progress over the last few decades.  However  uncertainly quantification with big data and complex models remains a challenge in the field.   In theory, Bayesian statistics solves -- elegantly and straightforwardly -- the uncertainty quantification problem. There is therefore a need for ideas and methods for constructing useful and computationally scalable Bayesian procedures. This research project contributes towards that goal. The developed methodology can improve decision making in areas such as autonomous driving, medical diagnostics, bail decision, credit worthiness, criminal sentencing, to list a few. This research will also include training for graduate students. <br/><br/><br/>This project contributes to the development of theoretically sound, and computationally scalable Bayesian methodologies for the recovery of high-dimensional parameters. Toward that goal, the PI will develop a novel and widely applicable quasi-Bayesian (semi-parametric) framework for learning high-dimensional parameters.  The project  will also contribute to the development of Bayesian asymptotic theory with the analysis of high-dimensional, non-identifiable models.  Several high-profile models (e.g. neural network models)  widely  used in the applications are non-identifiable.  By applying the new framework to canonical correlation analysis, this project will also contribute to the development of flexible Bayesian solutions for high-dimensional sparse canonical correlation analysis,  with wide applicability in bio-medical research. This research project will also contribute to the computational aspects of high-dimensional Bayesian statistics with the development of several novel MCMC and VA algorithms. Finally, this research project will also contribute more broadly to statistics and machine learning with the development of Bayesian generative adversarial networks (GAN).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015376","Multivariate Distribution-Free Nonparametric Testing Using Optimal Transportation","DMS","STATISTICS","07/01/2020","06/26/2020","Bodhisattva Sen","NY","Columbia University","Standard Grant","Yong Zeng","06/30/2023","$300,000.00","","bodhi@stat.columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","","$0.00","Nonparametric methods have become increasingly popular in the theory and practice of statistics in recent times primarily because of their fundamental advantages over parametric methods: greater flexibility and more ""data-driven'' features. In this project the Principal Investigator (PI) analyzes the estimation, computation and uncertainty quantification in two important areas of nonparametric statistics. Special emphasis is given to methods applicable to multivariate data, an area that has received relatively less attention, though often necessary in performing real data analyses. The methods developed will be tuning-free, computationally feasible, and well-defined under minimal assumptions on the underlying models. On the collaborative front, the PI will continue interdisciplinary research in astronomy and in particular, some of the methodology discussed in this project will address important scientific questions arising from astronomy and will form the dissertation theses of two PhD students at Columbia. The PI will also continue the tradition of mentoring summer interns. The graduate student support will be used on interdisciplinary research and writing codes.<br/><br/><br/>The PI investigates two core directions of research in nonparametric statistics with special emphasis to multivariate data. The main trust of this project is to study multivariate distribution-free nonparametric rank-methods based that generalize the classical univariate rank-based procedures to multivariate data. This new general framework crucially uses ideas from the theory of optimal transport ? an important and very active research area in applied mathematics/probability/machine learning. The second part of the project is on multivariate nonparametric (heteroscedastic) mixture models and is directly motivated by astronomy collaborations involving the PI. Statistical inference of stellar populations of interest is complicated by significant observational limitations ? in particular, by heteroscedastic measurement errors. Indeed, almost all data sets in astronomy contain known (heteroscedastic) error measurements on every observation. This naturally leads to data that can be modeled as nonparametric (heteroscedastic) mixture models. The PI, along with his collaborators, will investigate several aspects of this problem: (a) estimation of the data distributions, (b) denoising the observations, and (c) studying the associated deconvolution and manifold learning problems. For both the above problems, a systematic theoretical study of the methods will be undertaken.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015365","Modeling and Inference for Dynamic Network Analysis","DMS","STATISTICS","07/01/2020","06/20/2020","Harry Crane","NJ","Rutgers University New Brunswick","Standard Grant","Yong Zeng","06/30/2024","$160,000.00","","hcrane@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","The project will initiate a systematic study of statistical models for network data and other complex data structures which commonly arise from interacting and self-organizing processes such as protein folding, gene expression, neural functioning, economic activity, and social behavior.  A focus of the project is to address novel challenges that cannot be handled by common approaches to statistical modeling and inference, which often rely on assumptions of (i) statistical regularity, (ii) short range dynamics driven by forces exogenous to the data, and (iii) representability of the data as the aggregation of isolated measurements taken on a representative sample of units.  These assumptions are often violated in complex data problems, which are characterized by (i) high levels of interaction among different components of the data, (ii) dynamical behaviors driven by endogenous feedback mechanisms, and (iii) partial or complete irreducibility of the data structure. The project will produce new methodologies, theoretical results, and conceptual insights for statistical inference in these settings. Beyond substantive technical contributions, which will have an impact across scientific domains, research from the project will be widely disseminated to the general public through the PI's participation in forums for communicating probability and statistics to interdisciplinary audiences.  The PI trains graduate students and runs a weekly seminar on the Foundations of Probability and Statistics, with videos uploaded for open public access at his website. In addition, the PI advocates strongly for peer review reform and open source publication, and will publish all work from this project for public peer review on Researchers.One, a non-profit publishing outlet aimed at increasing the quality and accessibility of peer review across research disciplines.<br/><br/>To achieve these aims the project will develop rigorous theory and robust statistical methods for analyzing dynamic and complex network data structures.  Desired outcomes include new theory, models, methods, and concepts for network analysis,  a deeper understanding of the scope and limitations of statistical tools for modern network analysis, and a general framework for modeling network data that arises across scientific disciplines. Model development lies at the core of the project, with a focus on extending recently proposed model classes of edge and relationally exchangeable network models, rewiring models, and graph-valued Levy process models to a flexible statistical framework for latent space relational models and network-valued autoregressive and state space models.  From these models, the project will produce a theoretical framework as well as a range of methodological tools for future developments in statistical network analysis.  The project will draw on concepts and techniques from a wide range of topics including Bayesian nonparametrics, spatial statistics, time series, probability theory, stochastic processes, and computing, as well as mathematical concepts from graph theory, combinatorics, and algebra.  The research will, therefore, contribute substantially to disciplines across the mathematical sciences, where network and complex data analysis have become increasingly relevant for scientific research in proteomics, genomics, economics, social science, finance, biology, computer science, and physics as well as methodologically driven disciplines within statistics and related fields, such as data science, artificial intelligence, and machine learning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2014636","Bootstrap Methods in High Dimensions: Complex Dependence Structures and Refinements","DMS","STATISTICS","07/01/2020","06/17/2020","Kengo Kato","NY","Cornell University","Standard Grant","Yong Zeng","06/30/2023","$100,000.00","","kk976@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","","$0.00","Bootstrap methods constitute a class of powerful statistical methodologies for uncertainty quantification in statistical analysis. Recent developments demonstrate that bootstraps are effective in developing theoretically valid inferential methods for high-dimensional or large-scale data. Modern-day data availability allows us to access large-scale data with complex dependence structures, for which there is a serious need to develop high-quality inference methods. Examples of such data include data observed on networks, multiway-clustered data often seen in economics, environmental data, and high-frequency financial data. In this project, the PI aims to develop bootstrap methods for such large-scale data with complex dependence structures, as well as make refinements on existing bootstrap methods for independent data. Additionally, the project will provide research training opportunities for graduate students.<br/><br/>The first line of research this project aims at is to develop novel bootstrap methods effective in high dimensions for i) exchangeable arrays, ii) irregularly spaced spatial data, and iii) diffusion (or, more generally, Markov) processes. Those new bootstrap methods have direct applications to inference with high dimensional data such as simultaneous and multiple testing and nonparametric inference such as the construction of simultaneous confidence bands. The second line of research is to make refinements on high dimensional bootstrap methodology with independent data. One is to develop error bounds for the general exchangeably weighted bootstrap in the high dimensional regime that are comparable to the Gaussian multiplier bootstrap. The other is to develop error bounds for the nonparametric bootstrap that can explain the superior performance in numerical experiments in the increasing dimension setup.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2014279","Collaborative Research: Fine-Grained Statistical Inference in High Dimension: Actionable Information, Bias Reduction, and Optimality","DMS","STATISTICS","07/01/2020","06/16/2020","Yuxin Chen","NJ","Princeton University","Standard Grant","Yong Zeng","06/30/2024","$100,000.00","","yuxinc@wharton.upenn.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","","$0.00","Emerging data science applications require efficient extraction of actionable insights from large and messy datasets. The number of relevant features often overwhelms the volume of data that is available, which dramatically complicates the statistical inference tasks and subsequent decision making. In the existing statistical literature, most of theory aims at understanding the average or global behavior of a statistical estimator in high dimensions. In many applications, however, it is often the case that the goal is not to explore the global behavior of a parameter estimator, but rather to perform inference and reasoning on its local, yet important, operational properties.  The techniques and methods developed in the project will further advance the interplay between a broad range of areas including high-dimensional statistics, harmonic analysis, statistical physics, optimization, complex analysis, and statistical machine learning. The project provides research training opportunities for graduate students.<br/><br/><br/>This project pursues fine-grained inferential procedures and theory, aimed at enlarging the uncertainty assessment toolbox for various low-complexity models in high dimensions. Focusing on a few stylized problems, this research program consists of four major thrusts: (1) construct optimal confidence intervals for linear functionals of eigenvectors in low-rank matrix estimation; (2) design fine-grained hypothesis testing procedures for sparse regression under general designs; (3) develop entry-wise inference schemes for principal component analysis with missing data; and (4) conduct reliable and adaptive statistical eigen-analysis under minimal eigen-gaps. Emphasis is placed on algorithms that are model-agnostic and fully adaptive to data heteroscedasticity. Addressing these issues calls for the development of new statistical theory that enables reliable inference for a broad class of local properties underlying the unknown parameters.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2014971","Empirical Process Theory for Complex Statistical Data Integration","DMS","STATISTICS","07/01/2020","08/08/2022","Takumi Saegusa","MD","University of Maryland, College Park","Continuing Grant","Yong Zeng","06/30/2024","$204,446.00","","tsaegusa@umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1269","","$0.00","Nowadays, every organization collects various data sets from numerous sources. If these data sets are combined, improved quality of inference will accelerate scientific discovery. Statistical analysis of merged data is, however, challenging because each data set often represents only a part of the entire target population and because combined data contain unidentified duplicated records from data sets which share data sources partially. This research provides theoretical and methodological foundations to address the issue of unavoidable bias in data integration arising from heterogeneity and duplication in merged data. With the proposed data integration technique, previously limited findings to smaller populations are combined to be generalized to a broader population. The proposed methodology serves well for privacy protection by avoiding record linkage that identifies duplication through private information. Another benefit is to overcome the shortage of relevant information in individual data sources without collecting costly(and possibly small) independent and identically distributed data all over again. Expected outcomes from this project will encourage the efficient and socially proper use of massive data in modern data analysis. The graduate student support will be used on interdisciplinary activities and writing codes. <br/><br/>The project delves into the intersection of empirical process theory, semi- and non-parametric inference, and sampling theory. Existing theory and methods fail to provide sufficient tools to study complex data integration problems characterized by bias and dependence due to heterogeneity and duplication. Inverse probability-weighted empirical process theory requires a special independence structure on weights and variables. Semi- and non-parametric inference often relies on the availability of the independent and identically distributed sample. Sampling theory handles dependence in a specific design but focuses on a parametric model without accounting for randomness in collected variables in a finite population framework. To address the paucity of probabilistic tools and techniques, the PI will develop a unified framework in connection with a weighted empirical process motivated by multiple frame surveys. This weighted empirical process is computable without identifying duplicated selections. The proposed tools and techniques will play a critical role in studying a general sample selection and missing data mechanisms such as a convenience sample, semiparametric estimation with misspecified models, and multiple observations for duplicated subjects in overlapping data sources. The particular problems under investigation include (a) uniform limit theorems under general missingness mechanisms, (b) robust M-estimation under model misspecification for data integration, and (c) general theory to integrate multiple probability measures that correspond to heterogeneous data sources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2013930","Advances in Bayesian Nonparametric Methods for Jointly Modeling Multiple Data Sets","DMS","STATISTICS","07/01/2020","06/27/2022","Li Ma","NC","Duke University","Continuing Grant","Yong Zeng","06/30/2024","$199,999.00","","li.ma@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","","$0.00","Bayesian nonparametrics is a statistical modeling framework that combines flexibility of classical nonparametric statistical methods with principled assessment of uncertainty under the Bayesian paradigm. Traditional Bayesian nonparametric methods, however, have largely focused on models based on a single data set, while many modern statistical scenarios involve multiple data sets of similar nature collected under related or comparative conditions. This project develops a suite of new modeling and computational strategies that are tailored for effective joint modeling of multiple data sets in ways that (i) capture cross-sample variation in modern complex data, and (ii) are computationally efficient to allow application to massive data. The developed methodology will have impact in a range of fields including biology, economics, education, astrophysics, political science, and climate science, where the task is to properly characterize variation across data sets. The project provides excellent research training opportunities for graduate students.<br/><br/>Novel models, methods, and algorithms will be developed in the context of two classes of widely used Bayesian nonparametric models: (i) mixture models with discrete random measure (DRM) mixing distributions (e.g., Dirichlet process mixtures) and (ii) tree-structured random measure (TSRM) models (e.g., Polya tree type models). These two model classes are different in nature with each having its own advantages and limitations in modeling multiple data sets, and as such the strategies in advancing these two model classes are distinct. A key limitation of DRM mixtures in modeling multiple samples is their lack of flexibility in characterizing the cross-sample variation, and thus a new latent variable modeling strategy will be developed for substantially enhancing their capacity in this regard. Also to be investigated are the theoretical and empirical properties of the resulting dispersion mixture models, as well as generalizations of this strategy in broader ranges of hierarchical models where incorporating flexible cross-sample variation in observed and latent quantities is important. For TSRM models, capable of characterizing complex cross-sample variation, the focus is on addressing their lack of scalability, both computational and statistical, with respect to increasing dimensionality as well as their sensitivity to the underlying tree structures, a critical component for building such models. The development for these two model classes will form a powerful and general toolbox that can be applied in a variety of scientific and engineering problems involving the analysis of multiple related data sets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2014928","Statistical Inference for Multilayer Network Data with Applications","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","08/01/2020","06/14/2020","Marianna Pensky","FL","The University of Central Florida Board of Trustees","Standard Grant","Yong Zeng","07/31/2024","$300,000.00","","Marianna.Pensky@ucf.edu","4000 CENTRAL FLORIDA BLVD","ORLANDO","FL","328168005","4078230387","MPS","1269, 7454","068Z","$0.00","Analysis of stochastic networks is extremely important and is used in a variety of applications such as sociology, biology, genetics, ecology, information technology and national security. Networks are very convenient for describing relationship between nodes that may represent people in a social network  or brain regions of a person. One of the properties of the majority of networks is that they can be divided into communities with distinct properties and connection patterns. These partitions can be used for answering a variety of questions such as finding tightly connected social groups, or identifying brain regions' connection patterns associated with a disease. While initial efforts were focused on analysis of a single network model, in the last few years one of the most important directions in network science has shifted to the study of sets of individual networks, the so-called multilayer networks, due to both the versatility of the multilayer networks and the variety of applications that can be addressed using this concept. The objective of this research is to develop tools for theoretical and algorithmic analysis of such networks. The theories developed can be applied to analysis of speech-related brain networks that can be affected by epilepsy surgery. This research will be carried out in collaboration with the Functional Brain Mapping and Brain Computer Interface Lab of Advent Health Hospital for Children. The techniques resulting from this project could be applied in a variety of fields that rely on analysis of multilayer stochastic network data: a) medical practice, since a better understanding of connections between brain regions associated with speech will result in more safe and efficient epileptic treatment options; b) medical research, by providing tools for taking into account individual variations of connections between brain regions associated with particular diseases; c) brain science research, by providing tools for analysis of brain networks and their variations; d) molecular biology, by proposing techniques for analyzing the enzymatic influences between proteins related to various functions; e) statistical genetics, by developing procedures  for simultaneous studies of gene networks related to several diseases; f) international relations and finance, by analyzing world trade and financial networks corresponding to various modalities; g) social sciences, by analyzing the similarities and the differences in communities related to various types of social connections.  Funding will also be used for training work force by carrying out various educational activities, and promoting interdisciplinary research and diversity.<br/><br/>The research agenda of this proposal will substantially advance the fields of non-parametric statistics in general, and the emerging field of network data analysis in particular. The spark of the interest in multilayer networks has led to a stream of publications on the subject. However, these publications fall into two very distinct categories: applications driven papers with no theoretical guarantees of the results and statistical papers where those guarantees are provided, but under very restrictive assumptions. While in many applications the main goal is to determine the differences between communities in different layers or sets of layers, the statistical papers focus entirely on the case where the communities are the same in all layers. Due to the absence of relevant theoretical results, in applications, the authors either utilize ad hoc techniques or are forced to make a questionable assumption that the community structure is the same for all the layers. For this reason, there is an overwhelming need for laying solid theoretical foundations and developing efficient computational algorithms for analysis of multilayer networks with diverse community structures. In particular, the objective of this research is the construction of non-parametric techniques that carry out estimation and clustering of multilayer networks where each layer follows the popular Stochastic Block Model and the community structures coincide for some layers and differ for the others. Furthermore, statistical procedures will be supplemented with the precision guarantees via oracle inequalities and minimax studies. This will be accomplished by application of modern algebraic techniques recently employed by the PI. In summary, the research will significantly broaden the arsenal of methods applicable to analysis of multilayer network data by developing techniques for non-parametric estimation and clustering that require few assumptions, are computationally viable, and are also accompanied by theoretical precision guarantees.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1945667","CAREER: High-Dimensional Statistical Models for Unsupervised Learning","DMS","STATISTICS","07/15/2020","07/11/2023","Arash Amini","CA","University of California-Los Angeles","Continuing Grant","Yong Zeng","06/30/2025","$317,745.00","","arash.amini@stat.ucla.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244201","3107940102","MPS","1269","1045","$0.00","The growing awareness of the importance of data and data analysis, coupled with the unprecedented growth in the amount of data in recent years, has led to concerted efforts by researchers in the fields now collectively referred to as data sciences, to develop new models capable of handling big complex datasets. The vast majority of the available data is unlabeled, which makes the modeling problem more challenging. This project will advance the field of modeling big complex unlabeled data.  The focus will be on learning from network data as well as learning dependency structures from regular data. Some of the concrete problems investigated are: What can an epidemic spreading over a network tell us about the structure of the network and the origin of the epidemic? What can the structure of the network tell us about the latent features of the nodes, for example, their grouping, or community in the case of social networks? Are there more refined structures in real networks beyond simple grouping or community structure? Can we learn complex networks from regular data that tell us about the nature of the dependency among the underlying variables (for example, what variables are the causes of a given variable)? How well do these often complex models fit the real data? Advancing on these questions has a direct impact on many scientific domains dealing with data. For example, genomics and computational biology, neuroscience, epidemiology, network security, social sciences and marketing, all benefit from advances in network analysis. Advances in dependency structure learning can improve causal inference procedures with impact on all scientific fields. This project on network epidemics has the potential to be transformative with immediate applications to the public health domain.<br/><br/>This project advances the state-of-the art in inferring complex relations from data in an unsupervised fashion. As a result, network inference and graphical modeling will play prominent roles in our approach. We will consider four main tasks: 1) Developing goodness-of-fit tests for structured network models, in particular those used in community detection and clustering. Despite advances in network modeling, there are concerns that current models are not capturing the complexity of real networks. A first step toward realistic network modeling is developing tools for assessing how well the models fit. 2) Advancing the state-of-the-art in modeling complex networks, presenting ideas on capturing self-similarity in real networks as well as hierarchical statistical models for multilayer networks. 3) Advancing inference based on network dynamics: Many networks are accompanied by dynamics governed by the network structure, e.g., the spread of rumors and diseases. We often observe the result of the dynamics (who gets infected over time) and would like to make inference about the origin of the dynamic or the structure of the underlying network. We will address the challenges in dealing with these questions in real networks where the presence of many cycles and incomplete information about the dynamic pose serious difficulties. 4) Advancing inference of high-dimensional dependency structures: Characterizing dependencies (correlation, causation, etc.) among a collection of random variables is a fundamental task of statistical analysis. The principal investigator will explore learning high-dimensional directed graphical models from data that are suitable for causal interpretations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1945136","CAREER: The Design-Based Perspective of Causal Inference in Complex Experiments","DMS","STATISTICS","07/01/2020","04/27/2023","Peng Ding","CA","University of California-Berkeley","Continuing Grant","Yong Zeng","06/30/2025","$314,860.00","","pengdingpku@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","1045","$0.00","Randomized experiments have been widely used in agriculture, industry, and clinical trials. R. A. Fisher formally discussed the value of randomization in experiments: it balances observed and unobserved covariates on average and serves as a basis for statistical inference. Classical results, however, are limited to simple experiments without rich covariates and complex time and hierarchical structures. Modern applications stemming from the social sciences and technology companies have richer covariates and more complex time and hierarchical structures. Motivated by these new applications, the PI will advance the theories and methodologies for the design and analysis of modern experiments for robust treatment effect estimation in various settings. Highlighting the role of the design of experiments, the PI will take a coherent design-based perspective of causal inference. In particular, the PI will propose various new experimental designs that can better balance covariates across experimental groups and develop statistical methods for these designs that are robust to model assumptions on the outcome generating processes. These theoretical results for experiments will also shed light on principled analyses of observational studies where controlled experiments are infeasible. The training component includes graduate and undergraduate course work as well as the development of software through the help of both undergraduate and graduate students. This constitutes a strong plan to integrate research and education.<br/><br/><br/>The design-based perspective of causal inference does not assume any strong outcome modeling assumptions and focuses on the treatment assignment mechanism that can be determined by the experimenters. Under this perspective, the PI will improve existing experimental designs to havebetter covariate balance and evaluate many model-based procedures when the corresponding model assumptions can be violated. The PI will first propose and analyze rerandomization in blocking, sequential and factorial settings, focusing on repeated sampling properties of the treatment effect estimators and discussing the estimators with and without covariate adjustment. The PI will then propose and analyze linear and nonlinear covariate-adjusted estimators for treatment effects, including the cases with and without noncompliance. Moreover, the PI will calibrate randomization tests with targeted weak null hypotheses and propose randomization tests with robust and efficient covariate adjustment, based on detailed analyses of completely randomized experiments with covariates and finely stratified experiments. The PI will also establish randomization-based inferential frameworks and procedures for experiments with time and hierarchical structures. Finally, the PI will develop and disseminate open-source R software packages that implement the methodologies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015469","New Tools for Analyzing Complex Network and Text Data","DMS","STATISTICS","07/01/2020","06/17/2020","Jiashun Jin","PA","Carnegie-Mellon University","Standard Grant","Yong Zeng","06/30/2024","$250,000.00","","jiashun@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","079Z","$0.00","Network data and text data are frequently found in many different areas of science and business (for example, social networks, genetic networks, news,  tweets, and blogs). A problem of great interest in statistics and machine learning is how to extract information from network data and text data. The research in this project will have several components:  (a) collecting and cleaning large-scale network data sets and text data sets,  (b)  developing new models, methods, and theory for analyzing network data and text data, and (c) analyzing a large data set, which the PI and collaborators have collected and cleaned (the data set is on the publication data of statisticians). The research will have an impact in social science, business, and especially in understanding the publication patterns and trends of statisticians. The project also provides training opportunities for undergraduate and graduate students. <br/><br/>Network data and text data are widely available and contain valuable information for answering many scientific problems. This project will focus on network analysis and text learning and will make contributions in the following topics:   (a) development of models for network and text data,  (b) development of computationally feasible approaches to network global testing, pairwise comparison,  dynamic network analysis, and text learning,  and (c) development of new approaches for studying the research patterns and trends of the statistical community. The research will lead to new data sets and new methods and theory in the application areas of social networks, genetic networks, and text mining, and will have an impact in social science, business, and biology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2014018","Collaborative Research: Segmentation of Time Series via Self-Normalization","DMS","STATISTICS","07/15/2020","06/16/2020","Xiaofeng Shao","IL","University of Illinois at Urbana-Champaign","Standard Grant","Yong Zeng","06/30/2024","$149,999.00","","xshao@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","","$0.00","This project aims to develop new statistical methodology and theory for change-point analysis of time series data. Change-point models have wide applications in many scientific areas, including modeling the daily volatility of the U.S. financial market, and the weekly growth rate of an infectious disease such as coronavirus, among others. Compared with existing methodologies, this research will provide inference for a flexible range of change point models, which will remain valid under complex dependence relationships exhibited by real datasets. The methodologies ensuing from the project will be disseminated to the relevant scientific communities via publications, conference and seminar presentations, and the development of open-source software. The Principal Investigators (PIs) will jointly mentor a Ph.D. student and involve undergraduate students in the research, and offer advanced topic courses to introduce the state-of-the-art techniques in time series analysis.<br/><br/>Time series segmentation, also known as change-point estimation, is one of the fundamental problems in statistics, where a time series is partitioned into piecewise homogeneous segments such that each piece shares the same behavior. There is a vast body of literature devoted to change-point estimation in independent observations; however, robust methodology and rigorous theory that can accommodate temporal dependence is still scarce. Motivated by the recent success of the self-normalization (SN) method, which was developed by one of the PIs for structural break testing and other inference problems in time series, the PIs will advance the self-normalization technique to time series segmentation. Specifically, the PIs will develop a systematic and unified SN-based change-point estimation methodology and the associated theory for (i) segmenting a piecewise stationary time series into homogeneous pieces so within each piece a finite dimensional parameter is constant; (ii) segmenting a linear trend model with stationary and weakly dependent errors into periods with constant slope. The segmentation algorithms to be developed are broadly applicable to fixed-dimensional time series data and can be further extended to cover high-dimensional and locally stationary time series with proper modification of the self-normalized test statistics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015526","Randomized Trials with Non-Compliance: Extending the Angrist-Imbens-Rubin Framework","DMS","STATISTICS","07/01/2020","07/15/2022","Lu Mao","WI","University of Wisconsin-Madison","Continuing Grant","Yong Zeng","06/30/2024","$186,383.00","","lmao@biostat.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","Randomized controlled trial (RCT) refers to a design commonly employed by clinical and sociological studies where study participants are randomly assigned to an investigational treatment arm, or to a control arm to serve as comparison. With randomization as the unique device to eliminate systematic bias in treatment choice, RCT has been rightfully enshrined as the gold standard for any scientific inquiry, especially those involving human subjects. A challenging yet pervasive issue arises, however, when some participants in an RCT do not comply with the random assignment and instead self-select into the treatment of their choice. This self-selection compromises the objectivity of treatment assignment and thereby weakens the rigor of the experiment. A seminal paper by Angrist, Imbens, and Rubin (1996) provided invaluable insight into the difficult task of drawing valid causal inference in RCTs plagued by non-compliance. However, their work is focused on the average of a quantitative outcome as the metric of treatment effect and thus does not apply to other commonly encountered outcome types such as ordinal data (for example, tumor grade). Moreover, it is unclear whether their approach has utilized the available information on each participant to the fullest extent. With the advent of modern statistical/mathematical tools such as empirical processes, semiparametric theory, and functional analysis, the PI seeks to extend the Angrist-Imbens-Rubin (AIR) approach by targeting a much wider scope of effect size measures, or causal estimands, and studying their efficient estimation under a unified framework. The PI will also develop user-friendly software packages implementing the corresponding inference procedures and involve graduate students in the project. Successful completion of this project will equip investigators with more versatile and powerful tools to address non-compliance in RCTs, the mainstay of medical and sociological investigations. <br/><br/>Specifically, the general causal estimand is defined by an arbitrary smooth contrast in the marginal (potential) outcome distribution between the two arms. This formulation unifies seemingly disparate effect size measures for all kinds of outcome types, such as the average treatment effect (ATE), Mann-Whitney effect (i.e., probability of an outcome under treatment being greater than one under control), quantile treatment effect, distributional treatment effect, the win ratio, and so forth. Due to non-identifiability associated with non-compliance, interest is focused on a ?local? version of treatment effects, that is, contrasts made on the sub-population of compliers. These are natural extensions of the local ATE thoroughly studied by AIR. Under standard assumptions, simple nonparametric plug-in estimators are constructed for the local treatment effects based on nonparametric estimators for the complier outcome distributions. The operating characteristics of testing procedures based on these estimators will also be investigated. Finally, the entire framework will be re-cast in semiparametric-theoretical terms to optimize the statistical efficiency of both the estimation and testing procedures. This research will lay the groundwork for future extensions to accommodate baseline covariates, censored outcomes, and non-binary (and possibly time-dependent) treatment.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015382","Collaborative Research: Capturing Salient Features in Point Process Models via Stochastic Process Discrepancies","DMS","STATISTICS","07/01/2020","06/25/2020","Robert Wolpert","NC","Duke University","Standard Grant","Yong Zeng","06/30/2023","$104,215.00","","rlw@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","","$0.00","In the 21st century, mathematical modeling has emerged as a primary tool for scientific investigation.  Scientists construct mathematical and computer models intended to capture the important features of a complex physical phenomenon, and learn more about the phenomenon by exploring what values of any uncertain model parameters lead to model predictions that mimic closely what is observed in nature.  Still, at best the models typically cannot capture all of nature's complexity ? or, as George Box famously put it, ""All models are wrong, but some are useful.""  This is really a good thing for discovery ? often scientists can get new insights and develop deeper understanding by studying precisely why their models fail to match reality.  In this research the PIs will develop a new approach to quantifying this ""discrepancy"" between mathematical models and observations, intended specifically for problems in which the data are numerical counts of events or objects.  Examples arise in nearly every scientific field ? counts of volcanoes or earthquakes or disease cases; of galaxies or stars or exoplanets; of photons or gamma ray burst pulses or neutrinos.  This project will specifically address two classes of problems in astronomy.  One class concerns how astronomers can convert raw data measuring light from astrophysical objects (such as stars and galaxies) into estimates of properties of the sources (such as brightness and color) with accurately quantified uncertainties, even when the precise shapes of the objects are not known, and their images overlap.  A second class concerns using astronomical survey catalogs to learn the dominant demographic properties of stars, galaxies, or minor planets (such as asteroids), such as the distribution of their luminosities or masses. The NSF-funded Vera Rubin Observatory will produce data for both types of problems. The research will include developing fast, open-source<br/>computational algorithms implementing the new approaches.<br/><br/>The project is motivated by application areas in which salient feature discovery is threatened by model misspecification.  In applications with real-valued magnitude data with additive Gaussian errors, statisticians have addressed misspecification by introducing additive discrepancy processes into models, often using Gaussian processes.  The two problem areas addressed here require analysis of discrete count or point process data: photon counting data comprising images and time series from cosmic sources, or demographic data in astronomical survey catalogs.  Both areas rely on Poisson point process models, with an intensity function describing, say, the photon arrival rate (per unit area) as a function of direction and time, or the density of galaxies as a function of spatial location and luminosity.  Additive discrepancy models are not applicable to such discrete-data settings.  The team will develop new semiparametric methods that supplement parametric salient feature models with nonparametric discrepancy processes that flexibly model the departure of salient models from the true data generating process.  An approach serving as a starting point represents the true underlying intensity function as the product of the salient feature model and a stochastic multiplicative discrepancy process.  The salient feature model will be a parametricintensity function (e.g., with location, amplitude, scale, and shape parameters), sometimes in a superposition of multiple components (e.g., stars in an image, pulses in a transient burst, or population components). To model discrepancy from the salient model, a natural choice with appealing theoretical properties is a multiplicative gamma discrepancy process; composition with the Poisson point process leads to an overall negative binomial point process for the observations.  The team will implement this approach, and generalize it in several directions. For demographic models, the discrepancy models will be embedded in a hierarchical model that accounts for measurement error and selection effects.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015386","Collaborative Research: Capturing Salient Features in Point Process Models via Stochastic Process Discrepancies","DMS","STATISTICS","07/01/2020","07/07/2022","Thomas Loredo","NY","Cornell University","Continuing Grant","Yong Zeng","06/30/2024","$135,361.00","","loredo@astro.cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","","$0.00","In the 21st century, mathematical modeling has emerged as a primary tool for scientific investigation.  Scientists construct mathematical and computer models intended to capture the important features of a complex physical phenomenon, and learn more about the phenomenon by exploring what values of any uncertain model parameters lead to model predictions that mimic closely what is observed in nature.  Still, at best the models typically cannot capture all of nature's complexity ? or, as George Box famously put it, ""All models are wrong, but some are useful.""  This is really a good thing for discovery ? often scientists can get new insights and develop deeper understanding by studying precisely why their models fail to match reality.  In this research the PIs will develop a new approach to quantifying this ""discrepancy"" between mathematical models and observations, intended specifically for problems in which the data are numerical counts of events or objects.  Examples arise in nearly every scientific field ? counts of volcanoes or earthquakes or disease cases; of galaxies or stars or exoplanets; of photons or gamma ray burst pulses or neutrinos.  This project will specifically address two classes of problems in astronomy.  One class concerns how astronomers can convert raw data measuring light from astrophysical objects (such as stars and galaxies) into estimates of properties of the sources (such as brightness and color) with accurately quantified uncertainties, even when the precise shapes of the objects are not known, and their images overlap.  A second class concerns using astronomical survey catalogs to learn the dominant demographic properties of stars, galaxies, or minor planets (such as asteroids), such as the distribution of their luminosities or masses. The NSF-funded Vera Rubin Observatory will produce data for both types of problems. The research will include developing fast, open-source<br/>computational algorithms implementing the new approaches.<br/><br/>The project is motivated by application areas in which salient feature discovery is threatened by model misspecification.  In applications with real-valued magnitude data with additive Gaussian errors, statisticians have addressed misspecification by introducing additive discrepancy processes into models, often using Gaussian processes.  The two problem areas addressed here require analysis of discrete count or point process data: photon counting data comprising images and time series from cosmic sources, or demographic data in astronomical survey catalogs.  Both areas rely on Poisson point process models, with an intensity function describing, say, the photon arrival rate (per unit area) as a function of direction and time, or the density of galaxies as a function of spatial location and luminosity.  Additive discrepancy models are not applicable to such discrete-data settings.  The team will develop new semiparametric methods that supplement parametric salient feature models with nonparametric discrepancy processes that flexibly model the departure of salient models from the true data generating process.  An approach serving as a starting point represents the true underlying intensity function as the product of the salient feature model and a stochastic multiplicative discrepancy process.  The salient feature model will be a parametricintensity function (e.g., with location, amplitude, scale, and shape parameters), sometimes in a superposition of multiple components (e.g., stars in an image, pulses in a transient burst, or population components). To model discrepancy from the salient model, a natural choice with appealing theoretical properties is a multiplicative gamma discrepancy process; composition with the Poisson point process leads to an overall negative binomial point process for the observations.  The team will implement this approach, and generalize it in several directions. For demographic models, the discrepancy models will be embedded in a hierarchical model that accounts for measurement error and selection effects.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015195","Learning from Hidden Signatures in High-Dimensional Models","DMS","STATISTICS","07/01/2020","06/20/2020","Florentina Bunea","NY","Cornell University","Standard Grant","Yong Zeng","06/30/2023","$300,000.00","Marten Wegkamp","fb238@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","079Z","$0.00","Methods directly applicable to the determination of regulatory cell mechanisms associated with vaccine responses will be developed as an integral part of this project.  Such mechanisms are not directly observable or measurable, but a wealth of indirect measurements can be obtained, for instance on antibody titer-driven effects. New methodology, that meets the challenge of extracting these hidden signals from a very large volume of data, consisting of a diverse arrays of indirect measurements, are developed and  put to test immediately, for emerging pandemics data sets. More broadly,  techniques that allow for reliable, mathematically grounded, inference and prediction from essential, but hidden,  signatures will be developed and applied to data sets arising from  neuroscience and high-throughput text data. <br/><br/>A foundational study of inference and prediction from high-dimensional data with low-dimensional  embeddings modeled by factor models are undertaken in this project. Within new classes of  identifiable factor regression models in which the latent factors are interpretable, new scalable methods for estimating the regression coefficients of the latent factors  will be developed. Optimality of estimation from a finite-sample, minimax perspective, as well as the derivation of  the asymptotic limit of the estimates, and especially of their efficient asymptotic variance will be a cornerstone of research under this project.  Prediction from high-dimensional dependent features, with reduced-effective-rank covariance matrix, will be analyzed under generic factor regression models. In particular, interpolating predictors, popular in deep-learning,  will be contrasted with other contenders, with the aim of offering  fundamental understanding of model-free versus model-based prediction, when data arises from a factor regression model. Sparse topic models, together with inference and prediction from the hidden topics, will be studied as  companion models for data in which all the features are discrete. Applications of the newly developed, scalable and  theoretically founded methods  will constitute a focal point of this project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015250","Development of Methodologies to Formalize the Informal Rules of Causal Inference from Observational Studies Using Evidence Factors and Modern Optimization","DMS","STATISTICS","07/01/2020","07/07/2022","Bikram Karmakar","FL","University of Florida","Continuing Grant","Yong Zeng","06/30/2024","$147,856.00","","bkarmakar@ufl.edu","1523 UNION RD RM 207","GAINESVILLE","FL","326111941","3523923516","MPS","1269","1269, 9251","$0.00","Observational studies are relatively inexpensive, but often flawed, substitutes for randomized experiments to examine the causal effect of a treatment.  An observational study may be flawed because, before treatment, the observed treated group may not have been comparable to the untreated group, which can lead to a biased estimation of a treatment effect.  As one example, observational studies suggested hormone replacement therapy prevents heart attacks among postmenopausal women, while randomized trials showed otherwise.  Still, on multiple occasions, observational studies have provided strong statistical evidence to support implementation of an intervention, such as when observational studies provided strong evidence that smoking causes lung cancer.  In recent years, observational studies have provided evidence that teenage vaping has a serious effect on lung disease which has led to policy interventions to curb teenage vaping.  But the strength of observational study evidence is judged largely by informal/semi-formal rules.  For example, the evidence is considered stronger when a similar treatment effect is seen across many independently conducted studies.  How the rules that are used is not typically transparent during the assessment of the statistical evidence, and thus, how cautious one should be about how solid the evidence is for a causal claim is often not transparent.  This project aims to make how strong the evidence is from observational studies more transparent by developing statistical methodologies to formalize some of the existing informal rules on strengthening scientific evidence from observational studies.  To increase their accessibility, the PI, with help from a graduate student, will also incorporate, through software, lessons and projects, these methods in courses taught to graduate students from different empirical fields. <br/><br/>This project will develop several methods for expanding the scope of use of evidence factors in observational study designs. An evidence factors analysis builds statistically independent pieces of evidence (called evidence factors) which, if vulnerable, are vulnerable differently to potential biases. The PI will develop methodologies for evidence factors analysis in novel study designs, such as event studies.  The quality of a design and an analysis of a study will be evaluated by statistical power and design sensitivity.  The scope of evidence factors is limited if considered only under existing study designs.  This grant has the long-term goal of developing new and improved observational study designs which incorporate evidence factors analysis.  Construction of these designs typically requires solving NP-hard problems.  For example, evidence factors can be built in stratified designs, but creating such a design, while controlling for many confounders, requires solving an NP-hard graph partitioning problem. The PI will develop approximation algorithms to solve these design problems using discrete and combinatorial optimization methods.  These algorithms will likely also appeal to the applied mathematics community.  This project will also develop evidence factors analysis for robust inference in composite studies which combine, in one design and analysis, aspects of different studies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015339","Collaborative Research: Transfer Learning for Large-Scale Inference: General Framework and Data-Driven Algorithms","DMS","STATISTICS","07/01/2020","01/27/2022","Wenguang Sun","CA","University of Southern California","Standard Grant","Yong Zeng","06/30/2023","$120,000.00","","wenguans@marshall.usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","MPS","1269","079Z","$0.00","Transfer learning provides crucial techniques for utilizing data from related studies that are conducted under different contexts or on diverse populations. It is an important topic with a wide range of applications in integrative genomics, neuroimaging, computer vision and signal processing. This research work will provide new tools to scientific researchers who routinely collect and analyze high dimensional and complex data across different sources and platforms. This project aims to develop new analytical tools to improve conventional methods by delivering more informative and interpretable scientific findings. The developed transfer learning algorithms, which can reliably extract and combine knowledge from diverse data types and across different studies, will help address important issues from genomics applications. User-friendly software packages will be developed and made publicly available. Scientific researchers can use the tools to translate dispersed and heterogeneous data sources into new knowledge and medical benefits. This will help improve the understanding of the role of various genetic factors in complex diseases, and accelerate the development of new medicines and treatments in a cost-effective way. <br/><br/>Transfer learning for large-scale inference aims to extract and transfer the knowledge learned from related source domains to assist the simultaneous inference of thousands or even millions of parameters in the target domain. We aim to develop a general framework to gain understanding of the benefits and caveats of transfer learning in a wide range of large-scale inference problems including sparse estimation, false discovery rate analysis, sparse linear discriminant analysis and high-dimensional regression. Our research addresses two key issues in transfer learning: (a) What should be transferred? (b) How to transfer and prevent negative learning? We aim to pursue three major research goals. The first is to develop a class of computationally efficient and robust transfer learning algorithms for high-dimensional sparse inference. The general strategy is to first learning the local sparsity structure of the high-dimensional object through auxiliary data and then apply the structural knowledge to the target domain by adaptively placing differential weights or setting varied thresholds on corresponding coordinates. The second is to formalize a decision-theoretic framework for high-dimensional transfer learning that is applicable across the sparse and non-sparse regimes. Along this direction, we aim to develop a class of kernelized nonparametric empirical Bayes methods for data-sharing shrinkage estimation and multiple testing. The third is to address the urgent needs and new challenges arising from important genomics applications using the newly developed methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015341","Understanding Complexity and the Bias-Variance Tradeoff in High Dimensions: Theory and Data Evidence","DMS","STATISTICS","07/01/2020","06/16/2020","Bin Yu","CA","University of California-Berkeley","Standard Grant","Yong Zeng","06/30/2024","$300,000.00","","binyu@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","079Z","$0.00","The past decade has witnessed a significant rise in the usage of very large machine-learning models in modern data problems; these models have shown success in a variety of tasks, such as image classification, language translation, and speech recognition. More recently, machine learning is entering new fields, such as robotics, autonomous driving, and medicine. However, these models are often not robust to perturbations and are vulnerable to attacks by adversaries. These shortcomings warrant an urgent and insightful understanding of the ""black-box"" nature of these models. The principal investigator plans to understand these models by characterizing their ""complexity"" in a technical manner. A new complexity measure, based on the principle of minimum description length, sheds insight into classical statistical foundations as well as informing how and when these new high-dimensional models will work. This novel complexity measure is promising to enable applications to mission-critical fields like precision medicine, where the collection of a labeled dataset is expensive, by sample-size calculations and improving model selection with limited data. This research has both theoretical and applied impacts in the fields of statistics and machine learning including deep learning. In the duration of the project, graduate students will be trained in theory, domain-driven data science, and open-source software development. The research will be further disseminated through courses, an upcoming book, and presentations at workshops and conferences.<br/><br/>Deep neural networks (DNNs) in many cases generalize well in the sense that a DNN trained on one task often performs well on similar unseen data for the same task. They can do so despite being highly overparameterized, i.e., the number of parameters is much larger than the number of training samples.  Occam's razor and the bias-variance trade-off wisdom suggest to prefer a simple model when choosing from amongst models of varying complexity with similar performance. The good performance of DNNs, despite the overparametrization, has led many researchers to question the validity of the classical statistical principle of bias-variance trade-off (and preferring a simple model) for high-dimensional settings common in modern machine learning (ML) and statistical tasks.  In this project, the principal investigator begins by reconsidering the definition of a valid complexity measure ? which forms the basis of Occam?s razor and the bias-variance trade-off principle ? for high-dimensional models. Finding one such measure for high-dimensional models has remained a difficult task.  Merely counting the number of parameters is not a valid complexity measure, especially when the number of training examples is small.  The principle of minimum description length will be used to provide a systematic approach to understanding the complexity of high-dimensional linear models, kernel methods, and finally DNNs. The complexity measure will serve as a basis for understanding key concepts such as the bias-variance trade-off and for further analysis into high-dimensional models. The theoretical results will be augmented with an extensive set of data-inspired experiments.  After establishing the bias-variance trade-off with the new complexity measures, these measures will then be investigated for (i) selecting a simple model from amongst a set of competitive models, where simple will be defined via the MDL-based complexity and not the number of parameters, and (ii) regularizing or pruning a large (pre-trained) model, for example, in a transfer learning setting with limited dataset, by trading off the training performance with the complexity of the model.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015575","The Shape of Data: Using Topology and Geometry in Statistics","DMS","STATISTICS","07/01/2020","06/16/2020","Wolfgang Polonik","CA","University of California-Davis","Standard Grant","Yong Zeng","06/30/2024","$200,000.00","","wpolonik@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","079Z","$0.00","In our modern society, technical advances constantly give rise to new data collection mechanisms that allow scientists to ask questions of ever-increasing complexities. The field of statistics is pressed to develop methodologies that can be used for answering such scientific challenges. Many of these scientific challenges can be addressed by first attempting to decrease size and complexity of the data through what is called feature extraction. In a second step these features are used to devise sound statistical methodologies. This project considers extracting information about the shape of the data, and is developing and studying statistical and machine learning methodologies based on these features. The outcomes of this project are expected to impact the fields of statistics, machine learning and various fields of application. Existing collaborations of the PI with biologists, animal scientists and material scientists, will also facilitate dissemination to these fields. This project will also provide training opportunities for both graduate and undergraduate students. <br/><br/>The contributions of this project will lie in the intersection of statistics and machine learning. They will enhance toolboxes and advance knowledge through the derivation of novel methodologies, through developing a deep understanding of these methodologies by performing relevant theoretical analyses, and by providing implementations of the methodologies that are useful for practical purposes. More specifically, this project will (i) study the role of the Hodge Laplacian in the context of analyzing higher-relational data, including its role in clustering; (ii) derive theoretical results about stabilizing statistics such as Betti-curves that allow the construction  of asymptotically exact bootstrap based confidence intervals for functionals of persistent Betti-curves;  (iii) develop a novel, statistically enhanced persistence diagram thought the use of multiple testing, and (iv) Implementations will allow the exploration of finite sample properties of the new methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015285","Complexity of High-Dimensional Statistical Models: An Information-Based Approach","DMS","STATISTICS","09/01/2020","08/19/2022","Ming Yuan","NY","Columbia University","Continuing Grant","Yong Zeng","08/31/2024","$300,000.00","","ming.yuan@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","","$0.00","With big data come bigger goals ? many modern statistical applications not only involve large datasets that may offer new insights but also require deciphering highly intricate relationships among a large number of variables, often characterized by very complex and high-dimensional functions. As more data are acquired, these functions inevitably become more complex. Oftentimes the most fundamental challenge for these applications is how to quantify the complexity of such tasks and learn these functions from data in an efficient way, both statistically and computationally. Despite impressive progress made in recent years, the current approach towards this goal is limited by the discrete nature of the classical notion of computational complexity and is not suitable for statistical problems that are continuous. This project aims to develop an information-based approach that better accounts for both statistical and computational efficiencies. This new framework of complexity is expected to offer insights into the potential trade-off between statistical and computational efficiencies and to reveal the role of experimental design in alleviating computational burden. The project provides training for graduate students through involvement in the research.<br/><br/>Traditional nonparametric techniques based solely on smoothness are known to suffer from the so-called ""curse of dimensionality."" But in many scientific and engineering applications, the underlying high-dimensional object may have additional structures which, if appropriately accounted for, could help lift this barrier and allow for efficient methods to handle it. This project aims to develop a coherent framework to quantify the complexity of high dimensional models that appropriately accounts for both statistical accuracy and computational cost and helps better understand the role of these additional structures. The project will use this notion of complexity to examine several common yet notoriously difficult high-dimensional nonparametric regression problems: one based solely on smoothness, another based on smoothness and sparsity, and finally, one based on low-rank tensors. The exercise is designed to reveal interesting relationships between statistical and computational aspects of these problems and lead to the development of novel and optimal sampling and estimation strategies. The research will develop the new framework of complexity in more general statistical contexts as well and investigate its role in characterizing statistically and computationally optimal inference schemes. This will be achieved by developing new statistical methods and computational algorithms, theoretical study of their performance and fundamental limits, and the development of related mathematical tools and computational software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1944740","CAREER: Statistical Inference for Bayesian Machine Learning","DMS","STATISTICS","07/01/2020","07/13/2023","Veronika Rockova","IL","University of Chicago","Continuing Grant","Yong Zeng","06/30/2025","$357,173.00","","veronika.rockova@chicagobooth.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","1045","$0.00","With visible successes on a broad range of predictive problems, the role of machine learning (ML) has become increasingly recognized across a wide array of application domains ranging from economics to electronic commerce. In medicine, for instance, machine learning is routinely deployed  for image segmentation, registration, computer aided detection and diagnosis, brain function or activity analysis from fMR images, and text analysis of radiology reports using natural language processing. In spite of this nascent ML trend, there has been significant reluctance to delegate decision making entirely to machine intelligence. This has been largely due to  the absence of a formal statistical framework for  uncertainty quantification  and  interpretability. This yawning gap between theory and practice presents new exciting research opportunities for theoretical developments that will justify and unleash the potential machine-assisted decision making in real life. This project has two broad objectives. The first one is motivated by the currently unmet demand for theoretical justification of widely used Bayesian machine learning tools. The second objective is developing practicable methodology for interpretable machine learning, which is essential for gleaning insights into the behavior of real-world processes.  The research outlined in this project will bridge current conceptual divides between statistics and machine learning by solidifying Bayesian machine-assisted inference as statistically valid so that it can be safely used to tackle complex scientific problems arising in data-rich environments including imaging, personalized medicine, business analytics, marketing and economics. <br/><br/><br/>There has been a growing realization of the potential of Bayesian machine learning as a platform that can provide both flexible modeling, accurate predictions as well as coherent uncertainty statements. In particular, Bayesian Additive Regression Trees (BART) has emerged as one of today's most effective machine learning methods under minimal assumptions. BART has already proved itself to be broadly effective at unveiling structure hidden in high dimensional data across a wide variety of contemporary applications. Its theoretical properties for statistical inference, however, have remained unknown. The detailed research agenda of the first three goals aims at obtaining an in-depth theoretical understanding of BART (as well as some aspects of Bayesian deep learning) through the investigation of (1) uncertainty quantification and confidence set constructions, (2) adaptability to spatially inhomogeneous objects, (3) asymptotic normality for causal inference. The completion of these objectives will significantly advance the current frontier of semi-parametric and non-parametric Bayesian theory. The principal investigator will develop new scalable tools for interpretable machine learning which will extend the reach of ML to many new application areas and problem types involving big data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1952486","FRG: Collaborative Research: Quantile-Based Modeling for Large-Scale Heterogeneous Data","DMS","STATISTICS","06/01/2020","03/27/2020","Qi Zheng","KY","University of Louisville Research Foundation Inc","Standard Grant","Yong Zeng","05/31/2024","$150,000.00","","qi.zheng@louisville.edu","2301 S 3RD ST","LOUISVILLE","KY","402081838","5028523788","MPS","1269","079Z, 1616, 9150","$0.00","The rapid development of technology has led to the tremendous growth of large-scale heterogeneous data in science, economics, engineering, healthcare, and many other disciplines. For example, in a modern health information system, electronic health records routinely collect a large amount of information on many patients from heterogeneous populations across different disease categories. Such data provide unique opportunities to understand the association between features and outcomes across different subpopulations. Existing approaches have not fully addressed the formidable computational and statistical challenges. To tap into the true potential of information-rich data, this project will develop a new computational and statistical paradigm and solid theoretical foundation for analyzing large-scale heterogeneous data. In addition the project will also provide research training opportunities for graduate students.<br/> <br/>The project will build a unified, quantile-modeling based framework with an overarching goal of achieving effectiveness and reliability in analyzing heterogeneous data, especially when both the number of potential explanatory variables and the sample size are large. The specific goals are (1) to develop resampling-based inference for large-scale heterogeneous data; (2) to develop Bayesian algorithms and scalable and interpretable structure-aware approach for better inference; (3) to develop quantile-optimal decision rule estimation and inference with many covariates; (4) to develop novel estimation and inference procedure for large-scale quantile regression under censoring. The project will address some of the key barriers in scalability to data size and dimensionality, exploration of heterogeneity and structures, need for robustness, and the ability to make use of incomplete observations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1943500","CAREER: Flexible and Efficient Exploration of the Bayesian Framework for High Dimensional Modeling","DMS","STATISTICS","06/01/2020","06/07/2023","Naveen Naidu Narisetty","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Yong Zeng","05/31/2025","$315,972.00","","naveen@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","1045","$0.00","The modern era of Big Data brings unique opportunities as well as challenges to the statistician. While the Big Data revolution brings a great opportunity to obtain valuable and profound insights from the richness of data and to enhance data-driven decision making, it also brings challenging demands for innovation and knowledge discovery in three crucial aspects from statisticians and data scientists: (i) development of flexible models that can appropriately describe the complexities of the data (ii) efficient and valid statistical estimation and inferential procedures, and (iii) development of computational algorithms that scale-up to large datasets. The purpose of this project is to make advances in all the three aspects by fully exploring the Bayesian framework, which treats the parameters of a model to be random and provides an efficient mechanism to quantify the uncertainty of the model parameters. In particular, the techniques developed will be useful for analyzing datasets containing a large number of covariates, for learning the dependence structures between a large number of outcome variables, and for obtaining a comprehensive description of the impact of covariates on outcome variables by modeling their relationships at different quantile levels. The research developed will have impact on statistical practice in various disciplines including biology, economics, environmental sciences, marketing, and medical sciences. The training component will integrate research into teaching by offering special topics courses to graduate students based on the proposed research and by developing undergraduate research projects that incorporate research concepts at an accessible level. The PI will mentor high school research projects and organize a K-12 outreach workshop to provide exposure to modern statistics and its applications to high school students and teachers. <br/><br/>Statistically rigorous and computationally efficient Bayesian methodologies and inferential procedures will be developed which will be applicable for a variety of complex high dimensional models including generalized linear models, quantile regression models, and graphical models. General classes of Bayesian regularization priors will be proposed, and their regularization properties will be rigorously studied for a variety of commonly used likelihood functions. In contrast to most of the existing Bayesian approaches that focus on high dimensional estimation, a novel Bayesian framework for performing high dimensional Bayesian inference having valid frequentist properties will be developed. Scalable computational techniques that do not involve large matrix operations for obtaining point estimators from the posteriors as well as for sampling the full posterior distributions will be devised and their statistical properties will be studied. An attractive feature of the computational developments will be that they will be applicable to a diverse range of statistical models commonly used in practice. The research developed will be closely related to several highly active areas of modern statistics including high dimensional modeling, Bayesian computation, nonconvex regularization, post-selection inference, graphical models, and quantile regression, and will contribute to the advancement of and interaction between these areas.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2054808","Deep Learning and Random Forests for High-Dimensional Regression","DMS","STATISTICS","09/01/2020","08/03/2021","Jason Klusowski","NJ","Princeton University","Continuing Grant","Yong Zeng","07/31/2023","$157,960.00","","jason.klusowski@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","075Z, 079Z","$0.00","This project aims to investigate two of the most widely used and state-of-the-art methods for high-dimensional regression: deep neural networks and random forests. Despite their widespread implementation, pinning down their theoretical properties has eluded researchers until recently. The proposed research aims to add to the growing body of literature on their analysis, by both developing tools of theoretical value and providing guarantees and guidance for practitioners and applied scientists who use these popular methods frequently in their work.<br/><br/>The success of multi-layer networks has largely been buoyed by their ability to generalize well despite being able to fit most datasets, given enough parameters. This phenomenon is particularly striking when the input dimension is far greater than the available sample size, as is the case with many modern applications in molecular biology, medical imaging, and astrophysics, to name a few. A major component of the proposed work will be to obtain complexity bounds for classes of deep neural networks with controls on the size of their weights, which can then be used to bound generalization error and statistical risk. These complexity bounds reveal the role of complexity penalization, which is based on certain norms of the weights of the network. Motivated by these observations, another stream of the proposed research seeks to provide statistical guarantees of certain complexity penalized estimators and their adaptive properties. Current theoretical results for random forests are either for stylized versions of those that are used in practice or are asymptotic in nature and it is therefore difficult to determine the quality of convergence as a function of the parameters of the random forest. Furthermore, the setting for the analysis of more practical implementations of random forests is limited to structured, fixed-dimensional regression function classes. Given these restrictions, the first component of the proposal aims to investigate how random forests behave in the high-dimensional regime when the number of predictors grows with the sample size. Another research objective is to isolate and study families of flexible high-dimensional regression functions for which finite sample convergence rates can be established. The final endeavor of this project is to connect popular measures of variable importance  to the bias of random forests. Since variable importance measures are used for assessing the role each predictor variable plays in influencing the output, this connection will partially explain why random forests are adaptive to sparsity. The relationship will also help to theoretically motivate variable importance measures as useful tools for model interpretability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015291","Statistical Estimation from Decoupled Data","DMS","STATISTICS","07/01/2020","06/30/2022","Jonathan Niles-Weed","NY","New York University","Continuing Grant","Yong Zeng","06/30/2023","$249,999.00","","jdw453@nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1269","","$0.00","Modern statistics is defined by the fact that a great deal more data is available to practitioners than ever before. This is particularly the case in the sciences, where advances in experimental methodology across fields such as biology, chemistry, and physics have led to an explosion of different types of data, collected by different measurement apparatuses at potentially different times. Moreover, it may be difficult or impossible to connect data points from different experiments. For example, a chemist may apply two different measurement techniques to the same batch of molecules to obtain high-quality data about the whole batch, but it may be challenging to track the identities of particular molecules between measurements. The statistician who wishes to make the best possible inferences is faced with the difficult problem of how to integrate the data from different sources to conduct a unified analysis. Despite the ubiquity of this problem, rigorous statistical analyses of procedures designed to work with decoupled data are rare. The main goal of this project is to develop new tools for performing estimation tasks with decoupled data and to establish the fundamental limits of such techniques. This project will have impact on scientific and statistical methodology in both research and industrial settings.The graduate student support will be used on interdisciplinary research and writing codes.<br/> <br/>The project will investigate optimal rates of estimation for regression problems given access to decoupled data, and to establish potential trade-offs. Several intermediate regimes will be considered, for example, where the experimenter has access to many independent batches of shuffled data or to data with partial coupling information. This project will quantify the statistical price for learning with decoupled data via tight minimax bounds. This research is also aimed at establishing when minimax statistical procedures can be made computationally efficient, and investigating the possible presence of information theoretic-computational gaps in optimal rates of estimation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015577","Generative Modeling with Short Run Computing","DMS","STATISTICS","07/01/2020","06/23/2020","Yingnian Wu","CA","University of California-Los Angeles","Standard Grant","Yong Zeng","06/30/2024","$200,000.00","","ywu@stat.ucla.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244201","3107940102","MPS","1269","079Z","$0.00","In our daily lives, we constantly receive a large amount of sensory data in the form of images, texts, and speech, yet we can effortlessly make sense of the data by learning, recognizing, and understanding the patterns and meanings in the data. How this is done by the brain is still largely a mystery, and this is a central problem in machine learning and artificial intelligence, which has a vast scope of applications and is transforming our lives. One way to make sense of sensory data is to construct models to generate them, by assuming that the data are generated by some relatively simple hidden factors or causes. Such models are called generative models. To make sense of the data is to infer the hidden causes that generate the input data, and this can be accomplished by short-run computing dynamics. The main goal of this project is to develop such generative models and the associated short-run computing dynamics. The project has the potential to lead to new learning techniques that can be useful in applications such as computer vision. The PI will also train graduate students supported by this grant and further enhance the graduate and undergraduate courses taught by the PI. <br/> <br/>Generative models unify supervised, unsupervised, and semi-supervised learning in a principled likelihood-based framework. While supervised learning has met tremendous successes in recent years, unsupervised and semi-supervised learning remains a challenge. The bottleneck for likelihood-based learning of generative models is the intractable computation of expectations which usually requires expensive Markov chain Monte Carlo (MCMC) sampling, whose convergence can be problematic. The main motivation of the research is to get around this bottleneck. The following are specific aims of the proposed research: (1) Variational optimization of short-run MCMC dynamics for the sampling computations in likelihood-based learning of generative models, by combining the advantages of MCMC and variational inference. (2) Developing biologically plausible generative models with multiple layers of hidden variables, and the associated short-run inference and synthesis dynamics that can account for feedbacks and inhibitions between hidden variables.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015242","Collaborative Research: Extremes in High Dimensions: Causality, Sparsity, Classification, Clustering, Learning","DMS","STATISTICS","07/01/2020","06/18/2020","Gennady Samorodnitsky","NY","Cornell University","Standard Grant","Yong Zeng","06/30/2024","$300,000.00","","gs18@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","","$0.00","In recent years, through news reports and first-hand experience, the general public has become keenly aware of extreme events, in particular, of extreme weather conditions such as extended heat waves, periods of extreme cold, an increase in the number and intensity of tornadoes and hurricanes, or periods of record precipitation resulting in unprecedented floods.  Just in the past few years, the insurance claims from extreme climatic events have been staggering, which include the Missouri River flood in April 2019 ($10.8B), Hurricane Michael in October 2018  ($25B), the California wildfires in December 2017 ($18.7B), the US drought/heatwave in 2012 ($33.9B), and Hurricane Sandy in October 2012 ($73.4B).  This list does not include non-climatic extreme events such as the financial crisis from 2008 nor the current covid-19 pandemic. Many of the extreme events experienced today that are weather, environmental, industrial, epidemiological, economic, or social media related are occurring at a more frequent rate, which often result in huge losses to our society in a variety of ways from financial to human life to our way of life. While the occurrence of extreme events is reasonably well understood in steady state situations, it has become clear that the preponderance of extremes events suggest that the steady-state assumption is no longer valid.  The key objective of this research is to try to understand causal impacts of various factors from a potentially large array of variables including changing environmental conditions, demographic movements within the US, changing landscapes, and changing economic conditions, on the frequency and magnitude of extreme events.  From many variables, we hope to produce methodology to extract the important features in the data that have a direct impact on describing and predicting extremes.  This research is potentially of use to policymakers who need to anticipate and plan for extreme events leading to sensible strategies for mitigating their impact on society. The graduate student support will be used for interdisciplinary research.<br/><br/>The principal goal of this research project is to design new tools for analyzing and modeling extremes in a myriad of situations that go well beyond the boundaries of classical extreme value theory. These include detection of often nonlinear sets of much smaller dimension that can provide an adequate description of extremes in high dimensions, for which we hope to apply the powerful modern learning techniques (such as graph-based learning methods) that allow us to determine this extremal support from the data. In general, detecting sparsity in the exponent measure describing high-dimensional extremes, i.e., locating (often numerous) low-dimensional regions which carry most of the support of exponent measure will be a key focus of this research. A second main thrust of this research centers on the issue of causality in both small and large dimensional problems. In the most basic form, a set of variables X is said to be tail causal to a dependent vector Y if certain changes in X (sometimes themselves extreme but not always so) impact the tail behavior of Y. An important setting of this type is the potential outcomes framework for causality of extreme events, which will be a major focus in this project's research agenda.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015320","Collaborative Research: Shape-Based Imputation and Estimation of Fragmented, Noisy Curves with Application to the Reconstruction of Fossil Bovid Teeth","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","09/01/2020","04/20/2023","Ofer Harel","CT","University of Connecticut","Standard Grant","Yong Zeng","02/29/2024","$269,577.00","","ofer.harel@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1253, 1269","102Z, 1515","$0.00","Statistical analysis of shape data is relevant to a wide array of fields including biology, anthropology, chemistry, and medicine, to name just a few. Current methods for analyzing shape data generally focus on shapes that are fully observed. This project will develop methods for analyzing shapes that are only partially observed. Traditional missing data techniques are not applicable in the shape setting when 1) shapes are defined by functions and 2) shape-preserving transformations (e.g. translation, rotation, scaling, re-parameterization, etc.) must be accounted for. This project will formalize and harness this perspective for data obtained from fragmentary fossil tooth images of the Family Bovidae (antelopes and buffaloes) as partially observed curves, representing the outlines of imaged objects, possibly with measurement error. The resulting taxonomic identifications will generate more robust estimates of the ecologically sensitive bovids. These improved estimates, in turn, afford novel insight into the paleoenvironmental context of early human evolution.<br/> <br/>The main goal of this project is to develop statistical methods for the analysis of partially observed shapes (i.e., fragmented curves). This goal will be accomplished via two different approaches: 1) developing computational methods for imputing a fragmented curve by matching and completing it based on a template or donor curve obtained from a sample of fully observed curves, and 2) developing Bayesian model-based methods for estimation and classification of the shape of a noisy, fragmented curve using an empirical prior on the overall shape of the curve. Both approaches will be built using Riemannian geometric tools for shape analysis that ensure proper invariance to shape preserving transformations including translation, scaling (when appropriate), rotation, and re-parameterization. These methods will be developed with the motivating application of the analysis and taxonomic classification of fractured and complete fossil teeth from the Family Bovidae. By generating a proxy for paleoenvironmental conditions, the project seeks to advance our understanding of the relationship between environmental change and hominin evolution.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015397","Generative Models for Complex Data: Inference, Sensing, and Repair","DMS","STATISTICS","07/01/2020","06/15/2020","John Lafferty","CT","Yale University","Standard Grant","Yong Zeng","06/30/2023","$250,000.00","","john.lafferty@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","079Z","$0.00","The research in this project lies at the boundary of statistics and machine learning, and is focused on studying new families of statistical models. A generative model is an algorithm that transforms random inputs into synthesized data to mimic data found in a naturally occurring dataset, such as a database of images. The research will explore theory, algorithms, and applications of generative models to gain insight into phenomena observed in practice but poorly understood in terms of mathematical principles. The work will also pursue new applications of generative models in computational neuroscience, at scales from the cellular level to the macro level of human cognition. Anticipated outcomes of the research include development of software that implements new methodology, training of graduate students across traditional disciplines, and the introduction of modern statistics and machine learning to undergraduates through research projects based on this work.<br/><br/>The technical objectives of the project include four interrelated aims. First is to investigate the statistical properties of variational programs that are widely used in deep learning, and to develop new approaches to building generative models for novel data types. The second aim is to explore new algorithms to solve inverse problems based on generative models. Third, a new form of robust estimation will be studied where a model is corrupted after it has been constructed on data. Model repair is motivated from the fact that increasingly large statistical models, including neural networks, are being embedded in systems that may be subject to failure. Finally, the project will develop applications of generative modeling and inversion algorithms for modeling brain imaging data, including the use of simultaneous recordings in different modalities.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1941945","CAREER: High-Dimensional M-Estimation Under Nonstandard Conditions","DMS","STATISTICS","07/15/2020","06/14/2023","Yang Ning","NY","Cornell University","Continuing Grant","Yong Zeng","06/30/2025","$324,477.00","","yn265@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","1045","$0.00","Modern technology in genomics, medical research and neuroscience generates enormous amounts of data, which calls for reliable statistical analysis tools.  While the past decades have witnessed a surge of research activities on the analysis of big data in statistics and data science, the existing statistical tools are not adequate to produce reliable results due to the complexity of the data structure or the manner in which the data are collected in modern applications. The project will develop novel statistical and computational tools to address the emerging challenges in modern big data and implement them within software packages. The project will benefit a broad range of researchers including biologists, epidemiologists, medical doctors and neuroscientists. The research is complemented by an equally important education and outreach plan including designing new undergraduate and graduate courses and recruiting underrepresented minorities into the summer research program. <br/><br/> <br/>This project will develop a novel computational and statistical framework for high-dimensional M-estimation under two types of nonstandard conditions. In the first project, the principal investigator will consider high-dimensional M-estimation with non-smooth loss functions (e.g. indicator function). The discontinuity of the loss function leads to nonstandard theory and requires new statistical methods equipped with more refined theoretical analysis. In the second project, the principal investigator will consider M-estimation subject to measurement constraints in the sense that the outcomes are only collected in a very small subset of a big dataset. The principal investigator will develop scalable computational algorithms and statistically valid estimation/inference procedures.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1952306","FRG: Collaborative Research: Quantile-Based Modeling for Large-Scale Heterogeneous Data","DMS","STATISTICS","06/01/2020","03/27/2020","Kengo Kato","NY","Cornell University","Standard Grant","Yong Zeng","05/31/2024","$150,000.00","","kk976@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","079Z, 1616","$0.00","The rapid development of technology has led to the tremendous growth of large-scale heterogeneous data in science, economics, engineering, healthcare, and many other disciplines. For example, in a modern health information system, electronic health records routinely collect a large amount of information on many patients from heterogeneous populations across different disease categories. Such data provide unique opportunities to understand the association between features and outcomes across different subpopulations. Existing approaches have not fully addressed the formidable computational and statistical challenges. To tap into the true potential of information-rich data, this project will develop a new computational and statistical paradigm and solid theoretical foundation for analyzing large-scale heterogeneous data. In addition the project will also provide research training opportunities for graduate students.<br/> <br/>The project will build a unified, quantile-modeling based framework with an overarching goal of achieving effectiveness and reliability in analyzing heterogeneous data, especially when both the number of potential explanatory variables and the sample size are large. The specific goals are (1) to develop resampling-based inference for large-scale heterogeneous data; (2) to develop Bayesian algorithms and scalable and interpretable structure-aware approach for better inference; (3) to develop quantile-optimal decision rule estimation and inference with many covariates; (4) to develop novel estimation and inference procedure for large-scale quantile regression under censoring. The project will address some of the key barriers in scalability to data size and dimensionality, exploration of heterogeneity and structures, need for robustness, and the ability to make use of incomplete observations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015428","Nonparametric Bayesian Methods for Joint Analysis of Recurrent Events and Survival Time","DMS","STATISTICS","08/01/2020","08/03/2020","Ju Hee Lee","CA","University of California-Santa Cruz","Standard Grant","Yong Zeng","07/31/2024","$124,945.00","Athanasios Kottas","juheelee@soe.ucsc.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1269","","$0.00","This research project will develop flexible statistical models for joint analysis of recurrent events and survival time. In long-term, follow-up studies, subjects may experience a recurrent event at multiple times, where the observation of subject-specific recurrent events is terminated by end of  the study, or a terminal event such as death. Scientifically relevant problems involving such data structures are prevalent in the biomedical sciences, as well as in econometrics and engineering. A critical issue in analyzing such data is that the history of the recurrent events and the risk of terminal event are interrelated. It is thus important to jointly model the underlying stochastic mechanisms, typically, in the presence of predictor variables that are expected to affect the occurrence of recurrent events and the survival time. A key objective of this project is to expand the inferential and predictive scope of existing techniques for joint analysis of recurrent events and survival time by developing novel statistical models that relax restrictive assumptions of state-of-the-art methods. To facilitate use of the methods by researchers and practitioners, publicly available software will be developed for implementing several of the statistical models. The project will create educational and research training opportunities for graduate students and seek to foster the participation of women and underrepresented groups in the research.<br/><br/><br/>This research project will develop general Bayesian modeling approaches for joint analysis of recurrent events and survival time. The modeling framework builds from Bayesian nonparametric mixtures of Erlang distributions for the survival responses, with covariate effects accommodated more flexibly than proportional hazard regression models. Different classes of joint models will be formulated by combining the nonparametric survival regression modeling methods with parametric models for the covariate-dependent recurrent event point process intensities. The joint models will capture general dependence between the recurrent event and survival processes, while allowing for heterogeneity between subjects. The primary objective is to develop a comprehensive joint modeling framework that significantly improves on model fit and predictive performance relative to the state-of-the-art shared frailty modeling methods. In the context of regression modeling for survival responses, the project will also expand the methodology in the burgeoning field of Bayesian nonparametrics. The research project has a substantial analytic component with regards to study of theoretical properties for the various models, as well as a significant computational component with regards to achieving computationally tractable model fitting. The practical utility of the new methods will be investigated with simulation studies and through applications involving analysis of data from cancer patients.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015517","Inference in High-Dimensional Statistical Models: Algorithmic Tractability and Computational Barriers","DMS","STATISTICS","09/01/2020","06/17/2020","David Gamarnik","MA","Massachusetts Institute of Technology","Standard Grant","Yong Zeng","08/31/2023","$200,000.00","","gamarnik@mit.edu","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","MPS","1269","079Z","$0.00","Extracting knowledge from data using statistical and machine learning methods often involves computations, which don't scale well with dataset sizes. This is dictated by the necessity of analyzing large scale statistical models, where the scale of the data ever increases due to our unprecedented ability to accumulative massive amounts of it. Often this leads to models where the number of parameters far exceeds the amount of collected data,  rendering many classical inference models ill-posed and classical computational methods prohibitively time consuming. Thus the value brought about by the abundance of data comes at the expense of the necessity to develop completely novel computational tools that are capable of dealing with the curse of dimensionality. While there is  an abundance  of literature devoted to designing efficient computational methods of inference in high-dimensional statistical models, it was discovered that many algorithms hit a certain computational barrier, beyond which seemingly only brute-force and thus computationally prohibitive algorithms can succeed. Not much is known regarding the fundamental computational limitations arising above this barrier, which is  popularly dubbed  the nformation Theoretic vs Computation gap. What is the origin of this barrier? Does it indeed correspond to the onset of algorithmically intractable problems, or is it just a matter of being more clever about designing faster algorithms? The project also provides research training opportunities for graduate students. <br/><br/>In the present project the PI develops a completely novel approach for understanding fundamental computational barriers arising in high dimensional statistical models. The approach  is based on powerful and illuminating insights derived from the field of statistical physics,  specifically the theory of spin glasses. In particular, the PI intends to establish that the onset of the algorithmic barriers is caused by phase transition in the landscape of the solution space, marking a drastic change in the solution space geometry of  underlying inference problems. This change in geometry of the solution space landscape taking the form of the so-called Overlap Gap Property (OGP), can further be used to rule out broad classes of algorithms as potential contenders to bridge the information theoretic and algorithmic gap. These classes of algorithms include  algorithms based on local improvements, such as  Gradient Descent and Stochastic Gradient Descend algorithms, algorithms based on Markov Chain Monte Carlo Method, algorithms broadly defined as Approximate Message Passing iterations, and algorithms based on constructing low-degree polynomials. The PI in particular intends to investigate the validity of a bold conjecture stating that for most, if not all of the known  models exhibiting apparent algorithmic barriers, the onset of this barrier coincides with the onset of the OGP. The PI intends to investigate this conjecture in the context of several widely studied modern models of high dimensional statistics and machine learning fields, including the Stochastic Block Model, the Spiked Tensor Model, and Wide Neural Networks model. All of these models are known to exhibit an apparent algorithmic hardness in some parameter regimes and thus these models offer a valuable framework for investigating the validity of the aforementioned conjecture, as well as algorithmic intractability implications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1951980","FRG: Collaborative Research: Quantile-Based Modeling for Large-Scale Heterogeneous Data","DMS","STATISTICS","06/01/2020","06/16/2023","Snigdha Panigrahi","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yong Zeng","05/31/2024","$300,000.00","Snigdha Panigrahi","psnigdha@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","079Z, 1616","$0.00","The rapid development of technology has led to the tremendous growth of large-scale heterogeneous data in science, economics, engineering, healthcare, and many other disciplines. For example, in a modern health information system, electronic health records routinely collect a large amount of information on many patients from heterogeneous populations across different disease categories. Such data provide unique opportunities to understand the association between features and outcomes across different subpopulations. Existing approaches have not fully addressed the formidable computational and statistical challenges. To tap into the true potential of information-rich data, this project will develop a new computational and statistical paradigm and solid theoretical foundation for analyzing large-scale heterogeneous data. In addition the project will also provide research training opportunities for graduate students.<br/> <br/>The project will build a unified, quantile-modeling based framework with an overarching goal of achieving effectiveness and reliability in analyzing heterogeneous data, especially when both the number of potential explanatory variables and the sample size are large. The specific goals are (1) to develop resampling-based inference for large-scale heterogeneous data; (2) to develop Bayesian algorithms and scalable and interpretable structure-aware approach for better inference; (3) to develop quantile-optimal decision rule estimation and inference with many covariates; (4) to develop novel estimation and inference procedure for large-scale quantile regression under censoring. The project will address some of the key barriers in scalability to data size and dimensionality, exploration of heterogeneity and structures, need for robustness, and the ability to make use of incomplete observations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1945428","CAREER: Post-Differentiation Inference","DMS","STATISTICS","06/01/2020","06/09/2023","Pierre Bellec","NJ","Rutgers University New Brunswick","Continuing Grant","Yong Zeng","05/31/2025","$318,259.00","","pcb71@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","1045","$0.00","Structured, high-dimensional regression problems is a current topic of major interest due recent applications in modern scientific fields.  Domain experts throughout the sciences (e.g., quantum physicists or data-scientists in bioinformatics) are now equipped with powerful regularized algorithms that leverage the structures hidden in this high-dimensional data.  This regularization enables prediction and estimation of unknowns where classical statistical methods are inefficient. On the other hand, these newly developed regularized algorithms often lack automatic inference capabilities in the form of confidence intervals.  The project's goal is to develop automatic confidence intervals on top of differentiable regularized estimators, with no or little constraint on the type of regularization used. This goal is algorithmic-centric: Develop methodologies and software that, given a differentiable regularized estimator chosen and favored by a domain expert, empower that estimator with confidence intervals comparable to those available in classical statistics.  The training component includes graduate and undergraduate course work, graduate student mentorship, and participation of the Rutgers REU program.<br/><br/><br/>The common regularization techniques that leverage structures in high-dimensional data incur a bias that is incompatible with the confidence intervals provided by classical statistical theory. Given a regularized estimator, the goal of the statistician is to provide a valid confidence interval, for instance by removing this bias and standardizing the variance.  Such inference scheme is currently only possible for a few specific regularized estimators, and one goal of the project is to extend such capability to almost any differentiable estimator. The motivation is that, if the estimator of interest is a differential function of the data, gradients of that estimator with respect to the observed data provide rich information that can be leveraged to construct valid confidence intervals in high-dimensional models where the literature has, so far, focused on prediction.  The challenges related to this approach span Statistics, Probability Theory, and Computer Science: The project aims to develop new, flexible asymptotic normality results for provable Type I error control, to understand new relationships between gradients and bias and between gradients and variance, to explain the role of degrees-of-freedom or their proxies in high-dimensions, and to develop algorithms and software that efficiently compute the gradients required for inference.  This Post-Differentiation Inference approach would empower Domain Experts throughout the sciences by providing uncertainty quantification on top of field-specific estimators, for instance in quantum or genomic applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1945266","CAREER: Online Multiple Hypothesis Testing: A Comprehensive Treatment","DMS","STATISTICS","07/01/2020","07/17/2023","Aaditya Ramdas","PA","Carnegie-Mellon University","Continuing Grant","Yong Zeng","06/30/2025","$364,125.00","","aramdas@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","1045","$0.00","It is common in the technological and pharmaceutical industries to test a large sequences of hypotheses over time. As an example in the latter case, suppose a lab is trying to develop a cure for a disease like Alzheimer's. This is a complex disease for which it is unlikely to find a single cure that works for everyone. It is much more likely that research on the drug will continue for years, if not decades, and every few months a new drug may be tested for its efficacy using a clinical trial. When we are testing whether a particular drug is any better than a placebo, we have no idea how many more drugs (hypotheses) we will test in the future, but we do know the results of the earlier tests. This is the setup considered by online multiple hypothesis testing, the topic of this project --- a large sequence of hypotheses are tested over time in an online fashion, and we would like to ensure that there are not too many false discoveries in this process just due to chance. A false discovery results not just in false hopes, but in millions of wasted dollars in follow up clinical trials, and possibly worse outcomes for patients. This project aims to develop novel methodology to test such a sequence of hypotheses so that certain common error metrics are controlled at any time. The training component for undergraduate and graduate students will prepare new researchers with inter-disciplinary education via the planned cross-disciplinary tutorials/workshops, and outreach to K-12 students.<br/><br/><br/>The methodology in offline multiple testing is rich, with a plethora of methods that control a wide variety of error metrics, and in fact the PI has contributed significantly to the literature recently. In contrast, the online multiple testing literature is less developed. This grant takes a holistic and comprehensive approach, that will result in new methods for a whole spectrum of error metrics: global null testing, family wise error rate, false discovery rate, false coverage rate, and simultaneous control of the false discovery proportion. The PI already has preliminary work on some of these fronts. We will also develop a public software package in R along with associated documentation to enable the easier assimilation and application of these methods. All methods will be accompanied by rigorous theoretical guarantees, is would be desirable in the aforementioned pharmaceutical application.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015528","Collaborative Research: Bayesian and Semi-Bayesian Methods for Detecting Relationships in High Dimensions","DMS","STATISTICS","08/15/2020","05/24/2022","Ray Bai","SC","University of South Carolina at Columbia","Standard Grant","Yong Zeng","05/31/2024","$99,562.00","","rbai@mailbox.sc.edu","1600 HAMPTON ST # 414","COLUMBIA","SC","292083403","8037777093","MPS","1269","9150","$0.00","In this big-data era, massive data sets are being generated routinely and we are seeing a growing need for powerful, reliable, and interpretable statistical learning tools to help understand these data.  The main ideas and approaches in this projectl focus on developing effective statistical learning tools to learn about complex and heterogeneous structures, such as those changing in time or varying among different groups of individuals, in high-dimensions. The activities will have a significant impact on high dimensional Bayesian analysis and modeling of nonlinear relationships. While most current efforts for high-dimensional Bayesian analyses have been focused on linear models, this project focuses on two ways of generalizing standard linear models to meet certain practical challenges: one is a generalized form of mixture modeling, termed as individualized variable selection, which enables each individual observation to have its own set of dependent variables through the employment of neuronized priors. Another extension is the Bayesian inference of index models that form a mixture structure. The project will lead to useful tools (or customized software) for discovering interpretable nonlinear and interactive patterns among a large number of potential variables. Various aspects of statistical modeling, design, and learning strategies integrated in our algorithms are  broadly applicable to problems involving  signal discovery in complex systems and high-dimensional data.  The project will also provide both educational and interdisciplinary research opportunities for graduate students, and will result in software useful to biomedical researchers, economists, social scientists, and many other practitioners. <br/><br/>In a vast number of regression problems, especially under high-dimensional settings, the structure of the association between covariates in hand and the target quantity of interest might be heterogeneous over observations, which calls for effective methods to detect such non-trivial structures. Standard procedures, including traditional variable selections, commonly overlook the existence of interplays of these heterogeneous factors. This research project aims to develop statistical procedures that identify the complicated relationship between response Y and a set of covariates X in flexible and computationally efficient ways.  Project 1 focuses on Bayesian individualized variable selection (BIVS), which generalizes standard linear regression models to quantify  heterogeneous effects among individual observations that differ in their  dependent variables with different magnitudes. The PIs will investigate its theoretical properties, including model selection consistency and its robustness when the model assumption is violated. Project 2 is devoted to the development of an efficient Bayesian method to infer the semi-parametric relationship between the response and covariates through general index models. The PIs will explore its computational feasibility and theoretical properties such as the posterior contraction rate on the estimation of the sufficient dimension reduction space. Project 3 focuses on a fast tuning parameter selection procedure by employing a generative process via neural networks. By using this procedure,  the cross-validation can be efficiently implemented for general models, such as the BIVS and Bayesian index models,  regularized variable selection, and nonparametric function estimation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015366","Representation and Subspace Learning for Decentralized and Dependent Data","DMS","STATISTICS","07/01/2020","03/30/2023","Ziwei Zhu","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Pena Edsel","09/30/2022","$129,473.00","","ziweiz@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","079Z","$0.00","Learning concise and informative representations of high-dimensional data is a precursor to the success of modern data analytics. However, recent years have witnessed many non-standard data regimes that impose unprecedented challenges for representation learning. The first scenario is that data are decentralized, that is, they are scattered across different places across which the communication is highly restricted. This is common for international companies that collect data worldwide, but cannot aggregate them due to constraints on network bandwidth or legal policies. The second scenario is that data exhibit significant temporal dependence, as seen in stock prices, traffic flow, and clinical trials. This project will develop novel statistical methods with theoretical guarantees to handle these modern data regimes. It also aims to train the next generation of data scientists under these important problem setups. <br/><br/>The principal investigator (PI) will develop novel methods and theory for subspace and representation learning for decentralized and dependent data. For decentralized data, the PI plans to design and study a new methodological framework for distributed estimation of a general latent variable model. This framework requires only one round of communication of model parameters, adapts to a wide range of complex latent variable models (including those based on deep neural nets) and has been shown to yield superior numerical performance over existing approaches. Another more specific setup that the PI will consider is distributed estimation of singular spaces, with applications to spectral clustering. For dependent data, the PI will focus on learning the top singular space of a low-rank Markov transition kernel to perform state compression and dimension reduction. The PI plans to solve the problem via maximizing the log-likelihood with either nuclear-norm penalty or rank constraint. The statistical rate of the resulting M-estimator will be explicitly derived, and new optimization algorithms will be developed to compute these problems with convergence guarantee.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015539","Collaborative Research: High-Dimensional Decision Making and Inference with Applications for Personalized Medicine","DMS","STATISTICS","06/15/2020","07/08/2021","Xingyuan Fang","PA","Pennsylvania State Univ University Park","Continuing Grant","huixia wang","07/31/2022","$101,814.00","Runze Li","xingyuan.fang@duke.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","079Z","$0.00","With the advent of data collection and storage technology, researchers can obtain large-scale and high-dimensional datasets at a low price. Such datasets offer exciting opportunities to make better decisions and reveal new discoveries to improve decision making in various applications, and meanwhile, also raise statistical challenges. Over the past decades, regularization methods such as Lasso, SCAD, and MCP have been proposed to conduct model estimation in the presence of high dimensional covariates. Various numerical algorithms have been developed for these methods, and their theoretical properties are well studied. However, questions of how to efficiently and effectively utilize high-dimensional data to make optimal decisions and conduct inference are relatively less studied, although such problems are of vital practical importance. This project will develop new methods and theories for making optimal decisions and conducting valid inference under high-dimensional settings. The methods have wide applications, for instance, in personalized medicine where the goal is to determine the optimal treatments for a patient based on predictor information, including several thousand genetic markers.  The principal investigators will develop and distribute user-friendly open-source software to practitioners and provide training opportunities to students at different levels.  <br/><br/>The project has three research aims. The first aim is to study the high-dimensional contextual bandit problem with binary actions, which is an online decision-making problem that finds applications in personalized healthcare and precision medicine. In this problem, the player sequentially chooses one action and observes a reward, where the goal is to maximize the reward. The principal investigators will develop a new algorithm to provide an optimal decision rule, which achieves the minimax optimal regret. The second aim is to study general inference problems that arise from high-dimensional stochastic convex optimization, where the goal is to quantify the uncertainties of the optimal objective value. The third goal is to consider the general stochastic linear bandit problem with a finite and random action space. The principal investigators will develop a new algorithm by using a best-subset-selection type estimator, and the approach achieves a ""dimension-free"" regret and meets existing lower-bound under the low-dimensional setting.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015325","Robust Post-Selection Inference with Application to Subgroup Analysis","DMS","STATISTICS","07/01/2020","06/20/2020","Jingshen Wang","CA","University of California-Berkeley","Standard Grant","Yong Zeng","06/30/2024","$220,000.00","","jingshenwang@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","","$0.00","Researchers have been facing challenges from high-dimensional data that contain many different characteristics for each subject. For example, biomedical scientists analyze tens of thousands of genomes to determine the cause of disease and find the most promising treatments; social scientists study differential policy impacts by leveraging vast amounts of personal data gathered from social media. This project lays out two lines of research aimed at developing valid statistical inference in this challenging environment. The insights and tools developed from this project would contribute to the advancement of a wide variety of other disciplines, including economics, education, health care, and biomedical sciences. In addition, graduate students will be engaged in the project to study the statistical guarantees and to develop relevant software packages for the research. Since the project develops modern statistical methodologies with substantial applications, it will be suitable for training graduate students with a broad range of skills.<br/><br/>This project addresses two research problems and seeks to provide insights, theory and tools for more informed decision making in high dimensions. The first problem focuses on studying the treatment effect heterogeneity. Quantification and characterization of heterogeneous treatment effects play an increasingly important role in evaluating the efficacy of social programs and medical treatments in the presence of high-dimensional covariates. In particular, the research will develop efficient procedures for estimating heterogeneous quantile treatment effects and subgroup average treatment effects via covariate balancing. The second problem focuses on studying the validity and invalidity of data splitting for conducting inference with high-dimensional data.  The project will address the issue of ?random-splitting bias? in estimating the regression coefficients when the number of dummy/imbalanced variables is sizable. To overcome this challenge, the project will develop a guided data splitting framework that splits the data into more balanced halves. Because the usage of random data splitting goes beyond post-selection inference, the framework developed and the possible solutions derived from it are broadly applicable to many data-driven investigations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015447","Collaborative Research: Fine-Grained Statistical Inference in High Dimension: Actionable Information, Bias Reduction, and Optimality","DMS","STATISTICS","07/01/2020","04/22/2021","Yuting Wei","PA","Carnegie-Mellon University","Continuing Grant","Pena Edsel","09/30/2021","$108,170.00","","ytwei@wharton.upenn.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","Emerging data science applications require efficient extraction of actionable insights from large and messy datasets. The number of relevant features often overwhelms the volume of data that is available, which dramatically complicates the statistical inference tasks and subsequent decision making. In the existing statistical literature, most of theory aims at understanding the average or global behavior of a statistical estimator in high dimensions. In many applications, however, it is often the case that the goal is not to explore the global behavior of a parameter estimator, but rather to perform inference and reasoning on its local, yet important, operational properties.  The techniques and methods developed in the project will further advance the interplay between a broad range of areas including high-dimensional statistics, harmonic analysis, statistical physics, optimization, complex analysis, and statistical machine learning. The project provides research training opportunities for graduate students.<br/><br/><br/>This project pursues fine-grained inferential procedures and theory, aimed at enlarging the uncertainty assessment toolbox for various low-complexity models in high dimensions. Focusing on a few stylized problems, this research program consists of four major thrusts: (1) construct optimal confidence intervals for linear functionals of eigenvectors in low-rank matrix estimation; (2) design fine-grained hypothesis testing procedures for sparse regression under general designs; (3) develop entry-wise inference schemes for principal component analysis with missing data; and (4) conduct reliable and adaptive statistical eigen-analysis under minimal eigen-gaps. Emphasis is placed on algorithms that are model-agnostic and fully adaptive to data heteroscedasticity. Addressing these issues calls for the development of new statistical theory that enables reliable inference for a broad class of local properties underlying the unknown parameters.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2004271","Algebraic Statistics 2020","DMS","ALGEBRA,NUMBER THEORY,AND COM, STATISTICS, MATHEMATICAL BIOLOGY","06/15/2020","03/10/2020","Elizabeth Gross","HI","University of Hawaii","Standard Grant","Andrew Pollington","08/31/2022","$34,200.00","Sonja Petrovic, Vishesh Karwa","egross@hawaii.edu","2425 CAMPUS RD SINCLAIR RM 1","HONOLULU","HI","968222247","8089567800","MPS","1264, 1269, 7334","7556, 9150","$0.00","Algebraic Statistics 2020 (AS2020) will be held June 22--June 26, 2020 at the University of Hawaii at Manoa in Honlulu, HI.  The five-day conference at the University of Hawaii at Manoa aims to achieve four objectives: 1) provide a forum for algebraists, mathematical biologists, and statisticians to learn about recent advances in the field and share their current challenges; 2) broaden participation in algebraic statistics in terms of gender, race, ethnicity, institution type, geography, and scientific discipline; 3) train junior researchers in this interdisciplinary area, which requires exposure to both algebra and statistics; and 4) foster collaboration with international researchers, specifically those from Japan, who have been making fundamental progress in the field in the last two decades. <br/><br/>Algebraic statistics is an interdisciplinary field that uses tools from commutative algebra, geometry, and combinatorics to address problems in mathematical statistics.  The main applications in the last decade have focused on data science, biology, and engineering. Algebraic Statistics 2020 will feature nine keynote speakers, from the US and abroad, three problem sessions led by established researchers in the field to engage junior researchers and those interested in learning more about algebraic statistics, contributed talks, and a poster session.  The conference website is http://bit.ly/AlgebraicStatistics2020.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015380","Meetings of New Researchers in Statistics and Probability","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","07/15/2020","08/09/2022","Simon Mak","NC","Duke University","Standard Grant","Yulia Gel","06/30/2024","$300,000.00","Alexander Volfovsky","sm769@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1260, 1269","7556","$0.00","The Committee on New Researchers of the Institute of Mathematical Statistics will hold its 2020, 2021, and 2022 conferences at the University of Pennsylvania, the University of Washington, and George Mason University, respectively, on the three days prior to the Joint Statistical Meetings. Each participant will give a brief introduction to his/her research, as well as a poster presentation. Plenary talks will be given by established researchers. There will be panel discussions on teaching and mentoring, publishing, funding, and finding collaborators. The other workshops supported by this grant will include oral and poster presentations by new researchers, plenary talks by established researchers, and open discussions of future directions for statistics, probability, and data science.<br/><br/>The University of Pennsylvania, the University of Washington, and George Mason University, will host a conference for new researchers in statistics and probability on the three days prior to the Joint Statistical Meetings in 2020, 2021, and 2022, respectively. The meeting is organized by the IMS Committee on New Researchers and held for junior researchers working in different areas of Statistics and Probability. The primary objective of the conference is to provide a platform for interaction among new researchers, as well as opportunities to seek mentorship from leading researchers in the field. This conference series is explicitly aimed at training the future leaders and researchers in probability and statistics. This funding will further support new researcher participation at the Preparing for Careers in Teaching Statistics and Data Science Workshop, an annual workshop for early career educators in statistics and probability, to be held the day prior to the Joint Statistical Meetings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1952386","FRG: Collaborative Research: Generative Learning on Unstructured Data with Applications to Natural Language Processing and Hyperlink Prediction","DMS","STATISTICS","07/01/2020","04/02/2020","Wing Hung Wong","CA","Stanford University","Standard Grant","Yulia Gel","06/30/2024","$250,000.00","","whwong@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","075Z, 079Z, 1616","$0.00","This project addresses the pressing needs of analyzing ?big? unstructured data and tackles some artificial intelligence questions from the statistical perspective, which requires the focused and synergistic e?orts of a collaborative team. Specifically, the project develops generative models for statistical learning and leverages dependence relations modeled by graphical models in hyperlink prediction, which are applicable to topic sentence generation and protein structure identification. It will lead to a substantial improvement in the accuracy of generative learning based on numerical embeddings, particularly in topic sentence generation and hyperlink prediction. The integrated program of research and education will have significant impacts on machine learning and data science, social and political sciences, and biomedical and genomic research, among others. The project requires extensive algorithm and software development for natural language processing and multimedia data integration. The PIs, their postdocs, and students will develop innovative computational algorithms and software for the analysis of large-scale unstructured complex data. The advanced computational tools will be disseminated to facilitate technology transfer. <br/><br/><br/>The project will address some fundamental issues in two important areas of unstructured data analysis in machine learning and intelligence. In particular, the proposed research will develop a statistical framework for generative learning, which is primarily motivated by applications for unstructured data, namely topic sentence generation and high-order hyperlink prediction. The research will develop powerful generative methods for generating instances or examples to describe and interpret the corresponding learning model.  Moreover, it will develop network models for modeling high-order interactions and relations of units by identifying hidden structures in networks. It will proceed in two areas: (1) instance generation and topic sentence generation; (2) hyperlink prediction for multiway relations in hypergraphs. In the first area, instance generation, particularly sentence generation, will be performed collaboratively with numerical embeddings in categorization and regression. In the second area, hyperlinks will be predicted based on observed pairwise as well as unobserved high-order relations, characterized by graphical models with hidden structures. Special effort will be devoted to inverse learning, the integration of data from multiple sources, and extracting latent structures of networks.  Finally, the research will develop computational tools and design practical methods that have desirable statistical properties.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015460","New Directions in Bayesian Change-Point Analysis","DMS","STATISTICS","08/15/2020","07/01/2021","Nilabja Guha","MA","University of Massachusetts Lowell","Standard Grant","Yong Zeng","07/31/2024","$139,984.00","Jyotishka Datta","Nilabja_Guha@uml.edu","600 SUFFOLK ST STE 212","LOWELL","MA","018543624","9789344170","MPS","1269","","$0.00","Almost all dynamic and random processes in nature go through sudden and significant structural changes. Often the change is in the observable quantity, e.g. fuel prices or stock indices or crime activities changing significantly in response to a change in an unobservable, latent factor such as an economic phenomenon or a public policy change, or a disease outbreak. Such ?change-points? are routinely observed across all scientific disciplines and applications, such as economics, epidemiology, social sciences, cybersecurity and finance. Specific examples could be changing regression when the observed variable depends on predictors through a mean structure that changes with time, or change points in data with massive dimensions, such as high-resolution imaging data or complex connected graphs. While there is a substantial literature proposing elaborate methods for detecting change points in different settings, there has been limited consideration of Bayesian methods for change-points in hierarchical models with complex dependence or sparsity structures. This research fills this gap with new statistical tools motivated by specific real-life applications, by developing theoretical framework while retaining efficiency and usefulness in current applications. The project integrates graduate education and training with statistical research, and emphasizes upholding societal and ethical considerations that create and foster an inclusive and diverse community.<br/><br/>In higher dimensions, the problem of detecting change-points and the changing structure is often rendered extremely difficult owing to a combinatorial computational complexity. Through this research, the PIs outline a comprehensive framework, both theoretical and methodological, in the context of change point estimation encompassing problems that may arise in different field of applications. In particular, the PIs build fundamentally new Bayesian methods that can 1) perform sparse signal recovery in a changing linear regression with consistency guarantees 2) detect change-points in dependence structure via changes in a Gaussian graphical model, and 3) build an innovative method for handling ?ultra-high?-dimensional objects via random projections to drastically reduce the computational burden. Theoretical machinery will be developed to provide probabilistic rigor and consistency guarantee. Computationally efficient algorithms will be developed, and user-friendly software tools will be deployed in R for the usage of the developed methods by the scientific community at large.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015417","Multidimensional Latent Variable Models for Large and Complex Event History Data","DMS","STATISTICS, Methodology, Measuremt & Stats","07/01/2020","06/20/2020","Zhiliang Ying","NY","Columbia University","Standard Grant","Yong Zeng","06/30/2023","$200,000.00","","zying@stat.columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269, 1333","","$0.00","The project consists of two parts which are motivated by and applicable to educational assessment and health sciences. Advances in modern computer and information technology enable educational assessments to measure comprehensive problem-solving skills in virtual environments in which examinees experience interactively with computers. The existing evaluation methods only look at the final answers, ignoring vast behavioral data collected over the course of interaction. The first part of the research explores the entire interactive problem-solving processes by individuals so that comprehensive problem-solving skills can be assessed efficiently and more accurately. The developed new tools will have direct impacts on the design and analysis of large scale national and international educational assessments such as the National Assessment of Educational Progress (NAEP) and the Programme for International Student Assessment (PISA), which are the two most important assessment schemes on the primary and secondary education. The second part develops novel statistical approaches to analyzing large scale health system data. The new developments could be used to ascertain efficacy and monitor side effects for drugs currently used in healthcare management programs. They could also lead to new statistical tools for analyzing behavioral data, which are common in social science studies. The project provides research training opportunities for graduate students.<br/><br/>The research develops latent variable models for moderately high dimensional counting process data and dynamic regression models for counting process data when both covariates and events are sparse. For latent variable/factor models, the research addresses the fundamental and challenging issue of identifiability by finding suitable constraints, which also lead to more parsimonious and interpretable models. Valid inferential methods are developed by establishing crucial asymptotic results under appropriate regularity conditions. Stochastic gradient-based algorithms are constructed for efficiently carrying out parameter estimation. For the multidimensional counting process models with frailty and dynamic covariates, the research addresses the challenging issue of sparsity, in terms of both events and covariates. By exploring certain special structures inherent in such data, the research establishes suitably normalized asymptotic theories for parameter estimation so that valid inference can be conducted. The covariate sparsity and correlated frailty make the asymptotic theory challenging as standard techniques used for counting process models are no longer appropriate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1954046","Fifth Workshop on Higher-Order Asymptotics and Post-Selection Inference; June 21-23, 2020; St. Louis, Missouri","DMS","STATISTICS","03/01/2020","01/17/2020","Todd Kuffner","MO","Washington University","Standard Grant","Pena Edsel","02/28/2022","$15,000.00","","kuffner@math.wustl.edu","ONE BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","1269","7556, 9150","$0.00","This award supports participation of graduate students, postdocs and other early-career researchers in the Fifth Workshop on Higher-Order Asymptotics and Post-Selection Inference (WHOA-PSI), to take place June 21 through June 23, 2020 at Washington University in St. Louis. The goals of the workshop include (i) to provide a forum to disseminate and discuss the most recent advances in inference after model selection or dimension reduction; (ii) to promote collaboration between theoretical, methodological and applied researchers; and (iii) to advertise applications for which new post-selection inference methods are needed. The workshop features more than 30 invited talks and 2 poster sessions, representing a large number of research groups who work in relevant fields from all over the world.  <br/><br/>New areas of substantial activity within the past year include causal inference, the feasibility of selective inference after machine learning methods have been employed, and Bayesian post-selection inference. Post-selection inference in linear models remains a very active research area, with no consensus yet reached regarding what should be the targets of inference, which errors to control, what sort of conditioning should be used, and how to combine statistical principles such as decision-theoretic optimality, Fisherian relevance, robustness, and Neyman-Pearson approaches to testing. Our understanding of when and how appropriate bootstrap methods can be developed in the context of post-selection inference and high-dimensional inference continues to evolve. This workshop provides an opportunity for further discussion and progress on these important issues. It will also increase awareness of applied problems in need of new inference tools. The website for this workshop is https://www.math.wustl.edu/~kuffner/WHOA-PSI-5.html.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1955925","Topology for Data Science: An Introductory Workshop for Undergraduates","DMS","INFRASTRUCTURE PROGRAM, TOPOLOGY, STATISTICS, COMPUTATIONAL MATHEMATICS, Algorithmic Foundations, CDS&E-MSS","01/15/2020","05/19/2023","Stacey Hancock","MT","Montana State University","Standard Grant","Joanna Kania-Bartoszynska","06/30/2023","$30,474.00","David Millman","stacey.hancock@montana.edu","216 MONTANA HALL","BOZEMAN","MT","59717","4069942381","MPS","1260, 1267, 1269, 1271, 7796, 8069","7556, 7926, 7929, 9150","$0.00","Topology for Data Science (T4DS) is a day-long workshop designed to introduce undergraduate students to data science and topology, to be held on March 25, 2020, in Bozeman, MT. The workshop is scheduled to coincide with the National Conference on Undergraduate Research (NCUR)---a gathering of nearly 4,000 talented undergraduate students from across the world---hosted by Montana State University, March 26-28. This award will support the development and dissemination of workshop materials and will fund travel and participation of 28 undergraduates. An additional 22 students from local communities or those already planning to attend NCUR will also have the opportunity to participate. For a day, the participants will be immersed in the fast-growing area of data science, through the lens of topology. This workshop is the first of its kind: T4DS engages students in a hands-on, collaborative experience, requiring only discrete mathematics and a desire to try something new as prerequisites.<br/><br/>T4DS will start with an overview of how to ""think with data"" through data exploration and visualization, continuing with a brief journey into the field of topology and how to use topological descriptors to summarize data. In the afternoon, students will investigate how to cluster data based on those topological descriptors, and will apply what they've learned to a new data set. The day will conclude with a reception, where a panel of five faculty members representing topology and data science will discuss their experiences with including undergraduates in research and potential career opportunities in data science. The content of T4DS will cross the disciplinary lines of computer science, statistics, and mathematics. It will blend topological data analysis, data mining, and broader data science content, delivered using an active-learning pedagogy. Most of the participants will not have any experience in topology, yet they will cluster data using topological descriptors, which will give them a glimpse into the topological data cycle. Moreover, this workshop will lay the foundation for developing materials on other aspects of data science, which will allow the organizers to broaden the offering of similar tutorials throughout Montana and surrounding states, including tribal colleges and geographically remote rural areas. After the workshop, tutorials will be distributed through Github and the Carpentries Lab---a repository of high-quality, community-reviewed, discoverable lessons--- reaching a large, diverse, international community. The conference website and workshop application can be found at http://www.montana.edu/datascience/t4ds/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015490","Model Evaluation in Modern Predictive Regimes: Case Influence and Model Complexity","DMS","STATISTICS","07/01/2020","06/26/2020","Yoonkyung Lee","OH","Ohio State University","Standard Grant","Yong Zeng","06/30/2024","$250,000.00","Yunzhang Zhu","yklee@stat.osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269","079Z","$0.00","Models are an integral component of scientific inquiries, and they can be used as effective devices in many practical areas as engineering, commerce, and governance. There have been remarkable advances in the way statistical models are defined from data for prediction or for accurate descriptions of the world we observe. While complex predictive models have routinely emerged, our understanding of these models and methods for their evaluation have been lagging. Appropriate tools for assessing models and recognizing their deficiencies, and proper ways to account for their complexity are in great need. To fill these gaps, the project aims to develop methodologies and computational tools for assessment of case influence on general models in predictive settings. And it will extend the notion of model complexity to general prediction rules for model comparison and calibration by using the overall model sensitivity to data perturbation. Results from this research will bring great benefit not only to science and engineering through the practice of refined statistical modeling, but also to society at large through applications. In particular, the project will have practical utility in outlier detection for many scientific applications, fraud/threat detection and prevention for many business applications, and detection of adversarial attacks for artificial intelligence applications. Moreover, it will advance our understanding of modern algorithmic models such as deep learning through the research on model complexity and foster interdisciplinary research. The project will provide research training opportunities for graduate students. Computational tools developed will be distributed as open-source software.<br/><br/>To characterize the sensitivity of a predictive model to data, the PIs will develop novel approaches to case influence assessment for general modeling procedures, encompassing many modern statistical learning techniques for classification and regression. Extending case deletion statistics and case influence graph in linear regression, the project will offer a variety of new case influence measures for classification in particular. In addition, the PIs will develop efficient computational algorithms for evaluating those case influence measures by utilizing a homotopy technique to relate two modeling problems with the original data and perturbed data under various perturbation schemes. Further, the project will examine model complexity through the lens of model sensitivity to data perturbation considered in case influence assessment and extend the notion of model degrees of freedom to general modeling procedures including large-margin classifiers. This extension will be based on the relation between expected optimism and model complexity in the risk estimation framework where model sensitivities to perturbation of individual cases can be linked to the conditional expected optimism.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2005243","Statistics in the Big Data Era","DMS","STATISTICS","01/01/2020","12/16/2019","Peng Ding","CA","University of California-Berkeley","Standard Grant","Pena Edsel","12/31/2022","$19,950.00","Haiyan Huang","pengdingpku@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","7556","$0.00","This award supports participation by graduate students and junior researchers in the conference ""Statistics in the Big Data Era"" held at the Simons Institute for the Theory of Computing at the University of California, Berkeley during May 27-29, 2020. This conference focuses on the changing role and nature of the discipline of statistics in the time of a data deluge in many applications, and on the increasing success of artificial intelligence at performing many data analysis tasks. The conference will feature approximately twenty invited speakers who have made fundamental contributions to the theory and practice of statistics and machine learning. The meeting aims to bring together experts in statistical methodology and theory for complex and big data with researchers focused on a range of applications, from genomics to social networks, and to provide opportunities for early-career researchers to learn about both emerging methods and applications. This conference will be an especially important opportunity for graduate students and early-career researchers to see recent advances and emerging challenges in the theory, methodology, and application of statistics in the big data era.<br/><br/>Statistics plays a central role in quantitative research in many disciplines, including social and biological sciences. With the advances in information technology, statistics has gone through a paradigm shift from conventional methods to big data analytics and modern machine learning. This conference will bring together current leading statisticians and the next generation of statistical leaders to discuss achievements and difficulties in the statistical analysis of big data. The meeting will focus on methodology for solving challenges involving big data with complex and heterogeneous structure. The discussions will promote opportunities for statisticians to identify important research topics and critical applications in science and industry. Another important component is a poster session in which students and early-career researchers present their work. The conference program also includes discussion of statistics and data science education in the era of big data.<br/><br/>More information about the conference can be found at https://simons.berkeley.edu/workshops/statistics-big-data-era.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2006304","The 8th Workshop on Biostatistics and Bioinformatics","DMS","STATISTICS","05/01/2020","03/02/2023","Yichuan Zhao","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Yong Zeng","04/30/2024","$8,000.00","","yichuan@gsu.edu","58 EDGEWOOD AVE NE","ATLANTA","GA","303032921","4044133570","MPS","1269","7556","$0.00","The 8th Workshop on Biostatistics and Bioinformatics will be held on the campus of Georgia State University, Atlanta, May 8-10, 2020.  The goal of the workshop is to reflect recent advances in biostatistics and bioinformatics and the new challenges in these research areas. The invited speakers include renowned experts and promising young researchers.  Graduate students and postdocs will gain valuable research experience by making poster presentations. Four to five Best Student Poster Awards of the ASA Georgia Chapter will be selected and announced during the award ceremony of the workshop. In addition, a short course ""Learning and Implementing Bayesian Adaptive Designs"" will be provided during the workshop. The workshop will encourage graduate students and junior researchers as poster speakers or participants. In particular, the conference organizers will invite a large number of early-career and underrepresented researchers to participate in the workshop.<br/> <br/>The workshop will focus on frontiers of high-dimensional and big data analyses, recent advances in causal inference, new developments in functional data analysis, new frontiers in fMRI data analysis, and generate opportunities for collaboration, discussion and dissemination of new ideas. The workshop will establish an excellent platform for close collaboration between different universities in the research of biostatistics and bioinformatics. The conference organizers will contact appropriate departments of historically black universities in the Metro Atlanta area, invite underrepresented members to participate in the workshop, and encourage them to apply for financial support through this award. The travel supports provided by this grant are particularly for junior researchers and graduate students.  More information about the workshop can be found at https://math.gsu.edu/yichuan/2020Workshop/index.html.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015454","Iterative Algorithms for Statistics: From Convergence Rates to Statistical Accuracy","DMS","STATISTICS","07/01/2020","08/15/2022","Martin Wainwright","CA","University of California-Berkeley","Continuing Grant","Pena Edsel","11/30/2022","$300,000.00","","wainwrigwork@gmail.com","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","","$0.00","Science, engineering, and industry are all being revolutionized by the modern era of data science, in which increasingly large and rich forms of data are now available.  The applications are diverse and broadly significant, including data-driven discovery in astronomy, statistical machine learning approaches to drug design, and decision-making in robotics and automated driving, among many others.  This grant supports research on techniques and models for learning from such massive datasets, leading to computationally efficient algorithms that can be scaled to the large problem instances encountered in practice. The PI plans to integrate research and education through the involvement of graduate students in the research, the inclusion of the research results in courses at UC Berkeley and in publicly available web-based course materials, as well as in mini courses at summer schools and workshops. This project will also provide mentoring and support for graduate students and postdocs who are female or belong to URM communities.<br/><br/><br/>Many estimates in statistics are defined via an iterative algorithm applied to a data-dependent objective function (e.g., the EM algorithm for missing data and latent variable models; gradient-based methods and Newton's method for M-estimation; boosting algorithms used in non-parametric regression).  This projectl gives several research thrusts that are centered around exploiting the dynamics of these algorithms in order to answer statistical questions, with applications to statistical parameter estimation; selection of the number of components in a mixture model; and optimal bias-variance trade-offs in non-parametric regression.  In more detail, the aims of this project include (i) providing a general analysis of the EM algorithm for non-regular mixture models and related singular problems, in which very slow (sub-geometric) convergence is typically observed; (ii) developing a principled method for model selection based on the convergence rate of EM, and to prove theoretical guarantees on its performance; developing a general theoretical framework for combining the convergence rate of an algorithm with bounds on its (in)stability so as to establish bounds on the statistical estimation error; and (iii) providing a complete analysis of the full boosting path for various types of boosting updates, including kernel boosting, as well as gradient-boosted regression trees, and to analyze the ""overfitting"" regime, elucidating conditions under which overfitting does or does not occur.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2014861","Analyzing Dependent Extremes via Joint Quantile Regression","DMS","STATISTICS","09/01/2020","06/20/2020","Surya Tokdar","NC","Duke University","Standard Grant","Yong Zeng","08/31/2023","$149,999.00","","st118@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","","$0.00","What is common between Hurricane Harvey and the Great Recession? Both are examples of ordinary and loosely connected processes attaining extraordinary levels in a synchronized manner. Accurately predicting the size and frequency of synchronized extreme outcomes is crucial to robust risk management. But this task remains a difficult challenge to data science that has been mostly built around the notions of average behavior and independence. A case in point is regression analyses where one studies how a group of outcomes are simultaneously influenced by a common set of predictors. Current statistical methods can either address inter-dependency between multiple outcomes, or model transitions from ordinary to extraordinary levels of a single outcome. But nothing satisfactory exists to handle both. This research fills this gap with new data analysis tools based on quantile driven regression analysis. The new tools are specialized for analyzing data recorded over space and time or within network clusters. They are subjected to detailed mathematical scrutiny for accuracy and reliability. Applications to finance and environmental sciences are carried out to assess the usefulness of the new tools in scientific investigation and policy making. The project integrates statistical research with software development and graduate education. <br/><br/>Quantiles are simply percentiles expressed in terms of a level varying from zero through one, as opposed to a percentage point. The quantiles of a variable give direct access to its smooth transitions between ordinary and extraordinary levels. In standard quantile regression, one estimates the effects of predictors at any given quantile level of an outcome. Such estimation is easy to carry out when observation units are mutually independent; there is no need of a detailed probabilistic model for the predictor-outcome relation. But data with known dependency structures present a far more complex challenge; accurate estimation requires adjusting for intrinsic noise correlation between units in close proximity. Such a task remains beyond the scope of ordinary quantile regression methods due to their model-free nature and their focus on single quantiles in isolation. In contrast, joint quantile regression makes this task feasible by incorporating a full probabilistic model for the outcome and enabling a joint estimation at all quantile levels at once. The research investigates the use of copula modeling to address noise correlation in joint quantile regression, focusing greatly on appropriate customization of the copula formulation for each data type. A rigorous asymptotic analysis is carried out toward statistical guarantees of the resulting tools. Quantitative and visualization-based diagnostic tools will be developed for model assessment and selection. All tools will be incorporated in the R package ?qrjoint? available through The Comprehensive R Archive Network.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015411","Collaborative Research: Bayesian and Semi-Bayesian Methods for Detecting Relationships in High Dimensions","DMS","STATISTICS","08/15/2020","08/04/2020","Jun Liu","MA","Harvard University","Standard Grant","Yulia Gel","07/31/2023","$120,000.00","","jliu@stat.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269","","$0.00","In this big-data era, massive data sets are being generated routinely and we are seeing a growing need for powerful, reliable, and interpretable statistical learning tools to help understand these data.  The main ideas and approaches in this projectl focus on developing effective statistical learning tools to learn about complex and heterogeneous structures, such as those changing in time or varying among different groups of individuals, in high-dimensions. The activities will have a significant impact on high dimensional Bayesian analysis and modeling of nonlinear relationships. While most current efforts for high-dimensional Bayesian analyses have been focused on linear models, this project focuses on two ways of generalizing standard linear models to meet certain practical challenges: one is a generalized form of mixture modeling, termed as individualized variable selection, which enables each individual observation to have its own set of dependent variables through the employment of neuronized priors. Another extension is the Bayesian inference of index models that form a mixture structure. The project will lead to useful tools (or customized software) for discovering interpretable nonlinear and interactive patterns among a large number of potential variables. Various aspects of statistical modeling, design, and learning strategies integrated in our algorithms are  broadly applicable to problems involving  signal discovery in complex systems and high-dimensional data.  The project will also provide both educational and interdisciplinary research opportunities for graduate students, and will result in software useful to biomedical researchers, economists, social scientists, and many other practitioners. <br/><br/>In a vast number of regression problems, especially under high-dimensional settings, the structure of the association between covariates in hand and the target quantity of interest might be heterogeneous over observations, which calls for effective methods to detect such non-trivial structures. Standard procedures, including traditional variable selections, commonly overlook the existence of interplays of these heterogeneous factors. This research project aims to develop statistical procedures that identify the complicated relationship between response Y and a set of covariates X in flexible and computationally efficient ways.  Project 1 focuses on Bayesian individualized variable selection (BIVS), which generalizes standard linear regression models to quantify  heterogeneous effects among individual observations that differ in their  dependent variables with different magnitudes. The PIs will investigate its theoretical properties, including model selection consistency and its robustness when the model assumption is violated. Project 2 is devoted to the development of an efficient Bayesian method to infer the semi-parametric relationship between the response and covariates through general index models. The PIs will explore its computational feasibility and theoretical properties such as the posterior contraction rate on the estimation of the sufficient dimension reduction space. Project 3 focuses on a fast tuning parameter selection procedure by employing a generative process via neural networks. By using this procedure,  the cross-validation can be efficiently implemented for general models, such as the BIVS and Bayesian index models,  regularized variable selection, and nonparametric function estimation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015489","DMS-EPSRC: Change-Point Detection and Localization in High Dimensions: Theory and Methods","DMS","STATISTICS","09/01/2020","08/04/2020","Alessandro Rinaldo","PA","Carnegie-Mellon University","Standard Grant","Yong Zeng","08/31/2023","$280,000.00","OSCAR  HERNAN MADRID PADILLA","arinaldo@cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","Statistical change point analysis is concerned with identifying abrupt changes in data when measurements are collected over time. A fundamental challenge is to discriminate among changes corresponding to structural, but possibly subtle, variations in the underlying data generative model from those that may simply be due to random fluctuations. Change point models arise naturally in a variety of scientific and industrial applications, including security monitoring, neuroimaging, financial trading, ecological statistics, climate change, medical monitoring, sensor networks, disease outbreak risk assessment, flu trend analysis, genetics and many others. Though a host of well-established methods for the statistical analysis of change point problems is available to practitioners, the prevailing framework suffers from important limitations: (i) it often relies on traditional modeling assumptions of limited expressive power that are inadequate to capture the size and inherent complexity of modern datasets and (ii) it is not directly applicable to non-standard data types, such as networks or graph-structured signals. The broad goal of this project is to develop novel theories, practicable methods, and software tools for a variety of new change point settings to tackle complex, big data problems and advance the practice of statistical inference for change-point analysis. In addition to its scientific output, the project will bring together a diverse group of researchers, some from underrepresented groups in Statistics. The project will provide research training opportunities for graduate students.<br/><br/>The project includes three main research aims: (1) to derive statistically optimal and computationally efficient procedures for detecting the presence and estimating the positions of change points in various offline high-dimensional statistical models, ranging from simple univariate mean change point models to more complicated settings involving high-dimensional parameters, such covariance covariances, regression models, dynamic single and multi-layered network models; (2) to derive novel guarantees and methods for change point detection and localization in graph-structured signals; and (3) to develop methods for optimal sequential change point analysis in complex data streams.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015569","Semiparametric Methods for Analysis of Complex Data","DMS","STATISTICS","08/01/2020","07/22/2020","Meng Li","TX","William Marsh Rice University","Standard Grant","Yong Zeng","07/31/2023","$99,999.00","","meng@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","","$0.00","In the era of big data, complex data naturally arise in modern scientific applications. For example, the advance in computation and technology has enabled the routine collection of high-frequency functional data and high-resolution images. Scientists are facing daunting challenges from the data, including the massive scale, intricate dependence structures, and various shape constraints that are vital for scientific interpretability. This project will develop new flexible methods for shape-constrained regression and high-dimensional quantile regression to comprehensively depict the dependence between variables, with a focus on providing scalable implementation and theoretically guaranteed inference. These tools will address pressing statistical and computational challenges, leading to broad applications in medicine, neuroscience, cancer-related studies, and industrial settings. The project will also develop and distribute open-source software and provide research opportunities for undergraduate and graduate students. <br/><br/>The project will develop novel semiparametric methods for high-dimensional quantile regression and shape-constrained regression. The PI will investigate a paradigm shift in high-dimensional regression from a joint, iterative scheme to a two-step, distributed scheme. This strategy allows the utilization of parallel computation and is coupled with proper uncertainty propagation to ensure statistical optimality and frequentist coverage of simultaneous confidence and credible bands. Several regimes using functional and image data will be considered, for example, mean regression, quantile regression, and variable selection. The project will also develop new methods for nonparametric regression under shape constraints, including local sparsity and stationary points of unknown functions. The project will enrich the statistical toolbox to cope with complex data by developing a suite of semiparametric methods that are theoretically sound and computationally efficient.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2022942","National Institute of Statistical Sciences Writing Workshop for Junior Researchers","DMS","STATISTICS","07/01/2020","07/09/2021","James Rosenberger","NC","National Institute of Statistical Sciences","Continuing Grant","Yulia Gel","06/30/2022","$35,600.00","Lingzhou Xue","JRosenberger@niss.org","19 TW ALEXANDER DR","RESEARCH TRIANGLE PARK","NC","277090152","9196859300","MPS","1269","7556","$0.00","This award provides partial support for the National Institute of Statistical Sciences (NISS) to conduct A NISS Writing Workshop for Junior Researchers at the annual Joint Statistical Meetings (JSM), August 2-4, 2020, in Philadelphia, PA, and the 2021 workshop August 8-10, 2021 in Seattle, WA. The writing workshop provides individual hands-on guidance and mentoring for 25 junior researchers in statistics, biostatistics, and data science on how to write journal articles and research funding proposals. Junior researchers apply to the workshop by May 15 with a writing sample and are notified of acceptance no later than June 15 after pairing applicants with mentors and senior editors.<br/><br/><br/>The ability to write well is a critical but often short-changed component of the education of statisticians and data scientists. The inability to write well can hinder not only publication of a researcher's results but also grant proposals.  From 2007 to 2016, 2018, and 2019, NISS organized technical writing workshops for junior researchers in statistics, biostatistics and data science. The 2020 and 2021 NISS writing workshops will consist of two parts. The Sunday tutorial is an all-day session that covers scientific writing, as well as how to organize a paper. The workshop will cover ethical issues, writing grant proposals, strategies for journal choice, effective use of graphics, and understanding and responding to reviewers' comments.  At the end of this session, participants will meet with their mentor who will have analyzed a paper the participant submitted prior to JSM.  The Tuesday session focuses on specific issues for improving a manuscript, especially useful for participants whose native language is not English.  It will include the writing process and details of grammar, sentence structure, and word choice.  A panel of successful senior researchers whose native language is not English will discuss their experiences with technical writing in English and its importance to their success. Previous workshops have been very successful with many workshop participants going on to become associate editors or co-editors of major statistical journals. Individuals from among the 109 graduates from 2007 to 2011, the first five years, have held at least 23 associate editorships and one co-editorship. Information and registration instructions for the upcoming writing workshop are provided at:https://www.niss.org/events/2020-niss-writing-workshop-junior-researchers-jsm<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1938935","ISBA 2020: 15th World Meeting of the International Society for Bayesian Analysis -- June 29-July 3, 2020","DMS","STATISTICS","01/01/2020","12/16/2019","Li Ma","NC","Duke University","Standard Grant","Pena Edsel","12/31/2021","$30,000.00","","li.ma@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","7556","$0.00","This award supports travel for participants in the 2020 World Meeting of the International Society for Bayesian Analysis (ISBA) held in Kunming, China, June 29 - July 3, 2020. The conference themes include theory, modeling, and applications in Bayesian statistics. The grant provides support for graduate students and postdoctoral researchers from U.S.-based institutions to travel to the conference. Particular emphasis is given to supporting women and members of underrepresented groups.<br/><br/>Statisticians play a critical role in the analysis of noisy, complex data that are used to make scientific discoveries as well as practical decisions under uncertainty. Bayesian statistics provides a coherent framework that facilitates integrating information from different sources and communicates findings and conclusions using probabilities. As a result, Bayesian approaches have been widely adopted in a variety of modern applications in areas such as biology, physics, psychology, economics, finance, environmental science, and engineering, along with many others. Bayesian models and methods are also the foundation for many recent developments in machine learning and artificial intelligence. The 2020 ISBA World Meeting brings together the diverse international community of researchers and practitioners who develop and use Bayesian statistical methods for sharing recent findings, exchanging ideas, and discussing new, challenging problems. The international meeting exposes participants to ideas and colleagues from other countries, with whom they may not ordinarily interact. Participation in the conference will help inform junior statisticians about the key problems and methods that shape research in modern Bayesian statistics and provide them with opportunities to learn from more established researchers and to build collaborative relationships.<br/><br/>Additional information is available on the meeting web page https://bayesian.org/isba2020-home/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2012907","Quality and Productivity Research Conference - Data Science and Statistics for Quality","DMS","STATISTICS","04/01/2020","02/27/2020","Eric Chicken","FL","Florida State University","Standard Grant","Pena Edsel","06/30/2022","$22,690.00","","chicken@stat.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","7556","$0.00","The 37th annual Quality and Productivity Research Conference will be hosted by Florida State University in Tallahassee, Florida, on June 8 through June 11 of 2020. This conference is the main annual meeting for the Quality and Productivity Section of the American Statistical Association, the leading professional association for statisticians in the US. The 2020 conference theme reflects the changing nature of the discipline of statistics, ""Data Science and Statistics for Quality."" The overall aim of the conference is promotion of data science in diverse applied areas, in particular those associated with the fields of quality and process control. Massive amounts of data are being collected on a daily basis, processed and analyzed in virtually every branch of modern society and every aspect of everyday life, and so the science, quality, and statistical communities are obligated to keep up with the rapid growth and variety of collected data and provide up-to-date methodologies and guidance to those using this data. This conference will explore and promote ideas that will further this goal. Student participation is a vital part of the conference. The students from numerous universities and graduate programs across the US, especially including students from under-represented groups will be fully engaged. NSF funds are used to support their active participation in the conference and its associated one-day short course. Students will be invited to submit contributed and poster presentations and compete for travel scholarships. <br/><br/>This conference attracts prominent researchers from academia, industry, and government. Statisticians, data scientists, and practitioners from these areas will propose and discuss the latest ideas and cutting-edge modern methodologies in all aspects of big data analysis and their application to various and diverse fields. The conference will focus on the progress made in such computationally intensive fields such as data mining, machine learning, functional data analysis, image reconstruction, statistical process control, and uncertainty quantification, among others. It will be a unique venue for participants to form collaborations and interact with the next generation of rising students interested in these fields. The first day of the conference is a short course emphasizing computational methods for analyzing big data, ""Introduction to Data Science the Tidy Way."" The remaining three days consist of plenary, invited, and contributed presentations and poster sessions. More details on the conference can be found at its web page: https://qprc2020.com.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2038556","FRG: Collaborative Research: Statistical Approaches to Topological Data Analysis that Address Questions in Complex Data","DMS","OFFICE OF MULTIDISCIPLINARY AC, TOPOLOGY, STATISTICS, , CDS&E-MSS","07/01/2020","07/09/2020","Jessica Kehe","WI","University of Wisconsin-Madison","Standard Grant","Swatee Naik","08/31/2023","$334,780.00","","jjkehe@wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1253, 1267, 1269, 1798, 8069","062Z, 1206, 1616","$0.00","As both real and simulated data become increasingly complex due to improved instrumentation and deeper understanding of the underlying data-generating mechanisms, improved statistical methodology is required for proper analysis. Fields such as astronomy and biology that have spatial intricate, web-like data (e.g., the large-scale structure of the Universe, fibrin networks) can benefit from methodology that exploits the web-like information.  The field of Topological Data Analysis (TDA) has great potential for the innovations needed to address these important and challenging scientific questions.  This project will extend existing TDA algorithms, statistical theory and applications, and make the advancement easily accessible by incorporating the work into the freely available R package TDA.  Moreover, the research will train undergraduate and graduate students in an interdisciplinary and collaborative environment.<br/><br/>The goals of this project are (1) to extend existing algorithms in TDA to allow statistically rigorous inferences and improved visualization, (2) to develop the statistical theory necessary to apply hypothesis testing to sets of topological descriptors, (3) to develop justifiable algorithms for parameter selection, and (4) to apply these methods to complex data, especially to critical areas in astrophysics.  These developments will make TDA more accessible to scientists and data analysts across disciplines and will give TDA a rigorous statistical foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1954271","Support for 6th International Conference on Establishment Statistics; June 15-18, 2020; New Orleans, LA","DMS","STATISTICS","02/01/2020","12/20/2019","Donna LaLonde","VA","American Statistical Association","Standard Grant","Pena Edsel","01/31/2022","$18,000.00","","donnal@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","7556","$0.00","The International Conference on Establishment Statistics-VI (ICES-VI) will take place  June 15-18, 2020 in New Orleans, Louisiana. A nation's official statistics is directly affected by the quality of the data derived from its business, economic and agricultural surveys. Unlike the situation for social and demographic surveys, there are too few commonly accepted and practiced methodologies for these surveys. The most important reasons for this situation are that establishment surveys face much more difficult design and execution problems and that far less is published on strategies to solve these problems. ICES was created to address this situation by providing a venue for researchers to share their expertise in new areas of establishment statistics and to reflect on state-of-the-art methodologies.<br/><br/>The International Conference on Establishment Surveys (ICES) provides a forum to present current methods being used for surveys of businesses, farms, and institutions; to offer new or improved technologies for solving the unique problems associated with such establishment surveys; and to promote international interchange of ideas by providing an opportunity for researchers around the world to discuss their work and to exchange ideas for interdisciplinary and cross-country research. The survey world has continued to change in the four years since the last conference. There are increased possibilities for electronic communication, reductions in response rates, and greater opportunities to harmonize international data. During this conference leading experts and emerging researchers will have an opportunity to discuss these timely and important topics. The conference website is: https://ww2.amstat.org/meetings/ices/2020/index.cfm.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2014371","Developments in Gaussian Processes and Beyond: Applications in Geostatistics and Deep Learning","DMS","STATISTICS","08/01/2020","08/03/2020","Anindya Bhadra","IN","Purdue University","Standard Grant","Yong Zeng","07/31/2023","$120,000.00","","Bhadra@purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1269","","$0.00","Gaussian processes have diverse applications in statistics and machine learning and are of great contemporary interest. To give a few examples, they arise in the modeling of spatial data, computer experiments, and in studying the limits of deep neural networks. Key reasons for the appeal of Gaussian processes include their simplicity and wide tractability: the entire process is characterized by just the mean and the covariance functions. Yet, although Gaussian processes are popular with well-developed theoretical and computational properties, there are some distinct limitations in using them. Moreover, there are several situations where Gaussian processes are inappropriate as a modeling choice. New methodology will be developed to address some of these limitations, with wide-ranging implications from spatial statistics to deep learning. Publicly available software development, student mentoring, and broad dissemination of research will have impacts beyond the particular research problems at hand.<br/><br/>Key areas of the technical investigation are as follows. The first issue concerns the use of the ubiquitous Matern covariance function. A key benefit of the Matern family is the precise control over the smoothness of the resultant Gaussian processes (GP) realizations. However, the tails of the Matern covariance decay exponentially fast, which is inappropriate in the presence of polynomial dependence. Polynomial covariances such as Cauchy remedy this issue, but at the expense of a loss of control over smoothness, in that, GP realizations using Cauchy covariances are either infinitely differentiable or not at all. The PI will develop a new covariance function that combines the flexibility of the Matern and polynomial covariances. Next, the PI will study the limiting behavior of deep neural networks under global-local horseshoe regularization priors on the weights. The lack of bounded moments necessitates the construction of a new Levy process that can be used to study the limits of neural networks under such priors, thereby aiding uncertainty quantification. The PI will study the theoretical and computational properties of the resultant process. Finally, the PI will use recently developed global-local shrinkage approaches for Bayesian regularization in GP regression, with distinct improvements upon existing methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2012298","Collaborative Proposal: Models and Methods for High Quantiles in Risk Quantification and Management","DMS","STATISTICS","08/01/2020","07/27/2020","Zhengjun Zhang","WI","University of Wisconsin-Madison","Standard Grant","Yong Zeng","07/31/2023","$119,999.00","","zjz@stat.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","In recent years, vulnerabilities in financial markets, economies, and public health have posed increasingly severe risks to society. For monitoring natural disasters and forecasting epidemics, financial institutions and governmental organizations must invest in risk intelligence to clearly define, understand, measure, quantify, and manage their tolerance for and exposure to risk. By employing rigorous and robust analytics to measure, quantify, and forecast risk, business leaders and regulators can rely less on intuition and more on systematic methodologies to manage risk well and make sound policy decisions. This project will develop improved and powerful analytic tools for applied researchers, regulators, and practitioners to conduct risk assessment. These tools and techniques will have broad impacts in wide-ranging fields such as economics, finance, and insurance. The project also intends to provide training opportunities for graduate students and broaden the participation of underrepresented groups in statistics and actuarial science. <br/><br/>This research project focuses on the uncertainty quantification, back-test, and sensitivity analysis for both conditional and unconditional risk measures computed from mathematical models. This project develops a computationally efficient two-step inference for an ARMA-GARCH model and fits parametric and semi-parametric distribution family to residuals. The investigators will study semi-supervised learning for risk analysis when other variables with a large sample size are available. They also plan to validate residual-based bootstrap methods for quantifying risk uncertainty and develop efficient ways for risk forecasts and back-tests. The new methodologies combine some modern statistical techniques such as extreme value theory for forecasting catastrophic risk, weighted estimation for handling both infinite variance and persistent volatility, and empirical likelihood method for efficient hypothesis testing. These techniques are robust and applicable to various problems in risk management and other research fields requiring uncertainty quantification.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2000420","Second Annual Data Science Workshop: Emerging Data Science Methods for Complex Biomedical and Cyber Data","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","02/01/2020","01/13/2020","Jie Chen","GA","AUGUSTA UNIVERSITY RESEARCH INSTITUTE, INC.","Standard Grant","Pena Edsel","01/31/2022","$13,850.00","Varghese George","jiechen@augusta.edu","1120 15TH ST STE CJ3301","AUGUSTA","GA","309120004","7067212592","MPS","1260, 1269","7556","$0.00","The Second Annual Data Science Workshop entitled ""Emerging Data Science Methods for Complex Biomedical and Cyber Data"" will be held on Augusta University (AU) Riverfront campus, March 26-27, 2020. The workshop aims to foster collaborative research among data science, statistics and other related disciplines for the purpose of addressing the very hardest and most important data- and model-driven scientific challenges, and for developing the much-needed skills for the next generation STEM workforce. The Workshop participants will learn statistical and data science methods to handle the enormously complex biomedical and cyber science data, and help them develop analytical thinking, statistical reasoning, communication skills and creativity. Participants will be strongly encouraged to submit abstracts for poster sessions held during the entire workshop. This award will support the invited speakers, around fifteen undergraduate and graduate students to attend the event and to present research posters closely related to the theme of the workshop.  <br/><br/>The topics of the workshop include deep learning, statistical machine learning, differential privacy, Bayesian data integration and cybersecurity data modeling, among others, building on AU's unique strength and focus in medical and cyber science research. The workshop will be in the form of specific research overviews and lectures provided by leading experts who have done prominent work in the respective topic areas. At the end of each of the two days, a panel discussion will be held, led by an expert panelist and all speakers of the day, for encapsulating the various topics presented on the day and for addressing questions and comments from the audience.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1944904","CAREER: Inference for High-Dimensional Structures via Subspace Learning: Statistics, Computation, and Beyond","DMS","STATISTICS","07/01/2020","05/10/2021","Anru Zhang","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","11/30/2021","$155,873.00","","anru.zhang@duke.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","1045","$0.00","High-dimensional arrays commonly arise from modern scientific and technological research and have been a central topic in modern statistics and data science. Some areas such as genetics, microbiome studies, brain imaging, hyperspectral imaging, etc., yield a large amount of high-dimensional array data; while in some other areas, data can be recast into high-dimensional array form to facilitate analysis. In these situations, the target parameter is often high-dimensional/high-order, but the important information may lie in dimension-reduced subspaces induced by various structural conditions. How to efficiently exploit these subspaces poses significant statistical and computational challenges. This project aims to address these challenges from a perspective of subspace learning. By taking into account dimension-reduced and low-order subspaces, the PI aims to address a series of statistical and machine learning questions by developing new methodologies and theories with statistical and computational advantages. <br/><br/>This project will progress along three major directions: (i) fast estimation and inference for high-dimensional arrays via important subspace sketching; (ii) high-order clustering with theoretical guarantees; (iii) ultrahigh-order tensor singular value decomposition via a tensor-train parameterization. The research will be applicable to a variety of topics involving high-dimensional matrix and tensor data, such as genetics and genomes, reinforcement learning, neuroimaging analysis, material science, recommender design, etc. The PI will also develop user-friendly software packages for the new algorithms and make them available for public use. The PI is committed to training students, especially those from groups underrepresented in STEM, through involvement in the research project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1939082","Talking Across Fields","DMS","PROBABILITY, ALGEBRA,NUMBER THEORY,AND COM, STATISTICS, Combinatorics","01/01/2020","11/13/2019","Sumit Mukherjee","NY","Columbia University","Standard Grant","Pawel Hitczenko","12/31/2020","$30,000.00","","sm3949@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1263, 1264, 1269, 7970","7556","$0.00","This award provides travel support of participants to the conference titled ""Talking Across Fields"" will take place at Stanford University, on January 31-February 2, 2020. It is expected to gather about 17 talks given by some of the world leaders in combinatorics, probability, and statistics, and more than fifty other participants. <br/><br/>The purpose of this meeting is to explore the fundamental connections between probability, combinatorics, algebra, and their applications in statistics and scientific computing. In particular, the meeting will explore recent advances in rates of convergence of Markov chains, exchangeability, random graphs, and sampling, and the foundational role they play in our understanding of randomness, symmetry, and computation. <br/>The conference webpage can be found at the URL https://statistics.stanford.edu/conferences/talking-across-fields<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."

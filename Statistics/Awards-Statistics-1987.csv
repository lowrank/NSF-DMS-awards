"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"8711924","Mathematical Sciences: Multivariable Regular and ExponentialVariation with Application to Multiple Regression and to    Convolutions of Multivariate Distributions","DMS","STATISTICS","09/01/1987","04/04/1988","Daren Cline","TX","Texas A&M Research Foundation","Continuing Grant","Alan Izenman","02/28/1990","$25,880.00","","dcline@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","","$0.00","     This research will seek results and their applications of                  multivariate regular variation.  The applications will be to                    least square estimators in multiple regression with random                      predictors.  The notable part will be that the assumptions on the               underlying distributions will be minimal, allowing for infinite                 moments and disparity among distributional types.  Basic among                  problems of study is the regular variation of probability tails                 for product of a random vector with an independent scaler and for               the matrix of cross products formed from a random vector.  A                    related problem to be studied will involve the exponential tail                 behavior of convolutions.  Extensions of the concepts of                        subexponentiality and convolution equivalency will be sought.                   The research will have applications for multivariate infinitely                 divisible laws for compound multivariate distributions."
"8702980","Mathematical Sciences: Inference Problems in Multivariate   Statistical Analysis","DMS","STATISTICS","07/01/1987","02/19/1988","Robb Muirhead","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Alan Izenman","12/31/1989","$83,476.00","","robb@stat.lsa.umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","        This research is concerned with problems of inference in                classical multivariate procedures such as multivariate analysis                 of variance, discriminant analysis , principal components                       analysis, and canonical correlations analysis.  The problems                    posed are of two main types.  One involves problems in estimating               canonical correlations and the eigenstructures of parameter                     matrices arising in multivariate analysis of variance.  The                     investigators propose to apply decision-theoretic concepts and to               evaluate the properties and behavior of estimates based on these                concepts.  The second type of problem to be investigated involves               hypothesis testing.  Widely used tests in multivariate analysis                 are ones which hypothesize the equality of a subset of the                      characteristic roots of a parameter matrix.  No small sample                    properties (unbiasedness, monotonicity of the power functions,                  admissibility, etc.) are known for common tests, including the                  likelihood ratio test.  The usefulness of the bootstrapping                     technique in problems of inference in multivariate analysis will                be investigated.                                                                        Classical multivariate statistical procedures are                       characterized by assumptions of linearity, independence and                     normality.  These procedures are widely used throughout the                     sciences to analyze experimental and observational data.  The                   statistical procedures for making some of the estimates used in                 these multivariate analyses are crude and, therefore, improving                 them may have a significant impact on the conclusions scientists                draw about relationships between measurements.  Also in this                    setting, our current knowledge about the performance of                         statistical decision making procedures applies strictly to                      experiments that are repeated a very large number of times.                     Since replicating experiments is usually expensive, it is                       typically done with low frequency.  The research into the                       performance of statistical decision making procedures when the                  number of replicates is small is expected to provide valuable                   information and insight to a broad spectrum of the scientific                   community."
"8705646","Mathematical Sciences: Asymptotic Methods for Bayesian and  Likelihood Analysis","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","07/01/1987","04/10/1989","Luke-jon Tierney","MN","University of Minnesota-Twin Cities","Continuing Grant","Alan Izenman","12/31/1990","$231,092.00","Robert Kass, Joseph Kadane","luke@stat.uiowa.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269, 1271","9251, 9263","$0.00","        The primary goal of this research is to provide widely                  applicable methodology for Bayesian analysis based on accurate,                 higher-order asymptotic theory.  The main analytical tools to be                refined and employed are based on Laplace's method for                          approximating integrals and the theory of weak convergence of                   distributions.  The effect of the parameterization on the size of               apporximation errors will be explored and diagnostic tools for                  assessing the adequacy of an approximation will be obtained.                    Approximations for problems with multimodal posterior densities                 and margianl posterior densities of implicitly defined functions                will be dervied.  The first order behavior of posterior                         distributions based on general nonparametric prior distributions                on densities and hazard functions will be investigated and the                  possibility of extending second order approximations to                         nonparametric, infinite dimensional problems will be explored.                          Bayesian methods are aimed at improving our ability to                  incorporate existing (prior) information into decision and                      prediction strategies.  The Baysian approach has remained rather                philosophical until the recent computer revolution.  The current                thrust of excitement and activity in this area derives from the                 recently obtained computational capability brought by the                       computer revolution and the critical need brought by inundation                 of information relevant to any subject of interest.  These                      scientists are leaders in the effort to dramatically improve                    computational capabilities for Bayesian methods.  Their research                will create more powerful statistical techniques for                            comprehensively using informaion in the decision making process."
"8703083","Mathematical Sciences: Statistics of Spatial Data","DMS","STATISTICS","06/01/1987","02/23/1988","Noel Cressie","IA","Iowa State University","Continuing Grant","Alan Izenman","11/30/1989","$105,348.00","","ncressie@uow.edu.au","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","","$0.00","        This reseach into statistics of spatial data has several                aims, including: (1) to investigate spatial prediction and                      mapping, (2) to investigate recursive parameter estimation in                   spatial models, (3) to estimate a stationary distribution from                  irregularly located spatial data, (4) to find the weakest                       hypotheses under which inference can be made at a different scale               than that of the data, and (5) to develop spatial experimental                  designs.                                                                                This research responds to needs in the geosciences for new              statistical procedures as are required to address questions                     concerning acid rain, water and air pollution, oil and gas                      deposits from the large volumes of relevant data now being                      collected.  Statistical scientists are interested in developing                 statistical procedures for studying these important problems and                this research promises to contribute to this effort."
"8701198","Mathematical Sciences: Robustness in Multivariate and       Directional Data Analysis","DMS","PROBABILITY, STATISTICS","07/01/1987","06/15/1989","David Tyler","NJ","Rutgers University New Brunswick","Standard Grant","Sallie Keller-McNulty","12/31/1991","$59,130.00","","david.tyler@rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1263, 1269","","$0.00","     This research will look at robust estimation of multivariate               location and dispersion.  The standard classical estimators are                 the sample mean vector and sample covariance matrix.  The affine                equivariant M-estimator is robust but has several unresolved                    problems associated with it.  Direct efforts to solve the                       problems of nonuniqueness for finite sample sizes and lack of a                 convergent algorithm to find these estimates have failed.  The                  investigator proposes two stage estimators to circumvent these                  difficulties.                                                                        The second problem is of identifying non-elliptical point                  clouds based on the robust residuals.  Emphasis will be placed on               detecting data sets which cause the breakdown of the M-                         estimators.                                                                          The investigator also suggests to study the efficiencies of                M-estimators and the higher breakdown statistics under a non-                   i.i.d. setting."
"8706754","Mathematical Sciences: Optimal Bayesian Design for          Non-Linear Problems","DMS","STATISTICS","07/01/1987","05/03/1988","Kathryn Chaloner","MN","University of Minnesota-Twin Cities","Continuing Grant","Alan Izenman","12/31/1989","$110,303.00","Kinley Larntz","kathryn@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","1269, 9220, 9263","$0.00","        The goal of this research is to provide the theory and the              methods for finding efficient experimental designs for non-linear               problems.  In the non-linear case, the efficiency of a design                   depends on the values of the unknown parameters and a traditional               approach to the problem has been to design for a best guess of                  the parameters.  A Bayesian approach uses a prior distribution                  and takes into account not only the best guess but also the                     uncertainty associated with the best guess.  Bayesian designs can               also be interpreted in a non-Bayesian framework as designs that                 minimize the average asymptotic variances.  The authors will                    extend their results on the important special problem of                        designing an experiment where the response increases with dose                  according to a logistic distribution.  The primary tools they                   must further develop in order to obtain successful extensions are               optimization algorithms.   They first intend to work on                         sequential designs.                                                                     This research will permit more information to be                        incorporated into the design of efficient experiments.  Inproved                computational capabilities over the last decade makes this work                 possible.  Prior to the computer revolution, scientific                         experiments were simply designed with computational feasibility a               major consideration.  Not only has this limitation been largely                 eliminated, but also larger and more complex experiments are                    being conducted.  Efficient designs for modern science are                      critical for sound utilization of the nation's research and                     development funds."
"8617919","Mathematical Sciences: Clustering Algorithms","DMS","COMPUTATIONAL BIOLOGY ACTIVITI, INSTRUMENTAT & INSTRUMENT DEVP, STATISTICS, Political Science, KNOWLEDGE & DATABASE SYSTEMS, ARTIFICIAL INTELL & COGNIT SCI","08/15/1987","04/10/1989","John Hartigan","CT","Yale University","Continuing Grant","Peter Arzberger","01/31/1991","$257,560.00","","hartigan@stat.yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1107, 1108, 1269, 1371, 6835, 6856","9263","$0.00","     The research will deal with the broad area of cluster                      analysis.  Cluster analysis is classification of objects in to                  groups of similar ones.  The principal investigator has been a                  pioneer in the field of cluster analysis.  He will investigate                  several statistical problems connected with clustering.  The                    major problems with the clustering algorithms are to answer the                 following four questions.  (1) How many clusters?  (2) Are the                  clusters real?  (3) Are the clusters reliable? and (4) What                     algorithms should be used.  Statistical methods will help answer                these questions.  Multivariate density estimation procedures will               be used.  Some of these techniques are used in spatial analysis                 as well."
"8657071","Mathematical Sciences: Presidential Young Investigator Award","DMS","STATISTICS","06/15/1987","02/08/1994","Thomas Sellke","IN","Purdue Research Foundation","Continuing Grant","Alan Izenman","05/31/1994","$189,239.00","","tsellke@stat.purdue.edu","1281 WIN HENTSCHEL BLVD","WEST LAFAYETTE","IN","479064182","3174946200","MPS","1269","9227","$0.00","     The research interests of the investigator deal with various               aspects of sequential analysis.  Sequential survival analysis                   makes it possible to construct sequential tests of known size and               power under the frequently used Cox proportional hazards model                  for the data.  As a special case of sequential experimental                     design, one can find the drug dose which will produce a certain                 response with a specified probability and find the stress level                 which will cause a specified percentage of components to fail.                  Observed significance levels for tests of hypothesis are commonly               used as judging criteria by the frequentists.  Bayesian analysis                for lower bounds on the evidence against the null hypothesis                    shows that the observed significance levels typically give a very               misleading impression as to the actual evidence in the data.                    Professor Sellke has already published several results in all of                the above areas.  He will continue his work in them."
"8705812","Mathematical Sciences: Adaptive Sampling","DMS","STATISTICS","06/15/1987","06/12/1987","Steven Thompson","AK","University of Alaska Fairbanks Campus","Standard Grant","Alan Izenman","11/30/1988","$16,490.00","","skt@stat.psu.edu","2145 N. TANANA LOOP","FAIRBANKS","AK","997750001","9074747301","MPS","1269","","$0.00","        This research is to develop the theory and methodology of               adaptive sampling designs.  The proposed approach involves a                    combination of theoretical development to determine optimal                     adaptive designs and computer simulation to compare practical                   adaptive designs with non adaptive ones.  Taking advantage of the               conditional mean square error structure of the population,                      design-induced unbiasedness is proposed to provide robustness                   against possible departures from assumptions about populations.                 Simulated sampling of cluster point process realizations is                     proposed to examine one important class of adaptive designs.                            This research is in the subfield of experimental design, in             the general area of statistics.  The goal is to utilize the                     information in an experiment more efficiently by developing the                 theory of designing experiments so that experimental units may be               sampled from a population according to rules that may depend                    sequentially on observations of previously selected units.                      Significant progress in this direction has the long term                        potential for revolutionizing the practice of scientific                        experimentation and providing for more efficient use of our                     national research and development funds."
"8717859","Mathematical Sciences: Estimation in Multivariate Analysis  and Related Topics","DMS","STATISTICS","07/15/1987","03/28/1988","Leonard Haff","CA","University of California-San Diego","Continuing Grant","Alan Izenman","06/30/1990","$44,553.00","","","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","","$0.00","     The research is in the area of multivariate estimation.  The               first project is estimation of multivariate normal covariance                   matrix.  A strategy for correcting the eigenvalue distortion will               be followed.  Solutions of some optimization problems will                      circumvent the inadequacies of invariant Bayes estimators and the               Stein estimator.  Other topics include the estimation of                        discriminant coefficients and the development of principal                      components within the context of discriminant analysis.  Discrete               analogues of the methods in the first project will be used to                   estimate several Poisson means.  The principal investigator has a               record of some of the best existing work in this area.  He will                 continue to pursue his ideas."
"8702060","Mathematical Sciences: Nonparametric Function Estimation andStochastic Modeling","DMS","STATISTICS","07/15/1987","07/02/1987","John Rice","CA","University of California-San Diego","Standard Grant","Alan Izenman","12/31/1989","$64,686.00","","rice@stat.berkeley.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","9263","$0.00","        This reseach will focus on several problems in applied                  probability and statistics which stem from biophysical                          experiments that aim to measure current flow through ionic                      channels in cell membranes.  This measurements are usually noisy                binary signals; the mathematical model usually used by                          neurophysiologists is that the state of the molecule is a Markov                jump process with a finite state space, and that the observed                   data is a function of that process.  The fundamental reseach                    questions are:  What may be learned about the Markov process from               the observations, and how may it be learned?  Other research                    topics in the area of nonparametric function estimation and                     smoothing will also be studied.                                                         This research is in the general area of statistical time                series analysis.  This research will focus on developing better                 methods to get information from data that can take on only one of               two values at any given time, but that can move from one state to               the other as time goes by.  Such data is a common result of                     biophysical experiments and this research promises to inhance                   and, to make more efficient, the accrual of knowledge from                      biophysical research."
"8702620","Mathematical Sciences: Investigations in Statistical        Decision Theory, Bayesian Analysis, Probability and         Computation","DMS","STATISTICS","07/01/1987","04/18/1989","James Berger","IN","Purdue Research Foundation","Continuing Grant","John V. Ryff","06/30/1991","$670,404.00","Shanti Gupta, Herman Rubin, Steven Lalley","berger@stat.duke.edu","1281 WIN HENTSCHEL BLVD","WEST LAFAYETTE","IN","479064182","3174946200","MPS","1269","9263","$0.00","        This team will work in four areas:  decision theory,                    Bayesian analysis, probability, and computation.  The decision-                 theoretic problems to be considered include multivariate                        estimation, the development of estimated loss methodology,                      ranking and selection for both classical and hierarchical                       Bayesian models, sequential decision problems and adaptive                      estimation and testing.  Problems in Bayesian analysis that will                be addressed include the development of robust Bayesian methods                 for testing of precise hypotheses, hierarchical Baysian analysis,               density and function estimation, and the development of                         noninformative priors.  Within probability theory and sequential                analysis, problems to be considered include study of branching                  diffusions, investigation of one-dimensional Gibbs states,                      sequential methods in survival analysis, and certain problems in                distribution theory.  Finally, the computational problems to be                 considered include Bayesian calculation via quadrature, Monte                   Carlo, and approximations, generation of random variables, and                  related problems of density simulation.                                                 This research team will work on a broad spectrum of problems            in statistics and probability, the solutions to which will                      provide more efficient and more comprehensive methods for                       producing information, extracting information from data and                     making informed decisions.  Overcoming computational problems is                a requirement for progress on some of these fronts."
"8708083","Mathematical Sciences: Multi-Dimensional Statistical        Analysis","DMS","STATISTICS, CROSS-DIRECTORATE PROGRAMS","07/01/1987","08/24/1989","Ingram Olkin","CA","Stanford University","Continuing Grant","Bernard McDonald","12/31/1990","$344,062.00","","IOLKIN@STAT.STANFORD.EDU","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269, 9120","9135","$0.00","        The underlying theme that unifies this project is the                   development of inference for multidimensional statistical                       analysis.  This work has several different thrusts, some of which               have been of continuing interest to the principal investigator,                 others new.                                                                             The primary area of interest is the generation of                       multivariate distributions with given marginal distributions.                   This will include research on bivariate distributions generated                 from Bernoulli distributions;  families of bivariate                            distributions;  families of distributions generated from                        mixtures;  bivariate uniform distributions;  bivariate logistic                 distributions; and bivariate distributions with constraints.                    Multivariate models with structured parameters and measures of                  inequality will also be investigated."
"8715614","Mathematical Sciences: Sequential Experimentation and       Adaptive Control","DMS","STATISTICS","07/01/1987","11/16/1988","Tze Lai","CA","Stanford University","Continuing Grant","Alan Izenman","12/31/1990","$150,550.00","","lait@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","     This research will study three problems.  The first set is                 called bandit problems in sequential design and scheduling.  A                  multi-armed bandit problem selects between k independent                        stochastic sequences at every stage n according to a specified                  rule.  The ""index rule"" selects a sequence with maximum ""Gittins                index"".  These indices are not easily computable.  The                          investigator will continue to develop simple approximations to                  the optimal stopping problems.                                                       The second set of problems uses the boundary crossing theory               for sample means to find useful approximations to certain Bayes                 tests.  The investigator has already found such approximations                  for general exponential families.  Now he proposes to extend the                results beyond the exponential families and to higher dimensions.                    The third set of problems deals with stochastic regression                 and adaptive control.  The classical stochastic adaptive control                is the optimal regulation of the output when it is designed as a                linear combination of previous p outputs and q inputs.  The                     output is regulated according to various criteria such as                       mimimizing the sum of squares etc..  The investigator already has               several results in this area and he plans to pursue these                       problems further."
"8702854","Mathematical Sciences: Canonical Analysis of Multivariate   Time Series","DMS","STATISTICS","07/01/1987","02/10/1988","Ruey Tsay","PA","Carnegie-Mellon University","Continuing Grant","Alan Izenman","12/31/1989","$43,757.00","","ruey.tsay@gsb.uchicago.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","     The project will develop methodology for multivariate time                 series analysis.  Multivariate model building is a very important               and current topic for research in the time series area.  The                    proposed methods will be based on the canonical representation of               the process and have the potential of revealing hidden                          simplifying structures of the process.  This will be useful in                  obtaining an appropriate linear model for the process under                     study.  The main tool used will be canonical analysis of vector                 processes, including lagged variables.  The investigator has                    already significant results in the area and has excellent                       experience in applying the procedures to real-world problems.                   Since the research is primarily concerned with canonical                        analysis, many results obtained will have applicability beyond                  time series analysis."
"8701770","Mathematical Sciences: Bayesian Statistical Theory and      Methodology","DMS","STATISTICS, Methodology, Measuremt & Stats","07/15/1987","01/23/1990","Joseph Kadane","PA","Carnegie-Mellon University","Continuing Grant","Alan Izenman","12/31/1990","$207,102.00","","kadane@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 1333","","$0.00","        The proposed research on Bayesian statistical theory and                methods falls into five categories:  (1)  The development of                    appropriate statistical models and methodology for the analysis                 of selected data and other types of non-random samples often                    encountered.  Particular emphasis will be placed on comparing the               information in a selection sample with that in an unrestricted                  random sample.  (2)  Problems of combining expert opinions,                     including a formal study of how the opinions that experts report                might differ from their true opinions, and the development of                   conditions for determing whether a given rule of combination is                 an admissible one.  (3)  Problems of optimal inspection and                     control in which it is necessary to schedule a sequence of costly               inspections of a stochastic process in order to detect as quickly               as possible the event that the process has gone out of control or               has entered some desirable target area.  (4)  Problems of optimal               stopping in which each member of a team of decision makers                      receives only partial information about the observations in a                   sequential sample, they connot communicate directly with each                   other, and they must develop an optimal strategy for jointly                    stopping the process.  (5)  The development of appropriate                      Bayesian statistical methods for use in various legal settings.                         This research is in the general area of Bayesian statistics,            an approach to statistical decision making that incorporates the                pertinent knowledge that exists prior to the collection of new                  data bearing on the issue.  The general theory behind this                      approach is not new, but has been of little practical use to                    date.  This is because the mathematical equations that need to be               solved to use even a little bit of prior information require                    large amounts of computational horsepower and clever algorithms,                and, until recently, such computational resources have not been                 generally available.  With the required resources becoming more                 commonly available, the potential impact of this research grows.                As pathbreaking Bayesian methods for specific practical examples                are developed, as is proposed here, this approach is expected to                spread and to revolutionize the way data is used to make                        decisions."
"8701201","Mathematical Sciences: Problems in Mathematical Statistics","DMS","STATISTICS","06/15/1987","04/19/1988","Gordon Simons","NC","University of North Carolina at Chapel Hill","Continuing Grant","Alan Izenman","11/30/1989","$141,740.00","David Ruppert","","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","     This research will be done by four investigators.  The first               investigator will study the consistency, rates of convergence,                  distribution theory and finite-sample properties of a class of                  nonparametric change-point estimators.  He will also investigate                the asymptotic distribution theory of degenerate U-statistics                   computed from dependent data and compare three nonparametric                    confidence interval techniques which are based upon subseries                   values of a general statistics from a stationary sequence.                           The second investigator will study aspects of smoothing                    methods, location dependent smoothing, smoothing techniques                     applied to model selection, classification, bandwidth and kernel                selection and hazard functions.                                                      The third investigator will study several estimators of data               transformation to normality or to symmetry and homoscedasticity,                generation of efficient nonlinear experimental design by                        stochastic approximation and jointly with the fourth                            investigator, he will look at nonparametric sequential age                      replacement policies.                                                                The last investigator will study the strategic implecations                of using the Ascombe-Colton model for sequential clinical trials,               as apposed to the standard Neyman-Pearson model.  He will also                  continue to study approximation methods for optimal stopping                    problemsand distributional questions for some functionals of                    Brownian motion, of statistical interest."
"8609819","Mathematical Sciences: Supercomputing in Experimental Design","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS, , , NUMERIC, SYMBOLIC & GEO COMPUT, , ADVANCED COMP RESEARCH PROGRAM, SPECIAL PROGRAMS-RESERVE","02/01/1987","05/17/1989","Jerome Sacks","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Alan Izenman","07/31/1990","$452,695.00","","sacks@niss.org","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269, 1271, 2136, 2780, 2865, 3337, 4080, 9145","1269, 9263","$0.00",""
"8703802","Mathematical Sciences: Model Robust Design and Inference","DMS","STATISTICS","06/15/1987","08/10/1987","Jerome Sacks","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Alan Izenman","11/30/1989","$84,571.00","","sacks@niss.org","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","","$0.00","        This research will be on experimental designs in settings               where systematic departures from exact linear models are allowed                but controlled.  The study of design problems in dimensions                     higher than one is facilitated greatly by controlling the                       departure through the use of a stochastic process prior.                        Problems with and without measurement error are of interest.                    Important and interesting problems fitting the latter case arise                in numerical integration and in the selection of inputs to                      computer model experiments and simulations.  Algorithms for                     computing designs are to be devised, studied, and compared.                     Prediction of linear designs are to be devised, studied and                     compared.  Prediction of linear functionals of the underlying                   regression or process will be emphasized; non-linear functionals                bring added concerns also requiring study.  The complexity of the               problems and the necessity for addressing complex design spaces                 leads to the need and use of advanced computational resources.                          The primary goal of statistical experimental design is to               assess the feasibility of a proposed experiment and to optimize                 the use of resources in the experiment.  Most of current                        statistical theory applies to very simply configured experiments                and analysis techniques.  This is so because, until the computer                revolution, the feasibility of experiments were severely limited                by data collection and computational capabilities.  This research               is to broaden the theory of experimental design, to realistically               apply to more modern scientific settings where technological                    advances are permitting more complex questions to be studied in                 controlled and controlled experiments."
"8702195","Mathematical Sciences: Time Series Analysis and LongitudinalData","DMS","STATISTICS","07/01/1987","05/23/1988","Richard Jones","CO","University of Colorado at Denver","Continuing Grant","Alan Izenman","12/31/1989","$38,538.00","","","13001 E 17TH PLACE F428","AURORA","CO","800452571","3037240090","MPS","1269","9263","$0.00","        This research is to develop an approach to modelling                    correlation over time by means of auto-regressive moving average                estimates rather than by means of the traditional assumption of                 equicorrelated repeated measures.  Traditional statistical theory               is restricted by the assumptions that (1) measurements taken on                 experimental subjects are all taken serially at the same times,                 (2) the series of measurements taken on each subject can be well                represented by polynomials of the same degree, (3) the polynomial               coefficients have a multivariate normal distribution across                     subjects and (4) the number of subjects is greater than the sum                 of the degree of the polynomials and the number of constants used               to model the dependent variables in the multivariate analysis.                  The proposed research aims to provide a theoretical framework                   that will accommodate experiments where these assumptions are                   unrealistic and invalid.  A major tool to be used in determining                algorithms for calculating the exact likelihoods is the Kalman                  filter, by which serial correlations can be modeled within each                 subject.                                                                                This research is in the general area of time series                     analysis, an area receiving an increasing amount of attention in                statistics.  The attention is reflective of the push to free                    statistical methods from ""classical"" assumptions which were                     largely imposed because of the traditionally high costs of                      computation.  The aim of this research is to provide sound                      statistical methods for a common experimental situation;                        experiments in this class do not have appropriate, i.e.                         meaningful, statistical tools available for assessing effects and               trends.  The experimental situation which will be impacted by                   this research occurs in many sciences and is characterized by the               goal of assessing the effects of some factors or treatments that                may change over time, coupled with a lack of experimenter control               over exactly when and where measurements can be taken."
"8701189","Mathematical Sciences: Computer Science and Statistics:     Opportunities at the Interface","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","07/15/1987","07/21/1987","Richard Heiberger","PA","Temple University","Standard Grant","Alan Izenman","08/31/1988","$9,844.00","","","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","MPS","1269, 1271","9263","$0.00","     This research aims to bring recent and anticipated advances                in supercomputing, parallel architectures, multivariate graphics,               simulation and stochastic processes to bear on developing new and               improved methods for solving large-scale statistical problems.                  An example of such is finding efficient solutions to large-scale                least squares problems.                                                              This award is to support the collection and dissemination of               scientific findings on the interface between the fields of                      statistics and computer science.  Activity at the interface                     between these two fields is not new.  But because of the rapid                  advances in both fields, the nature of this activity has evolved                rapidly and is constantly changing.  Dissemination of these                     scientific findings will help stimulate the development of                      statistical methods to meet the new requirements of modern modes                of scientific experimentation."
"8701814","Mathematical Sciences: Empirical and Hierarchical Bayes     Estimation in Finite Population Sampling, Quality Assurance,and Random Effects Models","DMS","STATISTICS","06/01/1987","01/27/1988","Malay Ghosh","FL","University of Florida","Continuing Grant","Alan Izenman","11/30/1989","$68,716.00","","ghoshm@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","","$0.00",""
"8701027","Mathematical Sciences: Rationalization of the ""D-optimal""   and the ""varience plus bias"" approaches to experimental     design.","DMS","STATISTICS","05/01/1987","03/24/1988","Norman Draper","WI","University of Wisconsin-Madison","Continuing Grant","Alan Izenman","10/31/1989","$54,240.00","","draper@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00",""
"8702402","Mathematical Sciences: Empirical Bayes Approaches to        Multiparameter Estimation","DMS","STATISTICS","07/15/1987","04/10/1989","Nan Laird","MA","Harvard University","Continuing Grant","Alan Izenman","12/31/1990","$266,042.00","","6177321056","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","","$0.00","        This research will contribute to methodology for                        constructing empirical Bayes confidence intervals and hypothesis                tests, and will provide theory and/or simulation to validate                    these statistics.  The investigators propose to expand current                  technology to include non-Gaussian distributions and vector                     parameters.                                                                             A common feature of many empirical experiments is the desire            to draw conclusions about a series of similar parameters.  The                  statistical approach known as Empirical Bayes has proved useful                 in this setting.  The proposal is to view each item to be                       estimated as part of an ensemble, thereby achieving considerable                gains for the set.  Current methods are applicable to very                      special types of data, i.e. data that is distributed in the shape               of a bell (Gaussian).  Developing the Empirical Bayes approach                  for a variety of types of data will increase the information                    scientists can obtain from their experiments."
"8610127","Mathematical Sciences: Developments in Time Series          Methodology","DMS","STATISTICS","01/01/1987","04/21/1989","Peter Bloomfield","NC","North Carolina State University","Continuing Grant","Alan Izenman","06/30/1990","$130,424.00","Sastry Pantula","bloomfld@eos.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","","$0.00",""
"8704341","Mathematical Sciences: Semi-Parmetric Regression with       Counting Data","DMS","STATISTICS","08/01/1987","07/21/1987","Brian Yandell","WI","University of Wisconsin-Madison","Standard Grant","Alan Izenman","07/31/1990","$6,149.00","","byandell@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","        This research is to establish the distributional properties             of estimates and pivotal quantities in semi-parametric regression               models with counting data, focusing on Poisson and binomial data                in the setting of the generalized linear model.  Estimates will                 be considered semi-parametric in the sense of having a parametric               component and a component which is subject to some ""smoothing""                  constraint, usually in the setting of generalized splines.                      Necessary tools that must be developed for this research and for                transfering results from this research to the applied statisician               are efficient computational methods for smooth general linear                   model problems in which the design points fall on a regular,                    multidimensional grid.                                                                  This research in statistical distribution theory will                   provide useful analytical tools which translate directly into                   graphical procedures for selecting probability models and                       diagnostic procedures for assessing their adequacy.  This                       research is directed toward situations where simple parametric                  models of counting data are not warranted."
"8702111","Mathematical Sciences: Some Topics in Generalized RegressionModels","DMS","STATISTICS","06/01/1987","04/27/1989","Donald Pierce","OR","Oregon State University","Standard Grant","Alan Izenman","11/30/1990","$157,977.00","Daniel Schafer","","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","MPS","1269","","$0.00","        The research which is proposed here is mainly concerned with            two problems in methods associated with generalized linear                      models:  diagnostics for tests of significance and data analytic                methods in the presence of overdispersion.  While there has been                substantial development of diagnostic techniques associated with                point estimation in generalized linear models, the research                     proposed is to study the effects of individual data points on                   tests of hypotheses concerning parameters of the model.  The                    purpose  includes ascertainment of whether the significance of a                test is overly dependent on a very small part of the data, and                  other diagnostic issues.  The proposed technique is based on the                contributions of individual data points to the likelihood ratio                 test statistic.  The second part of the proposed research                       concerns regression and analysis where the response variables are               like binomial or Poisson but exhibit more variation that would be               expected from these probability distributions.  Several issues                  related to this case are proposed for study:  (i)  the                          similarities and differences in overdispersion models based on                  covariates being omitted and based on covariables being measured                with error, (ii) the analysis of data when there is                             overdispersion due to both omitted variables and measurement                    errors, and (iii) data analytic methods involbing interactions                  when there is overdispersion present.                                                   Regression methods are used to predict what some entity will            be or will do based on related information and to determine the                 strength of association between measurements and responses.  The                proposed research will enhance and augment the collection of                    statistical research tools and procedures that are referred to                  collectively as ""regression analysis"".  This research promises to               lead to improvements in the performance of these tools when                     variables are measured with error and when influencial predictor                variables are unknown or unavailable, and therefore omitted.                    Since this situation is extremely common in experimental and                    social science, improvements will have a wide positive impact."
"8701426","Mathematical Sciences: Statistics and Probability","DMS","STATISTICS, NUMERIC, SYMBOLIC & GEO COMPUT","05/15/1987","12/15/1988","Lucien Le Cam","CA","University of California-Berkeley","Continuing grant","Alan Izenman","10/31/1990","$626,700.00","Pressley Millar","","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269, 2865","9263","$0.00",""
"8706119","Computer - Intensive Statistics in Mathematical Astronomy","DMS","MODERN ANALYSIS, STATISTICS, COMPUTATIONAL MATHEMATICS","09/01/1987","05/20/1988","Irving Segal","MA","Massachusetts Institute of Technology","Continuing grant","Alan Izenman","02/28/1990","$26,000.00","","IES@MATH.MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1262, 1269, 1271","9263","$0.00","     This research will determine appropriate new and innovative                computer-intensive statistical procedures and apply them to                     samples of discrete extragalactic sources defined by objective                  criteria, and to their relations with the predictions of                        mathematical cosmologies.  The probabilities of such deviations                 as may be observed, assuming the sample is unbiased and accepted                cosmology is correct, will be estimated by Monte Carlo methods.                 Statisical acceptability of the chronometric and Friedman                       cosmological hypotheses will be tested by comparison of their                   predicted observable relations with those observed in complete,                 or other statistically objectively delineated, published samples.               Computer applicable multivariate estimation for luminosity                      functions at several frequencies will be investigated.                               This research is cross-disciplinary, involving a symbiosis                 of Statistics with Computational Cosmology.  The fields of                      Statistics, Lie Groups, Modern Analysis, Geometry, Relativity,                  and Cosmology are all scientifically relevant in an essential way               to the testing, validation, and formulation of cosmological                     theories of the nature of the redshift and its objective                        statistical implications.  Senior scientists from these fields,                 which are normally  non-interacting, will combine their expertise               in re-analyzing extragalactic data and retesting the redshift                   theory."
"8706393","Mathematical Sciences: Robust Recursive Procedures and      Computationally Efficient Detection of Model Failures","DMS","STATISTICS","08/01/1987","02/19/1988","Roy Welsch","MA","Massachusetts Institute of Technology","Continuing grant","Alan Izenman","01/31/1990","$75,880.00","Alexander Samarov","","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1269","","$0.00","     This research will explore a broad class of recursive robust               estimation procedures.  Recursive procedures are important when                 it is too expensive to store all of the previous data and when                  real-time estimates are required.  Update procedures can provide                good approximate estimates at lower costs.  The particular                      problem to be investigated relates to abrupt changes in                         parameters of static or dynamic models that arise in many areas.                This type of problem is called jump, failure, or fault detection                in engineering and in quality control, and as detection of model                instability or change point analysis in statistics and                          econometrics.  Much of the past work assumes an underlying                      Gaussian distribution.  The principal investigator will explore                 more resistent recursive procedures."
"8703798","Mathematical Sciences: Integration of Surveys","DMS","STATISTICS, METHOD, MEASURE & STATS","07/15/1987","04/28/1988","Pramod Pathak","NM","University of New Mexico","Continuing grant","Alan Izenman","12/31/1989","$54,777.00","","","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","MPS","1269, 1333","","$0.00","        This research aims to develop a unified framework for                   integrating surveys so as to guide the design of a multi-purpose                survey program.  Algorithms will be developed for optimal                       integration based on more than one objective function and for                   sequential integration of surveys.  The investigators will                      attempt to use the one-to-one correspondence between general                    sampling schemes and unit-by-unit sequential sampling to reduce                 the size of the underlying transportation problem, thereby                      expediting the solution of the integration problem and, in                      certain cases, providing solutions in simple closed form.                               This research addresses the problem of integrating surveys.             The problem of designing a cost-effective sampling program for                  two or more surveys which maximize the overlap between the old                  and the new samples is known to be of considerable practical                    interest in the design of large-scale periodic and multi-purpose                surveys.  Although this problem has been studied for several                    decades, no unified theory for it has yet been developed.  The                  object of this project is to develop a unified framework within                 which to evaluate the integration of surveys.  New algorithms and               computer software will be developed for optimal integration of                  surveys under general sampling schemes.  Optimality properties of               these and other existing methods will be evaluated."
"8703401","Mathematical Sciences: Asymptotic Methods of Statistics","DMS","STATISTICS","07/01/1987","02/19/1988","P. Bhattacharya","CA","University of California-Davis","Continuing grant","Alan Izenman","12/31/1989","$59,344.00","","","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","","$0.00","     The research will deal with asymptotic problems in                         nonparametric inference.  For inference on regression, due to the               possibility of heavy-tailed residual distribution, regression is                viewed in more general way than conditional expectation.                        Regression estimation with homogeneous residuals and various                    generalizations of the well-known Theil estimator will be sought.               Inference on autoregressive processes with heavy-tailed residuals               is part of such problems as well.  Kernel and nearest neighbor                  methods will be used for estimation of a conditional quantile                   function.  The important issue in these methods is the choice of                the smoothing parameter.  Other topics of study will include                    inference on linear regression based on truncated and censored                  data,  use of covariate information via nonparametric regression                in observational studies, methods for parallel regression, non-                 parallel regression and tests of parallelity."
"8706044","Mathematical Sciences: A Robust Approach to the Prediction  Problem","DMS","STATISTICS","07/01/1987","06/09/1987","Paul Horn","OH","University of Cincinnati Main Campus","Standard Grant","Alan Izenman","12/31/1989","$46,461.00","","paul.horn@uc.edu","University Hall, Suite 530","Cincinnati","OH","452210222","5135564358","MPS","1269","","$0.00","        The research from this proposal will create techniques that             will enhance the way in which scientists predict responses from                 limited amounts of data.  The techniques will be robust in nature               thereby precluding unduly restrictive distributional assumptions.               An approach will be taken similar to what has been done with                    confidence intervals:  robust estimates of location and spread                  will be used to create prediction intervals for a function of a                 set of future observations.  Recent advances in the field of                    adaptive estimation will also be used to classify the underlying                distribution.                                                                           This research will go far beyond currently available                    statistical methods for predicting responses.  Statistical                      prediction will be developed for new types of data, types for                   which existing statistical methods have been used even though                   scientists knew they were misfit because no other methods                       existed.  Since the ability to predict is vital to scientists and               engineers, the results from this research will aid in advancing                 the state-of-the-art in a variety of fields."
"8712334","Mathematical Sciences: Statistics and Environmental Factors in Health (SEFH): Statistical Problems in Environmental Measurement and Regulation","DMS","STATISTICS","12/01/1987","05/16/1990","Donald Thomsen","CT","Societal Institute of the Mathematical Sciences (SIMS)","Continuing grant","Alan Izenman","05/31/1991","$141,209.00","","","97 Parish Road South","New Canaan","CT","068404424","2039661008","MPS","1269","","$0.00","This research will focus on five areas of environmetrics: the partitioning of spatial variability, confidence bands for interpolated random functions, quantile estimates for integrated processes, using covariate information for quantile estimation, and environmental indicies for trend estimation. In the first area, the effects of modeling the regression structure in the spatial covariance function will be assessed. In the second area, the problem is to calculate the probability that the entire field is below a specified level, conditional on the observations. The feasibility of approximating this probability with a series expansion of the conditional covariance field that is amenable to the application of Hotelling's inequality will be determined. In the third area, distributional modelling will be used to relate tail probabilities of a monthly integrated concentration process, allowing for the non-exhaustive nature of historical data. In the fourth area, extensions of Pareto tail estimators for the case of jointly distributed vector random variables will be sought. In the fifth area, methods to account for the spatial autocorrelation of separate monitoring station data will be developed. This research is in the general area of spatial statistics and is motivated by statistical problems in environmental measurement and regulation. There is increasing attention in statistics being directed at these problems involving interesting and variable dependencies among the observations. Results in this area are needed for informative policy development and sound regulation practices."
"8706072","Mathematical Sciences: RUI: Partition Study for Successive Difference Analysis","DMS","STATISTICS","06/15/1987","03/08/1988","Neil Schwertman","CA","California State University-Chico","Standard Grant","Alan Izenman","11/30/1989","$31,003.00","","nschwertman@csuchico.edu","1st and Normal Streets","Chico","CA","959290001","9168956116","MPS","1269","9229, 9251","$0.00","This research is to attack the thorny problem of analyzing data that comes from measurements made, or observed, repeatedly but not regularly over time. A novel approach using successive differences is proposed as the basis for analysis assuming piece- wise linearity in the interval of varying lengths. The question to be studied is how to decide where to subdivide the time interval. The traditional methods for analyzing data which is collected repeatedly over time were developed to answer questions posed by the financial community. Financial data is not missing and can be collected at regular fixed time intervals. Because of the structure available in the data driving the early developments in this subfield, the assumptions of a fixed and rigid data structure is built into these analytic approaches. In many fields of research, however, data cannot be collected according to a fixed time schedule as can be done when measuring dollars and cents. Radically new methods of analysis are needed to obtain valid tools for such data, in as much as conclusions are being drawn and decisions based on results from very ad hoc analytical procedures. This research aims to work toward a statistical theory that will be valid for this general framework of scientific investigation."

"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"2413552","Collaborative Research: Statistical Network Integration","DMS","STATISTICS","07/01/2024","06/17/2024","Joshua Cape","WI","University of Wisconsin-Madison","Continuing Grant","Yulia Gel","06/30/2027","$37,235.00","","jrcape@wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","126900","1269","$0.00","This project pursues the contemporary problem of statistical network integration facing scientists, practitioners, and theoreticians. The study of networks and graph-structured data has received growing attention in recent years, motivated by investigations of complex systems throughout the biological and social sciences. Models and methods have been developed to analyze network data objects, often focused on single networks or homogeneous data settings, yet modern available data are increasingly heterogeneous, multi-sample, and multi-modal. Consequently, there is a growing need to leverage data arising from different sources that result in multiple network observations with attributes. This project will develop statistically principled data integration methodologies for neuroimaging studies, which routinely collect multiple subject data across different groups (strains, conditions, edge groups), modalities (functional and diffusion MRI), and brain covariate information (phenotypes, healthy status, gene expression data from brain tissue). The investigators will offer interdisciplinary mentoring opportunities to students participating in the research project and co-teach a workshop based on the proposed research.<br/><br/>The goals of this project are to establish flexible, parsimonious latent space models for network integration and to develop efficient, theoretically justified inference procedures for such models. More specifically, this project will develop latent space models to disentangle common and individual local and global latent features in samples of networks, propose efficient spectral matrix-based methods for data integration, provide high-dimensional structured penalties for dimensionality reduction and regularization in network data, and develop cross-validation methods for multiple network data integration. New theoretical developments spanning concentration inequalities, eigenvector perturbation analysis, and distributional asymptotic results will elucidate the advantages and limitations of these methods in terms of signal aggregation, heterogeneity, and flexibility. Applications of these methodologies to the analysis of multi-subject brain network data will be studied. Emphasis will be on interpretability, computation, and theoretical justification.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2412853","Statistical Learning and Inference for Network Data with Positive and Negative Edges","DMS","STATISTICS","07/01/2024","06/25/2024","Weijing Tang","PA","Carnegie-Mellon University","Continuing Grant","Yulia Gel","06/30/2027","$85,179.00","","weijingt@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","126900","1269, 7794","$0.00","Networks, representing relationships or interactions between subjects in complex systems, are ubiquitous across diverse engineering and scientific disciplines. However, real-world relationships often go beyond simple presence or absence, which poses challenges and necessitates the development of advanced methods. This project focuses on an important class of heterogeneous networks -- ?signed networks?, where relationships can be positive (for example, friendship, alliance, and mutualism) or negative (for example, enmity, disputes, and competition). Such signed relationships are prevalent and exhibit substantially different and unique interaction patterns. This project aims to provide a comprehensive investigation on signed networks through statistical model-based learning and inference, pushing the frontier of our understanding of the role of negative edges in real-world complex systems. The research outcome will stimulate interdisciplinary research and make significant contributions in a broad range of scientific domains, including political science, biochemistry, medicine, genetics, ecology, and business and marketing. The project will support and train STEM workforce members by providing research training opportunities for undergraduate and graduate students.<br/><br/>This project will develop novel statistical methodologies and theories for analyzing signed networks, focusing on the integration of negative relationships in three core problems: (a) understanding the formation mechanism of signed networks guided by fundamental social theories; (b) detecting communities in signed networks by leveraging unique patterns; and (c) learning informative and interpretable embeddings for signed networks to assist downstream analysis. For the first problem, the investigator will provide a valid statistical inference method under novel nonparametric graphon models for signed networks and study real-world evidence of conceptual theories to understand its formation mechanism. For the second problem, new fast community detection methods will be developed under a novel stochastic block model with a hierarchical structure for signed networks, with associated theory emphasizing the positive impacts of negative relationships. Finally, the project will tackle the problem of embedding learning by developing a general latent space framework. The developed methods, algorithms, and theories in this project will be applicable to various practical problems across different domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2337882","CAREER: Perturbation Methods for Quantifying Uncertainties in Machine Learning Models","DMS","STATISTICS","07/01/2024","06/25/2024","Snigdha Panigrahi","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Yulia Gel","06/30/2029","$85,389.00","","psnigdha@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","126900","1045","$0.00","In applied machine learning and statistics, it is common practice to search through many different models before estimating a best model: one that is simple to explain, while still providing good predictive performance. Over the past decade, several methods have emerged which first estimate a model from a range of choices, and then fit the estimated model to extract useful trends and predict future outcomes. However, predictive accuracy, on its own, has limited explanatory value and point estimators with high uncertainties may lead to poor replicability down the line.  Yet, most such models, supervised or unsupervised, lack uncertainties for the related estimators. This project introduces a new class of perturbation methods to quantify uncertainties in machine learning models, with various applications in regression, classification, and dimension reduction. The research plans have three main goals.  The first goal is to develop methods that can be used with different types of data and are not limited to specific models. The second goal is to ensure that the methods can be scaled and applied to decentralized datasets on multiple machines. The last goal is to create versatile methods that can be used with different estimation techniques. An overarching goal is to allow researchers to apply these techniques to various data types and forms, without being constrained by unrealistic assumptions or limited methods for model estimation. The project's educational and outreach plans are closely tied to its research plans. The project will help the PI conduct summer training programs with K-12 outreach, develop new curricula, and broaden participation of underrepresented groups in the field.<br/><br/>This project aims to develop methods for attaching uncertainties to the outputs of model estimation methods. The research agenda is structured into three main aims. In the first aim, the project will introduce distribution-free methods that can quantify uncertainties in a flexible class of semiparametric models. In the second aim, the project will develop distributed methods that use decentralized data from a cluster of nodes to quantify uncertainties in an estimated model. These methods will need only basic, aggregated statistics from each node and will be accompanied by communication-efficient algorithms. In the third aim, the project will focus on developing perturbation methods as a versatile approach for uncertainty quantification that can be used in a wide range of model estimation algorithms, both supervised and unsupervised.  To achieve these aims, the project will use perturbation to exploit a link between the geometric properties of estimators and their underlying probability, and will employ an integrated approach using mathematical statistics, probability theory and optimization. Throughout the project and after its completion, the methodology and open-source software will be applied to improve biomedical decision-making and replication in health studies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413953","Collaborative Research: Statistical Inference for High Dimensional and High Frequency Data: Contiguity, Matrix Decompositions, Uncertainty Quantification","DMS","STATISTICS","07/01/2024","06/21/2024","Lan Zhang","IL","University of Illinois at Chicago","Standard Grant","Jun Zhu","06/30/2027","$155,372.00","","lanzhang@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","126900","","$0.00","To pursue the promise of the big data revolution, the current project is concerned with a particular form of such data, high dimensional high frequency data (HD2), where series of high-dimensional observations can see new data updates in fractions of milliseconds. With technological advances in data collection, HD2 data occurs in medicine (from neuroscience to patient care), finance and economics, geosciences (such as earthquake data), marine science (fishing and shipping), and, of course, in internet data. This research project focuses on how to extract information from HD2 data, and how to turn this data into knowledge. As part of the process, the project develops cutting-edge mathematics and statistical methodology to uncover the dependence structure governing HD2 data. It interfaces with concepts of artificial intelligence. In addition to developing a general theory, the project is concerned with applications to financial data, including risk management, forecasting, and portfolio management. More precise estimators, with improved margins of error, will be useful in all these areas of finance. The results are of interest to main-street investors, regulators and policymakers, and the results are entirely in the public domain. The project will also provide research training opportunities for students.<br/> <br/>In more detail, the project will focus on four linked questions for HD2 data: contiguity, matrix decompositions, uncertainty quantification, and the estimation of spot quantities. The investigators will extend their contiguity theory to the common case where observations have noise, which also permits the use of longer local intervals. Under a contiguous probability, the structure of the observations is often more accessible (frequently Gaussian) in local neighborhoods, facilitating statistical analysis. This is achieved without altering the underlying models. Because the effect of the probability change is quite transparent, this approach also enables more direct uncertainty quantification. To model HD2 data, the investigators will explore time-varying matrix decompositions, including the development of a singular value decomposition (SVD) for high frequency data, as a more direct path to a factor model. Both SVD and principal component analysis (PCA) benefit from contiguity, which eases both the time-varying construction, and uncertainty quantification. The latter is of particular importance not only to set standard errors, but also to determine the trade-offs involved in estimation under longitudinal variation: for example, how many minutes or days are required to estimate a covariance matrix, or singular vectors? The investigators also plan to develop volatility matrices for the drift part of a financial process, and their PCAs. The work on matrix decompositions will also benefit from projected results on spot estimation, which also ties in with contiguity. It is expected that the consequences of the contiguity and the HD2 inference will be transformational, leading to more efficient estimators and better prediction, and that this approach will form a new paradigm for high frequency data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413301","Next-Generation Functional Data Analysis via Machine Learning","DMS","STATISTICS","07/01/2024","06/21/2024","Guanqun Cao","MI","Michigan State University","Standard Grant","Yulia Gel","06/30/2027","$170,000.00","","caoguanq@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","126900","079Z, 1269","$0.00","Classical functional data refer to curves or functions, i.e., the data for each variable are viewed as smooth curves, surfaces, or hypersurfaces evaluated at a finite subset of some interval in one-, two- or three-dimensional Euclidean spaces (for example, some period of time, some range of pixels or voxels, and so on). The independent and identically distributed functional data are sometimes referred to as first-generation functional data. Modern studies from a variety of fields record multiple functional observations according to either multivariate, high-dimensional, multilevel, or time series designs. Such data are called next-generation functional data. This project will elevate the focus on developing machine learning (ML) and artificial intelligence-based methodologies tailored for the next-generation of functional data analysis (FDA). The project will bridge the gap between theoretical knowledge and practical application in ML and FDA. While there have been efforts to integrate ML into the FDA field, these initiatives have predominantly concentrated on handling relatively straightforward formats of functional data. In addition, multiple student research training opportunities will be offered, and high-performance statistics software packages will be developed. These packages will enable researchers from various disciplines to investigate complex relationships that exist among modern functional data.<br/><br/>The widespread utilization of resilient digital devices has led to a notable increase of dependent, high-dimensional, and multi-way functional data. Consequently, the existing toolkit?s efficiency diminishes when tasked with addressing emerging FDA challenges. The PI will introduce: (i) Deep neural networks-based Lasso for dependent FDA; (ii) Optimal multi-way FDA; and (iii) Transfer learning for FDA, and will develop flexible and intelligent ML based estimators, classifiers, clusters, and investigate their statistical properties including the bounds of the prediction errors, convergence rates and minimax excess risk. The proposed methodology will be particularly useful for modeling complex functional data whose underlying structure cannot be properly captured by the existing statistical methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413747","Collaborative Research: NSF MPS/DMS-EPSRC: Stochastic Shape Processes and Inference","DMS","STATISTICS","08/01/2024","06/20/2024","Sebastian Kurtek","OH","Ohio State University","Standard Grant","Yulia Gel","07/31/2027","$199,555.00","","kurtek.1@osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","126900","1269, 7929","$0.00","The intimate link between form, or shape, and function is ubiquitous in science. In biology, for instance, the shapes of biological components are pivotal in understanding patterns of normal behavior and growth; a notable example is protein shape, which contributes to our understanding of protein function and classification. This project, led by a team of investigators from the USA and the UK, will develop ways of modeling how biological and other shapes change with time, using formal statistical frameworks that capture not only the changes themselves, but how these changes vary across objects and populations. This will enable the study of the link between form and function in all its variability. As example applications, the project will develop models for changes in cell morphology and topology during motility and division, and changes in human posture during various activities, facilitating the exploration of scientific questions such as how and why cell division fails, or how to improve human postures in factory tasks. These are proofs of concept, but the methods themselves will have much wider applicability. This project will thus not only progress the science of shape analysis and the specific applications studied; it will have broader downstream impacts on a range of scientific application domains, providing practitioners with general and useful tools.<br/> <br/>While there are several approaches for representing and analyzing static shapes, encompassing curves, surfaces, and complex structures like trees and shape graphs, the statistical modeling and analysis of dynamic shapes has received limited attention. Mathematically, shapes are elements of quotient spaces of nonlinear manifolds, and shape changes can be modeled as stochastic processes, termed shape processes, on these complex spaces. The primary challenges lie in adapting classical modeling concepts to the nonlinear geometry of shape spaces and in developing efficient statistical tools for computation and inference in such very high-dimensional, nonlinear settings. The project consists of three thrust areas, dealing with combinations of discrete and continuous time, and discrete and continuous representations of shape, with a particular emphasis on the issues raised by topology changes. The key idea is to integrate spatiotemporal registration of objects and their evolution into the statistical formulation, rather than treating them as pre-processing steps. This project will specifically add to the current state-of-the-art in topic areas such as stochastic differential equations on shape manifolds, time series models for shapes, shape-based functional data analysis, and modeling and inference on infinite-dimensional shape spaces.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413833","Collaborative Research: Nonparametric Learning in High-Dimensional Survival Analysis for causal inference and sequential decision making","DMS","STATISTICS","07/01/2024","06/18/2024","Shanshan Ding","DE","University of Delaware","Standard Grant","Jun Zhu","06/30/2027","$200,000.00","Wei Qian","sding@udel.edu","220 HULLIHEN HALL","NEWARK","DE","197160099","3028312136","MPS","126900","9150","$0.00","Data with survival outcomes are commonly encountered in real-world applications to capture the time duration until a specific event of interest occurs. Nonparametric learning for high dimensional survival data offers promising avenues in practice because of its ability to capture complex relationships and provide comprehensive insights for diverse problems in medical and business services, where vast covariates and individual metrics are prevalent. This project will significantly advance the methods and theory for nonparametric learning in high-dimensional survival data analysis, with a specific focus on causal inference and sequential decision making problems. The study will be of interest to practitioners in various fields, particularly providing useful methods for medical researchers to discover relevant risk factors, assess causal treatment effects, and utilize personalized treatment strategies in contemporary health sciences. It will also provide useful analytics tools beneficial to financial and related institutions for assessing user credit risks and facilitating informed decisions through personalized services. The theoretical and empirical studies to incorporate complex nonparametric structures in high-dimensional survival analysis, together with their interdisciplinary applications, will create valuable training and research opportunities for graduate and undergraduate students, including those from underrepresented minority groups.<br/><br/>Under flexible nonparametric learning frameworks, new embedding methods and learning algorithms will be developed for high dimensional survival analysis. First, the investigators will develop supervised doubly robust linear embedding and supervised nonlinear manifold learning method for supervised dimension reduction of high dimensional survival data, without imposing stringent model or distributional assumptions. Second, a robust nonparametric learning framework will be established for estimating causal treatment effect for high dimensional survival data that allows the covariate dimension to grow much faster than the sample size. Third, motivated by applications in personalized service, the investigators will develop a new nonparametric multi-stage algorithm for high dimensional censored bandit problems that allows flexibility with potential non-linear decision boundaries with optimal regret guarantees.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413721","New Directions in Bayesian Heterogeneous Data Integration: Methods, Theory and Applications","DMS","STATISTICS","07/01/2024","06/17/2024","Sharmistha Guha","TX","Texas A&M University","Continuing Grant","Tapabrata Maiti","06/30/2027","$49,899.00","","sharmistha@tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","126900","1269","$0.00","As the scientific community is moving into a data-driven era, there is an unprecedented opportunity for the integrative analysis of network and functional data from multiple sources to uncover important scientific insights which might be missing when these data sources are analyzed in isolation. To this end, this project plans to transform the current landscape of integrating network and functional data, leveraging their combined strength for scientific advancements through the development of innovative hierarchical Bayesian statistical models. The proposed work holds transformative promise in vital scientific domains, such as cognitive and motor aging, and neurodegenerative diseases. It will enhance scientific collaborations with neuroscientists using multi-source image data for targeted investigations of key brain regions significant in the study of motor and cognitive aging. Moreover, the proposed research will facilitate the prediction of images, traditionally acquired via costly imaging modalities, utilizing images from more cost-effective alternatives, which is poised to bring about transformative changes in the healthcare economy. The open-source software and educational materials created will be maintained and accessible to a wider audience of statisticians and domain experts. This accessibility is anticipated to foster widespread adoption of these techniques among statisticians and domain scientists. The PI's involvement in conference presentations, specialized course development, curriculum expansion, graduate student mentoring, undergraduate research engagement with a focus on under-represented backgrounds, and provision of short courses will enhance dissemination efforts and encourage diverse utilization of the developed methods.<br/><br/>The proposed project aims to address the urgent need for principled statistical approaches to seamlessly merge information from diverse sources, including modern network and functional data. It challenges the prevailing trend of analyzing individual data sources, which inherently limits the potential for uncovering innovative scientific insights that could arise from integrating multiple sources. Hierarchical Bayesian models are an effective way to capture the complex structures in network and functional data. These models naturally share information among heterogeneous objects, providing comprehensive uncertainty in inference through science-driven joint posterior distributions. Despite the potential advantages of Bayesian perspectives, their widespread adoption is hindered by the lack of theoretical guarantees, computational challenges, and difficulties in specifying robust priors for high-dimensional problems. This proposal will address these limitations by integrating network and functional data, leveraging their combined strength for scientific advancements through the development of innovative hierarchical Bayesian models. Specifically, the project will develop a semi-parametric joint regression framework with network and functional responses, deep network regression with multiple network responses, and Bayesian interpretable deep neural network regression with functional response on network and functional predictors. Besides offering a novel toolbox for multi-source object data integration, the proposed approach will advance the emerging field of interpretable deep learning for object regression by formulating novel and interpretable deep neural networks that combine predictive power with statistical model interpretability. The project will develop Bayesian asymptotic results to guarantee accurate parametric and predictive inference from these models as a function of network and functional features and sample size, an unexplored domain in the Bayesian integration of multi-object data. The proposed methodology will significantly enhance the seamless integration of multimodal neuroimaging data, leading to principled inferences and deeper comprehension of brain structure and function in the study of Alzheimer's disease and aging.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2412895","Statistical Entropic Optimal Transport: Theory, Methods and Applications","DMS","STATISTICS","07/01/2024","06/17/2024","Gonzalo Mena","PA","Carnegie-Mellon University","Continuing Grant","Yong Zeng","06/30/2027","$62,726.00","","gmena@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","126900","075Z, 079Z","$0.00","Optimal transport provides a sensible mathematical framework to address the fundamental statistical question of how a statistician measures the distance between two distributions based on possibly large high-dimensional datasets. A variation of the original transportation problem featuring an entropic penalization has appeared as a more scalable alternative, fueling a wave of new results and successful applications in domains such as genomics, neuroscience, and economics, to name a few.  Despite its practical success and the achieved understanding of some of its fundamental statistical properties, there is still a substantial gap between theory and practice in the entropic optimal transport framework. This project will bridge this gap through new methods grounded in an improved theoretical understanding of entropic optimal transport, potentially generating an innovative set of applications in the life sciences. Graduate students will be trained within the scope of this project.<br/><br/><br/>The core of this project focuses on two intimately related thrusts: first, to develop a foundation for inference in parametric models with entropic optimal transport and to identify the regimes for which this framework is best suited. This includes the problem of model-based clustering in high-dimensional, non-asymptotic regimes and a study of the robustness of entropic-optimal-transport estimators. Second, the PIs will develop statistical applications of entropic optimal transport in Alzheimer?s disease neuropathology and spatial transcriptomics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2412832","Collaborative Research: Statistical Modeling and Inference for Object-valued Time Series","DMS","STATISTICS","07/01/2024","06/17/2024","Changbo Zhu","IN","University of Notre Dame","Continuing Grant","Jun Zhu","06/30/2027","$56,755.00","","czhu4@nd.edu","836 GRACE HALL","NOTRE DAME","IN","465566031","5746317432","MPS","126900","","$0.00","Random objects in general metric spaces have become increasingly common in many fields. For example, the intraday return path of a financial asset, the age-at-death distributions, the annual composition of energy sources, social networks, phylogenetic trees, and EEG scans or MRI fiber tracts of patients can all be viewed as random objects in certain metric spaces. For many endeavors in this area, the data being analyzed is collected with a natural ordering, i.e., the data can be viewed as an object-valued time series. Despite its prevalence in many applied problems, statistical analysis for such time series is still in its early development. A fundamental difficulty of developing statistical techniques is that the spaces where these objects live are nonlinear and commonly used algebraic operations are not applicable. This research project aims to develop new models, methodology and theory for the analysis of object-valued time series. Research results from the project will be disseminated to the relevant scientific communities via publications, conference and seminar presentations. The investigators will jointly mentor a Ph.D. student and involve undergraduate students in the research, as well as offering advanced topic courses to introduce the state-of-the-art techniques in object-valued time series analysis.<br/><br/>The project will develop a systematic body of methods and theory on modeling and inference for object-valued time series. Specifically, the investigators propose to (1) develop a new autoregressive model for distributional time series in Wasserstein geometry and a suite of tools for model estimation, selection and diagnostic checking; (2) develop new specification testing procedures for distributional time series in the one-dimensional Euclidean space; and (3) develop new change-point detection methods to detect distribution shifts in a sequence of object-valued time series. The above three projects tackle several important modeling and inference issues in the analysis of object-valued time series, the investigation of which will lead to innovative methodological and theoretical developments, and lay groundwork for this emerging field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2412403","Robust Extensions to Bayesian Regression Trees for Complex Data","DMS","STATISTICS","08/01/2024","06/17/2024","HENGRUI LUO","TX","William Marsh Rice University","Continuing Grant","Tapabrata Maiti","07/31/2027","$58,710.00","","hl180@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","126900","","$0.00","This project is designed to extend the capabilities of tree-based models within the context of machine learning. Tree-based models allow for decision-making based on clear, interpretable rules and are widely adopted in diagnostic and learning tasks. This project will develop novel methodologies to enhance their robustness. Specifically, the research will integrate deep learning techniques with tree-based statistical methods to create models capable of processing complex, high-dimensional data from medical imaging, healthcare, and AI sectors. These advancements aim to significantly improve prediction and decision-making processes, enhancing efficiency and accuracy across a broad range of applications. The project also prioritizes inclusivity and education by integrating training components, thereby advancing scientific knowledge and disseminating results through publications and presentations.<br/><br/>The proposed research leverages Bayesian hierarchies and transformation techniques on trees to develop models capable of managing complex transformations of input data. These models will be tailored to improve interpretability, scalability, and robustness, overcoming current limitations in non-parametric machine learning applications. The project will utilize hierarchical layered structures, where outputs from one tree serve as inputs to subsequent trees, forming network architectures that enhance precision in modeling complex data patterns and relationships. Bayesian techniques will be employed to effectively quantify uncertainty and create ensembles, providing reliable predictions essential for critical offline prediction and real-time decision-making processes. This initiative aims to develop pipelines and set benchmarks for the application of tree-based models across diverse scientific and engineering disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2412015","Statistical methods for point-process time series","DMS","STATISTICS","07/01/2024","06/17/2024","Daniel Gervini","WI","University of Wisconsin-Milwaukee","Standard Grant","Jun Zhu","06/30/2027","$149,989.00","","gervini@uwm.edu","3203 N DOWNER AVE # 273","MILWAUKEE","WI","532113188","4142294853","MPS","126900","","$0.00","This research project will develop statistical models and inference methods for the analysis of random point processes. Random point processes are events that occur at random in time or space according to certain patterns; this project will provide methods for the discovery and analysis of such patterns. Examples of events that can be modelled as random point processes include cyberattacks on a computer network, earthquakes, crimes in a city, spikes of neural activity in humans and animals, car crashes in a highway, and many others. Therefore, the methods to be developed under this project will find applications in many fields, such as national security, economy, neuroscience and geosciences, among others. The project will also provide training opportunities for graduate and undergraduate students in the field of Data Science.<br/><br/>This project will specifically develop statistical tools for the analysis of time series of point processes, that is, for point processes that are observed repeatedly over time; for example, when the spatial distribution of crime in a city is observed for several days. These tools will include trend estimation methods, autocorrelation estimation methods, and autoregressive models. Research activities in this project include the development of parameter estimation procedures, their implementation in computer programs, the study of theoretical large sample properties of these methods, the study of small sample properties by simulation, and their application to real-data problems. Other activities in this project include educational activities, such as the supervision of Ph.D. and Master's students, and the development of graduate and undergraduate courses in Statistics and Data Science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2412628","Collaborative Research: Partial Priors, Regularization, and Valid & Efficient Probabilistic Structure Learning","DMS","STATISTICS","07/01/2024","06/17/2024","Ryan Martin","NC","North Carolina State University","Standard Grant","Yulia Gel","06/30/2027","$160,000.00","","rgmarti3@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","276950001","9195152444","MPS","126900","1269","$0.00","Modern applications of statistics aim to solve complex scientific problems involving high-dimensional unknowns.  One feature that these applications often share is that the high-dimensional unknown is believed to satisfy a complexity-limiting, low-dimensional structure.  Specifics of the posited low-dimensional structure are mostly unknown, so a statistically interesting and scientifically relevant problem is structure learning, i.e., using data to learn the latent low-dimensional structure.  Because structure learning problems are ubiquitous and reliable uncertainty quantification is imperative, results from this project will have an impact across the biomedical, physical, and social sciences.  In addition, the project will offer multiple opportunities for career development of new generations of statisticians and data scientists.<br/><br/>Frequentist methods focus on data-driven estimation or selection of a candidate structure, but currently there are no general strategies for reliable uncertainty quantification concerning the unknown structure.  Bayesian methods produce a data-dependent probability distribution over the space of structures that can be used for uncertainty quantification, but it comes with no reliability guarantees.  A barrier to progress in reliable uncertainty quantification is the oppositely extreme perspectives: frequentists' anathema of modeling structural/parametric uncertainty versus Bayesians' insistence that such uncertainty always be modeled precisely and probabilistically.  Overcoming this barrier requires a new perspective falling between these two extremes, and this project will develop a new framework that features a more general and flexible perspective on probability, namely, imprecise probability.  Most importantly, this framework will resolve the aforementioned issues by offering new and powerful methods boasting provably reliable uncertainty quantification in structure learning applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2412408","Monitoring time series in structured function spaces","DMS","STATISTICS","07/01/2024","06/14/2024","Piotr Kokoszka","CO","Colorado State University","Standard Grant","Yulia Gel","06/30/2027","$292,362.00","","Piotr.Kokoszka@colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","126900","1269","$0.00","This project aims to develop new mathematical theory and statistical tools that will enable monitoring for changes in complex systems, for example global trade networks. Comprehensive databases containing details of trade between almost all countries are available. Detecting in real time a change in the typical pattern of trade and identifying countries where this change takes place is an important problem. This project will provide statistical methods that will allow making decisions about an emergence of an atypical pattern in a complex system in real time with certain theoretical guarantees. The project will also offer multiple interdisciplinary training opportunities for the next generation of statisticians and data scientists.<br/><br/>The methodology that will be developed is related to sequential change point detection, but is different because the in-control state is estimated rather than assumed. This requires new theoretical developments because it deals with complex infinite dimensional systems, whereas existing mathematical tools apply only to finite-dimensional systems. Panels of structured functions will be considered and methods for on-line identification of components undergoing change will be devised. All methods will be inferential with controlled probabilities of type I errors. Some of the key aspects of the project can be summarized in the following points. First, statistical theory leading  to change point monitoring schemes in infinite dimensional function spaces will be developed. Second, strong  approximations valid in Banach spaces will lead to assumptions not encountered in scalar settings and potentially to different threshold functions. Third, for monitoring of random density functions, the above challenges will be addressed in custom metric spaces. Fourth, since random densities are not observable, the effect of estimation will be incorporated. The new methodology will be applied to viral load measurements, investment portfolios, and global trade data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413952","Collaborative Research: Statistical Inference for High Dimensional and High Frequency Data: Contiguity, Matrix Decompositions, Uncertainty Quantification","DMS","STATISTICS","07/01/2024","06/21/2024","Per Mykland","IL","University of Chicago","Standard Grant","Jun Zhu","06/30/2027","$219,268.00","","mykland@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","126900","","$0.00","To pursue the promise of the big data revolution, the current project is concerned with a particular form of such data, high dimensional high frequency data (HD2), where series of high-dimensional observations can see new data updates in fractions of milliseconds. With technological advances in data collection, HD2 data occurs in medicine (from neuroscience to patient care), finance and economics, geosciences (such as earthquake data), marine science (fishing and shipping), and, of course, in internet data. This research project focuses on how to extract information from HD2 data, and how to turn this data into knowledge. As part of the process, the project develops cutting-edge mathematics and statistical methodology to uncover the dependence structure governing HD2 data.  In addition to developing a general theory, the project is concerned with applications to financial data, including risk management, forecasting, and portfolio management. More precise estimators, with improved margins of error, will be useful in all these areas of finance. The results will be of interest to main-street investors, regulators and policymakers, and the results will be entirely in the public domain. The project will also provide research training opportunities for students.<br/> <br/>In more detail, the project will focus on four linked questions for HD2 data: contiguity, matrix decompositions, uncertainty quantification, and the estimation of spot quantities. The investigators will extend their contiguity theory to the common case where observations have noise, which also permits the use of longer local intervals. Under a contiguous probability, the structure of the observations is often more accessible (frequently Gaussian) in local neighborhoods, facilitating statistical analysis. This is achieved without altering the underlying models. Because the effect of the probability change is quite transparent, this approach also enables more direct uncertainty quantification. To model HD2 data, the investigators will explore time-varying matrix decompositions, including the development of a singular value decomposition (SVD) for high frequency data, as a more direct path to a factor model. Both SVD and principal component analysis (PCA) benefit from contiguity, which eases both the time-varying construction, and uncertainty quantification. The latter is of particular importance not only to set standard errors, but also to determine the trade-offs involved in estimation under longitudinal variation: for example, how many minutes or days are required to estimate a covariance matrix, or singular vectors? The investigators also plan to develop volatility matrices for the drift part of a financial process, and their PCAs. The work on matrix decompositions will also benefit from projected results on spot estimation, which also ties in with contiguity. It is expected that the consequences of the contiguity and the HD2 inference will be transformational, leading to more efficient estimators and better prediction, and that this approach will form a new paradigm for high frequency data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413748","Collaborative Research: NSF MPS/DMS-EPSRC: Stochastic Shape Processes and Inference","DMS","STATISTICS","08/01/2024","06/20/2024","Anuj Srivastava","FL","Florida State University","Standard Grant","Yulia Gel","07/31/2027","$200,000.00","","anuj@stat.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","126900","1269, 7929","$0.00","The intimate link between form, or shape, and function is ubiquitous in science. In biology, for instance, the shapes of biological components are pivotal in understanding patterns of normal behavior and growth; a notable example is protein shape, which contributes to our understanding of protein function and classification. This project, led by a team of investigators from the USA and the UK, will develop ways of modeling how biological and other shapes change with time, using formal statistical frameworks that capture not only the changes themselves, but how these changes vary across objects and populations. This will enable the study of the link between form and function in all its variability. As example applications, the project will develop models for changes in cell morphology and topology during motility and division, and changes in human posture during various activities, facilitating the exploration of scientific questions such as how and why cell division fails, or how to improve human postures in factory tasks. These are proofs of concept, but the methods themselves will have much wider applicability. This project will thus not only progress the science of shape analysis and the specific applications studied; it will have broader downstream impacts on a range of scientific application domains, providing practitioners with general and useful tools.<br/> <br/>While there are several approaches for representing and analyzing static shapes, encompassing curves, surfaces, and complex structures like trees and shape graphs, the statistical modeling and analysis of dynamic shapes has received limited attention. Mathematically, shapes are elements of quotient spaces of nonlinear manifolds, and shape changes can be modeled as stochastic processes, termed shape processes, on these complex spaces. The primary challenges lie in adapting classical modeling concepts to the nonlinear geometry of shape spaces and in developing efficient statistical tools for computation and inference in such very high-dimensional, nonlinear settings. The project consists of three thrust areas, dealing with combinations of discrete and continuous time, and discrete and continuous representations of shape, with a particular emphasis on the issues raised by topology changes. The key idea is to integrate spatiotemporal registration of objects and their evolution into the statistical formulation, rather than treating them as pre-processing steps. This project will specifically add to the current state-of-the-art in topic areas such as stochastic differential equations on shape manifolds, time series models for shapes, shape-based functional data analysis, and modeling and inference on infinite-dimensional shape spaces.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413864","Statistical Properties of Neural Networks","DMS","STATISTICS","07/01/2024","06/18/2024","Sourav Chatterjee","CA","Stanford University","Standard Grant","Tapabrata Maiti","06/30/2027","$225,000.00","","souravc@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","126900","1269","$0.00","Neural networks have revolutionized science and engineering in recent years, but their theoretical properties are still poorly understood. The proposed projects aim to gain a deeper understanding of these theoretical properties, especially the statistical ones. It is a matter of intense debate whether neural networks can ""think"" like humans do, by recognizing logical patterns. The project aims to take a small step towards showing that under ideal conditions, perhaps they can. If successful, this will have impact in a vast range of applications of neural networks. This award includes support and mentoring for graduate students.<br/><br/>In one direction, it is proposed to study features of deep neural networks that distinguish them from classical statistical parametric models. Preliminary results suggest that the lack of identifiability is the differentiating factor. Secondly, it is proposed to investigate the extent to which neural networks may be seen as algorithm approximators, going beyond the classical literature on universal function approximation for neural networks. This perspective may shed light on recent empirical phenomena in neural networks, including the surprising emergent behavior of transformers and large language models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413834","Collaborative Research: Nonparametric Learning in High-Dimensional Survival Analysis for causal inference and sequential decision making","DMS","STATISTICS","07/01/2024","06/18/2024","Zhezhen Jin","NY","Columbia University","Standard Grant","Jun Zhu","06/30/2027","$100,000.00","","zj7@columbia.edu","615 W 131ST ST","NEW YORK","NY","100277922","2128546851","MPS","126900","","$0.00","Data with survival outcomes are commonly encountered in real-world applications to capture the time duration until a specific event of interest occurs. Nonparametric learning for high dimensional survival data offers promising avenues in practice because of its ability to capture complex relationships and provide comprehensive insights for diverse problems in medical and business services, where vast covariates and individual metrics are prevalent. This project will significantly advance the methods and theory for nonparametric learning in high-dimensional survival data analysis, with a specific focus on causal inference and sequential decision making problems. The study will be of interest to practitioners in various fields, particularly providing useful methods for medical researchers to discover relevant risk factors, assess causal treatment effects, and utilize personalized treatment strategies in contemporary health sciences. It will also provide useful analytics tools beneficial to financial and related institutions for assessing user credit risks and facilitating informed decisions through personalized services. The theoretical and empirical studies to incorporate complex nonparametric structures in high-dimensional survival analysis, together with their interdisciplinary applications, will create valuable training and research opportunities for graduate and undergraduate students, including those from underrepresented minority groups.<br/><br/>Under flexible nonparametric learning frameworks, new embedding methods and learning algorithms will be developed for high dimensional survival analysis. First, the investigators will develop supervised doubly robust linear embedding and supervised nonlinear manifold learning method for supervised dimension reduction of high dimensional survival data, without imposing stringent model or distributional assumptions. Second, a robust nonparametric learning framework will be established for estimating causal treatment effect for high dimensional survival data that allows the covariate dimension to grow much faster than the sample size. Third, motivated by applications in personalized service, the investigators will develop a new nonparametric multi-stage algorithm for high dimensional censored bandit problems that allows flexibility with potential non-linear decision boundaries with optimal regret guarantees.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413405","Collaborative Research: Statistical Optimal Transport: Foundation, Computation and Applications","DMS","STATISTICS","07/01/2024","06/18/2024","Kengo Kato","NY","Cornell University","Standard Grant","Yong Zeng","06/30/2027","$160,000.00","","kk976@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","126900","079Z","$0.00","Comparing probability models is a fundamental task in almost every data-enabled problem, and Optimal Transport (OT) offers a powerful and versatile framework to do so. Recent years have witnessed a rapid development of computational OT, which has expanded applications of OT to statistics, including clustering, generative modeling, domain adaptation, distribution-to-distribution regression, dimension reduction, and sampling.  Still, understanding the fundamental strengths and limitations of OT as a statistical tool is much to be desired. This research project aims to fill this important gap by advancing statistical analysis (estimation and inference) and practical approximation of two fundamental notions (average and quantiles) in statistics and machine learning, demonstrated through modern applications for measure-valued data. The project also provides research training opportunities for graduate students. <br/><br/>The award contains three main research projects. The first project will develop a new regularized formulation of the Wasserstein barycenter based on the multi-marginal OT and conduct an in-depth statistical analysis, encompassing sample complexity, limiting distributions, and bootstrap consistency. The second project will establish asymptotic distribution and bootstrap consistency results for linear functionals of OT maps and will study sharp asymptotics for entropically regularized OT maps when regularization parameters tend to zero. Building on the first two projects, the third project explores applications of the OT methodology to two important statistical tasks: dimension reduction and vector quantile regression. The research agenda will develop a novel and computationally efficient principal component method for measure-valued data and a statistically valid duality-based estimator for quantile regression with multivariate responses. The three projects will produce novel technical tools integrated from OT theory, empirical process theory, and partial differential equations, which are essential for OT-based inferential methods and will inspire new applications of OT to measure-valued and multivariate data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413426","Collaborative Research: Synergies between Steins Identities and Reproducing Kernels: Modern Tools for Nonparametric Statistics","DMS","STATISTICS","07/01/2024","06/17/2024","Krishnakumar Balasubramanian","CA","University of California-Davis","Standard Grant","Yong Zeng","06/30/2027","$169,999.00","","kbala@ucdavis.edu","1850 RESEARCH PARK DR STE 300","DAVIS","CA","956186153","5307547700","MPS","126900","079Z","$0.00","The project aims to conduct comprehensive statistical and computational analyses, with the overarching objective of advancing innovative nonparametric data analysis techniques. The methodologies and theories developed are anticipated to push the boundaries of modern nonparametric statistical inference and find applicability in other statistical domains such as nonparametric latent variable models, time series analysis, and sequential nonparametric multiple testing. This project will enhance the interconnections among statistics, machine learning, and computation and provide training opportunities for postdoctoral fellows, graduate students, and undergraduates. <br/><br/>More specifically, the project covers key problems in nonparametric hypothesis testing, intending to establish a robust framework for goodness-of-fit testing for distributions on non-Euclidean domains with unknown normalization constants. The research also delves into nonparametric variational inference, aiming to create a particle-based algorithmic framework with discrete-time guarantees. Furthermore, the project focuses on nonparametric functional regression, with an emphasis on designing minimax optimal estimators using infinite-dimensional Stein's identities. The study also examines the trade-offs between statistics and computation in all the aforementioned methods. The common thread weaving through these endeavors is the synergy between various versions of Stein's identities and reproducing kernels, contributing substantially to the advancement of models, methods, and theories in contemporary nonparametric statistics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413823","Robust and efficient Bayesian inference for misspecified and underspecified models","DMS","STATISTICS","07/01/2024","06/18/2024","Steven MacEachern","OH","Ohio State University","Standard Grant","Tapabrata Maiti","06/30/2027","$300,000.00","Ju Hee Lee, Hang Joon Kim","snm@stat.osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","126900","","$0.00","This research project aims to improve data-driven modelling and decision-making.  Its focus is on the development of Bayesian methods for low-information settings.  Bayesian methods have proven to be tremendously successful in high-information settings where data is of high-quality, the scientific/business background that has generated the data is well-understood, and clear questions are asked.  This project will develop a suite of Bayesian methods designed for low-information settings, including those where (i) the data show particular types of deficiencies, such as a preponderance of outlying or ?bad data?, (ii) a limited conceptual understanding of the phenomenon under study leads to a model that leaves a substantial gap between model and reality, producing a misspecified model or a model that is not fully specified, and (iii) when there is a shortage of data, so that the model captures only a very simplified version of reality.  The new methods will expand the scope of Bayesian applications, with attention to problems in biomedical applications and psychology.  The project will provide training for the next generation of data scientists. <br/> <br/>This project has two main threads.  For the first, the project will develop diagnostics that allow the analyst to assess the adequacy of portions of a posited model.  Such assessments point the way toward elaborations that will bring the model closer to reality, improving the full collection of inferences.  These assessments will also highlight limitations of the model, enabling the analyst to know when to make a decision and when to refrain from making one.  The second thread will explore the use of sample-size adaptive loss functions for modelling and for inference.  Adaptive loss functions have been used by classical statisticians to improve inference by exploiting the bias-variance tradeoff.  This thread will blend adaptivity with Bayesian methods.  This will robustify inference by providing smoother likelihoods for small and moderate sample sizes and by relying on smoother inference functions when the sample size is limited.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413404","Collaborative Research: Statistical Optimal Transport: Foundation, Computation and Applications","DMS","STATISTICS","07/01/2024","06/18/2024","Xiaohui Chen","CA","University of Southern California","Standard Grant","Yong Zeng","06/30/2027","$180,000.00","","xiaohuic@usc.edu","3720 S FLOWER ST FL 3","LOS ANGELES","CA","900890701","2137407762","MPS","126900","079Z","$0.00","Comparing probability models is a fundamental task in almost every data-enabled problem, and Optimal Transport (OT) offers a powerful and versatile framework to do so. Recent years have witnessed a rapid development of computational OT, which has expanded applications of OT to statistics, including clustering, generative modeling, domain adaptation, distribution-to-distribution regression, dimension reduction, and sampling.  Still, understanding the fundamental strengths and limitations of OT as a statistical tool is much to be desired. This research project aims to fill this important gap by advancing statistical analysis (estimation and inference) and practical approximation of two fundamental notions (average and quantiles) in statistics and machine learning, demonstrated through modern applications for measure-valued data. The project also provides research training opportunities for graduate students. <br/><br/>The award contains three main research projects. The first project will develop a new regularized formulation of the Wasserstein barycenter based on the multi-marginal OT and conduct an in-depth statistical analysis, encompassing sample complexity, limiting distributions, and bootstrap consistency. The second project will establish asymptotic distribution and bootstrap consistency results for linear functionals of OT maps and will study sharp asymptotics for entropically regularized OT maps when regularization parameters tend to zero. Building on the first two projects, the third project explores applications of the OT methodology to two important statistical tasks: dimension reduction and vector quantile regression. The research agenda will develop a novel and computationally efficient principal component method for measure-valued data and a statistically valid duality-based estimator for quantile regression with multivariate responses. The three projects will produce novel technical tools integrated from OT theory, empirical process theory, and partial differential equations, which are essential for OT-based inferential methods and will inspire new applications of OT to measure-valued and multivariate data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413553","Collaborative Research: Statistical Network Integration","DMS","STATISTICS","07/01/2024","06/17/2024","Jess Arroyo","TX","Texas A&M University","Continuing Grant","Yulia Gel","06/30/2027","$37,118.00","","jarroyo@tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","126900","1269","$0.00","This project pursues the contemporary problem of statistical network integration facing scientists, practitioners, and theoreticians. The study of networks and graph-structured data has received growing attention in recent years, motivated by investigations of complex systems throughout the biological and social sciences. Models and methods have been developed to analyze network data objects, often focused on single networks or homogeneous data settings, yet modern available data are increasingly heterogeneous, multi-sample, and multi-modal. Consequently, there is a growing need to leverage data arising from different sources that result in multiple network observations with attributes. This project will develop statistically principled data integration methodologies for neuroimaging studies, which routinely collect multiple subject data across different groups (strains, conditions, edge groups), modalities (functional and diffusion MRI), and brain covariate information (phenotypes, healthy status, gene expression data from brain tissue). The investigators will offer interdisciplinary mentoring opportunities to students participating in the research project and co-teach a workshop based on the proposed research.<br/><br/>The goals of this project are to establish flexible, parsimonious latent space models for network integration and to develop efficient, theoretically justified inference procedures for such models. More specifically, this project will develop latent space models to disentangle common and individual local and global latent features in samples of networks, propose efficient spectral matrix-based methods for data integration, provide high-dimensional structured penalties for dimensionality reduction and regularization in network data, and develop cross-validation methods for multiple network data integration. New theoretical developments spanning concentration inequalities, eigenvector perturbation analysis, and distributional asymptotic results will elucidate the advantages and limitations of these methods in terms of signal aggregation, heterogeneity, and flexibility. Applications of these methodologies to the analysis of multi-subject brain network data will be studied. Emphasis will be on interpretability, computation, and theoretical justification.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413484","New Approaches to Sensitivity Analysis in Observational Studies","DMS","STATISTICS","09/01/2024","06/17/2024","Colin Fogarty","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Yong Zeng","08/31/2027","$58,516.00","","fogartyc@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","126900","075Z, 079Z","$0.00","While randomized experiments remain the gold standard for elucidating cause and effect relations, countless societally important ""what-if?"" questions cannot be addressed through clinical trials for a litany of reasons, ranging from ethical concerns to logistical infeasibility. For this reason, observational studies, wherein the assignment of group status to individuals is outside the control of the researcher, often represent the only path forward for inferring causal effects. While observational data are often inexpensive to collect and plentiful, regrettably, they suffer from inescapable biases due to self-selection. In short, associations between group status and outcomes of interest need not reflect causal effects, as the groups being compared might have considerable differences on the basis of factors unavailable for adjustment. This project will develop new methods for sensitivity analysis in observational studies, which answer the question, ""How much-unmeasured confounding would need to exist to overturn a study's finding of a causal effect?"" Quantifying the robustness of observational findings to hidden bias will help frame the debate around the reliability of such studies, allowing researchers to highlight findings that are particularly resilient to lurking variables. This project provides both theoretical guidance on how to extract the most out of a sensitivity analysis and computationally tractable methods for making this guidance actionable. Moreover, when randomized experimentation is possible, the developed methods will help researchers use existing observational studies for hypothesis generation, enabling them to find sets of promising outcome variables whose causal effects may be verified through follow-up experimentation. This award includes support for work with graduate students.<br/><br/>This project develops a new set of statistical methods for conducting sensitivity analyses after matching. These methods aim to overcome shortcomings of the existing approach, conferring computational, theoretical, and practical benefits. The project will provide a new approach to sensitivity analysis after matching called weighting-after-matching. The project will establish computational benefits, theoretical improvements in design sensitivity, and practical improvements in the power of a sensitivity analysis by using weighting-after-matching in lieu of the traditional unweighted approach. The project will also establish novel methods for sensitivity analysis with multiple outcome variables. These innovations will include a scalable multiple testing procedure for observational studies, facilitating exploratory analysis while providing control of the proportion of false discoveries, and methods for sensitivity analysis using weighting-after-matching for testing both sharp null hypotheses of no effect at all and hypotheses on average treatment effects. Finally, the project will establish previously unexplored benefits from using matching and weighting in combination, two modes of adjustment in observational studies commonly viewed as competitors. This will help bridge the divide between matching estimators and weighting estimators in the context of a sensitivity, in so doing providing a natural avenue for theoretical comparisons of these approaches.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2412052","Tackling High Dimensionality for Modern Machine Learning: Theory and Visualization","DMS","STATISTICS","07/01/2024","06/17/2024","Yiqiao Zhong","WI","University of Wisconsin-Madison","Continuing Grant","Tapabrata Maiti","06/30/2027","$67,291.00","","yiqiao.zhong@wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","126900","","$0.00","This research project aims to address the recent challenges of modern machine learning from a statistical perspective. Deep Learning and particularly Large Language Models have the potential to transform our society, yet their scientific underpinning is much less developed. In particular, large-scale black-box models are deployed in applications with little understanding about when they may or may not work as expected. The research is expected to advance the understanding of modern machine learning. It will also provide accessible tools to improve the interpretations and safety of models. This award will involve and support graduate students.<br/><br/>The project is motivated by recent statistical phenomena such as double descent and benign overfitting that involve training a model with many parameters. Motivated by the empirical discoveries in Deep Learning, the project will develop insights into overfitting in imbalanced classification in high dimensions and the effects of reparametrization in contrastive learning. Understanding the generalization errors under overparametrization in practical scenarios, such as imbalanced classification, will likely lead to better practice of reducing overfitting. This project will also explore interpretations for black-box models and complicated methods: (1) in Transformers, high-dimensional embedding vectors are decomposed into interpretable components; (2) in t-SNE, embedding points are assessed by metrics related to map discontinuity. By using classical ideas from factor analysis and leave-one-out, this project will result in new visualization tools for interpretations and diagnosis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413891","Nonparametric estimation in causal inference: optimality in traditional models and newer ones","DMS","STATISTICS","08/01/2024","06/14/2024","Matteo Bonvini","NJ","Rutgers University New Brunswick","Continuing Grant","Yong Zeng","07/31/2027","$59,393.00","","mb1662@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","126900","075Z","$0.00","This project provides new methods for estimating causal effects from non-randomized studies. Quantifying the causal effect of a variable on another one is of fundamental importance in science because it allows for the understanding of what happens if a certain action is taken, e.g., if a drug is prescribed to a patient. When randomized experiments are not feasible, e.g., because of costs or ethical concerns, quantifying the effect of a treatment on an outcome can be very challenging. Roughly, this is because the analysis must ensure that the treated and untreated units are ?comparable,? a condition implied by proper randomization. In these settings, the analyst typically proceeds in two steps: 1) they introduce the key assumptions needed to identify the causal effect, and 2) they specify a model for the distribution of the data, often nonparametric, to accommodate modern, complex datasets, as well as the appropriate estimation strategy. One key difficulty in non-randomized studies is that estimating causal effects typically requires estimating nuisance components of the data distribution that are not of direct interest and that can be potentially quite hard to estimate. Focused on the second part of the analysis, this project aims to design optimal methods for estimating causal effects in different settings. Informally, an optimal estimator converges to the true causal effect ?as quickly as possible? as a function of the sample size and thus leads to the most precise inferences. Establishing optimality has thus two fundamental benefits: 1) it leads to procedures that make the most efficient use of the available data, and 2) it serves as a benchmark against which future methods can be evaluated. In this respect, the theoretical and methodological contributions of this project are expected to lead to substantial improvements in the analysis of data from many domains, such as medicine and the social sciences. The project also aims to offer opportunities for training and mentoring graduate and undergraduate students.<br/><br/>For certain estimands and data structures, the principles of semiparametric efficiency theory can be used to derive optimal estimators. However, they are not directly applicable to causal parameters that are ?non-smooth? or for which the nuisance parts of the data distribution can only be estimated at such slow rates that root-n convergence of the causal effect estimator is not attainable. As part of this project, the Principal Investigator aims to study the optimal estimation of prominent examples of non-smooth parameters, such as causal effects defined by continuous treatments. Furthermore, this project will consider optimal estimation of ?smooth? parameters, such as certain average causal effects, in newer nonparametric models for which relatively fast rates of convergence are possible, even if certain components of the data distribution can only be estimated at very slow rates. In doing so, the project aims to propose new techniques for reducing the detrimental effect of the nuisance estimators? bias on the quality of the causal effect estimator. It also aims to design and implement inferential procedures for the challenging settings considered, thereby enhancing the adoption of the methods proposed in practice.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413557","Collaborative Research: Systemic Shock Inference for High-Frequency Data","DMS","STATISTICS","07/01/2024","06/14/2024","Jose Figueroa-Lopez","MO","Washington University","Standard Grant","Jun Zhu","06/30/2027","$99,957.00","","figueroa@math.wustl.edu","ONE BROOKINGS DR","SAINT LOUIS","MO","63110","3147474134","MPS","126900","","$0.00","Unexpected ?shocks,? or abrupt deviations from periods of stability naturally occur in time-dependent data-generating mechanisms across a variety of disciplines.  Examples include crashes in stock markets, flurries of activity on social media following news events, and changes in animal migratory patterns during global weather events, among countless others.  Reliable detection and statistical analysis of shock events is crucial in applications, as shock inference can provide scientists deeper understanding of large systems of time-dependent variables, helping to mitigate risk and manage uncertainty.  When large systems of time-dependent variables are observed at high sampling frequencies, information at fine timescales can reveal hidden connections and provide insights into the collective uncertainty shared by an entire system.  High-frequency observations of such systems appear in econometrics, climatology, statistical physics, and many other areas of empirical science that can benefit from reliable inference of shock events.  This project will develop new statistical techniques for the both the detection and analysis of shocks in large systems of time-dependent variables observed at high temporal sampling frequencies. The project will also involve mentoring students, organizing workshops, and promoting diversity in STEM.  <br/><br/>The investigators will study shock inference problems in a variety of settings in high dimensions.  Special focus will be paid to semi-parametric high-frequency models that display a factor structure.  Detection based on time-localized principal component analysis and related techniques will be explored, with a goal towards accounting for shock events that impact a large number of component series in a possibly asynchronous manner.  Time-localized bootstrapping methods will also be considered for feasible testing frameworks for quantifying the system-level impact of shocks.  Complimentary lines of inquiry will concern estimation of jump behavior in high-frequency models in multivariate contexts and time-localized clustering methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2412629","Collaborative Research: Partial Priors, Regularization, and Valid & Efficient Probabilistic Structure Learning","DMS","STATISTICS","07/01/2024","06/17/2024","Chuanhai Liu","IN","Purdue University","Standard Grant","Yulia Gel","06/30/2027","$160,000.00","","chuanhai@purdue.edu","2550 NORTHWESTERN AVE # 1100","WEST LAFAYETTE","IN","479061332","7654941055","MPS","126900","","$0.00","Modern applications of statistics aim to solve complex scientific problems involving high-dimensional unknowns.  One feature that these applications often share is that the high-dimensional unknown is believed to satisfy a complexity-limiting, low-dimensional structure.  Specifics of the posited low-dimensional structure are mostly unknown, so a statistically interesting and scientifically relevant problem is structure learning, i.e., using data to learn the latent low-dimensional structure.  Because structure learning problems are ubiquitous and reliable uncertainty quantification is imperative, results from this project will have an impact across the biomedical, physical, and social sciences.  In addition, the project will offer multiple opportunities for career development of new generations of statisticians and data scientists.<br/><br/>Frequentist methods focus on data-driven estimation or selection of a candidate structure, but currently there are no general strategies for reliable uncertainty quantification concerning the unknown structure.  Bayesian methods produce a data-dependent probability distribution over the space of structures that can be used for uncertainty quantification, but it comes with no reliability guarantees.  A barrier to progress in reliable uncertainty quantification is the oppositely extreme perspectives: frequentists' anathema of modeling structural/parametric uncertainty versus Bayesians' insistence that such uncertainty always be modeled precisely and probabilistically.  Overcoming this barrier requires a new perspective falling between these two extremes, and this project will develop a new framework that features a more general and flexible perspective on probability, namely, imprecise probability.  Most importantly, this framework will resolve the aforementioned issues by offering new and powerful methods boasting provably reliable uncertainty quantification in structure learning applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413425","Collaborative Research: Synergies between Steins Identities and Reproducing Kernels: Modern Tools for Nonparametric Statistics","DMS","STATISTICS","07/01/2024","06/17/2024","Bharath Sriperumbudur","PA","Pennsylvania State Univ University Park","Standard Grant","Yong Zeng","06/30/2027","$179,999.00","","bks18@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","126900","079Z","$0.00","The project aims to conduct comprehensive statistical and computational analyses, with the overarching objective of advancing innovative nonparametric data analysis techniques. The methodologies and theories developed are anticipated to push the boundaries of modern nonparametric statistical inference and find applicability in other statistical domains such as nonparametric latent variable models, time series analysis, and sequential nonparametric multiple testing. This project will enhance the interconnections among statistics, machine learning, and computation and provide training opportunities for postdoctoral fellows, graduate students, and undergraduates. <br/><br/>More specifically, the project covers key problems in nonparametric hypothesis testing, intending to establish a robust framework for goodness-of-fit testing for distributions on non-Euclidean domains with unknown normalization constants. The research also delves into nonparametric variational inference, aiming to create a particle-based algorithmic framework with discrete-time guarantees. Furthermore, the project focuses on nonparametric functional regression, with an emphasis on designing minimax optimal estimators using infinite-dimensional Stein's identities. The study also examines the trade-offs between statistics and computation in all the aforementioned methods. The common thread weaving through these endeavors is the synergy between various versions of Stein's identities and reproducing kernels, contributing substantially to the advancement of models, methods, and theories in contemporary nonparametric statistics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2412833","Collaborative Research: Statistical Modeling and Inference for Object-valued Time Series","DMS","STATISTICS","07/01/2024","06/17/2024","Xiaofeng Shao","IL","University of Illinois at Urbana-Champaign","Standard Grant","Jun Zhu","06/30/2027","$174,997.00","","xshao@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","126900","","$0.00","Random objects in general metric spaces have become increasingly common in many fields. For example, the intraday return path of a financial asset, the age-at-death distributions, the annual composition of energy sources, social networks, phylogenetic trees, and EEG scans or MRI fiber tracts of patients can all be viewed as random objects in certain metric spaces. For many endeavors in this area, the data being analyzed is collected with a natural ordering, i.e., the data can be viewed as an object-valued time series. Despite its prevalence in many applied problems, statistical analysis for such time series is still in its early development. A fundamental difficulty of developing statistical techniques is that the spaces where these objects live are nonlinear and commonly used algebraic operations are not applicable. This research project aims to develop new models, methodology and theory for the analysis of object-valued time series. Research results from the project will be disseminated to the relevant scientific communities via publications, conference and seminar presentations. The investigators will jointly mentor a Ph.D. student and involve undergraduate students in the research, as well as offering advanced topic courses to introduce the state-of-the-art techniques in object-valued time series analysis.<br/><br/>The project will develop a systematic body of methods and theory on modeling and inference for object-valued time series. Specifically, the investigators propose to (1) develop a new autoregressive model for distributional time series in Wasserstein geometry and a suite of tools for model estimation, selection and diagnostic checking; (2) develop new specification testing procedures for distributional time series in the one-dimensional Euclidean space; and (3) develop new change-point detection methods to detect distribution shifts in a sequence of object-valued time series. The above three projects tackle several important modeling and inference issues in the analysis of object-valued time series, the investigation of which will lead to innovative methodological and theoretical developments, and lay groundwork for this emerging field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413558","Collaborative Research: Systemic Shock Inference for High-Frequency Data","DMS","STATISTICS","07/01/2024","06/14/2024","Benjamin Boniece","PA","Drexel University","Continuing Grant","Jun Zhu","06/30/2027","$26,626.00","","cooper.boniece@drexel.edu","3141 CHESTNUT ST","PHILADELPHIA","PA","191042875","2158956342","MPS","126900","","$0.00","Unexpected ?shocks,? or abrupt deviations from periods of stability naturally occur in time-dependent data-generating mechanisms across a variety of disciplines.  Examples include crashes in stock markets, flurries of activity on social media following news events, and changes in animal migratory patterns during global weather events, among countless others.  Reliable detection and statistical analysis of shock events is crucial in applications, as shock inference can provide scientists deeper understanding of large systems of time-dependent variables, helping to mitigate risk and manage uncertainty.  When large systems of time-dependent variables are observed at high sampling frequencies, information at fine timescales can reveal hidden connections and provide insights into the collective uncertainty shared by an entire system.  High-frequency observations of such systems appear in econometrics, climatology, statistical physics, and many other areas of empirical science that can benefit from reliable inference of shock events.  This project will develop new statistical techniques for the both the detection and analysis of shocks in large systems of time-dependent variables observed at high temporal sampling frequencies. The project will also involve mentoring students, organizing workshops, and promoting diversity in STEM.  <br/><br/>The investigators will study shock inference problems in a variety of settings in high dimensions.  Special focus will be paid to semi-parametric high-frequency models that display a factor structure.  Detection based on time-localized principal component analysis and related techniques will be explored, with a goal towards accounting for shock events that impact a large number of component series in a possibly asynchronous manner.  Time-localized bootstrapping methods will also be considered for feasible testing frameworks for quantifying the system-level impact of shocks.  Complimentary lines of inquiry will concern estimation of jump behavior in high-frequency models in multivariate contexts and time-localized clustering methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2415067","Collaborative Research: New Regression Models and Methods for Studying Multiple Categorical  Responses","DMS","STATISTICS","01/15/2024","01/26/2024","Aaron Molstad","MN","University of Minnesota-Twin Cities","Continuing Grant","Yong Zeng","08/31/2025","$67,380.00","","amolstad@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","126900","079Z","$0.00","In many areas of scientific study including bioengineering, epidemiology, genomics, and neuroscience, an important task is to model the relationship between multiple categorical outcomes and a large number of predictors. In cancer research, for example, it is crucial to model whether a patient has cancer of subtype A, B, or C and high or low mortality risk given the expression of thousands of genes. However, existing statistical methods either cannot be applied, fail to capture the complex relationships between the response variables, or lead to models that are difficult to interpret and thus, yield little scientific insight. The PIs address this deficiency by developing multiple new statistical methods. For each new method, the PIs will provide theoretical justifications and fast computational algorithms. Along with graduate and undergraduate students, the PIs will also create publicly available software that will enable applications across both academia and industry.<br/><br/>This project aims to address a fundamental problem in multivariate categorical data analysis: how to parsimoniously model the joint probability mass function of many categorical random variables given a common set of high-dimensional predictors. The PIs will tackle this problem by using emerging technologies on tensor decompositions, dimension reduction, and both convex and non-convex optimization. The project focuses on three research directions: (1) a latent variable approach for the low-rank decomposition of a conditional probability tensor; (2) a new overlapping convex penalty for intrinsic dimension reduction in a multivariate generalized linear regression framework; and (3) a direct non-convex optimization-based approach for low-rank tensor regression utilizing explicit rank constraints on the Tucker tensor decomposition. Unlike the approach of regressing each (univariate) categorical response on the predictors separately, the new models and methods will allow practitioners to characterize the complex and often interesting dependencies between the responses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413135","Trusted selective and predictive inference tools for modern data-driven applications","DMS","STATISTICS","07/01/2024","06/12/2024","Zhimei Ren","PA","University of Pennsylvania","Continuing Grant","Yong Zeng","06/30/2027","$55,409.00","","zren@wharton.upenn.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","126900","079Z","$0.00","Recent years have witnessed numerous exciting opportunities brought by the abundance of data and powerful machine learning algorithms. Along with the opportunities, it has been recognized that the complex nature of modern data and models makes them hard to analyze: the data can be high-dimensional and correlated in complicated ways, while the models are often of unprecedented sizes and black-box to the users. There is, therefore, a need for new statistical methodologies for understanding such data and models. This research project aims to develop new statistical tools for modern data-driven applications that provide rigorous theoretical guarantees under minimal assumptions. The results of this project will have a broad impact on applications, including genetics, personalized health, and online marketing. Open-source software for the newly developed methodologies will be released. This project will also contribute to the academic training of undergraduate and graduate students through their involvement in the research tasks.<br/> <br/>This research program has three specific aims. The first aim is to develop powerful multiple hypothesis testing procedures for dependent and high-dimensional data. The second aim is to provide valid statistical inference tools for adaptively collected data, where classical statistical inference tools often fail to deliver the promised guarantee. The third aim is devoted to distribution-free predictive inference in the face of distribution shift, with the focus on characterizing the distribution shift in different problems and developing distribution-free predictive inference tools that are robust to the corresponding distribution shift.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413679","Uncertainty quantification for iterative algorithms","DMS","STATISTICS","07/01/2024","06/14/2024","Pierre Bellec","NJ","Rutgers University New Brunswick","Continuing Grant","Tapabrata Maiti","06/30/2027","$74,675.00","","pcb71@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","126900","1269","$0.00","Learning from data is performed by iterative algorithms throughout statistics and machine learning. Iterative learning algorithms find models that best fit the labeled training data, in order to make predictions on unseen and unlabeled data. These iterative algorithms are run in computers until an optimality criterion is met or exhausting computational resources. Empirical evidence suggests that terminating algorithms early, before convergence, enhances prediction performance on unseen data in certain learning scenarios.  The project aims to develop theory to explain this early-stopping phenomenon, as well as practical methodologies to determine the optimal early iteration for best predictions on unseen data and saving computational resources. The research will involve students at both undergraduate and graduate levels.<br/><br/>Modern estimators in statistics and machine learning are ubiquitously defined as solutions to optimization problems, whether convex or nonconvex.  These optimization problems are solved iteratively using gradient descent and its accelerated variants, or proximal iterative schemes if the objective function is non-smooth. On the other hand, inferential methodologies such as debiasing, the construction of confidence intervals in regression models or estimation of the generalization error have so far been developed assuming algorithm convergence.  This research will bridge this gap and develop inferential methodologies for algorithm iterates, including at a constant number of iterations or far from convergence. The project aims to develop estimators of the generalization error of the iterates, with application to early stopping to minimize population error.  The project further plans to equip iterative algorithms with confidence intervals and hypothesis tests valid throughout the trajectory, allowing practitioners to perform hypothesis rejections and discoveries at early iterations, without relying on algorithm convergence.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413089","Learning Operators Between Infinite-Dimensional Hilbert Spaces","DMS","STATISTICS","09/01/2024","06/05/2024","Ambuj Tewari","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yulia Gel","08/31/2027","$250,000.00","","tewaria@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","126900","1269, 9263","$0.00","The use of differential equations is widespread in various fields of science and engineering. Differential equations are used to model and understand natural phenomena and to design engineered systems. However, differential equations can be quite challenging to solve numerically on a computer. Recently, there has been substantial interest in using statistical machine learning to develop fast approximations that can give answers with the accuracy of classical solvers and also run much faster. This emerging area leads to novel statistical questions about the limits and possibilities of statistical learning where the inputs and outputs are both functions. In mathematics, a mapping that takes functions as inputs and also produces functions as outputs is called an operator. This project aims to develop new theoretical tools and algorithm design techniques for this emerging area of operator learning. <br/><br/><br/>This project has several interrelated themes. First, it will develop a deep understanding of learning linear operators. Second, the project will consider specific classes of nonlinear operators that are most likely to lead to practical successes: classes defined by operator-valued kernels and neural operators. Third, this research project will consider natural families of estimators such as empirical risk minimization and its regularized versions. Fourth, the investigator will extend the results in previous themes to more general models of learning particularly adversarial online learning which views learning as a sequential game between Nature and Learner. The project will offer multiple training and mentoring opportunities for a future generation of statisticians.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2424045","Conference: 43rd Conference of Texas Statisticians","DMS","STATISTICS","06/15/2024","06/05/2024","Sunil Mathur","TX","The Methodist Hospital Research Institute","Standard Grant","Tapabrata Maiti","05/31/2025","$21,000.00","","smathur2@houstonmethodist.org","6670 BERTNER AVE","HOUSTON","TX","770302602","7134417885","MPS","126900","7556","$0.00","The 43rd Annual Conference of Texas Statisticians (COTS): AI, Machine Learning, and Other Related Statistical Techniques with Applications in Healthcare, scheduled for May 9-10, 2024, will be hosted at the Houston Methodist Research Institute (HMRI), situated at 6670 Bertner Ave, Houston, TX 77030, USA. Artificial intelligence (AI) and machine learning (ML) represent a transformative force across various industries, promising advancements in fields such as medical diagnosis, national security, and crime prevention. These technologies harness the power of data to generate models capable of learning, making decisions, and predicting outcomes. Over time, they refine and adapt, becoming more effective and versatile. However, the efficacy of AI and ML relies heavily on the principles and methodologies provided by statistical science. Statistical techniques underpin the construction of robust models in AI and ML, enabling the interpretation of their outputs. This synergy between AI, ML, and statistical science forms the backbone of cutting-edge advancements in data-driven decision-making. COTS, dedicated to AI, ML, and related statistical techniques serves as a crucial platform for statisticians to exchange insights and forge collaborative opportunities. Such gatherings drive innovation, pushing the boundaries of what is achievable with the integration of AI, ML, and statistical science.<br/><br/>COTS 2024 is dedicated to advancing the frontiers of AI and ML and related statistical techniques, particularly within healthcare. Experts will explore how AI can revolutionize treatment methodologies by harnessing patient-specific data, genetic profiles, and medical histories to tailor treatment plans with unprecedented precision, optimizing efficacy while minimizing adverse effects. COTS 2024 is committed to assembling a diverse array of leading experts, each bringing unique perspectives and expertise to the table. By fostering a robust scientific forum, the conference aims to facilitate rigorous discussions on the most recent research discoveries, spanning from fundamental research to practical applications aimed at enhancing human health and well-being. Moreover, COTS 2024 recognizes the importance of nurturing collaboration and mentorship within the scientific community. Through various networking opportunities and interactive sessions, the conference seeks to bridge the gap between junior and senior researchers, fostering an environment where knowledge exchange flourishes and innovative ideas take root and support underrepresented groups and minorities. In essence, COTS 2024 is not just a conference; it's a catalyst for transformative change, where cutting-edge research converges with real-world applications to shape the future of healthcare and beyond. Moreover, in collaboration with NSF, COTS 2024 is committed to uplifting underrepresented groups and minorities, ensuring that the benefits of progress are inclusive and accessible to all. NSF?s funding for COTS 2024 has facilitated the integration of diverse perspectives and expertise, driving forward the mission of COTS 2024 to enact meaningful change in healthcare and beyond, while prioritizing the empowerment of underrepresented groups and minorities.<br/>Conference website: https://learn.houstonmethodist.org/AI-2024#group-tabs-node-course-default1<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413718","Computer-intensive methods for dependent and complex data","DMS","STATISTICS","06/15/2024","06/05/2024","Dimitris Politis","CA","University of California-San Diego","Standard Grant","Jun Zhu","05/31/2027","$300,000.00","","dpolitis@ucsd.edu","9500 GILMAN DR","LA JOLLA","CA","920930021","8585344896","MPS","126900","079Z","$0.00","Ever since the fundamental recognition of the potential role of the computer in modern statistics, the bootstrap and other computer-intensive statistical methods have been developed extensively for inference with independent data. Such methods are even more important in the context of dependent data where the distribution theory for estimators and test statistics may be difficult or impractical to obtain. Furthermore, the recent information explosion has resulted in datasets of unprecedented size that call for flexible, and by necessity computer-intensive, methods of data analysis. Time series analysis in particular is vital in many diverse scientific disciplines. As a consequence of the development of efficient and robust methods for the statistical analysis of dependent data, more accurate and reliable inferences may be drawn from datasets of practical importance resulting in appreciable benefits to the society.  Examples include data from meteorology/atmospheric science (e.g. climate data), economics (e.g. stock market returns), biostatistics (e.g. fMRI data), and bioinformatics (e.g. genetics and microarray data). The project also involves developing curriculum, mentoring undergraduate students' research, supervising graduate students, and developing open-source software, organizing workshops.<br/><br/>The project focuses on the development of methods of inference for the analysis of dependent and otherwise complex data without relying on unrealistic and/or unverifiable model assumptions. In particular: (a) Subsampling and resampling for big data will be studied, including the notion of scalable subagging applied to deep learning to improve both speed as well as accuracy of estimation; (b) Central limit theorem for the median of a triangular array of dependent data will be proved with application to median-of-means and robust scalable subagging; (c) Model-free bootstrap will be studied and compared to conformal prediction in nonparametric regression; (d) A novel class of nonstationary dependent errors will be introduced with application to fitting large autoregressive (AR) models to nonstationary time series; (e) Markov resampling and linear process bootstrap will be developed for stationary random fields; (f) Skip-sampling of discrete Fourier transform ordinates will be introduced and compared to the traditional frequency domain bootstrap for stationary time series; (g) Smoothing estimators of time-varying covariance matrices will be constructed for locally stationary multivariate time series; (h) Bootstrap for time series with a seasonal component will be further developed; and (i) Multi-step ahead point and interval predictors will be constructed for nonlinear autoregressions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413074","False Discovery Control in Non-Standard Settings","DMS","STATISTICS","07/01/2024","06/05/2024","Armeen Taeb","WA","University of Washington","Continuing Grant","Yong Zeng","06/30/2027","$74,901.00","","ataeb@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981951016","2065434043","MPS","126900","","$0.00","Controlling the false positive error in model selection is a prominent paradigm for gathering evidence in data-driven science.  In model selection problems such as variable selection and graph estimation, models are characterized by an underlying Boolean structure, such as the presence or absence of a variable or an edge.  Therefore, false positive error or false negative error can be conveniently specified as the number of variables/edges that are incorrectly included or excluded in an estimated model.  However, the increasing complexity of modern datasets has been accompanied by the use of sophisticated modeling paradigms in which defining false positive error is a significant challenge.  For example, models specified by structures such as partitions (for clustering), permutations (for ranking), directed acyclic graphs (for causal inference), or subspaces (for principal components analysis) are not characterized by a simple Boolean logical structure, which leads to difficulties with formalizing and controlling false positive error. A new perspective is needed to provide reliable inference in modern data analysis. The methods developed in this project have the potential to impact a wide range of fields as varied as image analysis, geosciences, computational genomics, and many others. The research will engage both graduate and undergraduate students and will be disseminated to a broader audience through the development of new courses. <br/><br/>In this project, the PI develops a generic framework to organize classes of models as partially ordered sets (posets), which leads to systematic approaches for defining natural generalizations of false positive error and methodology for controlling this error. The project aims to use the poset framework to address the following questions: what attributes of the poset structure determine the power and computational complexities of false positive error controlling procedures? How can we exploit specific structures in posets to design powerful model selection methods? How do we provide false discovery rate guarantees over posets? Can we utilize the framework for learning rooted phylogenetic trees and performing highly correlated variable selection?<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413715","Wasserstein guided nonparametric Bayes","DMS","STATISTICS","07/01/2024","05/30/2024","Debdeep Pati","TX","Texas A&M University","Standard Grant","Tapabrata Maiti","06/30/2027","$299,669.00","Anirban Bhattacharya","debdeep@stat.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","126900","1269","$0.00","Stochastic generative models are a cornerstone of applied statistical modeling and inference. A generative model is an abstraction, and often a simplification, of a data generating mechanism using probabilistic tools, where specific features of interest regarding the generating mechanism are encapsulated into parameters of the generative model. Bayesian statistical inference is a popular statistical paradigm for combining such generative models for data with prior information about model parameters in a principled fashion to perform statistical inference on the unknown parameters. Some of the salient aspects behind the tremendous growth in popularity of Bayesian inference  include principled incorporation of domain information, an in-built penalty for model complexity allowing automatic model selection, and facilitating borrowing of information across different domains via hierarchical modeling. However, being inherently model-based, Bayesian statistics is intrinsically susceptible to departures from the postulated generative model.  Through this project, the investigators will explore and develop new statistical methodology for performing Bayesian inference allowing flexible departures from the generative model under consideration. A major focus will be the user-friendliness of the proposed approaches, circumventing the need for a user to explicitly build probabilistic models of increasing richness. The research will be disseminated through articles at prominent avenues and research presentations. Additionally, software packages for the methods developed will be made available publicly. The investigators are committed to enhancing the pedagogical component of the proposal through advising students and developing graduate and undergraduate topic courses.<br/><br/>Flexible nonparametric Bayesian methods have gained in popularity to address perceived issues of traditional Bayesian modeling regarding model-misspecification. The last thirty years have seen a proliferation of such methods, both in mainstream statistics as well as the machine learning community, as we continue to encounter increasing levels of complexities in modern datasets. However, nonparametric Bayesian methods can be challenging to implement as well as interpret. Furthermore, in many applications, the targets of interest are quite simple and it is essentially futile to model all aspects of the data. The fundamental aim of the proposed research is to develop a flexible Bayesian non-parametric approach that retains the generative modeling aspect of traditional parametric Bayesian modeling while avoiding a complete probabilistic specification of the data generating mechanism as typically performed in nonparametric Bayesian modeling. This will be performed by defining a modified likelihood function, leveraging ideas from the empirical likelihood literature as well as optimal transport theory, that centers around a user-specified parametric family of densities. An automated calibration procedure will be developed to control the extent of centering around the parametric model. The investigators will offer a firm theoretical underpinning of the proposed procedure and develop computationally efficient algorithms to carry out inferential tasks. The developed methods will be applied to scientific learning problems in neuroscience and nuclear physics to allow departures from existing scientific models in situations where their operating characteristics are less understood.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413858","Analysis of Non-Gaussian Tensor Time Series","DMS","STATISTICS","08/01/2024","05/29/2024","Rong Chen","NJ","Rutgers University New Brunswick","Standard Grant","Jun Zhu","07/31/2027","$300,001.00","Han Xiao","rongchen@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","126900","","$0.00","The project is motivated by problems such as geo-political event prediction, crime data analysis, and modeling transportation and trading networks. Many data from these applications share three common and salient features: (i) they can be represented as tensors (multi-dimensional arrays), (ii) they are generated over time and exhibit dynamic relationship, and (iii) the values of the data are binary, counts, proportions etc. Such diverse data types, which are referred to as the non-Gaussian tensor time series, call urgently for the development of more adaptable analytical tools. The investigators introduce a general, flexible and efficient framework for the modeling, interpretation and prediction for such non-Gaussian tensor time series through dynamic factor models. The dynamic factor models contain an observation layer specified for the generation of the non-Gaussian observations, and a latent layer to account for the dynamic and concurrent dependence. They are capable of extracting dynamic information, enhancing comprehension of underlying mechanisms, and generating reliable forecasts, and are therefore poised to assist organizations and policymakers in making well-informed decisions. The project advances education through the research training of both undergraduate and graduate students, as well as its incorporation into special topic courses. The project is also committed to promoting diversity and inclusion in STEM fields, and actively seeks to recruit students from groups that are historically under-represented in science and engineering.<br/><br/>A novel dynamic matrix factor model for non-Gaussian data marks a significant advancement in modeling large and complex dependent data. These models effectively tackle challenges arising from the size, complexity, and discreteness of the data. More specifically, the dynamic is introduced in the hidden layer through the factor structure with tensor Tucker or CP decompositions, and a nonlinear or generalized linear model (Poisson, negative binomial, Gamma, zero-inflated, etc) is employed in the observation layer for the generation of the observations. An autocovariance-based approach is used for the estimation of the loading matrices and vectors, which takes advantage of the temporal dependence to reduce the bias. A tensor autoregressive model is imposed on the factors to enable the forecasting. For the estimation of the autoregressive model, again an autocovariance-based procedure is used to mitigate the impact of the estimation error of factors. This approach's computational efficiency is particularly well suited for handling big and complex data. The methodology and theoretical analysis lay the groundwork for applying the moment method in broader models and applications. Furthermore, the methodology underscores the significance of the ""blessing of dependence"" phenomenon, demonstrating how the temporal dependence can be utilized to enhance/reduce the signal/noise to achieve more accurate estimation, compared to the corresponding works under the IID setting. It is noteworthy that the framework accommodates extensions to more general distributions, and the developed methodology has broad applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413327","Scalable and Generalizable Inference for Network Data","DMS","STATISTICS","07/01/2024","05/22/2024","Srijan Sengupta","NC","North Carolina State University","Standard Grant","Yulia Gel","06/30/2027","$175,000.00","","ssengup2@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","276950001","9195152444","MPS","126900","1269","$0.00","In an era where digital networks underpin crucial aspects of society and technology, from healthcare systems and social media to environmental monitoring and national security, understanding these complex networks is more important than ever. This project will advance the statistical analysis of complex and massive digital networks by addressing the challenges of accurately reflecting the diverse realities of networks and efficiently managing their vast scales. These advancements are expected to enable more reliable anomaly detection and robust analysis of large-scale networks through the introduction of novel statistical methods and computational tools. Furthermore, the project's educational and interdisciplinary initiatives are designed to equip the next generation of scientists, ensuring sustained impact across disciplines and contributing to the public good through engagement and nonprofit collaborations.<br/><br/>Addressing the limitations of traditional homogeneous models and computational inefficiencies, this project will contribute new methods to enhance statistical generalizability and scalability in network inference. The introduction of these flexible methodologies aims to improve the detection of anomalous motifs and the analysis of phenomena like the small-world property, core-periphery structures, and co-spectral graphs across diverse and complex network models. To overcome scalability challenges, the project introduces two novel algorithms, Predictive Subsampling (PredSub) and Aggregative Subsampling with Common Overlap (ASCO), designed to augment existing statistical methods for applicability to large datasets. These solutions will undergo thorough theoretical analysis and empirical validation, leveraging collaborations across fields such as epidemiology and digital health. With its potential to advance network data inference through methodological innovations and broad applications, the project promises significant intellectual merit and broader impacts, including educational programs, public engagement, and software development, fostering a multidisciplinary approach to solving contemporary scientific challenges.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2406154","Conference: Workshop on Translational Research on Data Heterogeneity","DMS","STATISTICS","03/01/2024","01/18/2024","Xuming He","MO","Washington University","Standard Grant","Tapabrata Maiti","02/28/2025","$16,000.00","Lan Wang","hex@wustl.edu","ONE BROOKINGS DR","SAINT LOUIS","MO","63110","3147474134","MPS","126900","7556","$0.00","The Workshop on Translational Research on Data Heterogeneity is scheduled to take place at the University of Washington at St. Louis, April 6 -- 7, 2024. In this digital age, large-scale data offer many new opportunities, holding great promise for researchers and decision-makers to understand important variations among sub-populations (i.e., data heterogeneity), explore associations between features and rare outcomes (e.g., rare diseases or extreme events), and make optimal personalized recommendations in areas of immediate practical relevance such as precision medicine and social programs. The proposed workshop focuses on data heterogeneity to tap into the true potential of information-rich data.<br/><br/>There exist formidable computational and statistical challenges in the analysis of heterogenous data. Some of the key barriers include scalability to data size and dimensionality, deep exploration of heterogeneity and structures in the data, need for robustness and replicability, and the ability to make sense of incomplete observations (e.g., due to censoring). The proposed workshop will serve as a platform for bringing some of the leading scholars in statistics and data science together to exchange new research ideas and to train the next-generation data scientists in the analysis of heterogeneous data.  The workshop will convene interdisciplinary researchers to discuss the forefront of heterogeneous data analysis and identify emerging areas for future research, emphasizing both methodology and applications.<br/><br/>Please visit https://sds.wustl.edu/events/workshop-translational-research-data-heterogeneity for updates.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2335568","Collaborative Research: Planning: FIRE-PLAN:High-Spatiotemporal-Resolution Sensing and Digital Twin to Advance Wildland Fire Science","DMS","S&CC: Smart & Connected Commun, STATISTICS, HDBE-Humans, Disasters, and th, Cross-BIO Activities, EPCN-Energy-Power-Ctrl-Netwrks","01/01/2024","08/09/2023","Haiyang Chao","KS","University of Kansas Center for Research Inc","Standard Grant","Yulia Gel","12/31/2025","$96,000.00","Melinda Adams","chaohaiyang@ku.edu","2385 IRVING HILL RD","LAWRENCE","KS","660457563","7858643441","MPS","033Y00, 126900, 163800, 727500, 760700","019E, 042E, 042Z, 132Z, 5294, 7275, 9150","$0.00","The number of catastrophic wildfires in the United States has been steadily increasing in recent decades, which generate casualties, large loss of properties, and dramatic environmental changes. However, it is difficult to make accurate predictions of wildland fire spread in real time for firefighters and emergency response teams. Although many fire spread models have been developed, one of the biggest challenges in their operational use is the lack of ground truth fire data at high spatiotemporal resolutions, which are indispensable for model evaluation and improvements. The objective of this planning project is to bring together wildland fire science researchers, fire sensing and data science experts, and diverse stakeholders to develop standards and requirements for high-spatiotemporal-resolution wildland fire sensing and digital twin construction. An organizing committee will be formed from wildland fire science, engineering, and stake holder communities including fire ecology and behavior modeling, pollution monitoring, robotics, cyber physical systems (CPS), wildfire fighting, indigenous cultural burns, and prescribed fires. A series of physical and remote workshops will be held focusing on themes such as open fire data for wildland fire modeling validation, digital twins for prescribed fires, and safe and efficient wildland fire data collection. <br/> <br/>Research tasks of this planning project include: 1) identification of key high-spatiotemporal-resolution fire metrics and data representations to support fire model validation and fire operations, 2) proposition of sensing strategies and algorithms for fire sensing and suppression robots and cyber physical systems that can support safe and efficient collection of desired high-resolution fire data, 3) development and evaluation of data assimilation and digital twin construction using high-resolution data to advance fire behavior modeling, coupled fire-atmosphere modeling, and smoke modeling, and 4) prototype and initial fire data ecosystem demonstration including collection of cultural burn data and establishment of GeoFireData, a benchmark fire data sharing and digital twin website, which can support different fire operation types such as fire spread model validation and controlled burn planning. The special attention will be devoted to interdisciplinary training of the next generation of scientists working with wildfire risks at the interface of computational sciences, engineering, ecology, and data sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2339904","CAREER: Theoretical foundations for deep learning and large-scale AI models","DMS","STATISTICS","07/01/2024","01/29/2024","Song Mei","CA","University of California-Berkeley","Continuing Grant","Tapabrata Maiti","06/30/2029","$81,984.00","","songmei@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","126900","1045","$0.00","Generative AI models have shown remarkable capabilities across various domains, making a transformative societal impact. However, their powerful capabilities present substantial challenges and risks due to limited theoretical foundations, especially regarding sensitive applications. The primary objective of this project is to establish a theoretical foundation for generative AI models including language models and diffusion models. The project will examine the capabilities and limitations of neural networks such as transformers and ResNets within these models, and develop techniques to interpret the algorithms implicitly implemented in these black-box systems. The theoretical investigation will leverage a diverse range of subjects including variational inference, sampling methods, high-dimensional statistics, computational complexity theory, and reinforcement learning theory. The results will provide valuable theoretical insights and promote the safe utilization of prevailing foundation models such as ChatGPT and DALLE. <br/><br/>This project will establish a theoretical foundation to elucidate the capabilities and limitations of language models and diffusion models. The project will investigate three key learning modalities: in-context learning, generative modeling, and decision making. For in-context learning, this project will analyze which algorithms transformers can implicitly implement, develop techniques to interpret the algorithms implemented in transformers, and provide guarantees on optimization and generalization during meta-training. This project will derive conditions for neural networks to represent high-dimensional score functions for diffusion-based generative modeling. For decision-making, the project will reveal how neural networks can be meta-trained to approximate bandit and reinforcement learning algorithms and investigate approaches to employing neural networks as decision-making agents. The outcomes will guide principled design and responsible deployment of AI models across disciplines. The activities include graduate student training and new course developments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2426499","Conference: SRCOS Summer Research Conference in Statistics and Biostatistics","DMS","STATISTICS","05/01/2024","04/29/2024","Katherine Thompson","KY","University of Kentucky Research Foundation","Standard Grant","Tapabrata Maiti","04/30/2025","$34,920.00","Gregory Hawk","katherine.thompson@uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","126900","7556, 9150","$0.00","The 59th Summer Research Conference (SRC) and Statistics Undergraduate Research Experience (SURE) will be held in Clemson, South Carolina on June 2-5, 2024. The Southern Regional Council on Statistics (SRCOS) is a consortium of statistics and biostatistics programs from 45 universities in 16 states in the Southern region. The SRC is an annual conference sponsored by the SRCOS. The purpose of the SRC is to encourage the interchange and mutual understanding of current research ideas in statistics and biostatistics, and to provide motivation and direction to further research progress. The SRC will give new researchers an opportunity to participate in the meeting and to interact closely with leaders in the field in a manner not possible at larger meetings. In addition to the graduate student participation, the 59th SRC will also include the fifth annual Statistical Undergraduate Research Experience (SURE) from June 3-5, 2024. SURE is a conference within a conference aimed to encourage the participation of undergraduate students from under-represented groups to pursue graduate education and career opportunities in STEM fields. SURE will include events specifically for undergraduate students and undergraduate mentors, such as a panel about career opportunities in statistics, a real data analytics workshop, and a speed-mentoring session with current statistics and biostatistics graduate students.<br/><br/>The SRC is particularly valuable for graduate students, isolated statisticians, and faculty from smaller regional schools in the southern region at drivable distances without the cost of travel to distant venues. Speakers will present formal research talks with adequate time allowed for clarification, amplification, and further informal discussions in small groups.  Under the travel support provided by this award, graduate students will attend and present their research in posters to be reviewed by more experienced researchers. Participation in SURE will encourage under-represented students to enter STEM fields, including statistics or biostatistics, and provide training to support this endeavor. The 59th SRC will strengthen the research of the statistics and biostatistics community as a whole and help bridge the gap for under-represented groups to pursue statistics or biostatistics, particularly in the sixteen states of the Southern Region. The SRCOS website can be found here: https://www.srcos.org; the 59th SRC website can be found here: https://www.srcos.org/conference; the SURE website can be found here: https://www.srcos.org/sure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2335570","Collaborative Research: Planning: FIRE-PLAN:High-Spatiotemporal-Resolution Sensing and Digital Twin to Advance Wildland Fire Science","DMS","S&CC: Smart & Connected Commun, STATISTICS, HDBE-Humans, Disasters, and th, Cross-BIO Activities, EPCN-Energy-Power-Ctrl-Netwrks","01/01/2024","08/09/2023","Ming Xin","MO","University of Missouri-Columbia","Standard Grant","Yulia Gel","12/31/2025","$51,998.00","","xin@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","033Y00, 126900, 163800, 727500, 760700","019E, 042E, 042Z, 132Z, 5294, 7275","$0.00","The number of catastrophic wildfires in the United States has been steadily increasing in recent decades, which generate casualties, large loss of properties, and dramatic environmental changes. However, it is difficult to make accurate predictions of wildland fire spread in real time for firefighters and emergency response teams. Although many fire spread models have been developed, one of the biggest challenges in their operational use is the lack of ground truth fire data at high spatiotemporal resolutions, which are indispensable for model evaluation and improvements. The objective of this planning project is to bring together wildland fire science researchers, fire sensing and data science experts, and diverse stakeholders to develop standards and requirements for high-spatiotemporal-resolution wildland fire sensing and digital twin construction. An organizing committee will be formed from wildland fire science, engineering, and stake holder communities including fire ecology and behavior modeling, pollution monitoring, robotics, cyber physical systems (CPS), wildfire fighting, indigenous cultural burns, and prescribed fires. A series of physical and remote workshops will be held focusing on themes such as open fire data for wildland fire modeling validation, digital twins for prescribed fires, and safe and efficient wildland fire data collection. <br/> <br/>Research tasks of this planning project include: 1) identification of key high-spatiotemporal-resolution fire metrics and data representations to support fire model validation and fire operations, 2) proposition of sensing strategies and algorithms for fire sensing and suppression robots and cyber physical systems that can support safe and efficient collection of desired high-resolution fire data, 3) development and evaluation of data assimilation and digital twin construction using high-resolution data to advance fire behavior modeling, coupled fire-atmosphere modeling, and smoke modeling, and 4) prototype and initial fire data ecosystem demonstration including collection of cultural burn data and establishment of GeoFireData, a benchmark fire data sharing and digital twin website, which can support different fire operation types such as fire spread model validation and controlled burn planning. The special attention will be devoted to interdisciplinary training of the next generation of scientists working with wildfire risks at the interface of computational sciences, engineering, ecology, and data sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2426029","Conference: Building a robust community: Joint International Conference on Robust Statistics and Conference on Data Science, Statistics, and Data Science","DMS","STATISTICS","07/01/2024","04/30/2024","David Kepplinger","VA","George Mason University","Standard Grant","Tapabrata Maiti","06/30/2025","$20,993.00","Anand Vidyashankar","dkepplin@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","126900","7556","$0.00","The award will provide travel support for early-career researchers and students in statistics, data science and related fields to attend the International Conference on Robust Statistics (ICORS) and the Conference on Data Science, Statistics and Visualization (DSSV) hosted at George Mason University in Fairfax, VA, from July 29 ? August 1, 2024. The organizers will work to recruit under-represented minorities in the above groups to actively participate in the conference by actively reaching out through the Caucus of Women in Statistics, the Washington Statistical Society and other chapters and sections of the American Statistical Association. The joint conference will bring together researchers, students and practitioners interested in the interplay of robust statistics, data analysis, computer science, and visualization and to build bridges between these fields for interdisciplinary research. Creating a forum to discuss recent progress and emerging ideas in these disciplines, the joint conference will facilitate fruitful dissemination and cross-pollination amongst various research groups. Early-career researchers and students will have the opportunity to share their research and ideas through presentations and a poster competition, and build connections with senior experts and practitioners. Building upon the successful history of ICORS and DSSV, the conferences also play an essential role in maintaining a cohesive group of international experts interested in robust statistics and related topics, whose interactions transcend the meetings and endure year-round.<br/><br/>Artificial Intelligence (AI) is becoming an inherent part of our lives, and several federal and state government agencies, research institutes, and industries are adopting these AI tools for various activities that could potentially improve real-life experiences. While there are several issues to be addressed in AI-based methods, the robustness of the Machine learning (ML) algorithms is a fundamental issue wherein one is concerned with adversarial contamination that can cause ML algorithms to fail. Associated with AI methods are privacy challenges, as modern methods tend to focus on personalized responses to AI responses. This joint conference will create a forum to discuss recent progress and emerging ideas on the interplay of robustness, interpretability and visualization for AI- and ML-methods and encourage informal contacts and discussions among all the participants. The conference plans to achieve this goal through several sessions and keynote addresses in these areas, integrating multiple disciplines, including privacy, Omics, spatial analytics, urban analytics, biostatistics, robustness, and visualization. The conference website can be found at https://icors-dssv2024.statistics.gmu.edu.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413618","Conference: Inaugural Berkeley-Stanford Workshop on Veridical Data Science","DMS","STATISTICS","05/01/2024","04/25/2024","Bin Yu","CA","University of California-Berkeley","Standard Grant","Tapabrata Maiti","04/30/2025","$11,000.00","","binyu@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","126900","7556","$0.00","The Inaugural Berkeley-Stanford Workshop on Veridical Data Science will be held on Friday <br/>May 31, 2024, at BIDS on UC Berkeley campus. The organizing committee consists of <br/>Bin Yu (UC Berkeley, co-chair), Russ Poldrack (Stanford, co-chair), Maya Mathur <br/>(Stanford), and Tiffany Tang (University of Michigan). This conference is jointly hosted <br/>by the Berkeley Institute of Data Science (BIDS) and the Department of Statistics, parts of <br/>UC Berkeley's College of Computing, Data Science, and Society (CDSS) and Stanford <br/>Data Science Center for Open and Reproducible Science (SDS-CORES).<br/><br/><br/>Data science is playing a ubiquitous role in science, society, and beyond, underscoring <br/>the urgent need to ensure the trustworthiness and safety of data science including <br/>machine learning. The data science life cycle (DSLC) is often a complex and multi-step <br/>process, including domain problem formulation, data collection and cleaning, data <br/>visualization, model/algorithm development, post-hoc visualization, interpretation, <br/>vetting and communication. In every step of a DSLC, human judgment calls are made <br/>and often unaccounted for in the traditional uncertainty quantification. Veridical data <br/>science (VDS) is the practice of rigorously conducting data analysis while making <br/>human judgment calls and using domain knowledge to extract and communicate useful, <br/>trustworthy information from data to solve a real-world domain problem. The <br/>Predictability-Computability-Stability (PCS) framework and documentation have been <br/>developed towards veridical data science. This conference is focused on veridical <br/>(truthful) data science (VDS) for responsible data analysis and decision-making. There <br/>will be 4 keynote speakers and 9 invited speakers. We expect around 100?150 <br/>attendees from both academia and industry. 8-10 lightning talk speakers will be <br/>selected based on submitted abstracts. This workshop intends to build a community of <br/>veridical data science researchers. https://na.eventscloud.com/website/69057<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2412746","New nonparametric theory and methods for censored data","DMS","STATISTICS","10/01/2024","04/23/2024","Bin Nan","CA","University of California-Irvine","Standard Grant","Jun Zhu","09/30/2027","$300,000.00","","nanb@uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","126900","079Z","$0.00","Estimating the survival time distribution given a set of predictors is of great importance in biomedical research and epidemiological studies, where the survival time is often censored, thus unobserved, due to dropouts or limited follow-up time. To obtain robust results, making weak model assumptions is desirable. This project focuses on two sets of tools without making assumptions about the functional form of any effect of a predictor on the survival time: one is a classical approach based on spline basis expansion, and the other is a modern approach based on deep neural networks. For the spline method, the project aims to develop a generally applicable distributional theory for the estimates of unknown functions in several widely used survival models, which is needed for making proper statistical inference but still lacking in the current literature. Furthermore, the deep neural network approach makes the weakest possible assumption and is the most flexible estimating method for an unknown multivariate function. The project considers deep neural networks with a full likelihood-based loss function for censored survival data and more general types of predictors that can randomly vary with time. The project aims to investigate both the numerical implementation and the theory for the estimation precision. This work will foster interdisciplinary research with epidemiologists, nephrologists, neurologists, and other scientists working on real scientific studies, and contribute to the well-being of human beings and the scientific community in a significant way through its versatile real-life applications, thus create an impact in and beyond statistical periphery. <br/><br/>Spline basis expansion is a commonly used approach for approximating an unknown smooth function, hence widely applied in estimating functional parameters. It is too often, however, that the approximation is treated as ?exact? in practice so to treat a nonparametric estimation problem as a parametric one because the asymptotic distributional theory for the spline estimation is lacking for models beyond the nonparametric linear model. The project takes advantages of recent developments in the random matrix theory to tackle the distributional theoretical problem of spline estimates in a broad range of commonly used statistical models in censored data analysis. The most general nonparametric problem is to estimate the conditional distribution other than, for example, the conditional mean or median. With the conditional distribution at hand, prediction becomes a choice of a particular characteristic of the conditional distribution and a prediction interval can be easily obtained. The project focuses on full likelihood-based loss functions characterized by the conditional hazard function and applies either deep neural networks or deep operator networks for the estimation of the conditional distribution function given functional covariates. In survival analysis, the functional covariates can be time-varying covariates that affect the hazard function in an arbitrarily way, for which no estimating method exists in the literature. Convergence rates of considered neural network methods with functional inputs will be established.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2423098","Conference: Forty Years at the Interplay of Information Theory, Probability and Statistical Learning","DMS","STATISTICS","05/01/2024","04/22/2024","Daniel Spielman","CT","Yale University","Standard Grant","Tapabrata Maiti","04/30/2025","$12,500.00","","spielman@cs.yale.edu","150 MUNSON ST","NEW HAVEN","CT","065113572","2037854689","MPS","126900","7556","$0.00","The award will support participants attending the conference ?Forty Years at the Interplay of Information Theory, Probability and Statistical Learning?, which will take place at Yale University during April 26?28, 2024. The conference is a three-day meeting comprising a diverse array of activities, including a poster session tailored for students, a reception aimed at fostering networking and connections among participants, and twenty-five research talks. The scientific theme of this conference is to explore the dynamic relationship and synergy between information theory, probability, and statistical learning. Beyond academic exploration, the conference aspires to nurture and inspire the next generation of scholars. Through engaging with leading experts, students and early career researchers will have the opportunity to gain insights into recent advancements and emerging challenges in these fields. We will actively encourage participants from underrepresented groups in several ways. We will collaborate with minority student organizations, professional associations, and community groups both within and outside Yale University to promote the workshop and leverage their networks for outreach, and we will offer financial assistance to minority participants to help cover the cost of attendance, travel, or accommodations to remove barriers to participation.<br/><br/>Statistics and information theory are deeply intertwined, both rooted in probability theory. Over the past four decades, this relationship has been meticulously explored and leveraged. In today's era marked by an unprecedented abundance of data across diverse domains and the growing influence of artificial intelligence, the indispensable tools wielded in these three disciplines continue to pave the way for discovering new limits of learning with complex data across various modalities. This conference serves as a gathering of leading researchers in these three disciplines and the next generation of data science leaders. Together, they will discuss new ideas to explore and enhance the connections between these fields, driving forward process and innovation. The website for the conference is: https://yalefds.swoogo.com/infotheory/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2413671","Travel: Junior travel support for the 2024 World Meeting of the International Society for Bayesian Analysis","DMS","STATISTICS","05/01/2024","04/19/2024","Sinead Williamson","TX","University of Texas at Austin","Standard Grant","Tapabrata Maiti","10/31/2024","$20,000.00","","sinead.williamson@mccombs.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","126900","","$0.00","This grant will fund 25 travel awards for US-based junior researchers attending the 2024 ISBA (International Society for Bayesian Analysis) World Meeting, to be held in Venice, Italy from 1-7 July 2024. The meeting's website is https://www.unive.it/web/en/2208/home. The ISBA World Meetings are the largest conferences in Bayesian statistics, attracting attendees from all around the world. The travel awards, averaging $800 per person, will partially offset the cost of travelling to Venice from the US, helping PhD students and other junior researchers to participate in the meeting. The selection committee will prioritize funding female researchers and members of minority groups underrepresented in the field.<br/> <br/>The 2024 ISBA World Meeting will feature keynote talks, invited and contributed talks, poster sessions, and short courses, selected to showcase cutting-edge research in Bayesian statistics. Attendance at the meeting will offer attendees a chance to learn about emerging research topics in Bayesian statistics. In addition, the scientific committee has selected as part of the program multiple talks and posters by junior researchers. This offers junior researchers an opportunity to share their research with an international audience of Bayesian statisticians.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2410953","Conference: ICSA 2024 Applied Statistical Symposium","DMS","STATISTICS","06/15/2024","02/27/2024","Qingxia Chen","TN","Vanderbilt University Medical Center","Standard Grant","Tapabrata Maiti","05/31/2025","$20,000.00","Dandan Liu","cindy.chen@vumc.org","1161 21ST AVE S STE D3300 MCN","NASHVILLE","TN","372320001","6153222450","MPS","126900","7556","$0.00","This award will play a pivotal role in promoting inclusivity and advancing the field of statistics by supporting the participation of graduate students, early-career researchers, and underrepresented groups in the International Chinese Statistical Association (ICSA) 2024 Applied Statistics Symposium, which will be held in Nashville, Tennessee from June 16-June 19, 2024. With the overarching theme of ""Data-Driven Decision Making: Unleashing the Power of Statistics?, the symposium will focus on collaboration and diversity. The event will not only promote education, research, and practical applications but also address broader societal goals by encouraging participation from women, minorities, and individuals with disabilities. The symposium's impact will extend beyond technical discussions, emphasizing professional development and bridging gaps in STEM fields through mentoring, career development, and opportunities for the next generation of statisticians.<br/>The conference is a comprehensive platform designed to advance statistical theory, computational methods, and practical applications. With plenary talks, invited sessions, student paper competitions, contributed posters, and short courses, the conference covers a broad spectrum of topics crucial to the evolving landscape of statistics, data science, and artificial intelligence. A key technical component includes the integration of foundational statistical theory with advanced computational methods, addressing practical challenges in various scientific domains. Special sessions, such as the panels on mentoring and career development, provide a focused forum to discuss strategies for promoting diversity and inclusion, particularly for women and individuals underrepresented in STEM. The symposium's website, accessible at https://symposium2024.icsa.org/, serves as a hub for seamless communication and resource sharing among participants.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2409890","Conference: SDSU Data Science Symposium","DMS","STATISTICS","02/01/2024","01/19/2024","Semhar Michael","SD","South Dakota State University","Standard Grant","Tapabrata Maiti","07/31/2024","$7,500.00","Gemechis Djira, Thomas Brandenburger","Semhar.Michael@sdstate.edu","940 ADMINISTRATION LN","BROOKINGS","SD","570070001","6056886696","MPS","126900","7556, 9150","$0.00","South Dakota State University will host the 6th Annual SDSU Data Science Symposium at the SDSU campus from February 5 to 6, 2024. The symposium brings together students, faculty, researchers, and industry professionals who engage in foundational research and applications of data science. This event, held annually since 2018 (excluding 2021 due to the COVID-19 pandemic) includes pre-conference workshops, keynote speakers, parallel oral presentations, undergraduate and graduate student poster presentations and competitions, as well as a career fair. Historically, more than 200 participants have attended, including students from up to 16 universities and representatives from up to 23 companies. With the 2024 event, this award will help expand both student participation and the number of junior speakers from rural, under-represented, and under-served areas of the Midwest and beyond.<br/><br/>The field of data science is expanding and interdisciplinary involving statisticians, computer scientists, mathematicians, etc. The SDSU-DSS stands out as a unique event in the region, bringing together participants from academia and industry to the Midwest. Parallel session tracks feature speakers from diverse fields such as statistics, mathematics, computer science, healthcare, finance, forensics, precision agriculture, and other data science-related topics. The symposium facilitates networking, collaborations, and exposes students to various career paths within mathematics, statistics, computer science, and other STEM areas. The symposium aims to 1) bring unique opportunities to the Midwest region, where students, faculty, business leaders of the region, and practitioners all gather to discuss the applications and foundation of data science; 2) provide hands-on, four-hour-long events on emerging topics/tools used in data science; 3) host presentations covering foundational and use-case aspects of data science and garnering interactive discussions and future collaborations 4) expand networks during the career fair and exhibit sessions, connecting faculty, students, and hiring managers of companies in the area.  The presentations and other content will be disseminated through Open PRAIRIE, an open-access institutional repository, ensuring widespread knowledge dissemination. The conference website is www.sdstate.edu/datascience.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2339439","CAREER: Towards a general recipe for fast high-dimensional scientific computing","DMS","APPLIED MATHEMATICS, STATISTICS","02/15/2024","02/12/2024","Yuehaw Khoo","IL","University of Chicago","Continuing Grant","Dmitry Golovaty","01/31/2029","$84,529.00","","ykhoo@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","126600, 126900","1045","$0.00","This project addresses the curse of dimensionality in solving the many-body Fokker-Planck and Schrdinger partial differential equations (PDEs), which are fundamental to understanding and predicting molecular structures, material properties, chemical reactions, extreme weather events, and pattern formations at various physical scales. The investigator aims to develop a general method to overcome high dimensionality of these PDEs via developing an iterative solver that combines the advantages of tensor-network, Monte Carlo, and convex optimization methods. The main idea is to solve for only a small number of descriptors of the solution (e.g., the statistical moments of the solution) and then recover a compressed representation of the solution through these descriptors. An improved solution procedure will contribute to the Material Genome Initiative by developing new computational tools for physical and chemical simulations. The proposed education plan aims to train graduate students for quantitative research at the intersection of mathematical and physical sciences, fostering a new generation of researchers well-equipped to tackle problems of national interest in scientific computing, machine learning, and quantum computing. Through research advising, a summer mentoring program, and the development of a summer school for quantum mechanics, the proposal aims to encourage the participation of underrepresented students in scientific research and higher education. <br/> <br/>Currently, Monte Carlo methods have been widely successful in many practical physical and chemical simulation tools due to their flexibility, despite potential drawbacks such as slow mixing time and high variance. An alternative approach to characterizing chemical/physical systems is to deterministically solve for the solution of a PDE over the entire space, which can work only for small (and consequently low-dimensional) problems. The proposed research attempts to address the limitations of sampling-based and PDE approaches through the development of an iterative solver. It involves fast iterations achieved by alternating between short-time Monte Carlo simulations and estimating a tensor-network ansatz. It relies on the initialization strategy underpinned by convex optimization in order to reduce the number of iterations. Monte Carlo variance of traditional sampling methods is significantly reduced since only a small number of tensor-network parameters need to be estimated. On the other hand, it significantly extends the flexibility of tensor-network methods by allowing stochastic operations. To this end, a novel tensor-network-based generative model is proposed where density can be learned from empirical samples without the use of any optimization. It would impact statistics by providing new density estimators and analysis without the curse of dimensionality. Further, a new moment method based on convex optimization is proposed for solving high-dimensional PDEs. This would develop mathematical programming, a tool traditionally used in operations research, into an effective tool for physics simulations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2409876","Conference: The 9th Workshop on Biostatistics and Bioinformatics","DMS","STATISTICS","05/01/2024","04/10/2024","Yichuan Zhao","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Tapabrata Maiti","04/30/2025","$10,000.00","","yichuan@gsu.edu","58 EDGEWOOD AVE NE","ATLANTA","GA","303032921","4044133570","MPS","126900","7556","$0.00","The 9th Workshop on Biostatistics and Bioinformatics will be held at Georgia State University,  May 03-05, 2024. It will provide a platform for senior researchers, junior researchers, and graduate students to discuss the latest advances and challenges in the field. The main objective of the workshop is to address emerging challenges in applications of AI, causal inference, and statistical genomics data analysis. The workshop will feature a keynote speaker, leading experts and young researchers, and it provides an invaluable opportunity for graduate students and young researchers to gain experience giving poster presentations. To present posters will help them to receive feedback from experts in the field, and engage in discussions with fellow attendees.  The workshop will provide a distinctive opportunity for collaboration, discussion, and dissemination of ideas.  In addition, the workshop places special emphasis on supporting young people, underrepresented individuals, and women through financial supports, aiming to cultivate an inclusive environment. Through the integration of interactive sessions, networking opportunities, and a commitment to the diversity, the workshop can enhance its overall impact.<br/><br/><br/>The workshop aims to push the boundaries of biostatistics and bioinformatics through collaborative efforts among various universities. The workshop maintains a very strong program, which includes one keynote speech by a renowned statistician, invited talks by established and emerging experts, poster presentations from junior researchers and graduate students.  The workshop will also offer a short course ""Tutorial on Deep Learning and Generative AI"" and five Best Student Poster Awards will be announced during the award ceremony. Special effort is dedicated to forging connections with historically black universities in the Metro Atlanta area, by inviting underrepresented individuals to participate in the workshop. We expect that the workshop will attract a significant number of participants from underrepresented groups. Travel support is available for junior researchers, graduate students, and underrepresented people by promoting the inclusivity and diversity within the workshop.  More information about the workshop can be found at the website:  https://math.gsu.edu/yichuan/2024Workshop/index.html.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2404998","Conference: The 2024 Joint Research Conference on Statistics in Quality, Industry, and Technology (JRC 2024) - Data Science and Statistics for Industrial Innovation","DMS","STATISTICS","03/15/2024","03/11/2024","Yili Hong","VA","Virginia Polytechnic Institute and State University","Standard Grant","Tapabrata Maiti","02/28/2025","$12,000.00","","yilihong@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","126900","7556","$0.00","This project funds U.S. based student participation in the 2024 Joint Research Conference on Statistics in Quality, Industry and Technology (JRC 2024), which will be held in Waterloo, Ontario, Canada, from June 17-20, 2024, at the University of Waterloo. The organization of this <br/>conference is also in partnership with Virginia Tech. JRC 2024 is a joint meeting of the 29th Spring Research Conference on Statistics <br/>in Industry and Technology (SRC) and the 40th Quality and Productivity Research Conference (QPRC), which happens once every four years.<br/> The theme of JRC 2024 is ""Data Science and Statistics for Industrial Innovation."" JRC 2024 aims to bring together researchers and<br/> practitioners worldwide who use statistics in quality, technology, and industrial contexts. The conference promotes communication <br/>among researchers and practitioners to enable and ensure the development and widespread use of novel insights and methodology. <br/>JRC 2024 has the potential to benefit society by offering participants the opportunities to gain knowledge and reshape their <br/>perspectives on topics associated with data science, statistics, and machine learning. JRC 2024 will advocate for the ethical <br/>application and understanding of data science, statistics, and machine learning for industrial innovation, which is beneficial for <br/>the long-term competitiveness of the U.S. industry. The conference provides a platform to disseminate knowledge to the broader <br/>community by sharing short course lecture notes, presentation slides, and posters on the conference website. JRC 2024 aims to broaden <br/>the participation of underrepresented groups (i.e., women, racial/ethnic minorities, etc.) in STEM disciplines. <br/><br/>JRC 2024 focuses on recent advancements in methodology, best practices, and innovative applications. Participation in JRC 2024 has the <br/>potential to advance knowledge and understanding of topics related to data science, statistics, and machine learning and how they can <br/>be relevant to industrial innovation. This conference traditionally attracts prominent statisticians, data scientists, quantitative <br/>analysts, and others with an established record of highly influential, methodological, and interdisciplinary research. These individuals <br/>will have the opportunity to discuss the current progress made in statistics and machine learning, such as big data technology, <br/>text modeling, the use of generative artificial intelligence in industrial innovation, and exchange novel ideas and experiences in <br/>working with modern data science to discover knowledge and apply it to numerous fields. JRC 2024 has the potential to disseminate <br/>new methods and data-driven approaches, the evaluation of previous findings, and the validation of theoretical approaches, <br/>stimulate further investigations regarding the benefits of working with statistics and machine learning methods for industry and <br/>increase the awareness of the need to use data science approach in industry. The conference will include three plenary presentations, <br/>18 invited paper sessions, four to six contributed sessions, a poster session, a technical tour, and a one-day short course. <br/>More details on the conference can be found on its web page: <br/>https://uwaterloo.ca/joint-research-conference-statistics-quality-industry-technology/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2347284","Conference: Design and Analysis of Experiments 2024","DMS","STATISTICS","02/01/2024","12/06/2023","John Morgan","VA","Virginia Polytechnic Institute and State University","Standard Grant","Tapabrata Maiti","01/31/2025","$18,000.00","Xinwei Deng, Anne Driscoll","jpmorgan@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","126900","7556","$0.00","The Design and Analysis of Experiments Conference 2024 (DAE 2024) will meet May 15-17, 2024 on the campus of Virginia Tech in Blacksburg, VA. It will bring together researchers from across the United States and beyond, and from both academia and industry, to focus on advancing the statistical techniques for experimentation that empower knowledge discovery. It will provide a forum for interaction, discussion, and exchange of ideas on novel research for designing effective experiments, and for analyzing the data that they produce. The aim of the conference is to increase the efficacy of data collection in areas as wide-ranging as autonomous driving, drug development, environmental monitoring, infectious disease dynamics, cybersecurity, and manufacturing, among many others, and so accelerate the pace of innovation in all of these domains. DAE 2024 will emphasize inclusion and mentoring of young researchers and minorities, and in so doing will be a cog in the development of the next generation of statistical experts in the techniques of experimental design.<br/><br/>Designed experimentation and the corresponding techniques for analysis are integral to the process of scientific discovery, be it in engineering, medicine, commerce, manufacturing, or indeed in any of the vast range of human activities where continuing knowledge acquisition is a requirement for advancement and success. Driven by these needs, developments are rapidly taking place in experimental design and analysis research, in both traditional and emerging areas of applications. As new areas of application arise, correspondingly new computational tools are enabling the development of better designs for data collection in complex problems. Technical sessions of DAE 2024 will include leading experts addressing Covering Arrays and Combinatorial Testing; Online Experimentation; Sequential Design, Active Learning, and Bayesian Optimization; Design Issues in Uncertainty Quantification; Orthogonal Arrays and Related Designs; Causal Inference and Experimental Design; Design Challenges in Transportation; and more. Additional features will include roundtable discussions, mentoring sessions for junior researchers, and poster sessions highlighting advancements in addition to those covered in the technical sessions.  Further details about the conference may be found at https://sites.google.com/view/dae2024/dae-2024.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2342821","Conference: Emerging Statistical and Quantitative Issues in Genomic Research in Health Sciences","DMS","STATISTICS","02/01/2024","01/24/2024","Xihong Lin","MA","Harvard University","Standard Grant","Tapabrata Maiti","01/31/2027","$61,920.00","","xlin@hsph.harvard.edu","1033 MASSACHUSETTS AVE STE 3","CAMBRIDGE","MA","021385366","6174955501","MPS","126900","7556","$0.00","The 2023 Conference of the Program in Quantitative Genomics (PQG), entitled ?Diversity in Genetics and Genomics? will take place at the Joseph B. Martin Conference Center at the Harvard Medical School on October 17-18, 2023.  This long-standing Harvard T. H. Chan School Public Health Program in Quantitative Genomics Conference series focuses on timely interdisciplinary discussions on emerging statistical and computational challenges in genetic and genomic science. The focus of each conference evolves in parallel to scientific frontiers.  A key feature of the series is its interdisciplinary nature, where quantitative and subject-matter scientists jointly discuss statistical and quantitative issues that arise in cutting-edge genetic and genomic research in human diseases. Conference participants critique existing quantitative methods, discuss emerging statistical and quantitative issues, identify priorities for future research, and disseminate results. Diversity in genetic and genomics has been increasingly embraced not only for enabling more powerful studies but also because of the need to avoid further exacerbation of structured inequalities in healthcare systems and to chart a path forward for their amelioration. Significant effort has been made in recent years to improve study participant and workforce diversity in genetics and genomics, including the inclusion of diverse groups in discovery and functional studies and translational efforts to empower or pave the road for equitable clinical impact. The 2023 conference will provide a platform to engage inter-disciplinary researchers to have in-depth discussions on the quantitative challenges and opportunities in increasing diversity in genetic and genomic research. We will make serious efforts to recruit junior researchers, including graduate students, postdoctoral fellows, in particular underrepresented minorities and women, as speakers and participants.  <br/><br/>The impetus for the 2023 conference theme comes from the pressing need to address the statistical and quantitative issues in diversity in genetic and genomic research. The three topics of the conference include (1) Diversity for gene mapping and studying variant functions; (2) Diversity for translational genetics: polygenic risk and clinical implementation; (3) How do we move forward while acknowledging the past? Examples of the first topic include multi-ancestry genetic association tests, fine-mapping, and eQTL analysis. Examples of the second topic include trans-ethnic polygenic risk prediction and transferred learning. Examples of the third topic include enhancing transparency in the use of population descriptors in genomics research and building global collaborative genetic research frameworks. The education and research activities discussed at the conference will make important contributions to advance efforts on increasing diversity of genetic and genomic research, and will help create the scientific basis and workforce required to ensure and sustain US competitiveness both economically and technologically, prolonging and saving lives, and promoting national security. For more information, see www.hsph.harvard.edu/pqg-conference/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2403813","Conference: Theory and Foundations of Statistics in the Era of Big Data","DMS","STATISTICS","02/01/2024","02/01/2024","Xin Zhang","FL","Florida State University","Standard Grant","Tapabrata Maiti","01/31/2025","$14,800.00","Srijan Sengupta","henry@stat.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","126900","7556","$0.00","The Department of Statistics at Florida State University (FSU) will host a three-day conference titled ""Theory and Foundations of Statistics in the Era of Big Data"" in Tallahassee, Florida, from April 19 to 21, 2024. The main objective of the conference is to bring together a global community of statisticians and data scientists to chart the state-of-the-art, challenges, and the future trajectory of contemporary statistical foundations, theory, and practice. The format of the conference includes three plenary sessions, six invited sessions showcasing current senior leaders in the field who have made foundational contributions to statistics, two special invited sessions for early-career researchers, a poster session for graduate students, and a banquet talk by a leading expert. The special invited sessions and poster session will provide a unique opportunity for early-career researchers and graduate students not only to showcase their research work but also to benefit from in-depth intellectual interactions with leaders in the field in a small conference setting.<br/><br/>The main objective of the conference is to bring together present-day statistics and science innovators and senior leaders with emerging young researchers to identify, discuss, and decipher solutions to these foundational issues and challenges faced by modern-day statistics and data science. Providing support for junior researchers and graduate students who do not have access to other sources of funding to attend this important and timely gathering of researchers working on the foundational aspects of statistical sciences is also key to maintaining the current leadership of U.S. institutions in this field. It is extremely timely to have such an event to stimulate and comprehend the major contemporary challenges in the foundation, theory, and implementation of the field that is currently playing such an important role in every sphere of social media, economic security, public health, and beyond. This conference will be in partnership with the International Indian Statistical Association (IISA) and co-sponsored by the American Statistical Association (ASA), the National Institute of Statistical Science (NISS), and the Institute of Mathematical Statistics (IMS). The conference website is https://sites.google.com/view/theory-and-foundations-of-stat/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2422478","CAREER: Next-Generation Methods for Statistical Integration of High-Dimensional Disparate Data Sources","DMS","STATISTICS","03/01/2024","03/12/2024","Irina Gaynanova","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Yong Zeng","05/31/2026","$24,238.00","","irinagn@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","126900","1045","$0.00","Multi-view data (collected on the same samples from multiple sources) are increasingly common with advances in multi-omics, neuroimaging and wearable technologies. For example, wearable devices such as physical activity trackers, continuous glucose monitors and ambulatory blood pressure monitors are worn concurrently to provide measurements of distinct subjects? characteristics. There is enormous potential in integrating that concurrent information from the distinct vantages to better understand between-view associations and improve prediction of health outcomes. Existing tools for data integration are sensitive to outliers, and are not designed for mixed data types (e.g. continuous skewed glucose measurements, zero-inflated activity counts, binary indicators of sleep/wake). The PI will develop a more robust framework for multi-view data integration that is better able to account for outliers, better match the mixed types of data actually collected, and be more accurate in separating common from view-specific signals. The new methods will be implemented in open-source software accompanied by reproducible workflow examples, providing immediate and easy access for other researchers. The educational component centers on the development of structured research experiences (SRE) for students. SRE enhances students written communication, software development and reproducible research skills, all of which are lacking in traditional curriculum. This will improve students? preparation for conducting research, and widen their STEM employment opportunities. The involvement of students from traditionally underrepresented groups will positively impact their retention rate and will broaden the participation of underrepresented groups in STEM.<br/><br/>Popular dimension reduction methods, such as principal component analysis and discriminant analysis, are tailored for single-view data, and thus fail to discover coordinated multi-view signals on a global level. On the other hand, existing multi-view dimension reduction methods suffer from reliance on the Gaussianity assumption, an inability to capture joint functional signals, and a lack of theoretical guarantees. The PI will address these drawbacks by (i) developing a joint dimension reduction framework for skewed continuous, binary and zero-inflated view types; (ii) a joint dimension reduction framework for mixed functional multi-view data and (iii) a new paradigm for simultaneous extraction of signals across views based on hierarchical low-rank constraints.  This work will lead to critically needed new statistical methods for data integration with direct relevance for researchers working with wearable monitors, microbiome and multi-omics data through interdisciplinary collaborations of the PI. The proposed structured research experiences will center on the design and reproducibility of simulations studies, and align with computational components of the proposed research, including direct students? involvement in multiple simulation studies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2349991","Conference: Statistics in the Age of AI","DMS","STATISTICS","03/01/2024","12/18/2023","Xiaoke Zhang","DC","George Washington University","Standard Grant","Tapabrata Maiti","02/28/2025","$18,400.00","","xkzhang@gwu.edu","1918 F ST NW","WASHINGTON","DC","200520042","2029940728","MPS","126900","7556","$0.00","The conference ?Statistics in the Age of AI? will be held at George Washington University, Washington, DC on May 9-11, 2024. With the boom of artificial intelligence (AI), partly accelerated by the launching of large language models (e.g., ChatGPT), AI tools have reached every corner of our society. In this era where many business and scientific problems are being tackled via AI systems, statistics has become more critical than ever since it can offer uncertainty quantification, causal analysis, and interpretability among others, which most AI systems are lacking. This conference will bring together researchers and practitioners in academics and industries to explore the impact of AI on statistical research, education, and practice and also to brainstorm how statistics can contribute to AI. The conference organizers encourage participation and attendance by students, post-doctoral scholars, early-career researchers, and individuals from underrepresented groups. <br/><br/>The conference features short courses, poster and oral presentations, and panel discussions. The two short courses will focus on causal inference and conformal inference respectively. The presentations and panel discussions will address efficient handling of data for AI models and architectures, uncertainty quantification, and responsible decision-making among other topics. Further information will become available on the conference website: https://statistics.columbian.gwu.edu/statistics-age-ai.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2338760","CAREER: Statistical Inference in Observational Studies -- Theory, Methods, and Beyond","DMS","STATISTICS","07/01/2024","01/10/2024","Rajarshi Mukherjee","MA","Harvard University","Continuing Grant","Jun Zhu","06/30/2029","$81,885.00","","rmukherj@hsph.harvard.edu","1033 MASSACHUSETTS AVE STE 3","CAMBRIDGE","MA","021385366","6174955501","MPS","126900","1045","$0.00","Causal inference refers to a systematic way of deciphering causal relationships between entities from empirical observations ? an epistemic framework that underlies past, present, and future scientific and social development. For designing statistical methods for causal inference, the gold standard pertains to randomized clinical trials where the researcher assigns treatment/exposure to subjects under study based on pure chance mechanisms. The random assignment negates systematic bias between the observed relationship between the treatment/exposure and outcome due to unknown common factors referred to as confounders. However, randomized clinical trials are often infeasible, expensive, and ethically challenging. In contrast, modern technological advancement has paved the way for the collection of massive amounts of data across a spectrum of possibilities such as health outcomes, environmental pollution, medical claims, educational policy interventions, and genetic mutations among many others. Since accounting for confounders in such data is the fundamental aspect of conducting valid causal inference, one of the major foci of modern causal inference research have been to design procedures to account for complex confounding structures without pre-specifying unrealistic statistical models. Despite the existence of a large canvas of methods in this discourse, the complete picture of the best statistical methods for inferring the causal effect of an exposure on an outcome while adjusting for arbitrary confounders remains largely open. Moreover, there are several popularly used methods that require rigorous theoretical justification and subsequent modification for reproducible statistical research in the domain of causal inference. This project is motivated by addressing these gaps and will be divided into two broad interconnected themes. In the first part, this project provides the first rigorous theoretical lens to the most popular method of confounder adjustment in large-scale genetic studies to find causal variants of diseases. This will in turn bring forth deeper questions about optimal statistical causal inference procedures that will be explored in the second part of the project. Since the project is designed to connect ideas from across statistical methods, probability theory, computer science, and machine learning, it will provide unique learning opportunities to design new courses and discourses.  The project will therefore integrate research with education through course development, research mentoring for undergraduate and graduate students, especially those from underrepresented groups, and summer programs.<br/><br/>This project will focus on two broad and interrelated themes tied together by the motivation of conducting statistical and causal inference with modern observational data. The first part of the project involves providing the first detailed theoretical picture of the most popular principal component-based method of population stratification adjustment in genome-wide association studies. This part of the project also aims to provide new methodologies to correct for existing and previously unknown possible biases in the existing methodology as well as guidelines for practitioners for choosing between methods and design of studies. By recognizing the fundamental tenet of large-scale genetic data analysis as the identification of causal genetic determinants of disease phenotypes, the second part of the project develops the first complete picture of optimal statistical inference of causal effects in both high-dimensional under sparsity and nonparametric models under smoothness conditions. Moreover, this part of the project responds to the fundamental question of tuning learning algorithms for estimating nuisance functions, such as outcome regression and propensity score for causal effect estimation, to optimize the downstream mean-squared error of causal effect estimates instead of prediction errors associated with these regression functions. The overall research will connect ideas from high-dimensional statistical inference, random matrix theory, higher-order semiparametric methods, and information theory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2338018","CAREER: Single-Fidelity vs. Multi-Fidelity Computer Experiments: Unveiling the Effectiveness of Multi-Fidelity Emulation","DMS","STATISTICS","06/01/2024","12/05/2023","Chih-Li Sung","MI","Michigan State University","Continuing Grant","Jun Zhu","05/31/2029","$79,437.00","","sungchih@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","126900","1045","$0.00","Computer models have become indispensable tools across diverse fields, enabling the simulation of complex phenomena and facilitating decision-making without costly real-world experiments. Traditionally, computer models are simulated using single, high-accuracy simulations, employing a high level of detail and resolution throughout. Recent advancements, however, have shifted attention towards multi-fidelity simulations, balancing computational cost and accuracy by leveraging various levels of detail and resolution in the simulation. A key question arises: is it more effective to use single-fidelity or multi-fidelity simulations? This is a question practitioners often confront when conducting computer simulations. The research aims to address this fundamental question directly, providing valuable insights for practical decision-making. By leveraging insights gained from computational cost comparisons, the research will enhance the ability to predict complex scientific phenomena accurately and has the potential to revolutionize fields such as engineering, medical science, and biology. The project contributes to outreach and diversity efforts, inspiring youth and increasing female representation in STEM research. Moreover, collaborations with diverse research groups, as well as involvement in the REU exchange program, provide opportunities to engage undergraduate students, nurturing their interest in research and encouraging them to pursue careers in STEM. Research findings will be disseminated through publications and conferences. The code developed will be shared to foster collaboration and encourage others to build upon these innovative methodologies.<br/><br/>This research addresses the fundamental question of whether to conduct single-fidelity or multi-fidelity computer experiments by investigating the effectiveness of multi-fidelity simulations. It begins by examining the computational cost comparison between the two approaches, finding that multi-fidelity simulations, under certain conditions, can theoretically require more computational resources while achieving the same predictive ability. To mitigate the negative effects of low-fidelity simulations, a novel and flexible statistical emulator, called the Recursive Nonadditive (RNA) emulator, is proposed to leverage multi-fidelity simulations, and a sequential design scheme based on this emulator is developed, which maximizes the effectiveness by selecting inputs and fidelity levels based on a criterion that balances uncertainty reduction and computational cost. Furthermore, two novel multi-fidelity emulators, called ""secure emulators,"" are developed, which theoretically guarantee superior predictive performance compared to single-fidelity emulators, regardless of design choices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2339241","CAREER: Learning stochastic spatiotemporal dynamics in single-molecule genetics","DMS","Cellular Dynamics and Function, STATISTICS, MATHEMATICAL BIOLOGY","07/01/2024","01/29/2024","Christopher Miles","CA","University of California-Irvine","Continuing Grant","Amina Eladdadi","06/30/2029","$239,517.00","","cemiles@uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","111400, 126900, 733400","068Z, 079Z, 1045, 7465, 8038","$0.00","The ability to measure which genes are expressed in cells has revolutionized our understanding of biological systems. Discoveries range from pinpointing what makes different cell types unique (e.g., a skin vs. brain cell) to how diseases emerge from genetic mutations. This gene expression data is now a ubiquitously used tool in every cell biologist?s toolbox. However, the mathematical theories for reliably extracting insight from this data have lagged behind the amazing progress of the techniques for harvesting it. This CAREER project will develop key theoretical foundations for analyzing imaging data of gene expression. The advances span theory to practice, including developing mathematical models and machine-learning approaches that will be used with data from experimental collaborators. Altogether, the project aims to create a new gold standard of techniques in studying spatial imaging data of gene expression and enable revelation of new biological and biomedical insights. In addition, this proposed research will incorporate interdisciplinary graduate students and local community college undergraduates to train the next generation of scientists in the ever-evolving intersection of data science, biology, and mathematics.  Alongside research activities, the project will create mentorship networks for supporting first-generation student scientists in pursuit of diversifying the STEM workforce. <br/><br/>The supported research is a comprehensive program for studying single-molecule gene expression spatial patterns through the lens of stochastic reaction-diffusion models. The key aim is to generalize mathematical connections between these models and their observation as spatial point processes. The new theory will incorporate factors necessary to describe spatial gene expression at subcellular and multicellular scales including various reactions, spatial movements, and geometric effects. This project will also establish the statistical theory of inference on the resulting inverse problem of inferring stochastic rates from only snapshots of individual particle positions. Investigations into parameter identifiability, optimal experimental design, and model selection will ensure robust and reliable inference. In complement to the developed theory, this project will implement and benchmark cutting-edge approaches for efficiently performing large-scale statistical inference, including variational Bayesian Monte Carlo and physics-informed neural networks.  The culmination of this work will be packaged into open-source software that infers interpretable biophysical parameters from multi-gene tissue-scale datasets.<br/><br/>This CAREER Award is co-funded by the Mathematical Biology and Statistics Programs at the Division of Mathematical Sciences and the Cellular Dynamics & Function Cluster in the Division of Molecular & Cellular Biosciences, BIO Directorate.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2340241","CAREER: New Frameworks for Ethical Statistical Learning: Algorithmic Fairness and Privacy","DMS","STATISTICS","07/01/2024","01/23/2024","Linjun Zhang","NJ","Rutgers University New Brunswick","Continuing Grant","Yong Zeng","06/30/2029","$90,127.00","","linjun.zhang@rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","126900","1045","$0.00","With the unprecedented impact of data science and machine learning in many aspects of our daily lives, such as healthcare, finance, education, and law, there is an urgent need to design ethical statistical learning algorithms that account for fairness and privacy. This project tackles the challenge of integrating ethical principles into the fabric of statistical learning. The approach prioritizes fairness by enhancing statistical algorithms to perform equitably, particularly in scenarios with limited sample sizes and where sensitive attributes are restricted by legal or societal norms. In parallel, this project addresses privacy by developing a general framework for studying the privacy-accuracy trade-off under new privacy constraints emerging with the advances in generative AI. The practical upshot of this work is the application of these methods to biomedical fields, accompanied by the release of open-source software, broadening the impact and encouraging ethical practices in statistical learning across various domains. This project promotes equitable and private data handling and provides research training opportunities to students.<br/><br/>The research objective of this project is to develop rigorous statistical frameworks for ethical machine learning, with a focus on algorithmic fairness and data privacy. More specifically, the project will: (1) develop innovative statistical methods that ensure fairness in a finite-sample and distribution-free manner; (2) design algorithms that ensure fairness while complying with societal and legal constraints on sensitive data; (3) establish new frameworks to elucidate the trade-off between statistical accuracy and new privacy concepts in generative AI, including machine unlearning and copyright protection. Taken together, the outcome of this research will build a firm foundation of ethical statistical learning and shed light on the development of new theoretical understanding and practical methodology with algorithmic fairness and privacy guarantees.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2339829","CAREER: Statistical foundations of particle tracking and trajectory inference","DMS","STATISTICS","04/01/2024","01/19/2024","Jonathan Niles-Weed","NY","New York University","Continuing Grant","Yong Zeng","03/31/2029","$89,991.00","","jdw453@nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","126900","1045","$0.00","Many problems in human microbiology, astronomy, high-energy physics, fluid dynamics, and aeronautics involve large collections of moving ""particles"" with complicated dynamics. Learning how these systems work requires developing statistical procedures for estimating these dynamics on the basis of noisy observations. The goal of this research is to develop scalable, practical, and reliable methods for this task, with a particular focus on developing statistical theory for applications in cosmology, cellular biology, and machine learning. This research will also include a large outreach component based on broadening access to research opportunities for undergraduates and graduate students.<br/><br/>The technical goals of this proposal are to develop computationally efficient estimators for multiple particle tracking in d dimensions when the particles evolve based on a known or unknown stochastic process, to develop Bayesian methods for posterior sampling based on observed trajectories, and to extend these methods to obtain minimax estimation procedures for smooth paths in the Wasserstein space of probability measures. The research also aims to develop estimators for more challenging models with the growth and interaction of particles.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2338464","CAREER: Distribution-Free and Adaptive Statistical Inference","DMS","STATISTICS","01/15/2024","01/11/2024","Lihua Lei","CA","Stanford University","Continuing Grant","Yulia Gel","12/31/2028","$75,564.00","","lihualei@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","126900","1045","$0.00","Recent years have witnessed a growing trend across scientific disciplines to embrace complex modeling and black-box machine learning algorithms. Despite the remarkable success of handling complex data structures and fitting sophisticated regression functions, there remains a substantial gap regarding the integration of rigorous statistical principles into these pipelines. The main difficulty revolves around achieving reliable uncertainty quantification and robust statistical inference without artificially simplifying the complexity inherent in these advanced tools. Most existing frameworks that aim to bridge the gap rely on strong assumptions under which the machine learning algorithm can accurately estimate the data generating distribution. Nevertheless, these assumptions are often hard to justify, especially for modern machine learning algorithms that have yet to be fully understood. This research project aims to develop new frameworks for statistical inference that wrap around any machine learning algorithms or complex models without concerning about failure modes. The resulting methods are able to address the potential threats to inferential validity caused by black-box machine learning algorithms in a wide range of applied fields, including medicine, healthcare, economics, political science, epidemiology, and climate sciences. Open source software will also be developed to help applied researchers integrate rigorous statistical inference into their domain-specific modeling workflows without compromising the effectiveness of modern tools in non-inferential tasks. This may further alleviate hesitation in adopting modern machine learning methods and catalyze collaboration between scientific and engineering fields. Throughout the project, the PI will mentor undergraduate and graduate students, equipping them with solid understandings of statistical principles to become future leaders in face of rapidly evolving machine learning techniques.<br/><br/>This proposal will focus on distribution-free inference, which is immune to misspecification of parametric models, violation of nonparametric assumptions like smoothness or shape constraints, inaccuracy of asymptotic approximations due to limited sample size, high dimensionality, boundary cases, or irregularity. To avoid making uninformative decisions, an ideal distribution-free inference framework should also be adaptive to good modeling. This means that it should be as efficient as other frameworks that rely on distributional assumptions. Adaptivity alleviates the tradeoff between robustness and efficiency. The PI will develop distribution-free and adaptive inference frameworks for three specific problems. First, in causal inference, tighter identified set can be obtained for partially identified causal effects by incorporating pre-treatment covariates. However, existing frameworks for sharp inference require estimating conditional distributions of potential outcomes given covariates. The PI will develop a generic framework based on duality theory that is able to wrap around any estimates of conditional distributions and make distribution-free and adaptive inference. Second, many target parameters in medicine, political economy, and causal inference can be formulated through extremums of the conditional expectation of an outcome given covariates. In contrast to classical methods that impose distributional assumptions to enable consistent estimation of the conditional expectation, the PI will develop a distribution-free framework for testing statistical null hypotheses and constructing valid confidence intervals on the extremums directly. Finally, the use of complex models and prediction algorithms in time series nowcasting and forecasting presents challenges for reliable uncertainty quantification. To address this, the PI will develop a framework based on model predictive control and conformal prediction that is able to wrap around any forecasting algorithms and calibrate it to achieve long-term coverage, without any assumptions on the distribution of the time series. The ultimate goal of this research is to bring insights and present a suite of tools to empower statistical reasoning with machine learning and augment machine learning with statistical reasoning.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2335569","Collaborative Research: Planning: FIRE-PLAN:High-Spatiotemporal-Resolution Sensing and Digital Twin to Advance Wildland Fire Science","DMS","S&CC: Smart & Connected Commun, STATISTICS, HDBE-Humans, Disasters, and th, Cross-BIO Activities, EPCN-Energy-Power-Ctrl-Netwrks","01/01/2024","08/09/2023","Xiaolin Hu","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Yulia Gel","12/31/2025","$52,000.00","","xhu@cs.gsu.edu","58 EDGEWOOD AVE NE","ATLANTA","GA","303032921","4044133570","MPS","033Y00, 126900, 163800, 727500, 760700","019E, 042E, 042Z, 132Z, 5294, 7275, 9150","$0.00","The number of catastrophic wildfires in the United States has been steadily increasing in recent decades, which generate casualties, large loss of properties, and dramatic environmental changes. However, it is difficult to make accurate predictions of wildland fire spread in real time for firefighters and emergency response teams. Although many fire spread models have been developed, one of the biggest challenges in their operational use is the lack of ground truth fire data at high spatiotemporal resolutions, which are indispensable for model evaluation and improvements. The objective of this planning project is to bring together wildland fire science researchers, fire sensing and data science experts, and diverse stakeholders to develop standards and requirements for high-spatiotemporal-resolution wildland fire sensing and digital twin construction. An organizing committee will be formed from wildland fire science, engineering, and stake holder communities including fire ecology and behavior modeling, pollution monitoring, robotics, cyber physical systems (CPS), wildfire fighting, indigenous cultural burns, and prescribed fires. A series of physical and remote workshops will be held focusing on themes such as open fire data for wildland fire modeling validation, digital twins for prescribed fires, and safe and efficient wildland fire data collection. <br/> <br/>Research tasks of this planning project include: 1) identification of key high-spatiotemporal-resolution fire metrics and data representations to support fire model validation and fire operations, 2) proposition of sensing strategies and algorithms for fire sensing and suppression robots and cyber physical systems that can support safe and efficient collection of desired high-resolution fire data, 3) development and evaluation of data assimilation and digital twin construction using high-resolution data to advance fire behavior modeling, coupled fire-atmosphere modeling, and smoke modeling, and 4) prototype and initial fire data ecosystem demonstration including collection of cultural burn data and establishment of GeoFireData, a benchmark fire data sharing and digital twin website, which can support different fire operation types such as fire spread model validation and controlled burn planning. The special attention will be devoted to interdisciplinary training of the next generation of scientists working with wildfire risks at the interface of computational sciences, engineering, ecology, and data sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2337943","CAREER: New data integration approaches for efficient and robust meta-estimation, model fusion and transfer learning","DMS","STATISTICS","06/01/2024","01/30/2024","Emily Hector","NC","North Carolina State University","Continuing Grant","Yulia Gel","05/31/2029","$85,143.00","","ehector@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","276950001","9195152444","MPS","126900","1045","$0.00","Statistical science aims to learn about natural phenomena by drawing generalizable conclusions from an aggregate of similar experimental observations. With the recent ?Big Data? and ?Open Science? revolutions, scientists have shifted their focus from aggregating individual observations to aggregating massive publicly available datasets. This endeavor is premised on the hope of improving the robustness and generalizability of findings by combining information from multiple datasets. For example, combining data on rare disease outcomes across the United States can paint a more reliable picture than basing conclusions only on a small number of cases in one hospital. Similarly, combining data on disease risk factors across the United States can distinguish local from national health trends. To date, statistical approaches to these data aggregation objectives have been limited to simple settings with limited practical utility. In response to this gap, this project develops new methods for aggregating information from multiple datasets in three distinct data integration problems grounded in scientific practice. The developed approaches are intuitive, principled and robust to substantial differences between datasets, and are broadly applicable in medical, economic and social sciences, among others. Among other applications, the project will deliver new tools to extract health insights from large electronic health records databases. The project will support undergraduate and graduate student training, course development, and the recruitment and professional mentoring of under-represented minorities in statistics. Further, the project will impact STEM education through a data science teacher training program in underserved communities.<br/><br/>This project develops intuitive, principled, robust and efficient methods in three essential data integration problems: meta-analysis, model fusion and transfer learning. First, the project delivers a set of meta-analysis methods for privacy-preserving one-shot estimation and inference using a new notion of dataset similarity. The primary novelty in the approach is the joint estimation of both dataset-specific parameters and a combined parameter that bears some similarity to the classic meta-estimator. Second, the project establishes model fusion methods that learn the clustering of similar datasets. The methods? unique feature is a model fusion that dials data integration along a spectrum of more to less fusion and thereby does not force model parameters from clustered datasets to be exactly equal. Third, the project develops flexible and robust transfer learning approaches that leverage historical information for improved statistical efficiency in a target dataset of interest. An important element of these approaches is a flexible specification of the type of models fit to the source datasets. All three sets of methods place a premium on interpretability, statistical efficiency and robustness of the inferential output. The project unifies the three sets of proposed methods under a formal data integration framework formulated around two axioms of data integration. Data integration ideas pervade every field of scientific study in which data are collected, and so the research contributes to scientific endeavors in the medical, economic and social sciences, among others.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."

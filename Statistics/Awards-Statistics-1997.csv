"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"9705045","Collaborative Research between General Motors Corporation   and Iowa State University","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","09/01/1997","09/05/1997","Dean Isaacson","IA","Iowa State University","Standard Grant","Lloyd E. Douglas","08/31/2000","$66,954.00","Kenneth Koehler, Stephen Vardeman","dli@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1253, 1269","0000, 1504, OTHR","$0.00","       The General Motors Corporation Technical Education Program   and the Department of Statistics at Iowa State University (ISU)  have a partnership under which General Motors (GM) employees  can earn a non-thesis M.S. Degree in Statistics taking regular  courses from the department via video tape.  The degree  requirements include a research component that results in a  formal paper (creative component).  Students from GM will have  co-major professors, a statistician from GM and a statistician  from ISU.  The purpose of this proposal is to create  opportunities for visits and interactions that will help the GM  students with this creative component research and introduce ISU  faculty to research problems of interest to industry.         This project will fund interactions of two different but  related types: 1) site visits by ISU personnel with GM students  and co-major professors and 2) collaborative research with  scientists at GM Research Labs.  In the summer of 1997, one ISU  statistician will visit GM students and co-major professors,  discussing company interests and areas for collaboration, and  identifying appropriate topics for creative component research.   In the summer of 1998, this person will spend 1 additional month  doing collaborative research with GM scientists.  In the summer  of 1998 a second faculty member will spend two months engaged  in visits and collaborative research.  As part of their visits  with GM research scientists, these faculty members will give  seminars for GM statisticians and engineers on modern statistical  methods.  This proposed interaction will produce topics for  publishable creative components, research ideas for ISU faculty  members and solutions for real industry problems.         The educational experience of the students at GM will be  greatly enhanced through the proposed interaction.  The GM  students will be exposed to both academic and industrial  approaches to research.  Ideally, their creative components will  both answer a particular q uestion for GM and raise new questions  for the ISU faculty members, that can serve as subsequent topics  for both personal research and for that of future ISU graduate  students.  The end result will then be the advancement of  knowledge in directions important to industry practice.         Since the projects at GM will likely combine statistics and  engineering, many will serve as focal points to bring together  ISU faculty members from Statistics and Engineering.  Additional  collaborative research at ISU is thus likely to follow from this  project.         This GOALI project is jointly supported by the MPS Office of  Multidisciplinary Activities (OMA) and the Division of  Mathematical Sciences (DMS)."
"9704573","Graphical Markov Models, Structural Equation Models, and    Related Models of Multivariate Dependence:  Structure,      Equivalence, Synthesis, and Extensions","DMS","STATISTICS","07/01/1997","03/24/1999","Michael Perlman","WA","University of Washington","Continuing Grant","Joseph M. Rosenblatt","12/31/2000","$313,722.00","David Madigan, Thomas Richardson","michael@ms.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, 9187, 9196, EGCH, OTHR","$0.00","DMS 9704573    Graphical Markov Models, Structural Equation Models, and   Related Models of Multivariate Dependence: Structure, Equivalence,   Synthesis, and Extensions.    David Madigan, Michael. D. Perlman, and Thomas. S.  Richardson    University of Washington  (together with Steen. A. Andersson, Indiana University)                                 ABSTRACT     Graphical Markov models (GMM) and the closely related structural   equation models (SEM) use graphs (= path diagrams), either   undirected, directed, or mixed, to represent multivariate   dependencies among stochastic variables in an economical and   computationally efficient manner. A GMM or SEM is constructed by   specifying local dependencies for each variable (= node of the   graph) in terms of its immediate neighbors, parents, or both, yet   can represent a highly varied and complex system of multivariate   dependencies by means of the global structure of the graph.   Nonetheless, the local specification permits efficiencies in   modeling, inference, and probabilistic calculations. This research  involves the development of more complex and comprehensive classes  of GMMs and SEMs, determination of the mathematical structure of   these (extremely vast) classes, and the development of more   efficient statistical and computational algorithms for the   discovery and analysis of appropriate models within these classes  for specific real-world applications.     Among their many applications, GMMs have become prevalent in   statistical science for the analysis of categorical data in   contingency tables, for the modeling of spatially-dependent   processes such as the spread of epidemics in human and animal   populations, and for the development of early warning systems   for severe weather conditions; in computer science (as Bayesian   networks) for information processing and retrieval, for robotics,  computer vision, and pattern recognition, for the debugging of   complex programs (such as Windows 95), and for the representation  of exp ert systems for medical diagnosis; and in decision science  (as influence diagrams) as models for information flow and control  and for combining the opinions of many decision-makers. SEMs have  long been used in fields such as genetics, sociology, econometrics,  and psychometrics as networks for representing the structure of   complex causal systems. A crucial feature of all these models is   that they are designed for fast computational implementation,  thereby facilitating the development of software that can ""reason""  about real world problems.    Related Websites:    LA Times Article, October 1996    http://www.hugin.dk/lat-bn.html   (Hosted by Hugin Website)    Microsoft trouble-shooting systems, employing GMMs   http://www.microsoft.com/support/tshooters.htm     Lockheed News Release describing application of GMMs   in Unmanned Underwater Vehicles.   http://www.lmsc.lockheed.com/newsbureau/pressreleases/9604.html    Air Force Institute of Technology Bayesian Network Page   http://www.afit.af.mil/Schools/EN/ENG/LABS/AI/BayesianNetworks    Website describing a GMM for forecasting severe weather in   NE Colorado  http://www.lis.pitt.edu/~dsl/hailfinder/"
"9703838","Analysis of Periodic Time Series","DMS","STATISTICS","07/01/1997","08/06/1997","Robert Lund","GA","University of Georgia Research Foundation Inc","Standard Grant","Marianthi Markatou","06/30/2001","$75,179.00","","rolund@ucsc.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, 1303, 9218, EGCH, HPCC, OTHR","$0.00","  Lund  9703838    This research considers periodic time series and their applications to problems in climatology.  Because of  the periodic nature of weather, tides, solar radiation, and other naturally cyclic processes, many related  time series inherit periodicities in their statistical structure. This research investigates some common   questions of analysis involving periodic series. Specifically examined are when a time series should be  regarded as periodic, how a periodic series should be modeled, how to estimate trends in periodic series,  and how to accurately forecast future values of periodic series.  To settle these questions, mathematical  properties of periodic autoregressive moving-average time series models are explored.  The main tool of  analysis is the time series Innovations Algorithm, which uses the derived covariance structure of the  periodic models being considered to compute model likelihoods and best linear predictors.      The mathematical and statistical results developed are used to investigate some current climatological  issues.  In particular, the developed trend estimation techniques are applied in a study of North American  temperature trends.  Trend estimates of periodic monthly series are compared to trend estimates of  stationary yearly series.  Since the yearly series are obtained by averaging the monthly series - a twelvefold series length reduction - the trend estimates from the monthly series are more accurate.  This results in a  better understanding of climatic change and global warming.  Likewise, the developed prediction methods  yield improved forecasts of climatological processes."
"9618715","New Testing Methodology for Ordered Categorical Data","DMS","STATISTICS","07/01/1997","02/01/1999","Arthur Cohen","NJ","Rutgers University New Brunswick","Continuing Grant","Joseph M. Rosenblatt","06/30/2001","$213,805.00","Harold Sackrowitz","artcohen@rci.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, 9148, 9161, 9183, AMPP, BIOT, MANU, OTHR","$0.00","  Cohen and Sackrowitz  9618715    A classical protocol for the evaluation of a treatment is to administer a  placebo to a set of individuals and  administer the treatment to another set of individuals.  Oftentimes the response variable is an ordered categorical variable such as no improvement, partial improvement, complete recovery.  The researcher is  interested in answering the question: Is the treatment ""better"" than the control?  Many recommended  testing procedures that are directed towards this question are found to be wanting.  Among these are the  likelihood ratio test and the Wilcoxon-Mann, Whitney test.   The research involves a review of various  definitions of ""better"" and the development of improved testing procedures.  The new procedures are  computer assisted and are shown to have desirable theoretical and practical properties.      The determination of whether or not a new product or new treatment or new method is better than a  standard is a fundamental issue in a variety of areas including biotechnology, materials, manufacturing,  and the social sciences.  Current methods used to make decisions in these situations are found to be either  inappropriate or, in many instances, weak in their ability to detect a superior product.  The investigators  study decision procedures based on a new approach which is effective in eliminating the shortcomings of  previous methods.  Theoretical arguments as well as extensive numerical calculations support the findings  and demonstrate the magnitude of the improvements."
"9700897","Statistics for Correlated Data: A Conference in Celebration of the 50th Anniversary of the Department of Statistics at  Iowa State University; Ames, Iowa; October 16-18, 1997","DMS","STATISTICS","09/01/1997","04/24/1997","Yasuo Amemiya","IA","Iowa State University","Standard Grant","James E. Gentle","02/28/1998","$11,900.00","Alicia Carriquiry","yza@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","0000, OTHR","$0.00"," DMS 9700897    Statistics for Correlated Data:  A Conference in Celebration of the 50th Anniversary of the Department of   Statistics at Iowa State University     Yasuo Amemiya    October 16-18, 1997  Scheman Building, Iowa State University, Ames, Iowa 50011    Abstract:    The Department of Statistics at Iowa State University, in conjunction   with its 50th anniversary, plans to host a conference that will   contribute to the advancement of statistical science and to the future   infrastructure of statistics.  The scientific theme of the conference   is ""Statistics for Correlated Data"", which covers many of the currently  important theoretical and applied research topics, and which   corresponds to the areas of the department's strength and tradition.    The plan for the conference is to make it as accessible as possible to   the diverse statistics community, and to encourage and support   researchers in early stages of their career.    Correlated data arise in modeling temporal and spatial processes, in   analyzing repeated measures, in complex sampling and experimental   schemes, and in a wide range of applied problems.  In this broad   subject, researchers tend to form separate groups defined by sub-topics  or the areas of application.  This conference attempts to gather   statisticians with interest in any of the theoretical or applied   sub-areas of correlated data statistics, and to serve as a place for   communicating and exchanging knowledge.  Sub-areas covered by the main   theme include time series, spatial statistics, sampling, longitudinal   analysis, multivariate analysis, applications to engineering, life, and  agricultural sciences.  It is planned to have 10-15 invited speakers   from outside Iowa State.  The conference also plans to have a special   invited poster session for young researchers.  This program will be   announced and advertised widely to obtain a large pool of applicants.  At least ten young researchers will be selected as invited   participants, and will be feat ured in the poster session.  In addition,  the participation of graduate students and young faculty members will   be promoted through the network of the Midwest departments of   statistics."
"9704649","Robust Parameter Design:  Modeling, Analysis and Layout     Techniques","DMS","STATISTICS","07/15/1997","07/14/1999","C. F. Jeff Wu","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Joseph M. Rosenblatt","06/30/2001","$262,609.00","Michael Hamada","jeffwu@isye.gatech.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, 9148, MANU, OTHR","$0.00","9704649  Chien-Fu  J. Wu     Robust parameter design employs statistical methods and engineering ideas to improve the quality of a product or process by making it less sensitive to uncontrollable variation.  It has had a big impact on manufacturing as demonstrated by many industrial case studies.  The main focus is to study modeling and analysis of data from parameter design experiments and layout techniques for efficient experimental planning.  First, three aspects of robust design with static characteristics are considered:  1. theoretical characterizations of robust factor settings and their relation to commonly used graphical methods; 2. loss due to poor quality using very general loss functions, especially where no adjustment factor can be identified; and 3. assessment of measurement systems that collect ordered categorical data.  A related problem to the last aspect is the modeling and analysis of ordered categorical data from designed experiments when some of the data responses are misclassified.  Next, two aspects of robust parameter design with functional characteristics (i.e., whose response is a functional relation, instead of a single value) are studied: comparison of generalized signal-to-noise ratio based method to a response function modeling method that employs a two-stage modeling, and the modeling and analysis of degradationcurves from experiments for robust reliability improvement. Finally, efficient planning of experiments is studied for parameter designswith static characteristics and with signal-response systems.  Novel theoretical and computational tools are employed to solve this problem.    Robust parameter design employs statistical methods and engineering ideas to improve the quality of a product or process by making it less sensitive to uncontrollable variation.  It has been successfully implemented as demonstrated by many industrial case studies.  The investigators study the use of robust parameter design in more complicated situations than considered to date, thereb y, extending the scope of robust parameter design.  Robust parameter design uses experiments whose two major issues are how to properly choose the experimental conditions (i.e., experimental plans) and how to model and analyze the data (i.e., analysis) to identify the important factors and recommend the optimal settings for these factors.  In Part I, (i) theoretical characterizations of robust factor settings are obtained and related to graphical methods, (ii) new strategies are developed for minimizing loss due to poor quality as measured by very general loss functions, (iii)assessment and analysis of measurement systems that collect ordered categorical data.  In Part II, (i) two methods of analyzing signal-response systems (e.g., gas gauge where the signal is the amount of gas in the gas tank and the response is the reading from the gas gauge) are compared (ii) general models and methods for analyzing degradation curves are investigated, (e.g., fluorescent light intensity decreases over time).  Using degradation data is strategic because information about reliability can be obtained sooner and more efficiently.  The research results in Part II will be tested on an automobile project.  In Part III, the problem of efficient and economic planning of experiments is studied for parameter design problems."
"9704711","Studies in Industrial Statistics","DMS","STATISTICS","07/15/1997","06/17/1999","Dennis Lin","PA","Pennsylvania State Univ University Park","Continuing Grant","Joseph M. Rosenblatt","06/30/2000","$132,450.00","","dkjlin@purdue.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, 9148, MANU, OTHR","$0.00","NSF DMS-9704711    Topics in Industrial Statistics  Dennis K.J. Lin   Penn State University    PROJECT SUMMARY      A major objective of the National Science Foundation is to improve the   nation's capacity for intellectual and economic growth.  It does this by supporting the discovery of new knowledge and the   enhancement of a skilled workforce.  American industry is becoming increasingly aware of the benefits   of running statistically designed experiments.  In complex experimental work, the number of potential factors is   large, but often only a few active factors are expected.  A problem frequently encountered is how to   identify the factors that matter, and consequently  improve the quality and productivity of the products.  The basic results we hope to obtain here will be extremely   useful to practitioners, since new designs will be furnished   that will save considerable cost when real world issues are involved.    When the number of potential factors is large and   only a few active factors are expected,  we do not need unbiased estimates to detect those active factors.  In this project, we propose the use of supersaturated   designs  --  allowing for a slight bias in estimation but   significantly reducing the design size in experimental work.  Some initial results were actually utilized in electronic   and chemical industries (specifically, IBM and Lonza).  The proposed methodology helps extract useful information that   may not be provided by ""traditional wisdom.""  From these practical experiences, it is anticipated that a much   wider application is possible.  On the other hand, this research has broad theoretical implications.   Some theoretical results have been obtained, and  it is shown that those results can be applied  to other statistical areas, such as linear models,   information theory and optimal design.  Another statistical area that has received a great deal of   attention in industry involves reliability problems.  The estimation of system (or component) reliability    is a common problem for many industries.  When a system consists of several components, and when the   cause of failure is unspecified, the estimation problem is   unresolved. This project proposes the use of a Bayesian approach.  Some initial results applied to IBM PS/2 personal computer data   have been obtained.  More general fundamental research will be   further studied.  Problems given here are very specific to be  realistically solved during the funding period.  However, it has a much wider research impact for both industrial   applications or academia type research."
"9704983","Statistical Methods for Models with Comstraints and         Incomplete Data","DMS","STATISTICS","07/15/1997","03/22/1999","Yehuda Vardi","NJ","Rutgers University New Brunswick","Continuing Grant","Joseph M. Rosenblatt","06/30/2000","$181,068.00","Cun-Hui Zhang","vardi@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, 9161, 9183, AMPP, BIOT, OTHR","$0.00","NSF: DMS 9704983   Statistical Methods for Models with   Constraints and Incomplete Data    Y.Vardi and C-H Zhang  Rutgers University    ABSTRACT      Statistical methods and inference tools for problems with   incomplete - data and constrained -parameters will be developed.    There are four main research topics: The first topic covers   regression and multisample data under selection bias that   depends on the response variable. A number of semiparametric   models (parametric weight functions, nonparametric  base  distribution) are studied and  maximum likelihood procedures  with related inference tools are developed. Applications to  HIV clinical trial are proposed.  The second topic focuses  on estimation in multinomial models subject to upper and lower  bound constraints on the cell probabilities. This  method is  incorporated into an  EM algorithm for image reconstruction  in discrete tomography . The third topic   extends current   semiparametric regression models to truncation-censoring   cases and heteroscedastic regression models with censored   data. The fourth topic centers on inference for   right-censored random (growth) curves, where the censoring  mechanism depends on the progression of the curve. This is  informative, nonignorable,  censoring and it poses  methodological challenges for the problems of comparing two  groups of curves (treatment and control). Methodolgy based  on U-type statistics is developed, and a semiparametric  accelerated growth regression model  is  introduced  to model relationship between the curves and their  corresponding covariates.    The research centers on statistical methods for problems  in which data collection is impaired by sampling bias,  missing observations, and models with partial information  on the  parameters.  The developed   methods contributes   directly to certain scientific areas considered of strategic  national importance. The research on estimating  cell-probabilities in constrained multinomial models is  incorporated in  an algo rithm for image reconstruction of  binary images in electron transmission tomography;   an important high-tech technology, still in research stages,  for estimating the atomic structure of a crystal layer by  taking projection measurements.  This is important in material  studies and in manufacturing.  The research on semiparametric  selection-bias models  relates to applications in   biotechnology, as it is  developed in preparation   for planned HIV vaccine trials. The goal here, is to have a   methodology  in place  for analyzing HIV-1 sequence data arising    from forthcoming  vaccine trials."
"9704524","Improved Estimation in Multivariate Location Models","DMS","STATISTICS","07/15/1997","03/24/1999","William Strawderman","NJ","Rutgers University New Brunswick","Continuing Grant","Joseph M. Rosenblatt","06/30/2001","$82,434.00","","straw@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, 9169, EGCH, OTHR","$0.00","9704524  STRAWDERMAN    This research project is devoted to the development of improved estimators in problems where several parameters are to be estimated.  Attention will be primarily focused on estimation of location parameters.  The types of improved estimators to be studied fall under the general category of shrinkage estimators and will typically be hierarchical bayes, empirical bayes or Stein-type estimators.  Attention will be focused on deriving improved estimators when the normality assumption is removed or the loss function is other than quadratic.  Conditions on prior distributions which lead to improved procedures will also be emphasized.    One of the most important problems in science involves the estimation of unknown quantities based on data obtained via experimentation and sampling.  It is typically the case that several parameters are to be estimated.  A standard method of estimating such  quantities ( called  parameters by statisticians) is to take the averages of several observations based on a series of experiments.  In certain circumstances, when several such parameters are to be estimated, it has been shown that one may obtain improved estimators by combining data across (seemingly) unrelated experiments.  We study this phenomenon, extend the class of problems where such improved estimators exist, and study the extent  of improvement possible over standard procedures.  This research is applicable  in virtually all areas of physical, biological and social science science.  In particular the author has been involved in a number of studies in environmental sciences where these techniques have been successfully employed."
"9703812","Research on Adaptive Estimation and Control of Dynamical    Systems","DMS","PROBABILITY, STATISTICS","08/01/1997","02/04/2000","Michael Katehakis","NJ","Rutgers University New Brunswick","Continuing Grant","Dean Evasius","07/31/2000","$100,000.00","Herbert Robbins","mnk@rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1263, 1269","0000, 9148, 9215, HPCC, MANU, OTHR","$0.00","DMS 9703812  Research on  Adaptive Estimation and Control of      Dynamical Systems.     Michael N. Katehakis and Herbert Robbins    Rutgers University    Abstract    This research involves work on adaptive control of dynamic systems.   The basic dynamic model is known as the ""Markov decision process with   incomplete information"" (MDP) problem,  where the transition law   and/or the expected one-period rewards may depend on unknown   parameters.  The most notable results in this area are based on ideas   utilizing either a separation principle and the related   certainty-equivalence rule, or uniformly efficient rules for the   model of  sequential allocation known as the multi-armed bandit   (MAB) problem.  Limitations of the certainty-equivalence rule are:   i)  there is no claim on the rate of convergence, and    ii)  there are cases  for which, with positive probability, this   rule can prematurely converge to a wrong parameter value so that   it eventually uses only a non-optimal policy. The  typical approach  in the latter studies  has been to  fit  the larger MDP model into  the smaller MAB one by considering each deterministic policy as a   reward-generating population (bandit).  A consequence of this is   that the resulting statistically  efficient procedures  involve   sampling from all deterministic policies and do not otherwise   utilize the optimization aspect of the problem. Thus, they become   limited in scope by data collection complexity.  The reason is   that in practice the state spaces of  MDP models tend  to be very  large and the set of deterministic policies is   immense.  In recent work the investigators have obtained adaptive   procedures with data collection requirements that are  proportional   to the number of state - action pairs of the MDP, under a minimal   irreducibility condition. A major direction of the proposed research   involves  the development of solutions for important more general   problems  such as  i) multi-chain MDPs,  ii) the case in which there   a re side constraints, and  iii) discounted streams of rewards.  A   second  important goal is the development of  new adaptive   statistical methods  that possess practically useful implementation   and optimality  properties for the related problems of detection of    total error and change points.      The main idea of adaptive control is to compute strategies (policies,   or control rules) for the operation of  a system that estimate the   unknown parameters of the system, and in doing so converge to a   strategy that is optimal for the true values of the unknown   parameters.  Applications arise in many  areas of  modern engineering,   finance, and operations research, such as reliability, maintenance,  quality control,  scheduling,  inventory, and  production planning.   Consequently, this type of problem has been widely studied in the   literature. However,  effective procedures that take into account and   optimize  the speed of  convergence  have been obtained only recently   for specific models, often, with prohibitive data collection   complexity.  A primary objective of the proposed research is the   development of relatively simple adaptive control procedures with   reasonable computational and memory  requirements for on-line   implementation, for a wide class of problems, utilizing ideas from   recent work of the investigators. Another  important goal is the   development of  new methods for specific models useful in such areas   as software reliability (error detection)  and  quality control   (change points).  This research relates to the following strategic  areas of national concern: high performance computing, communications,  and manufacturing."
"9703777","Foundations of Regression Graphics","DMS","STATISTICS","07/15/1997","04/30/2001","Ralph Cook","MN","University of Minnesota-Twin Cities","Standard Grant","John Stufken","06/30/2002","$120,000.00","","dennis@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","DMS 9703777    Foundations of Regression Graphics  Dennis Cook  University of Minnesota    ABSTRACT    This research centers on finding how  real-time, computationally intensive, graphical methods can be used to   investigate the statistical structure of regression problems.  It is   based on the premise that the full benefit of modern computer graphics   cannot be realized in regression analyses without sound theoretical   foundations.  Emphasis is placed on the expansion and application of   an emerging theory for regression graphics based on dimension-reduction   subspaces, and on the implications of this theory for the interpretation   of standard graphical constructions. Graphical foundations for   regressions with a censored survival time as the response are given   special attention.      The ability to visualize statistical data plays an important role   in analyzing complex relationships.  For example, a measure of   the marginal impact of mandatory testing in grade schools   requires understanding how different socioeconomic    settings influence performance. Recent advances in high performance   computing, one of the Federal Strategic Areas, allow for the opportunity   to develop new graphical techniques that go far beyond traditional   graphics in both applicability and power. This research centers on   finding how computer graphics can be used to investigate complex   statistical relationships.  It stems from the notion that, even in   complex systems, it is possible to construct easily understood graphical   displays that contain all or nearly all of the relevant information in   the data.  This ability to preserve information in easily understood   displays requires new theoretical foundations that are an   integral part of this study."
"9704327","Statistical Theory for Classification","DMS","STATISTICS","07/01/1997","09/29/2000","John Hartigan","CT","Yale University","Continuing Grant","Joseph M. Rosenblatt","09/30/2000","$190,000.00","","hartigan@stat.yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","0000, 9218, HPCC, OTHR","$0.00","  Hartigan  9704327    I explore the role of classification in probability and statistics. I use probability models in forming and  validating classes, and in identifying modes and other new objects, but I also use classification for  prediction and forming probability models. For example, in recognition problems, people compare new  objects with classes of known objects, and predict behavior of these new objects by assigning the new  objects to the classes and expecting the objects to behave similarly to known objects in those classes.      I consider how properly formed classes may be used for understanding and prediction. For example, in  congressional voting, members form a series of semi-permanent alliances or blocs that vote together on  certain classes of issues. The size and constitution of these blocs offers a way to study the evolution of  Congress."
"9700733","International Workshop on Wavelets in Statistics;           October 12-13, 1997; Durham, NC","DMS","STATISTICS","09/01/1997","05/22/1997","Brani Vidakovic","NC","Duke University","Standard Grant","James E. Gentle","08/31/1998","$10,000.00","","brani@tamu.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, OTHR","$0.00","DMS 9700733    INTERNATIONAL WORKSHOP ON WAVELETS IN STATISTICS   ISDS, Duke University, Durham, NC, USA.    October 12-13, 1997.      Brani Vidakovic    Abstract.    Statistical wavelet modeling and computational research have,   in recent years, become burgeoning areas in both theoretical   and applied statistics, and are beginning to impact developments   in statistical methodology and in various applied scientific fields.  Wavelet ideas are developing in statistics in areas such as   regression, density and function estimation, factor analysis,   modeling and forecasting in time series analysis, spatial statistics,   with ranges of application areas in science and engineering.  The emerging interests in Bayesian statistical modeling and wavelets   is generating exciting new directions for the interface of two research areas, with significant potential for future impact on applied work.    This international research workshop, focused on the interface of  wavelets and statistical science, is held in recognition of the recent   growth and current vibrancy of research at the interface of the   disciplines.  The meeting will provide an opportunity to summarize   the current status of research on wavelets in statistics, to explore   the applied impact that statistical wavelet modeling and wavelet in   statistics are having, and to suggest and stimulate novel theoretical,   methodological and computational research directions.  The workshop is   planned at an opportune time to assess and address the dual question   of the future prospects: for significant contributions of wavelet   ideas to statistical science and application, and for furthering   watelet-based technology through the use of statistical concepts and   models."
"9704495","Statistical Integration and Approximation","DMS","STATISTICS","07/01/1997","06/29/1999","Art Owen","CA","Stanford University","Continuing Grant","Joseph M. Rosenblatt","06/30/2000","$180,000.00","","owen@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","9218, HPCC","$0.00","  Owen  9704495    This research investigates the use of statistical sampling ideas in high dimensional numerical problems, particularly numerical integration and approximation.  Statistical thinking enters the picture because in  high dimensional problems one can get at most a sparse sample of the input space.  Surprising benefits  can accrue: for example it is known that randomizing a low discrepancy sequence can increase it's  accuracy.   For the case of numerical integration, this project looks at adaptive importance sampling  techniques in which the function values themselves are used to adapt the sampling towards the most  relevant region of the input space, while maintaining a data based estimate of integral accuracy.  In the  case of numerical approximation, this project will apply recent progress in hybrid numerical integration  methods to finding ways of building sample based approximations of functions with data determined error  estimates.    Numerical integration is a fundamental computational task with applications in physics, chemistry,  finance, statistics, and numerical approximation.  Numerical approximation is also fundamental, and the  advent of powerful and flexible computer simulators of scientific and engineering phenomena can only  increase the demand for computer experiment techniques based on approximation.  At present these computer models are widely used in the design of computer chips, automobile parts, airplanes, and in  global climate modeling.  These functions may take hours or even days of computer time and so finding  fast approximations (and estimating their accuracy) is important.  Better exploration of these functions,  call it function mining, can lead to faster introduction and greater reliability for new products.  It can also  be used to thoroughly explore the predictions of models for environmental change and storage of nuclear  wastes."
"9704487","Computer-intensive Methods for the Statistical Analysis     of Dependent Data","DMS","STATISTICS","09/01/1997","06/26/1997","Joseph Romano","CA","Stanford University","Standard Grant","William B. Smith","08/31/2001","$88,154.00","","romano@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","9148, 9197, 9218, EGCH, HPCC, MANU","$0.00","  Romano  9704487       This project involves the on-going development of resampling and subsampling as they apply to dependent data, i.e., time series, random fields, and (marked) point processes. The investigator's efforts are directed towards: (a) relaxing the conditions for asymptotic validity of computer-intensive methods for  dependent data (e.g., allow for  nonstationarity, slow mixing rate, etc.); (b) rendering  subsampling more `automatic' by devising a built-in procedure for estimation of the rate of convergence of the statistic in  question; (c) improving the accuracy of distribution estimation by techniques such as Richardson  extrapolation, and by optimal choice of block size; (d) developing appropriate resampling/subsampling  methods in the case of alternative data-collection scenarios (e.g. in the case of measurements at irregularly spaced locations, i.e., data from a marked point process; and (e) exploring the idea of `local' resampling    for time series.    This project involves the development of computer-intensive methods of statistical inference for the  analysis of  dependent data without having to rely on unrealistic or unverifiable model assumptions.  The  statistical analysis of dependent data is vital in many diverse scientific disciplines; thus this research has potentially many practical applications. For example, consider the problem of stochastic computer  simulation of complex Manufacturing Systems that is an important issue in Industrial Engineering; the methodology of subsampling for `almost' stationary time series is most helpful in order to assess  convergence and accuracy of the simulation. For another example, suppose that X(t) denotes  an  environmental measurement, e.g.  rain precipitation or ozone concentration  as measured at location t.   Typically, the t-points where X(t)  is measured are irregularly scattered in space or on the earth's surface.   The development of a general methodology for statistical inference involving data of this type is a  signif icant contribution in spatial statistics and, in particular, studies involving environmental data."
"9704557","Estimating Intrinsic Dimensionality","DMS","STATISTICS","08/01/1997","05/16/1997","Guenther Walther","CA","Stanford University","Standard Grant","William B. Smith","07/31/2001","$91,479.00","","Walther@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","  Walther  9704557    The goal of this research is to detect nonlinear structure in multivariate data by estimating the intrinsic dimensionality of a data set.  A multivariate data set that, apart from noise, falls into a lower-dimensional smooth submanifold is said to have intrinsic dimensionality equal to the smallest dimension of such a submanifold.  A knowledge or estimate of the intrinsic dimensionality of a data set contributes to the solution of two important problems in multivariate statistics and pattern analysis: The problem of finding an appropriate number of parameters for representing the data, and the problem of deciding whether a two- or three-dimensional representation of the data exists, which may then be analyzed visually.  This research develops a new way to estimate intrinsic dimensionality that promises be superior to existing methods for heuristic reasons, and investigates the statistical properties of the new estimator and its competitors.   The new estimator uses a method to smooth the shape of a multivariate data set in a nonlinear way which is based on tools from the field of mathematical morphology.     The goal of this research is to detect certain patterns in data, such as structured parts in medical images. Quite complicated and high-dimensional data sets have often certain simple, low-dimensional geometric structures in it. To extract information from these data in an efficient way it is important to find such structures and describe them in ways that are simple and and amenable for the processing by computers. This research uses certain geometric tools to develop such a processing system in a context where the patterns to be found are corrupted by noise, e.g. transmission errors or blurring in images."
"9700867","Pilot Projects to Explore Large Data Sets","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, COMPUTATIONAL MATHEMATICS","06/15/1997","03/27/1998","Jerome Sacks","NC","National Institute of Statistical Sciences","Standard Grant","Michael Steuerwalt","05/31/2001","$810,582.00","Alan Karr","sacks@niss.org","19 TW ALEXANDER DR","RESEARCH TRIANGLE PARK","NC","277090152","9196859300","MPS","1253, 1269, 1271","0000, 1504, 9263, OTHR","$0.00","Large data sets are a given in modern industry, technology and  science, and demand statistical attention to treat such central  issues as detecting new patterns and relationships in a reliable  and computationally feasible way.  While the need is apparent,  paths to bringing statistical science to bear on large data sets  are not clearly mapped, in part because the sheer volume of the  data prevents direct use of standard techniques.      This proposal comprises a pair of interconnected pilot projects  targeted to stimulate statistical science entree to this rapidly  changing terrain.  Each has a major industria;l partner, which  has committed substantial resources.  The projects and partners  are Drug Discovery (Glaxo Wellcome, Research Traingle Park, NC)  and Telecommunications Fraud (AT&T Laboratories, Murray Hill,  NJ).  In both instances there are specific scientific isues with  high-stakes implications for the industry at large; each is  speculative, in the sense that the path from data to information  to knowledge is not known in advance.    In drug discovery, the critical problem is to find new potent,  non-toxic compounds.  New statistical methods will be developed  to search substantial data sets generated by recent advances in  robotic synthesis and screening in order to identify key features  of compounds, leading to better ones, in the presence of highly  complex (and very high-dimensional) descriptions of the  molecules.    Within the hundreds of millions of long distance calls per day a  small fraction are illegal or unauthorized, but result in costs  of hundreds of millions of dollars annually.  The problem is to  detect and characterize, as rapidly as possible, patterns of  transactions that are unusual, and potentially fraudulent, with  special attention to controlling error rates (false alarms,  failures to detect), an issue exacerbated by the enormous  multiplicity of decisions made on a continual basis.    The two problem areas have common features: sequential  statistical  search for important or unusual patters in the data;  data that are highly complex in sheer quantity, in high-dimensional description of each data point, or in the diversity  of their sources.  Approaching these issues will be done by teams  of cross-disciplinary researchers at distributed sites and  closely managed by NISS.    This GOALI project is jointly supported by the MPS Office of  Multidisciplinary Activities (OMA) and the Division of  Mathematical Sciences (DMS)."
"9704432","Bayesian Time Series and Dynamic Models","DMS","STATISTICS","07/15/1997","04/20/2000","Mike West","NC","Duke University","Continuing Grant","Joseph M. Rosenblatt","06/30/2001","$272,000.00","","Mike.West@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, 9183, 9197, BIOT, EGCH, OTHR","$0.00","Bayesian Time Series and Dynamic Models  Mike West    Abstract    This research involves development of new statistical models,   methods and computational algorithms for analysis and forecasting  of time series. The investigations will include various aspects  of Bayesian state-space modelling for time series, introducing  new models for non-stationary time series models, methodology for  structured latent process analysis and time series decompositions,  new models and computational methods for multiple time series  analysis including dynamic latent factor models, dynamic Bayesian  hierarchical models, and non-Gaussian and non-linear models.   Part of the research involves investigation of strategies for   dealing with high-dimensional time series data sets and issues of  time series data reduction and summary. Stochastic simulation   and computer algorithms are ingredients, and will lead to new   computational tools for simulation-based Bayesian analyses in  various problems of dynamic modelling and time series analysis.  Related areas of the research will include time series model  uncertainty, assessment and prior specifications, problems of  contamination structures and data cleaning, and models   involving statistical mixture structure.     These projects represent advances in time series modelling and  analysis, and contributions to other areas of statistical science,  that will be of broad interest and practical relevance to   scientists and researchers in various disciplines as well as   to statisticians.  The new statistical methods resulting from  this project extend the scope of applicability of current time   series analysis tools to more complex, non-linear and multivariate  problems. A significant component of the research involves  computationally intensive methods to fit and explore models to  data in applications. The research is motivated, in part, by  existing problems of statistical modelling and inference in  current application areas in several fields, specifically  inclu ding projects in the neurosciences, in geology driven by  issues of global environmental change, in financial modelling  and forecasting, and in biomedical engineering and signal  processing."
"9704425","Non-Stationary Models for Spatial Statistics and Bayesian   Image Analysis","DMS","STATISTICS","07/15/1997","06/17/1999","David Higdon","NC","Duke University","Continuing Grant","Joseph M. Rosenblatt","06/30/2001","$141,000.00","","dhigdon@bi.vt.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","9183, 9189, 9197, BIOT, EGCH","$0.00","NSF DMS-9704425    Non-Stationary Models for Spatial Statistics and Bayesian Image   Analysis.    David Higdon  Duke University              Abstract:  This research develops anisotropic spatial  models for fully Bayesian inference with applications   to data which are correlated over space and time.  Two   distinct directions are taken.  First, in continuation of   previous research, Markov random field priors for lattices   are considered for modeling processes with occasional large   shifts in level.  Second, using a constructive convolution   approach, continuous models are developed that allow   non-stationarity and correlation structure that can vary over   space (and/or time).  Using such models for inference requires   that one account for uncertainty about just how flexible or  varying such models are.  A key part of the research is  developing methods which incorporate this source of uncertainty  in the statistical analysis.    Statistical modeling of phenomena that evolve over space and  possibly time is critical in areas such as medical imaging,  environmental monitoring, and detecting changes over time in  global climate.  Most spatial models in statistics assume that  over a given area, the underlying properties of a spatial (and  possibly temporal as well) pattern remain unchanged over the  region.  Though this is a useful simplification over limited  regions, on a larger scale such an assumption often results   in an unsatisfactory model.  This research will develop  statistical models that account for abrupt changes in the  nature of the spatial phenomenon.  Also, models will be   developed that allow properties of the spatial process to  evolve over spatial location.  This research will lead to  more realistic models for large scale spatial and spatio-temporal   phenomena.  Fitting these models to large datasets will require a   substantial computing effort and will likely rely high performance   computing resources.  Applications in global ocean climate change,   agriculture,  genetics, and medical imaging will be considered."
"9704400","Reliability, Statistical Applications of Order Restricted   Methods","DMS","STATISTICS","07/01/1997","07/20/1999","Henry Block","PA","University of Pittsburgh","Continuing Grant","Joseph M. Rosenblatt","06/30/2001","$274,657.00","Thomas Savits, Allan Sampson","hwb@stat.pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","0000, 9148, MANU, OTHR","$0.00","  Block, Sampson and Savits  9704400    This research focuses on the development of methodology and theory for some major problems facing the  statistics and reliability communities. One major emphasis is on the topic of burn-in. On-going research  involves determining optimal burn-in for classes of distributions and families of cost structures. Of  particular interest is the family of distributions with bathtub shaped hazard functions.  A second major  emphasis of this research is in the development of methods to support the use of the modern nonparametric ideas of order restricted inference in data analysis involving multivariate observations. A  great deal of theory has been developed for order restricted inference; however, the research focus here is  to utilize some of these results in important estimation and testing problems, in part, supported by the application of some of the computational results obtained by the investigators. This research makes   effective use of this elegant class of nonparametric procedures in modern data analysis.  Two other areas  of research are the further development of models and methodologies for data arising from accelerated  degradation experiments, and the continued development of the modeling of random length multivariate  data. These four research areas, while dealing with varied problems, utilize theory and methods from the  allied areas of reliability, order restricted inference and multivariate analysis.              Burn-in is a widely used engineering practice for increasing reliability by eliminating weak industrial  products before they are released for sale or use. This is an important topic in many modern American  industrial processes. Statistical tools are being developed to implement new production strategies which  are useful in determining how to deal with items which might fail early in critical uses.  The clinical  development of cost-effective new bioengineered pharmaceuticals often requires identification of those  patients who would b enefit substantially more from new biopharmaceuticals than from standard available  treatments. Some of this research focuses on developing improved statistical techniques to better identify  those patients who would significantly benefit from the new biotechnology.  Other aspects of the research  involve obtaining more effective statistical approaches to allow clinicians to predict the likelihood of  specific patients having a successsful response to a new treatment."
"9896159","Mathematical Sciences:  Computer Intensive Methods for      the Statistical Analysis of Time Series and Random Fields","DMS","STATISTICS","07/01/1997","03/13/1998","Dimitris Politis","CA","University of California-San Diego","Standard Grant","Stephen M. Samuels","05/31/1998","$2,452.00","","dpolitis@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","0000, OTHR","$0.00",""
"9703964","Computer-intensive Methods for the Statistical Analysis of  Dependent Data","DMS","STATISTICS","07/01/1997","04/03/1998","Dimitris Politis","CA","University of California-San Diego","Standard Grant","Joseph M. Rosenblatt","06/30/2000","$75,552.00","","dpolitis@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","0000, 9148, 9197, 9218, EGCH, HPCC, MANU, OTHR","$0.00","Politis  9703964    This project involves the on-going development of resampling and subsampling as they apply to  dependent data, i.e., time series, random fields, and (marked) point processes.  The investigator's efforts redirected towards:  (a) relaxing the conditions for asymptotic validity of computer-intensive methods for dependent data (e.g., allow for  nonstationarity, slow mixing rate, etc.); (b) rendering subsampling more `automatic' by devising a built-in procedure for estimation of the rate of convergence of the statistic in question; (c) improving the accuracy of distribution estimation by techniques such as Richardson extrapolation, and by optimal choice of block size; (d) developing appropriate resampling/subsampling methods in the case of alternative data-collection scenarios as is the case of measurements at irregularly spaced locations, i.e., data from a marked point process; and (e) exploring the idea of `local' resampling for time series.     This research focuses on the development of computer-intensive methods of statistical inference for the  analysis of dependent data without having to rely on unrealistic or unverifiable model assumptions.  The statistical analysis of dependent data is vital in many diverse scientific disciplines; thus this research has potentially many practical applications.  For example, consider the problem of stochastic computer simulation of complex manufacturing systems that is an important issue in Industrial Engineering; the methodology of subsampling for `almost' stationary time series is most helpful in order to assess convergence and accuracy of the simulation.  For another example, suppose that X(t) denotes an environmental measurement, e.g., rain precipitation or ozone concentration as measured at location t. Typically, the t-points where X(t) is measured are irregularly scattered in space or on the earth's surface.  The development of a general methodology for statistical inference involving data of this type is a significant contribution i n spatial statistics and, in particular, studies involving geological, atmospheric, and environmental data."
"9703876","New Approaches to Nonparametric Regression Estimation for   Continuous and Discrete Time Series","DMS","STATISTICS","07/01/1997","03/22/1999","Elias Masry","CA","University of California-San Diego","Continuing Grant","Joseph M. Rosenblatt","06/30/2001","$162,000.00","","masry@ece.ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","9197, 9218, EGCH, HPCC","$0.00","  Masry  9703876    The research is concerned with regression functions estimation in a hybrid setting: the underlying  processes are continuous in time, and so is the regression function, but the observations are taken at  discrete-times.  This is common in many practical situations where the collected data is irregularly spaced  as in environmental and oceanographic studies.   The instants at which the data is obtained constitute a  point process with a FINITE mean sampling rate but nevertheless result in no loss of information.   The  goal of the research is to identify appropriate non-equally spaced  sampling schemes; formulate suitable  estimates based on a modified Nadaraya-Watson approach and on the more recent local polynomial fitting  approach; and study their convergence properties including asymptotic normality and rates of strong  convergence. A fundamental requirement is that the consistency of the regression estimates, as the  number of observations tends to infinity, holds for ALL positive values of the mean sampling rate. Results  of this type do not hold for conventional equally-spaced data unless the sampling rate is allowed to diverge  to infinity.      Scientists and engineers collect huge amounts of data in wide range of disciplines including   communication systems (e.g., Internet traffic, satellites), econometrics (e.g. stock market), geology (e.g.  earthquakes), and environmental science (e.g. pollution levels). While much of the observed data is  continuous in time, the processing of the data is carried out by using computers which convert the data to  a digital form. The research develops digital processing methodologies (irregular sampling  methodologies) which do not lose any information during the conversion process from continuous-time  data to discrete-time data. The context of the research is 1) to forecast the future evolution of the  phenomena being observed and 2) to filter signals observed in the presence of corrupting noise."
"9896312","Mathematical Sciences: Presidential Young Investigator Award","DMS","STATISTICS","09/01/1997","08/17/1998","Janis Hardwick","IN","Purdue Research Foundation","Continuing Grant","Joseph M. Rosenblatt","08/31/1999","$39,251.00","","jphard@umich.edu","1281 WIN HENTSCHEL BLVD","WEST LAFAYETTE","IN","479064182","3174946200","MPS","1269","0000, 9227, OTHR","$0.00",""
"9705209","Multidimensional Depth Functions, Multidimensional          Generalized L-Statistics, and Related Procedures","DMS","STATISTICS","07/15/1997","06/27/1997","Robert Serfling","TX","University of Texas at Dallas","Standard Grant","William B. Smith","06/30/2001","$95,774.00","","serfling@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1269","0000, OTHR","$0.00","DMS 9705209  Serfling    In order to provide strengthened foundations for statistical analysis of multidimensional data, this research develops a general theory of statistical depth functions.  General theory is developed which unifies and extends the few examples of depth functions presently in the literature.  Based on the notion of center-outward ordering of multidimensional data points by ""depth,""  corresponding notions of multidimensional location, spread, quantiles, ranks, and other traditional one-dimensional sample statistics are formulated and studied in this research.  In this framework, statistics such as multidimensional L-statistics, rank statistics, and generalized forms of these statistics, are investigated (extending previous work of the investigator for the one-dimensional case).  Further, corresponding notions of ""contours"" are investigated.  Statistics are developed which perform well overall with respect to robustness criteria (e.g., breakdown points), equivariance (relevant to the geometric structure), computational ease, conceptual consistency (with associated population notions), and theoretical tractability.Tools used, and further developed, for this research include functional analytic and U-statistic methods.  Application contexts receiving special attention include robust and nonparametric regression and analysis of variance.    This research develops improved methods for analyzing multidimensional data.  For a ""cloud"" of data points, one wishes to have a sense of where the ""center"" is located, for example.  One can take the average of the points, or one can seek to define a ""middle point"" that is less influenced by the extremities of the data cloud.  Similarly, other representative features of the data cloud need to be defined as analogues or extensions of concepts already in use for analysis of simple one-dimensional data.  This research systematically treats such issues and develops new methods to be put into practice.  Such summary statistics enable the main featur es of a data cloud to be conveyed by means of a few easily interpretable numbers, thus enabling one to describe the data adequately within the confines of a conventional statistical report.  Whereas visual methods lose their effectiveness for dimensions greater than three, the summarizing methods developed in this research apply equally well for any number of dimensions.  Multidimensional data sets are arising increasingly in the very complex data-gathering activities now pursued in the various arenas of modern society and strategic national concern.  This research leads to tools for simplification and reduction of this complexity.  Further, this study develops methods for interpreting the data as but a sample from a target population about which one seeks to make statistical inferences.  For example, the question of what exactly is being estimated by such data is addressed.  Basic mathematical advances needed for development of these new statistical methods are also accomplished as part of this research."
"9704516","Graphical Markov Models, Structural Equation Models, and    Related Models of Multivariate Dependence:  Structure,      Equivalence, Synthesis, and Extensions","DMS","STATISTICS","07/01/1997","03/24/1999","Steen Andersson","IN","Indiana University","Continuing Grant","Joseph M. Rosenblatt","06/30/2001","$120,026.00","","standers@indiana.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","MPS","1269","0000, 9196, 9197, EGCH, OTHR","$0.00","DMS 9704516    Graphical Markov Models, Structural Equation Models, and   Related Models of Multivariate Dependence: Structure, Equivalence,   Synthesis, and Extensions.                            Steen A. Andersson                          Indiana University            (together with David Madigan, Michael. D. Perlman,            and Thomas. S.  Richardson, University of Washington)                                 ABSTRACT     Graphical Markov models (GMM) and the closely related structural   equation models (SEM) use graphs (= path diagrams), either   undirected, directed, or mixed, to represent multivariate   dependencies among stochastic variables in an economical and   computationally efficient manner. A GMM or SEM is constructed by   specifying local dependencies for each variable (= node of the   graph) in terms of its immediate neighbors, parents, or both, yet   can represent a highly varied and complex system of multivariate   dependencies by means of the global structure of the graph.   Nonetheless, the local specification permits efficiencies in   modeling, inference, and probabilistic calculations. This research  involves the development of more complex and comprehensive classes  of GMMs and SEMs, determination of the mathematical structure of   these (extremely vast) classes, and the development of more   efficient statistical and computational algorithms for the   discovery and analysis of appropriate models within these classes  for specific real-world applications.     Among their many applications, GMMs have become prevalent in   statistical science for the analysis of categorical data in   contingency tables, for the modeling of spatially-dependent   processes such as the spread of epidemics in human and animal   populations, and for the development of early warning systems   for severe weather conditions; in computer science (as Bayesian   networks) for information processing and retrieval, for robotics,  computer vision, and pattern recognition, for the debugging of   complex  programs (such as Windows 95), and for the representation  of expert systems for medical diagnosis; and in decision science  (as influence diagrams) as models for information flow and control  and for combining the opinions of many decision-makers. SEMs have  long been used in fields such as genetics, sociology, econometrics,  and psychometrics as networks for representing the structure of   complex causal systems. A crucial feature of all these models is   that they are designed for fast computational implementation,  thereby facilitating the development of software that can ""reason""  about real world problems.    Related Websites:    LA Times Article, October 1996    http://www.hugin.dk/lat-bn.html   (Hosted by Hugin Website)    Microsoft trouble-shooting systems, employing GMMs   http://www.microsoft.com/support/tshooters.htm     Lockheed News Release describing application of GMMs   in Unmanned Underwater Vehicles.   http://www.lmsc.lockheed.com/newsbureau/pressreleases/9604.html    Air Force Institute of Technology Bayesian Network Page   http://www.afit.af.mil/Schools/EN/ENG/LABS/AI/BayesianNetworks    Website describing a GMM for forecasting severe weather in   NE Colorado  http://www.lis.pitt.edu/~dsl/hailfinder/"
"9704732","Computationally Tractable Estimation Methods for Markov     Processes","DMS","STATISTICS","08/01/1997","07/06/1999","Peter Glynn","CA","Stanford University","Continuing Grant","Dean Evasius","01/31/2001","$85,500.00","","glynn@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, 9218, HPCC, OTHR","$0.00","NSF DMS-9704732    Computationally Tractable Estimation Methods for Markov Processes    Peter W. Glynn   Stanford University     Markov process theory provides a rich analytic and probablistic  structure which is intrinsically natural from a modelling  perspective.  Much of the literature on inference for continuous-  time Markov processes assumes that the process has been observed  continuously over some time interval. However, in practice, many  of the data sets available involve observing only discrete   ``snapshots'' of the process.  Existing theory on parameter  estimation in this setting often results in computationally  prohibitive methodologies. This research addresses the issue of  computationally tractable parameter estimation for discretely  observed continuous-time Markov processes. Many of the results  utilize Monte Carlo simulation to achieve tractability.  The   second fundamental question examined in this research is that   of error propagation through a stochastic system. When the output  performance measure of the stochastic system cannot be evaluated  in closed form the output measure is simulated from the modelled  system. To address this situation, the investigators develop  estimators and their sampling properties for functionals of the  stochastic system.    Statistical models have always proven a powerful tool for  purposes of modelling, understanding and predicting complex  systems.  This research advances the frontier of statistical  models for dependent observations.  For example, the statistical  methods of this research are applicable to the diverse areas of  modelling levels of pollutants and contaminants in air, soil   and water which evolve over time and/or space; forecasting  changes in the stock market; and predicting or assessing demand  for the development of an optimal communications network.  The  use of statistical models for purposes of modelling complex  systems has been limited in the past due to the state of   computing power; a state which has certainl y improved in recent  years.  In this research, the investigators develop a framework  for practical implementation of advanced statistical methodologies  which capitalizes fully on the high performance computing  available today. The research addresses the issue of modelling  under partially observed information.  For example, the electrical  or computer engineer may use such models to assess network status   when only partial information is available on the state of the  system; in predicting air quality for a given region, observations  of pollutant levels are made at sites irregularly located over the  region and often at irregular points in time. The theoretical  constructs necessary to implement the models in the more common  scenario when only partial information is available are presented.  Additionally, this research involves error assessment of the   predictions or output performance measure of an estimated complex  system again capitalizing on the availability of high powered  computing."
"9704431","New Directions in Predictive Learning for Classification","DMS","STATISTICS","07/15/1997","03/27/2001","Jerome Friedman","CA","Stanford University","Continuing Grant","John Stufken","06/30/2003","$398,670.00","","jhf@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, 9148, 9183, 9197, 9216, BIOT, EGCH, HPCC, MANU, OTHR","$0.00","Friedman  9704431    A predictive learning system is a computer program that constructs rules for predicting values of some property of a real system (""output"") given the values of other properties (""inputs"") of that system. Predictive learning systems attempt to construct or ""learn"" useful prediction rules purely by processing data taken from past successfully solved cases; that is, cases for which the values of both the output and input variables have been determined.  The role of the learning algorithm is to automatically extract and organize the information in this data to obtain accurate rules for predicting output values of new cases in which only the values of the input variables are known.    This research concentrates on new paradigms for the development of predictive learning methodology quite different from past approaches.  The goal is to produce faster more powerful learning algorithms. These new algorithms can then be applied to solve broader classes of more complex problems, such as those that arise in the fields of communications, biotechnology, environmental forecasting, and manufacturing process control.  The research will involve making advances in the areas of statistical theory, algorithm design, and high performance computing."
"9704324","Statistical Problems in Quality Control, Stochastic         Systems and Genetic Analysis","DMS","STATISTICS","07/01/1997","03/24/1999","Tze Lai","CA","Stanford University","Continuing Grant","Joseph M. Rosenblatt","06/30/2000","$255,000.00","David Siegmund","lait@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","9148, 9183, 9218, BIOT, HPCC, MANU","$0.00","  Lai and Siegmund  9704324    The investigators develop  a sequential method of change-point detection involving algorithms that are  not too demanding in computational and memory requirements for on-line implementation and yet are  nearly optimal from a statistical viewpoint.  Powerful techniques to tackle this problem are derived from recent advances in sequential testing theory and boundary crossing problems in random fields.  A closely  related direction of research is fixed sample change-point problems and their applications to signal  detection and gene mapping.  Related fundamental problems in boundary crossing probabilities of random  processes and random fields are also investigated and lead to definitive solutions of some long-standing  questions concerning the distribution of generalized likelihood ratio statistics in change-point models.  A  third related area of research is estimation and control of time series models and stochastic dynamical  systems whose parameters may change with time.  Although in practice abrupt parameter changes  typically occur very infrequently, the unknown times of their occurrence have led to prohibitive  complexity of the Bayes estimators and controllers in the literature.  By using parallel recursive  algorithms and combining some new ideas in sequential change-point detection with empirical Bayes  methodology, these difficulties are resolved to obtain asymptotically efficient estimation and control  schemes of manageable complexity.      One important objective of this research is the development of a powerful statistical methodology for quality control of modern industrial processes and for automated fault detection in complex manufacturing  systems.  Another objective development of  statistical methods appropriate for analysis of genetic linkage  data related to disease susceptibility and other traits in humans, animals and plants and for extraction of   relevant information from these data that may ultimately lead to better diagnostic tests and tr eatments of  the disease.  Applications involve the strategic areas of manufacturing and biotechnology."
"9704648","Computationally Tractable Estimation Methods for Markov     Processes","DMS","STATISTICS","08/01/1997","08/28/2000","Katherine Ensor","TX","William Marsh Rice University","Continuing Grant","Joseph M. Rosenblatt","01/31/2001","$64,503.00","","ensor@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","0000, 9218, HPCC, OTHR","$0.00","NSF DMS-9704648  Katherine Bennett Ensor     Markov process theory provides a rich analytic and probablistic structure which is intrinsically natural from a modelling perspective.  Much of the literature on inference for continuous-time Markov processes assumes that the process has been observed continuously over some time interval.  However, in practice, many of the data sets available involve observing only discrete ``snapshots'' of the process.  Existing theory on parameter estimation in this setting often results in computationally prohibitive methodologies.  This research addresses the issue of computationally tractable parameter estimation for discretely observed continuous-time Markov processes.  Many of the results utilize Monte Carlo simulation to achieve tractability.  The second fundamental question examined in this research is that of error propagation through a stochastic system.  When the output performance measure of the stochastic system cannot be evaluated in closed form the output measure is simulated from the modelled system.  To address this situation, the investigators develop estimators and their sampling properties for functionals of the stochastic system.    Statistical models have always proven a powerful tool for purposes of modelling, understanding and predicting complex systems.  This research advances the frontier of statistical models for dependent observations.  For example, the statistical methods of this research are applicable to the diverse areas of modelling levels of pollutants and contaminants in air, soil and water which evolve over time and/or space; forecasting changes in the stock market; and predicting or assessing demand for the development of an optimal communications network.  The use of statistical models for purposes of modelling complex systems has been limited in the past due to the state of computing power; a state which has certainly improved in recent years.  In this research, the investigators develop a framework for practical implementation  of advanced statistical methodologies which capitalizes fully on the high performance computing available today. The research addresses the issue of modelling under partially observed information.  For example, the electrical or computer engineer may use such models to assess network status when only partial information is available on the state of the system; in predicting air quality for a given region, observations of pollutant levels are made at sites irregularly located over the region and often at irregular points in time.  The theoretical constructs necessary to implement the models in the more common scenario when only partial information is available are presented.  Additionally, this research involves error assessment of the predictions or output performance measure of an estimated complex system again capitalizing on the availability of high powered computing."
"9702172","Bayesian Analysis of Correlated Categorical Data Models     Using Scale Mixtures of Multivariate Normal Link Functions","DMS","STATISTICS","07/15/1997","06/20/1997","Ming-Hui Chen","MA","Worcester Polytechnic Institute","Standard Grant","William B. Smith","06/30/2001","$70,773.00","","mhchen@merlot.stat.uconn.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","MPS","1269","0000, 9148, 9218, HPCC, MANU, OTHR","$0.00","   Bayesian Analysis of Correlated Categorical Data Models    Using Scale Mixtures of Multivariate Normal Link Functions                             Ming-Hui Chen                Worcester Polytechnic Institute                              Abstract      General Bayesian multivariate generalized linear models are   considered to analyze categorical data when two or more binary   or ordinal responses are taken from the same individual or   subject at one time or over time.  In order to conduct a unified   analysis of correlated categorical data, this research uses a   very rich class of scale mixtures of multivariate normal (SMMVN)   link functions. The SMMVN-links include multivariate probit,   Student's t links, logit, symmetric stable link, and many others.  The main focus of this research is to develop a complete exact   small sample Bayesian analysis of correlated categorical responses,   which involves prior elicitation, model comparisons (in particular,   choices of link functions), model diagnostics, and variable   selection. Various efficient computational algorithms are developed   by using sampling-based methods (e.g., Markov chain Monte Carlo).   This research also includes extensions which cover dynamic   models, simultaneous autoregressive models, missing covariates,   and meta-analysis.     Correlated categorical data arise frequently in biometrics,   science, education, and the pharmaceutical and computer industries.   For example, the measurements on the sites of a wafer, from which   computer chips are made, are spatially correlated; cholesterol   level (low, medium, high) and blood sugar level (low, medium, high)   on the same individual are correlated; and in tumorigenicity  experiments, different kinds of tumors may be observed from the   same mouse or the same rat. Models and analysis incorporating   correlation among multiple categorical responses enable researchers   to obtain results with greater precision.  Engineers or doctors   are now using very rough methods to analyze  such data, but more   advanced methods are needed to deal with all aspects of the problem.   Incorporating prior information leads to improved interpretation of  the results of a current study and to reduction of the cost of a   new experiment.  This research will provide practitioners with   more appropriate methods to perform a comprehensive analysis of   multivariate categorical data."
"9705034","Advanced Methods for the Statistical Analysis of Functional Magnetic Resonance Imaging Data","DMS","STATISTICS, Methodology, Measuremt & Stats","08/15/1997","06/29/1999","William Eddy","PA","Carnegie-Mellon University","Continuing Grant","Joseph M. Rosenblatt","07/31/2000","$300,000.00","Nicole Lazar, Christopher Genovese","bill@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 1333","0000, 9183, 9218, BIOT, HPCC, OTHR","$0.00","  Eddy, Genovese, & Lazar  9705034    Functional Magnetic Resonance Imaging (fMRI) is a powerful new tool for understanding the brain.   With fMRI, it is possible to study the human brain in action and trace its processing in unprecedented  detail.  During an fMRI experiment, a subject performs a carefully planned sequence of cognitive tasks  while magnetic resonance images of the brain are acquired.  The tasks are designed to exercise specific  cognitive processes and the measured signal contains information about the nature and location of the  resulting neural activity.  Neuroscientists use these data to help identify the neural processes underlying  cognition and to build and test theoretical models of cognitive function.  This is inherently a problem of  statistical inference, yet the statistical methods for fMRI are still undeveloped.  In this project, the  statistical methodology for these large and complex data sets is advanced on three fronts: dealing with  model response variation, developing better registration and acquisition methods, and analyzing spatial  activation patterns.      Functional Magnetic Resonance Imaging (fMRI) is a new tool that is currently being used to study the  brain and the way it functions.  Very large amounts of data, with considerable noise, are collected on  neural activity while specific cognitive tasks are being performed.  In this way, cognitive scientists hope to  understand the processes underlying the way humans think.  Statistical inference is a natural way of  approaching this question.  However, the complex nature of the data means that standard methods are not  applicable and the methodologies used in fMRI for data analysis are still relatively undeveloped.  The  current project advances the statistical methodology for fMRI data by working in three directions.  Brain  response to a given task varies not only by location, but also in different replications of the same  experiment.  This source of variability is not taken into account by the models  now in use.  The first  direction of the project incorporates this source of variation, resulting in more precise inferences.  Subject  motion during fMRI scanning is the focus of the second direction, while the third direction involves  quantifying how spatial patterns of activation change over time.  This allows the comparison of different  individuals and groups."
"9704934","Bayesian Preposterior Simulation","DMS","STATISTICS","08/01/1997","03/22/1999","Peter Mueller","NC","Duke University","Continuing Grant","Joseph M. Rosenblatt","07/31/2001","$146,326.00","","pmueller@math.utexas.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, OTHR","$0.00","DMS-9704934    BAYESIAN PREPOSTERIOR SIMULATION    Peter Mueller  Duke Univiversity      We expand the ambit of methods and programs developed for posterior   simulation to expected utility maximization.  The main tool is   simulation in an augmented probability model. For expected utility   maximization the original probability model on parameters and data is   augmented to an artificial probability model on decision variables,   model parameters and data. The augmented probability model is chosen   such that the marginal distribution on the decision parameters is   proportional to expected utility, and thus the mode of this marginal   distribution corresponds to the optimal design.  We develop novel   Markov chain Monte Carlo techniques amenable to simulation from this   auxiliary probability model and investigate algorithms for mode   estimation in a possibly high dimensional distribution based on a   simulated Monte Carlo sample.    The Bayesian framework provides coherent support for decision making   through an iterative cycle of problem structuring, uncertainty   modeling, preference modeling, expected utility maximization and   sensitivity analysis.  Recent computational developments in statistics   (the Gibbs sampler, stochastic substitution sampling) have   significantly widened the range of models used for uncertainty   modeling.  We study the use of these methods in other parts of the   decision making cycle, in particular expected utility maximization and   preference modeling.  The investigated methods are simulation based   and highly computational intensive and require state of the art   computer simulation, relating to the Federal Strategic Area of high   performance computing.  The impact of the research is to widen the   class of problems where expected utility maximization, i.e., formal   solution of decision problems, is practically feasible in a way   similar to how Gibbs sampling and related techniques have made complex   probability models accessible for uncertainty model ing."
"9610157","Workshop on Stochastic Model Building and Variable Selection","DMS","STATISTICS","09/01/1997","05/28/1997","Giovanni Parmigiani","NC","Duke University","Standard Grant","James E. Gentle","08/31/1998","$9,000.00","","gp@jimmy.harvard.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, OTHR","$0.00","DMS 9610157    WORKSHOP ON STOCHASTIC MODEL BUILDING AND VARIABLE SELECTION   ABSTRACT     As statistical analysis and modeling of larger and more complex data   sets is becoming more common, new and better computer based tools for   guiding the initial specification of the main features of statistical   models are being developed.  Recently, a new generation of stochastic   algorithms has been emerging as an important augmentation to   traditional deterministic strategies.  Examples include stochastic   search methods for variable selection, graphical models, selection of   variable transformations and interactions, wavelet thresholding, ARMA   modeling, CART, MARS, neural networks and more.  A workshop on   stochastic model building and variable selection that blends   methodology and case studies will significantly advance this field of   statistical research and will encourage dissemination of an important   class of modeling techniques to practitioners.     The workshop is organized in coordination with the NBER/NSF Time   Series Seminar and with the Workshop on Wavelets in Statistics. The   common research themes between stochastic model building, wavelets   analysis and times series analysis will be exploited to create both a   synergy at the research level and savings in travel expenses and   arrangements.  Funds from this grant will be used for domestic travel   expenses for participants based in the U.S.A, particularly junior   researchers, and others from underrepresented groups."
"9703720","Expanding the Spectral Envelope","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","07/15/1997","02/01/1999","David Stoffer","PA","University of Pittsburgh","Continuing Grant","Joseph M. Rosenblatt","06/30/2001","$150,000.00","","stoffer@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269, 1271","0000, 9216, 9263, HPCC, OTHR","$0.00","Stoffer   DMS-9703720    The concept of the spectral envelope was recently introduced as a general statistical method for the frequency domain analysis and scaling of qualitative-valued time series.  In the process of developing the technology, many other interesting extensions became evident.  This research involves the extension of the fundamental concept in various directions.  The first extension   involves matching two categorical sequences in an effort to discover whether they contain similar patterns and is motivated by the problem of matching of two DNA sequences.  The approach builds on the ideas used in defining the spectral envelope and focuses on what could be called coherency envelopes for measuring the similarity between two categorical time series.  Estimation is based on the fast Fourier transform so that the methods are computationally simple and fast, and can be applied to long sequences.  In another direction, the technology is extended to the analysis of qualitative spatial data.  The applications that motivate this project are automated image retrieval and pattern recognition with potential use in computer vision.  Specifically, this part of the research will focus on the analysis of categorical random fields via optimal scaling and the wave number spectral envelope.  Since many of the applications where the spectral envelope methodology has been an asset are situations where the assumptions of stationarity and homogeneity are not realistic, this research will explore the benefits of wavelet-based analysis over dynamic Fourier analysis.  Another extension of this research is to apply the concept to the analysis of real-valued time series collected in an experimental design where the primary interest is whether any, and how many, have common cyclic components.  This problem is motivated by numerous applications in the medical and behavioral sciences.    A statistical concept called the spectral envelope was recently introduced as a general method to study patterns in long seq uences of letters or symbols (such as codes).  The most well known application of this methodology is in biotechnology, specifically in the analysis of DNA.  In the process of developing the concept, many other practical extensions became evident.  This research involves the extension of the fundamental concept in various directions.  The first extension involves matching two non-numeric sequences in an effort to discover whether they contain similar patterns and is motivated by the biotechnical problem of finding similarities in two otherwise different genetic codes.  Another direction is to extend the technology to qualitative spatial data (such as images) with applications in automated image retrieval, automated pattern recognition, and computer vision.  This research will have an impact on robotics and automation technologies that will be useful, for example, in industrial manufacturing processes.  Moreover, the methodology used in matching sequences could have applications in computer vision and robotics where the problem is to automatically and quickly align two pictures of the same scene taken from different camera angles.  In addition, this research will focus on ways to incorporate this new technology into current medical applications such as understanding changes in biorythms in stressful situations."
"9704436","A New Class of Model Selection Criteria Based on Kullback's Symmetric Divergence","DMS","STATISTICS","07/01/1997","05/14/1997","Joseph Cavanaugh","MO","University of Missouri-Columbia","Standard Grant","Gabor Szekely","12/31/2000","$70,120.00","","cavanaugh@stat.missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269","0000, OTHR","$0.00","                  A New Class of Model Selection Criteria                   Based on Kullback's Symmetric Divergence                              Joseph E. Cavanaugh                     University of Missouri - Columbia    The selection of a statistical model from a collection of candidates can   often be facilitated by the use of a model selection criterion, which   evaluates a fitted model by assessing whether it offers an optimal balance   between ""goodness of fit"" and parsimony.  This research considers the   development of model selection criteria based on Kullback's symmetric   divergence measure.  The symmetric divergence is related to Kullback's   directed divergence, better known as the Kullback-Leibler information,   which serves as the basis for the well-known Akaike information criterion   and its subsequent variants.  In the context of model selection, the   symmetric and directed divergence can both be utilized to measure the   discrepancy between the model which presumably generated the data and a   fitted approximating model.  It can be argued, however, that the symmetric   divergence is more sensitive to deviations between these two models and   therefore functions as a better discriminant.  Consequently, an estimator   of the symmetric divergence may serve as a more effective model selection   criterion than an estimator of the directed divergence, provided that   the former estimator is accurate enough to sufficiently reflect the   sensitivity of the targeted measure.  The preceding notion serves as the   impetus for this research, which involves the development and investigation   of a new class of model selection criteria based on estimation of the   symmetric divergence.    Scientists who model phenomena are often faced with the dilemma of how to   choose an appropriate model to characterize an underlying set of data.    A model selection criterion is a measure which assigns a ""score"" to each   model in a candidate collection, an index which reflects how well the   associat ed model satisfies a certain optimality principle.  These scores   allow an analyst to simply and objectively choose a final model based on   an evaluation of a potentially expansive class of candidates.  Thus, model   selection criteria provide an ideal means for the computer to occupy a   central role as a decision maker in statistical investigations.    The importance of this notion is discussed by Cheeseman and Olford   (""Selecting Models from Data: Artificial Intelligence and Statistics IV,""   Springer-Verlag Lecture Notes in Statistics, 89, page v):     ""...Computers will increasingly be required to draw robust      inferences from data, sometimes very large quantities of data...      And, because the scale of the problems arising from large computer      databases quickly overwhelms the human analyst, it is desirable to      have a computer assume as much of the role of analyst as possible.""  In the future evolution of statistical methodologies and practices, model   selection criteria will play an increasingly vital function.  Thus, it is   essential that continuing work is conducted in this area, pertaining not   only to the evaluation and improvement of existing criteria, but also to   the introduction and investigation of new criteria based on appealing   statistical principles.  This research focuses on the latter.  The results   of this investigation will have potential impact on many scientific areas,   including engineering (e.g., image and signal processing), economics (e.g.,   econometric modeling), and computer science (e.g., artificial   intelligence).     ------------------------------------------------------------------------------"
"9711098","51st Session of the International Statistical Institute,    Istanbul, Turkey, August 18-27, 1997","DMS","STATISTICS","05/15/1997","04/25/1997","Ray Waller","VA","American Statistical Association","Standard Grant","James E. Gentle","04/30/1998","$20,000.00","","rwalter@calpoly.edu","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","0000, OTHR","$0.00","DMS9711098 51st Session of ISI, Istanbul  Abstract    The American Statistical Association (ASA), an affiliate of the   International Statistical Institute (ISI), requests funds for   approximately twenty (20) travel grants for the meetings of the 51st   Session of the ISI to be held in Istanbul, Turkey, from August 18-27,   1997.  The ISI holds its biennial meetings in capital cities around the  world, in odd-numbered years.  Of the approximately 500 United States   residents who are members of the ISI, about 60 percent attend the   biennial meeting.  The ISI meeting includes meetings of the Bernoulli   Society for Mathematical Statistics and Probability; the International   Association for Survey Statisticians (IASS); the International   Association for Statistical Computing (IASC); the International   Association for Official Statistics (IAOS); and the International   Association for Statistical Education (IASE) Thus, this will be an   umbrella meeting with sessions of interest for thousands of statisticians.  Further, the meetings bring together individuals from various areas of   statistical research.  Papers cover theoretical statistics and many   areas of application including engineering and the behavioral, social,   physical, and health sciences.  Copies of individual papers are made   available during the meeting and proceedings of the biennial sessions   are published.  Examples of some of the presentations by recipients of   past NSF travel grants include ""The Role of Electronic Communication in  Statistics Education,"" ""Object Oriented Methods in Statistics--  Graphical Construction and Editing of Plots,"" and ""Time Series   Applications in Astronomy and Meteorology.""    The travel grants will provide partial support to defray transportation  costs for a limited number of individuals, who will be selected by a   committee of ASA members.  Of primary importance to the ASA is the   emphasis placed on encouraging younger statisticians to participate in   the meeting.  ASA proposes to awa rd approximately twenty grants, with   the majority of the funds supporting individuals who received their   Ph.D. in 1988 or later.  The ASA encourages applications from all   qualified individuals, especially women and minorities.  ASA affirms   the policies of the NSF in this regard."
"9705166","Significance Testing of Pattern Correspondence Statistics:  Theoretical and Empirical Analysis","DMS","STATISTICS","08/01/1997","10/27/1999","Richard Smith","NC","University of North Carolina at Chapel Hill","Continuing Grant","William B. Smith","07/31/2000","$63,000.00","","rls@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","1303, EGCH","$0.00","SMITH  DMS 9705166    General circulation models (GCMs) are numerical models used by climatologists to predict patterns of future climatological change as a result of greenhouse gases and other factors.  In recent years, much attention has been given to the verification of GCMs from observational data, through pattern correlation statistics which measure how well the observed and model-based data agree.  The current research is a collaboration between a statistician and a climatologist, developing new tests for the significance of pattern correlation statistics and for estimating tuning parameters such as the weight to be given to different components of the climate signal.  The methodology includes developing new spatial-temporal models for the distribution of climate data, combining nonstationarity in space with long-range-dependence in time.    The broader context of the work is the debate over global warming and in particular whether observed changes in climate can be ascribed to man-made causes such as the greenhouse gas effect or merely reflect climate variability due to natural causes.  New statistical tests, currently being developed by the Principal Investigator, allow more rigorous discrimination between the effects of greenhouse gases and those of other causes such as sulfate aerosols and changes in solar flux.  The project is contributing to the strategic area of environment and global change, in the form of new methods of assessing the causes of global climate change."
"9711258","Symposium on Case Studies in Bayesian Statistics;           September 26-27, 1997; Pittsburgh, PA","DMS","STATISTICS","07/01/1997","06/11/1997","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","James E. Gentle","06/30/1998","$15,000.00","","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, OTHR","$0.00","          Symposium on Case Studies in Bayesian Statistics                         Robert E. Kass                   Carnegie Mellon University         A symposium entitled ``Case Studies in Bayesian Statistics 4''  will be held at Carnegie-Mellon University in Pittsburgh, Pennsylvania,   on Friday September 26 and Saturday September 27, 1997.  The symposium   will include four extended presentations of applications of Bayesian   methods to a range of medical and industrial applications. More   specifically, topics of the selected presentations inalude: modeling   for risk of breast cancer, pharmacokinetic modeling in drug   development, problems of customer value analysis, and statistical   aspects of functional MRI. In these applications the statistician  was an integral member of the research team. A contributed poster   session will also be held.  The objectives of the symposium are to:   (i) identify and focus attention on specific implementation and        theoretical problems that hinder applications of Bayesian         methods, and to identify candidate solutions;  (ii) provide a forum in which the interplay between statistical        theory and practice will be explored in the context of concrete         research projects;   (iii) provide a small-meeting atmosphere within which junior investigators        and graduate students can explore substantial Bayesian applications         with experienced researchers;  and  (iv) produce a  volume containing well-documented case studies and data        sets suitable for use by researchers, practitioners, educators         and students of applied statistics and other quantitative fields.        As increasingly much background information becomes available to   scientists undertaking an investigation, it is important to utilize   previous knowledge effectively in designing studies and analyzing   data. Bayesian statistical methods are tailored to this purpose.   There have been many recent advances in Bayesian statistical theory   and computation, b ut scientific meetings rarely spend substantial   time discussing applications. The purpose of this symposium is to   concentrate attention solely on applications of Bayesian statistics.   The goal is to elucidate the interplay between theory and practice   and thereby identify successful methods and indicate important   directions for future research."
"9705032","Latent Variable Models in Action:  Hierarchical Bayes and   Mixture Models for Repeated Discrete Measures with          Individual Differences","DMS","STATISTICS, Methodology, Measuremt & Stats","07/15/1997","07/06/1999","Brian Junker","PA","Carnegie-Mellon University","Continuing Grant","Joseph M. Rosenblatt","06/30/2001","$144,000.00","","brian@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 1333","0000, OTHR","$0.00","NSF DMS-9705032    Latent variable models in action: hierarchical Bayes and mixture   models for repeated discrete measures with individual differences    Brian Junker    Carnegie Mellon University        PROJECT ABSTRACT:    A central feature of this research is the development of widely   applicable methodology for latent variable models for measurement   problems in education, psychology and the social sciences.  This   methodology is being developed and tested in several specific areas:   Monotonicity and stochastic ordering properties that follow from the   strictly unidimensional latent variable representation are being   studied and applied to nonparametric scaling problems.  A promising   Markov chain Monte Carlo method is being extended and applied to a   variety of problems, including: correct modeling of rater variability   in educational achievement data; accomodating heterogeneous   catchability in multiple-recapture censuses; and developing methods   for multidimensional and hierarchical latent variable models for   discrete repeated measures.  In addition, the research addresses the   sensitivity of inferences to underspecification of the model.  A   second thrust of the research is to refine and develop existing   characterizations of unidimensional latent structure into a   statistical theory of, and statistical methods for assessing, latent   variable dimensionality.    This work aims to more fully blend psychometric and statistical   approaches to latent variable models for repeated discrete measures.   Psychometric methodology tends to concentrate on model building and   model features; and psychometric data analysis tends toward issues of   scaling (selecting questions that ``hang together'' in the sense that   a unidimensional latent variable model holds), reliability (ensuring   that the latent variable can be estimated well from the questions   selected), and the assessment of latent variable dimensionality from   data.  Statistical methodology tends to sidestep these bas ic   psychometric questions, and instead concentrates on finer model   adjustments, and various inferential and predictive tasks.  The focus   of this research is on statistical and psychometric features of latent   variable models for repeated measures data, which is of interest to   quantitative psychologists, educational measurement specialists, and   cognitive scientists, as well as other social scientists.  Much of the   work is collaborative in nature, and it is built around the   development of theory and methodology motivated from, and useful for,   substantive applications.    --------------------------------------------------------------------------"
"9704474","Modeling and Statistical Analysis of Mental Test Data","DMS","STATISTICS, Methodology, Measuremt & Stats","07/01/1997","05/16/2001","William Stout","IL","University of Illinois at Urbana-Champaign","Continuing Grant","John Stufken","06/30/2002","$306,846.00","","stout@stat.uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269, 1333","0000, OTHR","$0.00","    Modeling and Statistical Analysis of Mental Test Data              William Stout           University of Illinois              Abstract    This research involves the probabilistic modeling and statistical  analysis of multiple question(item) mental test data.  The   investigator has in the past used the nonparametric item response   theory(IRT) paradigm to study test fairness, the statistical   assessment of complex latent IRT structures, and the cognitive   process/psychometric modeling and cognitive diagnosis of test  taking examinees using the investigator co-developed Unified   Cognitive Model.  Now, it is proposed to develop a nonparametric   item level geometric description of the item structure analogous   to the classical subtest score level factor analytic approach.    It is planned to develop a practical operational cognitive  testing procedures based on the Unified model.  It is proposed   to further develop the test bias procedure, SIBTEST, in various   much needed ways.  A practical need cutting across all three   areas is forour nonparametric procedures to be modified to match   on liklihood based latent ability estimates as well as on their   currently used number correct scores.    The research involves developing mathematical models and statistical  procedures to improve standardized and classroom tests and to improve  the measurement of examinee performance on such tests.  There is a   special emphasis on providing an educationally useful concept mastery  profile for each examinee based upon his/her individual test question   responses, on providing fairer tests, and on assessing the   substantive complexity of skills required for performing well on a   test.  In particular, it is desired to     (i) provide for each item a description of what it measures best,   (ii) to produce fairer tests -- in particular by informing the         construction specifications of future tests about biased         item types that should be excluded, and   (iii) to provide for educators detail ed feedback of concept mastery         and nonmastery for each individual examinee to be used to         guide further instruction and remedial efforts.    Because standardized tests are ever more ubiquitous in American   society and noting their gatekeeper role, the above work, if   successful, should have a major impact on how effective standardized  tests in America are and indeed could improve the educational   process in the classroom."
"9705347","Generalized Linear Models","DMS","STATISTICS","07/15/1997","07/02/1999","Peter McCullagh","IL","University of Chicago","Continuing Grant","Joseph M. Rosenblatt","06/30/2000","$158,499.00","","pmcc@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00","McCullagh  9705347    This research will examine a number of issues, all bearing directly or indirectly on statistical models of the generalized linear type.  A major part of the work is concerned with multivariate models, either graphical dependence models,  or marginal models constructed for epidemiological or similar purposes.  The extent to which such models are capable of a causal interpretation will be examined.  Apart from specialized models such as those arising in the analysis of ranked data, a most pressing need has been for satisfactory methods for dealing with non-linear models having several components of variation.  Residual likelihood is one technique used in linear models for the estimation of variance components, by-passing the regression parameters.  The intention is to develop a similar strategy for generalized linear models in order to help focus attention on subsets of the parameters without compromising the inferences.  The final component of the research is related to the algebra of model formulae, and in particular, on the limitations of the algebra in common use, particularly where homologous factors are involved.  Only a minority of group-invariant subspaces correspond to interesting statistical models:  A promising alternative to group-invariance is monoid-invariance, which corresponds closely to factorial models.  The aim is to find a succinct way of specifying suitable invariant subspaces in a way that is unambiguous and can be understood by statistician and computer alike.  This exercise will involve a mixture of algebra and computational work.    Generalized linear models have been used in a wide variety of applications in the social, physical and biological sciences, in addition to commercial applications such as insurance and marketing.  Despite this success, there are a number of important areas in which further development would be beneficial.  Foremost among these are applications in which random effects accrue from several identifiable sources.  Examples incl ude longitudinal studies, genetic models for plant and animal breeding, and agricultural field experiments.  Methods will be developed to deal with patterns of dependence induced by such random effects.  A second area on which some progress has already been made is the connection between statistical model formulas and what are known in algebra as monoid-invariant subspaces.  The currently-used algebra for statistical models is incapable of recognizing that two factors have the same set of levels.  An extended algebra will be developed to accommodate this phenomenon."
"9703758","Regression Quantiles Computation and Applications","DMS","STATISTICS, CENTRAL & EASTERN EUROPE PROGR","07/01/1997","07/02/1999","Stephen Portnoy","IL","University of Illinois at Urbana-Champaign","Continuing Grant","John Stufken","06/30/2002","$172,402.00","","portnoy@stat.uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269, 5979","0000, 5994, 9197, 9218, EGCH, HPCC, OTHR","$0.00","  Portnoy  9703758:    Traditional Statistical Linear Modeling seeks to explain a response variable (e.g., wages) in terms of  predictor variables (e.g., education, job training, social characteristics, etc.).  The classical approach uses  ""lease squares"" to estimate the mean of the responses conditional on the predictors.  It is often found,  however, that those with higher responses depend very differently on the predictors than those with  middle or lower responses.  This variability is completely lost by the classical approach.  Thus, modern  regression quantile methods have become increasingly popular.  These methods seek to estimate the conditional quantiles (percentiles) of the response in terms of the predictors.  For example, it has been  found that high wages depend much more strongly on education than lower wages.  That is, not only are  high wages associated with more education, but the rate of return on education is significantly higher for  high wage earners than for lower wage earners.  Similarly, high electricity consumers during the summer  show a much greater difference between daytime peak use and nighttime use than lower users (presumably  because of air conditioners), and the length of long hospital stays depends more strongly on the severity of  the disease than does the length of shorter stays.  This ability to distinguish models on the basis of the size  of the response is finding extensive application in economics, social sciences, biostatistics, and other  areas.    Inevitably, successful continued diffusion of these methods is linked closely to the availability of  convenient and efficient software.  For modestly large problems, existing algorithms require  computational effort comparable to least squares.  However, as problem size grows, the computational  burden of the previous methods becomes heavy.  For large problems common in empirical labor  economics, large biomedical surveys, and other areas, new and more efficient computational techniques  would be highly des irable.  Thus, recent research proposes a two-pronged attack that has been shown to  yield dramatic improvements in computational efficiency.  One prong is the use of recently developed  interior point methods for linear programming.  The second is a form of stochastic preprocessing which  can drastically reduce the effective size of most statistical regression quantile problems.  Together, these   ideas appear capable of bringing the computational effort down to the level of least squares for problems  with sample sizes up to several million observations.  In even larger problems, theoretical evidence  indicates that regression quantile methods are even faster than least squares.  Practical realization of this  theoretical improvement would have significant consequences in the area of high performance  computation for large and massive data sets."
"9711624","Advanced Integrated Science Modeling Capability for         Integrated Assessment Studies","DMS","STATISTICS","11/15/1997","10/30/1997","Donald Wuebbles","IL","University of Illinois at Urbana-Champaign","Standard Grant","K Crank","04/30/2001","$280,000.00","Atul Jain","wuebbles@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","1317, EGCH","$0.00","9711624  Wuebbles    The objective of this research is to develop advanced modeling capabilities for the   natural science modules to better meet the needs of future integrated assessment studies.   The model will be designed to better represent spatial variations and to improve the   treatment of processes relevant towards evaluating the biogeochemical cycles important to   determining atmospheric composition and resulting climatic effects at the regional scale.   By extending the process level understanding of regional impacts into the Integrated   Assessment (IA) model, the work proposed here will substantially improve the   understanding of climate change impacts and extend the range of issues, which can be   addressed in an IA framework. For specific applications, this effort includes the   appropriate connections with a state-of-the-art energy/economics model. The model will   be applied to a selected set of studies to address policy related questions on climate   change; in particular the role of agriculture, forestry, and land-use for future greenhouse   gas emissions and their implications on future climate.   Although the primary focus of this model development will continue to be on the   natural sciences, a much more powerful tool for fully integrated assessment will be   achieved through a close interaction with the ongoing state-of-the-art modeling of   economic, ecosystem, and social science processes.  It will be an important aspect of this   project that it will have a well designed modeling tool that can be used in a variety of ways   for assessing the relationships between global change and the social sciences. In particular,   this modeling tool will allow the exploration of human dimension of global change   decision making and policy science in an integrated global and regional framework, while   providing fast and efficient first approximation answers to pressing policy questions   related to global change."
"9704349","Computational Methods for Mixed Effects Models","DMS","STATISTICS","07/15/1997","06/27/1997","Douglas Bates","WI","University of Wisconsin-Madison","Standard Grant","Joseph M. Rosenblatt","06/30/2001","$70,402.00","","bates@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","0000, OTHR","$0.00","       Computational Methods for Mixed-Effects Models        Douglas M. Bates, U. of Wisconsin - Madison              Abstract    This research extends current computational methods and software  implementations for the estimation of parameters in mixed-effects  models.  There is a special emphasis on extensions to models where  there are several levels of random effects nested within each other.  Another emphasis is on models where the within-subject response  depends nonlinearly on the parameters of interest.  The primary  implementation is in the S computer language and complements current  graphical and analytical methods available in that language.    When a response is measured on several occasions for each of several  ""subjects"" or experimental units, the resulting data are called  repeated-measures data.  If the data on each experimental unit are  collected over time, they are also known as longitudinal data.  A  statistical model for such data will usually incorporate both fixed  effects, parameters that describe the typical behaviour in the  population from which the experimental units are selected, and random  effects, quantaties that describe the variation within the population.  Models with both fixed effects and random effects are called  mixed-effects models.  This research provides new computational  methods for fitting mixed-effects models to observed data.  The new  methods are suitable for use with parallel computers and with other  high performance computing environments.  This is important because  some of the applications of mixed-effects models in manufacturing  (evaluation of integrated circuit designs) and biotechnology  (modelling of genetic effects in animal breeding) involve fitting  complicated models to thousands and sometime millions of observations;  a task that pushes the limits of current computers."
"9704809","Information Theory and Statistical Model Selection","DMS","STATISTICS","09/15/1997","09/12/1997","Dean Foster","PA","University of Pennsylvania","Standard Grant","Dean Evasius","08/31/1999","$104,000.00","Robert Stine","dean@foster.net","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, OTHR","$0.00","9704809  Foster and Stine    The research studies the use of information theory for selecting   statistical models.  The first component of the research characterizes  statistical model selection criteria as methods for data compression.  From this viewpoint, criteria for model selection choose the model   which minimizes the length of a compressed version of the observed   data.  The proposed research will allow one to make these   characterizations precise and to extend this paradigm.  The second   area of research exploits the relationship between model selection   and data compression, leading to methods of adaptive model  selection.  This research builds on and extends context trees which   are the underlying statistical devices used to obtain some of the   most effective data compression algorithms.  The third body of   research concerns the statistical properties of the  information theoretic estimators implied by these techniques. In   order to allow one to estimate such statistical properties from   observed data, the research also seeks to develop practical   bootstrap resampling methods to determine the accuracy of the   statistical estimates.  Each phase of the proposed research   combines statistical theory and  information theory with computing.  In addition to the use of  simulations to validate the proposed   methods, the research will provide publically available software   distributed via the Web that implements the various selection   criteria and associated modeling techniques.  Further numerical   simulations will benchmark the various large sample results,   measuring the practical performance of these procedures.    Given the increasing prevalence of large data sets in science and  economics, traditional statistical model building faces new   challenges.  The use of traditional variable selection methods   lead to overfitting, characterized by overly complex models   that result from chance variation.  This research attacks this   problem in several ways, first by using in formation theory to   construct a common framework in which to study the latest   developments in statistical model identification.  Building on   this framework, the research expands the scope of model selection   to encompass models that can adapt to structural changes.  In   addition, this work develops the associated statistical properties   of these methods, allowing comparison to classical procedures and   practical use in real-world applications."
"9704758","Statistical Model Building with Generalized Splines","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","07/15/1997","06/24/1999","Grace Wahba","WI","University of Wisconsin-Madison","Continuing Grant","Joseph M. Rosenblatt","06/30/2001","$218,420.00","","wahba@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269, 1271","0000, 9197, 9263, EGCH, OTHR","$0.00","Wahba  9704758    This research involves the development and testing of improved methods for estimating functions of many variables, given scattered, noisy, heterogeneous direct and indirect observations and prior information of various kinds.  Theoretical properties of the methods under development are established, and properties of the methods are also studied in test beds with simulated data where the `truth' is known.  Efficient numerical algorithms are developed which simultaneously implement statistical and computational objectives.  Finally the methods are tested on real observational data sets.  A unifying theme in all of the research is the exploitation of reproducing kernel Hilbert space methods, combined with cross validation, unbiased risk and likelihood techniques.    The statistical and numerical methods being developed have applicability in several broad areas:  (1) to the analysis of complicated medical and demographic data sets for the purpose of risk factor estimation (2) to the analysis of very large environmental data sets with the goal of extracting and presenting the maximal amount of information available in the data set; particular emphasis is on application to climate change (3) to the improvement and better understanding of methods used in supervised machine learning, and (4) to the analysis of data that is obtained from dynamical systems such as the evolving atmosphere or ocean, and the analysis of the strengths and weaknesses of mathematical models (such as numerical weather prediction or ocean models) that are used in the analysis of such data."
"9705158","Multiple Imputation: Research for the Third Decade","DMS","STATISTICS, Methodology, Measuremt & Stats","08/01/1997","06/24/1999","Donald Rubin","MA","Harvard University","Continuing Grant","Joseph M. Rosenblatt","07/31/2000","$240,492.00","John Barnard","donald.rubin@temple.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269, 1333","0000, OTHR","$0.00","  Multiple Imputation:  Research for the Third Decade    DMS-9705158    Donald B. Rubin and John Barnard   Harvard University     The third decade of multiple imputation begins with ever-growing   applications, even into areas not originally proposed  (e.g., environmental studies, chemistry), an increasing   availability of software, and an increasing amount of statistical   research work being conducted.  The expanded, almost routine, use   of multiple imputation means that many technical issues,   considered relatively minor in its early development, need   attention.  This research addresses several of the most important   issues, in particular:  (1) creating multiple imputations under   more general and flexible models; (2) creating ""nested"" multiple   imputations in data sets with large and highly variable fractions   of missing information; (3) using cross-match techniques to   analyze data sets having only a few multiple imputations but large   fractions of missing information, especially for obtaining valid   p-values; (4) analyzing data sets with nested multiple   imputations; and (5) conducting exploratory and diagnostic   analyses on multiply-imputed data sets.      Missing values are prevalent in many data sets and can be a  great hindrance in making inference.  Multiple imputation has   proven to be a useful mode of inference in the presence of missing   data.  The basic aim of multiple imputation is to allow users of   incomplete data sets, who typically have little information about   the missing-data mechanism, to reach valid statistical inferences   using only (1) standard complete-data analysis techniques and (2)   simple rules for combining the output of the complete-data   analyses.  Multiple imputation has been successfully used in many   contexts including in the social and economic sciences, the   history of science, and in biomedical applications.  This effort   extends the applicability and the ease of use of multiple   imputation, which enters its third decade where it  should become   an important tool in everyday statistical practice."
"9705156","Inference and Computation in Multi-Level Models","DMS","STATISTICS","07/15/1997","06/21/1999","Carl Morris","MA","Harvard University","Continuing Grant","Joseph M. Rosenblatt","06/30/2000","$241,332.00","David van Dyk","morris@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","0000, 1038, 1057, CVIS, OTHR","$0.00","DMS 9705156         INFERENCE AND COMPUTATION IN MULTI-LEVEL MODELS      Carl Morris     and     David van Dyk      Harvard University            Summary    Hierarchical and multi-level methods in statistics (Bayesian and   otherwise) have become ever more prevalent in modern statistical   practice, as better computational tools and computational advances make   these powerful methods more tractable.  This research will extend the   theory and computation for hierarchical models in general, and   especially for versions of them that seem promising for practical   applications, as suggested partly from the investigators' experience   with them for medical quality assurance, meta-analysis, image   processing, traffic engineering, and for other situations.  The   investigators will study and develop procedures for fast fitting of   exponential family models, of generalized linear models, and of   multivariate models.  They will work on making inferences and on   foundations.  They will study and evaluate diagnostic methods needed to   assess the modeling assumptions made for the (second level)   distributions that govern the individual parameters.  Operating   characteristics and risk functions are to be calculated.  Mindful of   the computational burden of these methods, the investigators will seek   good mathematical approximations and the efficient implementation of   these more precise, computationally intensive methods.    While multi-level models are still new, they are the focus of much   research because their applications and potential applications abound in   scientific, engineering, commercial, medical, and governmental contexts.    Applications of hierarchical models have been and are being made to   image processing, agriculture, genetics, health services, and to many   other areas.  For example, the Principal Investigator's past research   has included using and adapting multi-level statistical models to   extract more accurate information from satellite imagery data by having   the t wo levels represent separately the measurement error distributions   and the uncertainty about the ground truth.  The investigator also has   used these models to assess more accurately, from non-experimental   traffic data, whether specified highway safety improvements actually   reduce accidents.  Two-level models are needed because direct   treatment-control comparisons made without them tend to be biased when   safety improvements have been made mainly at high hazard locations.    Concern for these and other applications of multi-level models motivate   this research, which is aimed at advancing their proper use and at   optimizing their computational efficiency."
"9796229","Mathematical Sciences:  Leveraged Bootstrap","DMS","STATISTICS","06/01/1997","07/14/1997","Jian-Jian Ren","LA","Tulane University","Standard Grant","Joseph M. Rosenblatt","05/31/2000","$47,800.00","","jjren@umd.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","MPS","1269","0000, OTHR","$0.00",""
"9704570","Saddlepoint Approximations for Survival Analysis","DMS","STATISTICS","07/15/1997","06/20/1997","Snehalata Huzurbazar","WY","University of Wyoming","Standard Grant","Joseph M. Rosenblatt","06/30/2001","$70,000.00","","snehalata.huzurbazar@mail.wvu.edu","1000 E. University Avenue","Laramie","WY","820712000","3077665320","MPS","1269","0000, 9148, MANU, OTHR","$0.00","Re: NSF DMS-9704570  Saddlepoint Approximations for Survival Analysis   Snehalata V. Huzurbazar  University of Wyoming      The primary goal of this research is to develop saddlepoint   approximations to survival and hazard functions used for   bio-medical data or alternatively, approximations to reliability  functions used for engineering data. Modelling of survival data  results in complicated waiting times whose exact distributions   are intractable but they can be approximated using saddlepoint  methods. The latter are based on `higher-order' asymptotic  methods and yield extremely accurate approximations for density  and distribution functions which can then be combined to give  approximate survival and hazard functions.  The methodology is  extremely computer-intensive due to the nature of the calculations.    Survival analysis is primarily concerned with the modelling of  waiting time distributions arising from various scenarios.  The  investigator studies models for linear as well as acyclic  progression of diseases, models for competing risks, models with  feedback for cyclic behaviour as well as frailty models for  populations which are heterogeneous. Due to the complexity of the   distributions that describe waiting time behaviour in these  situations, conventional methods are restricted to models which  mainly use the exponential distribution. Use of saddlepoint  methods removes this restriction, yielding much more flexibility  in modelling.      The above models are tremendously useful in studying the  progression of various diseases.  For example, progression of  the HIV virus is mainly from one stage to the next; but in some  cases, patients can skip stages and also get better before  getting worse.   Progression of diseases is also studied using  frailty models, that is, situations in which the whole population  does not have the same risk for a disease. For example, certain  subgroups are more susceptible to diabetes than other subgroups,  or subpopulations may have different ri sks due to exposure to  different environmental factors.  Again, modelling the  progression of a disease in the different subgroups can lead to  fairly complicated models which tax currently used methodology  but can be handled using saddlepoint methods.  Finally, the  methods obtained from this research are easily transferred  into reliability models used in engineering and manufacturing."
"9615340","Mathematical Sciences:  Third North American Conference     of New Researchers in Statistics and Probability;           July 23-26, 1997; Laramie, Wyoming","DMS","STATISTICS","05/01/1997","08/19/1996","Snehalata Huzurbazar","WY","University of Wyoming","Standard Grant","James E. Gentle","04/30/1998","$14,000.00","Aparna Huzurbazar","snehalata.huzurbazar@mail.wvu.edu","1000 E. University Avenue","Laramie","WY","820712000","3077665320","MPS","1269","0000, OTHR","$0.00","DMS9615340  Huzubazar    This award provides partial support for the third New  Researchers' Conference in Statistics and Probability.  Participants at this conference are recent Ph.D. recipients and  doctoral students who are within a year of completion of their  degrees.  This is the only conference in North America  exclusively for new researchers in Statistics and Probability.  The first two conferences, held in August 1993 and July 1995,  were very successful and had close to fifty participants each.   The conference, planned by the New Researchers' Committee of the   Institute of Mathematical Statistics (IMS), will also be listed  as one of the special pre-meeting conferences at the 1997 annual  meeting of the IMS.    The objective of the conference is to bring together, in the  early years of their careers, researchers from diverse areas of   Statistics and Probability.  Research areas include  computationally intensive statistical methods, Bayesian and   non-Bayesian inference, applied probability and stochastic  process, as well as areas with an emphasis on application of  statistical methods such as biostatistics, environmental   statistics, and engineering.  The conference is limited to  50  participants, and lasts for 3 days during which each participant  is required to present her/his research in the form of a short  talk or in a poster. Such active participation is important for  researchers in beginning stages of their careers because it  facilitates an exchange of research ideas.  The program also  includes three overview talks by senior researchers on  cutting-edge research  in Statistics and Probability as well as a  panel discussion on how to initiate a research program.  The  conference will be held before the annual meetings of the  Institute of Mathematical Sciences so that participants can then  attend those meetings."
"9707364","International Conference on Bayesian Modeling:  ISBA-1997;  Istanbul, Turkey, August 16-18, 1997","DMS","STATISTICS","06/01/1997","05/28/1997","Hamparsum Bozdogan","TN","University of Tennessee Knoxville","Standard Grant","James E. Gentle","05/31/1998","$9,900.00","","BOZDOGAN@UTK.EDU","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","MPS","1269","0000, OTHR","$0.00","INTERNATIONAL CONFERENCE ON BAYESIAN MODELINE: ISBA-1997    The funds from this group travel grant are to enable,   coordinate, and support U. S. participation in the   Fifth World Meeting of the International Bayesian Analysis   (ISBA) which will be held in Istanbul, Turkey during August   16-18, 1997. This meeting will be a satellite meeting for the   51st Session of the International Statistical Institute (ISI) in   Istanbul, Turkey during August 18-27, 1997. This meeting is   planned around the theme of linking Bayesian statistics to other   fields of science to stimulate cross-fertilization of theoretical,   methodological and computational research directions to meet   our societal needs by focusing attention to the arena of applied   problems in cross-disciplinary areas such as in computer science,   in engineering, in econometrics, finance, forensic and medical   sciences.     A primary goal of these funds is to present opportunities   for young researchers, international researchers, and   underrepresented groups, such as women and ethnic minorities and   persons with disabilities and graduate students an opportunity to   interact, to share, and to disseminate their research and   expertise with other researchers in many cross-disciplinary fields   in the international arena. Experts from several Western and Eastern   European countries, Australia, Canada, Japan, the United Kingdom,   Middle East, Russia, Independent States of the former Soviet Union,   India, Africa, and the United States are expected to attend."
"9704739","Statistical Modeling of Multidimensional Movements of Free  Ranging Animals","DMS","STATISTICS","07/15/1997","04/26/1999","David Brillinger","CA","University of California-Berkeley","Continuing grant","Joseph M. Rosenblatt","06/30/2001","$241,000.00","","brill@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","9169, EGCH","$0.00","  Brillinger, Kie, Preisler and Stewart  9704739    This interdisciplinary research is aimed toward developing multidimensional models of motion based on  recorded trajectories of marine mammals and terrestrial animals among other moving entities. One central  objective is the development of flexible models that can  accommodate the movements of a variety of  faunal  taxa,  their intra- and inter-specific interactions, and their interactions with the environment.   Characterizing the trajectories of animals traveling great distances through the oceans for example   requires models for diffusion processes on the sphere.  Stochastic differential equations will have to be  adapted to this situation and in particular to include covariates,  nonparametric coefficients and non- Gaussian processes.  Basic inferential aspects of diffusion processes, many of which are as yet unstudied  particularly with respect to animal movement, will also be explored in this study.    Two primary data sets will be analyzed. One consists of geographic locations (determined daily) and dive  depth (determined every 30s to 60s) of northern elephant seals during their migrations of 3 to 8 months in  the north Pacific. The second data set consists of hourly locations of Rocky Mountain elk and mule deer, together with geographic information on habitat types. Since animal movements are determined by biological and physical features of the environment and this may be anticipated to vary with the climate,  this research relates to the Strategic Area of environment and global change within the general framework  of the National Science Foundation. The movement models to be developed will allow formal assessment  of temporal and spatial aspects of change and of wildlife responses to them."
"9704548","Studies in Efficient Design of Experiments","DMS","STATISTICS","07/15/1997","06/22/1999","Ching-Shui Cheng","CA","University of California-Berkeley","Continuing grant","Joseph M. Rosenblatt","06/30/2001","$144,000.00","","cheng@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, 9148, MANU, OTHR","$0.00","Proposal #DMS-9704548    Studies in Efficient Design of Experiments    Ching-Shui Cheng  University of California    ABSTRACT      This research involves several problems in experimental design.  Optimal blocking of fractional factorial designs is studied.    Blocking is an effective method for improving the efficiency of  an experiment by grouping the experimental units into more  homogeneous blocks.  How to choose a fractional factorial design  and a blocking scheme simultaneously in an optimal way is of  interest to both theoreticians and practitioners.  A new criterion  for choosing good blocking schemes by examining the alias patterns  of the interactions is formulated.  Methods of constructing optimal  and efficient designs under this criterion are investigated.  Some  unsolved problems in the unblocked case are also studied.  Another  area of research is the projection properties of orthogonal arrays.  In factor screening, often only a few factors among a large pool of  potential factors are active.  Under such assumption of effect  sparsity, it is important to consider projections onto small subsets  of factors.  An extensive study of the projection properties of  orthogonal arrays is carried out.  Connections with search designs  are also explored.  In addition to factorial designs, optimal and  efficient regression designs under random block-effects models are  studied by adopting the approach of approximate theory.    Experimental design is used extensively in a wide range of   scientific and industrial investigations.  In industrial experiments,  often a large number of factors have to be studied, but the  experiments are expensive to conduct.  In this case, the so called  fractional factorial designs in which only a small fraction of all  the possible combinations are observed are particularly useful.  In recent years, factorial designs have received considerable  attention, mainly due to the Japanese success in applying them to  improve quality and productivity in industrial manufa cturing.  One objective of this research is to study the construction of  efficient designs to extract more information, especially when  systematic sources of variation (such as heterogeneity of  experimental material or day-to-day environmental variations)  need to be eliminated to improve the precision.   Since often   only a few of the many potential factors are actually important,  this research also looks into the properties of some commonly  used designs when only a small number of factors are active.  Another research involves a problem arising from a recent   optometry experiment, which also has industrial applications."
"9704603","Differentiable Statistical Functionals and Bayes Asymptotics","DMS","STATISTICS","07/01/1997","03/24/1999","Richard Dudley","MA","Massachusetts Institute of Technology","Continuing grant","Joseph M. Rosenblatt","06/30/2001","$192,500.00","","rmd@math.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1269","0000, OTHR","$0.00","  Dudley  9704603    For differentiating functionals or operators on empirical distribution functions, most statisticians had  adopted compact or Hadamard differentiability. R. Dudley and co-workers are showing that Frechet   differentiability, well known to be superior when it holds, can be obtained by use of p-variation norms  for one-dimensional sample spaces, with optimal bounds on remainders. The program is next being  pursued in multidimensional spaces. Also,  R. Dudley and a co-worker are showing that a posteriori probabilities can be well approximated by normal (Gaussian) probabilities even with respect to relative error when the probabilities are very small.      In the study of differentiable statistical functionals, one is looking at how estimators or other statistics  depend on the observed data distribution: is the dependence smooth, approximately linear, or not, and  how can smoothness or lack thereof be quantified. In Bayes asymptotics, one begins with an assumed prior  probability distribution over an assumed surface of possible parameter values, and from enough  observations, finds an a posteriori distribution which has approximately a Gaussian form. The project is  showing that a simple method works very well, giving very accurate Gaussian approximations to the a posteriori probabilities.  By way of these approximations, the project includes studies of methods for choosing between competing statistical models, or theories, based on multiple data sets, as in multiple  experimental studies on the same or closely related questions."
"9703918","Importance Weighting in Dynamic and Static Monte Carlo","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","08/15/1997","04/29/1999","Wing Hung Wong","CA","University of California-Los Angeles","Continuing grant","Joseph M. Rosenblatt","07/31/2001","$300,000.00","","whwong@stanford.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269, 1271","0000, 9216, 9263, HPCC, OTHR","$0.00","9703918  Wong    In this research, dynamic weighting is developed as a method for dynamic Monte Carlo simulation that has the capability to efficiently sample relevant parts of the configuration space in the presence of many steep energy minima.  This method relies on an additional dynamic variable, namely the importance weight, to help the system overcome steep barriers. A new, non-Metropolis theory is introduced to guide the construction of such weighted samplers.  The method is designed to work in combination with the complementary idea of sampling a complexity ladder.  A second topic studied in this research is the use of rejection control in sequential importance sampling.  This is introduced in order to cope with difficulties of having highly skewed weights in static importance sampling Monte Carlo.  It will enhance the usefulness of static Monte Carlo in the simulation of high dimensional systems.    In statistics, Monte Carlo is an essential computational tool in the evaluation and study of likelihoods and posterior distributions.  The importance of this technique in practical Bayesian inference cannot be overstated.  Furthermore, the advances in Monte Carlo theory and method resulting from this investigation are of a general nature and have significance to many other areas in modern science and technology.  In physical sciences, dynamic Monte Carlo has long been an indispensible tool in the study of fluids, spin systems, phase transitions and critical phenomena, material growth and defect, and the behavior of polymers.  In biology, Monte Carlo advances our understanding of protein conformations, and plays an important role in genetic and evolutionary analysis.  In engineering, partly through its pivotal role in stochastic search methods, Monte Carlo is useful in such diverse areas as expert system, network optimization, machine learning and chip design.  Therefore, the findings of this research are expected to bring considerable benefit to many current areas of strategic importance rang ing from material research to protein engineering."
"9615912","Mathematical Sciences: Workshop on Advances in Smoothing:   Bumps, Jumps, Clustering and Discrimination; May 11-15,     1997; Houston, Texas","DMS","STATISTICS","03/01/1997","08/23/1996","David Scott","TX","William Marsh Rice University","Standard Grant","James E. Gentle","05/01/1998","$9,241.00","","scottdw@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","0000, OTHR","$0.00","DMS 9615912  Scott    This research will bring together a small number of participants  in a workshop entitled ""Advances in Smoothing:  Bumps, Jumps,  Clustering, and Discrimination.""  The workshop will include  leading authorities in several areas, together with other  researchers and students,to share recent findings that  potentially impact other fields.  Recent advances in  nonparametric smoothing could aid workers in clustering improve  the statistical foundations of their algorithms.  Technology  for identifying change points or jumps may help determine the  weight of evidence in favor of numbers of clusters.  Finally,  recent advances in nonparametric regression technology have  dramatically changed the face of algorithms for statistical  discrimination, which is closely related to clustering.  The  workshop will have 3 or 4 leaders, with numerous breakouts for  investigating possible interfaces and technology or theoretical  transfers.  In addition, young researchers, including faculty and  graduate students, will be invited to participate.  The workshop  will accommodate as many as 25-50 participants.    This research will support participation in a workshop entitled  ""Advances in Smoothing:  Bumps, Jumps, Clustering, and  Discrimination.""  Participants will include leading authorities  in these areas, together with other researchers and students.   The workshop will provide a forum for the sharing of recent  findings that potentially impact these other fields.  These  statistical techniques are widely used in science:  for example,  the discovery of the top quark, signal processing for speech  recognition, detection of production quality in manufacturing, to  name just a few.  The workshop will have 3 or 4 leaders.  In  addition, young researchers, including faculty and  graduate students, will be invited to participate.  The workshop  will accommodate 25-50 participants."
"9705599","Workshop on Approaches to the Analysis and Visualization    of Massive Data Sets; San Diego, California","DMS","GEOMETRIC ANALYSIS, STATISTICS","04/01/1997","04/09/1997","David Rocke","CA","University of California-Davis","Standard Grant","James E. Gentle","09/30/1997","$10,000.00","","dmrocke@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1265, 1269","9218, HPCC","$0.00","DMS 9705599  Approaches to the Analysis and Visualization of Massive Data Sets  David Rocke  Summary    Analysis and visualization of massive data sets is an important problem   which has only recently become relevant.  We now have the ability to   generate massive amounts of data from areas as diverse as satellite   imagery and the human genome project.  Computer hardware has progressed to   the point where data sets of a scale from hundreds of gigabytes to   terabytes can be stored, and data can be retrieved from them, but the   challenge of analysis and visualization has only just begun to be met.      This workshop will bring together scientists who generate massive data   sets in astrophysics, environmental science, geophysics, and other areas   and mathematical scientists from a broad array of backgrounds who can   develop methods to attack these problems.  The invitees will include   specialists in statistics, statistical computing, statistical graphics,   smooth and combinatorial optimization, computational geometry, databases,   and visualization, as well as astrophysics, geophysics, and environmental   science.     The product of this workshop will include not just a collection of papers   but also a white paper presenting an agenda for attacking these important   and salient problems."
"9706863","Bayesian Statistical Methods in Molecular Biology","DMS","APPLIED MATHEMATICS, STATISTICS, THEORY OF COMPUTING","09/01/1997","09/04/1997","Gary Churchill","NY","Cornell Univ - State: AWDS MADE PRIOR MAY 2010","Standard Grant","Michael H. Steuerwalt","08/31/2000","$113,000.00","","gary.chuchill@jax.org","373 Pine Tree Road","Ithica","NY","148502488","6072555014","MPS","1266, 1269, 2860","0000, OTHR","$0.00","Churchill 9706863 The investigator develops new statistical methods for problems in molecular biology. Three problem areas are addressed: sequence alignment, phylogeny construction and genetic mapping. Although diverse and individually well developed, these problems can be unified under a common statistical framework. In each problem, a graph-valued random variable is of primary interest. Hierarchical Bayesian models are developed for each problem with an emphasis on common features. The techniques used to compute posterior probabilities from these models are Markov chain Monte Carlo methods. This computational approach is neccessary because analytic solutions for these problems are not attainable. The overall goal of this project is to develop an inference framework that is of general utility in a variety of problems. Toward this end, simple models are considered first. The simple models are generalized to the extent that it is feasible. This project does not solve all of the problems of computational molecular biology. The purpose of investigating a broad range of problems is to develop a common framework for statistical inference. This project develops new methods for the analysis of data that arise in molecular biology and genetics. These statistical methods have a wide variety of applications and are motivated by the dramatic increase in molecular data due to the expansion of biotechnology and the human genome project. Ultimately this leads to a deeper understanding of evolution and the relationships among different groups of organisms and their individual genes. It also addresses the problem of locating the genes that affect specific traits in humans, plants and animals. The greatest potential impact is on gene mapping in agricultural plants, due to ongoing collaborations with plant breeders at Cornell University. Although the project emphasizes statistical theory underlying the analysis of data, the methods are implemented in softw are that is useful to other researchers."
"9708176","Research Conference: Computing Science and Statistics Interface Symposium to be held May 14-17, 1997 in Houston, Texas","DMS","STATISTICS","05/01/1997","04/21/1997","David Scott","VA","Interface Foundation of North America Inc","Standard Grant","James M. Davenport","04/30/1998","$14,400.00","","scottdw@rice.edu","P.O. Box 7460","Fairfax Station","VA","220397460","7039931691","MPS","1269","9218, HPCC","$0.00","Scott 9708176 This proposal requests funding to support young researchers and speakers to attend the 29th symposium on the Interface of Computing Science and Statistics in Houston, May 14-17, 1997, along with related conference expenses. The theme of this year's interface is ""Mining and Modeling Massive Data Sets In Science, Engineering, and Business,"" which is related to other special NSF initiatives. The Symposium is designed to bring together top researchers and workers in several related disciplines to focus on a breaking theme, and to bring younger researchers, graduate students, and interested scientifically minded individuals from a wide cross-section of fields and industries to interact and exchange information and ideas. This year's program has drawn a field of invited speakers from an international group, as well as some of the best minds in the American academic, computer, and business worlds. Furthermore, a large and diverse group of young individuals will attend. The conference also has a sub-theme of the environment and quantitative environmental science. The conference is sponsored by the Interface Foundation of North America, a non-profit educational corporation. The Statistics Department at Rice University is hosting the meeting with David W. Scott as organizer and program chair. Co-hosting institutions include M.D. Anderson Cancer Center and U. Texas School of Public Health. The Symposium has a number of cooperating societies. Data mining holds the promise of dramatically improving our utilization of massive data records accumulating in all areas of government, industry, business, and research. Some of the best statisticians, computer scientists, federal researchers, businessmen, and others are contributing to providing new algorithms and ideas for finding structure in massive data sets. Over 27 invited and 12 contributed paper sessions will be offered. Keynote speaker Jerry Friedman of Stanford will discuss the role of statistics in data mining research. This work poses serious hardware and software problems for computer vendors, and speakers from IBM, SGI, Oracle, AMS, and others will discuss hardware and software issues related to massive data sets management and analysis. Other sessions will describe case studies and war stories. A number of marketing researchers have won permission to describe usually proprietary work that should have educational scientific value. Special luncheon presentations will offer a more informal atmosphere and interaction with leading researchers in computing, genetic and molecular modeling, and network understanding. Visualization is a common thread, with sessions on the uses of virtual reality, exploration, and spatial statistics for massive data sets. The CEO's and chief scientists of some of the leading data mining and statistical software houses will be speaking at this year's meeting. Large scale optimization and wavelet techniques will also be covered among the many sessions. Each speaker will provide a manuscript for a proceedings of the conference, which is to be published in the early Fall. Thus the results and information provided during the session will be available to a very wide audience at low cost."

"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"2515927","Collaborative Research: Online Statistical Inference for Modern Machine Learning","DMS","STATISTICS","10/01/2025","06/23/2025","Likai Chen","MO","Washington University","Standard Grant","Tapabrata Maiti","09/30/2028","$99,322.00","","likai.chen@wustl.edu","1 BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","126900","075Z, 079Z, 1269","$0.00","This research aims to develop statistical tools to improve the reliability of artificial intelligence (AI) that is widely used in real-world systems such as automated decision-making, financial forecasting, and neuroscience research. Modern AI often relies on efficient machine learning algorithms to process large-scale, sequentially arriving datasets. While these algorithms are powerful, understanding their behavior and measuring their uncertainty remains a major scientific challenge. To bridge this gap, the investigators will focus on establishing mathematically rigorous methods for uncertainty quantification to build trustworthy AI. Applications will include enhancing theoretical guarantees and interpretability of neural networks, providing robust estimation and inference for econometric and biomedical studies, and detecting real-time change-points in high-dimensional time series data. The projects will promote the progress of science through open-source software and graduate education, and will support the national interest by contributing to reliable, data-driven decision-making in fields important to economic resilience, public health and national security.<br/><br/>This research will provide a comprehensive theoretical framework for online statistical inference in machine learning, focusing on constant learning-rate stochastic gradient descent (SGD) algorithms. It addresses fundamental challenges such as non-stationarity caused by arbitrarily fixed initialization and complex dependency structures arising in recursive estimation. The investigators will derive the limiting distributions of SGD-type estimators and construct confidence regions with guaranteed asymptotic coverage. Specific efforts will include (1) establishing Gaussian approximations for high-dimensional dropout regularization, (2) deriving limiting distributions for SGD under non-smooth quantile loss functions using characteristic function techniques, and (3) developing online inference procedures for quantile change-point detection in high-dimensional time series using a novel Bahadur representation. These methods will be supported by numerical experiments and implemented in publicly available software. The results shall provide foundational advances for statistical inference in modern machine learning, bridging theoretical developments with practical applications in dynamic, high-dimensional environments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2443867","CAREER: Robust Learning from Preference Feedback","DMS","STATISTICS","10/01/2025","01/21/2025","Cong Ma","IL","University of Chicago","Continuing Grant","Jun Zhu","09/30/2030","$85,950.00","","congma2015@gmail.com","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","126900","079Z, 1045","$0.00","In recent years, preference feedback?comparative inputs such as ?A is better than B??has emerged as a vital resource for guiding decision-making systems. Unlike explicit labels, preference feedback is often easier to collect and can be particularly valuable in subjective tasks where defining ideal outcomes is difficult. However, real-world preference data are often noisy, sparse, and heterogeneous, posing significant challenges to existing statistical methods. For example, recommendation systems may encounter incomplete feedback from users who abandon tasks due to fatigue or provide inconsistent inputs due to individual biases. This project aims to address the challenges of learning from preference feedback by developing robust statistical methods and advancing the theoretical foundations of preference-based learning. Additionally, it seeks to prepare students to tackle these challenges by integrating the research findings into innovative teaching platforms and educational curricula.<br/><br/>The project will focus on three key areas of learning from preference feedback: ranking from pairwise comparisons, user-item rating systems, and reinforcement learning from human feedback. To advance the field, the project will (1) develop robust algorithms for ranking that account for ill-conditioned sampling mechanisms and relax parametric modeling assumptions; (2) propose new estimation and uncertainty quantification methods for user-item ratings that work effectively in sparse and heterogeneous settings; and (3) introduce novel frameworks for reinforcement learning that incorporate ?out-of-list? preference feedback while addressing the issue of distribution shifts. Through these contributions, the project will bridge the gap between statistical theory and practical applications, creating tools to enhance decision-making systems across diverse domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515926","Collaborative Research: Online Statistical Inference for Modern Machine Learning","DMS","STATISTICS","10/01/2025","06/23/2025","Jiaqi Li","IL","University of Chicago","Standard Grant","Tapabrata Maiti","09/30/2028","$148,654.00","","jqli@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","126900","075Z, 079Z, 1269","$0.00","This research aims to develop statistical tools to improve the reliability of artificial intelligence (AI) that is widely used in real-world systems such as automated decision-making, financial forecasting, and neuroscience research. Modern AI often relies on efficient machine learning algorithms to process large-scale, sequentially arriving datasets. While these algorithms are powerful, understanding their behavior and measuring their uncertainty remains a major scientific challenge. To bridge this gap, the investigators will focus on establishing mathematically rigorous methods for uncertainty quantification to build trustworthy AI. Applications will include enhancing theoretical guarantees and interpretability of neural networks, providing robust estimation and inference for econometric and biomedical studies, and detecting real-time change-points in high-dimensional time series data. The projects will promote the progress of science through open-source software and graduate education, and will support the national interest by contributing to reliable, data-driven decision-making in fields important to economic resilience, public health and national security.<br/><br/>This research will provide a comprehensive theoretical framework for online statistical inference in machine learning, focusing on constant learning-rate stochastic gradient descent (SGD) algorithms. It addresses fundamental challenges such as non-stationarity caused by arbitrarily fixed initialization and complex dependency structures arising in recursive estimation. The investigators will derive the limiting distributions of SGD-type estimators and construct confidence regions with guaranteed asymptotic coverage. Specific efforts will include (1) establishing Gaussian approximations for high-dimensional dropout regularization, (2) deriving limiting distributions for SGD under non-smooth quantile loss functions using characteristic function techniques, and (3) developing online inference procedures for quantile change-point detection in high-dimensional time series using a novel Bahadur representation. These methods will be supported by numerical experiments and implemented in publicly available software. The results shall provide foundational advances for statistical inference in modern machine learning, bridging theoretical developments with practical applications in dynamic, high-dimensional environments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515562","Collaborative Research: Efficient Individualized Treatment Selection for Personalized Medicine","DMS","STATISTICS","09/15/2025","06/23/2025","Weibin Mo","IN","Purdue University","Standard Grant","Jun Zhu","08/31/2028","$60,000.00","","harrymok@purdue.edu","2550 NORTHWESTERN AVE # 1100","WEST LAFAYETTE","IN","479061332","7654941055","MPS","126900","075Z, 079Z, 8038","$0.00","Recent advances in data science, statistics, and machine learning have opened new possibilities in precision medicine, enabling clinicians to tailor treatments based on individual patient characteristics. This project focuses on developing a unified and efficient statistical framework to improve treatment decisions by leveraging rich demographic, socio-economic, and biomedical data. By advancing personalized decision-making, this research contributes to better health outcomes, more efficient healthcare delivery, and overall national well-being. The project also offers broad societal impact through its commitment to education, collaboration, and open science. The investigators will mentor graduate students and develop new coursework at the intersection of machine learning, statistics, and personalized medicine. In addition, all software tools developed will be released as open-source, supporting accessibility and reproducibility in scientific research. The interdisciplinary nature of the project encourages collaboration across statistics, medicine, and computer science, and prepares a next-generation workforce to tackle complex health data problems.<br/><br/>This project aims to develop an efficient learning framework for estimating optimal individualized treatment rules (ITRs) across a broad range of personalized medicine settings. The proposed methodology is based on semiparametric modeling and is designed to address complex relationships among covariates, treatments, and outcomes. Key challenges addressed include handling multiple treatment options with cross-treatment structures, modeling a variety of outcome types, and accommodating multi-stage decision-making with time-varying, history-dependent effects. The framework also supports incorporation of domain knowledge for interpretability and practical implementation. From a statistical perspective, the proposed methods achieve double robustness (consistency under two separate model specifications) and statistical efficiency (minimal asymptotic variance), even under model misspecification and in high-dimensional or limited-data scenarios. These contributions advance the state of the art in both semiparametric theory and algorithmic design for ITR estimation. The resulting models are interpretable, scientifically meaningful, and directly applicable to real-world medical problems, including drug development and treatment recommendation. This work not only contributes to foundational statistical theory but also facilitates translational research in healthcare.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2514233","Collaborative Research: Theory of Causal Learning","DMS","STATISTICS","09/15/2025","06/23/2025","Fang Han","WA","University of Washington","Standard Grant","John Kolassa","08/31/2028","$125,000.00","","fanghan@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981951016","2065434043","MPS","126900","075Z, 079Z","$0.00","How can we interpret results from complex machine learning algorithms? How can we mitigate the risks associated with using such models for policy decisions? This project addresses fundamental challenges in deriving valid, reliable, and interpretable causal conclusions from complex data using modern machine learning tools. As machine learning becomes increasingly integral to disciplines such as medicine, economics, education, and the social sciences, the demand for causal insight --- beyond predictive accuracy --- has become more pressing. Yet many machine learning algorithms function as ?black boxes?, offering limited transparency and lacking rigorous frameworks for replicability and uncertainty quantification. This project aims to establish a theoretical foundation for causal learning that makes outputs from machine learning explainable, statistically sound, and actionable in real-world decision-making. The work is complemented by educational and outreach activities that promote understanding of causal reasoning among students and the broader public. Planned efforts include public lectures, collaborations with K?12 educators, and integration of research findings into university curricula. Collaborative partnerships with institutions such as Microsoft, Eli Lilly, and the Fred Hutchinson Cancer Center will help translate methodological advances into impactful scientific and societal applications.<br/><br/><br/>Technically, the project advances causal learning through three interrelated aims. (1) It develops methods for imputing unobserved counterfactual outcomes --- the hypothetical ?what if? scenarios that form the core of causal reasoning --- by integrating flexible machine learning models with statistical principles to preserve both interpretability and rigor. (2) It promotes design-based approaches for quantifying uncertainty, particularly in settings where treatments are assigned randomly or pseudo-randomly via permutations. These methods isolate uncertainty from treatment allocation mechanisms, complementing model-based inference. (3) The project builds a statistical framework for finite-population inference, extending traditional inference techniques beyond super-population assumptions. By drawing on tools from empirical process theory and random matrix theory, the framework provides robust inferential guarantees in realistic data settings where independence and large-sample assumptions fail. Together, these contributions will advance the theory and practice of causal learning, bridging machine learning and statistics to improve both scientific understanding and data-informed decision-making.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2514344","From Semi-supervised Learning to Prediction-powered Inference","DMS","STATISTICS","09/15/2025","06/13/2025","Daniela Witten","WA","University of Washington","Standard Grant","Jun Zhu","08/31/2028","$250,000.00","","dwitten@u.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981951016","2065434043","MPS","126900","075Z, 079Z","$0.00","In today?s world, data are cheap, plentiful, and everywhere . . . except when they are not. The vast majority of books have been digitized, a substantial portion of the internet has been scraped, and low-cost biomedical technologies are available at a doctor?s (or patient?s) fingertips. But, often the data needed for a particular real-world problem is much harder to access, due to cost or other constraints. This project will answer the question: how can the outputs of machine learning or artificial intelligence algorithms be used to augment limited datasets in order to draw meaningful statistical conclusions?  <br/><br/>Consider a setting where the target of inference is a functional of the joint distribution of X and Y, and n independent and identically distributed observations of (X,Y) are available. This research considers the following questions: under what circumstances, by how much, and how can additional observations for which we only have access to X (and not Y) improve inference? The investigative team will consider this question first from a theoretical perspective, by establishing new semi-parametric efficiency results for semi-supervised learning (Project 1); then from a methodological perspective, by developing new and improved estimators for prediction-powered inference (PPI, Project 2); and finally from an applied perspective, by proposing PPI estimators of true positive rate, false positive rate, and area under the curve (Project 3).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2514234","Collaborative Research: Theory of Causal Learning","DMS","STATISTICS","09/15/2025","06/23/2025","Peng Ding","CA","University of California-Berkeley","Standard Grant","John Kolassa","08/31/2028","$125,000.00","","pengdingpku@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","126900","075Z, 079Z, 1269","$0.00","How can we interpret results from complex machine learning algorithms? How can we mitigate the risks associated with using such models for policy decisions? This project addresses fundamental challenges in deriving valid, reliable, and interpretable causal conclusions from complex data using modern machine learning tools. As machine learning becomes increasingly integral to disciplines such as medicine, economics, education, and the social sciences, the demand for causal insight --- beyond predictive accuracy --- has become more pressing. Yet many machine learning algorithms function as ?black boxes?, offering limited transparency and lacking rigorous frameworks for replicability and uncertainty quantification. This project aims to establish a theoretical foundation for causal learning that makes outputs from machine learning explainable, statistically sound, and actionable in real-world decision-making. The work is complemented by educational and outreach activities that promote understanding of causal reasoning among students and the broader public. Planned efforts include public lectures, collaborations with K?12 educators, and integration of research findings into university curricula. Collaborative partnerships with institutions such as Microsoft, Eli Lilly, and the Fred Hutchinson Cancer Center will help translate methodological advances into impactful scientific and societal applications.<br/><br/><br/>Technically, the project advances causal learning through three interrelated aims. (1) It develops methods for imputing unobserved counterfactual outcomes --- the hypothetical ?what if? scenarios that form the core of causal reasoning --- by integrating flexible machine learning models with statistical principles to preserve both interpretability and rigor. (2) It promotes design-based approaches for quantifying uncertainty, particularly in settings where treatments are assigned randomly or pseudo-randomly via permutations. These methods isolate uncertainty from treatment allocation mechanisms, complementing model-based inference. (3) The project builds a statistical framework for finite-population inference, extending traditional inference techniques beyond super-population assumptions. By drawing on tools from empirical process theory and random matrix theory, the framework provides robust inferential guarantees in realistic data settings where independence and large-sample assumptions fail. Together, these contributions will advance the theory and practice of causal learning, bridging machine learning and statistics to improve both scientific understanding and data-informed decision-making.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515561","Collaborative Research: Efficient Individualized Treatment Selection for Personalized Medicine","DMS","STATISTICS","09/15/2025","06/23/2025","Yufeng Liu","NC","University of North Carolina at Chapel Hill","Standard Grant","Jun Zhu","08/31/2028","$180,000.00","","yfliu@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","126900","075Z, 079Z, 8038","$0.00","Recent advances in data science, statistics, and machine learning have opened new possibilities in precision medicine, enabling clinicians to tailor treatments based on individual patient characteristics. This project focuses on developing a unified and efficient statistical framework to improve treatment decisions by leveraging rich demographic, socio-economic, and biomedical data. By advancing personalized decision-making, this research contributes to better health outcomes, more efficient healthcare delivery, and overall national well-being. The project also offers broad societal impact through its commitment to education, collaboration, and open science. The investigators will mentor graduate students and develop new coursework at the intersection of machine learning, statistics, and personalized medicine. In addition, all software tools developed will be released as open-source, supporting accessibility and reproducibility in scientific research. The interdisciplinary nature of the project encourages collaboration across statistics, medicine, and computer science, and prepares a next-generation workforce to tackle complex health data problems.<br/><br/>This project aims to develop an efficient learning framework for estimating optimal individualized treatment rules (ITRs) across a broad range of personalized medicine settings. The proposed methodology is based on semiparametric modeling and is designed to address complex relationships among covariates, treatments, and outcomes. Key challenges addressed include handling multiple treatment options with cross-treatment structures, modeling a variety of outcome types, and accommodating multi-stage decision-making with time-varying, history-dependent effects. The framework also supports incorporation of domain knowledge for interpretability and practical implementation. From a statistical perspective, the proposed methods achieve double robustness (consistency under two separate model specifications) and statistical efficiency (minimal asymptotic variance), even under model misspecification and in high-dimensional or limited-data scenarios. These contributions advance the state of the art in both semiparametric theory and algorithmic design for ITR estimation. The resulting models are interpretable, scientifically meaningful, and directly applicable to real-world medical problems, including drug development and treatment recommendation. This work not only contributes to foundational statistical theory but also facilitates translational research in healthcare.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2443145","CAREER: New Frontiers in Time Series Analysis","DMS","STATISTICS","09/01/2025","03/13/2025","Yao Zheng","CT","University of Connecticut","Continuing Grant","John Kolassa","08/31/2030","$35,878.00","","yao.zheng@uconn.edu","438 WHITNEY RD EXTENSION UNIT 1133","STORRS","CT","062699018","8604863622","MPS","126900","1045","$0.00","Modern time series data present complexities; these complexities, along with the rapidly growing array of new statistical and machine learning (ML) methods, have driven the demand for novel solutions to emerging problems. This CAREER project is driven by three fundamental research questions: (1) How to balance interpretability and accuracy in high-dimensional time series modeling and inference? (2) How to adaptively select time series models in real time for nonstationary data, while managing uncertainty? and (3) How to efficiently combine information from time series data with varying quality? This project aims to advance the field of time series analysis by developing novel statistical models, theories, and inference methods to address these issues. The results of this research will enhance dynamic network inference, facilitate real-time decision-making, and promote the integration of diverse time series data sources. This project will achieve educational impacts by integrating our research with mentoring undergraduate and graduate students, developing courses, and high-school outreach. Additionally, an interdisciplinary time series seminar series will be organized to promote cross-disciplinary interactions and provide students and junior researchers with exposure to diverse research in time series analysis.<br/><br/>This project will advance time series analysis on three main fronts: (1) develop Granger causality interpretable, recurrent neural network-based high-dimensional time series models to balance interpretability and accuracy; (2) develop an online, distribution-free procedure for adaptive time series model selection in nonstationary settings, addressing uncertainty via conformal miscoverage rate calibration; and (3) introduce new methods to efficiently combine time series data with different granularities and to impute data under general missing patterns. Underlying this research agenda is our overarching goal to tackle challenges due to the high-dimensionality, nonlinearity, nonstationarity, different granularity, and mixed quality and completeness of modern time series data. With an emphasis on statistical inference, we seek novel solutions by integrating existing statistical frameworks (Granger causality, model confidence sets, and factor models) with contemporary ML approaches (RNNs, model predictive control, and transfer learning). The results developed through our project will advance innovation in time series analysis, bridge the gaps between interpretability, uncertainty quantification, and black-box algorithms, and promote the use of time series data collected from diverse sources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515814","Collaborative Research: Ranking of Entities","DMS","STATISTICS","09/01/2025","06/18/2025","Gauri Datta","GA","University of Georgia Research Foundation Inc","Standard Grant","John Kolassa","08/31/2028","$180,000.00","Abhyuday Mandal","gauri@uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","126900","","$0.00","With limited resources available to tackle various challenging problems, policy makers and stakeholders often have to prioritize which problems to address in what locations or domains. In statistical terms, this involves the ranking of sub-populations and regions when limited (or no) directly observed data is available, and the available data can be on multiple aspects and of diverse types and modalities. The investigators will address the core theoretical, methodological and algorithmic challenges in such problems of ranking entities in multiple contexts of interest to the nation and to public life. The investigators will also develop techniques for measuring and quantifying the variability and uncertainty of such advanced, data-driven, principled ranking techniques to aid policymakers and stakeholders.<br/><br/>For data to be analyzed using hierarchical models, subject to multiple sources of variability and dependencies, the investigators will develop reliable estimates of the ranks of entities with an appropriate quantification of associated uncertainty. The proposed methodologies will follow a Bayesian framework or a resampling-based frequentist one. While these techniques are primarily computation-driven, the investigators will address theoretical foundations of the proposed approaches both in the Bayesian and in the resampling-based frequentist paradigms. Using scalable computational techniques and leveraging geometric and topological properties of data, the investigators will also develop novel methods for ranking and identification of extremes when multivariate responses are of interest, and address benchmarking for compatibility over hierarchy of domains in a principled way.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515815","Collaborative Research: Ranking of Entities","DMS","STATISTICS","09/01/2025","06/18/2025","Snigdhansu Chatterjee","MD","University of Maryland Baltimore County","Standard Grant","John Kolassa","08/31/2028","$70,000.00","","snigchat@umbc.edu","1000 HILLTOP CIR","BALTIMORE","MD","212500001","4104553140","MPS","126900","","$0.00","With limited resources available to tackle various challenging problems, policy makers and stakeholders often have to prioritize which problems to address in what locations or domains. In statistical terms, this involves the ranking of sub-populations and regions when limited (or no) directly observed data is available, and the available data can be on multiple aspects and of diverse types and modalities. The investigators will address the core theoretical, methodological and algorithmic challenges in such problems of ranking entities in multiple contexts of interest to the nation and to public life. The investigators will also develop techniques for measuring and quantifying the variability and uncertainty of such advanced, data-driven, principled ranking techniques to aid policymakers and stakeholders.<br/><br/>For data to be analyzed using hierarchical models, subject to multiple sources of variability and dependencies, the investigators will develop reliable estimates of the ranks of entities with an appropriate quantification of associated uncertainty. The proposed methodologies will follow a Bayesian framework or a resampling-based frequentist one. While these techniques are primarily computation-driven, the investigators will address theoretical foundations of the proposed approaches both in the Bayesian and in the resampling-based frequentist paradigms. Using scalable computational techniques and leveraging geometric and topological properties of data, the investigators will also develop novel methods for ranking and identification of extremes when multivariate responses are of interest, and address benchmarking for compatibility over hierarchy of domains in a principled way.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2443282","CAREER: Heavy-Tailed Priors for Robust Bayesian Inference in Ecology, Machine Learning, and Astronomy","DMS","STATISTICS","09/01/2025","06/16/2025","Jyotishka Datta","VA","Virginia Polytechnic Institute and State University","Continuing Grant","Yong Zeng","08/31/2030","$270,000.00","","jyotishka@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","126900","079Z, 1045","$0.00","Heavy-tailed probability distributions are routine across many scientific disciplines, from astronomy to ecology and finance and network modeling. Such distributions are often utilized in statistical modeling to incorporate non-linearity, robustness to large observations, and sparsity in high-dimensional data. The overarching goal of this project is to build new scalable statistical methods that incorporate heavy-tailed prior distributions in three disparate application areas: independent component estimation that recovers independent, latent sources from their observed mixtures, astronomical distance estimation from parallax measurements, and statistical modeling of compositional data. The research will result in powerful Bayesian tools with rigorous theoretical justification. This project will also narrow the critical gap between methodological advances in statistics and the tools used by the scientific community and promote increased usage and transparency of state-of-the-art Bayesian tools. The research findings will be incorporated into various educational activities to engage K-12 students. The project will provide research opportunities and training for graduate students and will enhance undergraduate and graduate curricula, accompanied by a monograph.<br/><br/><br/>This project develops Bayesian methodologies to address three significant statistical challenges: (1) unifying feature extraction techniques via novel latent space representations in independent component analysis, (2) improving astronomical distance estimation by incorporating measurement errors and non-linear relationships in parallax data, and (3) constructing prior distributions tailored for high-dimensional simplex-valued data that can adapt to arbitrary sparsity and dependence patterns. By leveraging heavy-tailed priors within hierarchical models, this work provides a new framework for controlling higher-order moments in blind source separation and as mixing densities for normal scale mixtures for handling non-linearity, robustness, and sparsity. The methods to be developed will be rigorously tested in applications spanning astronomy, blind source separation, community detection, and ecological modeling of species diversity and affinity, demonstrating their broad utility. The results will be disseminated through peer-reviewed publications in statistics, machine learning, and other scientific journals, and software implementations will be openly accessible as R packages that benefit the wider quantitative science community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515687","Adaptive Inference by Stabilized Cross-Validation","DMS","STATISTICS","09/01/2025","06/18/2025","Jing Lei","PA","Carnegie-Mellon University","Standard Grant","Jun Zhu","08/31/2028","$250,000.00","","jinglei@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133890","4122688746","MPS","126900","075Z, 079Z","$0.00","Modern data analysis and statistical learning are characterized by two defining features: complex data structures and black-box algorithms. The complexity of data structures arises from advanced data collection technologies and data-sharing infrastructures, such as imaging, remote sensing, wearable devices, and genomic sequencing. In parallel, black-box algorithms?particularly those stemming from advances in deep neural networks?have demonstrated remarkable success on modern datasets. This confluence of complex data and opaque models introduces new challenges for uncertainty quantification and statistical inference, a problem we refer to as ``black-box inference''.  This research project aims to develop flexible, valid inference procedures for modern complex data that harness the strengths of black-box machine learning algorithms. These contributions have potential applications in areas such as policy evaluation, model selection, treatment effect identification, and algorithmic fairness auditing.<br/><br/>A central focus of the project is the development of novel variants of a classical statistical tool: cross-validation, repurposed to enable adaptive inference in conjunction with powerful black-box models. Although cross-validation is widely used for evaluating estimator performance, its theoretical foundations remain limited, particularly in the context of complex data and modern algorithms. This research will begin with a multi-population comparison problem, using a stabilized cross-validation framework, and will then investigate performance guarantees of cross-validation in more general settings. The project will also develop new methods for adaptive population comparisons in high-dimensional and nonparametric regimes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2514240","Statistical Problems in Quantum Learning","DMS","OAC-Advanced Cyberinfrast Core, STATISTICS","08/01/2025","07/01/2025","Yazhen Wang","WI","University of Wisconsin-Madison","Standard Grant","Yong Zeng","07/31/2028","$344,228.00","","yzwang@stat.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","090Y00, 126900","075Z, 7203, 079Z","$0.00","The rapid growth in data generation and collection has led to a significant proliferation of large-scale datasets in recent years. Big data has profoundly transformed scientific research and knowledge discovery. Data science integrates statistical analysis, computational algorithms, and domain-specific knowledge to extract insights from big data, enabling solutions to complex real-world problems. The increasing scale and complexity of data have driven a growing demand for more advanced computational and statistical methods?ranging from hardware to software systems?particularly for machine learning applications. Quantum computing holds the potential to revolutionize data science, especially in computational statistics and machine learning, by enabling quantum learning to meet the emerging demand. This research project aims to investigate statistical challenges in quantum learning. The investigator will develop novel statistical techniques to demonstrate the advantage of quantum approaches over classical methods for tackling difficult machine learning tasks. Additionally, the investigator will actively engage in initiatives that integrate research with workforce development (including graduate students) and apply these advancements to address complex real-world problems.<br/><br/>A central issue in data science is the interplay between statistics and computation, with computational power being essential for developing effective methods to tackle increasingly complex challenges. Quantum computation, which involves preparing and manipulating quantum states of physical systems, offers the potential to revolutionize data science?particularly in computational statistics and machine learning?by enabling a new paradigm known as quantum learning. However, the intrinsic randomness of quantum mechanics introduces stochasticity into quantum computation, posing unique challenges. Data science, through its foundations in statistics and machine learning, is well-positioned to address these challenges by contributing to the development of quantum computing devices, algorithms, and learning techniques. This research project aims to develop statistical methodologies and theoretical foundations to address key problems in quantum learning. Specifically, it will study (i) statistical inference for the Boson sampling model, and (ii) statistical analysis for quantum state and process learning in both classical and quantum settings. The investigator will tackle emerging scientific problems through novel statistical and computational approaches and address the challenges that arise in solving complex learning tasks. The project seeks to establish rigorous, theoretically grounded statistical methodologies and computational procedures that will substantially advance our understanding of quantum learning from both statistical and computational perspectives.<br/><br/>This award by the Division of Mathematical Sciences is jointly supported by the NSF Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515285","Addressing Data Scarcity in Reinforcement Learning: Inference and Decision-Making under Complex Environments","DMS","STATISTICS","08/01/2025","06/18/2025","Yongyi Guo","WI","University of Wisconsin-Madison","Standard Grant","Tapabrata Maiti","07/31/2028","$154,999.00","","guo98@wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","126900","075Z, 079Z, 1269","$0.00","This project addresses the increasing demand for reliable and interpretable reinforcement learning (RL) systems that can function effectively in complex, data-limited environments. RL has demonstrated potential in fields such as healthcare and public policy, where decisions often need to be made with limited and noisy data. However, ensuring that these systems are statistically robust, interpretable, and socially responsible remains a challenge. This research aims to develop tools that improve decision quality, support valid statistical inference, and enhance interpretability in RL algorithms, ultimately building trust and accountability for real-world applications. The broader impacts include advancing the science of machine learning, improving decision-making in resource-constrained settings, and training future data scientists through interdisciplinary mentorship and open educational resources.<br/><br/>The research focuses on developing theoretical foundations and methods for reliable inference and decision-making in RL when data is scarce and models are misspecified. It consists of three main components: First, developing inference and decision tools for contextual bandits with misspecified reward models, where standard methods may fail. Second, constructing an inference framework for adaptive RL algorithms deployed across a population, utilizing a state-statistics decomposition to enhance interpretability and support principled personalization. Third, leveraging auxiliary offline data and structural assumptions to enable robust decision-making in nonstationary environments. This project will produce novel estimation methods and inference procedures with both finite-sample and asymptotic guarantees, along with publicly available software tools. Applications will include adaptive experimentation across various scientific domains, supported by interdisciplinary training in statistics, data science, and engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2531539","Collaborative Research: Bayesian Residual Learning and Random Recursive Partitioning Methods for Gaussian Process Modeling","DMS","STATISTICS, MSPA-INTERDISCIPLINARY, CDS&E-MSS","07/01/2025","06/27/2025","Li Ma","IL","University of Chicago","Standard Grant","Jodi Mead","07/31/2026","$60,290.00","","marlee@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","126900, 745400, 806900","1269, 1303, 5294, 9263","$0.00","Rare natural hazards (for example, storm surge and hurricanes) can cause loss of lives and devastating damage to society and the environment. For instance, Hurricane Katrina (2005) caused over 1,500 deaths and total estimated damages of $75 billion in the New Orleans area and along the Mississippi coast as a result of storm surge. Uncertainty quantification (UQ) has been used widely to understand, monitor, and predict these rare natural hazards.  The Gaussian process (GP) modeling framework is one of the most widely used tools to address such UQ applications and has been studied across several areas, including spatial statistics, design and analysis of computer experiments, and machine learning. With the advance of measurement technology and increasing computing power, large numbers of measurements and large-scale numerical simulations at increasing resolutions are routinely collected in modern applications and have given rise to several critical challenges in predicting real-world processes with associated uncertainty. While GP presents a promising route to carrying out UQ tasks for modern emerging applications such as coastal flood hazard studies, existing GP methods are inadequate in addressing several notable issues such as computational bottleneck due to big datasets and spatial heterogeneity due to complex structures in multi-dimensional domains. This project will develop new Bayesian GP methods to allow scalable computation and to capture spatial heterogeneity. The new methods, algorithms, theory, and software are expected to improve GP modeling for addressing data analytical issues across a wide range of fields, including physical science, engineering, medical science, public health, and business science. The project will develop and distribute user-friendly open-source software and provide interdisciplinary research training opportunities for undergraduate and graduate students.<br/><br/>This project aims to develop a new Bayesian multi-scale residual learning framework with strong theoretical support that allows scalable computation and spatial nonstationarity for GP modeling. This framework integrates and extends several powerful techniques respectively arising in the literature on GP and that on multi-scale modeling, including predictive process approximation, blockwise shrinkage, and random recursive partitioning on the domain. This framework decomposes the GP into a cascade of residual processes that characterize the underlying covariance structures at different resolutions and that can be spatially heterogeneous in a variety of ways. The new framework allows for adoption of blockwise shrinkage to infer the covariance of the residual processes and incorporates random partition priors to enable adaptivity to various spatial structures in multi-dimensional domains. New recursive algorithms inspired by wavelet shrinkage and state-space models will be developed to achieve linear computational complexity and linear storage complexity in terms of the number of observations. The resulting GP method will guarantee linear computational complexity in a serial computing environment and also be easily parallelizable. This Bayesian multi-scale residual learning method provides a new approach to addressing GP modeling issues among spatial statistics, design and analysis of computer experiments, machine learning, and nonparametric regression.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515104","Enhancing Reliable Data Sciences: Statistical Analysis for High-dimensional and Noisy Datasets with Complex Temporal and Nonlinear Structures","DMS","STATISTICS","07/01/2025","06/11/2025","Xiucai Ding","CA","University of California-Davis","Standard Grant","Tapabrata Maiti","06/30/2028","$175,000.00","","xcading@ucdavis.edu","1850 RESEARCH PARK DR STE 300","DAVIS","CA","956186153","5307547700","MPS","126900","1269","$0.00","This project develops new statistical and computational methods to enhance the reliability of data analysis in modern, large-scale datasets, particularly in the era of AI. As data science becomes increasingly central to fields such as finance, medicine, and engineering, there is a critical need for tools that can accurately analyze high-dimensional, noisy, and dynamically changing data. This research addresses fundamental challenges related to robustness, adaptivity, and structure in modern algorithms and statistical procedures. The work contributes to scientific advancement by strengthening the theoretical foundations of data analysis and enabling more accurate and interpretable results across complex applications. It also involves the development of open-source software to ensure broad accessibility and reproducibility and supports the training of junior researchers in advanced statistical methodology and computational techniques.<br/><br/>The project advances foundational understanding in statistics by integrating random matrix theory, manifold and deep learning, and time series analysis. It focuses on the following main areas: (1) analyzing the robustness of manifold and deep learning algorithms for high-dimensional, noisy, and nonlinear data; (2) developing statistical theory and methods for high-dimensional, nonstationary time series; and (3) combining insights from these two areas to study complex functional time series with nonlinear and temporal structures. The research examines new classes of nonlinear random matrices that arise from modern machine learning and dependent data structures. It also proposes adaptive versions of manifold learning methods and constructs new inferential frameworks suitable for time series datasets. These contributions are supported by theoretical guarantees and disseminated through interdisciplinary collaborations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2440180","CAREER: Statistical Inference in the Presence of Group Actions: Theory, Method, and Application","DMS","STATISTICS","07/01/2025","02/10/2025","Ye Zhang","PA","University of Pennsylvania","Continuing Grant","John Kolassa","06/30/2030","$90,000.00","","ayz@wharton.upenn.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","126900","1045","$0.00","In the rapidly expanding field of data science, the ability to understand group actions in data analysis is pivotal for a broad spectrum of scientific tasks. In mathematical terms, a ?group? is a collection of elements combined with an operation that links any two elements to form a third, adhering to closure, associativity, identity, and invertibility principles. A ?group action? involves applying elements of a group to another set?s elements, transforming them in structured ways, such as through rotations or reflections. These transformations are crucial in many data processing applications, including cryo-electron microscopy (cryo-EM), image registration, and multi-reference alignment. Each observation in these problems involves a common, unknown signal and an unknown group element, with the primary goal being to infer both the signal and the group elements accurately. This project aims to significantly advance statistical understanding and develop effective methodologies for handling data influenced by group actions. The wide existence of such data ensures that the progress we make towards our objectives will have a great impact not only on the statistics and machine learning community but also on a much broader scientific community, including fields such as structural biology, computer vision, and signal processing. This project will have educational outcomes that result in curriculum development, teaching, and outreach activities, including activities to K-12 students through the University of Pennsylvania Data Science Academy. The project will advance applications in image recognition and time series alignment, which have broad application in areas like medical imaging.<br/><br/>This project is structured around three main aims, each designed to tackle distinct aspects of group actions. First, the PI will improve the accuracy of orbit recovery in scenarios where the prior distributions of group elements are non-uniform, developing computationally efficient procedures that are effective under realistic conditions. Second, the PI will develop theories and methods for group synchronization problems, particularly under high noise levels and in situations with incomplete data, aiming to reduce the error of group recovery and provide entrywise inference. Third, the PI will address theoretical and computational challenges in the multi-reference alignment problem, developing procedures specifically designed for the cyclic structural nature of data, thereby enabling more precise uncertainty quantification. Together, these aims will not only enhance the theoretical understanding of and the ability to analyze group actions but also lead to the development of accurate and computationally efficient algorithms designed to tackle real-world challenges in data analysis where group actions are integral.  This research project will have impacts more broadly, in that it will result in software development and in the education of technical experts. These experts will use this software to advance applications in image recognition and time series alignment, which have broad application in areas like medical imaging.  These activities will then advance applications in image recognition and time series alignment, which have broad application in areas like medical imaging.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515171","Statistical Frameworks for Self-Supervised Representation Learning and Their Biomedical Applications","DMS","STATISTICS","07/01/2025","06/11/2025","Shulei Wang","IL","University of Illinois at Urbana-Champaign","Standard Grant","Jun Zhu","06/30/2028","$175,000.00","","shuleiw@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","126900","075Z, 079Z, 8038","$0.00","While recent advancements in large-scale machine learning models have shown impressive capabilities, they often rely on hundreds of millions of labeled samples. However, obtaining high-quality labels in many fields is extremely costly, so most available data remain unlabeled. For example, although millions of images and videos can be easily collected from social media platforms, manually labeling them is a tedious and time-consuming process. To address the challenge of limited labeled data, self-supervised representation learning has emerged as a promising approach in computer vision and natural language processing. It has already played a key role in the success of recent large language models. Despite its strong performance in practice, the theoretical understanding of self-supervised representation learning remains limited. Moreover, the problem of scarce labeled data also affects biomedical research, but the existing self-supervised methods cannot be directly applied due to the unique nature of biomedical datasets. This project aims to address these gaps by developing new theoretical frameworks for self-supervised representation learning, along with computational tools tailored to biomedical studies. It also includes educational efforts to engage students and the broader public with this growing area of research.<br/><br/>This project aims to advance the theoretical foundations of self-supervised representation learning and transform how unlabeled data are utilized in biomedical research. On the theoretical front, the project will investigate self-supervised learning on a low-dimensional nonlinear model, which effectively captures the invariant and intrinsic low-dimensional structure underlying observed data. Building on this nonlinear modeling framework, this project will develop a novel theory that explains the empirical success of self-supervised learning and clarifies the role of pseudo labels generated from unlabeled data. This theoretical foundation will inform the design of innovative and principled learning methodologies. On the application side, the project will integrate self-supervised representation learning into biomedical applications, including microbiome studies and omics-based longitudinal (trajectory) data. The project will develop new computational tools and software tailored to these contexts, enabling the effective use of large-scale unlabeled biomedical data. These advancements are expected to help address critical scientific questions and contribute to a deeper understanding of biological systems and human health.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2441652","CAREER: Distributional Approximation for Sharp Finite Sample Bounds with Applications to Dependent Data and Complex Estimators","DMS","STATISTICS","07/01/2025","02/04/2025","Morgane Austern","MA","Harvard University","Continuing Grant","Yong Zeng","05/12/2025","$82,562.00","","maustern@fas.harvard.edu","1033 MASSACHUSETTS AVE STE 3","CAMBRIDGE","MA","021385366","6174955501","MPS","126900","1045, 079Z","$0.00","AI and machine learning algorithms are transforming numerous scientific fields, with some of the most promising approaches relying on mathematical tools called ""finite sample probability bounds."" These bounds are crucial, for example, in reinforcement learning, which underpins the success of systems like AlphaGo. Additionally, they play a key role in uncertainty quantification and the theoretical analysis of black-box machine learning algorithms. However, classical finite sample bounds have a significant limitation: they are often overly conservative. This conservatism leads to underperforming algorithms and unnecessarily loose guarantees. This project is built around a novel yet straightforward idea: finite-sample bounds can be derived from infinite-sample results. By leveraging recent breakthroughs in optimal transport and probability theory, the project aims to develop a new method for deriving such inequalities. These methods will be applied to problems such as online data-driven decision-making, early stopping rules, and machine learning for multiscale physical models. The PI will interweave their research and teaching throughout the research period and beyond. In particular, the PI will provide research training opportunities to graduate students and develop undergraduate and graduate courses, with course materials made publicly available and with joint participation from industry.<br/><br/>Classical concentration inequalities, such as the ones derived by Hoeffding or Bernstein, are over-conservative, are regularly inadequate for heavy-tail distributions, and often rely on the assumption of independence. In this project, the PI tackles those limitations from a new angle, starting with an infinite-sample result for a given problem, such as a central limit theorem, and translating it into a finite-sample result for the same problem by using the concept of distributional approximations. The advantage of this novel proof method is threefold: Firstly, the derived bounds improve as the sample size grows. This leads to inequalities that are considerably tighter than the classical ones. Secondly, limit theorems often hold for many forms of dependent data. This opens a more promising path to derive finite sample probability bounds than conventional Chernoff-based techniques. Lastly, by extending this approach to non-Gaussian limits, the PI develops finite sample concentration inequalities for heavy-tailed statistics. This project will (1) develop a completely novel method for obtaining concentration inequalities, as well as new results for transport distances and optimal transport, (2) provide machine learning theorists with a new set of powerful probability tools for obtaining high-probability guarantees for high-dimensional estimators and dependent and structured data, and (3) provide tighter tail bounds which will lead to algorithmic improvements and improved uncertainty quantification.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2440824","CAREER: High-Dimensional Learning and Inference from Heterogeneous Data Sources","DMS","STATISTICS","07/01/2025","02/10/2025","Pragya Sur","MA","Harvard University","Continuing Grant","Tapabrata Maiti","05/12/2025","$88,793.00","","pragya@fas.harvard.edu","1033 MASSACHUSETTS AVE STE 3","CAMBRIDGE","MA","021385366","6174955501","MPS","126900","1045","$0.00","This project will develop novel statistical theories and methods for handling large, heterogeneous datasets. Modern scientific applications often produce heterogeneous data of different types for the same problem. For instance, a single-cell biologist may observe multiple types of sequencing data from diverse instruments, all relevant for understanding the biological pathways of a single complex disease.  The challenge lies in effectively combining these different data types to build statistical pipelines that outperform those developed using any one data type. Traditional statistical approaches struggle with this challenge. This project will establish a new statistical paradigm to address the complexities of such heterogeneous data while accounting for datasets with billions of variables. The project outcomes will facilitate principled prediction and inference in applications ranging from single-cell biology to precision health and neuroimaging. The project will involve graduate student participation and the development of new curricula at graduate and undergraduate levels that incorporate the project outcomes. Additionally, the research will engage medical professionals to facilitate the dissemination of the research products in current biomedical practice.<br/><br/><br/>This project will develop a modern statistical framework to address data heterogeneity in high dimensions, focusing on three key sub-themes: (i) creating principled and robust prediction strategies for multi-view learning, (ii) developing new inference pipelines and prediction analysis frameworks for meta-learning, and (iii) introducing novel inference methods for low-dimensional functionals under transfer learning. In multi-view learning, this project will quantify optimal strategies for cooperative learning, devise new adversarial learning techniques, and analyze the effects of interpolation learning.  In meta-learning, this project will introduce new debiasing strategies to tackle inference questions that arise during fine-tuning following an initial phase of pre-training.  In transfer learning, this project will develop general-purpose strategies for ranking source distributions and establish new inference schemes for low-dimensional functionals of scientific relevance. On the technical front, this project will introduce novel comparison inequalities, algorithmic proof methods, and leave-one-out techniques that effectively capture the interplay between high dimensionality and heterogeneity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515519","Deeper Understanding of Mean-field Models","DMS","STATISTICS","07/01/2025","06/11/2025","Sumit Mukherjee","NY","Columbia University","Standard Grant","John Kolassa","06/30/2028","$240,000.00","","sm3949@columbia.edu","615 W 131ST ST","NEW YORK","NY","100277922","2128546851","MPS","126900","","$0.00","This project addresses the critical need to approximate complex statistical models in the era of big data. The investigator aims to develop a comprehensive understanding of when simpler approximations can be effectively used, and to provide rigorous guarantees for their accuracy. The core challenge lies in the trade-off between model complexity (which is often necessary to capture the nuances of large datasets) and computational feasibility. While complex models offer rich descriptions, their computational demands can be prohibitive. The investigator proposes to explore whether simpler, approximated models can retain the essential features of their complex counterparts while significantly reducing computation time. Graduate students will be involved in this research.<br/><br/><br/>The research will concretely examine the validity of naive mean-field variational inference in several key areas where complex models are prevalent, which include (i) High-Dimensional Bayesian Regression (linear and logistic), (ii) Latent Dirichlet Allocation (LDA), (iii) Mixed Membership Models, and (iv) Exponential Random Graph Models (ERGMs). For each of these examples, the investigator plans to develop tailored inference methods, and provide rigorous, quantified bounds on the errors introduced by these approximations. This will offer a clear understanding of the trade-off between simplicity and accuracy. The overarching goal is to equip researchers using naive mean-field based variational inference with concrete guidelines on when and under what circumstances such methods can be reliably applied, and when they might fall short. This will advance the principled application of approximate inference techniques in the context of big data analytics, contributing to more efficient and reliable scientific discovery.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515716","Efficient Data Removal Under High-dimensional Asymptotics with Applications to Risk Estimation and Machine Unlearning","DMS","STATISTICS","07/01/2025","06/10/2025","Mohammad Ali Maleki","NY","Columbia University","Standard Grant","Tapabrata Maiti","06/30/2028","$240,000.00","","mm4338@columbia.edu","615 W 131ST ST","NEW YORK","NY","100277922","2128546851","MPS","126900","075Z, 079Z, 1269","$0.00","The rapid expansion of machine learning (ML) and artificial intelligence (AI) has created an urgent need for tools that enhance trust, and control over these technologies. This project aims to address that need by developing methods that help explain why a model makes certain predictions, improve model tuning, and detect harmful or misleading data?such as intentionally corrupted inputs introduced by adversaries?before they compromise the model?s reliability. A promising strategy for tackling these challenges is to analyze the impact of individual data points or subsets of data by estimating how their removal affects the model?s behavior. This research supports the national interest by advancing scientific understanding of AI, improving the robustness of decision-making systems, and contributing to the development of technologies that align with privacy protections.<br/><br/>The project investigates whether it is possible to develop computationally efficient algorithms that approximate the output of a model trained without a given subset of data, without having to retrain the model from scratch. This question is particularly challenging in high-dimensional settings, where the number of features is large relative to the sample size. The project focuses on designing data removal methods that are both scalable and theoretically sound in these regimes. The resulting algorithms will be evaluated in two important application areas: risk estimation and machine unlearning. Through this work, the project aims to lay the foundation for practical tools that improve model interpretability and accountability in complex learning systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515520","Unsupervised Learning and Nonlinear Dimension Reduction: Advances with Optimal Transport, Empirical Bayes, and Variational Inference","DMS","STATISTICS","07/01/2025","06/17/2025","Bodhisattva Sen","NY","Columbia University","Standard Grant","John Kolassa","06/30/2028","$240,000.00","","bodhi@stat.columbia.edu","615 W 131ST ST","NEW YORK","NY","100277922","2128546851","MPS","126900","075Z, 079Z","$0.00","Modern scientific data sets?ranging from single-cell RNA sequencing with tens of thousands of genes per patient, to galaxy-survey spectra with millions of stars, to user-item interaction matrices in online platforms?share two features: (i) ultra-high dimensionality and (ii) latent parameters that obey common structural laws (e.g., exchangeability, sparsity, or low-rank dependence). This project tackles both challenges at once. It advances statistical foundations for such problems by (1) providing a new framework to theoretically study empirical Bayes methods in these complex models that learn the latent-parameter distribution directly from the data, and (2) developing cutting-edge unsupervised dimension-reduction techniques that embed the high-dimensional observations into lower-dimensional representations while preserving the essential structure and relationships within the data. Together, these tools will transform ad-hoc prior modeling into an objective, data-driven procedure and yield principled, scalable inference for large-scale applications. Further, collaborations with astronomers will ensure immediate scientific impact, and several of the research directions will shape the Ph.D. dissertation of multiple Columbia graduate students, fostering the next generation of data-science leaders.<br/><br/><br/>The project integrates two tightly linked research thrusts. (a) Building on recent advances in nonparametric empirical Bayes, the PI will design flexible empirical Bayes estimators for latent-variable distributions for exchangeable parameters. The central innovation is to merge empirical Bayes ideas with variational approximations in general probabilistic latent-variable models, producing estimators that remain computationally tractable, and achieve optimal risk. (b) Classical linear dimensionality reduction techniques like principal component analysis and multidimensional scaling are often inadequate for datasets that are growing increasingly complex in fields such as genomics, astronomy, and finance. To tackle these challenges, the PI will design non-linear reduction techniques --- combining manifold learning with low-rank factor models ---informed by ideas from optimal transport and algebraic geometry. Together, these advances will deepen statistical theory at the nexus of empirical Bayes, unsupervised learning, and high-dimensional inference. All algorithms will be released as open-source R/Python packages accompanied by tutorials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515821","Nonlinear Functional Time Series Analysis","DMS","STATISTICS","07/01/2025","06/17/2025","Alexander Aue","CA","University of California-Davis","Standard Grant","Jun Zhu","06/30/2028","$240,000.00","","aaue@ucdavis.edu","1850 RESEARCH PARK DR STE 300","DAVIS","CA","956186153","5307547700","MPS","126900","","$0.00","This research will contribute to the analysis of modern and complex data observed as functions or curves. One example of such data is provided by yield curves used in economics as an indicator for future growth, inflation and interest rate expectations, and investor sentiment. Since data of this type is often observed across time, there will be a focus on developing theoretically justified and empirically validated forecasting algorithms, which can find applications in many fields of inquiry including finance, where practitioners are interested in predicting the volatility curves of intraday tick-by-tick transaction data of financial assets. The research is therefore of immediate interest in areas of application and will further connect statistics and fields significantly relying on data-analytic tools. In addition, the research will advance mathematical and computational statistics. It will produce doctoral students, who are theoretically and practically versed in both statistics and an area of application. The training and involvement of undergraduate students is also included through regular coursework, independent study and projects. <br/><br/>This research concerns the development of a comprehensive framework for the analysis of nonlinear and possibly non-Gaussian functional time series. Such functions naturally arise in a variety of contexts such as the modeling of cumulative intraday returns of financial assets. The research aims at providing a statistical foundation to analyze functional observations that exhibit non-linearity and are possibly heavy-tailed. Key outcomes to be achieved include new probabilistic results concerning the structure of nonlinear functional time series, the introduction of two new functional time series models as lead examples of the theoretical groundwork, namely a fully general version of generalized autoregressive conditional heteroscedastic processes as well as a novel random coefficient autoregressive model. These models are accompanied by a suite of inference procedures for further statistical analysis. Achieving the goals of the research projects will require non-standard approaches, as traditional sets of assumptions for the analysis of linear functional time series are expected to be of limited use in the setting studied under this research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2529893","Conference: Southern Regional Council on Statistics Summer Research Conference","DMS","STATISTICS","06/15/2025","06/10/2025","Katherine Thompson","KY","University of Kentucky Research Foundation","Standard Grant","John Kolassa","11/30/2025","$40,000.00","Gregory Hawk","katherine.thompson@uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","126900","7556, 9150","$0.00","The 60th Summer Research Conference (SRC) and Statistics Undergraduate Research Experience (SURE) will be held in Jekyll Island, Georgia on June 9-11, 2025. The Southern Regional Council on Statistics (SRCOS) is a consortium of statistics and biostatistics programs from universities in 16 states in the Southern region. The SRC is an annual conference sponsored by the SRCOS. The purpose of the SRC is to encourage the interchange and mutual understanding of current research ideas in statistics and biostatistics, and to provide motivation and direction to further research progress. The SRC will give new researchers an opportunity to participate in the meeting and to interact closely with leaders in the field in a manner not possible at larger meetings. In addition to the graduate student participation, the 60th SRC will also include the 6th annual Statistical Undergraduate Research Experience (SURE) from June 9-11, 2025. SURE is a conference within a conference aimed to encourage the participation of undergraduate students to pursue graduate education and career opportunities in STEM fields. SURE will include events specifically for undergraduate students and undergraduate mentors, such as a panel about career opportunities in statistics, a real data analytics workshop, and a speed-mentoring session with current statistics and biostatistics graduate students.<br/> <br/>The SRC is particularly valuable for graduate students, isolated statisticians, and faculty from smaller regional schools in the southern region at drivable distances without the cost of travel to distant venues. Speakers will present formal research talks with adequate time allowed for clarification, amplification, and further informal discussions in small groups.  Under the travel support provided by this award, graduate students will attend and present their research in posters to be reviewed by more experienced researchers. Participation in SURE will encourage undergraduate students to enter STEM fields, including statistics or biostatistics, and provide training to support this endeavor. The 60th SRC will strengthen the research of the statistics and biostatistics community as a whole and help bridge the gap for undergraduate students to pursue statistics or biostatistics, particularly in the sixteen states of the Southern Region. The SRCOS website can be found here: https://www.srcos.org; the SRC website can be found here: https://www.srcos.org/conference; the SURE website can be found here: https://www.srcos.org/sure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2513573","Conference: STATGEN25","DMS","STATISTICS","06/15/2025","05/22/2025","Sandra Safo","MN","University of Minnesota-Twin Cities","Standard Grant","John Kolassa","05/31/2026","$19,104.00","Kelsey Grinde","ssafo@umn.edu","2221 UNIVERSITY AVE SE STE 100","MINNEAPOLIS","MN","554143074","6126245599","MPS","126900","7556","$0.00","The 2025 Conference on Statistics in Genomics and Genetics (STATGEN 2025) will be held at the University of Minnesota in Minneapolis, MN from May 21 to May 23, 2025. The conference will bring together researchers, including students, postdoctoral researchers, and early career scientists, to share ideas and collaborate in the rapidly evolving fields of genetics and data science. This event is an invaluable opportunity for early-career researchers to connect with experts, build professional networks, and gain insight from cutting-edge scientific work. By supporting the participation of students and early career researchers, the conference will help cultivate the next generation of scientists who will contribute to advancing knowledge and innovations in this important area of research.<br/><br/>Building on the success of the inaugural meeting, the conference will expose attendees to cutting-edge research and methods in genomics, statistics, and data science. This event will contribute to ongoing dialogue and innovation in genetics and genomics and the integration of statistical methods into genomic research, supporting the development of future breakthroughs in areas such as personalized medicine and disease prevention.  Special emphasis will be placed on fostering the collaboration and participation of early-career researchers, with travel assistance provided by the NSF grant. By facilitating broad participation, the conference will help shape the next generation of researchers and leaders in the field of genomics and data science.  The conference will feature a variety of presentations, including keynotes, panels, invited, contributed, speed, and poster sessions, offering attendees the platform to explore the latest research and discoveries.  Attendees will discuss and promote the latest advancements in statistical theory, methods, and tools, particularly in genetics and genomics, aimed at making evidence-based statistical inferences from complex, noisy, and high dimensional data sources.  More information may be found athttps://www.sph.umn.edu/events-calendar/statgen-2025.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2512997","Conference: International Workshop on Applied Probability: Probability, Statistics and their Applications (IWAP 2025)","DMS","STATISTICS","06/15/2025","06/10/2025","Donald Martin","NC","North Carolina State University","Standard Grant","John Kolassa","05/31/2026","$15,000.00","Kimberly Sellers","demarti4@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","276950001","9195152444","MPS","126900","7556","$0.00","This award supports the participation of early career U.S.-based researchers in the ?International Workshop on Applied Probability: Probability, Statistics and their Applications (IWAP 2025),? which will take place June 9-13, 2025 in SAS Hall of North Carolina State University. In light of the many applications of probability and statistics in public policy, decision theory, social and health sciences and various scientific fields, the conference will offer a platform for advancing new statistical and mathematical methodologies and computational procedures. In addition to research presentations, there will be ample networking opportunities throughout the event, helping to facilitate the exchange of ideas. Thus, while the funding will primarily support students and participants with recent Ph.D.?s, and will help to educate and bring them to the forefront of this active and important research area, the conference will be beneficial for all of the approximately 50 to 60 workshop participants that are expected.<br/><br/>The conference will consist of eight plenary talks given by top researchers. There will also be both invited and contributed sessions. Topics that will be presented include scan statistics, graphs/networks, statistical machine learning, variable selection and quantum computing, with applications to such areas as genomics, chemistry, individualized treatment regimes in medical research, network traffic, economic forecasting, medical imaging, cluster, anomaly and change point detection, and forecasting tourist arrivals. See https://iwap2025.weebly.com/ for more information about IWAP 2025.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515471","Conference: 2025 Conferences for New Researchers in Statistics, Probability, and Data Science","DMS","PROBABILITY, STATISTICS","06/15/2025","06/10/2025","Armeen Taeb","WA","University of Washington","Standard Grant","Yong Zeng","05/31/2026","$39,987.00","Eardi Lila","ataeb@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981951016","2065434043","MPS","126300, 126900","7556","$0.00","The Committee on New Researchers of the Institute of Mathematical Statistics will hold its 25th conference at Vanderbilt University during the three days prior to the Joint Statistical Meeting. The event will include oral and poster presentations by new researchers, plenary talks by established researchers, and open discussions on future directions for statistics, probability, and data science. There will also be panel discussions on teaching and mentoring, publishing, funding, and collaborations.<br/><br/>The New Researchers Conference is an annual event organized under the auspices of the Institute of Mathematical Statistics by its Committee on New Researchers. It serves as the flagship meeting for early-career researchers in statistics, probability, and data science. In 2025, Vanderbilt University will host the 25th New Researchers Conference on the three days prior to the Joint Statistical Meetings. The primary objective of the conference is to provide a platform for interaction among new researchers and offer opportunities for mentorship from leaders in the field. This conference is explicitly aimed at developing the next generation of researchers in statistics and probability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2526584","Conference: Travel Awards for 14th International Conference on Bayesian Nonparametrics","DMS","STATISTICS","06/15/2025","06/10/2025","Garritt Page","UT","Brigham Young University","Standard Grant","John Kolassa","05/31/2026","$13,000.00","","page@stat.byu.edu","A-153 ASB","PROVO","UT","846021128","8014223360","MPS","126900","7556","$0.00","This award supports junior researcher travel to attend the 14th International Conference on Bayesian Nonparametrics that will be held on 23-27th, 2025 at UCLA's campus, Los Angeles, CA, USA. This conference is held biannually and is known to bring together leading experts and talented young researchers working on applications and theory of nonparametric Bayesian statistics. It is an official section meeting of the Bayesian Nonparametrics (BNP) section of the International Society for Bayesian Analysis (ISBA).  The BNP 14 meeting is also co-sponsored by the Institute of Mathematical Statistics (IMS), and it is endorsed by the Section of Bayesian Statistical Sciences of the American Statistical Association.  The main goals of the conference are to promote the interaction among scientists who develop and use BNP methods; to encourage discussions among researchers in various areas and fields of study; to foster cross-fertilization of ideas; to facilitate small group discussions among senior and junior researchers; and to disseminate results of the conference as widely as possible.  More than 250 participants are expected to attend.   BNP methods are statistical procedures that favor flexibility which permits relaxing many assumptions that are often times very rigid and/or hard to verify.  Due to this, there has been a surge in the number of fields that have begun to employ BNP methods such as genetics, archaeology, psychology, economics, neuroscience among others.    <br/><br/><br/>National Science Foundation support will enable the participation of 13 junior participants, including graduate students and postdoctoral scholars, at the conference. The International Conference on Bayesian Nonparametrics is the premier conference of BNP methods and allows weeklong discussions by including a good mix of methodological, applied, and theoretical sessions.  The topics of these sessions include BNP connected to deep learning, Bayesian density estimation, scalable nonparametric regression, hazard rate and survival function estimation (without or with covariates), estimation of spectral distribution of a time series, estimation of conditional density and density regression, classification, clustering, and estimation of the distribution of latent variables such as random effects and so on.   NSF participant support is directed to individuals with evidence of high-quality, early-career research accomplishments.  All the verified invited and contributed speakers are listed on the conference website at https://bnp14.org/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2516765","Conference: ICSA 2025 Applied Statistics Symposium","DMS","STATISTICS","06/15/2025","05/29/2025","Xiaojing Wang","CT","University of Connecticut","Standard Grant","John Kolassa","05/31/2026","$21,000.00","Ming-Hui Chen","xiaojing.wang@uconn.edu","438 WHITNEY RD EXTENSION UNIT 1133","STORRS","CT","062699018","8604863622","MPS","126900","7556","$0.00","This award will play a pivotal role in promoting advancing the field of statistics by supporting the participation of graduate students and early-career researchers in the International Chinese Statistical Association (ICSA) 2025 Applied Statistics Symposium, which will be held in Storrs, Connecticut from June 15 to June 18, 2025. With the theme of ?Insights into Complexity: Empowering Policy through Statistical Learning and Data Analytics,? the symposium will highlight the role of statistics and data science in addressing modern policy challenges. The symposium aims to bring together a global community to explore how statistical learning and data analytics can address complex decision-making in various disciplines. The symposium will unite researchers, practitioners, and students passionate about advancing statistical methods, biostatistics, data science, artificial intelligence, and their applications in various fields and foster new directions for statistical inference, facilitating discoveries from seemingly messy data to inform real-world impactful decisions.<br/><br/>The ICSA 2025 Applied Statistics Symposium will feature an engaging format, including plenary talks, invited sessions, and contributed posters. We prioritize broadening participation across all aspects of the conference, with a special focus on encouraging the active participation of early-career researchers (those who have earned their doctorates within the past five years) in invited sessions. For students, ICSA 2025 symposium offers numerous opportunities, including a student paper competition and the chance to present posters. The symposium's website, accessible at https://symposium2025.icsa.org/, serves as a hub for seamless communication and resource sharing among participants.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2516758","Conference: EVA 2025: The 14th International Conference on Extreme Value Analysis","DMS","STATISTICS","06/01/2025","05/22/2025","Vladas Pipiras","NC","University of North Carolina at Chapel Hill","Standard Grant","John Kolassa","11/30/2025","$20,000.00","Richard Smith, Mariana Olvera-Cravioto","pipiras@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","126900","7556","$0.00","The project will provide financial support for junior researchers to attend the 14th International Conference on Extreme Value Analysis (EVA 2025) to be organized and hosted at the University of North Carolina at Chapel Hill from June 23 to June 27, 2025. Extreme value analysis refers to the statistical modeling of extreme data observations, allowing to make probabilistic predictions beyond the range of observed data. With the mathematical foundation going back to the mid 20th century, the scope of EVA expanded considerably over the last decades, including applications in insurance, finance, engineering, atmospheric science, and other domains. The EVA conferences are bi-annual, and they attract some of the most prominent researchers in statistics and probability, as well as more applied domains working on extremes. The supported junior researchers will have opportunities to attend a satellite workshop with tutorials, to present their work in oral or poster sessions, and to participate in a data challenge and best student paper competition.  <br/><br/><br/>EVA started with univariate models and frequentist methods for independent and identically distributed observations, but has since seen extensions to time series, spatial and spatial-temporal data, Bayesian methods, and so on. It has had a profound impact on a wide range of applications in atmospheric science (droughts, floods), finance (value-at-risk, market crashes), engineering (reliability, rare event prediction), social networks (viral tweets) and other domains. Over the recent past, the field's impact has grown by making and exploiting connections to high-dimensional statistics (graphical models, dimension reduction), machine learning (prediction of extremes) and AI (training of neural networks) methods. These various themes, both classical and modern, will be well represented at the conference EVA 2025. The conference website can be found at https://eva2025.unc.edu.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2443410","CAREER: Extending the reach of empirical Bayes: Calibration, nuisance parameters, likelihood asymptotics, and machine learning","DMS","STATISTICS","06/01/2025","03/10/2025","Nikolaos Ignatiadis","IL","University of Chicago","Continuing Grant","Tapabrata Maiti","05/31/2030","$75,343.00","","ignat@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","126900","1045","$0.00","Scientists across various fields face the challenge of answering numerous related questions with limited or noisy data. For example, genomicists may need to assess thousands of genes using data from only a few subjects, while survey statisticians might analyze average incomes in many towns based on limited surveys. To address these challenges, researchers often borrow information from related questions, a process that requires sophisticated statistical reasoning due to varying underlying characteristics. Empirical Bayes offers a method to enhance individual question inference by sharing information, potentially improving statistical accuracy. However, it relies on strong modeling assumptions, limiting its applicability. This research aims to make empirical Bayes more powerful and accessible by integrating it with modern machine learning and causal inference, demonstrating its effectiveness under fewer assumptions, and developing methods to assess uncertainty more effectively. These advancements will help practitioners utilize empirical Bayes in various scientific and industry contexts without extensive statistical modeling. Additionally, the project will produce a monograph and provide training for students and preceptors on modern data science challenges using empirical Bayes.<br/> <br/> <br/>This research aims to advance empirical Bayes by developing new methodologies and statistical theories to address four key limitations: the assumption of known likelihoods, the treatment of nuisance parameter heterogeneity, the development of nonparametric inference methods, and the integration with machine learning. The project will create new inference methods for primary parameters by using empirical partially Bayes methods and Bayesian nonparametrics, enhancing frequentist Bayes multiple testing theory with guarantees like false discovery rate control. Additionally, the research seeks to learn the unknown Bayes rule through neural networks with a specialized loss function, incorporate James-Stein shrinkage for combining unbiased estimates with semisupervised machine learning, and integrate empirical partially Bayes inference with doubly robust double machine learning for large-scale causal inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2514925","Conference: The Past, Present, and Future of Statistics in the Era of AI","DMS","STATISTICS","06/01/2025","05/22/2025","Tatiyana Apanasovich","DC","George Washington University","Standard Grant","John Kolassa","05/31/2026","$18,400.00","Xiaoke Zhang, Subrata Kundu","apanasovich@gwu.edu","1918 F ST NW","WASHINGTON","DC","200520042","2029940728","MPS","126900","7556","$0.00","The Department of Statistics at George Washington University will host the conference ?The Past, Present, and Future of Statistics in the Era of AI? from May 8-10, 2025, in Washington, DC. This event will explore the evolving relationship between statistics and artificial intelligence (AI), examining its growing impact on scientific research, education, and society. The conference will explore AI?s influence on statistical research and its role in shaping the future of data-driven decision-making. The program will feature technical sessions, short courses, panel discussions, and poster presentations, providing a platform for knowledge exchange and interdisciplinary collaboration. The event will also include plenary talks from leading experts in the field, offering insights into cutting-edge developments and future directions. Additionally, we have invited practitioners from both academia and industry to attend and participate in panel discussions, ensuring a broad range of perspectives. A key priority of the conference is to promote the active participation of junior scholars, including as speakers and presenters, providing them with opportunities to engage with experts and showcase their work. <br/><br/><br/>The conference contributes to statistical science by facilitating dialogues among experts, inspiring new research directions, and promoting advancements in statistical methodologies and AI applications. In particular, discussions will focus on scalable data analysis, model interpretability, causal inference, and responsible AI. Key topics include uncertainty quantification through probabilistic modeling, Bayesian methods for AI-driven decision-making, and fairness and bias correction in machine learning. The program will also address reinforcement learning for dynamic treatment regimes and privacy-preserving statistical methods for large-scale datasets. The conference will serve as a hub for knowledge sharing, highlighting the evolving role of AI in statistical research and its broader implications for innovation and societal progress. More details about the event, including registration and program updates, can be found at https://statistics.columbian.gwu.edu/GW-STAT-90.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2527500","Conference: International Indian Statistical Association Annual Conference 2025","DMS","STATISTICS","06/01/2025","05/21/2025","Souparno Ghosh","NE","University of Nebraska-Lincoln","Standard Grant","John Kolassa","11/30/2025","$20,000.00","","sghosh5@unl.edu","2200 VINE ST # 830861","LINCOLN","NE","685032427","4024723171","MPS","126900","7556, 9150","$0.00","This project supports the International Indian Statistical Association (IISA) conference, 2025 - a four-day international conference that will take place at University of Nebraska-Lincoln, Lincoln, Nebraska from June 12 to June 15, 2025. This conference is the official annual meeting of the International Indian Statistical Association (IISA). It provides a platform to exchange ideas and showcase theoretical, methodological, and application of Statistics and Data Science across many scientific domains. The conference features plenary sessions, invited talks, panel discussions and workshops that will provide attendees with invaluable insight into real-world applications of Statistics and Data Science. It will promote education, research, and foster exchange of information and scholarly activities thereby facilitating the emergence of new ideas that will guide future research directions. The conference will also host student paper and poster competitions which will highlight the best research among emerging scholars.<br/><br/>IISA 2025 spans topics from statistical theory to advanced computational methods and application-focused modeling of complex data. It will bring together leading experts and emerging scholars in statistics, biostatistics, probability, data science and offer a forum to discuss recent progress in statistical theory and data science. The conference will provide a valuable opportunity to nurture emerging talents and foster collaboration. It has two hands-on workshops, multiple panel discussions, student paper and poster competitions that will make the experience engaging and impactful, leaving attendees better equipped to thrive in their research journeys. Plenary talks and special invited talks will be given by leading experts in Probability, Statistics, Machine Learning, and Data Science. Junior researchers will have the opportunity to present in invited sessions. Doctoral students will also have the opportunity to give oral presentations on a competitive basis. The official website of IISA 2025 is https://www.intindstat.org/conference2025/index.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2534466","Wasserstein guided nonparametric Bayes","DMS","STATISTICS","05/15/2025","05/21/2025","Debdeep Pati","WI","University of Wisconsin-Madison","Standard Grant","Tapabrata Maiti","06/30/2027","$299,669.00","","dpati2@wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","126900","1269","$0.00","Stochastic generative models are a cornerstone of applied statistical modeling and inference. A generative model is an abstraction, and often a simplification, of a data generating mechanism using probabilistic tools, where specific features of interest regarding the generating mechanism are encapsulated into parameters of the generative model. Bayesian statistical inference is a popular statistical paradigm for combining such generative models for data with prior information about model parameters in a principled fashion to perform statistical inference on the unknown parameters. Some of the salient aspects behind the tremendous growth in popularity of Bayesian inference  include principled incorporation of domain information, an in-built penalty for model complexity allowing automatic model selection, and facilitating borrowing of information across different domains via hierarchical modeling. However, being inherently model-based, Bayesian statistics is intrinsically susceptible to departures from the postulated generative model.  Through this project, the investigators will explore and develop new statistical methodology for performing Bayesian inference allowing flexible departures from the generative model under consideration. A major focus will be the user-friendliness of the proposed approaches, circumventing the need for a user to explicitly build probabilistic models of increasing richness. The research will be disseminated through articles at prominent avenues and research presentations. Additionally, software packages for the methods developed will be made available publicly. The investigators are committed to enhancing the pedagogical component of the proposal through advising students and developing graduate and undergraduate topic courses.<br/><br/>Flexible nonparametric Bayesian methods have gained in popularity to address perceived issues of traditional Bayesian modeling regarding model-misspecification. The last thirty years have seen a proliferation of such methods, both in mainstream statistics as well as the machine learning community, as we continue to encounter increasing levels of complexities in modern datasets. However, nonparametric Bayesian methods can be challenging to implement as well as interpret. Furthermore, in many applications, the targets of interest are quite simple and it is essentially futile to model all aspects of the data. The fundamental aim of the proposed research is to develop a flexible Bayesian non-parametric approach that retains the generative modeling aspect of traditional parametric Bayesian modeling while avoiding a complete probabilistic specification of the data generating mechanism as typically performed in nonparametric Bayesian modeling. This will be performed by defining a modified likelihood function, leveraging ideas from the empirical likelihood literature as well as optimal transport theory, that centers around a user-specified parametric family of densities. An automated calibration procedure will be developed to control the extent of centering around the parametric model. The investigators will offer a firm theoretical underpinning of the proposed procedure and develop computationally efficient algorithms to carry out inferential tasks. The developed methods will be applied to scientific learning problems in neuroscience and nuclear physics to allow departures from existing scientific models in situations where their operating characteristics are less understood.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2506882","Conference: Enriching Statistical Inference with Artificial Intelligence","DMS","STATISTICS","03/01/2025","02/18/2025","Faming Liang","IN","Purdue University","Standard Grant","John Kolassa","02/28/2026","$22,500.00","Chuanhai Liu, Xiao Wang","fmliang@purdue.edu","2550 NORTHWESTERN AVE # 1100","WEST LAFAYETTE","IN","479061332","7654941055","MPS","126900","7556","$0.00","The conference ""Enriching Statistical Inference with Artificial Intelligence"" will be held at Purdue University May 12-14, 2025.  During the past decade, deep learning has revolutionized data science, with transformative applications in fields such as computer vision, protein structure prediction, and natural language processing. These advancements underscore the immense potential of deep neural networks (DNNs) while revealing a gap in the statistical understanding of their mechanisms and successes. This conference seeks to address the gap by fostering a vibrant platform for exchanging ideas, advancing statistical theories to illuminate DNN performance, developing innovative artificial intelligence (AI) tools, and promoting interdisciplinary collaborations that harness the power of AI to solve real-world problems. This conference will significantly enrich statistical inference with AI, while also contributing to the evolution of AI by improving its robustness, interpretability, and uncertainty quantification.  This improved inference will then impact society broadly by improving the experience of users interacting with the products of AI.  <br/><br/>This conference will delve into cutting-edge research at the intersection of AI and statistical inference. Key topics will include investigating fundamental phenomena in deep learning, such as benign overfitting, and developing robust methods for uncertainty quantification in DNN models. The conference will also emphasize practical applications, leveraging DNNs to address foundational scientific challenges like causal inference and variable selection in complex systems. Participants will acquire state-of-the-art AI tools to tackle the complexities of contemporary data science while contributing to the advancement of modern statistical theory.  More information about this conference may be found at https://www.stat.purdue.edu/news/2024/bff9.html.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515675","Conference: SDSU Data Science Symposium","DMS","STATISTICS","02/15/2025","02/11/2025","Frederick Boehm","SD","South Dakota State University","Standard Grant","John Kolassa","07/31/2025","$7,502.00","Semhar Michael","Frederick.Boehm@sdstate.edu","940 ADMINISTRATION LN","BROOKINGS","SD","570070001","6056886696","MPS","126900","7556, 9150","$0.00","The Department of Mathematics and Statistics at South Dakota State University will host its seventh annual Data Science Symposium (February 6 & 7, 2025) in Brookings, South Dakota. This regional conference will assemble practitioners and researchers from many branches of data science, including statistics, mathematics, medicine and public health, precision agriculture, finance, forensic science, and others to exchange ideas, explore employment opportunities, and build collaborations. National Science Foundation funding will enable participation by students and early career attendees. The symposium is a highlight among conferences in the upper Midwest.<br/><br/>Parallel session tracks will feature speakers from diverse fields such as statistics, mathematics, computer science, healthcare, finance, forensics, precision agriculture, and other data science-related topics. The symposium facilitates networking, collaborations, and exposes students to various career paths within mathematics, statistics, computer science, and other STEM areas. The symposium aims to 1) bring unique opportunities to the Midwest region, where students, faculty, business leaders of the region, and practitioners all gather to discuss the applications and foundations of data science; 2) provide hands-on, four-hour-long instructions on emerging topics/tools used in data science; 3) host presentations covering foundational and use-case aspects of data science and garnering interactive discussions and future collaborations; 4) expand networks during the career fair and exhibit sessions, connecting faculty, students, and hiring managers of companies in the area. The three keynote presentations feature experts in data science in industry and academia and highlight the roles of artificial intelligence in modern data science. The conference web site is https://openprairie.sdstate.edu/datascience_symposium/2025/ .<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2516872","Minipatch Learning for Selection, Stability, Inference, and Scalability","DMS","STATISTICS","02/01/2025","02/05/2025","Genevera Allen","NY","Columbia University","Standard Grant","Yong Zeng","07/31/2026","$195,479.00","","genevera.allen@columbia.edu","615 W 131ST ST","NEW YORK","NY","100277922","2128546851","MPS","126900","068Z, 079Z, 8091","$0.00","Massive amounts of data are now collected by nearly every industry and academic discipline. Uncovering the hidden insights in such data holds the key to major scientific challenges such as understanding how the brain works, discovering mechanisms leading to diseases such as cancer and Alzheimer's disease, and combating climate change, among many others. But discovering key features and important relationships in complex and huge data poses major statistical and computational challenges. The investigator aims to develop new statistical machine learning approaches and theory for this task that break up huge data sets into small random subsets called minipatches to facilitate both faster computation and improved statistical efficiency. The new methods will be implemented in open-source software and applied to huge biomedical datasets in genomics and neuroscience. The project will provide undergraduate and graduate students training and professional development opportunities.<br/><br/>Discovering key features and important relationships in complex and huge data commonly found in biomedicine poses not only major computational challenges but also critical statistical challenges. To tackle these challenges, the investigator plans to develop a new framework termed minipatch learning. Inspired by the successes of random forests, stability approaches in high-dimensional statistics, and stochastic optimization strategies, the investigator will build ensembles from many random tiny subsets of both observations and features or variables called minipatches. While ensemble learning strategies are commonly used in supervised machine learning, the investigator will use minipatch learning for the tasks of feature selection, model-agnostic inference for feature importance, and learning relationships amongst features through graphical models. The approach, which trains on very tiny subsets of the data, is expected to have dramatic computational and memory savings. The investigator aims to show both theoretically and empirically that such a strategy poses significant statistical advantages as well.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2523484","Collaborative Research: Planning: FIRE-PLAN: Advancing Wildland Fire Analytics for Actuarial Applications and Beyond","DMS","STATISTICS, Human-Envi & Geographical Scis, HDBE-Humans, Disasters, and th, Cross-BIO Activities, Info Integration & Informatics","02/01/2025","02/13/2025","Yuzhou Chen","CA","University of California-Riverside","Standard Grant","Jun Zhu","09/30/2025","$102,687.00","","yuzhou.chen@ucr.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","MPS","126900, 141Y00, 163800, 727500, 736400","132Z","$0.00","The impacts of uncontrolled wildland fires range from the destruction of native vegetation to property damages to long-term health effects and losses of human lives. Increasing accuracy in projections of wildland fire activity, fire behavior, and wildland fire weather is the key toward developing more efficient fire control strategies and reducing the risks of wildfires. Recent studies have demonstrated that the tools of artificial intelligence (AI) can help in planning for upcoming prescribed burns by providing higher spatial and temporal fire weather forecasts and can also assist in developing more efficient strategies for wildfire risk mitigation. However, the modeling tools that are currently used to predict fire activity are largely subject to a number of temporal or spatial constraints. For instance, most deep learning (DL) approaches for wildfire risk analytics tend to be restricted in their capabilities to systematically capture the multidimensional information recorded at disparate spatio-temporal resolutions. Furthermore, such DL architectures are inherently static and do not explicitly account for complex dynamic phenomena, which is often the key behind the accurate assessment of wildfire driving factors. Finally, these models primarily rely on supervised learning approaches where a large number of task-specific labels (e.g., fire or no fire) are needed. To address these challenges in wildfire risk analytics, this project will leverage inherently interdisciplinary approaches at the interface of Earth system sciences, DL, computational topology, statistics, and actuarial sciences. <br/><br/>The project aims to introduce the concepts of topological data analysis (TDA) to wildfire predictive modeling, coupling them with such emerging AI machinery as time-aware graph neural networks. The resulting new methods are expected to better capture the shape patterns in the wildland fire processes with respect both to time and space and to assist in a more reliable statistical assessment of wildfire risks. The new high-fidelity predictive approaches will have the potential to deliver forecasts of fire behavior, fire activity, and fire weather at multiple spatial and temporal scales under scenarios of limited, noisy, or nonexistent labeled information. To enhance the utility of the research solutions in wildfire analytics, the researchers in this project will work in close collaboration with stakeholders, particularly, focusing on the insurance sector. The project will provide multiple interdisciplinary training opportunities at the nexus of wildfire sciences, AI, and mathematical sciences at all educational levels, from undergraduate students to practicing actuaries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2500874","Conference: Workshop on Experimental Designs in the Age of Artificial Intelligence","DMS","STATISTICS","01/15/2025","01/03/2025","Jingshen Wang","CA","University of California-Berkeley","Standard Grant","John Kolassa","12/31/2025","$20,610.00","","jingshenwang@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","126900","7556","$0.00","The workshop ""Advancements in Experimental Design in the Era of AI"" will be held from March 7-9, 2025, at the UC Berkeley campus Alumni House. This workshop aims to unite experts from various disciplines to develop and discuss recent advancements in experimental designs that combine classical approaches with cutting-edge artificial intelligence (AI)-driven techniques. In today's data-driven world, understanding cause-and-effect relationships is essential across fields such as healthcare, social policy, and industry, and randomized experiments serve as the gold standard for establishing causal relationships by systematically testing the effectiveness of treatments, interventions, or policies. Well-designed experiments not only yield reliable insights but also reduce costs, accelerate outcomes, and enhance public benefits. However, designing experiments that meet the complex needs of different fields, each with unique challenges and data requirements, is a significant task. While traditional experimental designs have been successful for decades, recent advancements in data collection and AI potentially offer new opportunities to enhance experimental efficiency and insights. This workshop aims to foster collaboration among researchers across different fields to create new experimental design strategies suitable for today's complex data environments.<br/><br/>The workshop aims to address the need for a unified approach to experimental design by bridging classical design of experiments (DoE) and modern adaptive methodologies, including reinforcement learning and AI-assisted designs. Classical DoE has been foundational in manufacturing, engineering, and quality control, emphasizing optimized balance and limited sample sizes. However, recent applications in clinical trials and digital platforms may require more adaptive approaches that dynamically adjust based on accruing data. These modern adaptive strategies ? such as response-adaptive randomization, enrichment designs, micro-randomization, and multi-arm bandits ? offer enhanced statistical efficiency and personalization but necessitate tailored statistical frameworks and causal inference methods. Despite their potential, the application of modern designs has been hindered by limited cross-disciplinary dialogue and implementation guidance. This workshop will convene experts in statistical design, biostatistics, econometrics, political science, and industry to foster interdisciplinary innovation in experimental methodologies. Objectives include fostering knowledge exchange across fields, advancing the integration of adaptive and classical designs, and applying AI tools to optimize experimental processes. By addressing practical challenges and promoting collaboration, the workshop aims to advance experimental design theory and practice, leveraging AI to tackle the complex data landscapes of modern research and industry applications.<br/><br/>For more information, please visit the workshop website at: https://www.design-ai.site/Berkeley-2025/ .<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2526477","Collaborative Research: Statistical Inference for Multivariate and Functional Time Series via Sample Splitting","DMS","STATISTICS","01/01/2025","04/02/2025","Xiaofeng Shao","MO","Washington University","Standard Grant","John Kolassa","06/30/2026","$146,738.00","","shaox@wustl.edu","1 BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","126900","1269","$0.00","Multivariate and functional time series are prevalent and routinely collected in many fields. Statistical inference of such time series is a fundamental problem in modern time series analysis and has broad applications in many scientific areas, including bioinformatics, business, climate science, economics, finance, genetics, and signal processing. Compared with existing methodologies, this research project will provide nonparametric inference procedures that can accommodate a wide range of dimensionality and require weak assumptions on the data generating processes. The methodology ensuing from the project will be disseminated to the relevant scientific communities via publications, conference and seminar presentations, and the development of open-source software. The project will involve multiple research mentoring initiatives, including efforts on broadening participation, and will offer advanced topic courses to introduce the state-of-the-art techniques in time series analysis. The project will provide a broad range of interdisciplinary training opportunities at all educational levels and will contribute to the future workforce professional development.<br/><br/>The project will develop a systematic body of methods and theory on inference for both multivariate (including high-dimensional) time series and functional time series based on sample splitting (SS) and self-normalization (SN). Recently, the SN technique has been advanced to the inference of high-dimensional time series, but it requires the use of a trimming parameter. Also, its scope of applicability is limited to high-dimensional time series with weak panel dependence which might be unrealistic in many modern time series applications. In turn, the existing SN for functional time series relies on dimension reduction by functional principal component analysis and, hence, the resulting procedure may be powerless when the alternative is orthogonal to the space spanned by the top principal components used in the procedure. To address these major limitations, this project will develop a new unified framework based on SS-SN, in conjunction with inference for multivariate and functional time series, and investigate its utility in application to analysis of time series of low, medium, high or infinite dimensions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2524356","Collaborative Research: Statistical Modeling and Inference for Object-valued Time Series","DMS","STATISTICS","01/01/2025","02/18/2025","Xiaofeng Shao","MO","Washington University","Standard Grant","Jun Zhu","06/30/2027","$174,344.00","","shaox@wustl.edu","1 BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","126900","","$0.00","Random objects in general metric spaces have become increasingly common in many fields. For example, the intraday return path of a financial asset, the age-at-death distributions, the annual composition of energy sources, social networks, phylogenetic trees, and EEG scans or MRI fiber tracts of patients can all be viewed as random objects in certain metric spaces. For many endeavors in this area, the data being analyzed is collected with a natural ordering, i.e., the data can be viewed as an object-valued time series. Despite its prevalence in many applied problems, statistical analysis for such time series is still in its early development. A fundamental difficulty of developing statistical techniques is that the spaces where these objects live are nonlinear and commonly used algebraic operations are not applicable. This research project aims to develop new models, methodology and theory for the analysis of object-valued time series. Research results from the project will be disseminated to the relevant scientific communities via publications, conference and seminar presentations. The investigators will jointly mentor a Ph.D. student and involve undergraduate students in the research, as well as offering advanced topic courses to introduce the state-of-the-art techniques in object-valued time series analysis.<br/><br/>The project will develop a systematic body of methods and theory on modeling and inference for object-valued time series. Specifically, the investigators propose to (1) develop a new autoregressive model for distributional time series in Wasserstein geometry and a suite of tools for model estimation, selection and diagnostic checking; (2) develop new specification testing procedures for distributional time series in the one-dimensional Euclidean space; and (3) develop new change-point detection methods to detect distribution shifts in a sequence of object-valued time series. The above three projects tackle several important modeling and inference issues in the analysis of object-valued time series, the investigation of which will lead to innovative methodological and theoretical developments, and lay groundwork for this emerging field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2436216","Collaborative Research: FDT-BioTech: Advancing Mathematical and Statistical Foundations to Enhance Human Digital Twin of Neurophysiological Modeling and Uncertainty Quantification","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, Engineering of Biomed Systems, CYBERINFRASTRUCTURE","01/01/2025","08/20/2024","Huixia Wang","DC","George Washington University","Standard Grant","Zhilan Feng","12/31/2027","$549,341.00","Chung Hyuk Park","judywang@gwu.edu","1918 F ST NW","WASHINGTON","DC","200520042","2029940728","MPS","125300, 126900, 534500, 723100","075Z, 079Z, 1269, 8038","$0.00","This project aims to develop the mathematical foundations for a digital twin (DT) system for individuals with autism spectrum disorder (ASD), focusing on dynamic modeling, prediction, uncertainty quantification, and treatment or intervention recommendation through DT-based optimization. ASD is characterized by challenges in social interaction, communication, and behavior, such as difficulties in forming relationships, understanding nonverbal cues, speech development, repetitive behaviors, and sensory sensitivities. The project will create a unified system integrating clinical and neuro-developmental data, analyzed using a DT healthcare paradigm. The DT technology will enable individualized models, and its predictive capabilities will allow healthcare providers to anticipate progression and adjust treatment or intervention proactively. Additionally, the continuous feedback loop from real-time data will enhance therapeutic outcomes. The developed methods and theories will have broader applicability to other medical areas, improving healthcare efficiency, reducing system burdens, and informing public health strategies. This will ultimately enhance care and promote community well-being. The project will also develop quality cyberinfrastructure to share algorithms, data, and open-source software with the community. Furthermore, the investigators plan to expand scientific impacts through collaborating with medical experts and industry scientists, training undergraduate and graduate students, and integrating research findings into course development.<br/><br/>The project will develop a DT framework by modeling brain activities with a unified data structure, linked to behavioral characteristics and interventions aligned with individuals' neuro-developmental processes. This system will integrate multimodal and multi-source data related to human health and development. It will establish foundational models for training and generating synthetic data from DT models, enabling personalized predictions of progression and uncertainty quantification through novel interdisciplinary approaches. The DT system consists of four research modules: (1) Develop computational models based on conditional variational auto-encoders (CVAE) and longitudinal CVAE to analyze brain activities, integrate diverse imaging data, and model neurodevelopmental processes. (2) Create a novel bilevel formulation for multi-distribution fine-tuning techniques on pretrained foundational models and a fast algorithm to learn from heterogeneous data sources to predict ASD outcomes. (3) Develop a model-free conformal prediction procedure to ensemble predictions from multiple models obtained with different modalities and progression simulations, integrating various types of uncertainties into one framework. (4) Develop a DT-based reinforcement learning framework to recommend personalized treatment/intervention plans that significantly improve online learning efficiency and clinical outcomes. The project will address challenges such as multimodality and multi-source data, high-dimensional features, dynamic progression of ASD symptoms, brain functional connectivity, and the need for personalized intervention or treatment recommendations and uncertainty quantification.<br/><br/>This project is jointly funded by the Division of Mathematical Sciences, the OAC Cyberinfrastructure for Sustained Scientific Innovation (CSSI) program, and the CBET Engineering of Biomedical Systems program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."

"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"2443867","CAREER: Robust Learning from Preference Feedback","DMS","STATISTICS","10/01/2025","01/21/2025","Cong Ma","IL","University of Chicago","Continuing Grant","Jun Zhu","09/30/2030","$85,950.00","","congma2015@gmail.com","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","126900","079Z, 1045","$0.00","In recent years, preference feedback?comparative inputs such as ?A is better than B??has emerged as a vital resource for guiding decision-making systems. Unlike explicit labels, preference feedback is often easier to collect and can be particularly valuable in subjective tasks where defining ideal outcomes is difficult. However, real-world preference data are often noisy, sparse, and heterogeneous, posing significant challenges to existing statistical methods. For example, recommendation systems may encounter incomplete feedback from users who abandon tasks due to fatigue or provide inconsistent inputs due to individual biases. This project aims to address the challenges of learning from preference feedback by developing robust statistical methods and advancing the theoretical foundations of preference-based learning. Additionally, it seeks to prepare students to tackle these challenges by integrating the research findings into innovative teaching platforms and educational curricula.<br/><br/>The project will focus on three key areas of learning from preference feedback: ranking from pairwise comparisons, user-item rating systems, and reinforcement learning from human feedback. To advance the field, the project will (1) develop robust algorithms for ranking that account for ill-conditioned sampling mechanisms and relax parametric modeling assumptions; (2) propose new estimation and uncertainty quantification methods for user-item ratings that work effectively in sparse and heterogeneous settings; and (3) introduce novel frameworks for reinforcement learning that incorporate ?out-of-list? preference feedback while addressing the issue of distribution shifts. Through these contributions, the project will bridge the gap between statistical theory and practical applications, creating tools to enhance decision-making systems across diverse domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2440180","CAREER: Statistical Inference in the Presence of Group Actions: Theory, Method, and Application","DMS","STATISTICS","07/01/2025","02/10/2025","Ye Zhang","PA","University of Pennsylvania","Continuing Grant","John Kolassa","06/30/2030","$90,000.00","","ayz@wharton.upenn.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","126900","1045","$0.00","In the rapidly expanding field of data science, the ability to understand group actions in data analysis is pivotal for a broad spectrum of scientific tasks. In mathematical terms, a ?group? is a collection of elements combined with an operation that links any two elements to form a third, adhering to closure, associativity, identity, and invertibility principles. A ?group action? involves applying elements of a group to another set?s elements, transforming them in structured ways, such as through rotations or reflections. These transformations are crucial in many data processing applications, including cryo-electron microscopy (cryo-EM), image registration, and multi-reference alignment. Each observation in these problems involves a common, unknown signal and an unknown group element, with the primary goal being to infer both the signal and the group elements accurately. This project aims to significantly advance statistical understanding and develop effective methodologies for handling data influenced by group actions. The wide existence of such data ensures that the progress we make towards our objectives will have a great impact not only on the statistics and machine learning community but also on a much broader scientific community, including fields such as structural biology, computer vision, and signal processing. This project will have educational outcomes that result in curriculum development, teaching, and outreach activities, including activities to K-12 students through the University of Pennsylvania Data Science Academy. The project will advance applications in image recognition and time series alignment, which have broad application in areas like medical imaging.<br/><br/>This project is structured around three main aims, each designed to tackle distinct aspects of group actions. First, the PI will improve the accuracy of orbit recovery in scenarios where the prior distributions of group elements are non-uniform, developing computationally efficient procedures that are effective under realistic conditions. Second, the PI will develop theories and methods for group synchronization problems, particularly under high noise levels and in situations with incomplete data, aiming to reduce the error of group recovery and provide entrywise inference. Third, the PI will address theoretical and computational challenges in the multi-reference alignment problem, developing procedures specifically designed for the cyclic structural nature of data, thereby enabling more precise uncertainty quantification. Together, these aims will not only enhance the theoretical understanding of and the ability to analyze group actions but also lead to the development of accurate and computationally efficient algorithms designed to tackle real-world challenges in data analysis where group actions are integral.  This research project will have impacts more broadly, in that it will result in software development and in the education of technical experts. These experts will use this software to advance applications in image recognition and time series alignment, which have broad application in areas like medical imaging.  These activities will then advance applications in image recognition and time series alignment, which have broad application in areas like medical imaging.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2440824","CAREER: High-Dimensional Learning and Inference from Heterogeneous Data Sources","DMS","STATISTICS","07/01/2025","02/10/2025","Pragya Sur","MA","Harvard University","Continuing Grant","Tapabrata Maiti","06/30/2030","$88,793.00","","pragya@fas.harvard.edu","1033 MASSACHUSETTS AVE STE 3","CAMBRIDGE","MA","021385366","6174955501","MPS","126900","1045","$0.00","This project will develop novel statistical theories and methods for handling large, heterogeneous datasets. Modern scientific applications often produce heterogeneous data of different types for the same problem. For instance, a single-cell biologist may observe multiple types of sequencing data from diverse instruments, all relevant for understanding the biological pathways of a single complex disease.  The challenge lies in effectively combining these different data types to build statistical pipelines that outperform those developed using any one data type. Traditional statistical approaches struggle with this challenge. This project will establish a new statistical paradigm to address the complexities of such heterogeneous data while accounting for datasets with billions of variables. The project outcomes will facilitate principled prediction and inference in applications ranging from single-cell biology to precision health and neuroimaging. The project will involve graduate student participation and the development of new curricula at graduate and undergraduate levels that incorporate the project outcomes. Additionally, the research will engage medical professionals to facilitate the dissemination of the research products in current biomedical practice.<br/><br/><br/>This project will develop a modern statistical framework to address data heterogeneity in high dimensions, focusing on three key sub-themes: (i) creating principled and robust prediction strategies for multi-view learning, (ii) developing new inference pipelines and prediction analysis frameworks for meta-learning, and (iii) introducing novel inference methods for low-dimensional functionals under transfer learning. In multi-view learning, this project will quantify optimal strategies for cooperative learning, devise new adversarial learning techniques, and analyze the effects of interpolation learning.  In meta-learning, this project will introduce new debiasing strategies to tackle inference questions that arise during fine-tuning following an initial phase of pre-training.  In transfer learning, this project will develop general-purpose strategies for ranking source distributions and establish new inference schemes for low-dimensional functionals of scientific relevance. On the technical front, this project will introduce novel comparison inequalities, algorithmic proof methods, and leave-one-out techniques that effectively capture the interplay between high dimensionality and heterogeneity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2441652","CAREER: Distributional Approximation for Sharp Finite Sample Bounds with Applications to Dependent Data and Complex Estimators","DMS","STATISTICS","07/01/2025","02/04/2025","Morgane Austern","MA","Harvard University","Continuing Grant","Yong Zeng","06/30/2030","$82,562.00","","maustern@fas.harvard.edu","1033 MASSACHUSETTS AVE STE 3","CAMBRIDGE","MA","021385366","6174955501","MPS","126900","1045, 079Z","$0.00","AI and machine learning algorithms are transforming numerous scientific fields, with some of the most promising approaches relying on mathematical tools called ""finite sample probability bounds."" These bounds are crucial, for example, in reinforcement learning, which underpins the success of systems like AlphaGo. Additionally, they play a key role in uncertainty quantification and the theoretical analysis of black-box machine learning algorithms. However, classical finite sample bounds have a significant limitation: they are often overly conservative. This conservatism leads to underperforming algorithms and unnecessarily loose guarantees. This project is built around a novel yet straightforward idea: finite-sample bounds can be derived from infinite-sample results. By leveraging recent breakthroughs in optimal transport and probability theory, the project aims to develop a new method for deriving such inequalities. These methods will be applied to problems such as online data-driven decision-making, early stopping rules, and machine learning for multiscale physical models. The PI will interweave their research and teaching throughout the research period and beyond. In particular, the PI will provide research training opportunities to graduate students and develop undergraduate and graduate courses, with course materials made publicly available and with joint participation from industry.<br/><br/>Classical concentration inequalities, such as the ones derived by Hoeffding or Bernstein, are over-conservative, are regularly inadequate for heavy-tail distributions, and often rely on the assumption of independence. In this project, the PI tackles those limitations from a new angle, starting with an infinite-sample result for a given problem, such as a central limit theorem, and translating it into a finite-sample result for the same problem by using the concept of distributional approximations. The advantage of this novel proof method is threefold: Firstly, the derived bounds improve as the sample size grows. This leads to inequalities that are considerably tighter than the classical ones. Secondly, limit theorems often hold for many forms of dependent data. This opens a more promising path to derive finite sample probability bounds than conventional Chernoff-based techniques. Lastly, by extending this approach to non-Gaussian limits, the PI develops finite sample concentration inequalities for heavy-tailed statistics. This project will (1) develop a completely novel method for obtaining concentration inequalities, as well as new results for transport distances and optimal transport, (2) provide machine learning theorists with a new set of powerful probability tools for obtaining high-probability guarantees for high-dimensional estimators and dependent and structured data, and (3) provide tighter tail bounds which will lead to algorithmic improvements and improved uncertainty quantification.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2443410","CAREER: Extending the reach of empirical Bayes: Calibration, nuisance parameters, likelihood asymptotics, and machine learning","DMS","STATISTICS","06/01/2025","03/10/2025","Nikolaos Ignatiadis","IL","University of Chicago","Continuing Grant","Tapabrata Maiti","05/31/2030","$75,343.00","","ignat@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","126900","1045","$0.00","Scientists across various fields face the challenge of answering numerous related questions with limited or noisy data. For example, genomicists may need to assess thousands of genes using data from only a few subjects, while survey statisticians might analyze average incomes in many towns based on limited surveys. To address these challenges, researchers often borrow information from related questions, a process that requires sophisticated statistical reasoning due to varying underlying characteristics. Empirical Bayes offers a method to enhance individual question inference by sharing information, potentially improving statistical accuracy. However, it relies on strong modeling assumptions, limiting its applicability. This research aims to make empirical Bayes more powerful and accessible by integrating it with modern machine learning and causal inference, demonstrating its effectiveness under fewer assumptions, and developing methods to assess uncertainty more effectively. These advancements will help practitioners utilize empirical Bayes in various scientific and industry contexts without extensive statistical modeling. Additionally, the project will produce a monograph and provide training for students and preceptors on modern data science challenges using empirical Bayes.<br/> <br/> <br/>This research aims to advance empirical Bayes by developing new methodologies and statistical theories to address four key limitations: the assumption of known likelihoods, the treatment of nuisance parameter heterogeneity, the development of nonparametric inference methods, and the integration with machine learning. The project will create new inference methods for primary parameters by using empirical partially Bayes methods and Bayesian nonparametrics, enhancing frequentist Bayes multiple testing theory with guarantees like false discovery rate control. Additionally, the research seeks to learn the unknown Bayes rule through neural networks with a specialized loss function, incorporate James-Stein shrinkage for combining unbiased estimates with semisupervised machine learning, and integrate empirical partially Bayes inference with doubly robust double machine learning for large-scale causal inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2506882","Conference: Enriching Statistical Inference with Artificial Intelligence","DMS","STATISTICS","03/01/2025","02/18/2025","Faming Liang","IN","Purdue University","Standard Grant","John Kolassa","02/28/2026","$22,500.00","Chuanhai Liu, Xiao Wang","fmliang@purdue.edu","2550 NORTHWESTERN AVE # 1100","WEST LAFAYETTE","IN","479061332","7654941055","MPS","126900","7556","$0.00","The conference ""Enriching Statistical Inference with Artificial Intelligence"" will be held at Purdue University May 12-14, 2025.  During the past decade, deep learning has revolutionized data science, with transformative applications in fields such as computer vision, protein structure prediction, and natural language processing. These advancements underscore the immense potential of deep neural networks (DNNs) while revealing a gap in the statistical understanding of their mechanisms and successes. This conference seeks to address the gap by fostering a vibrant platform for exchanging ideas, advancing statistical theories to illuminate DNN performance, developing innovative artificial intelligence (AI) tools, and promoting interdisciplinary collaborations that harness the power of AI to solve real-world problems. This conference will significantly enrich statistical inference with AI, while also contributing to the evolution of AI by improving its robustness, interpretability, and uncertainty quantification.  This improved inference will then impact society broadly by improving the experience of users interacting with the products of AI.  <br/><br/>This conference will delve into cutting-edge research at the intersection of AI and statistical inference. Key topics will include investigating fundamental phenomena in deep learning, such as benign overfitting, and developing robust methods for uncertainty quantification in DNN models. The conference will also emphasize practical applications, leveraging DNNs to address foundational scientific challenges like causal inference and variable selection in complex systems. Participants will acquire state-of-the-art AI tools to tackle the complexities of contemporary data science while contributing to the advancement of modern statistical theory.  More information about this conference may be found at https://www.stat.purdue.edu/news/2024/bff9.html.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515675","Conference: SDSU Data Science Symposium","DMS","STATISTICS","02/15/2025","02/11/2025","Frederick Boehm","SD","South Dakota State University","Standard Grant","John Kolassa","07/31/2025","$7,502.00","Semhar Michael","Frederick.Boehm@sdstate.edu","940 ADMINISTRATION LN","BROOKINGS","SD","570070001","6056886696","MPS","126900","9150, 7556","$0.00","The Department of Mathematics and Statistics at South Dakota State University will host its seventh annual Data Science Symposium (February 6 & 7, 2025) in Brookings, South Dakota. This regional conference will assemble practitioners and researchers from many branches of data science, including statistics, mathematics, medicine and public health, precision agriculture, finance, forensic science, and others to exchange ideas, explore employment opportunities, and build collaborations. National Science Foundation funding will enable participation by students and early career attendees. The symposium is a highlight among conferences in the upper Midwest.<br/><br/>Parallel session tracks will feature speakers from diverse fields such as statistics, mathematics, computer science, healthcare, finance, forensics, precision agriculture, and other data science-related topics. The symposium facilitates networking, collaborations, and exposes students to various career paths within mathematics, statistics, computer science, and other STEM areas. The symposium aims to 1) bring unique opportunities to the Midwest region, where students, faculty, business leaders of the region, and practitioners all gather to discuss the applications and foundations of data science; 2) provide hands-on, four-hour-long instructions on emerging topics/tools used in data science; 3) host presentations covering foundational and use-case aspects of data science and garnering interactive discussions and future collaborations; 4) expand networks during the career fair and exhibit sessions, connecting faculty, students, and hiring managers of companies in the area. The three keynote presentations feature experts in data science in industry and academia and highlight the roles of artificial intelligence in modern data science. The conference web site is https://openprairie.sdstate.edu/datascience_symposium/2025/ .<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2523484","Collaborative Research: Planning: FIRE-PLAN: Advancing Wildland Fire Analytics for Actuarial Applications and Beyond","DMS","STATISTICS, Human-Envi & Geographical Scis, HDBE-Humans, Disasters, and th, Cross-BIO Activities, Info Integration & Informatics","02/01/2025","02/13/2025","Yuzhou Chen","CA","University of California-Riverside","Standard Grant","Jun Zhu","09/30/2025","$102,687.00","","yuzhou.chen@ucr.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","MPS","126900, 141Y00, 163800, 727500, 736400","132Z","$0.00","The impacts of uncontrolled wildland fires range from the destruction of native vegetation to property damages to long-term health effects and losses of human lives. Increasing accuracy in projections of wildland fire activity, fire behavior, and wildland fire weather is the key toward developing more efficient fire control strategies and reducing the risks of wildfires. Recent studies have demonstrated that the tools of artificial intelligence (AI) can help in planning for upcoming prescribed burns by providing higher spatial and temporal fire weather forecasts and can also assist in developing more efficient strategies for wildfire risk mitigation. However, the modeling tools that are currently used to predict fire activity are largely subject to a number of temporal or spatial constraints. For instance, most deep learning (DL) approaches for wildfire risk analytics tend to be restricted in their capabilities to systematically capture the multidimensional information recorded at disparate spatio-temporal resolutions. Furthermore, such DL architectures are inherently static and do not explicitly account for complex dynamic phenomena, which is often the key behind the accurate assessment of wildfire driving factors. Finally, these models primarily rely on supervised learning approaches where a large number of task-specific labels (e.g., fire or no fire) are needed. To address these challenges in wildfire risk analytics, this project will leverage inherently interdisciplinary approaches at the interface of Earth system sciences, DL, computational topology, statistics, and actuarial sciences. <br/><br/>The project aims to introduce the concepts of topological data analysis (TDA) to wildfire predictive modeling, coupling them with such emerging AI machinery as time-aware graph neural networks. The resulting new methods are expected to better capture the shape patterns in the wildland fire processes with respect both to time and space and to assist in a more reliable statistical assessment of wildfire risks. The new high-fidelity predictive approaches will have the potential to deliver forecasts of fire behavior, fire activity, and fire weather at multiple spatial and temporal scales under scenarios of limited, noisy, or nonexistent labeled information. To enhance the utility of the research solutions in wildfire analytics, the researchers in this project will work in close collaboration with stakeholders, particularly, focusing on the insurance sector. The project will provide multiple interdisciplinary training opportunities at the nexus of wildfire sciences, AI, and mathematical sciences at all educational levels, from undergraduate students to practicing actuaries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2516872","Minipatch Learning for Selection, Stability, Inference, and Scalability","DMS","STATISTICS","02/01/2025","02/05/2025","Genevera Allen","NY","Columbia University","Standard Grant","Yong Zeng","07/31/2025","$195,479.00","","genevera.allen@columbia.edu","615 W 131ST ST","NEW YORK","NY","100277922","2128546851","MPS","126900","079Z, 8091, 068Z","$0.00","Massive amounts of data are now collected by nearly every industry and academic discipline. Uncovering the hidden insights in such data holds the key to major scientific challenges such as understanding how the brain works, discovering mechanisms leading to diseases such as cancer and Alzheimer's disease, and combating climate change, among many others. But discovering key features and important relationships in complex and huge data poses major statistical and computational challenges. The investigator aims to develop new statistical machine learning approaches and theory for this task that break up huge data sets into small random subsets called minipatches to facilitate both faster computation and improved statistical efficiency. The new methods will be implemented in open-source software and applied to huge biomedical datasets in genomics and neuroscience. The project will provide undergraduate and graduate students training and professional development opportunities.<br/><br/>Discovering key features and important relationships in complex and huge data commonly found in biomedicine poses not only major computational challenges but also critical statistical challenges. To tackle these challenges, the investigator plans to develop a new framework termed minipatch learning. Inspired by the successes of random forests, stability approaches in high-dimensional statistics, and stochastic optimization strategies, the investigator will build ensembles from many random tiny subsets of both observations and features or variables called minipatches. While ensemble learning strategies are commonly used in supervised machine learning, the investigator will use minipatch learning for the tasks of feature selection, model-agnostic inference for feature importance, and learning relationships amongst features through graphical models. The approach, which trains on very tiny subsets of the data, is expected to have dramatic computational and memory savings. The investigator aims to show both theoretically and empirically that such a strategy poses significant statistical advantages as well.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2500874","Conference: Workshop on Experimental Designs in the Age of Artificial Intelligence","DMS","STATISTICS","01/15/2025","01/03/2025","Jingshen Wang","CA","University of California-Berkeley","Standard Grant","John Kolassa","12/31/2025","$20,610.00","","jingshenwang@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","126900","7556","$0.00","The workshop ""Advancements in Experimental Design in the Era of AI"" will be held from March 7-9, 2025, at the UC Berkeley campus Alumni House. This workshop aims to unite experts from various disciplines to develop and discuss recent advancements in experimental designs that combine classical approaches with cutting-edge artificial intelligence (AI)-driven techniques. In today's data-driven world, understanding cause-and-effect relationships is essential across fields such as healthcare, social policy, and industry, and randomized experiments serve as the gold standard for establishing causal relationships by systematically testing the effectiveness of treatments, interventions, or policies. Well-designed experiments not only yield reliable insights but also reduce costs, accelerate outcomes, and enhance public benefits. However, designing experiments that meet the complex needs of different fields, each with unique challenges and data requirements, is a significant task. While traditional experimental designs have been successful for decades, recent advancements in data collection and AI potentially offer new opportunities to enhance experimental efficiency and insights. This workshop aims to foster collaboration among researchers across different fields to create new experimental design strategies suitable for today's complex data environments.<br/><br/>The workshop aims to address the need for a unified approach to experimental design by bridging classical design of experiments (DoE) and modern adaptive methodologies, including reinforcement learning and AI-assisted designs. Classical DoE has been foundational in manufacturing, engineering, and quality control, emphasizing optimized balance and limited sample sizes. However, recent applications in clinical trials and digital platforms may require more adaptive approaches that dynamically adjust based on accruing data. These modern adaptive strategies ? such as response-adaptive randomization, enrichment designs, micro-randomization, and multi-arm bandits ? offer enhanced statistical efficiency and personalization but necessitate tailored statistical frameworks and causal inference methods. Despite their potential, the application of modern designs has been hindered by limited cross-disciplinary dialogue and implementation guidance. This workshop will convene experts in statistical design, biostatistics, econometrics, political science, and industry to foster interdisciplinary innovation in experimental methodologies. Objectives include fostering knowledge exchange across fields, advancing the integration of adaptive and classical designs, and applying AI tools to optimize experimental processes. By addressing practical challenges and promoting collaboration, the workshop aims to advance experimental design theory and practice, leveraging AI to tackle the complex data landscapes of modern research and industry applications.<br/><br/>For more information, please visit the workshop website at: https://www.design-ai.site/Berkeley-2025/ .<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2524356","Collaborative Research: Statistical Modeling and Inference for Object-valued Time Series","DMS","STATISTICS","01/01/2025","02/18/2025","Xiaofeng Shao","MO","Washington University","Standard Grant","Jun Zhu","06/30/2027","$174,344.00","","shaox@wustl.edu","1 BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","126900","","$0.00","Random objects in general metric spaces have become increasingly common in many fields. For example, the intraday return path of a financial asset, the age-at-death distributions, the annual composition of energy sources, social networks, phylogenetic trees, and EEG scans or MRI fiber tracts of patients can all be viewed as random objects in certain metric spaces. For many endeavors in this area, the data being analyzed is collected with a natural ordering, i.e., the data can be viewed as an object-valued time series. Despite its prevalence in many applied problems, statistical analysis for such time series is still in its early development. A fundamental difficulty of developing statistical techniques is that the spaces where these objects live are nonlinear and commonly used algebraic operations are not applicable. This research project aims to develop new models, methodology and theory for the analysis of object-valued time series. Research results from the project will be disseminated to the relevant scientific communities via publications, conference and seminar presentations. The investigators will jointly mentor a Ph.D. student and involve undergraduate students in the research, as well as offering advanced topic courses to introduce the state-of-the-art techniques in object-valued time series analysis.<br/><br/>The project will develop a systematic body of methods and theory on modeling and inference for object-valued time series. Specifically, the investigators propose to (1) develop a new autoregressive model for distributional time series in Wasserstein geometry and a suite of tools for model estimation, selection and diagnostic checking; (2) develop new specification testing procedures for distributional time series in the one-dimensional Euclidean space; and (3) develop new change-point detection methods to detect distribution shifts in a sequence of object-valued time series. The above three projects tackle several important modeling and inference issues in the analysis of object-valued time series, the investigation of which will lead to innovative methodological and theoretical developments, and lay groundwork for this emerging field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2436216","Collaborative Research: FDT-BioTech: Advancing Mathematical and Statistical Foundations to Enhance Human Digital Twin of Neurophysiological Modeling and Uncertainty Quantification","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, Engineering of Biomed Systems, CYBERINFRASTRUCTURE","01/01/2025","08/20/2024","Huixia Wang","DC","George Washington University","Standard Grant","Zhilan Feng","12/31/2027","$549,341.00","Chung Hyuk Park","judywang@gwu.edu","1918 F ST NW","WASHINGTON","DC","200520042","2029940728","MPS","125300, 126900, 534500, 723100","075Z, 079Z, 1269, 8038","$0.00","This project aims to develop the mathematical foundations for a digital twin (DT) system for individuals with autism spectrum disorder (ASD), focusing on dynamic modeling, prediction, uncertainty quantification, and treatment or intervention recommendation through DT-based optimization. ASD is characterized by challenges in social interaction, communication, and behavior, such as difficulties in forming relationships, understanding nonverbal cues, speech development, repetitive behaviors, and sensory sensitivities. The project will create a unified system integrating clinical and neuro-developmental data, analyzed using a DT healthcare paradigm. The DT technology will enable individualized models, and its predictive capabilities will allow healthcare providers to anticipate progression and adjust treatment or intervention proactively. Additionally, the continuous feedback loop from real-time data will enhance therapeutic outcomes. The developed methods and theories will have broader applicability to other medical areas, improving healthcare efficiency, reducing system burdens, and informing public health strategies. This will ultimately enhance care and promote community well-being. The project will also develop quality cyberinfrastructure to share algorithms, data, and open-source software with the community. Furthermore, the investigators plan to expand scientific impacts through collaborating with medical experts and industry scientists, training undergraduate and graduate students, and integrating research findings into course development.<br/><br/>The project will develop a DT framework by modeling brain activities with a unified data structure, linked to behavioral characteristics and interventions aligned with individuals' neuro-developmental processes. This system will integrate multimodal and multi-source data related to human health and development. It will establish foundational models for training and generating synthetic data from DT models, enabling personalized predictions of progression and uncertainty quantification through novel interdisciplinary approaches. The DT system consists of four research modules: (1) Develop computational models based on conditional variational auto-encoders (CVAE) and longitudinal CVAE to analyze brain activities, integrate diverse imaging data, and model neurodevelopmental processes. (2) Create a novel bilevel formulation for multi-distribution fine-tuning techniques on pretrained foundational models and a fast algorithm to learn from heterogeneous data sources to predict ASD outcomes. (3) Develop a model-free conformal prediction procedure to ensemble predictions from multiple models obtained with different modalities and progression simulations, integrating various types of uncertainties into one framework. (4) Develop a DT-based reinforcement learning framework to recommend personalized treatment/intervention plans that significantly improve online learning efficiency and clinical outcomes. The project will address challenges such as multimodality and multi-source data, high-dimensional features, dynamic progression of ASD symptoms, brain functional connectivity, and the need for personalized intervention or treatment recommendations and uncertainty quantification.<br/><br/>This project is jointly funded by the Division of Mathematical Sciences, the OAC Cyberinfrastructure for Sustained Scientific Innovation (CSSI) program, and the CBET Engineering of Biomedical Systems program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."

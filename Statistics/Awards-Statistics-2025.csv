"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"2537417","Conference: Best of Statistical Science Workshop (BOSS 2026)","DMS","STATISTICS","10/01/2025","08/14/2025","Brani Vidakovic","TX","Texas A&M University","Standard Grant","John Kolassa","09/30/2026","$20,000.00","","brani@tamu.edu","400 HARVEY MITCHELL PKY S STE 300","COLLEGE STATION","TX","778454375","9798626777","MPS","126900","075Z, 079Z, 7556","$0.00","This project supports the BOSS (Best of Statistical Science) Workshop, a focused gathering of early-career and established researchers in Bayesian statistics, to be held in Spring 2026 at Texas A&M University. The primary goal of the workshop is to provide a dynamic platform for graduate students and junior researchers to engage directly with leading experts in Bayesian methodology, applications, and computation. The workshop is designed to foster cross-disciplinary collaboration, promote mentorship, and introduce participants to the breadth of modern developments in statistical science. NSF support will enable the participation of 20 early-career researchers, including postdoctoral scholars, tenure-track faculty, and advanced graduate students from across the United States. By connecting emerging scholars with senior researchers in an inclusive, discussion-oriented setting, the event aims to broaden participation and strengthen the national research capacity in modern statistics.<br/><br/>The workshop will feature technical sessions on recent advances in Bayesian methodology, including modern computational approaches such as variational inference, shrinkage priors, and high-dimensional modeling; the use of AI in prior elicitation; and Bayesian machine learning. Applications will span the geosciences, biomedicine, and environmental sciences. Invited speakers will present cutting-edge research, while dedicated poster sessions and mentoring events will facilitate direct interactions between junior participants and senior statisticians. By showcasing a broad spectrum of theory, computation, and scientific applications, the project aims to stimulate methodological innovation and promote knowledge transfer across subfields of statistical science. Outreach efforts will include the dissemination of workshop materials and recordings to broaden participation, with particular attention to engaging institutions traditionally underrepresented in Bayesian research.  More information may be found at https://calendar.tamu.edu/statistics/event/358551-2026-best-of-statistical-science-workshop-boss2026 .<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2443867","CAREER: Robust Learning from Preference Feedback","DMS","STATISTICS","10/01/2025","01/21/2025","Cong Ma","IL","University of Chicago","Continuing Grant","Jun Zhu","09/30/2030","$85,950.00","","congma2015@gmail.com","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","126900","079Z, 1045","$0.00","In recent years, preference feedback?comparative inputs such as ?A is better than B??has emerged as a vital resource for guiding decision-making systems. Unlike explicit labels, preference feedback is often easier to collect and can be particularly valuable in subjective tasks where defining ideal outcomes is difficult. However, real-world preference data are often noisy, sparse, and heterogeneous, posing significant challenges to existing statistical methods. For example, recommendation systems may encounter incomplete feedback from users who abandon tasks due to fatigue or provide inconsistent inputs due to individual biases. This project aims to address the challenges of learning from preference feedback by developing robust statistical methods and advancing the theoretical foundations of preference-based learning. Additionally, it seeks to prepare students to tackle these challenges by integrating the research findings into innovative teaching platforms and educational curricula.<br/><br/>The project will focus on three key areas of learning from preference feedback: ranking from pairwise comparisons, user-item rating systems, and reinforcement learning from human feedback. To advance the field, the project will (1) develop robust algorithms for ranking that account for ill-conditioned sampling mechanisms and relax parametric modeling assumptions; (2) propose new estimation and uncertainty quantification methods for user-item ratings that work effectively in sparse and heterogeneous settings; and (3) introduce novel frameworks for reinforcement learning that incorporate ?out-of-list? preference feedback while addressing the issue of distribution shifts. Through these contributions, the project will bridge the gap between statistical theory and practical applications, creating tools to enhance decision-making systems across diverse domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515927","Collaborative Research: Online Statistical Inference for Modern Machine Learning","DMS","STATISTICS","10/01/2025","06/23/2025","Likai Chen","MO","Washington University","Standard Grant","Tapabrata Maiti","09/30/2028","$99,322.00","","likai.chen@wustl.edu","1 BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","126900","075Z, 079Z, 1269","$0.00","This research aims to develop statistical tools to improve the reliability of artificial intelligence (AI) that is widely used in real-world systems such as automated decision-making, financial forecasting, and neuroscience research. Modern AI often relies on efficient machine learning algorithms to process large-scale, sequentially arriving datasets. While these algorithms are powerful, understanding their behavior and measuring their uncertainty remains a major scientific challenge. To bridge this gap, the investigators will focus on establishing mathematically rigorous methods for uncertainty quantification to build trustworthy AI. Applications will include enhancing theoretical guarantees and interpretability of neural networks, providing robust estimation and inference for econometric and biomedical studies, and detecting real-time change-points in high-dimensional time series data. The projects will promote the progress of science through open-source software and graduate education, and will support the national interest by contributing to reliable, data-driven decision-making in fields important to economic resilience, public health and national security.<br/><br/>This research will provide a comprehensive theoretical framework for online statistical inference in machine learning, focusing on constant learning-rate stochastic gradient descent (SGD) algorithms. It addresses fundamental challenges such as non-stationarity caused by arbitrarily fixed initialization and complex dependency structures arising in recursive estimation. The investigators will derive the limiting distributions of SGD-type estimators and construct confidence regions with guaranteed asymptotic coverage. Specific efforts will include (1) establishing Gaussian approximations for high-dimensional dropout regularization, (2) deriving limiting distributions for SGD under non-smooth quantile loss functions using characteristic function techniques, and (3) developing online inference procedures for quantile change-point detection in high-dimensional time series using a novel Bahadur representation. These methods will be supported by numerical experiments and implemented in publicly available software. The results shall provide foundational advances for statistical inference in modern machine learning, bridging theoretical developments with practical applications in dynamic, high-dimensional environments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515926","Collaborative Research: Online Statistical Inference for Modern Machine Learning","DMS","STATISTICS","10/01/2025","06/23/2025","Jiaqi Li","IL","University of Chicago","Standard Grant","Tapabrata Maiti","09/30/2028","$148,654.00","","jqli@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","126900","079Z, 075Z, 1269","$0.00","This research aims to develop statistical tools to improve the reliability of artificial intelligence (AI) that is widely used in real-world systems such as automated decision-making, financial forecasting, and neuroscience research. Modern AI often relies on efficient machine learning algorithms to process large-scale, sequentially arriving datasets. While these algorithms are powerful, understanding their behavior and measuring their uncertainty remains a major scientific challenge. To bridge this gap, the investigators will focus on establishing mathematically rigorous methods for uncertainty quantification to build trustworthy AI. Applications will include enhancing theoretical guarantees and interpretability of neural networks, providing robust estimation and inference for econometric and biomedical studies, and detecting real-time change-points in high-dimensional time series data. The projects will promote the progress of science through open-source software and graduate education, and will support the national interest by contributing to reliable, data-driven decision-making in fields important to economic resilience, public health and national security.<br/><br/>This research will provide a comprehensive theoretical framework for online statistical inference in machine learning, focusing on constant learning-rate stochastic gradient descent (SGD) algorithms. It addresses fundamental challenges such as non-stationarity caused by arbitrarily fixed initialization and complex dependency structures arising in recursive estimation. The investigators will derive the limiting distributions of SGD-type estimators and construct confidence regions with guaranteed asymptotic coverage. Specific efforts will include (1) establishing Gaussian approximations for high-dimensional dropout regularization, (2) deriving limiting distributions for SGD under non-smooth quantile loss functions using characteristic function techniques, and (3) developing online inference procedures for quantile change-point detection in high-dimensional time series using a novel Bahadur representation. These methods will be supported by numerical experiments and implemented in publicly available software. The results shall provide foundational advances for statistical inference in modern machine learning, bridging theoretical developments with practical applications in dynamic, high-dimensional environments.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2533984","Collaborative Research: FDT-BioTech: Digital Twins with Statistical Topological Learning of Periodontal Diseases","DMS","STATISTICS, Engineering of Biomed Systems, MSPA-INTERDISCIPLINARY","09/15/2025","08/20/2025","Yulia Gel","VA","Virginia Polytechnic Institute and State University","Standard Grant","Zhilan Feng","08/31/2028","$650,892.00","Andrei Zagvozdkin","ygl@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","126900, 534500, 745400","079Z, 8038, 1269, 075Z","$0.00","The digital twin (DT) paradigm presents a wide array of opportunities for modeling complex systems in biomedical sciences in a realistic manner, allowing researchers and healthcare professionals to explore various ?what-if? scenarios. In dental sciences, DTs can serve as virtual replicas of a patient's periodontal tissues and structures, enabling clinicians to address a variety of tasks such as simulating periodontal conditions, forecasting treatment outcomes, and personalizing dental care plans. However, achieving this vision is impossible without building confidence in making DTs in healthcare trustworthy which requires the development of novel mathematical and statistical foundations behind such fundamental questions as verification, validation, and uncertainty quantification (VVUQ) of dental DTs, robustness of dental DTs to uncertainties, and cohesive integration of multi-modal health-related data at disparate scales.<br/><br/>This project aims to develop novel mathematical and statistical methodology to establish a foundation of the artificial intelligence (AI)-driven framework for constructing reliable and personalized DTs for periodontal health. By integrating principles from statistical learning, topological data analysis, and generative AI, specifically, probabilistic diffusion models on graphs, the project opens a pathway to build ensembles of individualized dental DTs, termed ?periodontal digital siblings.? These DTs will capture patient variability and uncertainty, offering a more precise representation of individual health profiles. This inherently interdisciplinary effort bridges mathematics, statistics, machine learning, dental science, and healthcare, and promotes widely adoption of DTs in dentistry, with an ultimate goal to transform the prevention and treatment of periodontal disease through personalized, data-driven care. Additionally, this project offers a broad range of unique opportunities for interdisciplinary research training at the nexus of mathematical sciences, AI, and dental medicine, equipping the next generation of researchers with critical skills, and fostering cross-domain innovation and translational science.<br/><br/>This project is co-funded by the Statistics Program in the Division of Mathematical Sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515561","Collaborative Research: Efficient Individualized Treatment Selection for Personalized Medicine","DMS","STATISTICS","09/15/2025","06/23/2025","Yufeng Liu","NC","University of North Carolina at Chapel Hill","Standard Grant","Jun Zhu","08/31/2028","$180,000.00","","yfliu@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","126900","079Z, 075Z, 8038","$0.00","Recent advances in data science, statistics, and machine learning have opened new possibilities in precision medicine, enabling clinicians to tailor treatments based on individual patient characteristics. This project focuses on developing a unified and efficient statistical framework to improve treatment decisions by leveraging rich demographic, socio-economic, and biomedical data. By advancing personalized decision-making, this research contributes to better health outcomes, more efficient healthcare delivery, and overall national well-being. The project also offers broad societal impact through its commitment to education, collaboration, and open science. The investigators will mentor graduate students and develop new coursework at the intersection of machine learning, statistics, and personalized medicine. In addition, all software tools developed will be released as open-source, supporting accessibility and reproducibility in scientific research. The interdisciplinary nature of the project encourages collaboration across statistics, medicine, and computer science, and prepares a next-generation workforce to tackle complex health data problems.<br/><br/>This project aims to develop an efficient learning framework for estimating optimal individualized treatment rules (ITRs) across a broad range of personalized medicine settings. The proposed methodology is based on semiparametric modeling and is designed to address complex relationships among covariates, treatments, and outcomes. Key challenges addressed include handling multiple treatment options with cross-treatment structures, modeling a variety of outcome types, and accommodating multi-stage decision-making with time-varying, history-dependent effects. The framework also supports incorporation of domain knowledge for interpretability and practical implementation. From a statistical perspective, the proposed methods achieve double robustness (consistency under two separate model specifications) and statistical efficiency (minimal asymptotic variance), even under model misspecification and in high-dimensional or limited-data scenarios. These contributions advance the state of the art in both semiparametric theory and algorithmic design for ITR estimation. The resulting models are interpretable, scientifically meaningful, and directly applicable to real-world medical problems, including drug development and treatment recommendation. This work not only contributes to foundational statistical theory but also facilitates translational research in healthcare.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2514344","From Semi-supervised Learning to Prediction-powered Inference","DMS","STATISTICS","09/15/2025","06/13/2025","Daniela Witten","WA","University of Washington","Standard Grant","Jun Zhu","08/31/2028","$250,000.00","","dwitten@u.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981951016","2065434043","MPS","126900","075Z, 079Z","$0.00","In today?s world, data are cheap, plentiful, and everywhere . . . except when they are not. The vast majority of books have been digitized, a substantial portion of the internet has been scraped, and low-cost biomedical technologies are available at a doctor?s (or patient?s) fingertips. But, often the data needed for a particular real-world problem is much harder to access, due to cost or other constraints. This project will answer the question: how can the outputs of machine learning or artificial intelligence algorithms be used to augment limited datasets in order to draw meaningful statistical conclusions?  <br/><br/>Consider a setting where the target of inference is a functional of the joint distribution of X and Y, and n independent and identically distributed observations of (X,Y) are available. This research considers the following questions: under what circumstances, by how much, and how can additional observations for which we only have access to X (and not Y) improve inference? The investigative team will consider this question first from a theoretical perspective, by establishing new semi-parametric efficiency results for semi-supervised learning (Project 1); then from a methodological perspective, by developing new and improved estimators for prediction-powered inference (PPI, Project 2); and finally from an applied perspective, by proposing PPI estimators of true positive rate, false positive rate, and area under the curve (Project 3).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2514234","Collaborative Research: Theory of Causal Learning","DMS","STATISTICS","09/15/2025","06/23/2025","Peng Ding","CA","University of California-Berkeley","Standard Grant","John Kolassa","08/31/2028","$125,000.00","","pengdingpku@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","126900","075Z, 079Z, 1269","$0.00","How can we interpret results from complex machine learning algorithms? How can we mitigate the risks associated with using such models for policy decisions? This project addresses fundamental challenges in deriving valid, reliable, and interpretable causal conclusions from complex data using modern machine learning tools. As machine learning becomes increasingly integral to disciplines such as medicine, economics, education, and the social sciences, the demand for causal insight --- beyond predictive accuracy --- has become more pressing. Yet many machine learning algorithms function as ?black boxes?, offering limited transparency and lacking rigorous frameworks for replicability and uncertainty quantification. This project aims to establish a theoretical foundation for causal learning that makes outputs from machine learning explainable, statistically sound, and actionable in real-world decision-making. The work is complemented by educational and outreach activities that promote understanding of causal reasoning among students and the broader public. Planned efforts include public lectures, collaborations with K?12 educators, and integration of research findings into university curricula. Collaborative partnerships with institutions such as Microsoft, Eli Lilly, and the Fred Hutchinson Cancer Center will help translate methodological advances into impactful scientific and societal applications.<br/><br/><br/>Technically, the project advances causal learning through three interrelated aims. (1) It develops methods for imputing unobserved counterfactual outcomes --- the hypothetical ?what if? scenarios that form the core of causal reasoning --- by integrating flexible machine learning models with statistical principles to preserve both interpretability and rigor. (2) It promotes design-based approaches for quantifying uncertainty, particularly in settings where treatments are assigned randomly or pseudo-randomly via permutations. These methods isolate uncertainty from treatment allocation mechanisms, complementing model-based inference. (3) The project builds a statistical framework for finite-population inference, extending traditional inference techniques beyond super-population assumptions. By drawing on tools from empirical process theory and random matrix theory, the framework provides robust inferential guarantees in realistic data settings where independence and large-sample assumptions fail. Together, these contributions will advance the theory and practice of causal learning, bridging machine learning and statistics to improve both scientific understanding and data-informed decision-making.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515562","Collaborative Research: Efficient Individualized Treatment Selection for Personalized Medicine","DMS","STATISTICS","09/15/2025","06/23/2025","Weibin Mo","IN","Purdue University","Standard Grant","Jun Zhu","08/31/2028","$60,000.00","","harrymok@purdue.edu","2550 NORTHWESTERN AVE # 1100","WEST LAFAYETTE","IN","47906","7654941055","MPS","126900","075Z, 079Z, 8038","$0.00","Recent advances in data science, statistics, and machine learning have opened new possibilities in precision medicine, enabling clinicians to tailor treatments based on individual patient characteristics. This project focuses on developing a unified and efficient statistical framework to improve treatment decisions by leveraging rich demographic, socio-economic, and biomedical data. By advancing personalized decision-making, this research contributes to better health outcomes, more efficient healthcare delivery, and overall national well-being. The project also offers broad societal impact through its commitment to education, collaboration, and open science. The investigators will mentor graduate students and develop new coursework at the intersection of machine learning, statistics, and personalized medicine. In addition, all software tools developed will be released as open-source, supporting accessibility and reproducibility in scientific research. The interdisciplinary nature of the project encourages collaboration across statistics, medicine, and computer science, and prepares a next-generation workforce to tackle complex health data problems.<br/><br/>This project aims to develop an efficient learning framework for estimating optimal individualized treatment rules (ITRs) across a broad range of personalized medicine settings. The proposed methodology is based on semiparametric modeling and is designed to address complex relationships among covariates, treatments, and outcomes. Key challenges addressed include handling multiple treatment options with cross-treatment structures, modeling a variety of outcome types, and accommodating multi-stage decision-making with time-varying, history-dependent effects. The framework also supports incorporation of domain knowledge for interpretability and practical implementation. From a statistical perspective, the proposed methods achieve double robustness (consistency under two separate model specifications) and statistical efficiency (minimal asymptotic variance), even under model misspecification and in high-dimensional or limited-data scenarios. These contributions advance the state of the art in both semiparametric theory and algorithmic design for ITR estimation. The resulting models are interpretable, scientifically meaningful, and directly applicable to real-world medical problems, including drug development and treatment recommendation. This work not only contributes to foundational statistical theory but also facilitates translational research in healthcare.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2514233","Collaborative Research: Theory of Causal Learning","DMS","STATISTICS","09/15/2025","06/23/2025","Fang Han","WA","University of Washington","Standard Grant","John Kolassa","08/31/2028","$125,000.00","","fanghan@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981951016","2065434043","MPS","126900","075Z, 079Z","$0.00","How can we interpret results from complex machine learning algorithms? How can we mitigate the risks associated with using such models for policy decisions? This project addresses fundamental challenges in deriving valid, reliable, and interpretable causal conclusions from complex data using modern machine learning tools. As machine learning becomes increasingly integral to disciplines such as medicine, economics, education, and the social sciences, the demand for causal insight --- beyond predictive accuracy --- has become more pressing. Yet many machine learning algorithms function as ?black boxes?, offering limited transparency and lacking rigorous frameworks for replicability and uncertainty quantification. This project aims to establish a theoretical foundation for causal learning that makes outputs from machine learning explainable, statistically sound, and actionable in real-world decision-making. The work is complemented by educational and outreach activities that promote understanding of causal reasoning among students and the broader public. Planned efforts include public lectures, collaborations with K?12 educators, and integration of research findings into university curricula. Collaborative partnerships with institutions such as Microsoft, Eli Lilly, and the Fred Hutchinson Cancer Center will help translate methodological advances into impactful scientific and societal applications.<br/><br/><br/>Technically, the project advances causal learning through three interrelated aims. (1) It develops methods for imputing unobserved counterfactual outcomes --- the hypothetical ?what if? scenarios that form the core of causal reasoning --- by integrating flexible machine learning models with statistical principles to preserve both interpretability and rigor. (2) It promotes design-based approaches for quantifying uncertainty, particularly in settings where treatments are assigned randomly or pseudo-randomly via permutations. These methods isolate uncertainty from treatment allocation mechanisms, complementing model-based inference. (3) The project builds a statistical framework for finite-population inference, extending traditional inference techniques beyond super-population assumptions. By drawing on tools from empirical process theory and random matrix theory, the framework provides robust inferential guarantees in realistic data settings where independence and large-sample assumptions fail. Together, these contributions will advance the theory and practice of causal learning, bridging machine learning and statistics to improve both scientific understanding and data-informed decision-making.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515303","Collaborative Research: Generalized Fiducial Inference in Complex Systems: Causal Inference, Networks, and Normalizing Flows","DMS","STATISTICS","09/01/2025","08/14/2025","Jan Hannig","NC","University of North Carolina at Chapel Hill","Standard Grant","Yong Zeng","08/31/2028","$125,000.00","","jan.hannig@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","126900","079Z, 075Z","$0.00","Accurate statistical inference is essential for making reliable decisions in various fields, such as forensic science, medicine, economics, and machine learning. This project develops and advances generalized fiducial inference (GFI), an innovative statistical method that quantifies uncertainty without requiring subjective assumptions. By addressing complex real-world problems, such as evaluating evidence in criminal cases, understanding causal relationships in economics and health, and improving reliability in machine learning, the project will significantly enhance decision-making processes. Additionally, the project provides valuable research training opportunities for graduate students in science, technology, engineering, and mathematics (STEM), thereby contributing directly to national goals of promoting scientific advancement, health, prosperity, and welfare.<br/><br/>This collaborative research aims to advance generalized fiducial inference (GFI), building upon Fisher?s original fiducial argument and recent developments in modern statistics. The primary objectives include extending GFI methods to causal inference models, particularly instrumental variable models, and redefining GFI through normalizing flows to manage computational complexity in non-analytic scenarios. The project will also apply these methodological innovations to pressing real-world problems in forensic science, specifically addressing the accurate calibration of likelihood ratios from machine learning models, as well as, resources permitting, investigations into uncertainty quantification for social network learning and sports analytics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2529277","MATH-DT: Functional Surrogate Modeling for Human Agile Locomotion with Wearable Technology","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2025","08/05/2025","Nathan Wycoff","MA","University of Massachusetts Amherst","Standard Grant","Jun Zhu","08/31/2028","$490,000.00","Meghan Huber, Donghyun Kim, Wouter Hoogkamer","nwycoff@umass.edu","101 COMMONWEALTH AVE","AMHERST","MA","010039252","4135450698","MPS","125300, 126900, 745400","079Z, 8038","$0.00","Warfighters must maintain agility and performance in extreme conditions such as navigating rugged terrain, carrying heavy loads, and enduring prolonged exertion, often while facing unpredictable threats. Wearable technologies like robotic exoskeletons and advanced footwear have the potential to enhance warfighter performance and reduce injury risk. However, current design methods often rely on one-size-fits-all approaches and fail to account for how individuals adapt to these devices in real-world settings. This project addresses that gap by developing Digital Twins of agile locomotion in the form of personalized, data-driven simulations that model the complex and dynamic interaction between human movement, wearable technology, and the environment. By integrating real-time physiological and biomechanical data, these models enable better design, training, and deployment of active wearable technology to improve human agility. In addition to advancing national defense and security, this work has broad societal benefits to public health as the mathematical modeling techniques developed can also be used to improve wearable technology design for other user populations, such as those with motor impairment. <br/><br/>The overarching goal of this project is to develop mathematical methods enabling an advanced Digital Twin model of human agile locomotion, aimed at optimizing the design of advanced footwear technology to enhance human agility and mobility. In order to accomplish this, this project will advance the state of the art in statistical surrogate modeling, which currently is focused on vector-valued parameters, to accommodate parameters which are functions. This will require significant mathematical and methodological innovation as the parameter spaces are thus infinite dimensional. The investigators will develop an approach which searches within a manifold of finite but increasing dimension to find candidate functions to test. This new methodology will be developed using data from human locomotion when using wearable technologies in a lab setting. The investigators will first deploy this methodology to develop a novel active model reduction method which searches for parameter settings which are not accommodated by the current reduced model. Next, they will extend sequential design for data-efficient predictive modeling to the acquisition of functions, enabling a model of human adaptation in response to wearable devices. Finally, they will develop an infinite-dimensional extension of the Active Subspace method for dimension reduction to enable interpretable optimization of wearable devices. Taken together, this work will lead to a general-purpose framework for building Digital Twins of systems parameterized by functions, as well as a specific implementation of a Digital Twin for the complex system of a human bearing wearable technology.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2506060","Collaborative Research: CDS&E: Array-Variate Models for Longitudinal Data: Scalable Mixed and Autoregressive Approaches","DMS","STATISTICS","09/01/2025","08/06/2025","Partha Sarkar","FL","Florida State University","Standard Grant","John Kolassa","08/31/2028","$90,000.00","","ps24v@fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","126900","","$0.00","This project addresses the need for statistical models that help biomedical researchers better understand changes in biological markers across populations and within individual subjects over time, using multidimensional tabular data. The investigators will collaborate with neuroscientists at the University of Iowa College of Medicine to adapt and enhance statistical modeling approaches for analyzing data from real-world biomedical studies involving mice. This work seeks to address some of the fundamental statistical challenges associated with these complex datasets. The investigators will accomplish this by leveraging two types of time-dependent modeling approaches and extending these methods to datasets that contain far more variables than observations. The project is significant because these methods will assist biomedical researchers in tackling key healthcare questions, accelerating scientific discovery, and providing tools for interpreting complex biomedical data. To promote broad accessibility and impact, the methods and tools developed will be released as open-source software, advancing both statistical methodology and biomedical research. The project will also strengthen data science training for graduate students and contribute to curriculum development at the undergraduate and graduate levels, thereby preparing students for careers in academia and the biomedical industry. The investigators are committed to mentoring graduate students in both methodological innovation and the adaptation of statistical tools for biomedical applications. Additionally, an open-source software package will be released on a publicly accessible platform to extend the project?s impact across the broader scientific community. In these ways, the project serves the national interest by contributing to NSF?s mission to promote the progress of science and to advance the nation?s health.<br/><br/>This project will develop statistical methods for high-dimensional array-variate data with longitudinal and temporal dependencies, motivated by biomedical applications where data are often structured as multi-dimensional arrays with far more variables than observations. The investigators will design a suite of penalized likelihood and generalized Bayesian approaches for array-variate mixed-effects and autoregressive models, with a focus on inducing sparsity and low-rank structures in the mean, covariance, and precision arrays. Key innovations include the use of generalized likelihoods to broaden applicability beyond traditional Gaussian settings, and random projection matrices to compress mean, covariance, and precision array parameters, thus enhancing the computational scalability of Expectation-Maximization and Monte Carlo-based inference in high-dimensional settings. The proposed models and algorithms aim to produce interpretable results and remain computationally efficient even in applications with large numbers of variables and samples. Theoretical investigations will establish the consistency and optimality of these methods under minimal assumptions. In summary, this toolbox will enable flexible, scalable, and principled inference for array-variate data with complex temporal structure, advancing statistical methodology for structured biomedical datasets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2506059","Collaborative Research: CDS&E: Array-Variate Models for Longitudinal Data: Scalable Mixed and Autoregressive Approaches","DMS","STATISTICS","09/01/2025","08/06/2025","Kshitij Khare","FL","University of Florida","Standard Grant","John Kolassa","08/31/2028","$90,000.00","","kdkhare@stat.ufl.edu","1523 UNION RD RM 207","GAINESVILLE","FL","326111941","3523923516","MPS","126900","9150","$0.00","This project addresses the need for statistical models that help biomedical researchers better understand changes in biological markers across populations and within individual subjects over time, using multidimensional tabular data. The investigators will collaborate with neuroscientists at the University of Iowa College of Medicine to adapt and enhance statistical modeling approaches for analyzing data from real-world biomedical studies involving mice. This work seeks to address some of the fundamental statistical challenges associated with these complex datasets. The investigators will accomplish this by leveraging two types of time-dependent modeling approaches and extending these methods to datasets that contain far more variables than observations. The project is significant because these methods will assist biomedical researchers in tackling key healthcare questions, accelerating scientific discovery, and providing tools for interpreting complex biomedical data. To promote broad accessibility and impact, the methods and tools developed will be released as open-source software, advancing both statistical methodology and biomedical research. The project will also strengthen data science training for graduate students and contribute to curriculum development at the undergraduate and graduate levels, thereby preparing students for careers in academia and the biomedical industry. The investigators are committed to mentoring graduate students in both methodological innovation and the adaptation of statistical tools for biomedical applications. Additionally, an open-source software package will be released on a publicly accessible platform to extend the project?s impact across the broader scientific community. In these ways, the project serves the national interest by contributing to NSF?s mission to promote the progress of science and to advance the nation?s health.<br/><br/>This project will develop statistical methods for high-dimensional array-variate data with longitudinal and temporal dependencies, motivated by biomedical applications where data are often structured as multi-dimensional arrays with far more variables than observations. The investigators will design a suite of penalized likelihood and generalized Bayesian approaches for array-variate mixed-effects and autoregressive models, with a focus on inducing sparsity and low-rank structures in the mean, covariance, and precision arrays. Key innovations include the use of generalized likelihoods to broaden applicability beyond traditional Gaussian settings, and random projection matrices to compress mean, covariance, and precision array parameters, thus enhancing the computational scalability of Expectation-Maximization and Monte Carlo-based inference in high-dimensional settings. The proposed models and algorithms aim to produce interpretable results and remain computationally efficient even in applications with large numbers of variables and samples. Theoretical investigations will establish the consistency and optimality of these methods under minimal assumptions. In summary, this toolbox will enable flexible, scalable, and principled inference for array-variate data with complex temporal structure, advancing statistical methodology for structured biomedical datasets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2506058","Collaborative Research: CDS&E: Array-Variate Models for Longitudinal Data: Scalable Mixed and Autoregressive Approaches","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2025","08/06/2025","Sanvesh Srivastava","IA","University of Iowa","Standard Grant","John Kolassa","08/31/2028","$119,994.00","","sanvesh-srivastava@uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","126900, 745400","9150","$0.00","This project addresses the need for statistical models that help biomedical researchers better understand changes in biological markers across populations and within individual subjects over time, using multidimensional tabular data. The investigators will collaborate with neuroscientists at the University of Iowa College of Medicine to adapt and enhance statistical modeling approaches for analyzing data from real-world biomedical studies involving mice. This work seeks to address some of the fundamental statistical challenges associated with these complex datasets. The investigators will accomplish this by leveraging two types of time-dependent modeling approaches and extending these methods to datasets that contain far more variables than observations. The project is significant because these methods will assist biomedical researchers in tackling key healthcare questions, accelerating scientific discovery, and providing tools for interpreting complex biomedical data. To promote broad accessibility and impact, the methods and tools developed will be released as open-source software, advancing both statistical methodology and biomedical research. The project will also strengthen data science training for graduate students and contribute to curriculum development at the undergraduate and graduate levels, thereby preparing students for careers in academia and the biomedical industry. The investigators are committed to mentoring graduate students in both methodological innovation and the adaptation of statistical tools for biomedical applications. Additionally, an open-source software package will be released on a publicly accessible platform to extend the project?s impact across the broader scientific community. In these ways, the project serves the national interest by contributing to NSF?s mission to promote the progress of science and to advance the nation?s health.<br/><br/>This project will develop statistical methods for high-dimensional array-variate data with longitudinal and temporal dependencies, motivated by biomedical applications where data are often structured as multi-dimensional arrays with far more variables than observations. The investigators will design a suite of penalized likelihood and generalized Bayesian approaches for array-variate mixed-effects and autoregressive models, with a focus on inducing sparsity and low-rank structures in the mean, covariance, and precision arrays. Key innovations include the use of generalized likelihoods to broaden applicability beyond traditional Gaussian settings, and random projection matrices to compress mean, covariance, and precision array parameters, thus enhancing the computational scalability of Expectation-Maximization and Monte Carlo-based inference in high-dimensional settings. The proposed models and algorithms aim to produce interpretable results and remain computationally efficient even in applications with large numbers of variables and samples. Theoretical investigations will establish the consistency and optimality of these methods under minimal assumptions. In summary, this toolbox will enable flexible, scalable, and principled inference for array-variate data with complex temporal structure, advancing statistical methodology for structured biomedical datasets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515158","Active Sequential Change-Point Detection for High-dimensional Streaming Data","DMS","STATISTICS","09/01/2025","07/28/2025","Yajun Mei","NY","New York University","Standard Grant","Yong Zeng","08/31/2028","$175,000.00","","ym3377@nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","126900","075Z, 079Z","$0.00","This research project focuses on active sequential change-point detection for high-dimensional streaming data under sampling or resource constraints, with numerous important real-world applications, including biosurveillance, environmental monitoring, epidemiology, disaster management, homeland security, quality control in manufacturing engineering, and threat detection. The project aims to develop simple yet effective algorithms that are able to quickly detect undesired anomalies or events, subject to false alarm rates and sampling control constraints, when monitoring large-scale streaming data from complex systems. The results of the research are expected to advance the understanding of real-time anomaly detection and online monitoring of high-dimensional streaming data. Graduate students will also receive training through their involvement in the project's research. <br/><br/>This project aims to develop new mathematical, computational, and statistical theories and tools for active sequential change-point detection for high-dimensional streaming data under sampling or resource constraints.  Our specific research aims are to develop computationally scalable and statistically efficient algorithms to detect sparse changes in the high-dimensions under two settings of sample control constraints: (i) the sequential design setting where sampling matrices can be sequentially or adaptively chose based on past observed data, and (ii) the random design setting where the sampling matrices are random as in the modern statistics or machine learning literature. Moreover, real-world case studies in anomaly detection or online monitoring will be investigated. Results of the project are expected to significantly advance the state of the art in sequential analysis, change-point detection, multi-armed bandit problems, streaming data analysis, and large-scale inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515304","Collaborative Research: Generalized Fiducial Inference in Complex Systems: Causal Inference, Networks, and Normalizing Flows","DMS","STATISTICS","09/01/2025","08/14/2025","Thomas Chun Man Lee","CA","University of California-Davis","Standard Grant","Yong Zeng","08/31/2028","$125,000.00","","tcmlee@ucdavis.edu","1850 RESEARCH PARK DR STE 300","DAVIS","CA","956186153","5307547700","MPS","126900","079Z, 075Z","$0.00","Accurate statistical inference is essential for making reliable decisions in various fields, such as forensic science, medicine, economics, and machine learning. This project develops and advances generalized fiducial inference (GFI), an innovative statistical method that quantifies uncertainty without requiring subjective assumptions. By addressing complex real-world problems, such as evaluating evidence in criminal cases, understanding causal relationships in economics and health, and improving reliability in machine learning, the project will significantly enhance decision-making processes. Additionally, the project provides valuable research training opportunities for graduate students in science, technology, engineering, and mathematics (STEM), thereby contributing directly to national goals of promoting scientific advancement, health, prosperity, and welfare.<br/><br/>This collaborative research aims to advance generalized fiducial inference (GFI), building upon Fisher?s original fiducial argument and recent developments in modern statistics. The primary objectives include extending GFI methods to causal inference models, particularly instrumental variable models, and redefining GFI through normalizing flows to manage computational complexity in non-analytic scenarios. The project will also apply these methodological innovations to pressing real-world problems in forensic science, specifically addressing the accurate calibration of likelihood ratios from machine learning models, as well as, resources permitting, investigations into uncertainty quantification for social network learning and sports analytics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515246","Collaborative Research: Novel Bayesian Thresholding and Shrinkage Methods in Multiscale Domains with Applications","DMS","STATISTICS","09/01/2025","08/14/2025","Brani Vidakovic","TX","Texas A&M University","Standard Grant","Jun Zhu","08/31/2028","$100,123.00","","brani@tamu.edu","400 HARVEY MITCHELL PKY S STE 300","COLLEGE STATION","TX","778454375","9798626777","MPS","126900","079Z, 7203","$0.00","Modern science and engineering increasingly rely on extracting meaningful information from large and noisy datasets, such as those arising in medical imaging, environmental monitoring, telecommunications, and numerous other disciplines. This project develops advanced statistical methods that improve signal recovery and noise reduction through innovative shrinkage and thresholding techniques applied in multiscale domains like wavelets. In addition to classical computational tools, the project explores emerging directions involving quantum computing simulators to prototype quantum-inspired shrinkage methods, aligning with growing national and institutional emphasis on quantum technologies. These approaches simplify complex data by selectively attenuating noise while preserving essential features, leading to more accurate and interpretable results. The project integrates education by mentoring students at multiple levels, incorporating findings into graduate and undergraduate courses, and creating open-source software tools that promote reproducible research and broad access to cutting-edge statistical techniques.<br/> <br/>This research advances the theory and application of shrinkage estimation in multiscale settings, with a particular emphasis on quantum-inspired methodologies that complement classical Bayesian and frequentist frameworks. It develops adaptive block-shrinkage procedures employing priors that capture dependence among wavelet coefficients and introduces absolutely continuous shrinkage priors that maintain computational tractability without relying on spike-and-slab or point-mass priors. The project also devises novel thresholding strategies informed by refined extreme-value approximations and Bayesian decision rules based on Bayes factors. Computational implementation includes efficient posterior simulation algorithms and exploratory shrinkage techniques using quantum computing simulators. These innovations will contribute to foundational methodology for nonparametric regression, signal processing, and scalable high-dimensional inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515896","Taming sequential decision-making with reinforcement learning: non-stationarity, heterogeneity, and online/offline comparison","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","09/01/2025","08/14/2025","Ran Chen","MO","Washington University","Standard Grant","Tapabrata Maiti","08/31/2028","$149,997.00","","ran.c@wustl.edu","1 BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","125300, 126900","075Z, 1269, 079Z","$0.00","Many decision-making tasks in healthcare, business, and economics can be naturally framed as online sequential decision-making problems, where decisions are made and outcomes are observed iteratively to achieve long-term objectives. Reinforcement learning (RL) offers a powerful framework and has achieved significant success in engineering domains, including robotics and gaming.  However, human-centered tasks ? such as those in healthcare and business ? pose substantial new challenges for RL. These high-stakes tasks are more complex (e.g., non-stationary environments, heterogeneity across objects, and tension between leveraging historical data and the need to perform well in an interactive online setting) and impose more requirements (e.g., interpretability, performance guarantees, and computational efficiency). This project will address these challenges by conceptualizing and gaining insight into the aforementioned complications and by developing well-rounded methodologies that can effectively handle them and meet all requirements. The research outcomes will be broadly applicable to diverse fields, including but not limited to healthcare (e.g., patient treatment, mobile health), business (e.g., operations management, marketing, financial strategies), and economics (e.g., public policy). This project also integrates research and education by providing research training opportunities for students and incorporating the findings into course materials.<br/><br/>In more detail, the project will focus on three interrelated tasks: (1) investigating non-stationarity in a principled way and developing methods that are adaptive and robust to it; (2) developing personalized RL models and methods to address heterogeneity and studying its influence and implications; (3) systematically comparing online vs. offline RL through establishing principled criteria and gaining insights to guide algorithm development and evaluation. Individual tasks also include computational components that examine the trade-off between computational cost and statistical accuracy. Collectively, these components provide a well-rounded solution to online sequential decision-making problems in business and healthcare that face multifaceted challenges. This project is interdisciplinary and will leverage not only techniques in reinforcement learning and general machine learning but also ideas and tools from diverse technical subfields (e.g., nonparametric statistics, high-dimensional statistics, optimization, and applied mathematics), as well as domain expertise in business and healthcare.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515788","Collaborative Research: Causal Learning with High-dimensional Imaging Outcomes: Methods, Theory, and Algorithms","DMS","STATISTICS","09/01/2025","07/23/2025","Shan Yu","VA","University of Virginia Main Campus","Standard Grant","Yong Zeng","08/31/2028","$124,487.00","","sy5jx@virginia.edu","1001 EMMET ST N","CHARLOTTESVILLE","VA","229034833","4349244270","MPS","126900","079Z","$0.00","The analysis of imaging outcomes is a dynamic and rapidly evolving research field, driven by the growing accessibility of large-scale biomedical imaging databases. Imaging data, often characterized as functional data, presents unique opportunities and challenges for statistical analysis. Existing methods, however, are insufficient for handling the computational demands of large-scale medical imaging data or addressing issues such as unmeasured confounding and population heterogeneity in causal analysis. This research will develop advanced statistical tools to overcome these critical hurdles. By developing new techniques that efficiently process large-scale imaging information and provide more accurate causal insights, this work will advance national interests in scientific innovation and evidence-based decision-making. It will promote scientific progress in a vital area of imaging data analysis and aims to advance public health by enabling a deeper understanding of treatment effects from observational studies. The developed data analytics tools also have broad applicability across various fields, including aging research, digital health, and plant science, addressing challenges faced by modern society. Furthermore, the project will benefit the broader research community through the release of freely available software tools and will support STEM education by involving undergraduate and graduate students in hands-on research and integrating project findings into curriculum development.<br/> <br/>This project aims to develop a general functional data analysis (FDA) framework for analyzing large-scale imaging data and uncovering causal relationships between treatments/exposures and imaging responses. Specifically, the project will address challenges in large-scale observational imaging studies via three aims. First, it will develop functional regression models for imaging responses based on a distributed learning framework, enabling scalable yet accurate estimation and inference. Second, it will introduce an image-on-scalar instrumental variable regression to mitigate confounding bias in observational studies. Third, it will propose an image-on-scalar doubly robust regression method leveraging functional pseudo-outcomes to address population heterogeneity. The proposed methods will be rigorously evaluated using existing imaging studies and are expected to significantly advance the methodology, theory, and computation of FDA and causal inference. Additionally, by releasing open-source software, the project will empower researchers to harness vast amounts of imaging and functional data from publicly available repositories.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515763","Causal Inference under Interference: External Validity","DMS","STATISTICS","09/01/2025","08/14/2025","Michael Schweinberger","PA","Pennsylvania State Univ University Park","Standard Grant","John Kolassa","08/31/2028","$175,000.00","Jonathan Stewart","mus47@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","126900","","$0.00","An open problem in causal inference is the external validity of causal conclusions in connected populations with spillover. A well-designed experiment ensures internal validity, in the sense that causal conclusions are valid in the sample on which the causal conclusions are based. The problem of external validity concerns the question of whether - and how - the causal conclusions can be extended from the sample to the population of interest. For example, the federal government may wish to know whether an economic, financial, or public health intervention improves the welfare of population members. To do so, the government may conduct a pilot study, by sampling a subset of population members, applying the intervention to all sampled population members, and extending the conclusions from the sample to the population of interest. Extending conclusions from the sample to the population is complicated by spillover: e.g., an economic intervention may improve the welfare of treated population members along with the welfare of family members, friends, and neighbors due to spillover. In these and other applications in science and technology, it is essential to understand how causal conclusions can be extended from a sample to the population of interest in the presence of spillover.  This project will tackle the open problem of external validity in causal inference under interference. The project will also result in software available to the general public, and in workshops for high school students.<br/><br/>This project will address the question: How can causal conclusions based on a sample be generalized to the population of interest, when the observed outcomes of sampled units depend on the unobserved outcomes of unsampled units due to treatment spillover and outcome spillover in connected populations? It will make three primary contributions. First, it will characterize the direct and indirect causal effects of treatments on outcomes as explicit mathematical functions of the effects of treatment, treatment spillover, and outcome spillover. Second, it will provide scalable statistical procedures for estimating the direct and indirect causal effects of treatments on outcomes based on a sample from the population of interest. Third, it will provide theoretical guarantees for generalizing causal conclusions from a sample to the population of interest, when outcomes are dependent due to treatment spillover and outcome spillover.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515923","Statistical Inference for Exchangeable Network Models","DMS","STATISTICS","09/01/2025","08/14/2025","Robert Lunde","MO","Washington University","Standard Grant","Yong Zeng","08/31/2028","$139,848.00","","lunde@wustl.edu","1 BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","126900","075Z, 079Z","$0.00","In recent years, there has been a tremendous surge in the availability of relational data in various scientific fields. For example, in biology, an abundance of data has been collected from metabolic networks, gene regulatory networks, brain networks, and ecological networks. Consequently, there is substantial interest from the scientific community in principled statistical procedures for drawing inferences from such array data.  This project will develop broadly applicable statistical tools for such problems, enabling reliable inference across a range of applications, and will provide research training opportunities for graduate students.  <br/><br/>This project has three research aims.  The first aim is to develop uncertainty quantification methods for array prediction problems that offer rigorous theoretical guarantees even when complex forms of missingness are present.  The second aim is to develop an assumption-lean inference framework for regression problems involving network-linked data.   The third aim of the project is to develop variants of the permutation test in various two-sample problems involving network data.  Across all subproblems, we work under the framework of a jointly exchangeable array, which encompasses a vast range of data-generating processes and ensures that the developed methods will be widely applicable across different scientific domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515385","Hunt-and-Test Procedures: Design and Derandomization","DMS","STATISTICS","09/01/2025","07/14/2025","Richard Guo","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yong Zeng","08/31/2028","$175,000.00","","ricguo@umich.edu","1109 GEDDES AVE STE 3300","ANN ARBOR","MI","481091015","7347636438","MPS","126900","079Z, 075Z","$0.00","This project addresses a fundamental challenge in testing statistical hypotheses: reliably detecting signals from complex data while avoiding false discoveries due to ""double dipping"", a practice of unintentionally using the same data to both identify and test hypotheses. Double dipping inflates the rate of false findings, thereby undermining the credibility and replicability of scientific research across various fields, including health, economics, and political science. To overcome this problem, this project develops innovative statistical methods known as ""hunt-and-test"" procedures, which adaptively seek out meaningful signals in data and rigorously validate them without bias. By introducing novel derandomization techniques, the research also eliminates the randomness and variability inherent in current methods, thereby significantly enhancing the replicability of results. These improved methods will directly support the reliable analysis of scientific data and foster greater public trust in research outcomes. Additionally, the project supports educational advancement by training graduate and undergraduate students in the research project.<br/><br/>This project introduces a novel framework for constructing data-adaptive statistical tests through carefully designed and derandomized hunt-and-test procedures. ""Hunt and test"" randomly splits iid data into A and B, first using A to identify potential signal and then validating the signal with B, thereby maintaining rigorous calibration. However, this approach traditionally suffers from reduced statistical power and replicability due to the random data splitting. This research addresses these limitations by aggregating multiple replicates of the hunt-and-test procedure and recalibrating them using advanced techniques, such as rank-transformed subsampling ?a method recently developed by the investigator. The project's two primary aims are to enhance both the robustness and efficiency of this framework. Aim I develops a hunt-and-test strategy to extend the classical score test to accommodate infinite-dimensional parameters, such as regression functions and log-densities, while flexibly integrating machine learning for powerful signal detection and ensuring calibration via debiasing. Aim II advances derandomization methods, optimizing their computational speed and statistical power through low-order approximation techniques and adaptive data aggregation. Together, these innovations substantially improve the performance of statistical hypothesis testing, broadening its applicability to complex, semiparametric models and a variety of scientific disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515815","Collaborative Research: Ranking of Entities","DMS","STATISTICS","09/01/2025","06/18/2025","Snigdhansu Chatterjee","MD","University of Maryland Baltimore County","Standard Grant","John Kolassa","08/31/2028","$70,000.00","","snigchat@umbc.edu","1000 HILLTOP CIR","BALTIMORE","MD","212500001","4104553140","MPS","126900","","$0.00","With limited resources available to tackle various challenging problems, policy makers and stakeholders often have to prioritize which problems to address in what locations or domains. In statistical terms, this involves the ranking of sub-populations and regions when limited (or no) directly observed data is available, and the available data can be on multiple aspects and of diverse types and modalities. The investigators will address the core theoretical, methodological and algorithmic challenges in such problems of ranking entities in multiple contexts of interest to the nation and to public life. The investigators will also develop techniques for measuring and quantifying the variability and uncertainty of such advanced, data-driven, principled ranking techniques to aid policymakers and stakeholders.<br/><br/>For data to be analyzed using hierarchical models, subject to multiple sources of variability and dependencies, the investigators will develop reliable estimates of the ranks of entities with an appropriate quantification of associated uncertainty. The proposed methodologies will follow a Bayesian framework or a resampling-based frequentist one. While these techniques are primarily computation-driven, the investigators will address theoretical foundations of the proposed approaches both in the Bayesian and in the resampling-based frequentist paradigms. Using scalable computational techniques and leveraging geometric and topological properties of data, the investigators will also develop novel methods for ranking and identification of extremes when multivariate responses are of interest, and address benchmarking for compatibility over hierarchy of domains in a principled way.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2443282","CAREER: Heavy-Tailed Priors for Robust Bayesian Inference in Ecology, Machine Learning, and Astronomy","DMS","STATISTICS","09/01/2025","06/16/2025","Jyotishka Datta","VA","Virginia Polytechnic Institute and State University","Continuing Grant","Yong Zeng","08/31/2030","$270,000.00","","jyotishka@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","126900","079Z, 1045","$0.00","Heavy-tailed probability distributions are routine across many scientific disciplines, from astronomy to ecology and finance and network modeling. Such distributions are often utilized in statistical modeling to incorporate non-linearity, robustness to large observations, and sparsity in high-dimensional data. The overarching goal of this project is to build new scalable statistical methods that incorporate heavy-tailed prior distributions in three disparate application areas: independent component estimation that recovers independent, latent sources from their observed mixtures, astronomical distance estimation from parallax measurements, and statistical modeling of compositional data. The research will result in powerful Bayesian tools with rigorous theoretical justification. This project will also narrow the critical gap between methodological advances in statistics and the tools used by the scientific community and promote increased usage and transparency of state-of-the-art Bayesian tools. The research findings will be incorporated into various educational activities to engage K-12 students. The project will provide research opportunities and training for graduate students and will enhance undergraduate and graduate curricula, accompanied by a monograph.<br/><br/><br/>This project develops Bayesian methodologies to address three significant statistical challenges: (1) unifying feature extraction techniques via novel latent space representations in independent component analysis, (2) improving astronomical distance estimation by incorporating measurement errors and non-linear relationships in parallax data, and (3) constructing prior distributions tailored for high-dimensional simplex-valued data that can adapt to arbitrary sparsity and dependence patterns. By leveraging heavy-tailed priors within hierarchical models, this work provides a new framework for controlling higher-order moments in blind source separation and as mixing densities for normal scale mixtures for handling non-linearity, robustness, and sparsity. The methods to be developed will be rigorously tested in applications spanning astronomy, blind source separation, community detection, and ecological modeling of species diversity and affinity, demonstrating their broad utility. The results will be disseminated through peer-reviewed publications in statistics, machine learning, and other scientific journals, and software implementations will be openly accessible as R packages that benefit the wider quantitative science community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515687","Adaptive Inference by Stabilized Cross-Validation","DMS","STATISTICS","09/01/2025","06/18/2025","Jing Lei","PA","Carnegie-Mellon University","Standard Grant","Jun Zhu","08/31/2028","$250,000.00","","jinglei@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133890","4122688746","MPS","126900","075Z, 079Z","$0.00","Modern data analysis and statistical learning are characterized by two defining features: complex data structures and black-box algorithms. The complexity of data structures arises from advanced data collection technologies and data-sharing infrastructures, such as imaging, remote sensing, wearable devices, and genomic sequencing. In parallel, black-box algorithms?particularly those stemming from advances in deep neural networks?have demonstrated remarkable success on modern datasets. This confluence of complex data and opaque models introduces new challenges for uncertainty quantification and statistical inference, a problem we refer to as ``black-box inference''.  This research project aims to develop flexible, valid inference procedures for modern complex data that harness the strengths of black-box machine learning algorithms. These contributions have potential applications in areas such as policy evaluation, model selection, treatment effect identification, and algorithmic fairness auditing.<br/><br/>A central focus of the project is the development of novel variants of a classical statistical tool: cross-validation, repurposed to enable adaptive inference in conjunction with powerful black-box models. Although cross-validation is widely used for evaluating estimator performance, its theoretical foundations remain limited, particularly in the context of complex data and modern algorithms. This research will begin with a multi-population comparison problem, using a stabilized cross-validation framework, and will then investigate performance guarantees of cross-validation in more general settings. The project will also develop new methods for adaptive population comparisons in high-dimensional and nonparametric regimes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515247","Collaborative Research: Novel Bayesian Thresholding and Shrinkage Methods in Multiscale Domains with Applications","DMS","STATISTICS","09/01/2025","08/14/2025","Anirban DasGupta","IN","Purdue University","Standard Grant","Jun Zhu","08/31/2028","$100,000.00","","isaacnewton.anirban@gmail.com","2550 NORTHWESTERN AVE # 1100","WEST LAFAYETTE","IN","47906","7654941055","MPS","126900","7203, 079Z","$0.00","Modern science and engineering increasingly rely on extracting meaningful information from large and noisy datasets, such as those arising in medical imaging, environmental monitoring, telecommunications, and numerous other disciplines. This project develops advanced statistical methods that improve signal recovery and noise reduction through innovative shrinkage and thresholding techniques applied in multiscale domains like wavelets. In addition to classical computational tools, the project explores emerging directions involving quantum computing simulators to prototype quantum-inspired shrinkage methods, aligning with growing national and institutional emphasis on quantum technologies. These approaches simplify complex data by selectively attenuating noise while preserving essential features, leading to more accurate and interpretable results. The project integrates education by mentoring students at multiple levels, incorporating findings into graduate and undergraduate courses, and creating open-source software tools that promote reproducible research and broad access to cutting-edge statistical techniques.<br/> <br/>This research advances the theory and application of shrinkage estimation in multiscale settings, with a particular emphasis on quantum-inspired methodologies that complement classical Bayesian and frequentist frameworks. It develops adaptive block-shrinkage procedures employing priors that capture dependence among wavelet coefficients and introduces absolutely continuous shrinkage priors that maintain computational tractability without relying on spike-and-slab or point-mass priors. The project also devises novel thresholding strategies informed by refined extreme-value approximations and Bayesian decision rules based on Bayes factors. Computational implementation includes efficient posterior simulation algorithms and exploratory shrinkage techniques using quantum computing simulators. These innovations will contribute to foundational methodology for nonparametric regression, signal processing, and scalable high-dimensional inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515262","Collaborative Research: Distributional Balancing Methods for Advancing Causal Inference in Complex Settings","DMS","STATISTICS","09/01/2025","08/07/2025","Chan Park","IL","University of Illinois at Urbana-Champaign","Standard Grant","John Kolassa","08/31/2028","$108,000.00","","parkchan@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","126900","","$0.00","Recent advances in data science and statistics have revolutionized how researchers uncover cause-and-effect relationships from complex, real-world data. Many pressing questions?such as whether flu vaccination reduces infection rates, whether sanitation programs improve children?s health, or whether educational policies enhance student outcomes?cannot be answered through randomized experiments alone. Observational data, while abundant, often pose serious challenges due to hidden biases, unmeasured factors, or interconnected influences among individuals. For example, a person?s risk of flu depends not only on their own vaccination status but also on whether people around them are vaccinated, while unmeasured behaviors such as health-seeking habits can distort results. This project tackles these challenges by developing advanced statistical methodologies that improve the reliability of causal conclusions. In particular, it enhances a class of techniques known as distributional balancing methods, which create fair, comparable groups across the full range of observed variables. By extending these methods to account for complex data structures and unobserved confounding, the project will equip scientists and policymakers with more trustworthy evidence for decision-making. The research outcomes will impact healthcare, education, economics, and environmental policy, while also contributing to science through open-source software, user-friendly resources, and the training of students in cutting-edge statistical methods.<br/><br/>Technically, the project focuses on two complementary innovations. First, it develops a novel framework for distributional balancing in settings where data exhibit dependency structures, such as patients treated within hospitals, students nested within schools, or individuals connected by social networks. The proposed methodology constructs balancing weights by aligning the joint distribution of covariates between treatment groups while explicitly accounting for clustering and network effects, which pose major challenges for current balancing methods. The approach includes diagnostic procedures for assessing covariate balance under dependence and robust sensitivity analysis for evaluating the stability of causal conclusions. Second, the project introduces a new integration of instrumental variable (IV) techniques with reproducing kernel Hilbert space (RKHS)-based distributional balancing. This extension allows researchers to address unmeasured confounding by leveraging valid instruments and estimating balancing weights with respect to flexible, nonparametric distributional distances. The resulting IV-balancing methods provide both theoretical guarantees and computational efficiency, expanding the toolkit of modern causal inference. Together, these methodological advances fill critical gaps in existing frameworks, enabling robust causal analysis in complex observational studies and yielding immediate applications in healthcare policy evaluation, biomedical research, and other domains where confounding and dependency are inherent.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515542","New Methods for Scalable and Robust Simulation-Based Inference","DMS","STATISTICS","09/01/2025","08/05/2025","Yuexi Wang","IL","University of Illinois at Urbana-Champaign","Standard Grant","John Kolassa","08/31/2028","$155,000.00","","yxwang99@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","126900","075Z","$0.00","In fields like genetics, ecology, biology, economics, and psychology, scientists use complex structural models to better understand how the world works. These models aim to mimic real systems, such as how species interact, how crops grow, or how diseases spread, and often rely on key input values, or parameters, that need to be estimated from data. However, many of these models involve high-dimensional, richly structured parameter spaces, making traditional likelihood-based inference methods infeasible, especially for large and complicated datasets. Simulation-based inference (SBI) offers a powerful alternative by exploring these models using simulations rather than direct likelihood evaluation. Within this framework, Bayesian methods provide a natural way to combine expert knowledge with data. This project will develop new simulation-based inference (SBI) methods. These innovations will empower scientists make better use of complex models across diverse domains, supporting more scalable, efficient and reliable decision-making.<br/><br/>Current SBI methods face serious limitations when models involve many parameters or when the models do not fully align with the real-world systems that they are meant to represent.  This project will address two core challenges limiting the broader applicability of Bayesian inference in complex scientific models: scalability to high-dimensional parameter spaces and robustness to model misspecification. To improve scalability, this project introduces a novel solution based on score-based Langevin dynamics, which leverages gradient information approximated by deep conditional score-matching networks. To improve robustness, this project attempts to mitigate the misspecification issue by (1) refining the posterior based on their predictive performance, and (2) explicitly incorporating the level of model misspecification into the inference process.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515263","Collaborative Research: Distributional Balancing Methods for Advancing Causal Inference in Complex Settings","DMS","STATISTICS","09/01/2025","08/07/2025","Guanhua Chen","WI","University of Wisconsin-Madison","Standard Grant","John Kolassa","08/31/2028","$68,000.00","","gchen25@wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","126900","","$0.00","Recent advances in data science and statistics have revolutionized how researchers uncover cause-and-effect relationships from complex, real-world data. Many pressing questions?such as whether flu vaccination reduces infection rates, whether sanitation programs improve children?s health, or whether educational policies enhance student outcomes?cannot be answered through randomized experiments alone. Observational data, while abundant, often pose serious challenges due to hidden biases, unmeasured factors, or interconnected influences among individuals. For example, a person?s risk of flu depends not only on their own vaccination status but also on whether people around them are vaccinated, while unmeasured behaviors such as health-seeking habits can distort results. This project tackles these challenges by developing advanced statistical methodologies that improve the reliability of causal conclusions. In particular, it enhances a class of techniques known as distributional balancing methods, which create fair, comparable groups across the full range of observed variables. By extending these methods to account for complex data structures and unobserved confounding, the project will equip scientists and policymakers with more trustworthy evidence for decision-making. The research outcomes will impact healthcare, education, economics, and environmental policy, while also contributing to science through open-source software, user-friendly resources, and the training of students in cutting-edge statistical methods.<br/><br/>Technically, the project focuses on two complementary innovations. First, it develops a novel framework for distributional balancing in settings where data exhibit dependency structures, such as patients treated within hospitals, students nested within schools, or individuals connected by social networks. The proposed methodology constructs balancing weights by aligning the joint distribution of covariates between treatment groups while explicitly accounting for clustering and network effects, which pose major challenges for current balancing methods. The approach includes diagnostic procedures for assessing covariate balance under dependence and robust sensitivity analysis for evaluating the stability of causal conclusions. Second, the project introduces a new integration of instrumental variable (IV) techniques with reproducing kernel Hilbert space (RKHS)-based distributional balancing. This extension allows researchers to address unmeasured confounding by leveraging valid instruments and estimating balancing weights with respect to flexible, nonparametric distributional distances. The resulting IV-balancing methods provide both theoretical guarantees and computational efficiency, expanding the toolkit of modern causal inference. Together, these methodological advances fill critical gaps in existing frameworks, enabling robust causal analysis in complex observational studies and yielding immediate applications in healthcare policy evaluation, biomedical research, and other domains where confounding and dependency are inherent.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515792","Causal Inference in the Presence of Interference for Randomized Experiments","DMS","STATISTICS","09/01/2025","07/31/2025","David Choi","PA","Carnegie-Mellon University","Standard Grant","John Kolassa","08/31/2027","$155,146.00","","davidch@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133890","4122688746","MPS","126900","","$0.00","Statistical experiments can improve our understanding of complex systems, including the communities, organizations, and local economies that make up our society. In these systems, individuals may be connected by mechanisms such as social influence, economic competition, sharing of information, or the transmission of disease. The presence of such mechanisms is known as interference. Interference greatly complicates statistical analysis. It weakens the conclusions that may be drawn from an experiment, and requires the usage of assumptions whose correctness may be difficult to judge. As a result, statistical conclusions drawn under interference can have considerable caveats or limitations. This project will study interference and how it can be more safely modeled. Doing so can help researchers think more clearly about their experimental results when interference is present. It can also help researchers make fewer assumptions when they interpret their data.  This research will help investigators in fields like economics, which influence daily lives of people in society.<br/><br/>The project will extend a promising approach for detecting and describing interference, so that it may be applied to a broader variety of settings with no assumptions beyond what is known about the design of an experiment. It will also develop a new semiparametric approach for modeling interference, which can help researchers to draw credible statistical conclusions even when interference is strongly measurable over long distances. These conclusions will include test statistics with improved standard errors, and confidence intervals for numbers of individuals violating assumptions about interference.  As part of this project, freely available software will be released to help researchers use the methods that are developed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515789","Collaborative Research: Causal Learning with High-dimensional Imaging Outcomes: Methods, Theory, and Algorithms","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","09/01/2025","07/23/2025","Chunlin Li","IA","Iowa State University","Standard Grant","Yong Zeng","08/31/2028","$125,627.00","","chunlin@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","125300, 126900","079Z, 9150","$0.00","The analysis of imaging outcomes is a dynamic and rapidly evolving research field, driven by the growing accessibility of large-scale biomedical imaging databases. Imaging data, often characterized as functional data, presents unique opportunities and challenges for statistical analysis. Existing methods, however, are insufficient for handling the computational demands of large-scale medical imaging data or addressing issues such as unmeasured confounding and population heterogeneity in causal analysis. This research will develop advanced statistical tools to overcome these critical hurdles. By developing new techniques that efficiently process large-scale imaging information and provide more accurate causal insights, this work will advance national interests in scientific innovation and evidence-based decision-making. It will promote scientific progress in a vital area of imaging data analysis and aims to advance public health by enabling a deeper understanding of treatment effects from observational studies. The developed data analytics tools also have broad applicability across various fields, including aging research, digital health, and plant science, addressing challenges faced by modern society. Furthermore, the project will benefit the broader research community through the release of freely available software tools and will support STEM education by involving undergraduate and graduate students in hands-on research and integrating project findings into curriculum development.<br/> <br/>This project aims to develop a general functional data analysis (FDA) framework for analyzing large-scale imaging data and uncovering causal relationships between treatments/exposures and imaging responses. Specifically, the project will address challenges in large-scale observational imaging studies via three aims. First, it will develop functional regression models for imaging responses based on a distributed learning framework, enabling scalable yet accurate estimation and inference. Second, it will introduce an image-on-scalar instrumental variable regression to mitigate confounding bias in observational studies. Third, it will propose an image-on-scalar doubly robust regression method leveraging functional pseudo-outcomes to address population heterogeneity. The proposed methods will be rigorously evaluated using existing imaging studies and are expected to significantly advance the methodology, theory, and computation of FDA and causal inference. Additionally, by releasing open-source software, the project will empower researchers to harness vast amounts of imaging and functional data from publicly available repositories.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2443145","CAREER: New Frontiers in Time Series Analysis","DMS","STATISTICS","09/01/2025","03/13/2025","Yao Zheng","CT","University of Connecticut","Continuing Grant","John Kolassa","08/31/2030","$35,878.00","","yao.zheng@uconn.edu","438 WHITNEY RD EXTENSION UNIT 1133","STORRS","CT","062699018","8604863622","MPS","126900","1045","$0.00","Modern time series data present complexities; these complexities, along with the rapidly growing array of new statistical and machine learning (ML) methods, have driven the demand for novel solutions to emerging problems. This CAREER project is driven by three fundamental research questions: (1) How to balance interpretability and accuracy in high-dimensional time series modeling and inference? (2) How to adaptively select time series models in real time for nonstationary data, while managing uncertainty? and (3) How to efficiently combine information from time series data with varying quality? This project aims to advance the field of time series analysis by developing novel statistical models, theories, and inference methods to address these issues. The results of this research will enhance dynamic network inference, facilitate real-time decision-making, and promote the integration of diverse time series data sources. This project will achieve educational impacts by integrating our research with mentoring undergraduate and graduate students, developing courses, and high-school outreach. Additionally, an interdisciplinary time series seminar series will be organized to promote cross-disciplinary interactions and provide students and junior researchers with exposure to diverse research in time series analysis.<br/><br/>This project will advance time series analysis on three main fronts: (1) develop Granger causality interpretable, recurrent neural network-based high-dimensional time series models to balance interpretability and accuracy; (2) develop an online, distribution-free procedure for adaptive time series model selection in nonstationary settings, addressing uncertainty via conformal miscoverage rate calibration; and (3) introduce new methods to efficiently combine time series data with different granularities and to impute data under general missing patterns. Underlying this research agenda is our overarching goal to tackle challenges due to the high-dimensionality, nonlinearity, nonstationarity, different granularity, and mixed quality and completeness of modern time series data. With an emphasis on statistical inference, we seek novel solutions by integrating existing statistical frameworks (Granger causality, model confidence sets, and factor models) with contemporary ML approaches (RNNs, model predictive control, and transfer learning). The results developed through our project will advance innovation in time series analysis, bridge the gaps between interpretability, uncertainty quantification, and black-box algorithms, and promote the use of time series data collected from diverse sources.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515814","Collaborative Research: Ranking of Entities","DMS","STATISTICS","09/01/2025","06/18/2025","Gauri Datta","GA","University of Georgia Research Foundation Inc","Standard Grant","John Kolassa","08/31/2028","$180,000.00","Abhyuday Mandal","gauri@uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","126900","","$0.00","With limited resources available to tackle various challenging problems, policy makers and stakeholders often have to prioritize which problems to address in what locations or domains. In statistical terms, this involves the ranking of sub-populations and regions when limited (or no) directly observed data is available, and the available data can be on multiple aspects and of diverse types and modalities. The investigators will address the core theoretical, methodological and algorithmic challenges in such problems of ranking entities in multiple contexts of interest to the nation and to public life. The investigators will also develop techniques for measuring and quantifying the variability and uncertainty of such advanced, data-driven, principled ranking techniques to aid policymakers and stakeholders.<br/><br/>For data to be analyzed using hierarchical models, subject to multiple sources of variability and dependencies, the investigators will develop reliable estimates of the ranks of entities with an appropriate quantification of associated uncertainty. The proposed methodologies will follow a Bayesian framework or a resampling-based frequentist one. While these techniques are primarily computation-driven, the investigators will address theoretical foundations of the proposed approaches both in the Bayesian and in the resampling-based frequentist paradigms. Using scalable computational techniques and leveraging geometric and topological properties of data, the investigators will also develop novel methods for ranking and identification of extremes when multivariate responses are of interest, and address benchmarking for compatibility over hierarchy of domains in a principled way.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515791","Innovations in Integrative Association Testing for Large Genetic Data","DMS","STATISTICS","08/15/2025","08/05/2025","Zheyang Wu","MA","Worcester Polytechnic Institute","Standard Grant","Jun Zhu","07/31/2028","$174,999.00","","zheyangwu@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092280","5088315000","MPS","126900","8038, 079Z","$0.00","This project will develop new statistical tools to enhance the power and precision of discovering disease-associated genes. By integrating diverse sources of genomic information and improving how prior knowledge and statistical evidence are combined, the project aims to uncover subtle genetic signals that might otherwise be missed. These innovations have the potential to transform the understanding and treatment of complex diseases, such as neurodegenerative disorders. The project supports national interests by promoting scientific advancement and improving health outcomes. It also fosters education in statistics, data science, and bioinformatics, supports workforce development, and promotes collaboration across disciplines and sectors to enhance the societal impact of statistical research.<br/><br/>The research will advance statistical theory, methodology, and computation for integrative association testing in heterogeneous genomic data. It focuses on two core challenges: (1) designing more effective weighting strategies for incorporating prior information when combining statistical significances, and (2) developing new methods to integrate discrete statistics within a general hypothesis testing framework. The project will implement and apply these approaches to harmonized whole genome sequencing datasets, with a focus on amyotrophic lateral sclerosis and related neurodegenerative diseases. It also supports interdisciplinary education and research infrastructure by connecting expertise in statistics, computational biology, and medical genetics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515361","Laplace approximation in survival data analysis","DMS","STATISTICS","08/15/2025","08/05/2025","Lili Yu","GA","Georgia Southern University Research and Service Foundation, Inc","Standard Grant","Jun Zhu","07/31/2028","$119,952.00","","lyu@GeorgiaSouthern.edu","261 FOREST DR STE 3000","STATESBORO","GA","304586724","9124785465","MPS","126900","8038","$0.00","Survival data analysis is a vital area of statistics that helps researchers understand outcomes such as time to disease onset, recovery after treatment, or mortality. This type of analysis provides health professionals with insights into how various factors, such as treatments or risk exposures, affect patient outcomes over time. However, current methods for analyzing survival data using semiparametric linear models often face limitations: they either involve significant computational challenges or rely on inaccurate approximations. This project addresses these gaps by developing new and more accurate statistical methodologies that are also computationally efficient. These tools will improve our ability to assess the impact of clinical and environmental factors on survival, ultimately supporting better disease prevention and treatment strategies. In doing so, the research promotes national health, enhances healthcare effectiveness, and contributes to overall societal well-being. Beyond its technical contributions, the project delivers broad societal benefits through its strong commitment to education, collaboration, and open science. The investigators will mentor graduate students and create new interdisciplinary coursework at the intersection of statistics and medicine. All developed software tools will be made openly available to encourage reproducibility and accessibility in scientific research. By fostering collaboration across statistics, medicine, and computer science, the project helps train a next-generation workforce equipped to solve complex challenges in health data analysis.<br/><br/>The investigator will adopt the semiparametric accelerated failure time (AFT) model and use the Laplace approximation method to obtain a less biased estimator than existing approaches, followed by the development of a general bias correction method. The research program is structured around three key components, each addressing different modeling scenarios: (i) Developing Laplace approximated quasi-likelihood methods for both homoscedastic and heteroscedastic survival data in the framework of the AFT model. The Laplace approximation will incorporate the leading polynomial term as well as additional polynomial terms; (ii) Developing Laplace approximated penalized quasi-likelihood methods for both homoscedastic and heteroscedastic survival data with frailty in the framework of the AFT model; and (iii) Developing regularized penalized quasi-likelihood methods for both homoscedastic and heteroscedastic survival data with frailty and variable selection in the framework of the AFT model. Across all components, the investigators will undertake theoretical developments, develop efficient algorithms, and implement the methods in software. These tools will be applied to real-world datasets to demonstrate their practical utility and effectiveness.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515194","Statistical Understanding of Adversarial Training in Neural Networks","DMS","STATISTICS","08/15/2025","08/05/2025","Yue Xing","MI","Michigan State University","Standard Grant","John Kolassa","07/31/2028","$180,000.00","Younggeun Kim","xingyue1@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","126900","079Z, 075Z","$0.00","The increasing use of AI raises concerns about its reliability in real-world environments. Modern AI models, especially deep learning, can produce incorrect predictions when inputs are slightly and deliberately altered, a phenomenon known as adversarial attacks. These vulnerabilities can lead to critical errors in healthcare, scientific research, and security, where AI models guide important decisions. The project improves the robustness of AI by founding a new statistical framework. By improving the trustworthiness of data-driven tools, the project supports advances in multiple scientific fields.  This project will prepare undergraduate and graduate students to be competitive in robust data analysis, and will increase interests in Science and Mathematics at the pre-college level through K-12 outreach.<br/><br/>The research objective of this project is to establish statistical frameworks for robust adversarial training in neural networks and extend them to modern pre-training and fine-tuning paradigms. Specifically, the research goals include: (1) developing a theoretical foundation for adversarial training in two-layer neural networks; (2) designing scalable adversarial training algorithms that leverage dynamic attack strategies and selective sampling for computational efficiency; and (3) creating robust fine-tuning methods for pre-trained foundation models used in downstream tasks. These theoretical and algorithmic advances contribute to a deeper understanding of robustness in statistical learning and yield practical tools applicable across a broad range of disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515805","Flexible and Scalable Cluster Analysis of Longitudinal Microbiome Data to Define Functional Groups","DMS","STATISTICS","08/15/2025","08/05/2025","Yuan Jiang","OR","Oregon State University","Standard Grant","Jun Zhu","07/31/2028","$150,000.00","Lan Xue","yuan.jiang@oregonstate.edu","1500 SW JEFFERSON AVE","CORVALLIS","OR","973318655","5417374933","MPS","126900","8038, 079Z","$0.00","Current research on human and animal microbiomes is largely focused on monitoring and reshaping individual microbes or global microbial communities to diagnose, treat, and prevent diseases, as well as track and improve population health. These fine-scaled and coarse-scaled analyses likely miss an important intermediate ecological scale---functional groups of microbes, which may serve as potent biomarkers of host or ecosystem health as well as targets for medical therapies. This project aims to identify functional groups of microbes by learning their temporal dynamics through longitudinal microbiome studies. However, longitudinal microbiome data possess unique characteristics, such as compositionality, high dimensionality, sparsity, and temporal dependence, and their cluster analysis thus presents distinct challenges. The investigators will develop flexible and scalable functional cluster analysis methods to generate biologically meaningful microbial groups. The investigators will develop, distribute, document, and maintain R software packages for their developed methods, will provide tutorials with example datasets, and will test the software in real-world settings. The investigators will train high-school, undergraduate, and graduate students at the intersection of statistics, ecology, and genomics.<br/><br/>The project aims to expand the traditional toolbox of functional cluster analysis by introducing broader similarity measures of functional curves, incorporating the effects of external factors on the curves to be clustered, and developing a general framework for clustering longitudinal profiles from multivariate non-normal data. Specifically, this project will (1) innovate functional cluster analysis to enable the identification of microbial functional groups defined by novel and flexible subgroups of microbes with similar dynamic patterns, such as scale-invariant and gradient-sign-invariant subgroups; (2) develop adaptable and scalable clustering tools for specific functional groups that react similarly to external factors, in order to link microbial functional group profiles to ecosystem or host health factors; and (3) adapt the clustering tools tailored to the specific characteristics of microbiome data, and identify functional groups of microbes that contribute to wildlife and human health by applying the proposed analytical methods to real ecological and biomedical datasets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515767","Collaborative Research: Performance Guaranteed Statistical Learning with Multiple Classes of Models (guided by PCS)","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","08/15/2025","08/07/2025","Bin Yu","CA","University of California-Berkeley","Standard Grant","Tapabrata Maiti","07/31/2028","$140,000.00","","binyu@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","125300, 126900","1269, 075Z, 079Z","$0.00","This project will develop a next-generation statistical framework to improve the reliability and reproducibility of data science (DS) and artificial intelligence (AI). As DS and AI play an increasingly central role in science, healthcare, technology, and national security, it is essential that the methods used to analyze data are trustworthy and transparent. However, current data analysis tools often rely on the traditional assumption that data come from a specific form of probabilistic model?a practice that often fails to capture the complexity of modern data, leading to misleading conclusions and contributing to a growing crisis of scientific replication. This project studies a new framework called Predictability-Computability-Stability Inference (PCSI) for veridical data science (VDS) to help ensure that conclusions drawn from data are not only accurate but also stable, interpretable, and computationally practical. The research will also help train the next generation of data scientists, promote interdisciplinary collaboration, and support the responsible development of AI. By improving how uncertainty is measured and communicated, the project serves the national interest by strengthening scientific research integrity and public trust in data-driven decisions.<br/><br/>The PCSI approach evaluates multiple predictive algorithms and filters out those with insufficient performance, avoiding dependence on any single model and focusing uncertainty assessment on those that are adequately predictive. It uses multiple bootstrap samplings to address uncertainty in an integrated manner with the new form of uncertainty in PCSI: stability over pred-checked algorithms. It also employs a novel multiplicative calibration technique to ensure valid prediction coverage, improving robustness to subgroup structures. The project specifically aims to advance the PCS framework for veridical data science (VDS) by developing PCSI methods for key areas of machine learning, including classification, deep learning, and ensemble learning. The research consists of three thrusts: (1) developing PCSI for classification to improve uncertainty quantification, robustness, and accuracy in both binary and multi-class settings; (2) designing PCSI methods for deep learning and large language models using computationally feasible perturbations and calibrations to enhance stability, interpretability, and performance in modern AI; and (3) establishing theoretical foundations for PCSI and PCS-guided ensemble learning, showing that even under model mis-specification, PCSI can remain valid and outperform existing methods such as conformal inference under reasonable conditions. These developments will result in statistically sound, computationally efficient tools, along with software, publications, and educational materials to broaden participation and ensure broad dissemination.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515367","Collaborative Research: Statistical Modeling and Inference on Directed Network Data for Understanding Faculty Hiring Dynamics","DMS","STATISTICS","08/15/2025","08/14/2025","Tianxi Li","MN","University of Minnesota-Twin Cities","Standard Grant","Yong Zeng","07/31/2028","$80,000.00","","tianxili@umn.edu","2221 UNIVERSITY AVE SE STE 100","MINNEAPOLIS","MN","554143074","6126245599","MPS","126900","075Z, 079Z","$0.00","This research will advance the progress of science by creating a set of new statistical tools for analyzing complex networks that are fundamental to the nation's prosperity and welfare. Understanding the underlying structures of these networks is critical for making informed, data-driven decisions to promote better and higher productivity in academia. This project will analyze the complex networks of faculty hiring between U.S. universities to understand how factors such as institutional research productivity, geography, and field of study influence hiring dynamics. The outcomes will enhance the efficiency of the U.S. academic system and provide valuable insights for researchers and policymakers. A key guiding principle of this project is a commitment to broad engagement; all outreach, recruitment, and participatory activities are designed to be fully open to all Americans. The project will also create a faculty hiring dataset with open access to the public, release all new methods in a free software package, and develop training opportunities for the next generation of American data scientists.<br/><br/>From a technical perspective, this research will create a versatile statistical toolkit for analyzing weighted, directed networks, which pose significant challenges for existing methods. The investigators will develop four novel methodologies designed for commonly seen applications in analyzing the hiring networks. First, the project will establish a network-to-covariate regression model to handle count-based network data while accounting for complex dependencies between connections. Second, the research will introduce a nonparametric testing framework using network U-statistics to rigorously test for dependence structures. Third, a new method will be developed to identify and perform inference on ""core-periphery"" structures, allowing researchers to distinguish informative patterns from non-informative ones. Finally, the project will introduce a conformal inference framework to formally compare entire populations of networks, even when the networks differ in size. These new statistical methods will be validated through simulation and applied to the comprehensive faculty hiring network dataset, with results disseminated through peer-reviewed publications and the project's open-source software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2529471","Towards Bayesian Digital Twins: Enabling Uncertainty Propagation between Modules and across Assets","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, MSPA-INTERDISCIPLINARY","08/15/2025","08/05/2025","Oksana Chkrebtii","OH","Ohio State University","Standard Grant","Jun Zhu","07/31/2028","$350,000.00","Stephen Niezgoda","chkrebtii.1@osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","125300, 126900, 745400","8038, 079Z","$0.00","This project develops a statistical framework for understanding Digital Twins, a next-generation technology that integrates modeling, data collection, prediction, and decision-making in a two-way, real-time cycle for a physical, biological, or engineering system. This project explores a Digital Twin for a robotic surgical plate bending system that is integrated into a virtual surgical planning process to assist surgeons in planning and executing cranio-facial reconstructive surgery. This Digital Twin could maintain synchronized models of a human mandible after traumatic injury and a surgical fixation plate designed to stabilize it. A robotic plate bender - part of the system?s physical asset - would iteratively bend the plate, monitor changes in its shape, update the corresponding digital model, and plan the next bending operation, all in real time. This model is then coupled with the patient?s jaw geometry to ensure that the applied forces will not induce long-term weakening or failure. Each step of this complex process involves uncertainty, including errors in the plate or mandible models, noisy observations, and unmodeled variation in the robot?s performance. These uncertainties propagate throughout the Digital Twin system in nonlinear, interacting ways, posing potential risks to patient outcomes. Current Digital Twin frameworks often ignore or oversimplify these sources of uncertainty. This project addresses the critical need for robust, real-time methods to quantify, propagate, and manage uncertainty in Digital Twin systems, thereby promoting safer and more reliable medical interventions and supporting NSF?s mission to improve health and advance technological innovation.<br/><br/>The investigators will develop a Bayesian data assimilation framework for Digital Twins, enabling structured uncertainty quantification (UQ), verification, and validation across modular components and real-time physical interactions. This project addresses key Digital Twin UQ challenges by: (1) designing modular representations of uncertainty across system components; (2) developing a Bayesian approach to dynamically update Digital Twin models with real-time sensor data; and (3) formalizing two-way digital?physical interactions to support predictive decision-making. The framework will be demonstrated using a Virtual Surgical Planning system that includes robotic plate bending for autonomous point-of-care manufacturing of patient-specific implants. This testbed provides a rigorous platform for evaluating how Bayesian UQ methodologies improve real-time adaptation, predictive reliability, and decision support in safety-critical applications. The developed methods will also be portable to Digital Twin applications in autonomous manufacturing, such as those pursued in the NSF HAMMER-ERC Engineering Research Center (Hybrid Autonomous Manufacturing Moving from Evolution to Revolution).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515333","Robust Data-Driven Decision-Making: Human-AI Alignment, Adaptivity, and Optimality","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","08/15/2025","08/14/2025","Changxiao Cai","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Tapabrata Maiti","07/31/2028","$150,000.00","","cxcai@umich.edu","1109 GEDDES AVE STE 3300","ANN ARBOR","MI","481091015","7347636438","MPS","125300, 126900","079Z, 1269, 075Z","$0.00","AI-assisted decision-making systems have revolutionized fields ranging from medical treatment to online marketing by learning from data to optimize sequential decisions. However, real-world deployment reveals fundamental challenges that compromise system reliability and effectiveness. For instance, human users may distrust or selectively follow algorithmic recommendations, creating implementation gaps; operating environments often differ frequently from the conditions under which systems were trained; and complete knowledge about the environment is often unavailable. Existing methods typically assume perfect implementation and stable environments, leading to substantial performance degradation when these assumptions fail. This project aims to address these limitations by developing new theories and principled algorithms to enable robust and trustworthy decision-making under realistic constraints. In addition, it will provide valuable opportunities for training students at all levels in the STEM field and introducing the general audience to advances in data science and AI.<br/><br/>This project focuses on fundamental sequential decision-making problems: multi-armed bandit and reinforcement learning. The research pursues three complementary directions, aiming to characterize the fundamental statistical limits of learning and develop provably optimal algorithms while maintaining robustness to varied sources of uncertainties. The first thrust will devise trust-aware procedures that account for human behavioral factors when individuals deviate from algorithmic recommendations. The second thrust will tackle the challenge of distribution shifts between training data and deployment environments through robust transfer learning methods. The third thrust aims to design model-agnostic algorithms that function across diverse model types and automatically adapt to unknown environmental structures. The project will provide statistical insights that inform decision-making practice and develop efficient, robust procedures for real-world applications across various domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515156","Softmax mixture ensembles for leveraging LLM output","DMS","STATISTICS","08/15/2025","08/14/2025","Florentina Bunea","NY","Cornell University","Standard Grant","Tapabrata Maiti","07/31/2028","$250,000.00","Marten Wegkamp","fb238@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","126900","079Z, 1269, 075Z","$0.00","This project introduces and examines softmax mixture ensemble models to address contemporary questions related to the evaluation and interpretation of data generated by trained large language models (LLMs). These statistical models will help summarize varied document corpora by identifying their semantically meaningful latent topics. Current LLMs contain billions of parameters, which makes them difficult to interpret and use directly in subsequent analyses. There is an urgent need for corpus summaries that strike a balance between the complexity of the generating models and a user-friendly representation. This project aims to develop new metrics based on the interpretable corpus summaries to provide critical insights into the similarities and differences between human-generated and AI-generated text.<br/><br/>This research develops computationally efficient inferential methods, with sharp mathematical guarantees, for learning and analyzing softmax mixture parameters from data consisting of a collection of samples, each modeled as mixtures with common mixture components and sample-specific coefficients. The softmax mixture ensemble model will be shown to be a crucial building block in a more complex mixture-of-experts model. The project will provide experimental evidence for the benefits of this framework in analyzing LLM data. Solving the statistical questions of this project requires bringing to bear tools from optimization theory, probability and high-dimensional statistics, while addressing the application questions will require tools from computer science, specifically from the areas of natural language processing (NLP) and, more generally, AI. The ultimate goal is to develop procedures which yield accurate, robust, and interpretable results, readily applicable to scientific applications. The central goals of this research are parameter identifiability in softmax ensemble models, polynomial-time algorithms for parameter estimation in high-dimensional softmax ensemble models with dense and sparse parametrization, finite sample (minimax) guarantees, asymptotic inference for parameter estimates in softmax ensemble models, as well as the development of necessary tools for evaluating LLM output in open-ended text generation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515368","Collaborative Research: Statistical Modeling and Inference on Directed Network Data for Understanding Faculty Hiring Dynamics","DMS","STATISTICS","08/15/2025","08/14/2025","Wen Zhou","NY","New York University","Standard Grant","Yong Zeng","07/31/2028","$100,000.00","","w.zhou@nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","126900","079Z, 075Z","$0.00","This research will advance the progress of science by creating a set of new statistical tools for analyzing complex networks that are fundamental to the nation's prosperity and welfare. Understanding the underlying structures of these networks is critical for making informed, data-driven decisions to promote better and higher productivity in academia. This project will analyze the complex networks of faculty hiring between U.S. universities to understand how factors such as institutional research productivity, geography, and field of study influence hiring dynamics. The outcomes will enhance the efficiency of the U.S. academic system and provide valuable insights for researchers and policymakers. A key guiding principle of this project is a commitment to broad engagement; all outreach, recruitment, and participatory activities are designed to be fully open to all Americans. The project will also create a faculty hiring dataset with open access to the public, release all new methods in a free software package, and develop training opportunities for the next generation of American data scientists.<br/><br/>From a technical perspective, this research will create a versatile statistical toolkit for analyzing weighted, directed networks, which pose significant challenges for existing methods. The investigators will develop four novel methodologies designed for commonly seen applications in analyzing the hiring networks. First, the project will establish a network-to-covariate regression model to handle count-based network data while accounting for complex dependencies between connections. Second, the research will introduce a nonparametric testing framework using network U-statistics to rigorously test for dependence structures. Third, a new method will be developed to identify and perform inference on ""core-periphery"" structures, allowing researchers to distinguish informative patterns from non-informative ones. Finally, the project will introduce a conformal inference framework to formally compare entire populations of networks, even when the networks differ in size. These new statistical methods will be validated through simulation and applied to the comprehensive faculty hiring network dataset, with results disseminated through peer-reviewed publications and the project's open-source software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2513668","Generative Models for Predictive Insights and Inference in Multimodal Data","DMS","STATISTICS","08/15/2025","08/14/2025","Xiaotong Shen","MN","University of Minnesota-Twin Cities","Standard Grant","Tapabrata Maiti","07/31/2028","$175,000.00","","xshen@umn.edu","2221 UNIVERSITY AVE SE STE 100","MINNEAPOLIS","MN","554143074","6126245599","MPS","126900","075Z, 1269, 079Z","$0.00","The project utilizes generative artificial intelligence to create high-quality synthetic data that accurately represents complex real-world information?such as medical images, financial records, and social media text?while ensuring individual privacy is protected. By providing scientists, engineers, and students with safe and realistic data sets, the project accelerates discovery, strengthens the nation?s technological workforce, and supports informed decision-making in health, commerce, and security. Additionally, the open benchmarks and instructional materials generated by the project encourage participation in data science, allowing everyone to contribute to and benefit from scientific advancements.<br/><br/>The research develops a unified Generative Prediction and Inference framework that combines diffusion processes, normalizing flows, and transfer learning to model joint distributions of tabular and unstructured modalities. The framework samples synthetic multimodal data to improve supervised tasks such as image captioning and question answering, delivers calibrated uncertainty estimates, and tests for hallucinations in large language models. Key contributions include algorithms for domain adaptation, reliability metrics for trustworthy AI, and agent-based tools that automate analysis of complex datasets. The resulting software and evaluation suites establish new standards for multimodal data synthesis and statistical inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515470","Advancing nonparametric regression: from convex multi-index and Bayesian models to BART and neural networks","DMS","STATISTICS","08/15/2025","08/14/2025","Adityanand Guntuboyina","CA","University of California-Berkeley","Standard Grant","Tapabrata Maiti","07/31/2028","$175,000.00","","aditya@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","126900","1269","$0.00","This project addresses fundamental challenges in statistical modeling,<br/>to develop more accurate and reliable methods for<br/>analyzing complex data. As data becomes increasingly central to<br/>scientific discovery, economic prosperity, and national security, the<br/>need for advanced statistical tools is paramount. This research will<br/>create new techniques in nonparametric regression, a field of<br/>statistics focused on fitting models to data without pre-supposing the<br/>relationship's form. It confronts three recurrent obstacles in<br/>analyzing large datasets -- curse of dimensionality, ad-hoc tuning<br/>choices, and the tension between flexibility and interpretability -- by<br/>developing principled regression and density-estimation tools, thereby<br/>improving our ability to interpret complex information. The work<br/>forges new links between shape-constrained nonparametric methods and<br/>neural networks, adapts ideas from image processing to statistics, and<br/>also unites frequentist and Bayesian thinking through simple,<br/>intuitive priors. The development of these methods will have<br/>wide-ranging benefits in many applied fields. Furthermore, this<br/>project will contribute to the education and training of the next<br/>generation of statisticians and data scientists, ensuring that the<br/>nation remains at the forefront of this critical field.<br/><br/>The investigator will develop a suite of novel approaches to<br/>nonparametric regression. One area of focus is a new shape-constrained<br/>method for multi-index convex regression, which is designed to<br/>alleviate the curse of dimensionality and has close connections to<br/>single hidden-layer neural networks. Another key component of the<br/>research involves the systematic study of Total Generalized Variation<br/>(TGV) regularization for regression and density estimation problems<br/>that have both smooth and non-smooth components, a common challenge in<br/>fields like image processing. The project will also investigate the<br/>properties of the log-concave maximum likelihood estimator, with the<br/>aim of proving its suboptimality in high dimensions under the total<br/>variation distance. Additionally, the research will explore Bayesian<br/>approaches with innovative priors, such as those based on Cauchy<br/>processes, to model complex regression relationships and address the<br/>issue of tuning parameter selection. Finally, the research will<br/>develop Bayesian methods for mixed-derivative constrained regression,<br/>leading to the creation of an additive regression tree and piecewise<br/>linear fits for greater flexibility in multivariate settings. These<br/>research thrusts will be pursued through a combination of theoretical<br/>analysis and computational experiments, to produce<br/>practical and principled solutions to outstanding problems in<br/>nonparametric regression.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515902","Novel Bayesian Frameworks for Measurement Error Problems in Complex Multivariate Data","DMS","STATISTICS","08/15/2025","08/14/2025","Abhra Sarkar","TX","University of Texas at Austin","Standard Grant","Jun Zhu","07/31/2028","$175,000.00","","abhra.sarkar@utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","126900","079Z","$0.00","This project develops new statistical tools to address a common and important challenge in scientific research: drawing reliable conclusions from data in which observations on variables of interest are imprecise and contaminated by measurement errors. In many real-world studies, from nutrition and health research to astronomy, neuroimaging, and social science, measurements are often noisy, making it difficult to identify meaningful patterns or relationships. Existing statistical methods typically handle such problems under overly simplistic conditions, limiting their usefulness in complex, real-world, multivariate settings. By developing more flexible, principled methods that address realistic measurement error scenarios, this project aims to promote the national interest by supporting more accurate, data-driven decision-making in health, policy, and other applied fields. The project also contributes to workforce development in statistics and data science through graduate training, ensuring that students gain experience with modern data-driven research approaches.<br/><br/>Technically, the project develops novel Bayesian hierarchical frameworks for multivariate density deconvolution and related regression-with-errors-in-variables problems. It introduces covariate-informed density deconvolution methods that flexibly allow both the variables of interest and their measurement errors to vary with associated predictors. These methods incorporate automatic covariate selection and permit different sets of predictors for different coordinates of multivariate outcomes, enhancing both flexibility and interpretability. In addition, the project addresses the largely unexplored area of median density deconvolution, developing tools for modeling measurement errors centered around a median rather than a mean. Both topics are important yet overlooked scenarios in current research. While the proposed methods are demonstrated in nutrition epidemiology contexts, they are broadly generalizable to a wide range of scientific fields where measurement error poses a challenge.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515275","Data integration for heterogeneous data: A general framework for distribution shift, posterior drift and block missing data","DMS","STATISTICS","08/15/2025","08/14/2025","Annie Qu","CA","University of California-Irvine","Standard Grant","Yong Zeng","07/31/2028","$174,999.00","","aqu2@ucsb.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","126900","075Z, 079Z","$0.00","In the era of big data, researchers and analysts have unprecedented access to extensive datasets, opening up new opportunities for scientific discovery and predictive modeling. These datasets, often collected from different sources, contain various forms of heterogeneity, such as distribution heterogeneity, observation heterogeneity, and task heterogeneity. Effectively addressing these complexities is critical for maximizing their potential. This project tackles various sources of data integration in supervised learning by introducing a novel framework, the Representation Retrieval (R2) framework, which simultaneously addresses all three types of heterogeneity. The new framework combines advanced representation learning with sparse-induced machine learning algorithms to achieve its objectives. Additionally, the investigator develops a new integrative penalty designed to improve the integration and effectiveness of the learned representations. This project will lead to a new paradigm for extracting and integrating information from heterogeneous data sources, providing fundamental solutions and a rigorous framework to address these challenges. This project will have broad applications across various fields, including medical and health sciences, social sciences, political science, education, finance, marketing, and artificial intelligence. The investigator will integrate education with research by developing a new course on data integration.<br/> <br/>The new framework extracts a shared representation dictionary from multiple heterogeneous data sources, then selects multiple source-specific retrievers, and estimates source-specific learners. The representation dictionary, accessible to all data sources, contains a set of representers which can represent covariates in a low-dimensional latent space. Each data source employs its own retrievers to select informative representers which project their covariates into an appropriate latent space. Using these retrieved representations, source-specific learners can then be applied to predict responses. The project aims to make significant contributions to the field of data integration through the following advancements: (1) Formulating a General Data Integration Framework: Define a supervised learning problem that incorporates three types of heterogeneity. This formulation generalizes several well-studied problem setups as special cases. (2) Introducing the Representation Retrieval (R2) Framework: Develop a comprehensive framework to address all three types of heterogeneity and overcome common limitations in existing methods. (3) Addressing Distribution Heterogeneity: Leverage a ?partially-sharing structure"" to model distribution heterogeneity effectively, solve optimization problems with sparsity-induced penalties, and introduce a novel ?Selective Integration Penalty"" to encourage representers shared across multiple data sources. (4) Handling Observation Heterogeneity: Propose a non-imputation-based approach to manage observation heterogeneity, providing a robust alternative to conventional methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515084","Interpretable, stable and scalable machine learning and statistical inference","DMS","STATISTICS","08/15/2025","08/14/2025","Garvesh Raskutti","WI","University of Wisconsin-Madison","Standard Grant","Tapabrata Maiti","07/31/2028","$150,000.00","","raskutti@cs.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","126900","079Z, 1269, 075Z","$0.00","In recent years, machine learning and artificial intelligence have been widely deployed across numerous disciplines. Much of the focus of this work has been task-driven (e.g., generating text, predicting outcomes, etc.). However, there remain important follow-up concepts that are more closely aligned with human and project interests and interpretability, such as ""How accurate or reliable is the result?"" or ""How do we interpret this result?"" and ""What are the next steps?"" This project will address these questions using a statistical framework that assesses interpretability and stability for any so-called ""black-box"" approach typically used in machine learning. The outcomes of this work also lead directly to student training at an undergraduate level, so that students are familiar not only with ""how"" to implement methods, but also how to interpret outcomes and take next steps in the analysis. Furthermore, all the methods developed are made scalable, applying to extremely large and complex datasets by using computational heuristics known to reduce run-time and storage. The project provides research training opportunities for graduate students. <br/><br/>This project will address this issue by focusing on 3 specific thrusts: (1) estimating feature importance for arbitrary algorithms in a reliable and scalable way; (2) performing feature selection by developing a statistical hypothesis testing framework for these variable importance estimates; and (3) providing predictive confidence for predictions in a model-agnostic manner. Thrust (1) and (2) provide interpretability measures for black-box models, while thrust (3) addresses the issue of the reliability of predictions. The unifying theme to all 3 thrusts is providing both scalability and reliability guarantees through mathematical theory, simulations, and real data examples. Through these contributions, the project will address issues of interpretability and human confidence in machine learning algorithms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515523","Embedding learning for complex and heterogeneous networks","DMS","STATISTICS","08/15/2025","08/05/2025","Yinqiu He","WI","University of Wisconsin-Madison","Standard Grant","Yong Zeng","07/31/2028","$150,000.00","","yinqiu.he@wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","126900","079Z, 075Z","$0.00","Networks play a central role in representing complex relationships among interconnected entities across diverse scientific domains. The increasing scale and complexity of real-world networks often make analytic tasks computationally expensive or intractable. Learning low-dimensional embeddings from high-dimensional and dependent network data has emerged as a powerful strategy, distilling essential structural information into manageable, interpretable, and computationally efficient representations. These embeddings are instrumental in facilitating downstream analyses and enhancing the practical utility of complex networks. In many data-driven scientific inquiries, researchers require not only accurate embedding estimates but also rigorous uncertainty quantification to ensure reliable inference and decision-making. Furthermore, data collected across varying conditions, time periods, or modalities often lead to the prevalence of multiple heterogeneous networks, yielding pressing needs for comparative and integrative analyses. This project will address these vital challenges by developing comprehensive methodologies for the estimation, inference, and integration of network embeddings. Additionally, it will generate broad educational impacts through research training opportunities for graduate and undergraduate students, innovations in curriculum development, and public engagement through outreach activities.<br/><br/>This project will advance the statistical foundations of embedding learning for complex and heterogeneous networks through three core objectives. First, it will develop novel methodologies with rigorous theoretical guarantees for estimating and quantifying uncertainty in network embeddings under general models with relaxed assumptions. Second, the project will design statistically principled procedures for comparing network embeddings across distinct conditions, ensuring appropriate handling of inherent variability and effective detection of structural anomalies. Third, the project will construct an innovative framework for jointly analyzing and integrating embeddings from multiple heterogeneous networks. It will leverage the shared information across related but distinct network structures to fully exploit statistical efficiency. Collectively, these contributions will deepen the theoretical understanding of network embedding learning and produce a rigorous yet flexible toolkit that bridges the gap between statistical theory and practical network analysis in real-world applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515766","Collaborative Research: Performance Guaranteed Statistical Learning with Multiple Classes of Models (guided by PCS)","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","08/15/2025","08/07/2025","Minge Xie","NJ","Rutgers University New Brunswick","Standard Grant","Tapabrata Maiti","07/31/2028","$159,940.00","","mxie@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","125300, 126900","079Z, 075Z, 1269","$0.00","This project will develop a next-generation statistical framework to improve the reliability and reproducibility of data science (DS) and artificial intelligence (AI). As DS and AI play an increasingly central role in science, healthcare, technology, and national security, it is essential that the methods used to analyze data are trustworthy and transparent. However, current data analysis tools often rely on the traditional assumption that data come from a specific form of probabilistic model?a practice that often fails to capture the complexity of modern data, leading to misleading conclusions and contributing to a growing crisis of scientific replication. This project studies a new framework called Predictability-Computability-Stability Inference (PCSI) for veridical data science (VDS) to help ensure that conclusions drawn from data are not only accurate but also stable, interpretable, and computationally practical. The research will also help train the next generation of data scientists, promote interdisciplinary collaboration, and support the responsible development of AI. By improving how uncertainty is measured and communicated, the project serves the national interest by strengthening scientific research integrity and public trust in data-driven decisions.<br/><br/>The PCSI approach evaluates multiple predictive algorithms and filters out those with insufficient performance, avoiding dependence on any single model and focusing uncertainty assessment on those that are adequately predictive. It uses multiple bootstrap samplings to address uncertainty in an integrated manner with the new form of uncertainty in PCSI: stability over pred-checked algorithms. It also employs a novel multiplicative calibration technique to ensure valid prediction coverage, improving robustness to subgroup structures. The project specifically aims to advance the PCS framework for veridical data science (VDS) by developing PCSI methods for key areas of machine learning, including classification, deep learning, and ensemble learning. The research consists of three thrusts: (1) developing PCSI for classification to improve uncertainty quantification, robustness, and accuracy in both binary and multi-class settings; (2) designing PCSI methods for deep learning and large language models using computationally feasible perturbations and calibrations to enhance stability, interpretability, and performance in modern AI; and (3) establishing theoretical foundations for PCSI and PCS-guided ensemble learning, showing that even under model mis-specification, PCSI can remain valid and outperform existing methods such as conformal inference under reasonable conditions. These developments will result in statistically sound, computationally efficient tools, along with software, publications, and educational materials to broaden participation and ensure broad dissemination.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2533903","Conference: The 2025 Quality and Productivity Research Conference on Statistics in Quality, Industry, and Technology (QPRC 2025)- Industrial Innovation in the Era of AI","DMS","STATISTICS","08/15/2025","08/06/2025","Christina Mastrangelo","WA","University of Washington","Standard Grant","John Kolassa","07/31/2026","$10,000.00","","mastr@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981951016","2065434043","MPS","126900","079Z, 7556, 075Z","$0.00","The 2025 Quality and Productivity Research Conference on Statistics in Quality, Industry and Technology (QPRC 2025) will be held in Seattle, Washington from June 15-18, 2025 at the University of Washington. This meeting will be the 41st research conference sponsored by the American Statistical Association (ASA) Section on Quality and Productivity. The theme of QPRC 2025 is ?Industrial Innovation in the Era of AI.? QPRC 2025 aims to bring together researchers and practitioners from the world who use statistics in quality, technology, and industrial contexts. The conference promotes communication among researchers and practitioners to enable and ensure the development and widespread use of novel insights and methodology. The conference?s focus is on recent advancements in methodology, best practices and innovative applications. 80-85 conference attendees are anticipated and will have access to three plenary presentations, 18 invited paper sessions, four contributed paper sessions, a technical tour, and a one-day short course.<br/><br/>Participation in QPRC 2025 has the potential to advance knowledge and understanding of topics related to data science, statistics, and machine learning, and how they can be relevant for industrial innovation. This conference traditionally attracts prominent statisticians, data scientists, quantitative analysts, and others who have an established record of highly influential, methodological, and interdisciplinary research. These individuals will have the opportunity to discuss the current progress made in statistics and machine learning, such as big data technology, text modeling, the use of generative AI in industrial innovation, and exchange novel ideas and experiences in working with modern data science to discover knowledge and apply it to numerous fields. Hence, this conference has the potential to 1) disseminate new methods and data-driven approaches, the evaluation of previous findings, and the validation of theoretical approaches, 2) stimulate further investigations regarding the benefits of working with statistics and machine learning methods for industry, and 3) increase the awareness of the need to use data science approach in industry. QPRC 2025 is committed to providing an environment where participants from academia and industry will be able to discuss research and ideas. The conference website is https://www.qprc2025.org/ .<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515510","Toward Statistically Optimal Diffusion Generative Models: Accuracy, Complexity, and Privacy","DMS","STATISTICS","08/15/2025","07/31/2025","Jingbo Liu","IL","University of Illinois at Urbana-Champaign","Standard Grant","Yong Zeng","07/31/2028","$118,132.00","","jingbol@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","126900","075Z, 079Z","$0.00","This research project explores the statistical aspects of diffusion models, an emerging class of generative modeling techniques that are transforming current practices in image and video synthesis, scientific simulation, inverse problems, and offline reinforcement learning. The project will apply information theory methods to explain when and why diffusion models succeed under certain statistical assumptions and when they do not. The results of the research are expected to advance the understanding of diffusion-based generative models and inform how they can be improved in terms of generation quality, computational efficiency, and user privacy preservation. The project provides research topics for training undergraduate and graduate students in modern statistical and machine learning techniques.<br/><br/>Specifically, the project aims to address three technical questions: (1) What are the statistical limits of diffusion models in the minimax sense, especially the effect of low probability regions that may explain hallucination behaviors of generative models? (2) What is the optimal query complexity for sampling in diffusion models? The investigator will provide a systematic approach for the optimal query complexity by establishing connections with information-theoretic techniques previously used for analyzing the channel capacity. Accelerated diffusion methods will be constructed that nearly achieve this optimal complexity. (3) What is the fundamental trade-off between accuracy and differential privacy of diffusion models, and how can they be achieved?<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2447624","Physics Informed Forecast of High Resolution Spatio-Temporal Data with a Functional Spatial Regression Framework with PDE Smoothing","DMS","STATISTICS, CDS&E-MSS","08/01/2025","07/10/2025","Stefano Castruccio","IN","University of Notre Dame","Standard Grant","John Kolassa","07/31/2028","$240,000.00","David Richter","scastruc@nd.edu","940 GRACE HALL","NOTRE DAME","IN","465565708","5746317432","MPS","126900, 806900","9263","$0.00","This project addresses the critical need to improve scientific models that combine observed data with established physical laws. As researchers increasingly rely on large and complex datasets, there is growing interest in ""hybrid models"" that merge data-driven insights with the knowledge embedded in mathematical equations, such as those used in physics. However, current methods lack the theory necessary for reliable and interpretable results, especially in understanding spatial phenomena like fluid movement. This project will develop a new modeling framework that bridges the gap between data and physical understanding, enabling more accurate and consistent spatial predictions. The project will also foster outreach by creating open-source tools.<br/><br/>The project develops a novel functional framework for hybrid spatial models that regularize data-driven predictions using Partial Differential Equations (PDEs). The approach formulates spatial regression as a functional optimization problem, where the solution is penalized by the governing PDE, enabling the derivation of fundamental mathematical results. The research has three main objectives: (1) to establish a theoretical foundation for these hybrid models by ensuring well-posedness; (2) to implement inference and spatial interpolation using both finite element methods and Hilbert space basis decompositions; and (3) to apply the methodology to fluid dynamics, generating physically consistent predictions. This framework is transformative in statistical methodology, integrating data science with physical modeling. It advances inferential techniques for spatial functional data and supports reproducibility through open-source software.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515787","Statistical estimation, inference, and discovery of complex PDEs using physics-informed neural networks","DMS","STATISTICS","08/01/2025","07/29/2025","Debarghya Mukherjee","MA","Trustees of Boston University","Standard Grant","Yong Zeng","07/31/2028","$239,999.00","Yves Atchade","mdeb@bu.edu","1 SILBER WAY","BOSTON","MA","022151703","6173534365","MPS","126900","079Z, 075Z","$0.00","Understanding dynamical processes/systems by learning the coefficients of partial differential equations (PDEs) is a fundamental problem in theoretical and applied sciences. In the statistical literature, recovering unknown parameters of PDEs from noisy observations belongs to a class of problems known as inverse problems. Researchers have long been developing methodologies for solving inverse problems, driven by their fundamental importance across a wide range of scientific and engineering domains. Recently, physics-informed neural networks (PINNs) have gained popularity for simultaneously solving partial differential equations (PDEs) and estimating their parameters from noisy observations. Despite their empirical successes, the statistical properties of these estimators remain poorly understood. In particular, due to the complexity of neural networks and the non-parametric function estimation involved, PINNs often produce biased estimates of PDE parameters. Such an estimation bias in this context can lead to inaccurate inference about the physical parameter of interest, with potentially serious implications for downstream applications. This project aims to develop a rigorous statistical framework to draw reliable inferences about the parameters learned by PINNs. The PIs will develop a novel debiasing technique to remove the bias of estimators obtained from PINNs, thus facilitating inference. The method is quite general and can be extended in multiple directions with real-world applications. <br/><br/>This project contributes to advancing the literature on the squared-root-rate estimation of finite-dimensional functionals in semiparametric models, especially in the context of PDE learning via neural networks. Current methods rely on undersmoothing the nonparametric component and cannot be applied directly to deep neural network models and PINNs. The debiasing method to be developed allows researchers to bypass this important challenge for PINN models, which is also easy to implement. Further important extensions to Physics-informed neural operators (PINO), where the nonparametric component is an operator, will be developed. This research also advances the literature on high-dimensional statistics. In many applications, the exact form of the observed PDE may be unknown, leading to the PDE discovery problem, a uniquely challenging version of sparse high-dimensional regression. By leveraging techniques from sparse high-dimensional regression literature combined with the debiasing method, this research will develop a framework for solving the PDE discovery problem with high probability, along with reliable statistical guarantees. Finally, on the applied side, this research also aims to contribute to atmospheric science. Using the developed PINN methodologies, the PIs aim to obtain a more precise understanding of the dynamics of the intertropical convergence zone (ITCZ) in West Africa, which will ultimately improve the ability to predict rainfall under the tropics on a seasonal scale.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515376","Bayesian Model Selection and Model Combination Using Novel Developments of Stein's Unbiased Risk Estimator","DMS","STATISTICS","08/01/2025","07/14/2025","Jonathan Bradley","FL","Florida State University","Standard Grant","Yong Zeng","07/31/2028","$240,000.00","Michael Jauch","jrbradley@fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","126900","079Z, 075Z","$0.00","Statistical models ? probabilistic descriptions of the processes that give rise to observed data ? are an integral component of modern science across various disciplines, enabling researchers to learn from the data they collect and make predictions about future events. This research addresses the important challenges of model selection and model combination within the Bayesian statistical framework. Model selection involves choosing, from a collection of candidate models, the single model that best describes the data, while model combination involves constructing a hybrid model that outperforms any single model. The Bayesian framework has gained prominence in recent decades because it enables researchers to fit complex models to data, simultaneously account for multiple sources of uncertainty, and combine the information in newly observed data with prior scientific knowledge. This research will develop new, computationally efficient techniques for estimating a model?s prediction accuracy in the Bayesian framework and will apply these techniques to the problems of model selection and model combination. In the process, it will contribute to STEM education by training statisticians at the graduate level. It will also lead to publicly available software for researchers across a broad range of disciplines.<br/><br/>This research will include several novel projects aimed at developing Stein?s unbiased risk estimate (SURE) as a practical and computationally efficient tool for Bayesian analysis. SURE has become an established tool for model selection and parameter tuning in frequentist settings. However, SURE requires the computation of a penalty term, sometimes referred to as the generalized degrees of freedom, which adds a significant computational burden for complex estimators. Consequently, SURE has been applied considerably less for more computationally demanding Bayesian and machine learning models.  This research will: (1) develop a novel expression of SURE that is straightforward to compute via Markov chain Monte Carlo for Bayes estimators of a Gaussian mean resulting from essentially arbitrary prior distributions, along with extensions to unknown variances and continuous tuning parameters; (2) introduce methodology for fully Bayesian M-open inference for both Gaussian, improving upon existing model combination/selection techniques and allowing for fully Bayesian uncertainty quantification for machine learning models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515719","Collaborative Research: Resampling bridges for complex data","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","08/01/2025","07/23/2025","Daniel Nordman","IA","Iowa State University","Standard Grant","Tapabrata Maiti","07/31/2028","$200,000.00","","dnordman@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","125300, 126900","9150","$0.00","This research aims to produce efficient and accurate statistical methodology that can be applied in practice without strong assumptions about data. In many industrial and scientific applications, current statistical analyses often require an adequate model for data, which can often be uncertain and practically difficult to choose. This situation can be problematic, though, because any conclusions drawn from a mistaken model may be unreliable or misleading. As a remedy, a direct benefit of this project is to provide alternative statistical tools for complex data that are valid without the dangers of model choice or other stringent conditions about data. This research would therefore advance data-based inference in subject areas such as environmetrics, economics, finance, geology, astronomy, etc., which encounter different types of data and where model-free statistical methods can play an important role in data analysis. The project will also support the professional development of students through training in data science and modern, computer-intensive statistical methods.<br/><br/>This project particularly aims to produce ?bridged? resampling methods for complex data, in the sense of connecting and combining separate approaches for resampling (or re-using) data, in order to achieve better and more accurate statistical inference. Some of the problems to be addressed include the investigation and development of blended bootstrap techniques as a novel and general strategy for merging the strengths of subsampling and bootstrap, as two philosophically distinct resampling approaches for data. Further research goals involve the development of more versatile and effective empirical likelihood and bootstrap methods for time series and spatial data based on combining different types of empirical likelihood for dependent data (i.e., data-transformations and data-blocking) with new bootstrap schemes. These formulations of bridged resampling intend statistical methodology that has wide applicability and favorable performance under mild conditions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515718","Collaborative Research: Resampling bridges for complex data","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","08/01/2025","07/23/2025","Haihan Yu","RI","University of Rhode Island","Standard Grant","Tapabrata Maiti","07/31/2028","$99,924.00","","haihan.yu@uri.edu","75 LOWER COLLEGE RD RM 103","KINGSTON","RI","028811974","4018742635","MPS","125300, 126900","9150","$0.00","This research aims to produce efficient and accurate statistical methodology that can be applied in practice without strong assumptions about data. In many industrial and scientific applications, current statistical analyses often require an adequate model for data, which can often be uncertain and practically difficult to choose. This situation can be problematic, though, because any conclusions drawn from a mistaken model may be unreliable or misleading. As a remedy, a direct benefit of this project is to provide alternative statistical tools for complex data that are valid without the dangers of model choice or other stringent conditions about data. This research would therefore advance data-based inference in subject areas such as environmetrics, economics, finance, geology, astronomy, etc., which encounter different types of data and where model-free statistical methods can play an important role in data analysis. The project will also support the professional development of students through training in data science and modern, computer-intensive statistical methods.<br/><br/>This project particularly aims to produce ?bridged? resampling methods for complex data, in the sense of connecting and combining separate approaches for resampling (or re-using) data, in order to achieve better and more accurate statistical inference. Some of the problems to be addressed include the investigation and development of blended bootstrap techniques as a novel and general strategy for merging the strengths of subsampling and bootstrap, as two philosophically distinct resampling approaches for data. Further research goals involve the development of more versatile and effective empirical likelihood and bootstrap methods for time series and spatial data based on combining different types of empirical likelihood for dependent data (i.e., data-transformations and data-blocking) with new bootstrap schemes. These formulations of bridged resampling intend statistical methodology that has wide applicability and favorable performance under mild conditions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515732","Multiscale random matrices, inference and learning in high dimensions","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","08/01/2025","07/21/2025","Gustavo Didier","LA","Tulane University","Standard Grant","Yong Zeng","07/31/2028","$199,626.00","","gdidier@tulane.edu","6823 SAINT CHARLES AVE","NEW ORLEANS","LA","701185665","5048654000","MPS","125300, 126900","075Z, 079Z, 9150","$0.00","In several areas of scientific investigation, such as genomics, astrophysics, and network traffic, traditional modeling tools are inadequate for handling the vast amounts of data generated by modern technology. This project will bring together a team of collaborators to tackle the challenge of developing new methodologies for analyzing complex and multiscale data. The group is both multidisciplinary (mathematics, statistics, and signal processing) and international (U.S. and France). The driving area of application is the modeling of brain dynamics and connectivity, as well as its repercussions for our understanding of neurological functions and disorders. By training both graduate and undergraduate students, this project will contribute to preparing the U.S. workforce for jobs that require knowledge of some of the latest trends in data science, including high-dimensional statistics, machine learning, and AI.<br/><br/>The project will develop a mathematical framework for the fractal modeling of high-dimensional data through the lens of multiscale random matrices (MRMs). MRMs offer a general approach to large-scale, complex stochastic dynamics. They are particularly suited to the analysis of systems where the number of time series (namely, the dimension) is comparable to the number of observations. In the framework of MRMs, newly uncovered universality properties of random matrix statistics will guide the construction of robust asymptotic results. In addition, MRMs will underpin the development of graph-based methodology for high-dimensional, nonstationary systems. In the context of multi-sample problems, the project will further add to the recent stream of literature on random matrix theory applied to statistical learning by developing data-scientific methodologies that are intrinsically multiscale and robust.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515285","Addressing Data Scarcity in Reinforcement Learning: Inference and Decision-Making under Complex Environments","DMS","STATISTICS","08/01/2025","06/18/2025","Yongyi Guo","WI","University of Wisconsin-Madison","Standard Grant","Tapabrata Maiti","07/31/2028","$154,999.00","","guo98@wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","126900","075Z, 079Z, 1269","$0.00","This project addresses the increasing demand for reliable and interpretable reinforcement learning (RL) systems that can function effectively in complex, data-limited environments. RL has demonstrated potential in fields such as healthcare and public policy, where decisions often need to be made with limited and noisy data. However, ensuring that these systems are statistically robust, interpretable, and socially responsible remains a challenge. This research aims to develop tools that improve decision quality, support valid statistical inference, and enhance interpretability in RL algorithms, ultimately building trust and accountability for real-world applications. The broader impacts include advancing the science of machine learning, improving decision-making in resource-constrained settings, and training future data scientists through interdisciplinary mentorship and open educational resources.<br/><br/>The research focuses on developing theoretical foundations and methods for reliable inference and decision-making in RL when data is scarce and models are misspecified. It consists of three main components: First, developing inference and decision tools for contextual bandits with misspecified reward models, where standard methods may fail. Second, constructing an inference framework for adaptive RL algorithms deployed across a population, utilizing a state-statistics decomposition to enhance interpretability and support principled personalization. Third, leveraging auxiliary offline data and structural assumptions to enable robust decision-making in nonstationary environments. This project will produce novel estimation methods and inference procedures with both finite-sample and asymptotic guarantees, along with publicly available software tools. Applications will include adaptive experimentation across various scientific domains, supported by interdisciplinary training in statistics, data science, and engineering.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515679","Adaptive Data Integration: Harnessing Commonality amidst Heterogeneity","DMS","STATISTICS","08/01/2025","07/30/2025","Kaizheng Wang","NY","Columbia University","Standard Grant","John Kolassa","07/31/2028","$155,000.00","","kaizheng.wang@columbia.edu","615 W 131ST ST","NEW YORK","NY","100277922","2128546851","MPS","126900","075Z, 079Z","$0.00","This project develops powerful new methods for integrating diverse and evolving datasets, a critical challenge in modern science and technology. In an increasingly data-driven world, information is often collected from many sources, at different times, and in various formats, making it difficult to analyze as a whole. This research will create flexible and reliable tools that can automatically integrate complex information. The tools will be applied to assess the safety of autonomous vehicles by combining limited test data from a new city with vast amounts of driving data from other regions. The project will also develop open-source software for all researchers and create educational materials to train the next generation of scientists and engineers.<br/><br/>This research will establish dependable methodologies and solid theoretical foundations for data integration through three interconnected thrusts. First, the investigator will develop transfer learning methods for integrating samples from multiple sources to enable knowledge distillation and transfer across heterogeneous datasets. Second, the research will address the challenge of data streams that evolve over time by creating techniques that adapt to temporal distribution shifts. Third, the investigator will create principled approaches for learning latent structures by integrating multiple data views of the same subjects, such as combining social network information with individual user profiles. A key intellectual contribution of this work is the development of practical procedures that automatically adapt to unknown data heterogeneity with theoretical guarantees under minimal assumptions, overcoming a major limitation of current methods. The project will deliver innovative analytical tools, new theoretical insights into data integration, and open-source software packages to benefit the broader scientific community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2514240","Statistical Problems in Quantum Learning","DMS","OAC-Advanced Cyberinfrast Core, STATISTICS","08/01/2025","07/01/2025","Yazhen Wang","WI","University of Wisconsin-Madison","Standard Grant","Yong Zeng","07/31/2028","$344,228.00","","yzwang@stat.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","090Y00, 126900","075Z, 7203, 079Z","$0.00","The rapid growth in data generation and collection has led to a significant proliferation of large-scale datasets in recent years. Big data has profoundly transformed scientific research and knowledge discovery. Data science integrates statistical analysis, computational algorithms, and domain-specific knowledge to extract insights from big data, enabling solutions to complex real-world problems. The increasing scale and complexity of data have driven a growing demand for more advanced computational and statistical methods?ranging from hardware to software systems?particularly for machine learning applications. Quantum computing holds the potential to revolutionize data science, especially in computational statistics and machine learning, by enabling quantum learning to meet the emerging demand. This research project aims to investigate statistical challenges in quantum learning. The investigator will develop novel statistical techniques to demonstrate the advantage of quantum approaches over classical methods for tackling difficult machine learning tasks. Additionally, the investigator will actively engage in initiatives that integrate research with workforce development (including graduate students) and apply these advancements to address complex real-world problems.<br/><br/>A central issue in data science is the interplay between statistics and computation, with computational power being essential for developing effective methods to tackle increasingly complex challenges. Quantum computation, which involves preparing and manipulating quantum states of physical systems, offers the potential to revolutionize data science?particularly in computational statistics and machine learning?by enabling a new paradigm known as quantum learning. However, the intrinsic randomness of quantum mechanics introduces stochasticity into quantum computation, posing unique challenges. Data science, through its foundations in statistics and machine learning, is well-positioned to address these challenges by contributing to the development of quantum computing devices, algorithms, and learning techniques. This research project aims to develop statistical methodologies and theoretical foundations to address key problems in quantum learning. Specifically, it will study (i) statistical inference for the Boson sampling model, and (ii) statistical analysis for quantum state and process learning in both classical and quantum settings. The investigator will tackle emerging scientific problems through novel statistical and computational approaches and address the challenges that arise in solving complex learning tasks. The project seeks to establish rigorous, theoretically grounded statistical methodologies and computational procedures that will substantially advance our understanding of quantum learning from both statistical and computational perspectives.<br/><br/>This award by the Division of Mathematical Sciences is jointly supported by the NSF Office of Advanced Cyberinfrastructure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515765","OODA: Statistics with geometric sample spaces","DMS","STATISTICS","08/01/2025","07/21/2025","James Marron","NC","University of North Carolina at Chapel Hill","Standard Grant","Yong Zeng","07/31/2028","$299,978.00","Ezra Miller","marron@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","126900","075Z, 079Z","$0.00","This project develops mathematical foundations and statistical methods for data when each data object or the set of all possible data points has complex geometric structure. Data of this sort arises in an increasing multitude of societally important contexts. In forensic science, for example, fingerprints are not described by single numbers or even lists of numbers.  In geoscience, patterns in thin slices of rock or ice serve as signatures of chemical composition or history.  In medicine, imaging commonly produces high-resolution 3D scans that capture shapes, or even 4D scans (videos) that capture shapes evolving in time.  The shapes themselves can be complicated geometric objects: branching trees of blood vessels, airways, or nerve cell dendrites; folded or kinked surfaces of brains or teeth; segmented or irregular blobs that vary subtly from patient to patient.  Often, the geometry is not smooth: even from very close up, a branch point or a kink, for instance, does not look like a flat line, plane, or Euclidean space of higher dimension. These sorts of resolutely non-Euclidean phenomena pose fundamental challenges for data analysis, whose goal is to identify trends, search for anomalies, or classify. Even making these tasks precise in non-Euclidean geometric settings requires the mathematical foundations and statistical techniques targeted by this project.  These scientific pursuits serve as a platform to mentor a group of interdisciplinary junior researchers, from high school to postdoctoral, in a vertically integrated scientific research lab environment, and enhance their professional development through direct research funding and travel support.<br/><br/>The supported research will develop statistical methods to handle data sampled from non-Euclidean geometric spaces such as manifolds, algebraic varieties, simplices, and more general singular spaces, drawing on methods from probability, notably surrounding geometric central limit theorems on stratified spaces, as well as from algebraic, differential, and convex geometry. The project aims to produce specific contributions to (i) fundamental geometric statistics in non-smooth settings, including confidence regions and hypothesis testing for singularities as well as (ii) exploratory data analysis by deformation and slicing to find modes of variation in geometric sample spaces such as polyspheres, probability simplices (for compositional data), and more general semialgebraic varieties. This research meets the critical need for new methods in the emerging and important area of statistics and data science of complex data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515934","Inference for Geometric and Topological Data Analysis","DMS","GEOMETRIC ANALYSIS, STATISTICS","07/15/2025","07/14/2025","Wolfgang Polonik","CA","University of California-Davis","Standard Grant","Yong Zeng","06/30/2028","$270,000.00","","wpolonik@ucdavis.edu","1850 RESEARCH PARK DR STE 300","DAVIS","CA","956186153","5307547700","MPS","126500, 126900","075Z, 079Z","$0.00","Geometric and topological data analysis comprises a large collection of useful tools and methods in the modern landscape of data science with numerous applications in a broad variety of fields.  Relevant concepts have evolved into versatile tools that are reshaping the manner in which we handle, scrutinize, and make sense of intricate data structures. This project aims at tightening the connections between geometric and topological data analysis on the one hand, and statistics and machine learning on the other. This goes along with the training of both graduate and undergraduate students, which is another integral part of the project. <br/><br/>More specifically, the project will focus on theory and methodology based on core concepts in the field: the Euler characteristic, the graph Laplacians, and the heat kernel. Among others, this includes the development of large sample distribution theory for an appropriately weighted Euler characteristic process, and a novel two-sample testing procedure for isometry of manifolds based on the heat kernel signature. Crucially, the contributions of the project will result in novel insights catalyzing further developments, which will also benefit practitioners. The theoretical tools used in this project are a combination of tools from geometric probability theory, the analysis on Riemannian manifolds, and differential geometry. While these advanced topics lend themselves naturally to the training of graduate students in statistics, the overarching geometric nature of the project also allows for an exciting and meaningful involvement of advanced undergraduates.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515709","Generative Bayesian Inference","DMS","STATISTICS","07/15/2025","07/15/2025","Veronika Rockova","IL","University of Chicago","Standard Grant","Yong Zeng","06/30/2028","$245,190.00","","veronika.rockova@chicagobooth.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","126900","075Z, 079Z","$0.00","As artificial intelligence (AI) continues to proliferate rapidly through society, there is a growing interest in incorporating core statistical principles, such as uncertainty quantification, into predictive systems to enable reliable and trustworthy AI decision-making. This research program aims to bridge the current conceptual gap between statistics and AI by ensuring that machine-assisted predictions are statistically valid, thereby supporting their safe and effective application to complex scientific challenges in data-rich domains such as imaging, personalized medicine, business analytics, marketing, and economics. The research agenda is organized around two overarching objectives, unified by a common thread: generative models in which data are viewed as stochastic outputs of computer programs. Conducting predictive inference in these models poses significant challenges due to their inherently opaque, black-box structure. The first objective is to develop a novel Bayesian inferential framework for generative models, leveraging modern machine learning tools such as deep learning and Bayesian Additive Regression Trees (BART). This work will lay the methodological and theoretical foundations for a new class of ?generative Bayes? techniques that enable statistically principled inference in complex generative systems. The second objective focuses on advancing practical methodology for computerized adaptive testing (CAT), aimed at enhancing computer-human interactions through dynamically tailored questioning that adapts in real time to the respondent?s skill level with applications. <br/><br/>The overarching goal of this research is to integrate modern machine learning tools into statistical modeling while establishing rigorous theoretical foundations that justify their practical use. The first project will develop a novel generative Bayesian framework for quantile-based learning using Bayesian Additive Regression Trees (BART). The outcome will be a flexible generative toolkit capable of simulating from a wide range of conditional distributions?core components for addressing numerous inferential tasks, including prediction. This work will chart a new path for nonparametric modeling of conditional distributions (such as posterior and posterior predictive distributions) via quantile learning under minimal assumptions. The methodological advances will be supported by a comprehensive frequentist-Bayesian theoretical analysis to assess the fidelity of distributional reconstructions. The second project will provide new theoretical insights into widely used sparsity-inducing priors, such as the horseshoe prior, by evaluating their performance from a predictive perspective. These contributions will deepen our understanding of the predictive properties of sparse Bayesian models and further enhance their applicability. The third project aims to bring machine learning techniques, particularly Q-learning, into the realm of computerized adaptive testing, thereby extending classical item response theory models to enable more responsive and individualized assessment tools. Together, these three projects will significantly advance the frontiers of nonparametric Bayesian methodology, theory, and applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515698","Causal and Structural Inference for Heterogeneous Network Interference","DMS","STATISTICS","07/15/2025","07/10/2025","Yubai Yuan","PA","Pennsylvania State Univ University Park","Standard Grant","Yong Zeng","06/30/2028","$155,647.00","","yvy5509@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","126900","079Z, 075Z","$0.00","Network interference is a fundamental driving force behind phenomena such as intervention spillover, behavioral contagion, and information diffusion in interconnected systems. In this project, the investigator aims to develop statistical methods to understand and quantify whether, and to what extent, an individual?s behavior or status is influenced by others through network interactions. This project focuses on two central challenges in estimating network interference from observational data: individual heterogeneity and network confounding. To overcome the challenges, the investigator will develop an expressive and interpretable statistical model that adaptively learn how the effect of interference depends on individual-level contextual and network information. This model is expected to provide insights and an analytics tool towards the mechanisms of network interference. In addition, the investigator will design novel estimators to measure the causal effects of network interference, leveraging advanced machine learning techniques to address complex confounding among connected individuals and to make efficient use of limited experimental data. The methodologies developed in this project will advance the fields of causal inference and graph-based machine learning. The tools developed will have broad applicability to network data in social science, public health, political science, economics, and business, and will support new theoretical developments in areas such as social influence, disease transmission, marketing, cultural evolution, and collective behavior. The project will provide research opportunities for graduate students. <br/> <br/>In this project, the investigator aims to (1) design a data-driven framework for estimating heterogeneous spillover effects using graph neural networks; (2) develop intervention effect estimation method that integrates active learning to address sample size limitation which is common in real applications; and (3) construct a directed graphical model to identify latent propagation patterns in heterogeneous network cascades. The methodological foundation consists of two main innovations: an attention-based neural network model for robustly estimating individual exposure mapping, and a network mixture model for recovering diffusion structures at the population level from cascade data. To further improve estimation efficiency, the project introduces novel data augmentation strategies that leverage contextual information and network structure, enhancing the causal estimation accuracy of the intervention effect, even in data-scarce settings. The main advantage of the new methods is the decomposition of target estimands into two components: a global network-based interference structure and local individual heterogeneity. The latter is approximated using advanced graph machine learning techniques, enabling the model to strike a balance between expressiveness and interpretability. Overall, this research will provide theoretical and computational tools for studying network interference, leading to the development of open-source software tailored for practical applications across various disciplines.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515520","Unsupervised Learning and Nonlinear Dimension Reduction: Advances with Optimal Transport, Empirical Bayes, and Variational Inference","DMS","STATISTICS","07/01/2025","06/17/2025","Bodhisattva Sen","NY","Columbia University","Standard Grant","John Kolassa","06/30/2028","$240,000.00","","bodhi@stat.columbia.edu","615 W 131ST ST","NEW YORK","NY","100277922","2128546851","MPS","126900","075Z, 079Z","$0.00","Modern scientific data sets?ranging from single-cell RNA sequencing with tens of thousands of genes per patient, to galaxy-survey spectra with millions of stars, to user-item interaction matrices in online platforms?share two features: (i) ultra-high dimensionality and (ii) latent parameters that obey common structural laws (e.g., exchangeability, sparsity, or low-rank dependence). This project tackles both challenges at once. It advances statistical foundations for such problems by (1) providing a new framework to theoretically study empirical Bayes methods in these complex models that learn the latent-parameter distribution directly from the data, and (2) developing cutting-edge unsupervised dimension-reduction techniques that embed the high-dimensional observations into lower-dimensional representations while preserving the essential structure and relationships within the data. Together, these tools will transform ad-hoc prior modeling into an objective, data-driven procedure and yield principled, scalable inference for large-scale applications. Further, collaborations with astronomers will ensure immediate scientific impact, and several of the research directions will shape the Ph.D. dissertation of multiple Columbia graduate students, fostering the next generation of data-science leaders.<br/><br/><br/>The project integrates two tightly linked research thrusts. (a) Building on recent advances in nonparametric empirical Bayes, the PI will design flexible empirical Bayes estimators for latent-variable distributions for exchangeable parameters. The central innovation is to merge empirical Bayes ideas with variational approximations in general probabilistic latent-variable models, producing estimators that remain computationally tractable, and achieve optimal risk. (b) Classical linear dimensionality reduction techniques like principal component analysis and multidimensional scaling are often inadequate for datasets that are growing increasingly complex in fields such as genomics, astronomy, and finance. To tackle these challenges, the PI will design non-linear reduction techniques --- combining manifold learning with low-rank factor models ---informed by ideas from optimal transport and algebraic geometry. Together, these advances will deepen statistical theory at the nexus of empirical Bayes, unsupervised learning, and high-dimensional inference. All algorithms will be released as open-source R/Python packages accompanied by tutorials.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515821","Nonlinear Functional Time Series Analysis","DMS","STATISTICS","07/01/2025","06/17/2025","Alexander Aue","CA","University of California-Davis","Standard Grant","Jun Zhu","06/30/2028","$240,000.00","","aaue@ucdavis.edu","1850 RESEARCH PARK DR STE 300","DAVIS","CA","956186153","5307547700","MPS","126900","","$0.00","This research will contribute to the analysis of modern and complex data observed as functions or curves. One example of such data is provided by yield curves used in economics as an indicator for future growth, inflation and interest rate expectations, and investor sentiment. Since data of this type is often observed across time, there will be a focus on developing theoretically justified and empirically validated forecasting algorithms, which can find applications in many fields of inquiry including finance, where practitioners are interested in predicting the volatility curves of intraday tick-by-tick transaction data of financial assets. The research is therefore of immediate interest in areas of application and will further connect statistics and fields significantly relying on data-analytic tools. In addition, the research will advance mathematical and computational statistics. It will produce doctoral students, who are theoretically and practically versed in both statistics and an area of application. The training and involvement of undergraduate students is also included through regular coursework, independent study and projects. <br/><br/>This research concerns the development of a comprehensive framework for the analysis of nonlinear and possibly non-Gaussian functional time series. Such functions naturally arise in a variety of contexts such as the modeling of cumulative intraday returns of financial assets. The research aims at providing a statistical foundation to analyze functional observations that exhibit non-linearity and are possibly heavy-tailed. Key outcomes to be achieved include new probabilistic results concerning the structure of nonlinear functional time series, the introduction of two new functional time series models as lead examples of the theoretical groundwork, namely a fully general version of generalized autoregressive conditional heteroscedastic processes as well as a novel random coefficient autoregressive model. These models are accompanied by a suite of inference procedures for further statistical analysis. Achieving the goals of the research projects will require non-standard approaches, as traditional sets of assumptions for the analysis of linear functional time series are expected to be of limited use in the setting studied under this research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515716","Efficient Data Removal Under High-dimensional Asymptotics with Applications to Risk Estimation and Machine Unlearning","DMS","STATISTICS","07/01/2025","06/10/2025","Mohammad Ali Maleki","NY","Columbia University","Standard Grant","Tapabrata Maiti","06/30/2028","$240,000.00","","mm4338@columbia.edu","615 W 131ST ST","NEW YORK","NY","100277922","2128546851","MPS","126900","075Z, 079Z, 1269","$0.00","The rapid expansion of machine learning (ML) and artificial intelligence (AI) has created an urgent need for tools that enhance trust, and control over these technologies. This project aims to address that need by developing methods that help explain why a model makes certain predictions, improve model tuning, and detect harmful or misleading data?such as intentionally corrupted inputs introduced by adversaries?before they compromise the model?s reliability. A promising strategy for tackling these challenges is to analyze the impact of individual data points or subsets of data by estimating how their removal affects the model?s behavior. This research supports the national interest by advancing scientific understanding of AI, improving the robustness of decision-making systems, and contributing to the development of technologies that align with privacy protections.<br/><br/>The project investigates whether it is possible to develop computationally efficient algorithms that approximate the output of a model trained without a given subset of data, without having to retrain the model from scratch. This question is particularly challenging in high-dimensional settings, where the number of features is large relative to the sample size. The project focuses on designing data removal methods that are both scalable and theoretically sound in these regimes. The resulting algorithms will be evaluated in two important application areas: risk estimation and machine unlearning. Through this work, the project aims to lay the foundation for practical tools that improve model interpretability and accountability in complex learning systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515519","Deeper Understanding of Mean-field Models","DMS","STATISTICS","07/01/2025","06/11/2025","Sumit Mukherjee","NY","Columbia University","Standard Grant","John Kolassa","06/30/2028","$240,000.00","","sm3949@columbia.edu","615 W 131ST ST","NEW YORK","NY","100277922","2128546851","MPS","126900","","$0.00","This project addresses the critical need to approximate complex statistical models in the era of big data. The investigator aims to develop a comprehensive understanding of when simpler approximations can be effectively used, and to provide rigorous guarantees for their accuracy. The core challenge lies in the trade-off between model complexity (which is often necessary to capture the nuances of large datasets) and computational feasibility. While complex models offer rich descriptions, their computational demands can be prohibitive. The investigator proposes to explore whether simpler, approximated models can retain the essential features of their complex counterparts while significantly reducing computation time. Graduate students will be involved in this research.<br/><br/><br/>The research will concretely examine the validity of naive mean-field variational inference in several key areas where complex models are prevalent, which include (i) High-Dimensional Bayesian Regression (linear and logistic), (ii) Latent Dirichlet Allocation (LDA), (iii) Mixed Membership Models, and (iv) Exponential Random Graph Models (ERGMs). For each of these examples, the investigator plans to develop tailored inference methods, and provide rigorous, quantified bounds on the errors introduced by these approximations. This will offer a clear understanding of the trade-off between simplicity and accuracy. The overarching goal is to equip researchers using naive mean-field based variational inference with concrete guidelines on when and under what circumstances such methods can be reliably applied, and when they might fall short. This will advance the principled application of approximate inference techniques in the context of big data analytics, contributing to more efficient and reliable scientific discovery.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2531539","Collaborative Research: Bayesian Residual Learning and Random Recursive Partitioning Methods for Gaussian Process Modeling","DMS","STATISTICS, MSPA-INTERDISCIPLINARY, CDS&E-MSS","07/01/2025","06/27/2025","Li Ma","IL","University of Chicago","Standard Grant","Jodi Mead","07/31/2026","$60,290.00","","marlee@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","126900, 745400, 806900","9263, 1303, 1269, 5294","$0.00","Rare natural hazards (for example, storm surge and hurricanes) can cause loss of lives and devastating damage to society and the environment. For instance, Hurricane Katrina (2005) caused over 1,500 deaths and total estimated damages of $75 billion in the New Orleans area and along the Mississippi coast as a result of storm surge. Uncertainty quantification (UQ) has been used widely to understand, monitor, and predict these rare natural hazards.  The Gaussian process (GP) modeling framework is one of the most widely used tools to address such UQ applications and has been studied across several areas, including spatial statistics, design and analysis of computer experiments, and machine learning. With the advance of measurement technology and increasing computing power, large numbers of measurements and large-scale numerical simulations at increasing resolutions are routinely collected in modern applications and have given rise to several critical challenges in predicting real-world processes with associated uncertainty. While GP presents a promising route to carrying out UQ tasks for modern emerging applications such as coastal flood hazard studies, existing GP methods are inadequate in addressing several notable issues such as computational bottleneck due to big datasets and spatial heterogeneity due to complex structures in multi-dimensional domains. This project will develop new Bayesian GP methods to allow scalable computation and to capture spatial heterogeneity. The new methods, algorithms, theory, and software are expected to improve GP modeling for addressing data analytical issues across a wide range of fields, including physical science, engineering, medical science, public health, and business science. The project will develop and distribute user-friendly open-source software and provide interdisciplinary research training opportunities for undergraduate and graduate students.<br/><br/>This project aims to develop a new Bayesian multi-scale residual learning framework with strong theoretical support that allows scalable computation and spatial nonstationarity for GP modeling. This framework integrates and extends several powerful techniques respectively arising in the literature on GP and that on multi-scale modeling, including predictive process approximation, blockwise shrinkage, and random recursive partitioning on the domain. This framework decomposes the GP into a cascade of residual processes that characterize the underlying covariance structures at different resolutions and that can be spatially heterogeneous in a variety of ways. The new framework allows for adoption of blockwise shrinkage to infer the covariance of the residual processes and incorporates random partition priors to enable adaptivity to various spatial structures in multi-dimensional domains. New recursive algorithms inspired by wavelet shrinkage and state-space models will be developed to achieve linear computational complexity and linear storage complexity in terms of the number of observations. The resulting GP method will guarantee linear computational complexity in a serial computing environment and also be easily parallelizable. This Bayesian multi-scale residual learning method provides a new approach to addressing GP modeling issues among spatial statistics, design and analysis of computer experiments, machine learning, and nonparametric regression.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515171","Statistical Frameworks for Self-Supervised Representation Learning and Their Biomedical Applications","DMS","STATISTICS","07/01/2025","06/11/2025","Shulei Wang","IL","University of Illinois at Urbana-Champaign","Standard Grant","Jun Zhu","06/30/2028","$175,000.00","","shuleiw@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","126900","075Z, 079Z, 8038","$0.00","While recent advancements in large-scale machine learning models have shown impressive capabilities, they often rely on hundreds of millions of labeled samples. However, obtaining high-quality labels in many fields is extremely costly, so most available data remain unlabeled. For example, although millions of images and videos can be easily collected from social media platforms, manually labeling them is a tedious and time-consuming process. To address the challenge of limited labeled data, self-supervised representation learning has emerged as a promising approach in computer vision and natural language processing. It has already played a key role in the success of recent large language models. Despite its strong performance in practice, the theoretical understanding of self-supervised representation learning remains limited. Moreover, the problem of scarce labeled data also affects biomedical research, but the existing self-supervised methods cannot be directly applied due to the unique nature of biomedical datasets. This project aims to address these gaps by developing new theoretical frameworks for self-supervised representation learning, along with computational tools tailored to biomedical studies. It also includes educational efforts to engage students and the broader public with this growing area of research.<br/><br/>This project aims to advance the theoretical foundations of self-supervised representation learning and transform how unlabeled data are utilized in biomedical research. On the theoretical front, the project will investigate self-supervised learning on a low-dimensional nonlinear model, which effectively captures the invariant and intrinsic low-dimensional structure underlying observed data. Building on this nonlinear modeling framework, this project will develop a novel theory that explains the empirical success of self-supervised learning and clarifies the role of pseudo labels generated from unlabeled data. This theoretical foundation will inform the design of innovative and principled learning methodologies. On the application side, the project will integrate self-supervised representation learning into biomedical applications, including microbiome studies and omics-based longitudinal (trajectory) data. The project will develop new computational tools and software tailored to these contexts, enabling the effective use of large-scale unlabeled biomedical data. These advancements are expected to help address critical scientific questions and contribute to a deeper understanding of biological systems and human health.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2440180","CAREER: Statistical Inference in the Presence of Group Actions: Theory, Method, and Application","DMS","STATISTICS","07/01/2025","02/10/2025","Ye Zhang","PA","University of Pennsylvania","Continuing Grant","John Kolassa","06/30/2030","$90,000.00","","ayz@wharton.upenn.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","126900","1045","$0.00","In the rapidly expanding field of data science, the ability to understand group actions in data analysis is pivotal for a broad spectrum of scientific tasks. In mathematical terms, a ?group? is a collection of elements combined with an operation that links any two elements to form a third, adhering to closure, associativity, identity, and invertibility principles. A ?group action? involves applying elements of a group to another set?s elements, transforming them in structured ways, such as through rotations or reflections. These transformations are crucial in many data processing applications, including cryo-electron microscopy (cryo-EM), image registration, and multi-reference alignment. Each observation in these problems involves a common, unknown signal and an unknown group element, with the primary goal being to infer both the signal and the group elements accurately. This project aims to significantly advance statistical understanding and develop effective methodologies for handling data influenced by group actions. The wide existence of such data ensures that the progress we make towards our objectives will have a great impact not only on the statistics and machine learning community but also on a much broader scientific community, including fields such as structural biology, computer vision, and signal processing. This project will have educational outcomes that result in curriculum development, teaching, and outreach activities, including activities to K-12 students through the University of Pennsylvania Data Science Academy. The project will advance applications in image recognition and time series alignment, which have broad application in areas like medical imaging.<br/><br/>This project is structured around three main aims, each designed to tackle distinct aspects of group actions. First, the PI will improve the accuracy of orbit recovery in scenarios where the prior distributions of group elements are non-uniform, developing computationally efficient procedures that are effective under realistic conditions. Second, the PI will develop theories and methods for group synchronization problems, particularly under high noise levels and in situations with incomplete data, aiming to reduce the error of group recovery and provide entrywise inference. Third, the PI will address theoretical and computational challenges in the multi-reference alignment problem, developing procedures specifically designed for the cyclic structural nature of data, thereby enabling more precise uncertainty quantification. Together, these aims will not only enhance the theoretical understanding of and the ability to analyze group actions but also lead to the development of accurate and computationally efficient algorithms designed to tackle real-world challenges in data analysis where group actions are integral.  This research project will have impacts more broadly, in that it will result in software development and in the education of technical experts. These experts will use this software to advance applications in image recognition and time series alignment, which have broad application in areas like medical imaging.  These activities will then advance applications in image recognition and time series alignment, which have broad application in areas like medical imaging.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515104","Enhancing Reliable Data Sciences: Statistical Analysis for High-dimensional and Noisy Datasets with Complex Temporal and Nonlinear Structures","DMS","STATISTICS","07/01/2025","06/11/2025","Xiucai Ding","CA","University of California-Davis","Standard Grant","Tapabrata Maiti","06/30/2028","$175,000.00","","xcading@ucdavis.edu","1850 RESEARCH PARK DR STE 300","DAVIS","CA","956186153","5307547700","MPS","126900","1269","$0.00","This project develops new statistical and computational methods to enhance the reliability of data analysis in modern, large-scale datasets, particularly in the era of AI. As data science becomes increasingly central to fields such as finance, medicine, and engineering, there is a critical need for tools that can accurately analyze high-dimensional, noisy, and dynamically changing data. This research addresses fundamental challenges related to robustness, adaptivity, and structure in modern algorithms and statistical procedures. The work contributes to scientific advancement by strengthening the theoretical foundations of data analysis and enabling more accurate and interpretable results across complex applications. It also involves the development of open-source software to ensure broad accessibility and reproducibility and supports the training of junior researchers in advanced statistical methodology and computational techniques.<br/><br/>The project advances foundational understanding in statistics by integrating random matrix theory, manifold and deep learning, and time series analysis. It focuses on the following main areas: (1) analyzing the robustness of manifold and deep learning algorithms for high-dimensional, noisy, and nonlinear data; (2) developing statistical theory and methods for high-dimensional, nonstationary time series; and (3) combining insights from these two areas to study complex functional time series with nonlinear and temporal structures. The research examines new classes of nonlinear random matrices that arise from modern machine learning and dependent data structures. It also proposes adaptive versions of manifold learning methods and constructs new inferential frameworks suitable for time series datasets. These contributions are supported by theoretical guarantees and disseminated through interdisciplinary collaborations.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515471","Conference: 2025 Conferences for New Researchers in Statistics, Probability, and Data Science","DMS","PROBABILITY, STATISTICS","06/15/2025","06/10/2025","Armeen Taeb","WA","University of Washington","Standard Grant","Yong Zeng","05/31/2026","$39,987.00","Eardi Lila","ataeb@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981951016","2065434043","MPS","126300, 126900","7556","$0.00","The Committee on New Researchers of the Institute of Mathematical Statistics will hold its 25th conference at Vanderbilt University during the three days prior to the Joint Statistical Meeting. The event will include oral and poster presentations by new researchers, plenary talks by established researchers, and open discussions on future directions for statistics, probability, and data science. There will also be panel discussions on teaching and mentoring, publishing, funding, and collaborations.<br/><br/>The New Researchers Conference is an annual event organized under the auspices of the Institute of Mathematical Statistics by its Committee on New Researchers. It serves as the flagship meeting for early-career researchers in statistics, probability, and data science. In 2025, Vanderbilt University will host the 25th New Researchers Conference on the three days prior to the Joint Statistical Meetings. The primary objective of the conference is to provide a platform for interaction among new researchers and offer opportunities for mentorship from leaders in the field. This conference is explicitly aimed at developing the next generation of researchers in statistics and probability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2526584","Conference: Travel Awards for 14th International Conference on Bayesian Nonparametrics","DMS","STATISTICS","06/15/2025","06/10/2025","Garritt Page","UT","Brigham Young University","Standard Grant","John Kolassa","05/31/2026","$13,000.00","","page@stat.byu.edu","A-153 ASB","PROVO","UT","846021128","8014223360","MPS","126900","7556","$0.00","This award supports junior researcher travel to attend the 14th International Conference on Bayesian Nonparametrics that will be held on 23-27th, 2025 at UCLA's campus, Los Angeles, CA, USA. This conference is held biannually and is known to bring together leading experts and talented young researchers working on applications and theory of nonparametric Bayesian statistics. It is an official section meeting of the Bayesian Nonparametrics (BNP) section of the International Society for Bayesian Analysis (ISBA).  The BNP 14 meeting is also co-sponsored by the Institute of Mathematical Statistics (IMS), and it is endorsed by the Section of Bayesian Statistical Sciences of the American Statistical Association.  The main goals of the conference are to promote the interaction among scientists who develop and use BNP methods; to encourage discussions among researchers in various areas and fields of study; to foster cross-fertilization of ideas; to facilitate small group discussions among senior and junior researchers; and to disseminate results of the conference as widely as possible.  More than 250 participants are expected to attend.   BNP methods are statistical procedures that favor flexibility which permits relaxing many assumptions that are often times very rigid and/or hard to verify.  Due to this, there has been a surge in the number of fields that have begun to employ BNP methods such as genetics, archaeology, psychology, economics, neuroscience among others.    <br/><br/><br/>National Science Foundation support will enable the participation of 13 junior participants, including graduate students and postdoctoral scholars, at the conference. The International Conference on Bayesian Nonparametrics is the premier conference of BNP methods and allows weeklong discussions by including a good mix of methodological, applied, and theoretical sessions.  The topics of these sessions include BNP connected to deep learning, Bayesian density estimation, scalable nonparametric regression, hazard rate and survival function estimation (without or with covariates), estimation of spectral distribution of a time series, estimation of conditional density and density regression, classification, clustering, and estimation of the distribution of latent variables such as random effects and so on.   NSF participant support is directed to individuals with evidence of high-quality, early-career research accomplishments.  All the verified invited and contributed speakers are listed on the conference website at https://bnp14.org/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2516765","Conference: ICSA 2025 Applied Statistics Symposium","DMS","STATISTICS","06/15/2025","05/29/2025","Xiaojing Wang","CT","University of Connecticut","Standard Grant","John Kolassa","05/31/2026","$21,000.00","Ming-Hui Chen","xiaojing.wang@uconn.edu","438 WHITNEY RD EXTENSION UNIT 1133","STORRS","CT","062699018","8604863622","MPS","126900","7556","$0.00","This award will play a pivotal role in promoting advancing the field of statistics by supporting the participation of graduate students and early-career researchers in the International Chinese Statistical Association (ICSA) 2025 Applied Statistics Symposium, which will be held in Storrs, Connecticut from June 15 to June 18, 2025. With the theme of ?Insights into Complexity: Empowering Policy through Statistical Learning and Data Analytics,? the symposium will highlight the role of statistics and data science in addressing modern policy challenges. The symposium aims to bring together a global community to explore how statistical learning and data analytics can address complex decision-making in various disciplines. The symposium will unite researchers, practitioners, and students passionate about advancing statistical methods, biostatistics, data science, artificial intelligence, and their applications in various fields and foster new directions for statistical inference, facilitating discoveries from seemingly messy data to inform real-world impactful decisions.<br/><br/>The ICSA 2025 Applied Statistics Symposium will feature an engaging format, including plenary talks, invited sessions, and contributed posters. We prioritize broadening participation across all aspects of the conference, with a special focus on encouraging the active participation of early-career researchers (those who have earned their doctorates within the past five years) in invited sessions. For students, ICSA 2025 symposium offers numerous opportunities, including a student paper competition and the chance to present posters. The symposium's website, accessible at https://symposium2025.icsa.org/, serves as a hub for seamless communication and resource sharing among participants.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2512997","Conference: International Workshop on Applied Probability: Probability, Statistics and their Applications (IWAP 2025)","DMS","STATISTICS","06/15/2025","06/10/2025","Donald Martin","NC","North Carolina State University","Standard Grant","John Kolassa","05/31/2026","$15,000.00","Kimberly Sellers","demarti4@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","276950001","9195152444","MPS","126900","7556","$0.00","This award supports the participation of early career U.S.-based researchers in the ?International Workshop on Applied Probability: Probability, Statistics and their Applications (IWAP 2025),? which will take place June 9-13, 2025 in SAS Hall of North Carolina State University. In light of the many applications of probability and statistics in public policy, decision theory, social and health sciences and various scientific fields, the conference will offer a platform for advancing new statistical and mathematical methodologies and computational procedures. In addition to research presentations, there will be ample networking opportunities throughout the event, helping to facilitate the exchange of ideas. Thus, while the funding will primarily support students and participants with recent Ph.D.?s, and will help to educate and bring them to the forefront of this active and important research area, the conference will be beneficial for all of the approximately 50 to 60 workshop participants that are expected.<br/><br/>The conference will consist of eight plenary talks given by top researchers. There will also be both invited and contributed sessions. Topics that will be presented include scan statistics, graphs/networks, statistical machine learning, variable selection and quantum computing, with applications to such areas as genomics, chemistry, individualized treatment regimes in medical research, network traffic, economic forecasting, medical imaging, cluster, anomaly and change point detection, and forecasting tourist arrivals. See https://iwap2025.weebly.com/ for more information about IWAP 2025.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2529893","Conference: Southern Regional Council on Statistics Summer Research Conference","DMS","STATISTICS","06/15/2025","06/10/2025","Katherine Thompson","KY","University of Kentucky Research Foundation","Standard Grant","John Kolassa","11/30/2025","$40,000.00","Gregory Hawk","katherine.thompson@uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","126900","7556, 9150","$0.00","The 60th Summer Research Conference (SRC) and Statistics Undergraduate Research Experience (SURE) will be held in Jekyll Island, Georgia on June 9-11, 2025. The Southern Regional Council on Statistics (SRCOS) is a consortium of statistics and biostatistics programs from universities in 16 states in the Southern region. The SRC is an annual conference sponsored by the SRCOS. The purpose of the SRC is to encourage the interchange and mutual understanding of current research ideas in statistics and biostatistics, and to provide motivation and direction to further research progress. The SRC will give new researchers an opportunity to participate in the meeting and to interact closely with leaders in the field in a manner not possible at larger meetings. In addition to the graduate student participation, the 60th SRC will also include the 6th annual Statistical Undergraduate Research Experience (SURE) from June 9-11, 2025. SURE is a conference within a conference aimed to encourage the participation of undergraduate students to pursue graduate education and career opportunities in STEM fields. SURE will include events specifically for undergraduate students and undergraduate mentors, such as a panel about career opportunities in statistics, a real data analytics workshop, and a speed-mentoring session with current statistics and biostatistics graduate students.<br/> <br/>The SRC is particularly valuable for graduate students, isolated statisticians, and faculty from smaller regional schools in the southern region at drivable distances without the cost of travel to distant venues. Speakers will present formal research talks with adequate time allowed for clarification, amplification, and further informal discussions in small groups.  Under the travel support provided by this award, graduate students will attend and present their research in posters to be reviewed by more experienced researchers. Participation in SURE will encourage undergraduate students to enter STEM fields, including statistics or biostatistics, and provide training to support this endeavor. The 60th SRC will strengthen the research of the statistics and biostatistics community as a whole and help bridge the gap for undergraduate students to pursue statistics or biostatistics, particularly in the sixteen states of the Southern Region. The SRCOS website can be found here: https://www.srcos.org; the SRC website can be found here: https://www.srcos.org/conference; the SURE website can be found here: https://www.srcos.org/sure.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2513573","Conference: STATGEN25","DMS","STATISTICS","06/15/2025","05/22/2025","Sandra Safo","MN","University of Minnesota-Twin Cities","Standard Grant","John Kolassa","05/31/2026","$19,104.00","Kelsey Grinde","ssafo@umn.edu","2221 UNIVERSITY AVE SE STE 100","MINNEAPOLIS","MN","554143074","6126245599","MPS","126900","7556","$0.00","The 2025 Conference on Statistics in Genomics and Genetics (STATGEN 2025) will be held at the University of Minnesota in Minneapolis, MN from May 21 to May 23, 2025. The conference will bring together researchers, including students, postdoctoral researchers, and early career scientists, to share ideas and collaborate in the rapidly evolving fields of genetics and data science. This event is an invaluable opportunity for early-career researchers to connect with experts, build professional networks, and gain insight from cutting-edge scientific work. By supporting the participation of students and early career researchers, the conference will help cultivate the next generation of scientists who will contribute to advancing knowledge and innovations in this important area of research.<br/><br/>Building on the success of the inaugural meeting, the conference will expose attendees to cutting-edge research and methods in genomics, statistics, and data science. This event will contribute to ongoing dialogue and innovation in genetics and genomics and the integration of statistical methods into genomic research, supporting the development of future breakthroughs in areas such as personalized medicine and disease prevention.  Special emphasis will be placed on fostering the collaboration and participation of early-career researchers, with travel assistance provided by the NSF grant. By facilitating broad participation, the conference will help shape the next generation of researchers and leaders in the field of genomics and data science.  The conference will feature a variety of presentations, including keynotes, panels, invited, contributed, speed, and poster sessions, offering attendees the platform to explore the latest research and discoveries.  Attendees will discuss and promote the latest advancements in statistical theory, methods, and tools, particularly in genetics and genomics, aimed at making evidence-based statistical inferences from complex, noisy, and high dimensional data sources.  More information may be found athttps://www.sph.umn.edu/events-calendar/statgen-2025.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2527500","Conference: International Indian Statistical Association Annual Conference 2025","DMS","GVF - Global Venture Fund, STATISTICS","06/01/2025","08/13/2025","Souparno Ghosh","NE","University of Nebraska-Lincoln","Standard Grant","John Kolassa","12/31/2025","$23,845.00","","sghosh5@unl.edu","2200 VINE ST # 830861","LINCOLN","NE","685032427","4024723171","MPS","054Y00, 126900","7556, 9150, 5983, 075Z, 079Z","$0.00","This project supports the International Indian Statistical Association (IISA) conference, 2025 - a four-day international conference that will take place at University of Nebraska-Lincoln, Lincoln, Nebraska from June 12 to June 15, 2025. This conference is the official annual meeting of the International Indian Statistical Association (IISA). It provides a platform to exchange ideas and showcase theoretical, methodological, and application of Statistics and Data Science across many scientific domains. The conference features plenary sessions, invited talks, panel discussions and workshops that will provide attendees with invaluable insight into real-world applications of Statistics and Data Science. It will promote education, research, and foster exchange of information and scholarly activities thereby facilitating the emergence of new ideas that will guide future research directions. The conference will also host student paper and poster competitions which will highlight the best research among emerging scholars.<br/><br/>IISA 2025 spans topics from statistical theory to advanced computational methods and application-focused modeling of complex data. It will bring together leading experts and emerging scholars in statistics, biostatistics, probability, data science and offer a forum to discuss recent progress in statistical theory and data science. The conference will provide a valuable opportunity to nurture emerging talents and foster collaboration. It has two hands-on workshops, multiple panel discussions, student paper and poster competitions that will make the experience engaging and impactful, leaving attendees better equipped to thrive in their research journeys. Plenary talks and special invited talks will be given by leading experts in Probability, Statistics, Machine Learning, and Data Science. Junior researchers will have the opportunity to present in invited sessions. Doctoral students will also have the opportunity to give oral presentations on a competitive basis. The official website of IISA 2025 is https://www.intindstat.org/conference2025/index.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2443410","CAREER: Extending the reach of empirical Bayes: Calibration, nuisance parameters, likelihood asymptotics, and machine learning","DMS","STATISTICS","06/01/2025","03/10/2025","Nikolaos Ignatiadis","IL","University of Chicago","Continuing Grant","Tapabrata Maiti","05/31/2030","$75,343.00","","ignat@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","126900","1045","$0.00","Scientists across various fields face the challenge of answering numerous related questions with limited or noisy data. For example, genomicists may need to assess thousands of genes using data from only a few subjects, while survey statisticians might analyze average incomes in many towns based on limited surveys. To address these challenges, researchers often borrow information from related questions, a process that requires sophisticated statistical reasoning due to varying underlying characteristics. Empirical Bayes offers a method to enhance individual question inference by sharing information, potentially improving statistical accuracy. However, it relies on strong modeling assumptions, limiting its applicability. This research aims to make empirical Bayes more powerful and accessible by integrating it with modern machine learning and causal inference, demonstrating its effectiveness under fewer assumptions, and developing methods to assess uncertainty more effectively. These advancements will help practitioners utilize empirical Bayes in various scientific and industry contexts without extensive statistical modeling. Additionally, the project will produce a monograph and provide training for students and preceptors on modern data science challenges using empirical Bayes.<br/> <br/> <br/>This research aims to advance empirical Bayes by developing new methodologies and statistical theories to address four key limitations: the assumption of known likelihoods, the treatment of nuisance parameter heterogeneity, the development of nonparametric inference methods, and the integration with machine learning. The project will create new inference methods for primary parameters by using empirical partially Bayes methods and Bayesian nonparametrics, enhancing frequentist Bayes multiple testing theory with guarantees like false discovery rate control. Additionally, the research seeks to learn the unknown Bayes rule through neural networks with a specialized loss function, incorporate James-Stein shrinkage for combining unbiased estimates with semisupervised machine learning, and integrate empirical partially Bayes inference with doubly robust double machine learning for large-scale causal inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2514925","Conference: The Past, Present, and Future of Statistics in the Era of AI","DMS","STATISTICS","06/01/2025","05/22/2025","Tatiyana Apanasovich","DC","George Washington University","Standard Grant","John Kolassa","05/31/2026","$18,400.00","Xiaoke Zhang, Subrata Kundu","apanasovich@gwu.edu","1918 F ST NW","WASHINGTON","DC","200520042","2029940728","MPS","126900","7556","$0.00","The Department of Statistics at George Washington University will host the conference ?The Past, Present, and Future of Statistics in the Era of AI? from May 8-10, 2025, in Washington, DC. This event will explore the evolving relationship between statistics and artificial intelligence (AI), examining its growing impact on scientific research, education, and society. The conference will explore AI?s influence on statistical research and its role in shaping the future of data-driven decision-making. The program will feature technical sessions, short courses, panel discussions, and poster presentations, providing a platform for knowledge exchange and interdisciplinary collaboration. The event will also include plenary talks from leading experts in the field, offering insights into cutting-edge developments and future directions. Additionally, we have invited practitioners from both academia and industry to attend and participate in panel discussions, ensuring a broad range of perspectives. A key priority of the conference is to promote the active participation of junior scholars, including as speakers and presenters, providing them with opportunities to engage with experts and showcase their work. <br/><br/><br/>The conference contributes to statistical science by facilitating dialogues among experts, inspiring new research directions, and promoting advancements in statistical methodologies and AI applications. In particular, discussions will focus on scalable data analysis, model interpretability, causal inference, and responsible AI. Key topics include uncertainty quantification through probabilistic modeling, Bayesian methods for AI-driven decision-making, and fairness and bias correction in machine learning. The program will also address reinforcement learning for dynamic treatment regimes and privacy-preserving statistical methods for large-scale datasets. The conference will serve as a hub for knowledge sharing, highlighting the evolving role of AI in statistical research and its broader implications for innovation and societal progress. More details about the event, including registration and program updates, can be found at https://statistics.columbian.gwu.edu/GW-STAT-90.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2516758","Conference: EVA 2025: The 14th International Conference on Extreme Value Analysis","DMS","STATISTICS","06/01/2025","05/22/2025","Vladas Pipiras","NC","University of North Carolina at Chapel Hill","Standard Grant","John Kolassa","11/30/2025","$20,000.00","Richard Smith, Mariana Olvera-Cravioto","pipiras@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","126900","7556","$0.00","The project will provide financial support for junior researchers to attend the 14th International Conference on Extreme Value Analysis (EVA 2025) to be organized and hosted at the University of North Carolina at Chapel Hill from June 23 to June 27, 2025. Extreme value analysis refers to the statistical modeling of extreme data observations, allowing to make probabilistic predictions beyond the range of observed data. With the mathematical foundation going back to the mid 20th century, the scope of EVA expanded considerably over the last decades, including applications in insurance, finance, engineering, atmospheric science, and other domains. The EVA conferences are bi-annual, and they attract some of the most prominent researchers in statistics and probability, as well as more applied domains working on extremes. The supported junior researchers will have opportunities to attend a satellite workshop with tutorials, to present their work in oral or poster sessions, and to participate in a data challenge and best student paper competition.  <br/><br/><br/>EVA started with univariate models and frequentist methods for independent and identically distributed observations, but has since seen extensions to time series, spatial and spatial-temporal data, Bayesian methods, and so on. It has had a profound impact on a wide range of applications in atmospheric science (droughts, floods), finance (value-at-risk, market crashes), engineering (reliability, rare event prediction), social networks (viral tweets) and other domains. Over the recent past, the field's impact has grown by making and exploiting connections to high-dimensional statistics (graphical models, dimension reduction), machine learning (prediction of extremes) and AI (training of neural networks) methods. These various themes, both classical and modern, will be well represented at the conference EVA 2025. The conference website can be found at https://eva2025.unc.edu.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2534466","Wasserstein guided nonparametric Bayes","DMS","STATISTICS","05/15/2025","05/21/2025","Debdeep Pati","WI","University of Wisconsin-Madison","Standard Grant","Tapabrata Maiti","06/30/2027","$299,669.00","","dpati2@wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","126900","1269","$0.00","Stochastic generative models are a cornerstone of applied statistical modeling and inference. A generative model is an abstraction, and often a simplification, of a data generating mechanism using probabilistic tools, where specific features of interest regarding the generating mechanism are encapsulated into parameters of the generative model. Bayesian statistical inference is a popular statistical paradigm for combining such generative models for data with prior information about model parameters in a principled fashion to perform statistical inference on the unknown parameters. Some of the salient aspects behind the tremendous growth in popularity of Bayesian inference  include principled incorporation of domain information, an in-built penalty for model complexity allowing automatic model selection, and facilitating borrowing of information across different domains via hierarchical modeling. However, being inherently model-based, Bayesian statistics is intrinsically susceptible to departures from the postulated generative model.  Through this project, the investigators will explore and develop new statistical methodology for performing Bayesian inference allowing flexible departures from the generative model under consideration. A major focus will be the user-friendliness of the proposed approaches, circumventing the need for a user to explicitly build probabilistic models of increasing richness. The research will be disseminated through articles at prominent avenues and research presentations. Additionally, software packages for the methods developed will be made available publicly. The investigators are committed to enhancing the pedagogical component of the proposal through advising students and developing graduate and undergraduate topic courses.<br/><br/>Flexible nonparametric Bayesian methods have gained in popularity to address perceived issues of traditional Bayesian modeling regarding model-misspecification. The last thirty years have seen a proliferation of such methods, both in mainstream statistics as well as the machine learning community, as we continue to encounter increasing levels of complexities in modern datasets. However, nonparametric Bayesian methods can be challenging to implement as well as interpret. Furthermore, in many applications, the targets of interest are quite simple and it is essentially futile to model all aspects of the data. The fundamental aim of the proposed research is to develop a flexible Bayesian non-parametric approach that retains the generative modeling aspect of traditional parametric Bayesian modeling while avoiding a complete probabilistic specification of the data generating mechanism as typically performed in nonparametric Bayesian modeling. This will be performed by defining a modified likelihood function, leveraging ideas from the empirical likelihood literature as well as optimal transport theory, that centers around a user-specified parametric family of densities. An automated calibration procedure will be developed to control the extent of centering around the parametric model. The investigators will offer a firm theoretical underpinning of the proposed procedure and develop computationally efficient algorithms to carry out inferential tasks. The developed methods will be applied to scientific learning problems in neuroscience and nuclear physics to allow departures from existing scientific models in situations where their operating characteristics are less understood.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2440824","CAREER: High-Dimensional Learning and Inference from Heterogeneous Data Sources","DMS","STATISTICS","05/01/2025","07/03/2025","Pragya Sur","MA","Harvard University","Continuing Grant","Tapabrata Maiti","05/12/2025","$88,793.00","","pragya@fas.harvard.edu","1033 MASSACHUSETTS AVE STE 3","CAMBRIDGE","MA","021385366","6174955501","MPS","126900","1045","$0.00","This project will develop novel statistical theories and methods for handling large, heterogeneous datasets. Modern scientific applications often produce heterogeneous data of different types for the same problem. For instance, a single-cell biologist may observe multiple types of sequencing data from diverse instruments, all relevant for understanding the biological pathways of a single complex disease.  The challenge lies in effectively combining these different data types to build statistical pipelines that outperform those developed using any one data type. Traditional statistical approaches struggle with this challenge. This project will establish a new statistical paradigm to address the complexities of such heterogeneous data while accounting for datasets with billions of variables. The project outcomes will facilitate principled prediction and inference in applications ranging from single-cell biology to precision health and neuroimaging. The project will involve graduate student participation and the development of new curricula at graduate and undergraduate levels that incorporate the project outcomes. Additionally, the research will engage medical professionals to facilitate the dissemination of the research products in current biomedical practice.<br/><br/><br/>This project will develop a modern statistical framework to address data heterogeneity in high dimensions, focusing on three key sub-themes: (i) creating principled and robust prediction strategies for multi-view learning, (ii) developing new inference pipelines and prediction analysis frameworks for meta-learning, and (iii) introducing novel inference methods for low-dimensional functionals under transfer learning. In multi-view learning, this project will quantify optimal strategies for cooperative learning, devise new adversarial learning techniques, and analyze the effects of interpolation learning.  In meta-learning, this project will introduce new debiasing strategies to tackle inference questions that arise during fine-tuning following an initial phase of pre-training.  In transfer learning, this project will develop general-purpose strategies for ranking source distributions and establish new inference schemes for low-dimensional functionals of scientific relevance. On the technical front, this project will introduce novel comparison inequalities, algorithmic proof methods, and leave-one-out techniques that effectively capture the interplay between high dimensionality and heterogeneity.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2441652","CAREER: Distributional Approximation for Sharp Finite Sample Bounds with Applications to Dependent Data and Complex Estimators","DMS","STATISTICS","05/01/2025","07/03/2025","Morgane Austern","MA","Harvard University","Continuing Grant","Yong Zeng","05/12/2025","$82,562.00","","maustern@fas.harvard.edu","1033 MASSACHUSETTS AVE STE 3","CAMBRIDGE","MA","021385366","6174955501","MPS","126900","079Z, 1045","$0.00","AI and machine learning algorithms are transforming numerous scientific fields, with some of the most promising approaches relying on mathematical tools called ""finite sample probability bounds."" These bounds are crucial, for example, in reinforcement learning, which underpins the success of systems like AlphaGo. Additionally, they play a key role in uncertainty quantification and the theoretical analysis of black-box machine learning algorithms. However, classical finite sample bounds have a significant limitation: they are often overly conservative. This conservatism leads to underperforming algorithms and unnecessarily loose guarantees. This project is built around a novel yet straightforward idea: finite-sample bounds can be derived from infinite-sample results. By leveraging recent breakthroughs in optimal transport and probability theory, the project aims to develop a new method for deriving such inequalities. These methods will be applied to problems such as online data-driven decision-making, early stopping rules, and machine learning for multiscale physical models. The PI will interweave their research and teaching throughout the research period and beyond. In particular, the PI will provide research training opportunities to graduate students and develop undergraduate and graduate courses, with course materials made publicly available and with joint participation from industry.<br/><br/>Classical concentration inequalities, such as the ones derived by Hoeffding or Bernstein, are over-conservative, are regularly inadequate for heavy-tail distributions, and often rely on the assumption of independence. In this project, the PI tackles those limitations from a new angle, starting with an infinite-sample result for a given problem, such as a central limit theorem, and translating it into a finite-sample result for the same problem by using the concept of distributional approximations. The advantage of this novel proof method is threefold: Firstly, the derived bounds improve as the sample size grows. This leads to inequalities that are considerably tighter than the classical ones. Secondly, limit theorems often hold for many forms of dependent data. This opens a more promising path to derive finite sample probability bounds than conventional Chernoff-based techniques. Lastly, by extending this approach to non-Gaussian limits, the PI develops finite sample concentration inequalities for heavy-tailed statistics. This project will (1) develop a completely novel method for obtaining concentration inequalities, as well as new results for transport distances and optimal transport, (2) provide machine learning theorists with a new set of powerful probability tools for obtaining high-probability guarantees for high-dimensional estimators and dependent and structured data, and (3) provide tighter tail bounds which will lead to algorithmic improvements and improved uncertainty quantification.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2506882","Conference: Enriching Statistical Inference with Artificial Intelligence","DMS","STATISTICS","03/01/2025","02/18/2025","Faming Liang","IN","Purdue University","Standard Grant","John Kolassa","02/28/2026","$22,500.00","Chuanhai Liu, Xiao Wang","fmliang@purdue.edu","2550 NORTHWESTERN AVE # 1100","WEST LAFAYETTE","IN","47906","7654941055","MPS","126900","7556","$0.00","The conference ""Enriching Statistical Inference with Artificial Intelligence"" will be held at Purdue University May 12-14, 2025.  During the past decade, deep learning has revolutionized data science, with transformative applications in fields such as computer vision, protein structure prediction, and natural language processing. These advancements underscore the immense potential of deep neural networks (DNNs) while revealing a gap in the statistical understanding of their mechanisms and successes. This conference seeks to address the gap by fostering a vibrant platform for exchanging ideas, advancing statistical theories to illuminate DNN performance, developing innovative artificial intelligence (AI) tools, and promoting interdisciplinary collaborations that harness the power of AI to solve real-world problems. This conference will significantly enrich statistical inference with AI, while also contributing to the evolution of AI by improving its robustness, interpretability, and uncertainty quantification.  This improved inference will then impact society broadly by improving the experience of users interacting with the products of AI.  <br/><br/>This conference will delve into cutting-edge research at the intersection of AI and statistical inference. Key topics will include investigating fundamental phenomena in deep learning, such as benign overfitting, and developing robust methods for uncertainty quantification in DNN models. The conference will also emphasize practical applications, leveraging DNNs to address foundational scientific challenges like causal inference and variable selection in complex systems. Participants will acquire state-of-the-art AI tools to tackle the complexities of contemporary data science while contributing to the advancement of modern statistical theory.  More information about this conference may be found at https://www.stat.purdue.edu/news/2024/bff9.html.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2515675","Conference: SDSU Data Science Symposium","DMS","STATISTICS","02/15/2025","02/11/2025","Frederick Boehm","SD","South Dakota State University","Standard Grant","John Kolassa","07/31/2025","$7,502.00","Semhar Michael","Frederick.Boehm@sdstate.edu","1015 CAMPANILE AVENUE","BROOKINGS","SD","570070001","6056886696","MPS","126900","9150, 7556","$0.00","The Department of Mathematics and Statistics at South Dakota State University will host its seventh annual Data Science Symposium (February 6 & 7, 2025) in Brookings, South Dakota. This regional conference will assemble practitioners and researchers from many branches of data science, including statistics, mathematics, medicine and public health, precision agriculture, finance, forensic science, and others to exchange ideas, explore employment opportunities, and build collaborations. National Science Foundation funding will enable participation by students and early career attendees. The symposium is a highlight among conferences in the upper Midwest.<br/><br/>Parallel session tracks will feature speakers from diverse fields such as statistics, mathematics, computer science, healthcare, finance, forensics, precision agriculture, and other data science-related topics. The symposium facilitates networking, collaborations, and exposes students to various career paths within mathematics, statistics, computer science, and other STEM areas. The symposium aims to 1) bring unique opportunities to the Midwest region, where students, faculty, business leaders of the region, and practitioners all gather to discuss the applications and foundations of data science; 2) provide hands-on, four-hour-long instructions on emerging topics/tools used in data science; 3) host presentations covering foundational and use-case aspects of data science and garnering interactive discussions and future collaborations; 4) expand networks during the career fair and exhibit sessions, connecting faculty, students, and hiring managers of companies in the area. The three keynote presentations feature experts in data science in industry and academia and highlight the roles of artificial intelligence in modern data science. The conference web site is https://openprairie.sdstate.edu/datascience_symposium/2025/ .<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2523484","Collaborative Research: Planning: FIRE-PLAN: Advancing Wildland Fire Analytics for Actuarial Applications and Beyond","DMS","STATISTICS, Human-Envi & Geographical Scis, HDBE-Humans, Disasters, and th, Cross-BIO Activities, Info Integration & Informatics","02/01/2025","02/13/2025","Yuzhou Chen","CA","University of California-Riverside","Standard Grant","Jun Zhu","09/30/2025","$102,687.00","","yuzhou.chen@ucr.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","92521","9518275535","MPS","126900, 141Y00, 163800, 727500, 736400","132Z","$0.00","The impacts of uncontrolled wildland fires range from the destruction of native vegetation to property damages to long-term health effects and losses of human lives. Increasing accuracy in projections of wildland fire activity, fire behavior, and wildland fire weather is the key toward developing more efficient fire control strategies and reducing the risks of wildfires. Recent studies have demonstrated that the tools of artificial intelligence (AI) can help in planning for upcoming prescribed burns by providing higher spatial and temporal fire weather forecasts and can also assist in developing more efficient strategies for wildfire risk mitigation. However, the modeling tools that are currently used to predict fire activity are largely subject to a number of temporal or spatial constraints. For instance, most deep learning (DL) approaches for wildfire risk analytics tend to be restricted in their capabilities to systematically capture the multidimensional information recorded at disparate spatio-temporal resolutions. Furthermore, such DL architectures are inherently static and do not explicitly account for complex dynamic phenomena, which is often the key behind the accurate assessment of wildfire driving factors. Finally, these models primarily rely on supervised learning approaches where a large number of task-specific labels (e.g., fire or no fire) are needed. To address these challenges in wildfire risk analytics, this project will leverage inherently interdisciplinary approaches at the interface of Earth system sciences, DL, computational topology, statistics, and actuarial sciences. <br/><br/>The project aims to introduce the concepts of topological data analysis (TDA) to wildfire predictive modeling, coupling them with such emerging AI machinery as time-aware graph neural networks. The resulting new methods are expected to better capture the shape patterns in the wildland fire processes with respect both to time and space and to assist in a more reliable statistical assessment of wildfire risks. The new high-fidelity predictive approaches will have the potential to deliver forecasts of fire behavior, fire activity, and fire weather at multiple spatial and temporal scales under scenarios of limited, noisy, or nonexistent labeled information. To enhance the utility of the research solutions in wildfire analytics, the researchers in this project will work in close collaboration with stakeholders, particularly, focusing on the insurance sector. The project will provide multiple interdisciplinary training opportunities at the nexus of wildfire sciences, AI, and mathematical sciences at all educational levels, from undergraduate students to practicing actuaries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2516872","Minipatch Learning for Selection, Stability, Inference, and Scalability","DMS","STATISTICS","02/01/2025","02/05/2025","Genevera Allen","NY","Columbia University","Standard Grant","Yong Zeng","07/31/2026","$195,479.00","","genevera.allen@columbia.edu","615 W 131ST ST","NEW YORK","NY","100277922","2128546851","MPS","126900","068Z, 8091, 079Z","$0.00","Massive amounts of data are now collected by nearly every industry and academic discipline. Uncovering the hidden insights in such data holds the key to major scientific challenges such as understanding how the brain works, discovering mechanisms leading to diseases such as cancer and Alzheimer's disease, and combating climate change, among many others. But discovering key features and important relationships in complex and huge data poses major statistical and computational challenges. The investigator aims to develop new statistical machine learning approaches and theory for this task that break up huge data sets into small random subsets called minipatches to facilitate both faster computation and improved statistical efficiency. The new methods will be implemented in open-source software and applied to huge biomedical datasets in genomics and neuroscience. The project will provide undergraduate and graduate students training and professional development opportunities.<br/><br/>Discovering key features and important relationships in complex and huge data commonly found in biomedicine poses not only major computational challenges but also critical statistical challenges. To tackle these challenges, the investigator plans to develop a new framework termed minipatch learning. Inspired by the successes of random forests, stability approaches in high-dimensional statistics, and stochastic optimization strategies, the investigator will build ensembles from many random tiny subsets of both observations and features or variables called minipatches. While ensemble learning strategies are commonly used in supervised machine learning, the investigator will use minipatch learning for the tasks of feature selection, model-agnostic inference for feature importance, and learning relationships amongst features through graphical models. The approach, which trains on very tiny subsets of the data, is expected to have dramatic computational and memory savings. The investigator aims to show both theoretically and empirically that such a strategy poses significant statistical advantages as well.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2500874","Conference: Workshop on Experimental Designs in the Age of Artificial Intelligence","DMS","STATISTICS","01/15/2025","01/03/2025","Jingshen Wang","CA","University of California-Berkeley","Standard Grant","John Kolassa","12/31/2025","$20,610.00","","jingshenwang@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","126900","7556","$0.00","The workshop ""Advancements in Experimental Design in the Era of AI"" will be held from March 7-9, 2025, at the UC Berkeley campus Alumni House. This workshop aims to unite experts from various disciplines to develop and discuss recent advancements in experimental designs that combine classical approaches with cutting-edge artificial intelligence (AI)-driven techniques. In today's data-driven world, understanding cause-and-effect relationships is essential across fields such as healthcare, social policy, and industry, and randomized experiments serve as the gold standard for establishing causal relationships by systematically testing the effectiveness of treatments, interventions, or policies. Well-designed experiments not only yield reliable insights but also reduce costs, accelerate outcomes, and enhance public benefits. However, designing experiments that meet the complex needs of different fields, each with unique challenges and data requirements, is a significant task. While traditional experimental designs have been successful for decades, recent advancements in data collection and AI potentially offer new opportunities to enhance experimental efficiency and insights. This workshop aims to foster collaboration among researchers across different fields to create new experimental design strategies suitable for today's complex data environments.<br/><br/>The workshop aims to address the need for a unified approach to experimental design by bridging classical design of experiments (DoE) and modern adaptive methodologies, including reinforcement learning and AI-assisted designs. Classical DoE has been foundational in manufacturing, engineering, and quality control, emphasizing optimized balance and limited sample sizes. However, recent applications in clinical trials and digital platforms may require more adaptive approaches that dynamically adjust based on accruing data. These modern adaptive strategies ? such as response-adaptive randomization, enrichment designs, micro-randomization, and multi-arm bandits ? offer enhanced statistical efficiency and personalization but necessitate tailored statistical frameworks and causal inference methods. Despite their potential, the application of modern designs has been hindered by limited cross-disciplinary dialogue and implementation guidance. This workshop will convene experts in statistical design, biostatistics, econometrics, political science, and industry to foster interdisciplinary innovation in experimental methodologies. Objectives include fostering knowledge exchange across fields, advancing the integration of adaptive and classical designs, and applying AI tools to optimize experimental processes. By addressing practical challenges and promoting collaboration, the workshop aims to advance experimental design theory and practice, leveraging AI to tackle the complex data landscapes of modern research and industry applications.<br/><br/>For more information, please visit the workshop website at: https://www.design-ai.site/Berkeley-2025/ .<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2436216","Collaborative Research: FDT-BioTech: Advancing Mathematical and Statistical Foundations to Enhance Human Digital Twin of Neurophysiological Modeling and Uncertainty Quantification","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, Engineering of Biomed Systems, CYBERINFRASTRUCTURE","01/01/2025","08/20/2024","Huixia Wang","DC","George Washington University","Standard Grant","Zhilan Feng","12/31/2027","$549,341.00","Chung Hyuk Park","judywang@gwu.edu","1918 F ST NW","WASHINGTON","DC","200520042","2029940728","MPS","125300, 126900, 534500, 723100","075Z, 079Z, 1269, 8038","$0.00","This project aims to develop the mathematical foundations for a digital twin (DT) system for individuals with autism spectrum disorder (ASD), focusing on dynamic modeling, prediction, uncertainty quantification, and treatment or intervention recommendation through DT-based optimization. ASD is characterized by challenges in social interaction, communication, and behavior, such as difficulties in forming relationships, understanding nonverbal cues, speech development, repetitive behaviors, and sensory sensitivities. The project will create a unified system integrating clinical and neuro-developmental data, analyzed using a DT healthcare paradigm. The DT technology will enable individualized models, and its predictive capabilities will allow healthcare providers to anticipate progression and adjust treatment or intervention proactively. Additionally, the continuous feedback loop from real-time data will enhance therapeutic outcomes. The developed methods and theories will have broader applicability to other medical areas, improving healthcare efficiency, reducing system burdens, and informing public health strategies. This will ultimately enhance care and promote community well-being. The project will also develop quality cyberinfrastructure to share algorithms, data, and open-source software with the community. Furthermore, the investigators plan to expand scientific impacts through collaborating with medical experts and industry scientists, training undergraduate and graduate students, and integrating research findings into course development.<br/><br/>The project will develop a DT framework by modeling brain activities with a unified data structure, linked to behavioral characteristics and interventions aligned with individuals' neuro-developmental processes. This system will integrate multimodal and multi-source data related to human health and development. It will establish foundational models for training and generating synthetic data from DT models, enabling personalized predictions of progression and uncertainty quantification through novel interdisciplinary approaches. The DT system consists of four research modules: (1) Develop computational models based on conditional variational auto-encoders (CVAE) and longitudinal CVAE to analyze brain activities, integrate diverse imaging data, and model neurodevelopmental processes. (2) Create a novel bilevel formulation for multi-distribution fine-tuning techniques on pretrained foundational models and a fast algorithm to learn from heterogeneous data sources to predict ASD outcomes. (3) Develop a model-free conformal prediction procedure to ensemble predictions from multiple models obtained with different modalities and progression simulations, integrating various types of uncertainties into one framework. (4) Develop a DT-based reinforcement learning framework to recommend personalized treatment/intervention plans that significantly improve online learning efficiency and clinical outcomes. The project will address challenges such as multimodality and multi-source data, high-dimensional features, dynamic progression of ASD symptoms, brain functional connectivity, and the need for personalized intervention or treatment recommendations and uncertainty quantification.<br/><br/>This project is jointly funded by the Division of Mathematical Sciences, the OAC Cyberinfrastructure for Sustained Scientific Innovation (CSSI) program, and the CBET Engineering of Biomedical Systems program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2526477","Collaborative Research: Statistical Inference for Multivariate and Functional Time Series via Sample Splitting","DMS","STATISTICS","01/01/2025","04/02/2025","Xiaofeng Shao","MO","Washington University","Standard Grant","John Kolassa","06/30/2026","$146,738.00","","shaox@wustl.edu","1 BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","126900","1269","$0.00","Multivariate and functional time series are prevalent and routinely collected in many fields. Statistical inference of such time series is a fundamental problem in modern time series analysis and has broad applications in many scientific areas, including bioinformatics, business, climate science, economics, finance, genetics, and signal processing. Compared with existing methodologies, this research project will provide nonparametric inference procedures that can accommodate a wide range of dimensionality and require weak assumptions on the data generating processes. The methodology ensuing from the project will be disseminated to the relevant scientific communities via publications, conference and seminar presentations, and the development of open-source software. The project will involve multiple research mentoring initiatives, including efforts on broadening participation, and will offer advanced topic courses to introduce the state-of-the-art techniques in time series analysis. The project will provide a broad range of interdisciplinary training opportunities at all educational levels and will contribute to the future workforce professional development.<br/><br/>The project will develop a systematic body of methods and theory on inference for both multivariate (including high-dimensional) time series and functional time series based on sample splitting (SS) and self-normalization (SN). Recently, the SN technique has been advanced to the inference of high-dimensional time series, but it requires the use of a trimming parameter. Also, its scope of applicability is limited to high-dimensional time series with weak panel dependence which might be unrealistic in many modern time series applications. In turn, the existing SN for functional time series relies on dimension reduction by functional principal component analysis and, hence, the resulting procedure may be powerless when the alternative is orthogonal to the space spanned by the top principal components used in the procedure. To address these major limitations, this project will develop a new unified framework based on SS-SN, in conjunction with inference for multivariate and functional time series, and investigate its utility in application to analysis of time series of low, medium, high or infinite dimensions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2524356","Collaborative Research: Statistical Modeling and Inference for Object-valued Time Series","DMS","STATISTICS","01/01/2025","02/18/2025","Xiaofeng Shao","MO","Washington University","Standard Grant","Jun Zhu","06/30/2027","$174,344.00","","shaox@wustl.edu","1 BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","126900","","$0.00","Random objects in general metric spaces have become increasingly common in many fields. For example, the intraday return path of a financial asset, the age-at-death distributions, the annual composition of energy sources, social networks, phylogenetic trees, and EEG scans or MRI fiber tracts of patients can all be viewed as random objects in certain metric spaces. For many endeavors in this area, the data being analyzed is collected with a natural ordering, i.e., the data can be viewed as an object-valued time series. Despite its prevalence in many applied problems, statistical analysis for such time series is still in its early development. A fundamental difficulty of developing statistical techniques is that the spaces where these objects live are nonlinear and commonly used algebraic operations are not applicable. This research project aims to develop new models, methodology and theory for the analysis of object-valued time series. Research results from the project will be disseminated to the relevant scientific communities via publications, conference and seminar presentations. The investigators will jointly mentor a Ph.D. student and involve undergraduate students in the research, as well as offering advanced topic courses to introduce the state-of-the-art techniques in object-valued time series analysis.<br/><br/>The project will develop a systematic body of methods and theory on modeling and inference for object-valued time series. Specifically, the investigators propose to (1) develop a new autoregressive model for distributional time series in Wasserstein geometry and a suite of tools for model estimation, selection and diagnostic checking; (2) develop new specification testing procedures for distributional time series in the one-dimensional Euclidean space; and (3) develop new change-point detection methods to detect distribution shifts in a sequence of object-valued time series. The above three projects tackle several important modeling and inference issues in the analysis of object-valued time series, the investigation of which will lead to innovative methodological and theoretical developments, and lay groundwork for this emerging field.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."

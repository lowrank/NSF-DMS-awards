"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1419754","Inference After Predictor Selection","DMS","STATISTICS","09/01/2013","01/27/2014","BERTRAND CLARKE","NE","University of Nebraska-Lincoln","Standard Grant","Gabor Szekely","07/31/2017","$139,999.00","","bclarke3@unl.edu","2200 VINE ST BOX 830861","LINCOLN","NE","685032427","4024723171","MPS","1269","","$0.00","There are three goals for this project.  The first goal is to develop data-driven assessments of the complexity of data generators and data-driven assessments of the complexity of the predictive techniques to be used for a data generator and then relate them to each other.   It is expected that a complexity matching principle between data generators and their predictors will be established.  The motivation is to speed the search for predictors that have low generalization error.  The second goal is to develop techniques to derive modeling information from good predictors.  The motivation is to be able to make statements about the data generator beyond numerical prediction.  The third goal is to use these techniques on a complex data set for which a predictive approach is essential because the extreme complexity of the data means it defies conventional modeling.  The motivation is to verify that the complexity based techniques give reliable inferences for an important question such as `which of those who have suffered a traumatic event are likely to get post- traumatic stress disorder'.<br/><br/>The motivation for the overall project is to find ways to get information out of data that is so complex conventional techniques are ineffective.   Such data is becoming increasingly common as the number of data types increases and as data bases become more comprehensive.  The problem with conventional techniques seems to be that they assume a model that means something physically before there is a strong enough basis even to propose one.   The approach here is significant because it is overtly predictive:  Instead of proposing models, one can propose predictors that are easier to test and then study the predictors to make statements about whatever it was that generated the data.  This reverses the usual approach in which one models first and then predicts."
"1438957","CAREER: Subsampling Methods in Statistical Modeling of Ultra-Large Sample Geophysics","DMS","STATISTICS, Other Global Learning & Trng","08/01/2013","08/09/2015","Ping Ma","GA","University of Georgia Research Foundation Inc","Continuing Grant","Gabor Szekely","08/31/2017","$306,695.00","","pingma@uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269, 7731","1045, 1187, 5946","$0.00","Remote sensing of the Earth's deep interior is challenging. Direct sampling of the Earth's deep interior is impossible due to the extreme pressures and temperatures. Our knowledge of the Earth?s deep interior is thus pieced together from a range of surface observations. Among surface observations, seismic waves emitted by earthquakes are effective probes of the Earth?s deep interior and are relatively inexpensively recorded by networks of seismographs at the Earth's surface. Unprecedented volumes of seismic data brought by dense global seismograph networks offer researchers both opportunities and challenges to explore the Earth?s deep interior. The key challenge is that directly applying statistical methods to this ultra-large sample seismic data using current computing resources is prohibitive. To facilitate geophysical discoveries that can enhance our understanding of the Earth?s deep interior using current computing resources, the investigator proposes a family of novel statistical methods under a subsampling framework. The proposed methods provide an opportunity to study various distinct statistical problems, such as function estimation and variable selection, in a unified framework. The investigator will establish asymptotic and finite sample theory to investigate the approximation accuracy and consistency of the proposed methods.<br/><br/>How to analyze ultra-large sample data creates a significant challenge in almost all fields of science and engineering.  Scientists and engineers develop various solutions to tackle the problem, such as developing cloud computing for aggregating a wide range of computing resources and building powerful supercomputers.  However, the high cost of these solutions creates an extraordinary budget barrier for researchers. The proposed subsampling methods provide alternative methods to surmount this challenge. The theory to be established will benefit a wide spectrum of research in science and engineering. They will offer a unique educational experience for both undergraduate and graduate students to participate in cutting-edge statistical and interdisciplinary research and inspire new lines of researches in three distinct fields: statistics, geophysics, and computational biology."
"1309856","Extreme Value Theory and Fixed-Domain Asymptotics of Multivariate Random Fields","DMS","STATISTICS","07/01/2013","07/25/2013","Yimin Xiao","MI","Michigan State University","Standard Grant","Gabor Szekely","06/30/2016","$100,000.00","","xiao@stt.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","","$0.00","The aim of this project is to develop systematic approach to establish novel results and open new research directions on extreme value theory and fixed-domain asymptotics of multivariate random fields. Special emphasis is placed on characterizing cross-dependence structures of Gaussian or non-Gaussian, non-stationary and/or anisotropic multivariate random fields and on studying their effects on extreme value theory and fixed-domain asymptotics. In particular, the Investigator plans to combine his investigation of fractal and differential geometries of multivariate random fields with precise estimation of the excursion probabilities, parameter estimation, prediction and fixed-domain asymptotics of multivariate spatial and spatio-temporal processes.<br/> <br/>Multivariate random field models are in increasing demand in statistics, geophysics, environment sciences and other scientific areas, where many problems involve data sets with multivariate measurements obtained at spatial locations.  Common problems in applications of random field models including parameter estimation, prediction and the determination of threshold level on the random field. It is a major challenge to accurately determine the threshold level when the observations in the random field are correlated in space and time. The Investigator believes that the proposed research project will ultimately yield novel insights into the understanding of multivariate spatial and spatio-temporal models, multivariate extreme value theory, and further promote their applicability in other scientific areas.  The proposed activities will also help to identify young talent, to train graduate students and to develop their careers in the mathematical and statistical sciences."
"1309800","Statistical Inference for Functional Data in Time Series and Survey Sampling: Theory and Methods","DMS","STATISTICS","08/15/2013","08/27/2013","Lily Wang","GA","University of Georgia Research Foundation Inc","Standard Grant","Gabor Szekely","05/31/2015","$99,999.00","","lwang41@gmu.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","","$0.00","Sophisticated data collection facilities often produce data which are a set of functions, represented in the form of curves, images or shapes. The development of functional data analysis in theory and methodology has provided us important analytical tools to address challenging problems encountered in many important fields. In the proposed research, the investigator continues to build and enrich the theory and methodology of functional data. This proposal targets the development of powerful statistical tools for analyzing functional data in time series and survey sampling frameworks. Four related research topics are proposed for investigation. For each project, statistical properties of the estimators, statistical inferences governed by the underlying models, and theoretical properties of the inferences will be studied. The proposed methods can be used to estimate global quantities for dependent functional data, quantify and visualize the variability of the estimators, and make global inferences on the shape of the population quantities. <br/><br/>With ""big data"" of complex (such as longitudinal, functional, heterogeneous, or correlated) features becoming increasingly available for public use in many research areas, this proposal is one vehicle to address the challenges of analyzing such types of data. The success of the proposed projects provides effective and practical tools for dealing with large and complex structural data over time and space, representing advances in the theory and methodology of statistical analysis. The benefits to society at large of the proposed research include new methodology and inference tools for big data with complex features. These topics are of interest to statisticians, survey researchers, and indeed, more broadly for researchers in climatology, health, economics, engineering, environmental studies, meteorology, behavioral and social sciences."
"1309156","Collaborative Research:  High Dimensional Multivariate Analysis","DMS","STATISTICS","09/01/2013","09/06/2013","Ping-Shou Zhong","MI","Michigan State University","Standard Grant","Gabor Szekely","08/31/2016","$50,000.00","","pszhong@uic.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","","$0.00","Classical multivariate analyses are typically designed for fixed dimensional data, and are often not applicable, even invalid, for high-dimensional data.  Analysis of modern high-dimensional data call for novel multivariate statistical approaches in the ""large-p, small n"" paradigm.  This proposal consists of three broad objectives aiming to establish a set of multivariate inferential procedures that are adaptive to high dimensionality, and can accommodate a wide range of model and dependence structures.  The investigators will develop new thresholding methods to improve the power performance of the tests for high dimensional means and covariance when the signals are sparse and faint.  They also propose bandwidth estimators for the state of the art banding and tapering estimators for covariances, and hence make these two estimation approaches practical.  The project will also develop tests for nonparametric functions of high-dimensional covariates in partially linear models. <br/> <br/>The multivariate testing procedures obtained from the proposed projects will be readily applicable in selecting gene-sets which are associated with phenotype variations, or responsive to certain treatments in the forms of having different means or covariance. The successful application in gene-set analysis will enhance our understanding of gene regulations at a biological meaningful pathway level.  The research will also improve the applications of multivariate analysis in biology, marketing research, and financial risk management."
"1309665","Collaborative Research:   Renyi Divergence-based Robust Inference in Regression, Time Series and Association Studies.","DMS","STATISTICS","07/15/2013","07/24/2013","Tharuvai Sriram","GA","University of Georgia Research Foundation Inc","Standard Grant","Gabor Szekely","08/31/2017","$75,999.00","","tn@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","","$0.00","This collaborative research project focuses on developing a novel approach to dimension reduction in regression, time series, and multivariate association studies based on a family of Rényi divergences, with a central theme of providing estimators that are inherently robust to data contamination, sustaining only a minimal loss in efficiency. This family not only characterizes the conditional independence underlying the concept of sufficient dimension reduction in regression and time series, but also characterizes independence between canonical variates in multivariate association studies. The novelty of the approach lies in exploiting a tuning parameter of the family, which balances the efficiency and the degree of robustness of the estimators. In each of the three areas, this project focuses on investigating a host of issues such as: (i) the computation of estimates, (ii) the detection of the true dimension, (iii) the selection of an optimal tuning parameter, and (iv) a formal justification of the method via theory. Furthermore, the project focuses on carrying out an in-depth study of robustness via influence functions and sample/empirical influence functions. Finally, the project focuses on finding an optimal Rényi divergence measure that is both robust and efficient, without the need for prior outlier detection or removal. <br/><br/>Rapid advances in technology have led to an information overload in most sciences. A typical characteristic of many contemporary datasets is that they are relatively high-dimensional in nature. This has prompted a shift in the applied sciences toward a different relationship-study genre arising in regression, time series and multivariate association, popularly known as dimension reduction, whose goal is to reduce the dimensionality of the variables as a first phase in the data analysis. However, the presence of outliers in high-dimensional datasets adversely affects the performance of existing dimension reduction methodologies, resulting in conclusions that are not completely reliable. Given that outliers are commonly encountered in high-dimensional datasets and that their presence is hard to detect, there is an urgent need to identify dimension reduction methods that possess some degree of automatic robustness, or non-sensitivity, to outliers. The proposed project provides robust dimension reduction methods, which would contribute significantly to the analysis of high-dimensional data arising in fields such as the social sciences, machine learning, sports, economics, environmental studies, morphometrics and cancer studies, among others. In fact, this project will not only provide novel tools for scientists in various disciplines to obtain reliable conclusions on high-dimensional data analysis, but also significantly advance the statistical theory, thereby paving a new research path in dimension reduction."
"1307642","Inference After Predictor Selection","DMS","STATISTICS","08/15/2013","08/07/2013","BERTRAND CLARKE","FL","University of Miami School of Medicine","Standard Grant","Gabor Szekely","02/28/2014","$139,999.00","","bclarke3@unl.edu","1400 NW 10TH AVE","MIAMI","FL","331361000","3052843924","MPS","1269","","$0.00","There are three goals for this project.  The first goal is to develop data-driven assessments of the complexity of data generators and data-driven assessments of the complexity of the predictive techniques to be used for a data generator and then relate them to each other.   It is expected that a complexity matching principle between data generators and their predictors will be established.  The motivation is to speed the search for predictors that have low generalization error.  The second goal is to develop techniques to derive modeling information from good predictors.  The motivation is to be able to make statements about the data generator beyond numerical prediction.  The third goal is to use these techniques on a complex data set for which a predictive approach is essential because the extreme complexity of the data means it defies conventional modeling.  The motivation is to verify that the complexity based techniques give reliable inferences for an important question such as `which of those who have suffered a traumatic event are likely to get post- traumatic stress disorder'.<br/><br/>The motivation for the overall project is to find ways to get information out of data that is so complex conventional techniques are ineffective.   Such data is becoming increasingly common as the number of data types increases and as data bases become more comprehensive.  The problem with conventional techniques seems to be that they assume a model that means something physically before there is a strong enough basis even to propose one.   The approach here is significant because it is overtly predictive:  Instead of proposing models, one can propose predictors that are easier to test and then study the predictors to make statements about whatever it was that generated the data.  This reverses the usual approach in which one models first and then predicts."
"1305858","Statistical Inference for Functional and High-Dimensional Time Series","DMS","STATISTICS","08/01/2013","08/09/2015","Lajos Horvath","UT","University of Utah","Continuing Grant","Gabor Szekely","07/31/2017","$199,999.00","Alexander Aue","horvath@math.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","MPS","1269","9150","$0.00","The proposed research provides novel methodology for the statistical inference of functional and high-dimensional time series with a focus on multi-sample functional data and panel data. The investigators thereby extend the range of functional data analysis beyond the currently available models. This is relevant because the proposed models have a number of important applications in economics and finance, geophysics and engineering. A characteristic common to many functional data applications is the presence of dependence. The theory of functional data, however, is as of today mainly focused on independent processes with notable exceptions being given by the functional autoregressive and linear processes. This proposal represents a comprehensive research plan for developing an extended tool-kit for the analysis of functional and high-dimensional time series data.  It contains the following parts:development of fully functional tests for independence and stationarity, and diagnostic tests that do not require dimension reduction; advancement of functional principal component analysis by taking into account that different operators should be used under the null and alternative hypothesis and by providing novel theory for the case of an increasing number of principal components; introduction and development of the theoretical foundation of the concept of functional analysis of variance, including procedures to cluster functional time series observations into groups; advancement of the methodology of panel data to more general models, explicitly allowing for the high-dimensionality of the observations but notably not requiring stationarity of the panels; and the breaking of new ground by combining functional series with high-dimensional time series methodology. This requires the development of sophisticated new statistical methodology, including the refinement and extension of the theory of (vector-valued) Hilbert space-valued observations. The research also includes a significant innovative computational component. To aid the dissemination of results, we plan to make the relevant software freely available via the Internet. Completion of the proposal gives statisticians and practitioners new tools for analyzing different forms of functional data.<br/><br/>The proposal is interdisciplinary in nature, with applications in diverse fields ranging from finance and economics (tick-by-tick transaction data, joint movement of several economic indicators, the effect of policy changes on economic processes), environmental science (monitoring air pollution, changes in temperature, change in the occurrences of certain meteorological extremes), and to geophysics (magnetic field readings of magnetometers). In the context of financial data, independence and stationarity testing can be used to determine if, for example, the functional autoregressive model is appropriate for high resolution asset price data. If so then further estimation techniques can be applied towards predicting asset values as well as other techniques in economic forecasting. By applying the functional analysis of variance to magnetic field measurements taken from several different locations one may categorize these locations according to the magnetic field behavior they exhibit. This may influence the implementation of radio communication in these areas. The research is therefore of immediate interest for practitioners and will further connect statistics and fields of science with a significant  statistical component. It also advances the theory of mathematical  statistics. The proposed research produces doctoral students, among them female and  minority students, theoretically and practically versed in both statistics and  an area of application. The training and involvement of undergraduate students  in this research is also included through regular coursework, independent study and projects."
"1309210","Collaborative Research: High Dimensional Multivariate Analysis","DMS","STATISTICS","09/01/2013","09/06/2013","Song Chen","IA","Iowa State University","Standard Grant","Gabor Szekely","08/31/2017","$69,999.00","","songchen@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","9150","$0.00","Classical multivariate analyses are typically designed for fixed dimensional data, and are often not applicable, even invalid, for high-dimensional data.  Analysis of modern high-dimensional data call for novel multivariate statistical approaches in the ""large-p, small n"" paradigm.  This proposal consists of three broad objectives aiming to establish a set of multivariate inferential procedures that are adaptive to high dimensionality, and can accommodate a wide range of model and dependence structures.  The investigators will develop new thresholding methods to improve the power performance of the tests for high dimensional means and covariance when the signals are sparse and faint.  They also propose bandwidth estimators for the state of the art banding and tapering estimators for covariances, and hence make these two estimation approaches practical.  The project will also develop tests for nonparametric functions of high-dimensional covariates in partially linear models. <br/> <br/>The multivariate testing procedures obtained from the proposed projects will be readily applicable in selecting gene-sets which are associated with phenotype variations, or responsive to certain treatments in the forms of having different means or covariance. The successful application in gene-set analysis will enhance our understanding of gene regulations at a biological meaningful pathway level.  The research will also improve the applications of multivariate analysis in biology, marketing research, and financial risk management."
"1313224","Distance-based variable selection for high-dimensional biological data","DMS","STATISTICS","09/15/2013","09/09/2013","Daniel Nettleton","IA","Iowa State University","Standard Grant","Gabor Szekely","08/31/2016","$150,000.00","","dnett@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","7334, 9150","$0.00","The overall objective of the research project is to provide statisticians and biological scientists new and improved statistical tools for measuring variable importance and selecting key variables in high-dimensional biological data.  The two major ingredients underlying the research are a distance-based procedure across multiple dimensions and a tilting/weighting-based importance measure for any specific dimension. The distance-based methods, e.g., the multi-response permutation procedure and the distance covariance, provide the ability to handle data even if the number of dimensions is larger than the sample size.  The tilting/weighting-based procedures allow variable importance to be evaluated for any dimension in the presence of any number of other variables. Thus, variable importance is evaluated in the multivariate context rather than on univariate marginal distributions.  In addition, the new methods will allow the number of selected variables to exceed the sample size; allow forward selection, backward selection, and sparse penalized weighting; minimize perturbation to the dependence structures actually present in the data; require minimal structural assumptions; and be sensitive to a wide range of multivariate dependencies, including some difficult or even impossible to detect with existing methods.<br/><br/>The methods developed as part of this project have a wide range of applications in biomedical and agricultural industries.  Modern genomics tools allow researchers to simultaneously measure thousands of variables that contain information about DNA, RNA, and protein characteristics of organisms.  The high-dimensional data generated by these modern high-throughput technologies must be mined to identify the variables that are most associated with health outcomes or other important traits.  Uncovering of such associations is crucial in a variety of areas including drug discovery, genetic risk analysis, personalized medicine, and plant and animal breeding. This research project will provide tools to help make these discoveries possible.  Reliable software implementations of the new methods will be created, maintained, archived in public repositories, and freely disseminated to genomics researchers and industry practitioners working with a diverse range of organisms and different high-throughput technologies.  The research activity will enhance collaborations and partnerships among researchers from both computational/statistical fields and experimental/biomedical fields."
"1305154","Bayesian Information Criteria and Problems of Parameter Identifiability","DMS","STATISTICS","07/01/2013","05/08/2015","Mathias Drton","WA","University of Washington","Continuing Grant","Gabor Szekely","06/30/2016","$240,000.00","","md5@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","","$0.00","This project is concerned with statistical model selection by means of optimization of information criteria.  Specifically, the investigator develops a generalization of the Bayesian information criterion for irregular model selection problems, such as determining the number of components in mixture models or the number of factors in latent factor models.  The main difficulty in these irregular model selection problems is a lack of parameter identifiability.  The investigator studies identifiability properties of widely used statistical models to provide the mathematical foundation for application of the new information criterion.<br/><br/>Virtually every scientific data analysis brings about a problem of statistical model choice, where the different statistical models capture different scientific hypotheses.  In many applications, the hypotheses involve latent variables that cannot or were not observed.  Such latent variables could be, for instance, notions of intelligence in a psychological study or variables describing a patient's genetic composition in a medical study.  Statistical models that are formulated using such latent variables typically lack the regularity properties that underlie the justification of standard statistical procedures.  This project develops new statistical techniques for model selection that are theoretically justified and allow for an improved assessment of model uncertainty in a wide array of applications in which influential unobserved variables are at play."
"1252624","CAREER: Flexible Network Estimation from High-Dimensional Data","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2013","06/09/2017","Daniela Witten","WA","University of Washington","Continuing Grant","Gabor Szekely","06/30/2020","$400,000.00","","dwitten@u.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269, 8048","1045","$0.00","This research involves the development of new statistical methods and theory for graphical modeling on the basis of high-dimensional data, in which the number of features exceeds the number of observations. In certain applications, such as the estimation of transcriptional regulatory networks on the basis of gene expression data, existing techniques are inadequate for two reasons: the assumptions that underlie these techniques are insufficient for accurate network recovery in the face of such high dimensionality, and furthermore the assumptions that are made may be unrealistic for the data. To address these two problems, the investigator proposes to study (a) a set of techniques for more effectively learning one or more Gaussian graphical models by making more effective and structured assumptions about the topology of the true conditional dependence networks, via convex penalties and other techniques; and (b) more flexible frameworks for estimating conditional dependence relationships without the usual Gaussianity assumptions.<br/><br/>In recent years, new technologies and fast computers have resulted in the generation and availability of vast amounts of data in fields as diverse as molecular biology, marketing, finance, sociology, linguistics, and computer vision. Unfortunately, analyzing this type of ""big data"" poses severe statistical challenges, and the classical statistical toolset cannot be applied. Therefore, developing effective statistical machine learning techniques for making sense of very large-scale data sets is crucial for progress in many areas of science as well as industry, in order to bridge the gap between the data that is being collected and the scientific and industrial questions that are being asked about the data. As an example, being able to estimate gene networks on the basis of genomic data has important implications for understanding biological processes, and for making progress towards the treatment of cancer and other disease. This proposal involves (1) developing techniques for improved network estimation on the basis of high-dimensional data sets; (2) disseminating the resulting techniques to the statistical and biomedical communities via publications, seminars, and the public release of software;  (3) training PhD students in statistical machine learning techniques for big data; and (4) increasing the exposure of high school students, undergraduates, and members of underrepresented groups to statistical machine learning and big data challenges via short courses, conference presentations, and other activities."
"1307566","New Directions in Quantile-based Modeling and Analysis","DMS","STATISTICS","09/01/2013","08/01/2013","Xuming He","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","08/31/2017","$210,000.00","","xmhe@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Quantile as a data descriptive and analytic tool has earned its place in statistics for over a hundred years. In recent years, research on quantile modeling to incorporate the effect of covariates and to handle multivariate data has accelerated in response to the needs arising from a broad area of applications.  The investigator addresses an important but often neglected question on the validity of posterior inference on quantile regression for the pseudo-Bayesian methods that have become popular in the literature. The investigator conducts a careful investigation into how the choice of a working likelihood and the choice of a prior play their respective roles, both in finite-sample problems, and in the asymptotic theory. The investigator studies a new class of shrinking priors as an asymptotic framework to understand the efficiency gains of the Bayesian methods for estimation and prediction of quantiles in data sparse areas and in problems involving high dimensional covariates. The proposed research will deepen our understanding of the validity of pseudo-posterior inference and suggest asymptotically valid and efficient inferential methods for quantile regression at single or multiple quantile levels. The research will also facilitate a new pseudo-Bayesian framework for model selection beyond quantile regression. Furthermore, the investigator studies a new notion of quantile for multivariate data.<br/><br/>The proposed activities will stimulate novel ideas and critical thinking in the areas of quantile modeling and Bayesian inference. The new insights and the new tools to be developed will be useful for estimation, prediction, and hypothesis testing regarding rare events in climate research, public health, and other scientific endeavors.  The notion of multivariate quantiles will lead to an efficient statistical downscaling method for better climate projections at localized scales. The proposed activities will engage graduate students directly as part of their academic training. The investigator will work with other researchers and scientists to ensure that the research results are disseminated appropriately to the broad scientific community."
"1308919","Iterated filtering:  New theory, algorithms and applications","DMS","STATISTICS","07/01/2013","07/24/2013","Edward Ionides","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","06/30/2017","$99,994.00","Yves Atchade","ionides@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Partially observed Markov process models provide a general framework for formulating and answering questions about dynamic systems. Evaluation of the likelihood for these models can be formulated as a filtering problem. Iterated filtering algorithms carry out repeated sequential Monte Carlo filtering operations to maximize the likelihood. Current theory for iterated filtering justifies the parameter update at each iteration via a stochastic approximation to the first derivative of the log likelihood. Our new approach to iterated filtering theory and methodology draws on similarities with data cloning (i.e., methods where Markov chain Monte Carlo algorithms are applied to multiple copies of the data to provide likelihood-based inference). The relationship with iterated filtering is that each filtering iteration is analogous to creating a new clone of the data. This new theoretical perspective leads to novel novel algorithms. In the context of the previous stochastic approximation theory of iterated filtering, the new algorithms behave as though the intractable second derivative of the likelihood were known. Indeed, the proposed algorithm generates an estimate of the Fisher information as a bi-product. Preliminary results, on a simple ecological model and on a challenging inference problem arising from fitting a malaria transmission model to time series data, show that a new iterated filtering algorithm out-performs previous methods. As well as advancing methodological capabilities for time series analysis via mechanistic models, the investigators will develop applications to two other related classes of statistical problems: longitudinal data analysis via mechanistic models, and inference for complex dynamic data structures. As concrete examples, the investigators will study the use of iterated filtering techniques for (i) relating pathogen genetic sequence data to HIV transmission models; (ii) using longitudinal data to inform stochastic dynamic models of sexual behaviors related to HIV transmission; (iii) inference via summary statistics and pseudo likelihood criteria, with an application to partially observed dynamic network models.<br/><br/>Many scientific challenges involve the study of nonlinear stochastic dynamic systems about which only noisy or incomplete measurements are available. Except when the system is small, state-of-the-art statistical methods are required to make efficient use of available data and to provide modeling flexibility that promotes model criticism. The novel iterated filtering algorithms developed by the investigators will be used to study disease transmission systems with the goal of informing policy for the detection, control and potential eradication of infectious diseases. The PI is already engaged in the interface between statistical methodology development, epidemiology and public policy. The proposed research will directly benefit understanding of malaria and HIV transmission, but will also provide methodological tools and case studies relevant to other disease systems. More broadly, the methodology developed will be applicable to inference problems for dynamic systems arising throughout the biological, physical, social, health and engineering sciences. Open source software for all the methodology developed will be included in the R package {pomp} (http://cran.r-project.org/web/packages/pomp) for which the PI is a co-developer. Advances in iterated filtering methodology will be disseminated as part of the PIs ongoing agenda to spread the use of formal statistic methods for partially observed dynamic systems."
"1308890","Nonregular asymptotics under dependence and inference on change points in graphical networks","DMS","STATISTICS","09/01/2013","08/29/2013","Moulinath Banerjee","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","08/31/2016","$115,000.00","","moulib@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","The proposed project will make two main intellectual contributions: (i) the development of non-regular asymptotic theory for a class of problems involving dependent data (time series models, for example), (ii) inference on change-points for time-varying graphical networks. Non-regular problems, of increasing importance in modern statistics, are those where natural estimators are highly non-linear in the data and deriving their asymptotic properties requires the application of sophisticated tools (like modern empirical process methods), in contrast to the asymptotic linearization techniques that work for estimators arising in `regular' parametric and semiparametric problems. Shape-restricted inference for time series data, for which not much is yet known, and which has important implications for some pressing problems (as in the monotone trends observed in global warming and environmental pollution) will be one important focus in the proposed study of non-regular methods. Another goal is to develop a unified theoretical framework for the study of M-estimators (i.e. estimators obtained by minimizing/maximizing a random criterion function) for finite dimensional parameters in the dependent data setting, which will provide a generic approach (a paradigm as well as a set of tools) to the study of a variety of problems that have been, hitherto, solved on a case-by-case basis and other similar problems that arise in important applications in economics and biology.   As far as (ii) is concerned, the problem of time-varying graphs, either observed or unobserved, whose structures undergo sudden massive changes at certain points in time, is of prime importance in a variety of examples, ranging from biology and engineering to social sciences and economics.  Rigorous inferential procedures for determining such `change-points' in time -- regime changes -- in a variety of network models (like Markov transitioning random graphs, Markov random fields), that are interesting both from a mathematical perspective and in that they provide useful models for many observed phenomena, will be developed. <br/><br/>The proposed research program is motivated by problems arising in a variety of fields, ranging from climatology and environmental studies to economics and genomics. More specifically, part of the proposed project deals with making intelligent predictions on increasing or decreasing `trend' functions, e.g. the GDP output of a rapidly developing country, global temperature trends over time, by taking advantage of their pre-known monotone shape. The proposed techniques are novel and expected to enjoy considerable benefits over existing statistical procedures. A related goal is to develop new theoretical tools for addressing a wealth of statistical procedures that share some key common features and are of considerable importance in problems arising in economics. The second part of the project is focused on investigating sudden `regime' changes in mechanisms called `networks' which describe interactions among a collection of entities: for example, genetic networks which capture how genes interact with each other and with proteins to regulate bodily functions, social networks where a group of people interact on sites like Facebook or Twitter and exchange information in the process. Drastic changes in network behavior typically represent the onset of a critical event, say a disease in the gene network setting, or socio-economic upheaval in the social network setting, and it is therefore important to identify them. Inter-disciplinary collaborations with biologists, economists and environmental scientists will be pursued actively to enhance the impact of the proposed research."
"1309808","Distance Correlation Measures, with Applications to Astrophysical Databases and Diffusion Tensor Imaging","DMS","EXTRAGALACTIC ASTRON & COSMOLO, STATISTICS","09/15/2013","03/11/2016","Donald Richards","PA","Pennsylvania State Univ University Park","Standard Grant","Gabor Szekely","08/31/2016","$195,000.00","Megan Romer, Mercedes Richards","richards@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1217, 1269","1206","$0.00","The principal investigator (PI) will conduct theoretical and applied research on distance correlation methods, with applications to high-dimensional astrophysical databases. This research will be accomplished with co-authors in statistics and astronomy, and with graduate students. The PI and a Co-PI will develop Hankel transforms for distributions on matrix spaces and apply the results to develop goodness-of-fit testing procedures for data consisting of symmetric positive definite random matrices. The PI and a Co-PI will analyze high-dimensional astrophysical databases on galaxy clusters to identify associations and correlations between multiple astrophysical variables.  Research on distance correlation measures for mixtures of Gaussians and general heavy-tailed distributions will be carried out.<br/><br/>The advances in inference for data consisting of positive definite random matrices will be applied to develop new statistical theory and methods for modeling variability in biological shapes and analyzing the movement of water molecules in biological tissue. The research program benefits human development and education in the society through the training of graduate students and joint research with post-doctoral and faculty colleagues."
"1309213","Robust Inference for Dependent Data","DMS","STATISTICS","08/01/2013","07/02/2015","Zhibiao Zhao","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","07/31/2016","$121,459.00","","zuz13@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","The investigator proposes two different approaches to address a broad variety of parametric and nonparametric inference problems under unknown nonstationarity and dependence.  The first approach explores several self-normalization based methods to avoid the estimation of complicated limiting variance function. The proposed methodology can be used to address various nonparametric inference problems, such as nonparametric mean regression, quantile regression, and nonparametric inference for locally stationary processes. The second approach proposes a random perturbation based framework for robust inferences. The idea is to suppress the unknown nonstationarity and dependence at the cost of inflated noise level. The proposed random perturbation method is robust against unknown nonstationarity and dependence, and it is generally applicable for univariate and multivariate parameter inferences, conditional mean regression inference, nonparametric inference, change-point analysis, and nonlinear regression.<br/><br/>Many practical data exhibit complicated time-varying pattern and dependence. Without taking into account such unknown nonstationarity and serial dependence, existing statistical methods developed for independent data or stationary data may not work well or even fail. The proposal aims to develop cutting-edge statistical inference techniques and theories for a broad variety of parametric and nonparametric problems under unknown nonstationarity and dependence. The results from this project can be widely applied to data from climatic, economic, financial, and longitudinal studies. The project will integrate research and education through involvement of both undergraduate and graduate students. The investigator will disseminate research results through teaching, publications, and seminar presentations."
"1332693","WORKSHOP ON TRAINING STUDENTS TO EXTRACT VALUE FROM BIG DATA","DMS","INFRASTRUCTURE PROGRAM, STATISTICS, Information Technology Researc, CDS&E-MSS","08/15/2013","08/08/2013","Michelle Schwalbe","DC","National Academy of Sciences","Standard Grant","Gabor Szekely","07/31/2015","$150,000.00","","mschwalbe@nas.edu","2101 CONSTITUTION AVE NW","WASHINGTON","DC","204180007","2023342254","MPS","1260, 1269, 1640, 8069","1640, 7433, 7556, 8083, 9263","$0.00","The National Academies will plan and organize a cross-disciplinary public workshop to explore perspectives on the training that students from diverse fields need in order to extract value from Big Data. The workshop will identify key skills that are needed to prepare students for analyzing Big Data. It will consider the needs of industry, academia, and government and build on the experience gained from emerging courses and curricula for this topic. Invited participants to the workshop will include experts in statistics, machine learning, databases, large-scale computing systems, streaming computing, user domains, health and biological informatics, and industry.<br/><br/>Many traditional methods for data analysis do not work, or do not work well, with the massive amounts of data emerging in numerous endeavors. In order to reliably extract insight from Big Data, students need to learn new skills, and many of those skills cut across multiple disciplines and, thus, are not necessarily accessible through standard courses and curricula. During the academic year 2012-2013, a number of courses were introduced at various universities to impart the needed understanding. The planned workshop will enable educators to share insights gained from these courses and make adjustments. A summary report of the workshop will make these insights available more broadly."
"1309312","Conference on Advanced Statistical Methods for Underground Seismic Event Monitoring and Verification","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","02/15/2013","01/31/2013","Minge Xie","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","01/31/2015","$31,040.00","Rong Chen","mxie@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269, 1271","7556, 9263","$0.00","The Conference on Advanced Statistical Methods for Underground Seismic Event Monitoring and Verification will be held March 7-8, 2013 in Washington DC area. The ability of monitoring and verification of underground seismic events is vital to national security and disaster preparation and prevention. The research and development of monitoring and verification technology have been extensive. Significant advances have been made in sensor development and also in the development of seismic signal processing methodologies that have the ability to verify any underground seismic events as well as distinguish signals generated from different mechanisms with sensors placed a vast distance away from the event site. The research and development is highly interdisciplinary, involving geophysics, sensor development, signal processing, and statistics. To further enhance the collaboration between statisticians and other scientists in the field and to investigate the potential approaches of combining state-of-art statistical methods with advanced sensor technology, we propose to organize a conference on advanced statistical methods for underground seismic event monitoring and verification. The conference will bring statisticians and the other experts in the field together to discuss the current status of field, to exchange ideas, and to brainstorm novel approaches. The conference will also serve a venue to generate new interests among statisticians to this field, to enlist statisticians to bring in their new statistical ideas and approaches to deal with this important problem.<br/><br/>The conference will be collaboration between academics and federal agencies. The experts in the federal agencies will share their vision and experience and also any urgent specific problems. The conference will consists of general sessions where plenary speakers and discussants present their research findings and proposals of future research, with floor discussion encouraged, and a study group session where participants are separated into several groups and discuss and deliberate specific topics. At the end, the conference organizer will produce a report to be shared with federal agencies and participants, and be published in scientific community newsletters such as ASA news to generate interests in the larger community. The conference will attract, engage, educate and train students and next generation scientists to work in the area of research and development of underground seismic event detection technology using advanced statistical and mathematical tools. Such exposure and training is essential for them to become statisticians capable of collaborating effectively with scientists and researchers in the field and to generate new innovative approaches to real problems that have been difficult to tackle in underground event detection and related fields."
"1308960","New Developments on Quantile Regression Analysis of Censored Data: Theory, Methodology and Computation","DMS","STATISTICS","09/01/2013","08/29/2013","Lan Wang","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","08/31/2016","$119,999.00","","lanwang@mbs.miami.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","Quantile regression has recently emerged as a valuable semiparametric alternative to the popular Cox model for analyzing censored data. It directly models survival time; thus is easy to interpret. More importantly, it relaxes the proportional hazards constraint associated with the Cox model and is particularly powerful for heterogeneous data.  Despite the remarkable recent progress, several important and challenging statistical problems remain unsolved.  For example, there exists limited literature on censored quantile regression when the sample arises from an observational study and is not representative of the target population; when the censored data come from genomics studies involving high-dimensional covariates; or when random effects are present due to the incorporation of latent variables. Motivated by these challenging problems, this project will develop novel methodology, theory and algorithms, which have the potential to significantly advance the applications of censored quantile regression. The PI will rigorously study the theoretical properties of the proposed new procedures and investigate their applications in practical data analysis.  <br/><br/>Censored data arise in diverse fields such as economics, engineering, medicine, psychology and sociology. The new methodology and theory are expected to make important contributions to the current body of knowledge on statistical analysis of survival data.  In particular, the proposed research will make timely contributions to high-dimensional data analysis with censored responses, which has important applications in modern genomics and is still a relatively unexplored research area. The PI will develop useful software packages and make them freely available to the research community. The research results will be incorporated in different levels of statistical courses.  The PI will also incorporate her research activity with graduate education.  Students from minority groups will be especially encouraged to participate in the proposed projects."
"1310096","Collaborative Research: Developing a Theoretical and Methodological Framework for High Dimensional Markov Chain Monte Carlo","DMS","STATISTICS","07/01/2013","05/01/2015","Galin Jones","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","06/30/2016","$100,002.00","","galin@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","The investigators study multivariate methods for assessing the quality and ensuring the reliability of a Markov chain Monte Carlo (MCMC) experiment.  This work is strongly motivated by research in Bayesian methods for functional neuroimaging experiments, but will be applicable in any MCMC simulation.  Usually, Markov chain output is used to estimate a vector of parameters that contains multiple mean and variance parameters along with quantiles.  A fundamental question is when to terminate such a simulation.  The investigators study sequential fixed-volume stopping rules that allow construction of confidence regions for estimating the target vector, which describe the reliability of the resulting estimates.  Using these methods requires that the Markov chain converges at a geometric rate, which in turn yields a limiting distribution for the Monte Carlo error with an associated covariance matrix.  Estimating this matrix forms a major component of the research-a long standing open question in MCMC output analysis.  The investigators improve on existing methods, which enable effective estimation in the case where the target vector is moderately large. Moreover, the investigators study several methods for handling the setting in truly high-dimensional settings, i.e. when there are many more parameters than iterations in the Markov chain.  The investigators also formally study the convergence rates of component-wise MCMC samplers often encountered in the functional neuroimaging settings.  <br/><br/>Complex probability models are commonly used to help gain understanding of phenomenon in a range of fields including science, engineering, medicine, education, and law.  An example that motivates the investigators work is that of applied cognitive scientists modeling brain activity.  Inference from such probability models is usually obtained from computational approximations.  For a widely used computational technique, the investigators study the convergence properties and develop formal stopping rules focusing on high-dimensional practically relevant settings.  The statistical methodology developed here will provide scientists with sophisticated output analysis techniques, leading to greater confidence and reliability for their computational results."
"1308566","Nonparametric classification, tuning parameter selection, and asymptotic stability for high-dimensional data","DMS","STATISTICS","07/01/2013","05/05/2014","Yang Feng","NY","Columbia University","Continuing Grant","Gabor Szekely","06/30/2016","$129,980.00","Zhiliang Ying","yf31@nyu.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","","$0.00","Technological innovations have provided a primary force in advancement of scientific research and in social progress. High-throughput data of unprecedented size and complexity are frequently seen in diverse fields of science and humanity, ranging from computational biology and health studies to financial engineering and risk management. Such high-dimensional data have initiated many important problems in  contemporary statistics where feature selection plays pivotal roles. The proposed project has the following three interrelated objectives in the theme of high-dimensional data with applications in classification and variable selection. (1) To introduce a nonparametric classification framework for high-dimensional data. The target of this research is to integrate the nonparametric component to the classical parametric methods for classification (e.g., penalized logistic regression, linear discriminant analysis) under high-dimensional settings without incurring much computational burden. Asymptotic properties are investigated regarding the excess risk. (2) To investigate the asymptotic properties of cross-validation for tuning parameter selection in high-dimensional variable selection. The goal here is to perform a systematic study on the asymptotic behavior of major cross-validation methods for choosing the tuning parameter when various penalty functions (LASSO, SCAD, MCP, etc.) are used. By delineating the properties of the classical cross-validation, a new modified cross-validation method for the purpose of choosing the optimal tuning parameter in the solution path is developed that achieves model selection consistency. (3) To introduce the notion of asymptotic stability for maximum penalized likelihood estimators. Despite the extensive literature on the maximum penalized likelihood estimators in high-dimensional settings, the research on the stability of the estimators has been very limited. The investigators aim to introduce the notion of asymptotic stability for a general class of maximum penalized likelihood estimators, study the behavior and evaluate the performance of these estimators when different penalty functions are applied.<br/><br/>The analysis of ""big data"" now pervasive across many scientific disciplines poses challenges as well as opportunities to the field of statistics. A major goal of this proposal is to make methodological and theoretical contributions to the important and challenging topic of high-dimensional classification and variable selection. The proposed research will have broad impacts on many disciplines of science, including health/life sciences, economics, finance, astronomy and sociology, among others. In these fields, variable selection, feature extraction, sparsity explorations are crucial for knowledge discovery. The investigators have been interacting with researchers at New York State Psychiatric Institute at the Columbia University Medical Center, Computational Biology Center of the Memorial Sloan-Kettering Cancer Center and Center for Computational Learning Systems at Columbia University. The results of the proposed investigations will be used for understanding mental health issues, for identifying risk factors in diseases of cancer and for predicting failures in complex engineering systems. On the educational side, the proposed work will be incorporated into new courses on the state-of-the-art high-dimensional statistical learning. It will also be integrated into the training of undergraduate and graduate students, especially of under-represented groups, in terms of Ph.D. dissertations and undergraduate research projects."
"1309960","Optimal tests for weak, sparse, and complex signals with application to genetic association studies","DMS","STATISTICS","08/15/2013","08/01/2013","Zheyang Wu","MA","Worcester Polytechnic Institute","Standard Grant","Gabor Szekely","07/31/2017","$109,999.00","","zheyangwu@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","MPS","1269","","$0.00","Detection of sparse and weak signals is a key for analyzing big data in many fields. Recent statistical research has made celebrated theoretical progress in revealing the detectability boundaries under the Gaussian means model and an idealized linear regression model. Detectability boundary illustrates the border in the two-dimensional phase space of signal sparsity and weakness, below which the signals are asymptotically too weak and sparse to be detectable by any statistical methods. Certain statistics are optimal for these models in the sense that they reach the boundary (i.e., the least requirements) for reliable signal detection. However, there are significant gaps between these theoretical models and practical meaningful models. In this project, the investigators extend statistical theory to handle weak, sparse, correlated, and interactive signals under the framework of generalized linear models. The investigators develop optimal testing procedures to address the realistic data features in genome-wide association studies and next-generation sequence studies. <br/><br/>Statistical theory and methodology development for the detection of weak and sparse signals is foundational for analyzing big data. The goal of this project is to extend statistical theoretical study to address complex signals that are correlated and interactively influential to quantitative or categorical responses. This study is of great interest in data science and is critical to many applications. For example, one perplexing problem of current genetic studies is the missing heritability of complex traits even after many genetic factors have been identified. The proposed work specifically addresses the features of those hidden disease genes yet to be discovered. Unlike some genetic studies based on heuristic arguments, this research combines the power of rigorous statistical theory, first-hand practices in the field, and cutting-edge data from genome-wide association studies and next-generation sequence studies. The proposed project is highly promising in the hunt for the missing heritability. Highly improved gene-detection techniques will help to identify more causative genes of complex human diseases, which will lead to the elucidation of disease pathogenesis and design of targeted therapeutics, thus have a far-reaching impact on improving quality of life."
"1442192","Adaptive Design Based upon Covariate Information: New Designs and Their Properties","DMS","STATISTICS","08/25/2013","04/23/2014","Feifang Hu","DC","George Washington University","Standard Grant","Gabor Szekely","06/30/2016","$108,807.00","","feifang@gwu.edu","1918 F ST NW","WASHINGTON","DC","200520042","2029940728","MPS","1269","","$0.00","Covariate information is usually available and often plays a critical role in a clinical study.  The stratified permuted block design and the classical covariate-adaptive designs have been widely employed to balance important covariates in clinical trials. Both designs have some serious drawbacks. In addition, there is no theoretical justification of the covariate-adaptive designs in the literature. In this project, two new families of adaptive designs are proposed and their properties are studied. The first family of designs overcomes the drawbacks of  the stratified permuted block design and the classical covariate-adaptive design, and hence provides better balance. The second family of designs is proposed to detect the interaction between treatment and covariate more efficiently. Also the investigator introduces a new technique (called ""drift conditions"") to study the asymptotic properties of covariate-adaptive designs. This project will produce new sequential tools for solving many practical problems. The proposed methods will be applied to some specific applications.<br/><br/>The objective of this project is to develop new methods for clinical trials based upon covariate information.  With today's advanced technology, it becomes easier and easier  to collect useful covariate information in sequential experiments. For example, scientists have identified many new biomarkers that may link to certain diseases over the past several decades. Since one is now able to collect information on important biomarkers (covariate information) of each patient, it becomes more and more important to incorporate information on covariates into the design of clinical trials. The investigator will propose two new families of adaptive designs and study their properties. The first family of designs overcomes the drawbacks of the classical covariate-adaptive designs. The second family of designs is proposed to detect the interaction between treatment and covariate more efficiently. Upon completion of this project, one will be able to apply new designs in clinical trials for personalized medicine. The research project will produce some advanced statistical tools, which may be applied in many fields including drug development, medical studies, industrial experiments, economics and finance."
"1306394","Efficient Designs for Complex Scientific Experiments","DMS","STATISTICS","07/15/2013","06/22/2015","Sam Hedayat","IL","University of Illinois at Chicago","Standard Grant","Gabor Szekely","06/30/2017","$155,875.00","","hedayat@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","","$0.00","The investigator has two-fold purpose in this project both relate to cost-effective methods of collecting high-efficiency data in Phase I, Phase II and Phase III pharmaceutical trials involving drug discovery/drug abuse, heart-related QT/QTc studies, evaluation of new drugs for unwanted effects and also in experiments dealing with processing of minerals involving chemical reactions. Akin to PK/PD models, one requires complex non-linear models in dealing with medical/chemical/environmental studies. In this context, dwelling on Klimpels floatation recovery model, chemical kinetic model and compartment model, the investigator develops analytically sound and innovative techniques for optimal/nearly optimal estimation of the underlying model parameters with constrained experimental conditions.<br/><br/>The aim of this proposal is to provide research practitioners in fMRI studies, medicine, and pharmacy with ready-to-use statistical/computational software for carrying out cost-effective experiments. Research outcomes on compartment models will allow formal rigorous inference and analysis based on clinical or experimental data for complicated HIV dynamic models. Research practitioners in fMRI studies, medicine, and pharmacy can save time and money by using the new to-be-developed software-based crossover designs."
"1347844","CAREER: Nonparametric Models Building, Estimation, and Selection with Applications to High Dimensional Data Mining","DMS","STATISTICS","07/01/2013","08/14/2013","Hao Zhang","AZ","University of Arizona","Continuing Grant","Gabor Szekely","06/30/2014","$96,133.00","","hzhang@math.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1269","0000, 1045, 1187, OTHR","$0.00","Nonparametric methods are increasingly applied to regression, classification and density estimation, both in statistics and other related areas such as data mining and machine learning. However, a key difficulty with nonparametric models is model fitting for high dimensional data due to the curse of dimensionality. Another difficulty is model inference and interpretation, i.e., how to evaluate or test individual variable effects on the complex surface fit. For heterogeneous data with complicated covariance structure, nonparametric model estimation is even more challenging. The objectives of this proposal are to develop novel and widely applicable procedures to simultaneous model selection and estimation for nonparametric models and their related paradigms in data mining. In the framework of reproducing kernel Hilbert space (RKHS), the PI proposes a host of new regularization techniques for several families of models: smoothing spline ANOVA models for correlated data, semiparametric regression models, support vector machines for supervised and semi-supervised learning. The proposed methodologies constitute key advances over standard methods through their unified framework for achieving model sparsity and function smoothing altogether, their tractable theoretical properties, and their easy adaptation to high dimensional problems. The PI will study asymptotic behaviors of the proposed estimators, explore data-driven procedures for tuning regularization parameters, and develop computation algorithms and softwares to implement the proposed procedures. The PI will also examine finite sample performance of new methods via extensive simulation studies and real data analysis.<br/><br/>In the current information era, the volume and complexity of scientific and industrial databases have been exponentially expanding. As a consequence, the data form keeps gaining higher and higher dimensionality. Analysis of such data poses new challenges to statisticians and is becoming one of the most important research topics in modern statistics. The purpose of this project is to significantly increase the available tools for analyzing complex high dimensional data. In this project, the PI aims to accomplish the following three goals: (1) meet the challenges of nonparametric model estimation and selection within a unified mathematical framework; (2) develop flexible methods with desired statistical properties and high-performance statistical softwares for mining massive data; (3) integrate research opportunities and findings from the above two activities into disciplinary and interdisciplinary statistical education at graduate, undergraduate and high school levels. This research will broaden traditional understanding of nonparametric inferences <br/>and model selection, provide a broad range of researchers and practitioners in various fields including sociology, economics, environmental, biological and medical sciences with state-of-the-art data analysis tools, and help to prepare the next-generation students with the necessary modern statistical perspectives. <br/> <br/>"
"1309507","Flexible Modeling for High-Dimensional Complex Data: Theory, Methodology, and Computation","DMS","STATISTICS","07/01/2013","05/15/2015","Hao Zhang","AZ","University of Arizona","Continuing Grant","Gabor Szekely","06/30/2017","$150,000.00","Ning Hao","hzhang@math.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1269","","$0.00","In high dimensional data analysis, the relationships among predictors can be highly nonlinear and non-additive, and taking into account such complex structures may significantly improve model prediction power and provide crucial insight about the underlying data generation mechanism. The goal of this project is to develop and study new statistical and data mining methodologies for detecting nonlinear and non-additive patterns in high dimensional sparse models. When the data dimension is ultra-high, interaction selection is extremely challenging, both numerically and theoretically, due to curse of dimensionality. There are very limited tools available in practice and theory is scant. In this project, the investigators give a comprehensive treatment to the problem of high-dimensional interaction selection. They propose and study novel selection and modeling techniques for a variety of regression and classification models. Fast and robust large-scale computational algorithms are derived. In addition, the investigators are committed to establishing high dimensional theory for interaction selection and providing a solid foundation for the new methods. The investigators also propose and study a unified theory and computation framework to identify nonlinear effects for a broad class of nonparametric regression models. Special effort is spent on addressing computational issues such as multiple parameter tuning, regularization solution path/surface algorithms, and development of user friendly statistical software packages.<br/><br/>Big and high dimensional data offer us fascinating and unprecedented opportunities to gain extraordinary insight from data. On the other hand, the scale and volume of data create tremendous challenges for standard analysis tools to extract useful information. The goal of this project is to develop innovative statistical and data mining methods, solid mathematical theory, and powerful computational tools and software to capture hidden and possibly complex patterns when the data dimension is high. One challenging problem to be tackled in this project is high dimensional interaction selection. In genome-wide association studies (GWAS), there is growing evidence that gene-gene and gene-environment interactions can provide key insight about complex biological pathways that underpin human diseases. However, there are very few effective, well-grounded, and computationally attractive tools available in practice to identify interactions for high dimensional data. The investigators try to fill this gap by conducting thorough investigation on the problem. The results from this project research can significantly advance theory and as well as contribute new statistical tools for practical use. The proposed methods have a wide range of scientific applications such as biology, biomedicine, and environmental studies. This project integrates research, education, and interdisciplinary collaboration through developing new graduate and undergraduate courses and involving students in the research activities."
"1308319","Computer-intensive methods for nonparametric time series analysis","DMS","STATISTICS","08/01/2013","05/27/2015","Dimitris Politis","CA","University of California-San Diego","Continuing Grant","Gabor Szekely","07/31/2017","$240,000.00","","dpolitis@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","","$0.00","The project focuses on the development of methods of inference for the analysis of time series and random fields that do not rely on unrealistic or unverifiable model assumptions.  In particular, the investigator and his colleagues are working on: (a) consistent estimation of the matrix-valued autocovariance sequence of a multivariate stationary time series, and a subsequent linear process bootstrap procedure that is valid even in the context of high-dimensional processes; (b) flat-top kernels and their application to improved nonparametric estimation of a hazard rate function and to aggregation of spectral density estimators; (c) testing for the support of a probability density and testing for over-differencing of a time series; (d) a new block bootstrap procedure for time series that are periodically or almost periodically correlated;  (e) estimation and testing in the context of locally stationary time series and resampling  inference for possibly inhomogeneous marked point processes; and (f) different aspects of resampling  with functional data, including the difficult open problem of appropriately studentizing a functional statistic.   <br/><br/>Ever since the fundamental recognition of the potential role of the computer in modern statistics, the  bootstrap and other computer-intensive statistical methods have been developed extensively for inference with independent data. Such methods are even more important in the context of dependent data where the distribution theory for estimators and test statistics may be difficult or impractical to obtain.  Furthermore, the recent information explosion has resulted in datasets of unprecedented size that call for flexible, nonparametric, and--by necessity--computer-intensive methods of data analysis. Time series analysis in particular is vital in many diverse scientific disciplines, e.g., in economics, engineering, acoustics, geostatistics, biostatistics, medicine, ecology, forestry, seismology, and meteorology.  As a consequence of the proposal's development of efficient and robust methods for the statistical analysis of dependent data, more accurate and reliable inferences may be drawn from datasets of practical import resulting in appreciable benefits to society.  Examples include data from meteorology/atmospheric science (e.g. climate data), economics (e.g. stock market returns), biostatistics (e.g. fMRI data), and bioinformatics (e.g. genetics and microarray data)."
"1309954","Collaborative Research:   Renyi Divergence-based Robust Inference in Regression, Time Services and Association Studies","DMS","STATISTICS","07/15/2013","07/24/2013","Ross Iaci","VA","College of William and Mary","Standard Grant","Gabor Szekely","06/30/2017","$54,001.00","","riaci@wm.edu","1314 S MOUNT VERNON AVE","WILLIAMSBURG","VA","231852817","7572213965","MPS","1269","","$0.00","This collaborative research project focuses on developing a novel approach to dimension reduction in regression, time series, and multivariate association studies based on a family of Rényi divergences, with a central theme of providing estimators that are inherently robust to data contamination, sustaining only a minimal loss in efficiency. This family not only characterizes the conditional independence underlying the concept of sufficient dimension reduction in regression and time series, but also characterizes independence between canonical variates in multivariate association studies. The novelty of the approach lies in exploiting a tuning parameter of the family, which balances the efficiency and the degree of robustness of the estimators. In each of the three areas, this project focuses on investigating a host of issues such as: (i) the computation of estimates, (ii) the detection of the true dimension, (iii) the selection of an optimal tuning parameter, and (iv) a formal justification of the method via theory. Furthermore, the project focuses on carrying out an in-depth study of robustness via influence functions and sample/empirical influence functions. Finally, the project focuses on finding an optimal Rényi divergence measure that is both robust and efficient, without the need for prior outlier detection or removal. <br/><br/>Rapid advances in technology have led to an information overload in most sciences. A typical characteristic of many contemporary datasets is that they are relatively high-dimensional in nature. This has prompted a shift in the applied sciences toward a different relationship-study genre arising in regression, time series and multivariate association, popularly known as dimension reduction, whose goal is to reduce the dimensionality of the variables as a first phase in the data analysis. However, the presence of outliers in high-dimensional datasets adversely affects the performance of existing dimension reduction methodologies, resulting in conclusions that are not completely reliable. Given that outliers are commonly encountered in high-dimensional datasets and that their presence is hard to detect, there is an urgent need to identify dimension reduction methods that possess some degree of automatic robustness, or non-sensitivity, to outliers. The proposed project provides robust dimension reduction methods, which would contribute significantly to the analysis of high-dimensional data arising in fields such as the social sciences, machine learning, sports, economics, environmental studies, morphometrics and cancer studies, among others. In fact, this project will not only provide novel tools for scientists in various disciplines to obtain reliable conclusions on high-dimensional data analysis, but also significantly advance the statistical theory, thereby paving a new research path in dimension reduction."
"1255187","CAREER: Bringing richly structured Bayesian models into the discrete-data realm via new data-augmentation theory and algorithms","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2013","06/29/2017","James Scott","TX","University of Texas at Austin","Continuing Grant","Gabor Szekely","08/31/2018","$400,000.00","","james.scott@mccombs.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1269, 8048","1045","$0.00","The modern Bayesian toolbox contains many highly structured models tailored for continuous data.  These tools allow us to handle data sets that are not merely large, but also dense: varying in time or space, rich with covariates, or deeply layered with hierarchical structure, and indexed in ever more baroque ways. But the field has not progressed nearly as far in modeling discrete data.  Indeed, many common discrete-data models have long been viewed as too difficult to work with on a routine basis, due to the analytically inconvenient form of the likelihood functions that arise.  The investigator will develop inferential tools for discrete-data problems that exploit new data-augmentation schemes as well as recent advances in parallel and distributed computing.<br/><br/>Discrete data sets typically involve either event-count or categorical outcomes (yes or no, this choice or that).  The goal of the investigator's research is to leverage detailed spatial information to better model and predict these outcomes.  In many cases this will involve physical space.  For example, spatial patterns are very important when public health authorities look for excess reports of respiratory infection at a cluster of hospitals, or when law-enforcement officers deploy equipment that detects radiation anomalies at a crowded public event.  But it may also involve a more abstract notion of space.  For example, patients in a clinical trial can be located in a space defined by their genes and behavior.  This information is useful for personalized medicine: that is, deciding whether someone belongs to a special sub-group that is helped by a drug, even if the wider population isn't.  Though the proposed research is in the area of statistical methodology, the work is inherently interdisciplinary, and seeks to provide statistical solutions for pressing scientific problems. The PI has very good ideas how to transform the education of statistics in a more interdisciplinary and more data oriented way so that UT-Austin can become ""one of the most innovative statistics programs in the world"" as the PI writes. His research is integrated into the education of the new Division of Statistics and Scientific Computation (SSC) at UT-Austin."
"1308009","Statistical Inference Based on an Integrated Likelihood","DMS","STATISTICS","07/01/2013","07/14/2013","Thomas Severini","IL","Northwestern University","Standard Grant","Gabor Szekely","06/30/2016","$99,996.00","","severini@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1269","","$0.00","Integrated likelihood methods provide a promising approach to likelihood inference in which any nuisance parameters in the model are eliminated by averaging the likelihood with respect to a weight function for the nuisance parameter.Such an  integrated likelihood offers a number of advantages over other approaches to likelihood inference: it is always available; it is based on averaging rather than maximization, an approach that is often more reliable; it has a certain type of finite-sample optimality; by appropriate selection of the weight function it has many of the same properties of marginal and conditional likelihood functions, when either of those is available.  Integrated likelihood methods combine ideas from both Bayesian and non-Bayesian   inference and, hence, provide a hybrid method with many of the benefits of both approaches.  These methods represent a new way of thinking about likelihood inference in models with nuisance parameters in which the traditional approach of eliminating nuisance parameters through maximization is replaced by averaging. The research will focus on three broad areas: a study of the asymptotic properties of point estimators and the associated standard errors of maximum integrated likelihood estimators; the use of integrated likelihood methods for estimation in models with an unknown function, and the application of integrated likelihood theory and methodology to models with random effects.  In each of the areas, models with a high-dimensional nuisance parameter will be of particular interest. This work will lead to better understanding of the  properties of likelihood-based methods of inference as well as the development of new statistical methodology based on those results.<br/><br/>This research develops a new approach to statistical theory and methodology, based on the use of an integrated likelihood function.   These methods are used in the analysis of virtually all statistical models and in all fields of application. In particular, integrated<br/>likelihood methods are useful in complex statistical models and these methods have been used successfully in applications ranging from the reliability of computer software to the analysis of genetic data. In contrast to some other recently-developed methods, which require considerable background in advanced statistical theory, the integrated likelihood approach is computation-based and relatively straightforward to understand and to implement. Thus, the results of this proposed research are useful for researchers in a wide range of fields. The results also further our understanding of the properties of statistical models and, hence, play an important role in the education of researchers in statistics and related fields."
"1330132","EAGER: Algorithm-Hardware Co-Design for Multivariate Data Analysis","DMS","STATISTICS, Big Data Science &Engineering","07/15/2013","04/24/2014","Wing Hung Wong","CA","Stanford University","Continuing Grant","Christopher Stark","06/30/2016","$299,938.00","","whwong@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269, 8083","1269, 7916, 8083","$0.00","The goal of this project is to develop new methods for unsupervised learning from multivariate data based on counting and comparing frequencies of data patterns.  A recursive testing approach will be used to infer the multivariate distribution. The investigator will use hardware-algorithm co-design to achieve qualitative improvement over existing methods in computational time as well as in the maximum data dimension and sample size that can be handled. The economic feasibility of making this methodology widely available will also be investigated. <br/><br/>This research is motivated by the challenge of ""Big Data"" analysis where the high dimensionality and extremely large sample size had made it infeasible to apply traditional statistical methods. The new methods developed in this project will be applied to several ""big data"" applications such as the analysis of videos, next generation sequencing data and microblogs. By developing the statistical methods for such analyses as well as customized computing resources to make these methods scalable to extremely large data sets, this research will enable more effective use of the rich information embedded in these data. Finally, the multidisciplinary approach integrating statistical, computational and hardware expertise is well suited for the training of next generation data scientists."
"1307973","Collaborative Research: Randomization inference for contemporary problems in statistics","DMS","STATISTICS","09/01/2013","08/29/2013","Joseph Romano","CA","Stanford University","Standard Grant","Gabor Szekely","08/31/2016","$150,000.00","","romano@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","The investigators continue the development of new methodology and the accompanying mathematical theory for problems in multiple testing and inference, driven by the many burgeoning applications in the information age.  Further motivation for valid methods stems from exploratory analysis of large data sets, where the process of ""data snooping"" (or ""data mining"") often leads to challenges of multiple testing and simultaneous inference.  In such problems, the statistician is faced with the challenge of accounting for all possible errors resulting from a complex analysis of the data, so that any resulting inferences or conclusions can reliably be viewed as ""real"" rather than spurious findings or artifacts of the data.  It is safe to say that the mathematical justification of sound statistical methods is not keeping pace with the demand for valid new tools.  In particular, the investigators develop randomization tests as  inferential methods for semi-parametric and nonparametric models that do not rely on unverifiable assumptions.  To a great extent, resampling methods, such as the bootstrap and subsampling, are successful in many problems, at least in an asymptotic sense, but for many problems they are unsatisfactory.  Examples of such problems in contemporary statistics include ""high"" dimensional problems, where the ""curse of dimensionality"" may cause resampling methods to break down, and ""non-regular"" problems, where a lack of convergence of the approximation that is not at least locally uniform in the underlying data generating process may cause resampling methods to break down.   Some specific problems addressed include Tobit regression and linear regression with weak instruments.  Moreover, resampling methods do not enjoy exact finite-sample validity, which is perhaps the main reason permutation and rank tests are so commonly used in many fields, such as medical studies.  The investigators apply randomization tests to many new problems that statisticians face, despite issues of high dimensionality, simultaneous inference, unknown dependence structures, non-Gaussianity, etc.  An exciting feature of the approach is that, properly constructed, randomization tests enjoy good robustness properties in situations where the assumptions guaranteeing finite-sample validity may fail.  Mathematical theory is developed as well as feasible computational constructs.<br/><br/>Useful statistical methodology is the key tool to analyzing any study or scientific experiment.  Recently, the demand for efficient and reliable confirmatory statistical methods has grown rapidly, driven by problems arising in the analysis of DNA microarray biotechnology, econometrics, finance, educational evaluation, global warming, and astronomy, as well as many others.  In general, the philosophical approach is to develop practical methods that have both robustness of validity and robustness of efficiency so that they may be applied in increasingly complex situations as the scope of modern data analysis continues to grow.  The broader impact of this work is potentially quite large because the resulting inferential tools can be applied to such diverse fields as genetics, bioengineering, image processing and neuroimaging, clinical trials, education, astronomy, finance and econometrics. The results will be widely disseminated, and public software of new statistical tools made accessible whenever possible. The many thriving fields of applications demand new statistical methods, creating challenging and exciting opportunities for young scholars under the direction of the investigators."
"1346916","Travel Support for The Future of Statistical Sciences Workshop, London","DMS","STATISTICS, Methodology, Measuremt & Stats","08/15/2013","08/23/2013","J. Lynn Palmer","VA","American Statistical Association","Standard Grant","Gabor Szekely","07/31/2014","$45,000.00","Ronald Wasserstein","palmer@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269, 1333","7556","$0.00","""The Future of Statistical Sciences"" workshop will be held in London November 11-12, 2013. This research conference is the capstone event of the International Year of Statistics.  It is being organized by the same five societies (American Statistical Association (ASA), International Biometric Society (IBS), International Mathematical Society (IMS), International Statistical Institute (ISI), and the Royal Statistical Society (RSS)) that founded the International Year of Statistics. The Workshop will bring together not only researchers but thought leaders from across the globe, including junior researchers in statistics who are rising stars, scientists from data-intensive fields, funders, and professional science writers to discuss the current state and future directions of the statistical sciences and to consider actionable strategies for advancing the profession. Bringing these statistical and other leaders together with other participants through this Workshop will provide an extraordinary opportunity to enhance existing collaborations and build new ones, which will greatly enhance the future statistical workforce. In addition to these benefits, a greater online presence and publications in high-profile scientific journals, a paper on the current state and future directions of statistics and the statistical workforce will be developed and made widely available. More information about the conference can be found at: http://www.statistics2013.org/about-the-future-of-the-statistical-sciences-workshop/<br/><br/>This award supports the participation of U.S. junior researchers, women, and members of underrepresented groups in an international workshop to be held in London November 11-12, 2013. The workshop is being put together by the five largest professional statistical societies in the world. Statisticians work in a vast array of fields: advancing science; creating new methodology; providing information that shapes public policy; making our medicines more effective, our food safer, and our planet better. Statisticians come from many cultures, speak many languages, and arrive at statistics as their profession through many paths. This workshop will showcase the breadth and importance of statistics and highlight the extraordinary opportunities for statistical research in the coming decade.  The workshop will bring together a diverse group of top statistical researchers, including many relatively young researchers, women and minorities, and thought leaders, including non-statisticians in diverse areas, and give them and other participants an opportunity to think about where statistics should go as a discipline and the lessons learned that can guide us in the future.  It will be a space in which statistical scientists and scientists in other fields can interact and develop a shared vision for the future."
"1344683","Bayes 250 Conference","DMS","STATISTICS","08/01/2013","07/31/2013","James Berger","NC","Duke University","Standard Grant","Gabor Szekely","07/31/2014","$20,000.00","","berger@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","7556","$0.00","A five-day conference is being held at Duke University during December 15-19, 2013. The conference consists of three workshops. The Objective Bayes 2013 Workshop, from December 15-19, consists of three days of scientific sessions, one day of tutorials for graduate students and new researchers, and a poster session, all centering on major recent developments in objective Bayesian methodology. The EFaB@Bayes 250 Workshop, from December 15-16, promotes research and education on Bayesian methods in economics, finance and business, and academic-industry interactions and outreach across these areas. Bayes 250 Day on December 17, celebrates the 250th anniversary of the presentation of Thomas Bayes' paper, with lectures showing the transformative impact of Bayesian analysis on a number of scientific disciplines. The scientific topics considered include foundations of objective Bayesian analysis; objective priors, unification of statistics; spatial and temporal methods; model uncertainty with huge model spaces; nonparametric analysis; massive multiple testing and subgroup analysis; computational issues, especially with massive data; uncertainty quantification; the interface of statistics and computational modeling of processes; and objective Bayesian methods in high energy physics and astrophysics.<br/> <br/>The Bayes 250 Conference, a five day conference at Duke University during December 15-19, 2013, brings together leading researchers in Bayesian analysis from around the world. The last decade has seen an explosion of interest in Bayesian statistical methodology across engineering, medicine and science, and this meeting provides a timely forum for the exchange of recent research developments, provides opportunities for new researchers and underrepresented groups, and establishes new collaborations that will channel efforts into pending problems and open new directions for investigation. The conference is timed to coincide with the 250th anniversary of the publication of the famous paper by Thomas Bayes. The organization of the meeting is designed to facilitate interactions, with at most 7 talks per day and extensive discussion built in. Tutorials and an evening poster session enhance the conference experience for graduate students and new researchers. Funding primarily supports graduate students and new researchers."
"1309057","Bayesian Recursive Partitioning and Inference on the Structure of High-Dimensional Distributions","DMS","STATISTICS","07/01/2013","04/09/2015","Li Ma","NC","Duke University","Continuing Grant","Gabor Szekely","06/30/2016","$159,873.00","","li.ma@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","","$0.00","This research concerns one of the most important and pervasive classes of problems in modern data analysis---inference on the structure of probability distributions. Specific inference problems to be addressed fall into two broad categories. The first involves inference on the structure of a single probability distribution, including estimation of joint and conditional densities, variable selection in linear regression, and the testing of independence and conditional independence among variables. The second involves inference on the relationship across multiple distributions. This includes testing whether two (or more) data samples have the same underlying distribution, and learning the structure of their difference, with particular interest given to finding local structures---differences that lie in small subsets---in large high-dimensional spaces. To address these problems, the investigator puts forward a novel framework for constructing Bayesian priors on multivariate distributions through recursive partitioning. Inference using this framework is flexible and adaptive. Moreover, the generative nature of these priors facilitates the modeling of dependence structure across multiple distributions and this leads to powerful methods for comparing distributions. To address the computational challenges in high-dimensional problems, the investigator lays out a set of computational strategies and proposes to develop several algorithms that can drastically improve the efficiency of Bayesian posterior inference in high-dimensional problems. These strategies utilize the recursive nature of the proposed framework to efficiently explore the global landscape of the corresponding posterior distributions.<br/><br/>Inference on the structure of probability distributions lies at the heart of many scientific inquiries, and new statistical theory and methods are urgently needed to accommodate the ever increasing dimensionality of data sets that is commonplace in modern scientific investigations. Two specific applications that motivate this project are the analysis of high-dimensional flow cytometry data in systems biology for unraveling the functional relationships among proteins as well as the mapping of human genes to various qualitative and quantitative traits, in particular those of common diseases such as cancer and diabetes. The concepts, theory, methodology, and algorithms developed in this project will be directly applicable to these problems, as well as to the analysis of data sets arising from a wide variety of other fields ranging from environmental science to economics."
"1307178","Statistical Analysis of High Dimensional Manifold Data","DMS","STATISTICS","07/01/2013","05/29/2015","Sungkyu Jung","PA","University of Pittsburgh","Continuing Grant","Gabor Szekely","06/30/2016","$110,000.00","","sungkyu@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","","$0.00","Manifold-valued data appear frequently in shape and image analysis, computer vision, biomechanics and many others. Medical imaging data in studies of the variability of human organ shapes lie on a fairly high dimensional nonlinear manifolds, where the challenge is two-fold: high dimensionality with a low sample size (data are expensive to gather) and naturally imposed non-Euclidean geometry. The proposed research aims to answer scientific questions arising in object shape analysis and to provide solid mathematical basis for more complex problems. First, the investigator takes and extends the strategy of backward dimension reduction with regularization framework. Sparse representation of shape and directional data are proposed and their properties are studied. In the regression context, polynomial regression for manifold-valued response to model and test non-geodesic trends is proposed. An extension of local polynomial modeling is also considered. The investigator also explores efficient computational methods. <br/><br/>The study of object shape is crucial for understanding the population of human anatomical objects and revealing the interplay between biomarkers/clinical outcomes and object shape variations. Due to the advanced technology, the modern object shape data become big and complex, but conventional methods lack considerations on the special geometric structure of the data types. This research project aims to provide new statistical methodologies for exploratory and confirmatory analysis of the large-scale non-standard data types, including the object shapes. The project will also produce statistical tools, which may be applied in many fields including biomechanics, computer vision, medical studies and biological sciences."
"1306972","Estimation, model selection and inference in two classes of non- and semi-parametric models for repeated measurements","DMS","STATISTICS","08/15/2013","08/27/2013","Shujie Ma","CA","University of California-Riverside","Standard Grant","Gabor Szekely","07/31/2016","$99,948.00","","shujie.ma@ucr.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","MPS","1269","","$0.00","In this proposal, the PI plans to develop new non- and semi- parametric regression strategies in the context of repeated measures data with diverging number of covariates. The methods are supported by large sample theories. Specifically, the PI aims at: (1) developing efficient estimation and model selection procedures by incorporating the within-subject correlations in two proposed broad classes of models: a generalized varying index coefficient additive model and a generalized additive coefficient model. The PI proposes to employ a group penalized estimation method with polynomial splines; (2) proposing an oracally efficient and computationally expedient two-step spline estimation procedure for nonparametric additive functions. The proposed two-step estimator is proved to have the oracle efficiencies in terms of both model estimation and selection. Such superior properties are achieved by taking advantage of the joint asymptotics of spline functions with different smoothing parameters and regularization; (3) conducting statistical inferences after model selection. The PI proposes a new nonparametric inferential tool to test whether a nonparametric function has a given parametric form, and constructs simultaneous confidence bands to provide global inference of functions with their asymptotic properties established; and (4) studying model checking problems for the proposed structured models by an integrated conditional moment test.<br/><br/>The proposal meets the immediate needs from various scientific areas for analyzing high dimensional repeated measures data within the non- and semi- parametric framework. The proposed research is motivated by the real data problems coming from the PI's interactions with researchers from different disciplines. The data applications include gene-environment and risk factor-environment interactions, nutrition scores, children growth, and economic growth problems. The completion of the proposed projects will greatly enhance the capability of researchers to analyze high-dimensional repeated measures data with more reliable, flexible and effective statistical methods. The research methods and results will be disseminated through journal publications, seminars, conferences and workshops, and they will be incorporated into a graduate course on longitudinal data analysis. Moreover, the project will promote teaching and training of undergraduate and graduate students on the state-of-the-art techniques in the research topics related to this proposal. The PI plans to provide practitioners with easy-to-implement and easy-to-interpret well-documented procedures coded in software such as R and to make the software packages available to the public."
"1308270","Collaborative Research: Developing a theoretical and methodological framework for high dimensional Markov chain Monte Carlo","DMS","STATISTICS","07/01/2013","04/22/2015","James Flegal","CA","University of California-Riverside","Continuing Grant","Gabor Szekely","06/30/2016","$99,998.00","","jflegal@ucr.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","MPS","1269","","$0.00","The investigators study multivariate methods for assessing the quality and ensuring the reliability of a Markov chain Monte Carlo (MCMC) experiment.  This work is strongly motivated by research in Bayesian methods for functional neuroimaging experiments, but will be applicable in any MCMC simulation.  Usually, Markov chain output is used to estimate a vector of parameters that contains multiple mean and variance parameters along with quantiles.  A fundamental question is when to terminate such a simulation.  The investigators study sequential fixed-volume stopping rules that allow construction of confidence regions for estimating the target vector, which describe the reliability of the resulting estimates.  Using these methods requires that the Markov chain converges at a geometric rate, which in turn yields a limiting distribution for the Monte Carlo error with an associated covariance matrix.  Estimating this matrix forms a major component of the research-a long standing open question in MCMC output analysis.  The investigators improve on existing methods, which enable effective estimation in the case where the target vector is moderately large. Moreover, the investigators study several methods for handling the setting in truly high-dimensional settings, i.e. when there are many more parameters than iterations in the Markov chain.  The investigators also formally study the convergence rates of component-wise MCMC samplers often encountered in the functional neuroimaging settings.  <br/><br/>Complex probability models are commonly used to help gain understanding of phenomenon in a range of fields including science, engineering, medicine, education, and law.  An example that motivates the investigators work is that of applied cognitive scientists modeling brain activity.  Inference from such probability models is usually obtained from computational approximations.  For a widely used computational technique, the investigators study the convergence properties and develop formal stopping rules focusing on high-dimensional practically relevant settings.  The statistical methodology developed here will provide scientists with sophisticated output analysis techniques, leading to greater confidence and reliability for their computational results."
"1308765","Analysis of Markov Chain Monte Carlo Algorithms with Applications to Bayesian Generalized Linear Mixed Models","DMS","STATISTICS","08/15/2013","08/27/2013","Jorge Roman Aponte","TN","Vanderbilt University","Standard Grant","Gabor Szekely","07/31/2015","$50,000.00","","jcroman@mail.sdsu.edu","110 21ST AVE S","NASHVILLE","TN","372032416","6153222631","MPS","1269","9150","$0.00","Bayesian statistical methods have become popular and their use continues to grow in the applied sciences. This increase in popularity is largely due to the availability of Markov chain Monte Carlo (MCMC) algorithms which allow for the estimation of posterior distributions. However, in a large number of applications, MCMC-based estimates are reported without a valid measure of their quality and there is no coherent strategy for deciding when to stop the simulation. To a large extent, this is due to the fact that analyses (asymptotic and non-asymptotic) of the usual MCMC estimators are typically challenging. For example, as opposed to classical Monte Carlo methods, establishing the central limit theorems (CLTs) that allow for an asymptotic analysis of MCMC estimators is not straightforward. This is a serious practical problem that needs attention because the current valid strategies for assessing the quality of estimation and the choice of (MCMC) sample size rest upon theoretical assumptions such as the existence of CLTs. This project addresses this issue and consists of two parts. The first part consists of asymptotic and non-asymptotic analyses of MCMC estimators based on Gibbs samplers for several widely applicable Bayesian versions of the generalized linear mixed model. The investigator considers probit and identity link functions as well as popular choices of proper and improper prior densities for the parameters. In the second part of the project, the investigator attempts to generalize several results for two-variable Gibbs samplers concerning the convergence rate of the algorithm and its connection to the parametrization of the statistical model. The goal is to generalize known results to the general k-variable Gibbs sampler. Since Gibbs samplers are very popular MCMC algorithms, these results will likely have many applications. In particular, they could be used in the analyses performed in the first part of the project. <br/><br/>This project addresses the quality of estimation in MCMC procedures used in Bayesian statistics, which is a very important issue in the applied sciences. The reason why this is so important is that misleading MCMC-based estimates can lead to incorrect conclusions, which could potentially negatively affect public policy. The main goal of this research is to find simple sufficient conditions (that the user can check) under which the considered MCMC procedures are honest; that is, there is at least one valid measure of the quality of estimation and a coherent strategy for deciding when to stop the simulation. The more ambitious goal is to provide the MCMC user with explicit (non-asymptotic) bounds on the number of iterations needed to achieve a predetermined level of accuracy. The statistical models and MCMC algorithms considered in this project have numerous applications in nearly every scientific discipline. Consequently, the results produced in this project will be used by researchers in many different fields."
"1409504","CAREER: An integrated probabilistic approach to discrete and continuous extremal problems via information theory","DMS","PROBABILITY, STATISTICS","01/01/2013","05/26/2020","Mokshay Madiman","DE","University of Delaware","Continuing Grant","Tomek Bartoszynski","04/30/2021","$392,865.00","","madiman@udel.edu","220 HULLIHEN HALL","NEWARK","DE","197160099","3028312136","MPS","1263, 1269","1045, 1187","$0.00","Mathematics  abounds  with extremal  problems  problems  where  the  goal  is  to  minimize some  functional  applied  to  a  class  of  objects  under  some constraint,  identify  the  extremal  objects,  and  investigate  the stability  of  extrema.  Relevant  examples  range  from  some  in the  continuous  world  (isoperimetric  phenomena  in  convex geometry,  functional  analytic  inequalities),  to  some  in  the<br/>discrete  world  (structural  phenomena  in  additive  combinatorics), and  some  in  both  (maximum  entropy  problems  in  statistics, limit  theorems  in  probability).  A  natural  language  for  all  of these  problem  classes  is  probability,  and,  although  not obvious,  information  theory.  The  project  will  develop  new formulations  of  extremal  problems  from  each  of  these  fields in  terms  of  information-theoretic  inequalities,  and  then  use  a variety  of  tools  from  analysis,  probability,  convex  geometry, combinatorics,  and  information  theory,  to  make  progress  on them.  The  unifying  nature  of  the  perspective  adopted  will bridge  discrete  and  continuous  problems  using  a  common  set  of tools,  and  enable  significant  cross-fertilization.  Furthermore, some  of  the  information-theoretic  inequalities  developed, combined  with  statistical  decision  theory,  will  be  applied  to novel  statistical  challenges  involving  multiple  players  that arise  in  engineering,  economics,  and  biology  (specifically, theoretical  foundations  for  the  problems  of  data  pricing  and distributed  inference).    The  project  will  use information-theoretic  thinking  to  make  advances  on  challenging mathematical  problems  from  the  three  seemingly  disparate  fields of  convex  geometry,  arithmetic  combinatorics,  and  probability.<br/><br/>Apart  from  the  intrinsic  significance  of  these  areas  within mathematics,  they  have  much  practical  significance - convex geometry  finds  applications  in  medical  tomography,  arithmetic combinatorics  in  computer  science,  and  probability  is ubiquitous  as  the  foundation  of  statistical  inference.  The interpretability  and  unifying  nature  of  the  proposed  research, and  the  diversity  of  tools  it  uses,  create  wonderful  opportunities   for  student  motivation.  Newly  developed  courses  and  a resource  website  on  information  theoretic  approaches  to extremal  problems  will  exploit  these  opportunities.  The investigator  will  disseminate  key  findings  through  survey articles,  organize  an  interdisciplinary  workshop,  and communicate  the  excitement  of  research  through  non-academic public  lectures  to  attract  promising  students  to  the mathematical  sciences.  The  applied  component  of  the  research would  also  have  broad  impact,  by  contributing  to  how  data collectors  and  vendors  come  up  with  pricing  mechanisms  (e.g., for  pricing  of  advertisements  by  search  engines),  and  by improving  the  way  networks  of  sensors  collect  and  use  data for  various  applications  (e.g.,  for  disaster  recovery coordination  or  smart  kindergartens)."
"1332709","A NISS/ASA Writing Workshop for New Researchers","DMS","STATISTICS","07/01/2013","07/19/2013","J. Lynn Palmer","VA","American Statistical Association","Standard Grant","Gabor Szekely","12/31/2014","$16,800.00","","palmer@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","7556","$0.00","This award will support a Writing Workshop for Junior Researchers, which will be held at the Joint Statistical Meetings, August 3-8,  2013, in Montreal, Canada. The workshop will train junior researchers in effective technical writing. The goal is to help new researchers in the statistical sciences who seek to publish their research or to present their research plans in the form of grant proposals for federal funding. Researchers, especially new researchers, often have difficulty disseminating their research results not because of the quality of the research but rather because of inappropriate choices of publication venues for the particular research and/or because of poor presentation of technical material to the chosen audience. The National Institute of Statistical Sciences and the American Statistical Association will manage the Workshop. <br/><br/>This workshop will open with tutorial sessions on the organization of material for a technical article or grant application, on technical writing techniques and on the specific missions and audiences of key journals in the statistical sciences. Then each participating new researcher will work individually with an experienced journal editor as mentor to address these issues on an individualized basis for draft of the new researcher's work in progress. Revisions following this guidance will be critiqued by the mentor to assure that the new researcher's implementation of writing techniques has been successful before the article or the grant proposal is submitted for review."
"1255045","CAREER: Statistical Methodology in Multi-view Learning with Large Data","DMS","STATISTICS, Division Co-Funding: CAREER, EPSCoR Co-Funding","06/01/2013","02/25/2013","Mark Culp","WV","West Virginia University Research Corporation","Standard Grant","Gabor Szekely","05/31/2018","$400,000.00","","mculp@stat.wvu.edu","886 CHESTNUT RIDGE ROAD","MORGANTOWN","WV","265052742","3042933998","MPS","1269, 8048, 9150","1045, 9150","$0.00","In complex scientific research classification and regression problems, it is common for several different data sets to be used to describe the response. Each of these data sets provides a unique view of the response, but typically none of these views describes the response perfectly.  The investigator develops computationally efficient statistical methodology to model multi-view data within a framework for statistical analysis, variable and view selection, and the interpretation of results.  The methodology is based on both regularization approaches involving sparse penalties in additive models and algorithmic intensive iterative-based approaches.  In addition, the investigator provides a solid foundation for analysis of residuals in this context, establishes the consistency of the model, and addresses the practical issue of concurvity. The investigator is committed to raising awareness in scientific research communities and industries of the advantages of using these modeling techniques in the analysis of complex research problems involving data from multiple sources and to training future statisticians and related professionals through hands-on experiences with these data sets.  To support this effort, the investigator develops and maintains a powerful, user-friendly statistical software package to implement this methodology.<br/><br/>In the modern world our ability to collect data from many different sources has expanded dramatically due in part to computer innovations over the past few decades.  What has not kept pace is the ability to analyze data from many different sources simultaneously.  As a result, scientific researchers in academia and industry are not fully harnessing the information that can be found by appropriately combining multiple, diverse sources of data in a way that can provide interpretable results. This is a challenging problem involving advances in statistics, computer science, mathematics, and database management.  The investigator addresses this problem from a statistical analysis viewpoint.  Many applications of this research involve new statistical methods to help with cancer research, pharmacology, genetics, proteomics, text data processing, and homeland security.   The need for analyses of large, complex, multi-view data sets is substantial in scientific research today, and this need is currently unmet. The results of this work are transforming how researchers in many fields analyze and interpret data."
"1301845","15th IMS New Researchers Conference","DMS","STATISTICS","08/01/2013","03/26/2013","Aarti Singh","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","07/31/2014","$25,000.00","","aartisingh@cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","7556","$0.00","The proposed meeting will take place over three days, August 1-3, 2013, at the Centre de recherches mathematiques in Montreal.<br/>This proposal describes plans for the 15th New Researchers Conference, a conference series organized <br/>by and held for junior researchers under the auspices of the Institute of Mathematical Statistics (IMS). <br/>The primary objective is to provide a much needed venue for interaction among new researchers in <br/>statistics and probability. The proposed meeting will take place over three days, August 1-3, 2013, at the <br/>Centre de recherches mathematiques in Montreal. The meeting will be held immediately preceding the <br/>Joint Statistical Meetings (JSM). Participants will be statisticians and probabilists who have received their <br/>Ph.D. within the past five years or are expecting to receive their degree within the same year. Each <br/>participant will present a short expository talk and a poster. Topics will cover a variety of areas in <br/>statistics and probability, from theory and methods to applications. Senior speakers will give plenary <br/>talks for inspiration and take part in four discussion panels covering topics of importance for young <br/>people embarking on an academic/research career: teaching, mentoring, publishing and funding. <br/> <br/>This conference series is explicitly aimed at training the future leaders and workers in statistics and <br/>probability. In helping to create networks of new researchers, it lays the groundwork for future <br/>collaboration and informal exchange of ideas and knowledge. It is also critical for building professional <br/>cohesion within the fields of statistics and probability and setting new frontiers for research. Meeting <br/>people outside of one's research specialty area, and learning about their research, favors a more <br/>comprehensive view of research, which is important when taking editor positions and other professional <br/>service activities. The conference attracts participants with different backgrounds and nationalities, and <br/>underrepresented groups (women, minorities and people with disabilities) are explicitly encouraged to <br/>attend. The conference website is available at http://www.math.mcgill.ca/nrc2013."
"1308462","Modeling Neural Activity: Statistics, Dynamical Systems and Networks","DMS","STATISTICS, MATHEMATICAL BIOLOGY","04/01/2013","03/25/2013","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Xiaoming Huo","03/31/2014","$12,000.00","","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 7334","7556","$0.00","The workshop ""Modeling Neural Activity: Statisics, Dynamical Systems, and Networks (MONA)"" will be held June 26-28, 2013, in Lihue, Hawaii. Computational neuroscience has grown, in distinct directions, from the success of biophysical models neural activity, the attractiveness of the brain-as-computer metaphor, and the increasing prominence of statistical and machine learning methods throughout science. This has helped create a rich set of ideas and tools associated with ``computation'' to studying the nervous system, but it has also led to a kind of balkanization of expertise. There is, especially, very little overlap between mathematical and statistical research in this area.  Important breakthroughs in computational neuroscience could come from research strategies that are able to combine what are currently largely distinct approaches.  A workshop ""Modeling Neural activity: Statisics, Dynamical Systems, and Networks (MONA)"" will be held June 26-28, 2013, in Lihue, Hawaii, with the purpose of exploring fruitful interactions of modeling ideas that come from mathematics, statistics, and biophysics.  An additional purpose of the workshop is to bring together U.S. and Japanese researchers in this area. While computational neuroscience is represented strongly in both the U.S. and Japan there has been too little concrete communication and interaction between research groups across our two countries.  Interaction across American and Japanese researchers should facilitate the advance of cross-disciplinary work.<br/><br/>Many disorders, such as ADHD, autism, and schizophrenia, as well as stroke and various neurodegenerative diseases, are thought to involve dysfunction of neural networks. Because computational neuroscience aims to supply principles for understanding the activity of individual and collective neural firing patterns, its successes can help in formulating mechanistic descriptions of pathophysiology. The workshop ``Modeling Neural Activity: Statisics, Dynamical Systems, and Networks (MONA)"" is at the interface between mathematics, statistics, and neural network analysis, and has as its goal to generate new and productive lines of research."
"1309174","Advancing Theory and Computation in Statistical Learning Problems","DMS","STATISTICS","07/01/2013","05/19/2015","Ryan Tibshirani","PA","Carnegie-Mellon University","Continuing Grant","Gabor Szekely","06/30/2017","$150,000.00","","ryantibs@berkeley.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","This research is composed of four related statistical learning projects. The first two projects are theoretical. In the first, the investigator will study of degrees of freedom (i.e., the effective number of parameters) of adaptive modeling techniques. It has been shown that variable selection procedures based on the L1 norm, such as the lasso, exhibit control over their effective number of parameters, since adaptivity here is counterbalanced by shrinkage in coefficient estimation. This project instead considers adaptive procedures that do not employ shrinkage, such as best subset selection, in which the effective number of parameters is (comparatively) greatly inflated. In the second project, the investigator will examine trend filtering, a recently proposed nonparametric regression estimator fit by penalizing the L1 norm of discrete derivatives. Trend filtering estimates can be computed efficiently (e.g., using the work of the third project), but their theoretical properties are not well-understood. The goal is to study the rate of convergence of trend filtering estimates over broad function classes, and make detailed comparisons to existing nonparametric regression estimators (such as smoothing splines, locally adaptive regression splines, etc.). The last two projects are computational. The third project is focused on efficient computations for the generalized lasso path algorithm. The generalized lasso is an estimator that encourages specific structural properties, as opposed to pure sparsity itself, using the L1 norm; one such example is the trend filtering estimator mentioned above. The fourth and final project is an extension of the idea behind stagewise regression to general convex regularization problems. Forward stagewise regression is a simple, scalable algorithm whose estimates can be seen as an approximation to the lasso regularization path. The stagewise extension to general problems produces efficient approximation algorithms for the group lasso, matrix completion, and more; approximation guarantees are unknown and will be studied.  <br/><br/>Statistical modeling, estimation, and inference are becoming integral aspects of problems in many scientific disciplines. As a result, the field of statistical learning---which broadly encapsulates these three statistical tasks---has witnessed a recent explosion of research. Arguably, current research in this field focuses on creating new methods or extending methods to new domains, and much less so on understanding existing methods. Instead, the investigator will pursue four projects aimed at (i) deepening our understanding of a few well-known (but not as well-understood) statistical learning techniques, and (ii) developing algorithms so that we can employ these techniques efficiently at a larger scale, and hence evaluate their performance.  Code for such algorithms will be made freely available through open-source software.  Potential applications of this work include the forecasting of medical diagnoses, the modeling of brain signals in neuroscience, and the development of recommender systems."
"1310002","Optimality Landscapes and Exploratory Data Analysis","DMS","ADVANCES IN BIO INFORMATICS, STATISTICS, Cross-BIO Activities, MSPA-INTERDISCIPLINARY","08/01/2013","07/14/2013","Andrew Nobel","NC","University of North Carolina at Chapel Hill","Standard Grant","Gabor Szekely","07/31/2017","$270,000.00","Sreekalyani Bhamidi","nobel@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1165, 1269, 7275, 7454","8007","$0.00","The investigators and their students study the development, implementation and application of iterative search procedures for unsupervised exploratory data analysis. In particular, they develop statistically principled procedures for discovering patterns in high dimensional data, including biclustering and correlation mining of genomic data, and community detection in complex networks arising in computational sociology and public policy.  Complementing the methodological component of the research, the investigators and their students also study the development of general theoretical tools to analyze iterative data mining procedures, and the properties of their associated local optima. They develop probabilistic tools, including new variants of Stein's method for normal approximation and new Gaussian comparison theorems, to understand asymptotic properties of typical local optima, and the dependence of these optima under different assumptions on the underlying signal, beginning with the null setting in which only noise is present. Their research is carried out in the context of ongoing collaborations with UNC faculty in the Medical School, and in the Departments of Genetics, Public Policy, and Mathematics. <br/><br/>The broad subject of the proposal is the development, theoretical analysis, and application of exploratory methods for large data sets. By exploratory methods, we mean those that search large data sets for significant patterns or configurations that may be of organizational or scientific interest. Examples include patterns that may distinguish types of a disease, that help target a drug or assess its efficacy, and patterns that identify among a large number of people a smaller community who frequently exchange text messages.  In many cases, a numerical score is used to assess the potential importance of a pattern, and attention then turns to finding a pattern with a large score.  Our primary interest is in search procedures that begin with a candidate pattern, then search for closely related patterns in the data that have higher score, repeating this procedure until they reach a pattern where no further (local) improvements are possible. Procedures of this sort are routinely applied in large data problems where finding the ``best'' pattern (the pattern with the largest score) is computationally prohibitive.  We are developing and applying new, statistically based search procedures for several important tasks arising in the exploratory analysis of large data sets, including data mining and community detection.  At the same time, we are developing fundamental theory to justify and inform the application of the iterative search procedures. Our work is being carried out in the context of ongoing collaborations with UNC faculty in the Medical School, and in the Departments of Genetics, Public Policy, and Mathematics."
"1309619","Collaborative Research: Inference for Linear Model Parameters in Model-free Populations","DMS","STATISTICS","09/15/2013","09/11/2013","Kai Zhang","NC","University of North Carolina at Chapel Hill","Standard Grant","Gabor Szekely","08/31/2015","$50,000.00","","zhangk@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","It is common statistical practice to employ a linear, least squares analysis, even though the assumptions justifying the inference from such an analysis are not valid. The current goal is to provide a guide to useful inferential statements that can be justifiably used in the face of this common dilemma. For this purpose, begin with a nearly model-free description of the data generating process: the observations are a random sample from a population consisting of vector-valued covariates and accompanying real responses. Inference is in the form of a linear description of the relation between the response and the covariates. The target of inference is the, suitably defined, best linear description of the population dependence of the response on the covariates. A first task is to provide a mathematical framework to accurately describe such a situation and enable its rigorous analysis. Within this formulation possible forms of inference can then be investigated. It can be shown that the conventional sample least-squares estimate of the linear coefficients has certain asymptotic optimality properties. But the conventional standard errors and confidence intervals for these coefficients are in general not asymptotically correct. Correct asymptotic inference is provided by suitable forms of either the bootstrap or the so-called sandwich estimator. The current research will discuss variations of these inferential procedures.  It will also describe situations in which these inferential procedures provide trustworthy results for realistic sample sizes. The model-free perspective leads to additional understanding of other important statistical settings involving possibly informative covariates. One of these relates to Randomized Clinical Trials in which interest centers on the Average Treatment Effect, compared to that of a placebo or alternate, standard treatment. The general formulation suggests use of a new estimator and related inference, and the properties and variants of this will be investigated.<br/><br/>Statistical practice is built on and justified by corresponding statistical theory. That theory has a common overarching paradigm: Statistical data is observed. A statistical description is adopted for this data and the data is then analyzed according to this statistical description. There is a presumption in this paradigm that the analytic model agrees sufficiently well with the actual model that generated the data. This is often not the case in practice. The current proposal builds a new, coherent theory that goes beyond the common paradigm in that it allows the statistical model for the data and the model for the analysis to be very different. The effect of the proposed research should be to first warn practitioners of often encountered but rarely recognized dangers. These are inherent in the common practice of using linear models such as regression analysis and ANOVA when they may not sufficiently accurately represent the true nature of the statistical sample. It will then provide alternate forms of inference that are valid and can be responsibly utilized in such situations. To complement its theoretical, methodological orientation the research maintains close connections with applications through the applied activities of several of the senior investigators in diverse areas including social science - especially criminology -operations research and health care."
"1308260","Collaborative Research:  Randomization Inference for Contemporary Problems in Statistics","DMS","STATISTICS","09/01/2013","08/29/2013","Azeem Shaikh","IL","University of Chicago","Standard Grant","Gabor Szekely","08/31/2016","$120,000.00","","amshaikh@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","","$0.00","The investigators continue the development of new methodology and the accompanying mathematical theory for problems in multiple testing and inference, driven by the many burgeoning applications in the information age.  Further motivation for valid methods stems from exploratory analysis of large data sets, where the process of ""data snooping"" (or ""data mining"") often leads to challenges of multiple testing and simultaneous inference.  In such problems, the statistician is faced with the challenge of accounting for all possible errors resulting from a complex analysis of the data, so that any resulting inferences or conclusions can reliably be viewed as ""real"" rather than spurious findings or artifacts of the data.  It is safe to say that the mathematical justification of sound statistical methods is not keeping pace with the demand for valid new tools.  In particular, the investigators develop randomization tests as  inferential methods for semi-parametric and nonparametric models that do not rely on unverifiable assumptions.  To a great extent, resampling methods, such as the bootstrap and subsampling, are successful in many problems, at least in an asymptotic sense, but for many problems they are unsatisfactory.  Examples of such problems in contemporary statistics include ""high"" dimensional problems, where the ""curse of dimensionality"" may cause resampling methods to break down, and ""non-regular"" problems, where a lack of convergence of the approximation that is not at least locally uniform in the underlying data generating process may cause resampling methods to break down.   Some specific problems addressed include Tobit regression and linear regression with weak instruments.  Moreover, resampling methods do not enjoy exact finite-sample validity, which is perhaps the main reason permutation and rank tests are so commonly used in many fields, such as medical studies.  The investigators apply randomization tests to many new problems that statisticians face, despite issues of high dimensionality, simultaneous inference, unknown dependence structures, non-Gaussianity, etc.  An exciting feature of the approach is that, properly constructed, randomization tests enjoy good robustness properties in situations where the assumptions guaranteeing finite-sample validity may fail.  Mathematical theory is developed as well as feasible computational constructs.<br/><br/>Useful statistical methodology is the key tool to analyzing any study or scientific experiment.  Recently, the demand for efficient and reliable confirmatory statistical methods has grown rapidly, driven by problems arising in the analysis of DNA microarray biotechnology, econometrics, finance, educational evaluation, global warming, and astronomy, as well as many others.  In general, the philosophical approach is to develop practical methods that have both robustness of validity and robustness of efficiency so that they may be applied in increasingly complex situations as the scope of modern data analysis continues to grow.  The broader impact of this work is potentially quite large because the resulting inferential tools can be applied to such diverse fields as genetics, bioengineering, image processing and neuroimaging, clinical trials, education, astronomy, finance and econometrics. The results will be widely disseminated, and public software of new statistical tools made accessible whenever possible. The many thriving fields of applications demand new statistical methods, creating challenging and exciting opportunities for young scholars under the direction of the investigators."
"1309465","High-dimensional Multi-stage Statistical Learning with Application to Dynamic Treatment Regimens","DMS","STATISTICS","08/01/2013","07/17/2013","Rui Song","NC","North Carolina State University","Standard Grant","Gabor Szekely","07/31/2017","$109,999.00","","rsong@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","","$0.00","Dynamic treatment regimens (DTRs) are sequential decision rules for individual patients that can adapt over time to an evolving illness. The ultimate goal is to accommodate heterogeneity among patients and find the DTR which will produce the best long term outcome. The patients receive treatments are often at multiple decision times. The effects of the covariates are often complex and the dimension of covariates are often very high. The broad, long-term objectives of this project are to develop statistical learning methodologies for optimal, personalized and single-stage or dynamic treatment regimens. Specifically, this project aims 1) to develop flexible semiparametric modeling tailored in single-stage or dynamic treatment regimens; 2) to develop a penalized Q-learning and valid statistical inference for estimating optimal dynamic regimens with censored outcome; 3) to develop effective variable selection strategies which can simplify and improve implementation and reproducibility of personalized treatment regimens. For all the goals, the desired asymptotic properties will be established rigorously and suitable numerical algorithms will be provided.<br/><br/>The outlined research project will bring more insights into multi-stage, high-dimensional statistical learning and will benefit future studies in this area. This study will develop flexible models and Q-learning methods in estimating personalized treatment regimens. A successful completion of this research will not only fill important gaps in statistical theory, but will also yield new tools for applied statisticians and other scientists. This project will foster more intensive collaborations among investigators from the Department of Statistics, the Department of Mathematics and the Health Systems Research Group in the Edward P. Fitts Department of Industrial and Systems Engineering at North Carolina State University. The proposed study will promote teaching, training and learning at North Carolina State University. Research conducted in this study will help develop advanced graduate courses in statistical learning and semiparametric methodology. It will create challenging statistical projects for graduate students. The results will be disseminated broadly through presentations at seminars, conferences and the internet, which may promote interdisciplinary research among scientists from diverse fields."
"1254840","CAREER:  New Directions in Spatial Statistics","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2013","07/01/2014","Debashis Mondal","IL","University of Chicago","Continuing Grant","Gabor Szekely","02/28/2015","$152,239.00","","mondal@wustl.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269, 8048","1045","$0.00","The  de Wijs process (also known as the  Gaussian free field in statistical physics) is a fundamental spatial process that arises as the scaling limit of lattice based Gaussian Markov random fields and generalizes Brownian motion in two-dimensions. However, at present,  there is a wide gap between the theory of Gaussian free field (including the subsequent theory of random fields) in statistical physics and modern probability, and the current practice of spatial statistics via lattice based Gaussian Markov random fields. Thus, there is great need to bridge this gap to develop a principled framework for statistics and inference of spatial models  and to pursue novel computations that make such inferences feasible.  This  project will consider formulating appropriate functionals of the de Wijs process to construct useful random fields and novel matrix-free computations via conjugate gradient and other methods, and will focus on developing new areas of scientific applications. The proposed research will also shed new light on and allow deeper understanding of theoretical and computational issues discussed by many researchers in spatial statistics in the past decades.  Novel matrix-free computations  will provide further impetus to study parametric bootstrap methods and multi-scale modeling, and to construct a new class of non-Gaussian random fields.  The project will contribute  to obtaining enhanced scientific understanding in studies of environmental bioassays, arsenic contamination of groundwater and distributions of galaxies. <br/><br/>Advances in the field of spatial statistics are important because new statistical methods can be applied to a wide range of scientific questions in fields such as astronomy, agriculture, biomedical imaging, computer vision, climate and environmental studies, epidemiology and geology. The de Wijs process is one fundamental spatial process that generalizes Brownian motion from time to space.  Using the de Wijs process as a fundamental building block, this project will develop novel mathematics and derive fast, efficient and large-scale statistical computations so that various scientific questions can be answered in a practical way. This will lead to new developments  for the analysis of continuum spatial data  and spatial point patterns, and will allow us to obtain enhanced scientific understanding in studies of environmental bioassays, arsenic contamination of groundwater and distributions of galaxies.  The statistics and the computations that will be developed in this project will also be particularly relevant for various research problems that arise in environmental or global change, and in health studies. Finally, this project will integrate research and educational activities through the development of new graduate and undergraduate courses and will also provide valuable training and learning opportunities for students at graduate and undergraduate levels."
"1309998","Spectral Methods for Contextualizing relational data","DMS","STATISTICS","08/15/2013","08/09/2015","Karl Rohe","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","07/31/2017","$120,000.00","","karlrohe@stat.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","Networks (or graphs) can represent the relationships in complex systems with myriad interacting elements.  Two primary examples are social networks that represents the set of friendships in a group of people and biological networks that represent the functional relationships between proteins in a living cell.  Many substantive questions can be phrased as questions of (a) the network structure and (b) supplementary measurements on the actors and their relationships.  This project will provide a statistical framework to simultaneously analyze relational (i.e. network) data and its contextualizing measurements.  The primary objective is to study the joint variability between the relational data and covariate measurements on the actors in the network.  A secondary objective is to begin studying the joint variability among a sample of networks on the same set of actors.  In both objectives, this project will (1) propose a general nonparametric model and a set of simple parametric models, (2) devise fast spectral estimators, and (3) provide estimation theory that examines the statistical performance of the spectral estimators under the nonparametric and parametric models. <br/><br/>In the age of big data, data sets are both larger and more complex, often coming from measurements on complex systems with myriad interacting elements;  social and biological networks can represent the relationships in complex systems and these substantive questions are, in essence, questions regarding networks.  The biological networks in the ENCODE research are an example.  Moreover, the relationships in complex systems are often measured with a rich set of supplemental information on the actors and their relationships.  This research program will provide a statistical framework, including models, algorithms, and theory, to study the supplemental information in tandem with the network, thereby contextualizing the network and the relationships."
"1309356","Estimation of Convex Objects","DMS","STATISTICS","07/01/2013","07/01/2015","Adityanand Guntuboyina","CA","University of California-Berkeley","Continuing Grant","Gabor Szekely","06/30/2017","$257,510.00","","aditya@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","","$0.00","In this proposal, an aspect of the interaction between convexity and statistics is addressed where nonparametric statistical estimation problems are studied in which convexity is present largely as a constraint controlling the unknown object of interest. Attention is focused on four prominent such problems including convex regression and log-concave density estimation. The importance of these problems is well recognized in the statistical community as well as various other disciplines and many papers have been written on them. However, many unsolved questions exist in the estimation theory and methodology for these problems especially in the multidimensional case. Indeed, the theory and methodology here is nowhere as sophisticated as that of certain other areas of nonparametric statistics such as classical function estimation under smoothness and sparsity constraints. The main goal of this proposal is to bridge this gap. The emphasis is on the following areas of research: (a) Studying the theoretical properties of the commonly used estimators such as MLE and least squares estimators, (b) Establishing a minimax theory (determination of the minimax rate of convergence, constructing of approximately minimax estimators etc), (c) Understanding adaptive estimation, (d) implementing practical algorithms for computing minimax and adaptive estimators, and (e) constructing alternative simpler estimators based on classical ideas from nonparametric function estimation such as smoothing and kernel based estimation. Our methods of analysis involve ideas from convex geometry, empirical processes, nonparametric statistics and information theory.  <br/><br/>In recent years, there has been a significant influx of ideas and methods into statistics from the fields of convex geometry and optimization. A main reason for this is the predominance of large datasets in contemporary applied statistics where efficient computation is a necessity and convex optimization techniques are tailor-made for such applications. This proposal aims to further our understanding of this deep connection between convexity and statistics by focusing on statistical problems where convexity is present as a constraint controlling the unknown objects of interest. The main goal of this research is to bring the theoretical and methodological developments in this important area of statistics to the same level of sophistication present in other well-studied related areas of statistics such as classical function estimation. The proposed research has applications in a diverse set of fields ranging from engineering to economics. Specifically, the problems studied here arise in areas such as computed tomography, target reconstruction from laser-radar measurements, robotic tactile sensing, image analysis, geometric tomography, estimation of production, utility and demand/supply functions in economics and operations research, decentralized detection etc. Results coming out of this research will also contribute to the mathematical fields of approximation theory, convex geometry and theoretical statistics."
"1265202","FRG: Collaborative Research: Statistical Modeling and Inference of Vast Matrices for Complex Problems","DMS","STATISTICS","07/15/2013","06/11/2015","Ming Yuan","WI","Morgridge Institute for Research, Inc.","Continuing Grant","Gabor Szekely","08/31/2017","$278,004.00","","ming.yuan@columbia.edu","330 N ORCHARD ST","MADISON","WI","537151119","6083164335","MPS","1269","1616","$0.00","Technological advances make it possible to collect and store large data with relatively low costs. As a result, scientific studies in a wide range of fields routinely generate a massive amount of data. Oftentimes, our ability to obtain measurements outpaces our ability to derive useful information from them. These pressing challenges serve as the ultimate motivation for the proposed research. In particular, this collaborative proposal presents novel research plans on the development of statistical theory and methodologies as well as computational techniques for a host of problems involving large matrices ranging from covariance matrices, volatility matrices, density matrices to relational matrices.<br/><br/>The past few years have witnessed an explosion of data as a result of scientific and technological advances. As data storage cost continues to fall, the focal point on these big data has been transitioning inevitably from data management towards deriving actionable insights from them. There is a pressing need to respond to these challenges and understand the profound impact of large data on scientific research and knowledge discovery. The proposed research project deals with emerging problems that arise naturally at the frontier of a multitude of scientific and technological fields such as systems biology, high-frequency finance, and quantum computing among others. As a consequence, the proposed research effort will not only push forward the state-of-the-art of statistical understanding of large matrices, but also facilitate the advancement of various scientific fields and their embracement of the digital revolution."
"1308227","Personalized  classification, moment selection, and time-varying networks for large-scale longitudinal data","DMS","ADVANCES IN BIO INFORMATICS, STATISTICS, Cross-BIO Activities, MSPA-INTERDISCIPLINARY","09/01/2013","08/29/2013","Annie Qu","IL","University of Illinois at Urbana-Champaign","Standard Grant","Gabor Szekely","08/31/2016","$210,000.00","","aqu2@uci.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1165, 1269, 7275, 7454","8007","$0.00","This research project aims to develop new statistical theory, methods and computing algorithms to solve real world problems where the data present unique features such as large volume, large variety  and large velocity of change. Traditional methods relying on parametric likelihood functions are no longer feasible for high-dimensional longitudinal data. The PI and her students intend to develop personalized classification strategies for subjects with high heterogeneity variation, and propose to identify subgroups from longitudinal observations through nonparametric random effects estimation.  The proposed research also intends to select and optimally combine high-dimensional moment conditions to reduce the dimensionality of large numbers of moment conditions, while retaining the important information from the data to achieve estimation efficiency.  In addition, a time-varying network model will be proposed to address dynamic changes of network structures using flexible nonparametric modeling. The proposal also seeks to develop highly efficient computational algorithms for solving optimization problems which involve high-dimensional parameter estimations and matrix operations. The proposed research project will help to tackle fundamental questions in statistical science and will stimulate interest from a large group of scientists in the fields of longitudinal/correlated data analysis, classification, random effects modeling, moment selection, low rank approximation, and time-varying networks for high-dimensional correlated data.<br/><br/>The proposed research topics have many important applications in the biomedical sciences, genomics, environmental sciences, and economics. For example, the personalized classification method is applicable for personalized medicine, where individuals with different biomarkers can receive different medical interventions to get more effective treatment. The time-varying network model is powerful for identifying time-evolving network associations for brain and biological functions, social interaction, and environmental influence over time. In addition, the moment selection method is applicable for panel data in econometrics applications. The PI will integrate the proposed research areas substantially into educational activities through development of new topic courses. The research will also significantly advance undergraduate and graduate students' learning and training."
"1321692","CAREER: Sparse Modeling and Estimation with High-dimensional Data","DMS","STATISTICS","01/01/2013","06/04/2014","Ming Yuan","WI","Morgridge Institute for Research, Inc.","Continuing Grant","Gabor Szekely","06/30/2015","$217,757.00","","ming.yuan@columbia.edu","330 N ORCHARD ST","MADISON","WI","537151119","6083164335","MPS","1269","0000, 1045, 1187, OTHR","$0.00","With the recent advances in science and technology, high dimensional data are becoming a commonplace in diverse fields. The goal of this proposed research is to develop methods and theory for several basic classes of statistical problems associated with this type of data. Among the central questions are the nature of sparsity in different contexts, and how it determines our ability or inability to deal with high dimensional data. The investigator studies a reproducing kernel Hilbert space based framework to exploit sparsity for general predictive problems. The framework underpins the connections among various popular methods that encourage sparsity, and provides an opportunity to study them in a unified fashion, which in turn will foster the development of improved methods and algorithms. The investigator will also consider the problem of covariance matrix estimation and selection. The research concentrates on understanding the nature of and connection among various notions of sparsity for large covariance matrix, and their relationship with Gaussian graphical models.<br/><br/>From the world's most powerful telescopes to the finest atomic force microscopes, from the flourishing financial market to the fast-growing World-Wide Web, high dimensional and massive data are being produced at an astonishing rate. Immediate access to copious amount of interesting and important information presents unprecedented opportunities, but also creates unique challenges, to mathematicians in general and statisticians in particular. Development of statistical theory to understand the nature of their fundamental characteristics, and methodology to address the associated issues, including those discussed in this proposal, will advance our intellectual exploration and knowledge, and undoubtedly benefit a multitude of scientific and technological fields -- genomics, medical imaging, communication networks, and finance are just a few well known examples.<br/>"
"1308424","Computer Experiments with Tuning or Calibration Parameters: Modeling, Estimation and Design","DMS","STATISTICS","07/01/2013","05/04/2015","C. F. Jeff Wu","GA","Georgia Tech Research Corporation","Continuing Grant","Gabor Szekely","06/30/2017","$170,000.00","","jeffwu@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","","$0.00","In the statistical approach to computer experiments, Gaussian process models are often employed to describe the relationship between the simulation output and the input variables. There are three types of input variables: control variables, tuning parameters and calibration parameters. The tuning parameter can be the mesh density in finite element analysis. Calibration parameters are also part of the computer code but not part of the physical experiment. The combined data from computer and physical experiments are used to calibrate the computer model. These two types have received much less attention in the literature. The main goal of this proposal is to study some issues in modeling, estimation and design for tuning and calibration parameters in computer experiments. A class of nonstationary Gaussian process models is proposed, which can be used to efficiently link data from simulations with different tuning parameter values. Issues on covariance modeling and comparisons of competing models are studied. For designing computer experiments, typical use of space-filling designs is replaced by non-uniform designs that can better reflect the nonstationary nature of information in the data. For calibration parameters, the standard estimation procedure is shown to be asymptotically inconsistent. A new theoretical framework is proposed for studying the estimation properties, including modification and new estimation procedures to achieve consistency and optimal convergence rates.<br/><br/>The last decade has seen rapid advances in realistic physical modeling and efficient numerical methods, which make it possible to use complex mathematical models to mimic physical realities. Computer simulations can be much faster or less costly than running physical experiments. Furthermore, physical experiments can be difficult or infeasible to conduct. Therefore computer simulations are now routinely used in lieu of physical experimentations. Computer modeling and experiments have become popular in scientific and engineering investigations. They have helped reap benefits ranging from reduced development cycle time, better product, to cost reduction. In view of the wide range of applications of complex system simulations, the proposed work should have broad-based impacts on a variety of problems in autos and aerospace, computational material design, geological and atmospheric studies, and green energy simulations. It will be incorporated into publicly released software like R, thus directly benefiting practitioners in industries and researchers in academe."
"1258701","Participant Support for the 8th Conference on Extreme Value Analysis","DMS","STATISTICS","06/15/2013","03/19/2013","Liang Peng","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","05/31/2014","$10,000.00","Richard Smith, Zhengjun Zhang","lpeng@gsu.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","7556","$0.00","Abstract:  This  proposal  provides partial support to the US participants of the 8th Conference on Extreme Value Analysis to be held on July 8--12, 2013 at the Fudan University, Shanghai, China, where PIs  are  members of Scientific committee and organizing committee. EVA conferences started in Gothenburg, Sweden in 1998, then they became regular meetings for researchers in extreme value analysis. The aim of these conferences is to bring together a diverse range of researchers, practitioners, and graduate students whose work is related to the analysis of extreme values in a broad sense. <br/><br/>In recent years, extreme events have been becoming more and more frequent and severe, and extreme events result in huge losses to our society. For example, the US now has been suffering from the biggest drought since 1956.  How to model and predict such extreme events is of great importance. The major reason for this proposal is to provide partial support, mostly to graduate students and junior faculty from the United States, to boost the participation of US researchers, to improve the visibility and to encourage further research on extreme value analysis in the United States."
"1310438","Nonparametric Bayesian Regression for Categorical Responses: Novel Methodology for Modeling, Inference and Applications","DMS","STATISTICS","07/01/2013","06/30/2015","Athanasios Kottas","CA","University of California-Santa Cruz","Continuing Grant","Gabor Szekely","09/30/2017","$200,000.00","","thanos@soe.ucsc.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1269","","$0.00","The investigator develops flexible Bayesian mixture models and corresponding methods for inference for a number of regression problems with ordinal categorical responses. More specifically, methodology is developed for: nonparametric mixture regression for binary responses; ordinal regression, including multivariate ordinal responses and mixed ordinal-continuous responses; dynamic modeling for ordinal regression relationships; and modeling and risk assessment for bioassay dose-response studies with an ordinal classification. All the methods build from nonparametric priors, including both general mixture prior models as well as more structured forms as motivated by the application area and inferential objectives of the model. The key research activity involves flexible regression modeling based on either dependent nonparametric priors for the process of response distributions indexed by covariate values or nonparametric mixture models for the joint distribution of the response(s) and covariates. These classes of models enable rich inference for both the regression relationship and for the conditional response distribution. Hence, they improve model fit and predictive performance compared to standard parametric models, but also relative to existing Bayesian semiparametric work. For all the modeling approaches under development, the investigator studies relevant theoretical properties, model specification, prior elicitation, Markov chain MonteCarlo posterior simulation techniques, and model checking and comparison.<br/><br/>Regression problems with ordinal categorical responses -- involving data on response variables recorded on an ordinal scale and on associated explanatory variables -- are of key importance in various fields of the biomedical, environmental and social sciences. As researchers from these fields collect more and more data involving ordinal responses, especially over time or time and space, the need for analyses that enhance theirunderstanding of underlying processes grows. This inspires the need for sufficiently rich statistical models that can accommodate general ordinal regression relationships. The primary motivation for this research is to expand the catalog of ordinal regression modeling tools available to such scientists, in the process expanding the methodology in the field of Bayesian nonparametrics, a burgeoning area of Bayesian statistics. Due to their generality, the statistical methods developed under this research project have the potential for substantive applications in several scientific fields. A particularly promising area of application involves evolutionary biology problems on estimation of the form of natural selection as it relates phenotypic traits to ordinal fitness measures, such as survival, maturity  or reproductive success. For such settings, improved estimation of the fitness surface as well as understanding of its temporal and/or spatial evolution can have an impact on effective decision making for the population under study."
"1412343","Collaborative Research: Axially symmetric processes and intrinsic random functions on the sphere","DMS","STATISTICS, EPSCoR Co-Funding","07/31/2013","12/02/2013","Haimeng Zhang","NC","University of North Carolina Greensboro","Standard Grant","Gabor Szekely","08/31/2016","$47,468.00","","h_zhang5@uncg.edu","1000 SPRING GARDEN STREET","GREENSBORO","NC","274125068","3363345878","MPS","1269, 9150","9150","$0.00","In spatial statistics, a wide variety of methods and models have been developed in Euclidean spaces. Additional theory and methods are needed for analyzing processes and phenomena on the sphere, many of which are of utmost importance in the geophysical sciences. Data from global networks of in situ and satellite sensors, for instance, are used to monitor a wide array of important climatological variables such as temperature and precipitation. The goal of this project is to study random processes on the sphere beyond the usual homogeneity assumption using two approaches. The first approach is to consider axially symmetry on the sphere, where the random process exhibits longitudinal symmetry rather than being rotation invariant on the entire sphere.  The second approach of this research extends the intrinsic random functions and generalized covariance functions to processes on the sphere. The notion of intrinsic random functions has been developed in order to handle a process with an unknown and possibly non-constant mean function while preserving a linkage to stationarity. The investigators plan to achieve a fundamental understanding of the covariance structures of the non-homogenous processes on the sphere and to develop associated estimation procedures. In addition, these approaches are extended to more sophisticated situations including multivariate random processes and spatio-temporal processes on the sphere. All of these methods and models are applied to global-scale temperature data from surface and satellite sensors.<br/><br/>The models and methods developed here provide new tools for improving our understanding of global-scale phenomena in general and multi- decadal temperature variations in particular.  The motivation of this project comes from a desire to understand the statistical characteristics of global-scale temperature variations during the instrumental (1880s onward) and satellite (1979 onward) periods. As a result, two important and widely used data sets are analyzed here:(1) tropospheric temperature data from National Oceanic and Atmospheric Administration satellite-based Microwave Sounding Unit and (2) surface-based instrumental data from the Hadley Centre in the United Kingdom and Climatic Research Unit at the University of East Anglia. The geostatistical analysis of these data sets on the sphere provides profound information on the state of our changing planetary environment. The project also establishes and encourages research and education collaborations between Indiana University and Mississippi State University."
"1308877","Distance and Dissimilarity Information in Statistical Model Building","DMS","STATISTICS","08/01/2013","05/23/2016","Grace Wahba","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","07/31/2018","$400,002.00","","wahba@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","The objective of this research is to greatly expand the collection of statistical tools that exploit pairwise distance and dissimilarity information in statistical model building for regression, classification, and variable/pattern selection at different scales. This includes use of information that involves non-metric pairwise dissimilarity information. In this work dissimilarity information may be subjective, noisy, incomplete, confined within a nonlinear manifold, may come from multiple sources and may be inconsistent. Previous results have shown how this information may be embedded into a Euclidean space, so that methods that operate in a Euclidean space can be used. Two recent novel and very powerful tools, distance correlation and distance components, have provided for principled testing of correlations between arbitrary groups of variables and testing of equality of distributions, based only on pairwise Euclidean distances, and requiring essentially no distributional assumptions. Thus, combining methods that embed non-metric information into a Euclidean space followed by use of distance correlation and distance components that operate on Euclidean data provide an important new approach to using ""messy"" pairwise data. Furthermore distance correlation and distance components are being extended to certain regression, classification and variable/pattern selection problems via parametrization, tuning and testing techniques, preceded, when appropriate by embedding techniques. A series of tasks to implement aspects of this program provides advances in the major statistical tasks of regression, classification and variable/pattern selection for non-traditional information in a principled way.<br/><br/><br/>This work provides a vast extension of the set of practical tools available to the statistician/data analyst and to modelers in a wide variety of scientific fields to extract information to predict, classify, and select important variables/patterns from data sets from small to large, that include distance or dissimilarity information from a variety of structures that are becoming increasingly available and important in practice. The proposed work provides a new set of important and useful tools for improved statistical data analysis that will be widely disseminated, and impact society to the extent that they provide aid to researchers in the extraction of information in biological, medical, environmental and other data sets that contain information of public interest. The project includes high level training of a Ph.D. student in an important STEM area."
"1303942","Symposium on ""Advances in Statistical Methods for the Analysis of Observational and Experimental Data""","DMS","STATISTICS","07/01/2013","03/27/2013","Dennis Boos","NC","North Carolina State University","Standard Grant","Gabor Szekely","06/30/2014","$10,000.00","Heejung Bang","boos@stat.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","7556","$0.00","This award supports travel for participants in the symposium Advances in Statistical Methods for the Analysis of Observational and Experimental Data, held July 13, 2013 on the North Carolina State University campus in Raleigh, North Carolina. The past several decades have seen numerous significant advances in statistical theory and methodology for the analysis of data from experimental and observational studies.  These developments have had a profound impact on statistical practice.  The conference features presentations by leaders in the development of statistical theory and methods in areas such as censored time-to-event data, missing data, longitudinal data, and causal inference. <br/><br/>The conference will serve as a forum for discussion of developments in statistical methodology and theory that have propelled advances in research in the health, biological, epidemiological, social, and economic sciences.  The conference offers a perfect opportunity for junior researchers to gain a valuable perspective on emerging trends and the opportunity to interact with senior and mid-career level leaders responsible for past and present advances.<br/><br/>Conference web site:  http://www.stat.ncsu.edu/events/2013_tsiatis_symposium/"
"1305725","Regression trees for some problems with multi-dimensional data","DMS","STATISTICS","08/15/2013","08/23/2013","Wei-Yin Loh","WI","University of Wisconsin-Madison","Standard Grant","Gabor Szekely","07/31/2017","$130,000.00","","loh@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","The investigator develops regression tree solutions for some important problems with complex and high-dimensional data. Complexity includes missingness, censoring, mixed variable types, and correlated measurements taken at random time points. One specific problem is identification of subgroups for differential treatment effects in comparative trials involving time-varying covariates. A second problem is importance scoring and thresholding of variables and a third is detection of differential test item functioning in testing and evaluation. The main approach relies on adapting and extending the GUIDE decision tree algorithm to these problems. Expected difficulties and challenges include minimizing error rates and computational cost as well as ensuring unbiased selection of the variables used to split the nodes of the trees.<br/><br/>The ability to collect and generate greater amounts of data at faster speeds creates new difficulties to data analysis and interpretation. For example, the health industry is looking into using genetic information and repeated observations over time to find personalized treatments for diseases. The proposed research will extend a statistical approach based on decision trees to solve problems such as: (i) identifying subpopulations of patients who benefit more from a one treatment over another, based on repeated observations on health and other outcomes over time, (ii) identifying and ranking genetic and other variables with respect to their importance in prediction of illness and their interactions with treatments, and (iii) identifying test items in testing and evaluation that discriminates against people due to their gender, race, or socio-economic and cultural background.  A decision tree model has the unique advantage that it is easy to apply and intuitive to interpret. The latter property is crucially important to understanding and advancing the science."
"1308872","Structural-Information Enhanced Inference for Large-Scale and High-Dimensional Data","DMS","STATISTICS","08/01/2013","07/11/2013","Chunming Zhang","WI","University of Wisconsin-Madison","Standard Grant","Gabor Szekely","07/31/2016","$130,001.00","","cmzhang@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","A fundamental research issue for large-scale and high-dimensional inference procedures is how can we incorporate the structural information from the data to enhance large-scale computing, machine learning and statistical inference. Learning and exploiting such structure is crucial towards better analysis of complex datasets. This proposal aims to incorporate important structures of the data and association among feature variables and the response variable into devising efficient experimental approaches and algorithms for large scale problems and understanding theoretical properties of the procedures. Project 1 develops a feature screening approach in the high dimension, low sample size paradigm which takes into account the correlation structure among the features. The PI proposes a framework of inference for selecting feature variables relevant to the response variable. In the context of large-scale simultaneous inference, the hypotheses are often accompanied with certain structural prior information. Project 2 proposes a new multiple testing procedure, which maintains control of the false discovery rate while incorporating the prior information. Large-scale multiple testing tasks often exhibit dependence, and leveraging the dependence among individual tests is an important but challenging problem in statistics. Project 3 proposes a multiple testing procedure which allows general dependence structures and heterogeneous dependence parameters.<br/><br/>This proposal aims to tackle some challenging research problems, arising from frontiers of biological, medical and scientific research, with a common theme of exploiting structural information in high-dimensional data. New tools for stochastic modeling, computational algorithms, parameter learning, and statistical inference applied to large-scale and high-dimensional data, for example, brain fMRI imaging data and datasets from genome-wide association studies on breast cancer, will be developed. Dissemination of these developments will enhance new knowledge discoveries, and strengthen interdisciplinary collaborations. The research will also be integrated with educational practice through multi-disciplinary courses on the contemporary state-of-the-art data mining and machine learning, and benefit the training and learning of undergraduate, graduate students and underrepresented minorities."
"1310391","Learning Compositional Sparse Coding Models for Natural Images","DMS","STATISTICS","08/15/2013","06/29/2015","Yingnian Wu","CA","University of California-Los Angeles","Continuing Grant","Gabor Szekely","07/31/2016","$150,000.00","","ywu@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","","$0.00","The general goal of this project is to develop generative models for images of natural scenes, as well as associated algorithms for unsupervised learning of such models from natural images. The learned models can then be used for image representation and pattern recognition. A particular class of models investigated in this project are the compositional sparse coding models, where the images are represented by automatically learned dictionaries of templates, and each template is a compositional pattern of wavelets that provide sparse coding of the images. The PI and collaborators also investigate related models where the templates are inhomogeneous Markov random fields whose energy functions are defined by the sparse coding wavelets. <br/> <br/>Image understanding is at the hearts of many modern technologies. It is also a major function of human brains. The key to image understanding is automatic discovery of patterns in the images. The goal of this project is to develop methods for learning dictionaries of patterns or ""visual words"" for representing images of our environments. The learned dictionaries can be very useful for image understanding and object recognition."
"1310119","Estimation of High Dimensional Matrices of Low Effective Rank with Applications to Structural Copula Models","DMS","STATISTICS","07/01/2013","05/18/2015","Marten Wegkamp","NY","Cornell University","Continuing Grant","Gabor Szekely","06/30/2016","$200,000.00","Florentina Bunea","marten.wegkamp@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","","$0.00","The central goals of this  proposal are:<br/>(a)  To provide sharp finite sample  bounds, in various matrix norms, on the accuracy of the sample covariance estimator of high dimensional covariance matrices of  reduced effective rank;<br/>(b)  To extend these results to functional data and characterize classes of covariance operators of reduced effective rank. To use these results  to develop fully data driven methods, with strong theoretical justification,  for eigenvalue and eigenvector selection,  in finite samples. To apply these results to modeling vehicle emissions exhaust;   <br/>(c) To  study factor models  of  high dimensional  correlation matrices of elliptical copulas. To obtain minimax  estimators of these matrices and to use these results in classification problems in breast cancer data. <br/>There are interesting connections between our proposed research and existing results on estimation of  covariance or correlation matrices under sparsity constrains. However, estimation  under the existing sparsity types (entry-wise, row-wise, off-diagonal decay) cannot be used for modeling general types of dependency. The proposed work bridges this gap,  and  poses  different mathematical and computational challenges. <br/><br/><br/>Modeling high dimensional data  and evaluating their variability  presents  increasing challenges  in many scientific disciplines.  For instance,  such challenges occur in modeling network data  in genetics and molecular biology; high dimensional portfolios in economics; and samples of curves in psychology, public health, transportation and urban planning. Substantially better solutions   can be provided  whenever  the data is  generated by a model with low dimensional structure. In the statistical problem of high dimensional covariance and correlation matrix estimation, this proposal will formulate the relevant  notion of  low dimensional structure (for instance, low effective rank or approximate low dimensional factor models). The need for  a systematic investigation of  various  classes of covariance matrices in high dimensional  models, especially in functional data settings, only begun to be recognized in recent years. This proposal is therefore a timely addition to the currently limited battery of methods and theoretical results in this important area.  The usefulness of these techniques will be demonstrated by applications to data from genomics, proteomics  and environmental engineering. Free software that implements the developed  methodology will be made available on the web in a readily implementable form."
"1308458","Scalable Spectral Methods for Statistical Analysis","DMS","STATISTICS","08/15/2013","07/27/2014","Tao Shi","OH","Ohio State University","Continuing Grant","Gabor Szekely","05/31/2014","$1,390.00","","taoshi@stat.osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","MPS","1269","","$0.00","Recent rapid developments in information technology, fast computation, data collection methods and storage capacity have lead us to an uncharted territory of data analysis.The scientific research community encounters not only massive volume of data, but also datesets with complicated and nontraditional structures. For example, NASA's satellites generate a massive amount of high-dimensional datasets for climate studies and it is critical to pull out important climate features that may help explain climate changes and predict the future. Meanwhile, the advance in communication technology leads to recent surges on the usage and studies of massive networks such as computer networks and social networks. The emergence of these massive data with nontraditional structures presents new challenges in statistical methodology developments, and even more importantly, algorithm innovations. To address these challenges in this proposal the PI intends to develop scalable spectral methods for statistical inference on Euclidian data, non-Euclidian data, and relationship data. These newly emerging nontraditional type of data and problems provide us with great opportunities for exciting scientific discovery. Spectral methods are natural tools for extracting information and knowledge from these nontraditional data. Along with theoretical analysis of the properties and accuracies of these spectral methods, the PI will develop scalable algorithms and test them in real world applications like climate study and network analysis.<br/><br/>The project is motivated by real scientific problems and real world applications related with massive datasets with nontraditional data structures. Besides theoretical development in statistical methodology,the PI will work with collaborators in the fields of Atmospheric Science, Computer Science, Meteorology, and Statistics to apply the proposed inference tools to climate change study. This new set of methods and related softwares for inference on massive datasets will greatly help geoscientists and climate modeler in analyzing climate records and calibrating climate models. Collaborating with Computer Scientists and Statisticians, the PI will also develop scalable spectral algorithms for network analysis, with applications to biological networks and social networks. These tools will assist a broad range of scientists, engineers, and business analysts in scientific exploration, technology innovation, and service improvement."
"1305474","Analysis of Longitudinal or Multivariate Data with Nonignorable Missing Values","DMS","STATISTICS","09/15/2013","09/05/2013","Jun Shao","WI","University of Wisconsin-Madison","Standard Grant","Gabor Szekely","08/31/2017","$180,001.00","","shao@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","In many statistical applications, some data from sampled units are missing because of various reasons. In most survey problems, some sampled units cooperate in the survey but fail to provide answers to some or all survey items. In medical or health studies, data are often longitudinal and many patients drop out before the end of the study. The rates of missing data in surveys or medical studies are often appreciable, especially when data are longitudinal and/or multivariate (e.g., many questions in a survey). When data missing depends on observed data only, the missingness mechanism or propensity is called ignorable. Otherwise, missing data are nonignorable. There is a rich literature on methodology for handling ignorable missing data. Nonignorable missing data are much more difficult to handle compared with ignorable missing data, since missingness propensity depends on unobserved values and, thus, model fitting is very challenging.  For example, assumptions have to be imposed to ensure the identifiability and estimability of unknown quantities and these assumptions cannot be checked using data because of the presence of missing values. The proposed research focuses on estimation and inference based on longitudinal or multivariate data with nonignorable missing values. The investigator studies three general topics. (1) When missing data are nonignorable, applying existing methods developed for the case of ignorable missing data leads to biased estimators. Research is needed to derive approximately unbiased and consistent estimators for parameters of interest. Under some assumptions on the missingness propensity and/or the data distribution for the case of no nonresponse, the investigator studies several approaches for constructing asymptotically valid estimators. These approaches are all semiparametric and make use of a covariate that helps to identify parameters under nonignorable missingness (and is therefore named as a nonresponse instrument). Adopted estimation methods include pseudo likelihood, estimating equations, generalized method of moments, data transformation, approximate conditional likelihood, imputation, and some techniques of handling measurement errors. For survey data, the model-assisted approach is adopted. (2)  In addition to the bias and consistency, the investigator studies the asymptotic efficiency of estimators. For longitudinal or multivariate data with nonignorable missing values, it is difficult to make use of observed data from units having incomplete data. Efforts will be made to use more or all observed data. (3) Most surveys require a variance estimator for each survey estimator for the purpose of error assessment. Statistical inference such as setting confidence sets also requires variance estimators. A basic requirement for variance estimators is their approximate unbiasedness and consistency. In each proposed research topic, the investigator studies variance estimation after a valid estimator is derived, using methods such as linearization, substitution, or resampling.<br/><br/>Since the proposed research topics are motivated by problems in survey agencies such as the Census Bureau, the Bureau of Labor Statistics, and Statistics Canada, or by data sets in medical and health studies, results obtained from this proposed research will have significant impact on the methodology of handling missing data and variability estimation. Since research on nonignorable missing data, especially for multivariate or longitudinal data, is far from complete, results from this proposal will shed light on further scientific research in this area."
"1310294","Complex Experiments and High-Input Simulators:  Challenges in Design, Prediction and Sensitivity","DMS","STATISTICS","07/01/2013","07/14/2013","Thomas Santner","OH","Ohio State University","Standard Grant","Gabor Szekely","06/30/2017","$100,000.00","Angela Dean, Christopher Hans","santner.1@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","MPS","1269","","$0.00","The investigators study five problem areas.  (1) A primary objective is to provide designs that have high predictive efficiency over a wide class of objective functions for simulator-only and simulator+physical experiments. Experimental designs will be constructed based on a new Bayesian prediction criterion. (2) The researchers will formulate regularization priors for use with simulators that require large numbers of inputs and are sampled sparsely.  In particular, regularization priors will be developed for regression coefficients in the mean structure for non-stationary GP models as well as new priors to regularize the parameters controlling the correlation function. (3) Batch sequential design and analysis methodology will be developed to determine settings of control variables for minimizing the mean and variability of the response in the presence of non-controllable environmental variables.  (4) The investigators will develop methods for determining the sensitivity of simulator outputs to inputs in mixed quantitative-qualitative variable settings, including non-rectangular input applications.  (5) Methodology will be devised for constructing space-filling designs for computer codes with high-dimensional, non-rectangular regions whose boundaries can only be determined numerically.  For example, computational models of long-scale temporal phenomena such as climate and galaxy formation models, have inputs regions are only partially known a priori.<br/><br/>The complex physical and biological processes required to provide advances in many engineering and scientific fields are increasingly<br/>studied using deterministic computer simulators coded from physics- or biology-based mathematical models.  In many such applications, the data from related physical experiments are also available.  The goals in such studies range from optimizing system performance in a given environment to designing systems that perform well in a wide variety of environments.  The investigators are conducting research on efficient methods for determining input combinations at which to run complex simulator codes and associated physical experiments. They are also developing methods to extract the maximum information about the relevant engineering and scientific questions from such combined experiments.  Techniques and results developed by the investigators are used in the specific collaborative projects in which the investigators participate.  These include the following applications: (a)  Tissue engineering where stress-strain simulators form part of the process of designing specialty tissues such as meniscal substitutes which are meant to perform well in a patient population; (b) Design of coating systems to extend the life of machine tools in tribological and wear applications; (c) Decreasing the shrinkage in injection molding applications; (d) Prediction of climate and weather based on detailed circulation computer models of<br/>the atmosphere."
"1329240","Long range dependence and resampling methodology for spatial data","DMS","STATISTICS","01/01/2013","02/14/2013","Soumendra Lahiri","NC","North Carolina State University","Continuing Grant","Gabor Szekely","04/30/2014","$133,033.00","","s.lahiri@wustl.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","","$0.00","This project concentrates on (i) developing limit theory for a class of long range dependent spa­tial processes under various spatial sampling designs, including the case of irreguraly spaced data-sites, which is encountered frequently in spatial applications; (ii) developing new resampling methodology for spatial data under both short-and long-range dependence that are immune to the e.ects of the curse of dimensionality, (iii) developing Edgeworth expansion theory for spatial data for both regularly and irregularly spaced cases, and (iv) investigating higher order properties of resampling methods for spatial data and study their higher order properties. <br/><br/><br/>The proposed project aims to make important theoretical and methodological contributions to several critical areas of spatial statistics that have a wide of potential applications but the state of the current literature on these areas is very sparse. In addition to advancing the state of statistical methodology for spatially referenced data, the proposed research would also bene.t many other areas of sciences, such as Astronomy, Hydrology, Geology, Economics, Atmospheric Sciences, etc. where spatial data exhibiting di.erent forms of dependence are known to occur naturally, and model-free statistical methods such as those proposed in the project play an important role in their analysis. Further, the project would lead to the development of human resources through advising of Ph.D. students and mentoring of junior researchers."
"1301377","Some problems in nonparametric statistics","DMS","STATISTICS","07/01/2013","01/27/2016","Peter Hall","CA","University of California-Davis","Continuing Grant","Gabor Szekely","06/30/2017","$240,005.00","","pghall@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","","$0.00","The research program will draw together a wide range of new ideas for developing solutions to diverse practical problems, involving both very high-dimensional settings, where the number of components is much larger than sample size, and also contexts where dimension is much smaller than sample size.  Direct benefits will accrue from addressing these cases together, not least through the development of new statistical theory.  That aspect, among others, will give the program the authority it needs to carry individual research projects beyond the confines of their particular, immediate applications. <br/><br/>Specifically, the program will introduce new ways of fitting diverse and virtually assumption-free models in the presence of measurement error.  It will also introduce highly accurate ways of clustering data in the form of random functions; it will propose new, practicable ways of constructing confidence intervals; it will develop new techniques for modelling complex relationships in vector-valued data; it will introduce new resampling methods for very high-dimensional data; and it will show that models based on relatively strict assumptions can be used to motivate methodology that is largely assumption-free."
"1308376","Monte Carlo methods for complex multimodal distributions with applications in Bayesian inference","DMS","STATISTICS","08/15/2013","07/30/2013","Qing Zhou","CA","University of California-Los Angeles","Standard Grant","Nandini Kannan","09/30/2016","$120,000.00","","zhou@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","","$0.00","When a posterior distribution has multiple modes, unconditional expectations, such as the posterior mean, may not offer informative summaries of the distribution. Motivated by this problem, the investigator proposes to develop Markov chain Monte Carlo (MCMC) methods that may generate sufficient samples from the domain of attraction of every major mode and therefore construct estimates for the probability mass of and conditional expectations given a domain. Computational methods will be developed to build the landscape of a distribution based on an MCMC sample. This project will contribute novel methodologies on MCMC and Bayesian inference with multimodal posterior distributions, and generalize theory on adaptive Markov chains. A new algorithm, based on the framework of the multi-domain sampler, will be developed to group dynamically domains separated by low barriers and to construct the tree of sublevel sets for a distribution. The tree includes local modes as terminal nodes and barriers as internal nodes. This project also develops Bayesian inference methods via domain-based estimation and algorithms to quantify the stability of a posterior mode and its domain of attraction, with applications in Bayesian missing data problems and structure estimation. Convergence and ergodicity of the multi-domain sampler with global moves will be studied under the framework of doubly adaptive MCMC. A theoretical model, based on the tree of sublevel sets, will be developed to facilitate convergence and efficiency analysis of MCMC algorithms. <br/> <br/>Scientific problems in many disciplines may be solved by sampling from a given probability distribution. Monte Carlo methods, Markov chain Monte Carlo in particular, are a class of stochastic simulation algorithms that may draw samples from almost any distribution. However, these algorithms suffer from low efficiency when the distribution has multiple local modes. Therefore, the first significance of the proposed project comes from its applicability to many problems in various scientific fields, including statistical physics, chemical physics, and computational biology. On the other hand, there are almost no existing methods that can extract useful information about a multimodal distribution from Monte Carlo samples. The proposed project includes a systematic development of computational methods for constructing novel and comprehensive summaries about a multimodal distribution via a unified graphical representation for the landscape of a distribution. This can greatly enhance the current understanding of many problems in statistics and machine learning by, for example, quantifying the difficulty of a problem and providing visualization of a high-dimensional objective function."
"1310127","Adaptive designs: Sequential tests of multiple hypotheses","DMS","STATISTICS","07/01/2013","06/20/2013","Jay Bartroff","CA","University of Southern California","Standard Grant","Gabor Szekely","06/30/2017","$150,000.00","","bartroff@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","1269","","$0.00","The investigator will develop adaptive designs for the analysis of multiple sequential data streams, about which it is desired to test a set of statistical hypotheses. The designs will require minimal knowledge (none in some cases) of the dependence between these data streams -- since such dependence is often strong but difficult or impossible to model in practice -- and will provide sequential multiple hypothesis testing procedures which reach conclusions about each individual stream and hypothesis individually, as well as the set of hypotheses as a whole by controlling overall error rates, while being adaptively efficient by ""dropping"" a data stream once no additional information is needed to reach a confident conclusion. This will be done for the two most widely-used multiple testing error metrics, false-discovery rate and familywise error rate. For these two metrics, the investigator will: (1) develop sequential multiple testing procedures and a unified theory for existing procedures; (2) assess their performance and optimality through analysis and extensive numerical simulations; (3) find expressions or approximations for their operating characteristics including achieved error rates and expected total and maximum sample size; (4) apply the procedures to real data in genetics, genomics, and multi-arm biomedical trials and modify the procedures to aid practical implementation in these areas; and (5) develop free software packages to facilitate applications.<br/><br/>The project will provide new statistical methods to handle the analysis of multiple data streams arriving sequentially over time in a coherent fashion.  The need to handle this type of data arises in a plethora of real-world situations including terrorist threat detection, quality control, finance, biomedical clinical trials for new treatments, disease monitoring in human and animal populations, genetics, and genomics.  Despite this, there are few existing statistical procedures that one can use except in some special cases. The procedures developed in this project will therefore be widely applicable by scientists and workers in these areas. Methodologically, the work will be a breakthrough in a fundamental yet largely unexplored area of statistics, with many benefits and broader impacts to society."
"1309273","Collaborative Research: New Directions for Research on Some Large-Scale Multiple Testing Problems","DMS","STATISTICS","07/15/2013","06/11/2015","Sanat Sarkar","PA","Temple University","Continuing Grant","Gabor Szekely","06/30/2017","$126,575.00","","sanat@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","MPS","1269","","$0.00","The paradigm shift of hypothesis testing from single to multiple hypotheses, often large number of them, in statistical applications to modern scientific investigations, such as brain imaging, microarray analysis, astronomy, atmospheric science, drug discovery and many others, has generated tremendous upsurge of research in the field of large-scale multiple testing in the last one and half decades. Nevertheless, some fundamentally important theoretical as well as methodological issues arising in many of these investigations still remain to be fully addressed before developing the necessary statistical tools. For instance, in clinical pharmacogenomics involving multiple testing, methods controlling false discoveries are yet to be developed in non-asymptotic setting when these hypotheses are tested group sequentially which is often required in order to meet economical and ethical concerns, or when these hypotheses belong to tree-structured hierarchical families which often happens due to importance based ordering of different sets of hypotheses. Also, in many practical applications of multiple testing where the order in which the tests are to be performed is pre-specified or can be assessed based on available data, but the potential improvements of the existing FDR methodologies exploiting this pre-ordering are yet to be explored. The project seeks to develop new and innovative multiple testing methods tackling these outstanding and related issues by focusing on the following three broad areas of research: (i) group sequential multiple testing, (ii) fixed sequence multiple testing, and (iii) testing multiple families of hypotheses. The project covers a wide spectrum of important multiple testing problems statisticians face in many practical settings. These problems are new and pose several technical challenges, as the existing theory and methodologies on multiple testing controlling false discoveries need to be extended from the framework of single stage or single family to that of multiple stages or multiple families. The proposed research has the potential to open up the door for research on multiple testing in newer directions. It not only aims at advancing the theory of multiple testing but also pays special attention to applications of the developed theories. <br/><br/>This project is expected to have a broad impact on the theory and practice of statistics. It aims at modernizing the field of statistics by advancing research in areas of importance in modern scientific experiments, and thus can benefit the society. For instance, the project can potentially pave the way for novel techniques to address statistical issues faced in modern drug discoveries and biomedical experiments. It would also benefit education through training of graduate students and incorporation of the developed methodologies in statistics courses. The results will be disseminated through presentations and discussions at national and international conferences, and visits to other institutions. The software to be developed under this project will be made available, free of charge, to the scientific community."
"1343889","Analysis of Survival Data using Copula Models","DMS","STATISTICS","07/01/2013","02/04/2014","Antai Wang","NJ","New Jersey Institute of Technology","Standard Grant","Gabor Szekely","06/30/2015","$59,179.00","","aw224@njit.edu","University Heights","Newark","NJ","071021982","9735965275","MPS","1269","","$0.00","The main objective of this research is to use Archimedean copula models to analyze survival data. The proposed strategies are useful in medical research and financial data analysis. The investigator will address three different problems. The first problem is to solve the identifiability problem for Archimedean copula models for dependent censored data. The second problem concerns left truncated bivariate data. The investigator will explore ways to best estimate the unknown parameters in Archimedean copula models using truncated bivariate data. A model selection procedure for Archimedean copula models will also be investigated. The last problem is to establish guidelines for selecting models belonging to Archimedean copula family based on survival data. Research will be conducted to demonstrate the performance of the proposed strategies.<br/><br/>The proposed methods and strategies are motivated by clinical trials involving dependent censoring problem and the study of the correlated bivariate survival data in AIDS research. The results of this project will be helpful for determining the underlying relationship between random variables when they are subject to different censoring patterns. The theoretic results will contribute to the advancement of the statistical theory on correlation studies and deepen the understanding of the dependence structure of Archimedean copula models."
"1310319","Tensor Regressions and Applications in Neuroimaging Data Analysis","DMS","STATISTICS","07/01/2013","04/30/2015","Hua Zhou","NC","North Carolina State University","Continuing Grant","Gabor Szekely","07/31/2016","$120,000.00","Lexin Li","huazhou@ucla.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","","$0.00","Rapidly advancing medical imaging technologies are producing massive amounts of complex imaging data, and are imposing unprecedented demands for new statistical methodology. The investigators aim to integrate advanced statistical modeling with modern computational techniques to address some most challenging questions arising from medical imaging analysis. The investigators propose a novel statistical framework and develop accompanying theory and algorithms for tensor regression, i.e., regression with image covariates that are in the form of multidimensional arrays / tensors. They study a variety of regularization schemes in the context of tensor regression to stabilize estimation, improve risk property, and reconstruct sparse signals. They also develop methodology within the tensor regression framework for scientific applications including brain region and connectivity pattern identification, imaging based disease diagnosis, and multiple imaging modalities analysis. The project offers a systematic solution to a family of imaging data problems, and also provides a new class of statistical regression methods.<br/><br/>One of the most intriguing questions in modern science is to understand human brains, both those of general population and those with neuropsychiatric and neurodegenerative disorders. Advanced medical imaging technologies provide powerful tools to help address the question, producing imaging data of unprecedented size and complexity. The investigators aim to develop a host of novel statistical methods, theories, and highly scalable algorithms for the analysis of massive medical imaging data. The proposed research is expected to make significant contributions on two fronts: timely response to the growing needs and challenges of neuroimaging data analysis, and development of an utterly new and broad statistical framework and the associated methodology that contributes to the advance of the statistical discipline."
"1305948","Summer Research Conference in Statistics and Biostatistics, Summer 2013","DMS","STATISTICS","05/15/2013","05/02/2013","Donald Edwards","SC","University of South Carolina at Columbia","Standard Grant","Gabor Szekely","04/30/2014","$10,000.00","","edwards@stat.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","MPS","1269","7556, 9150","$0.00","The investigator and his colleagues will organize and conduct a Summer Research Conference (SRC) in statistics and biostatistics at Montgomery Bell State Park, Tennessee June 2-5, 2013.  The SRC is an annual conference sponsored by the Southern Regional Council on Statistics (SRCOS).  Its purpose is to encourage the interchange and mutual understanding of current research ideas in statistics and biostatistics, and to give motivation and direction to further research progress.  The project will focus on young researchers, placing them in close proximity to leaders in the field for person-to-person interactions in a manner not possible at most other meetings.  Speakers will present formal research talks with adequate time allowed for clarification, amplification, and further informal discussions in small groups.  Under the travel support provided by this award students will attend and present their research in posters to be reviewed by more experienced researchers.<br/><br/>The Southern Regional Council on Statistics is a consortium of statistics and biostatistics programs in the South, stretching as far west as Texas and as far north as Maryland.  It currently has member programs at 39 universities in 16 states in the region.  This project will fund student participation in the 2013 Summer Research Conference (SRC) sponsored by SRCOS.  The meeting is particularly valuable for students and faculty from smaller regional schools at drivable distances, affording them the opportunity to participate and interact closely with internationally-known leaders in the field without the cost of travel to distant national or international venues.  It will strengthen the research of the statistics and biostatistics community as a whole, and particularly in the relatively underdeveloped southern region.  For all details, go to the conference link at  http://louisville.edu/sphis/bb/srcos-2013."
"1308400","A Comprehensive Framework for Fully Efficient Robust Estimation and Variable Selection, with Application to High-Dimensional and Complex Data","DMS","STATISTICS","08/01/2013","08/28/2015","Howard Bondell","NC","North Carolina State University","Continuing Grant","Gabor Szekely","07/31/2018","$149,997.00","","bondell@stat.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","","$0.00","This research addresses robustness in both estimation and variable selection within the context of today's complex data structures. The overarching theme of the research is that carefully specified moment restrictions combined with appropriate weighting of the data will lead to the ideal goals of full efficiency in estimation and variable selection which remains stable in the presence of atypical observations. The methodology is developed via generalized empirical likelihood, which yields estimated weights for each observation. In the process, this automatically downweights observations that may deviate from the model, thus reducing their influence. Meanwhile, the estimators have no loss of efficiency compared with the fully efficient model-based estimator if the model were correctly specified, even in finite samples. Taking this point of view allows a unified framework to the construction of robust and efficient procedures that can be developed for a variety of models. The foundation of efficiency and robustness allows variable selection to be built into the methods to handle, not only the moderate, but also the high-dimensional setting. Due to the performance of the baseline approach, the variable selection consistency under contamination and misspecification can improve on existing selection methods that rest on a starting point that may be already non-robust or less than fully efficient. <br/><br/>Modern scientific data is characterized by a wealth of information. The data explosion has arisen in diverse areas running the gamut from drug discovery to the financial markets and even homeland security. While the massive influx of data has led to breakthroughs in these fields, it brings many statistical issues to the forefront. In particular, it can be an overwhelming task to determine the relevant predictor variables that provide a suitable model. Meanwhile, with today's complex data, this postulated model will surely be only a simplification of reality. Thus it is inevitable that some of the data will deviate, perhaps significantly, from the model, although it is still useful for the bulk of the data and can provide meaningful insight. This research targets the essential task of developing techniques to perform estimation and variable selection, while also allowing for some of the data to deviate from the model without greatly affecting the results. The methods developed from this research are robust to outliers and model misspecification, while still maintaining efficiency for both estimation and variable selection even in the presence of this contamination. Thus it will be a key component to enable meaningful results in the face of complex data."
"1309009","Statistical Analysis of  Time Series with Long Memory","DMS","STATISTICS","09/01/2013","06/22/2015","Murad Taqqu","MA","Trustees of Boston University","Continuing Grant","Gabor Szekely","08/31/2017","$199,999.00","Mamikon Ginovyan","murad@math.bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","MPS","1269","","$0.00","During the last decades, long memory statistical models have become an important part of theoretical and applied time series analysis.  These models are characterized by slowly decaying correlation functions at infinity, or by spectral densities possessing singularities (poles or zeros) at the origin.  These features change in an essential way the statistical estimation and prediction procedures, and as a consequence, many of the methods and results used for analyzing short-memory time series models are no longer appropriate.  The main objective of the proposal is to develop rigorous estimation and prediction procedures for a broad class of time series models that possess various types of memory structures.  The PIs will study estimation and prediction problems for second order discrete or continuous time stationary random processes with spectral density functions that may have singularities, and the related analytical problems from Toeplitz operators theory, which serve as tools both in the discrete and continuous time case.  In the estimation problem, the PIs will investigate the statistical properties of various estimators of the unknown parameters of the model, which depend on the memory structure and the smoothness of the spectral density.  In the prediction problem the PIs will study the rate of decrease of the relative prediction error as the length of observed past increases. <br/><br/>In many practical applications (for instance, economics, finance, computer science, hydrology), the data is well described by time series exhibiting both long memory and seasonality, or by some functions of such time series.  The proposed  model captures all these as special cases, and provides a sensible way to analyze certain macroeconomic time series (inflation and interest rates, monetary aggregates, revenue series, etc.), financial time series (volatility of financial asset returns, forward exchange market premia, etc.) as well as time series which arise in the analysis of computer traffic networks."
"1309586","Sparse Graphical Models for Multivariate Time series","DMS","STATISTICS","08/15/2013","08/01/2013","Mohsen Pourahmadi","TX","Texas A&M University","Standard Grant","Gabor Szekely","07/31/2015","$120,000.00","","pourahm@stat.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1269","","$0.00","The objective of this research is to develop sparse graphical models in the spectral domain to help visualize connectivity (correlation) among neurophysiological signals recorded as a multivariate time series. Partial coherence, the spectral-domain analogue of the partial correlation, will be used as a measure of functional connectivity which identifies the frequency region that drives the correlation between any two component series adjusted for the linear effects of the others. In the neuroscience applications, the vertices of a graph may represent different voxels while an edge between two vertices reflect a direct connection between the signals at the two voxels. The absence of an edge is indicated by a null partial coherence for the two signals, and the ability to detect it is the key in the construction of a meaningful graph. At present, partial coherence is computed by estimating the spectral density matrix first and then inverting it. This classical approach works well so long as the dimension of the series or the number of voxels is small relative to the length of the series. Serious computational complexity and statistical stability problems arise when estimating the partial coherences for high-dimensional fMRI time series. The stability and complexity are invariably influenced by factors such as the degree of spectral smoothing and size of the matrix to be inverted. The goal of this research is to completely avoid these issues by estimating the inverse spectral density matrix directly using the penalized normal likelihood in analogy with the recent developments in sparse estimation of Gaussian graphical models leading to the fast graphical lasso methodology. It will exploit an under-utilized fact that the spectral density matrix of a multivariate stationary process at each frequency is actually the covariance matrix of a random vector of the same dimension with complex entries.<br/><br/>Spectral-domain methodologies are commonly used in the analysis of multivariate time series data arising from biological, physical and engineering sciences. This research elevates the general concepts and techniques of Gaussian graphical models from the standard multivariate data to the multivariate time series setup in the spectral domain, and develops graphical lasso methodology for structured covariance (precision) matrices. The proposed work is interdisciplinary in nature with immediate applications to the analysis of neuroscience data. Its focus on high-dimensional data analysis has immediate impacts on settings where multivariate time series data are collected such as in financial markets, epidemiology, environmental monitoring and global change. A graduate student will be involved in the research project, the results will be incorporated in graduate courses and presented in seminars and workshops accessible to researchers outside the field of statistics."
"1309004","Conditional Inference Algorithms for Graphs, Tables, and Point Processes","DMS","STATISTICS","08/01/2013","07/30/2014","Matthew Harrison","RI","Brown University","Continuing Grant","Gabor Szekely","07/31/2016","$120,000.00","","Matthew_Harrison@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1269","9150","$0.00","The statistical challenges posed by discrete-valued matrix data (such as contingency tables, co-occurrence tables, adjacency matrices of graphs and networks, and multivariate binary time series) can often be greatly reduced by focusing on the conditional distribution of the pattern of entries in the matrix, given the margins of the matrix.  Although this conditioning simplifies the statistical challenges, it greatly increases the computational challenges of any associated statistical procedures. This project has two principle aims: (1) to design practical methods and algorithms for statistical inference about the conditional distribution of a matrix given its margins, and (2) to specialize these methods and algorithms to the scientific needs of a variety of disciplines, including neuroscience, ecology, network analysis, educational testing, and combinatorial approximation.<br/><br/>With the advent of new technologies for gathering and storing large amounts of complex data, statistics is playing an increasingly central role in science, technology, engineering, medicine and commerce.  These new data sources require new types of statistical thinking and improved statistical algorithms.  This project develops new algorithms for the statistical analysis of networks, such as social networks, ecological networks, and brain networks.  Preliminary results are already being used by neuroscience collaborators to better understand the structure of human seizures.  This project also funds the training of graduate students who will become the next generation of innovators in science and engineering."
"1344749","Conference Proposal -- StatFest 2013","DMS","STATISTICS","08/01/2013","08/02/2013","Javier Rojo","TX","William Marsh Rice University","Standard Grant","Gabor Szekely","04/30/2014","$43,744.00","","jrojo@iu.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","7556","$0.00","StaFest conference will be held on October 12, 2013 at Rice University. The StaFest is an annual event partially supported by the American Statistical Association through the Committee on Minorities in Statistics. This year, the International Year of Statistics, this event will be especially significant. The goal of the StatFest is to showcase the work by statisticians in all realms of human endeavors, and by doing so, to encourage undergraduate students -- mostly non-traditional and underrepresented -- to pursue graduate careers in the statistical sciences. In addition, opportunities for graduate fellowships are discussed in the context of opportunities for graduate work in specific institutions. These goals are achieved by gathering a cadre of dynamic speakers who are top researchers in their respective domains (government, industry, academia, etc.) who are able to transmit their enthusiasm for the statistical sciences. In addition, a panel of graduate students discusses more specifically opportunities, challenges, and anecdotes that can be helpful for the soon-to-be graduate students. The conference also offers plenty of one-on-one discussion opportunities between undergraduate students and professional statisticians through communitarian meals (breakfast, lunch, and two coffee breaks) and, for the first time in the history of StatFests, an undergraduate research poster session at the end of the day. Students that will be recruited from across Texas. Traditionally the StatFest has recruited students from institutions in the host city. But this year, due to the International Year of Statistics, it is envisioned that the event will have more of a regional impact rather than a local one.<br/> <br/>The conference will assemble a group of top professional statisticians from academia, industry and government to showcase their activities with the purpose of enticing undergraduate students to pursue graduate careers in the statistical sciences. Students, mostly from underrepresented groups in the statistical sciences, engage in conversations about, and learn about cutting edge problems and solutions in, various realms of scientific inquiry, and most importantly, students come to appreciate the fundamental role that the statistical sciences play in supporting and guiding the fundamental scientific research and in the process get interested in pursuing graduate careers. The StatFest continues to have a major positive impact on the decisions of undergraduate students regarding graduate work. As a substantial percentage of those students attending the StatFest are from underrepresented groups in the statistical sciences, the StatFest -- clearly and compellingly-- provides a launching platform for many underrepresented students. These efforts will have an impact in helping to alleviate the well-documented national critical need to train more students from underrepresented groups in postgraduate careers in the mathematical sciences."
"1307838","Optimal treatment policies and adaptive screening for functional predictors","DMS","STATISTICS","09/01/2013","08/29/2013","Ian McKeague","NY","Columbia University","Standard Grant","Nandini Kannan","08/31/2017","$180,000.00","Min Qian","im2131@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","","$0.00","High-dimensional patient profiles based on biomedical images, mass spectrometry, or gene expression, might one day be used to guide treatment selection and improve outcomes.  The first part of the project is devoted to the development of new statistical methodology for assessing the effectiveness of individualized treatment policies based on such high-dimensional profiles.  The approach involves specifying the interaction between the treatment and patient profile in terms of a functional regression model, so data from randomized clinical trials can be utilized to simultaneously evaluate the effectiveness of the treatment policies, measured in terms of mean outcome when all patients follow the policy, and to identify features of patient profiles that optimize the interaction effect over competing treatments.  The second part of the project concerns a new way of calibrating screening procedures based on marginal regression for detecting the presence of significant predictors in high-dimensional profiles.  Standard inferential methods are known to fail in this setting due to the non-regular limiting behavior of the estimated regression coefficient of selected predictors.  To circumvent this non-regularity, a new bootstrap calibration procedure is developed in order to better reflect small-sample behavior.  <br/><br/>Although many methods for analyzing high-dimensional patient profile data have become available over the last 10-20 years, they are primarily for the purpose of finding the key predictors of patient outcomes. Relatively little attention has been paid to the problem of assessing the value of individualized treatment policies to optimize patient outcomes. The major innovation of the project is that new ways of estimating the value of such optimal decision rules in terms of expected patient outcomes are developed.  In addition, a new adaptive resampling test procedure is developed to address a central problem in high-dimensional screening by computing p-values in a way that adapts to the inherent instability of post-model-selected parameter estimates.  The project has broader impacts related to recent advances in biomedical imaging, mass spectrometry, and high-throughput gene expression technology, all of which produce massive amounts of data on individual patients.  The effective use of such data has the potential to open up the possibility of tailoring treatments to individual patients. The proposed methods could be applied, for example, to brain imaging data to design treatments for depression, to PET studies that compare patients treated with cognitive therapy and patients treated with anti-depressants in order to determine which treatment is more likely to benefit a given patient, to mass spectrometry profiling for detecting differences between cancer cases and controls in a way that may contribute to personalized cancer care, and to gene expression profiles for designing individualized therapies for cancer or cardiovascular disease.  Another broader impact is in the training of Ph.D. students mentored by the PI and Co-PI, and in the development of modules for new graduate courses designed to introduce Ph.D. students to functional data analysis and inference for optimal treatment policies."
"1310068","Asymptotic Theory and Resampling Methods for High Dimensional Data","DMS","STATISTICS","07/01/2013","07/14/2015","Soumendra Lahiri","NC","North Carolina State University","Continuing Grant","Gabor Szekely","06/30/2017","$200,000.00","","s.lahiri@wustl.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","","$0.00","This project seeks to make important theoretical and methodological contributions to several critical areas of nonparametric statistical inference for high dimensional data. Specifically, this project concentrates on (i) developing empirical likelihood methods for high dimensional data that, among other applications, allows for simultaneous testing of a large number of hypotheses with user-specified confidence levels even with a moderate sample size; (ii) developing bootstrap methodology for high dimensional data for post-variable selection inference; (iii) developing limit theory for studying  first- and higher- order asymptotic properties of statistical methods in high dimensions; and (iv) investigating theoretical properties of the proposed and existing resampling methods in high dimensions.<br/><br/>In recent years, high dimensional data appear routinely in many areas of sciences (e.g., Molecular Genetics, Finance, Climate studies, brain mapping, etc.) and in an ever increasing number of everyday activities (e.g., social networking, internet browsing, etc.). This presents unique challenges for information extraction, as traditional statistical methods do not perform well in such ""needle in a haystack"" situations  - where the relevant information is confounded by the presence of a huge number of irrelevant variables. The proposed research seeks to address this need directly by developing novel statistical methods for high dimensional data without stringent assumptions on the data structure."
"1309162","Collaborative Research:   New Directions for Research on Some Large-Scale Multiple Testing Problems","DMS","STATISTICS","07/15/2013","07/01/2015","Wenge Guo","NJ","New Jersey Institute of Technology","Continuing Grant","Gabor Szekely","06/30/2017","$73,656.00","","wenge.guo@njit.edu","University Heights","Newark","NJ","071021982","9735965275","MPS","1269","","$0.00","The paradigm shift of hypothesis testing from single to multiple hypotheses, often large number of them, in statistical applications to modern scientific investigations, such as brain imaging, microarray analysis, astronomy, atmospheric science, drug discovery and many others, has generated tremendous upsurge of research in the field of large-scale multiple testing in the last one and half decades. Nevertheless, some fundamentally important theoretical as well as methodological issues arising in many of these investigations still remain to be fully addressed before developing the necessary statistical tools. For instance, in clinical pharmacogenomics involving multiple testing, methods controlling false discoveries are yet to be developed in non-asymptotic setting when these hypotheses are tested group sequentially which is often required in order to meet economical and ethical concerns, or when these hypotheses belong to tree-structured hierarchical families which often happens due to importance based ordering of different sets of hypotheses. Also, in many practical applications of multiple testing where the order in which the tests are to be performed is pre-specified or can be assessed based on available data, but the potential improvements of the existing FDR methodologies exploiting this pre-ordering are yet to be explored. The project seeks to develop new and innovative multiple testing methods tackling these outstanding and related issues by focusing on the following three broad areas of research: (i) group sequential multiple testing, (ii) fixed sequence multiple testing, and (iii) testing multiple families of hypotheses. The project covers a wide spectrum of important multiple testing problems statisticians face in many practical settings. These problems are new and pose several technical challenges, as the existing theory and methodologies on multiple testing controlling false discoveries need to be extended from the framework of single stage or single family to that of multiple stages or multiple families. The proposed research has the potential to open up the door for research on multiple testing in newer directions. It not only aims at advancing the theory of multiple testing but also pays special attention to applications of the developed theories. <br/><br/>This project is expected to have a broad impact on the theory and practice of statistics. It aims at modernizing the field of statistics by advancing research in areas of importance in modern scientific experiments, and thus can benefit the society. For instance, the project can potentially pave the way for novel techniques to address statistical issues faced in modern drug discoveries and biomedical experiments. It would also benefit education through training of graduate students and incorporation of the developed methodologies in statistics courses. The results will be disseminated through presentations and discussions at national and international conferences, and visits to other institutions. The software to be developed under this project will be made available, free of charge, to the scientific community."
"1310795","Collaborative Research: Inference for Linear Model Parameters in Model-free Populations","DMS","STATISTICS","09/15/2013","09/11/2013","Lawrence Brown","PA","University of Pennsylvania","Standard Grant","Gabor Szekely","08/31/2014","$199,395.00","Richard Berk, Edward George, Linda Zhao, Andreas Buja","lbrown@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","","$0.00","It is common statistical practice to employ a linear, least squares analysis, even though the assumptions justifying the inference from such an analysis are not valid. The current goal is to provide a guide to useful inferential statements that can be justifiably used in the face of this common dilemma. For this purpose, begin with a nearly model-free description of the data generating process: the observations are a random sample from a population consisting of vector-valued covariates and accompanying real responses. Inference is in the form of a linear description of the relation between the response and the covariates. The target of inference is the, suitably defined, best linear description of the population dependence of the response on the covariates. A first task is to provide a mathematical framework to accurately describe such a situation and enable its rigorous analysis. Within this formulation possible forms of inference can then be investigated. It can be shown that the conventional sample least-squares estimate of the linear coefficients has certain asymptotic optimality properties. But the conventional standard errors and confidence intervals for these coefficients are in general not asymptotically correct. Correct asymptotic inference is provided by suitable forms of either the bootstrap or the so-called sandwich estimator. The current research will discuss variations of these inferential procedures.  It will also describe situations in which these inferential procedures provide trustworthy results for realistic sample sizes. The model-free perspective leads to additional understanding of other important statistical settings involving possibly informative covariates. One of these relates to Randomized Clinical Trials in which interest centers on the Average Treatment Effect, compared to that of a placebo or alternate, standard treatment. The general formulation suggests use of a new estimator and related inference, and the properties and variants of this will be investigated.<br/><br/>Statistical practice is built on and justified by corresponding statistical theory. That theory has a common overarching paradigm: Statistical data is observed. A statistical description is adopted for this data and the data is then analyzed according to this statistical description. There is a presumption in this paradigm that the analytic model agrees sufficiently well with the actual model that generated the data. This is often not the case in practice. The current proposal builds a new, coherent theory that goes beyond the common paradigm in that it allows the statistical model for the data and the model for the analysis to be very different. The effect of the proposed research should be to first warn practitioners of often encountered but rarely recognized dangers. These are inherent in the common practice of using linear models such as regression analysis and ANOVA when they may not sufficiently accurately represent the true nature of the statistical sample. It will then provide alternate forms of inference that are valid and can be responsibly utilized in such situations. To complement its theoretical, methodological orientation the research maintains close connections with applications through the applied activities of several of the senior investigators in diverse areas including social science - especially criminology -operations research and health care."
"1255406","CAREER: Simultaneous and Sequential Inference of High-dimensional Data with Sparse Structure","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2013","06/09/2017","Wenguang Sun","CA","University of Southern California","Continuing Grant","Gabor Szekely","06/30/2019","$400,001.00","","wenguans@marshall.usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","1269, 8048","1045","$0.00","The accurate and reliable recovery of sparse signals in massive and complex data has been a fundamental question in many scientific fields. The discovery process usually involves an extensive screening through a large number of hypotheses to separate signals of interest and also recognize their patterns. The situation can be described as finding needles of various shapes in a haystack. Despite the enormous progress on methodological work in data screening, pattern recognition and related fields, there have been little theoretical studies on the issues of optimality and error control in situations where a large number of decisions are made sequentially and simultaneously. These issues are among the central topics in modern Statistics; hence it is imperative to develop solid theory and powerful data-driven methods to help understand, regulate and optimize the dynamic decision process of sparse signal and pattern recovery. The specific research goals in this proposal are: to study the optimality theory and develop data-driven methods for a broad class of interrelated problems in signal detection, multiple testing and pattern classification; to develop a dynamic scheme for data acquisition, resource allocation and decision making for effective and accurate signal recovery; and to develop a compound decision theoretic framework for large-scale simultaneous and sequential inference. <br/> <br/>The data screening and pattern recognition problems may arise from a wide range of scientific applications such as bioinformatics, finance, signal and language processing, image analysis, and geographical and astronomical surveys. These problems have significantly contributed to the rapid growth of a new and active interdisciplinary research area in data mining that has attracted substantial interests from applied mathematicians, statisticians and computer scientists. The proposed research provides important insights on some fundamental issues in these problems such as how the size of large data sets can be reduced significantly without losing many signals, how the signals can be separated from noise optimally, how the shapes and patterns of different objects can be recognized accurately, and how the inflation of errors in a large number of decisions can be controlled effectively. User-friendly software will be developed and made freely available for public use. The investigator will integrate the proposed research into educational activities through developing new courses for the young USC Statistics program, and through mentoring and training both undergraduate and graduate students to help them participate effectively in an information era overwhelmed by massive data."
"1252795","CAREER: Deformations in statistics, cosmology and image analysis","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2013","05/19/2020","Ethan Anderes","CA","University of California-Davis","Continuing Grant","Gabor Szekely","06/30/2021","$400,004.00","","anderes@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269, 8048","1045","$0.00","Smooth invertible transformations, or deformations, are fast becoming important tools in modern data analysis. The nonlinear nature of deformations makes these objects extremely powerful while at the same time making them challenging to estimate and theoretically explore. This proposal is dedicated to the development and theoretical understanding of deformations applied to three specific areas of research: statistics, cosmology and image analysis. The theoretical properties of estimated deformations for generating nonparametric and semiparametric statistical estimates are analyzed through a surprising connection with Stein's method. In addition, the investigator focuses on recent results found in the theory of optimal transport, which has the potential to provide a rigorous theoretical foundation for deformable templates. The computational aspects of estimated deformations will utilize a new Euler-Lagrange characterization of a penalized maximum likelihood estimate, which can significantly relieve the typical computational burden associated with estimation. One consequence will be to make these methods available for widespread use by statistical practitioners in a broad range of problems: nonparametric and semiparametric density estimation, estimating gravitational lensing in cosmology and posterior sampling techniques, to name a few. Another intellectual merit of this proposal are the scientific ramifications of two new proposed deformation estimates of weak lensing of the cosmic microwave background (CMB): a wavelet/Slepian quadratic estimator and a new Bayesian lensing estimator. Gravitational lensing studies have become one of the most successful tools for probing the nature of dark matter. The precise estimation of lensing is important for a number of reasons including, but not limited to, understanding cosmic structure, constraining cosmological parameters and detecting gravity waves. The investigator proposes to uses wavelets and Slepian multi-tapers to adapt the quadratic estimate to local foreground contaminants and sky cuts, which are ubiquitous features in most modern cosmological surveys. The investigator proposes a new Bayesian estimator which has the potential to dramatically change the way gravitational lensing studies are done and how they are integrated within other astronomical surveys.<br/><br/>Smooth invertible transformations, or deformations, are fast becoming important tools in modern data analysis.  They have been used with spectacular success in the field of computational anatomy where time varying vector field flows which generate deformations are used to statistically analyze medical fMRI images and quantify abnormal morphological structure. In cosmology, deformations are used to model gravitational distortions of the cosmic microwave background from dark matter density fluctuations, and have resulted in a deeper understanding of cosmic structure. Even though these important tools are becoming integrated in modern scientific methods, the statistical properties of estimated deformations have been largely unexplored. This proposal is dedicated to the development and theoretical understanding of deformations applied to three specific areas of research: statistics, cosmology and image analysis. The tools resulting from this project will be useful, not only in statistics, but also in other branches of science and technology ranging from genetics to machine learning. In the field of physics, for example, the potential scientific progress resulting from gravitational lensing estimation could a have broad impact on scientific understanding and the future of scientific research. Moreover, it is becoming increasingly important to train graduate and undergraduate students with the tools necessary to successfully navigate interdisciplinary work, and who are prepared for independent research. The interdisciplinary nature of the proposal will foster a culture of collaboration that will reach the fundamentals of statistical education and will deepen ties with statistics and other physical sciences. In addition, through the integration of research and education, the proposal will teach both graduate and undergraduate students research skills. The result will be two fold. First, it will train graduate students to become creative independent researchers who can contribute within an academic environment. Second, it will educate undergraduates to navigate a work environment which values creative independent investigation."
"1265203","FRG: Collaborative Research:  Statistical Modeling and Inference of Vast Matrices for Complex Problems","DMS","STATISTICS","07/15/2013","05/16/2014","Yazhen Wang","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","06/30/2018","$721,996.00","Christina Kendziorski Newton","yzwang@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","1616","$0.00","Technological advances make it possible to collect and store large data with relatively low costs. As a result, scientific studies in a wide range of fields routinely generate a massive amount of data. Oftentimes, our ability to obtain measurements outpaces our ability to derive useful information from them. These pressing challenges serve as the ultimate motivation for the proposed research. In particular, this collaborative proposal presents novel research plans on the development of statistical theory and methodologies as well as computational techniques for a host of problems involving large matrices ranging from covariance matrices, volatility matrices, density matrices to relational matrices.<br/><br/>The past few years have witnessed an explosion of data as a result of scientific and technological advances. As data storage cost continues to fall, the focal point on these big data has been transitioning inevitably from data management towards deriving actionable insights from them. There is a pressing need to respond to these challenges and understand the profound impact of large data on scientific research and knowledge discovery. The proposed research project deals with emerging problems that arise naturally at the frontier of a multitude of scientific and technological fields such as systems biology, high-frequency finance, and quantum computing among others. As a consequence, the proposed research effort will not only push forward the state-of-the-art of statistical understanding of large matrices, but also facilitate the advancement of various scientific fields and their embracement of the digital revolution."
"1262034","9th Conference on Bayesian Nonparametrics","DMS","STATISTICS","04/01/2013","03/18/2013","Subhashis Ghoshal","NC","North Carolina State University","Standard Grant","Gabor Szekely","03/31/2014","$20,000.00","","subhashis_ghoshal@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","7556","$0.00","The 9th Conference on Bayesian Nonparametrics is going to be held in Amsterdam, The Netherlands, from June 10 to 14, 2013.  Bayesian nonparametrics has evolved as one of the fastest growing areas of research in modern statistics. Its applications areas include genetics, finance, survival analysis, sociology, networks and machine learning. The conference is the most important meeting of researchers working in theory, methodology and all types of applications of Bayesian nonparametrics all over the world. This grant supports junior researchers currently working in U.S. institutions (graduate students, postdoctoral researchers and junior faculty generally within three years of completion of their terminal degree) to participate in the conference. Participation in this meeting is critical for junior researchers working in this area. The primary objective of this conference is to bring together experts and young researchers, as well as theoreticians and practitioners, who use Bayesian nonparametric techniques. The conference has a well-structured balanced program covering various areas of the subject. The scientific committee of the conference consists of renowned international experts on Bayesian nonparametrics and related topics. The meeting will include four overview plenary talks, forty-two invited talks, six contributed talks and a contributed poster session. Many of the invited speakers including a plenary speaker are women.<br/><br/>Providing support for junior researchers who do not have access to other sources of funding to attend the important international gathering of scientists working on one of the fastest growing areas of statistical sciences is key to maintaining the current leadership of American institutions in this field. These workshops in the past were always characterized by a congenial atmosphere particularly supportive of junior researchers. The conference will include a series of activities especially designed to maximize the active participation of young researchers and to provide them with many opportunities for interaction with other young researchers and with more senior colleagues. The conference will also provide American researchers opportunity to exchange ideas with leading researchers from elsewhere in the world such as Europe, Asia and Latin America. In addition, the conference will provide opportunities for young researchers to disseminate widely the results of their work, not only through contributed talks and posters, but also by facilitating the publication of peer-reviewed papers and a proposed conference volume to be published by a leading publisher. The extensive poster session and some slots for contributed talks are especially reserved for young researchers. Women and minorities are highly encouraged to take part in the conference with the help from the travel support."

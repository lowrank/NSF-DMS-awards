"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1106518","Fourier Methods in the Analysis of nonstationary and nonlinear stochastic processes","DMS","STATISTICS","08/01/2011","06/23/2011","Suhasini Subba Rao","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","07/31/2015","$127,137.00","","suhasini@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","","$0.00","The investigator develops new Fourier based methods for analyzing nonstationary and nonlinear time series. Fourier analysis is the well established de facto tool for analyzing linear, stationary time series. There are several reasons for this (i) the discrete Fourier transform asymptotically uncorrelates a stationary time series (ii) if the time series is stationary and linear, then estimates of the spectral density function can be used to identify the underlying linear model (iii) the spectral density function can be used as a means of checking goodness of fit of a linear model. However, it has long been observed that several time series models do not fit well within the stationary, linear model framework. Over long periods of time the assumption of stationarity is often quite unrealistic.  Even over short periods of time, the assumption of linearity can be too strong. Applying standard Fourier methods to such data can lead to uninformative and misleading conclusions. But in contrast to linear models, there does not exist universal methods for comparing non-nested, nonlinear models, checking adequacy of any given model, etc. As increasingly complex time series models are introduced, it has become increasingly important to develop such methods, and the investigator addresses these issues. The investigator focuses on three areas where, in applications, nonstationarity and nonlinearity can arise (i) nonstationary discrete time stochastic processes (ii) functional time series with random sampling (iii) nonlinear, stationary time series. These are detailed below. In the first project the investigator exploits the fact that the discrete Fourier transform only decorrelates second order stationary time series to characterize and model nonstationary behavior. In the second project the investigator considers continuous time series, which are only observed at discrete, randomly sampled time points. Here the focus is on functional time series, and the investigator defines a modified version of the discrete Fourier transform to test for stationarity and to develop goodness of fit tests. As mentioned above, often the assumption of linearity can be too strong, and in the third project the investigator considers stationary time series' which are not necessarily linear. The investigator defines a variant of the spectral density which captures the pair-wise dependence structure of a time series. This transformation allows one to understand the dependence structure of the time series on different parts of the domain of the time series. Using this transformation the investigator checks for model adequacy, tests for equality of pair-wise dependence between two time series and measures the dependence between two time series through an appropriate transformations of the data.  <br/><br/>The analysis of data which is observed over time (usually called a time series) is studied in several disciplines, including the atmospheric sciences, economics etc. As the observations are over time, usually there is dependence (a simple measure of dependence is correlation) between neighboring observations. Understanding and modeling this dependence allows one to forecast (for example, future global temperatures) and compare various different time series (for example, different financial markets). Under the assumption that the time series is stationary (the overall structure does not change over time), and linear (the transition in the times series is smooth), a rich literature on modeling the correlation structure exists. However, there are several real data examples where there are no realistic reasons that these assumptions should hold true, and indeed they could be an oversimplification of the system or simply wrong. In this project, the investigator develops statistical tools which allows one to check whether a time series satisfies the usual assumptions, and if not, how they may violate these assumptions, what impact this may have on standard statistical analysis and how it may effect the conclusions."
"1106450","Modeling, Computational and Inferential Issues in Fingerprint and Health Monitoring Applications","DMS","STATISTICS","09/15/2011","03/18/2013","Chae Young Lim","MI","Michigan State University","Standard Grant","Gabor Szekely","08/31/2015","$169,495.00","Tapabrata Maiti, Sarat Dass, Chae Young Lim","lim@stt.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","","$0.00","Complex data structures with intrinsic multivariate and multilevel characteristics arise frequently in various scientific disciplines. This proposal discusses two such application domains, namely, the areas of health monitoring and fingerprint based authentication. The scientific questions posed are usually associated with high-dimensional parameter spaces, such as spaces of point patterns and functions, and are addressed in a conceptually unified and meaningful way with the development of novel hierarchical models on general object spaces. These hierarchical models are flexible and adept at capturing salient data characteristics, such as clustering and spatial dependence, whose forms vary from one application to another. The proposal develops statistical methodology utilizing parametric multivariate generalized linear mixed models and non-parametric Dirichlet process priors, extended to the space of objects, for studying attributes and the effect of covariates encountered in fingerprint and socio-economic-health applications. Due to the high-dimensionality of the intrinsic spaces, several innovative procedures are developed to overcome ensuing computational challenges in the Bayesian framework, including theoretically justified approximations to the likelihood and predictive inference. An added feature of these inferential tools is to extend posterior analysis of quantities such as means, variances and credible sets, in a meaningful way to the space of objects. <br/> <br/>The scientific goals addressed in this proposal will benefit research in public and social health, engineering and legal forensics. The impact on societal and demographic policy making, for example, will be in the discovery of socio-demographic regions with extreme health and economic conditions, the identification of their potential causes and in the decisions made to mobilize resources accordingly. The proposed research has impact on how forensic evidence should be reported as well. Many forensic scientists as well as legal scholars have become increasingly aware of the shortcomings of fingerprint evidence as is presented in a court of law, and that methodology for assessing the extent of uniqueness of fingerprints requires further scientific validity. Several of these issues are addressed by developing quantitative methods for reporting fingerprint evidence, for example, when additional fingerprint attributes such as quality are available. This research will have broader impact in health and security surveillance, and their monitoring."
"1101212","Travel Support for the 8th Workshop on Bayesian Nonparametrics (BNP 2011)","DMS","STATISTICS","06/01/2011","02/22/2011","Abel Rodriguez","CA","University of California-Santa Cruz","Standard Grant","Gabor Szekely","05/31/2012","$8,000.00","","abelrod@uw.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1269","7556","$0.00","Travel Support for the 8th Workshop on Bayesian Nonparametrics (BNP 2011)<br/><br/>This proposal seeks to secure travel support for junior U.S. researchers (graduate students, postdoctoral <br/>researchers and junior faculty within three years of completion of their terminal degree) to participate in the 8th Workshop on Bayesian Nonparametrics, to be held in Veracruz, Mexico, from June 26 to 30, 2011. The objective of the meeting is to bring together experts and young talented scientists devoted to the study and application of Bayesian nonparametric techniques. The workshop is organized under the auspices of the International Society for Bayesian Analysis (ISBA) and counts with a scientific committee formed by renowned international scientists.  The meeting will include four tutorials on special topics, a series of invited and contributed talks, and a contributed poster session. This is the 8th in series of (mostly) biannual meetings held internationally since 1997, and is the premier venue for dissemination of research in Bayesian nonparametrics. <br/><br/>Bayesian nonparametric (BNP) methods constitute an extremely important area of research in the statistical <br/>sciences, which has recently generated enormous interest and has become one of the fastest growing areas of <br/>research within Bayesian statistics.  Applications of BNP methods include areas as diverse as genetics, finance, sociology and machine learning.  Providing support for junior researchers who do not have access to other sources of funding to attend the most important international gathering of scientists working on one the fastest growing areas of statistical sciences is key to maintaining the current leadership of American institutions in this field.  The conference will include a series of activities specially designed to maximize the active participation of young researchers and to provide them with as many opportunities for interaction with other young researchers and with more senior colleagues.  In addition, the conference will provide opportunities for these young researchers to disseminate widely the results of their work, not only through contributed talks and posters, but also by facilitating the publication of peer-reviewed papers.  Due to the location of the meeting, and the opportunity for travel award recipients to participate for free in the Mexican Workshop on Bayesian Statistics (TAMEB). It is expected that the workshop will attract junior Hispanic/Latino U.S. researchers, providing them the opportunity to engage colleagues from Latin American and the rest of the world."
"1106694","New Methodology for Estimating Random Effects and for Statistical Simulation Studies","DMS","STATISTICS","08/01/2011","07/25/2013","Jeffrey Hart","TX","Texas A&M Research Foundation","Continuing Grant","Gabor Szekely","07/31/2015","$170,665.00","","hart@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","","$0.00","The investigator proposes two distinct threads of research. One thread involves estimation of the distributional components of random effects and mixed effects models, and the other is an investigation of BayesSim, a new and potentially more efficient way of doing statistical simulation studies. Random and mixed effects models are extremely useful statistical models that have wide applications. The investigator studies minimum distance methods for nonparametrically estimating the distributions of the random components of such models. This study includes an investigation of bootstrap methods for placing error bounds on these nonparametric estimates. The investigator also studies the use of minimum distance methods for testing common assumptions, such as normality and independence of random effects and errors, associated with mixed models. Complicated models in the modern statistics world have made simulation the most often used means of investigating new statistical methodology. Almost every simulation study published in a statistics journal proceeds as follows. A few models are selected, hundreds or thousands of data sets are generated from each model, the methodology of interest is applied to every data set, and the results are summarized. A different simulation strategy, called BayesSim, is considered by the investigator. The main idea is to do a better job of sampling all the relevant models. The strategy is to generate one data set (or at most a few data sets) from each of hundreds or thousands of randomly selected models. This approach has the potential of providing more complete information about a statistical method, while doing so at a reduced computational cost relative to the traditional simulation method.<br/><br/>Random effects models are often used for microarray data in genetics. Determining whether certain genes express more for diseased patients than for healthy ones is often the goal of a microarray study. The incorrect specification of distributions in the random effects model could mean that such genes go undetected. Part of the research in this proposal is aimed at improving methods of determining these distributions. Mixed effect models are used in small area estimation, an enor- mously important technique in the field of survey sampling. Estimation of quantities in small areas, such as counties, from surveys taken in the small area are often unreliable, due to small sample sizes. Small area techniques use information from nearby larger areas to infer or predict quantities of interest in the small area. For example, the U.S. Census Bureau has a special program, called Small Area Income and Poverty Estimates, that provides more current estimates of certain income and poverty statistics than those from the most recent decennial census. Small area estimates made by the U.S. Census Bureau affect allocation of federal funds to local jurisdictions, and hence have a major impact on U.S. society. The investigator's research on random and mixed effects models could improve methods for small area estimation. BayesSim has the potential of improving the answers obtained from any statistical simulation study, and hence could have a large impact on the entire field of statistics."
"1106494","Monte Carlo Methods for Analysis of Large Spatial Data","DMS","STATISTICS","08/01/2011","07/24/2011","Faming Liang","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","06/30/2015","$190,000.00","Marc Genton","fmliang@purdue.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","","$0.00","Spatial data sets are analyzed in many scientific disciplines, such as ecology, geology, and environmental sciences.  However, the classical approaches, such as Kriging and Bayesian hierarchical Gaussian modeling, often break down for large data sets due to expensive matrix inverse operations, whose computational complexity increases in cubic order with the number of spatial locations. To alleviate this difficulty, various approximation approaches, such as covariance tapering,  lower-dimensional space spatial process approximation, likelihood approximation and Markov random field approximations, have been proposed under the general idea of approximating the original spatial model with a computationally convenient model. A general concern on these approaches is the adequacy of approximation. In this proposal, the investigators propose three new approaches, Bayesian auxiliary lattice approach, Bayesian site selection approach and marginal inference approach. The Bayesian auxiliary lattice approach introduces an auxiliary lattice to the space of observations and defines a hidden Gaussian Markov random field on the auxiliary lattice.  By using some analytical results of Gaussian Markov random fields,  the Bayesian auxiliary lattice approach completely avoids the problem of matrix inversion in likelihood evaluation. The Bayesian site selection approach reformulates the problem of spatial model estimation as a problem of Bayesian variable selection. It works with only a small proportion of the data at each iteration and thus significantly reduces the dimension of the data. The marginal inference approach is proposed based on the idea of bootstrap resampling. Like the Bayesian site selection approach, it works with only a small proportion of the data at each iteration and thus significantly reduces the dimension of the data. It is worth noting that the Bayesian site selection and marginal inference approaches are conceptually very different from the approximation approaches existing in the literature. The existing approximation approaches are to approximate the original model using a computationally convenient model. Instead, the Bayesian site selection and marginal inference approaches seek to reduce the dimension of the data,  while not sacrificing the complexity of the original model. In this proposal, the investigators also extend the proposed approaches to spatio-temporal models with applications to satellite climate data. How to deal with missing data for spatio-temporal models are addressed.<br/><br/>The intellectual merit of this project is to provide some computationally efficient or data dimension reduction approaches for statistical analysis of large spatial data. The new approaches address some core problems in spatial data analysis, such as large matrix inversion and missing data imputation. The new approaches are expected to play a major role in statistical analysis of geostatistical data, satellite climate data and other large spatial data. This project will have broader impacts in both communities of spatial statistics and computational atmospheric sciences. The research results will be disseminated to the communities via direct collaboration with researchers in other disciplines, conference presentations, books, and papers to be published in academic journals.  The project will have also significant impacts on education through direct <br/>involvement of graduate students in the project and incorporation of results into undergraduate and graduate courses."
"1105634","Dimension Reduction, Model Selection and Classification in Functional Data Analysis.","DMS","STATISTICS","09/01/2011","08/30/2012","John Stufken","GA","University of Georgia Research Foundation Inc","Standard Grant","Gabor Szekely","08/31/2014","$119,999.00","","jstufken@gmu.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","","$0.00","Functional data analysis aims to model and analyze data sets where a datum is a random function, e.g. a curve or a high dimensional image. Due to the fast growth of modern data collection methods, such data sets become more and more prevalent in many biological, medical and industrial applications. Functional data are viewed as infinite dimensional vectors in a functional space, and are usually observed on discrete points and measured with error. Due to the infinite dimensional nature of functional data, dimension reduction is essential for visualizing, modeling and making inference on these data. In the proposed project, the investigator will study new, computationally efficient dimension reduction methods for functional data based on spline approximations, and use asymptotic theory to develop new statistical devices for model selection and inference. The investigator will also study classification problems in functional data, by combining the proposed dimension reduction techniques with modern machine learning methods.<br/><br/>The proposed research is motivated by data from colon carcinogenesis experiments, hypertension studies, AIDS clinical trials and functional magnetic resonance imaging experiments. The proposed project will benefit the society by advancing knowledge in these scientific fields. To achieve broader dissemination of the research results, the investigator will provide free and user friendly software to all scientific researchers. A new course on functional data analysis will be developed in the investigator's institute. The new course aims to nurture the ability of students to analyze real and innovative data sets and help them gain deeper understanding of modern statistical methods and theory."
"1106816","Novel nonparametric methods for prognosis studies with missing covariates","DMS","STATISTICS","08/01/2011","05/27/2011","Xiao Song","GA","University of Georgia Research Foundation Inc","Standard Grant","Gabor Szekely","07/31/2015","$149,966.00","Lily Wang","xsong@uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","","$0.00","Survival data with missing time-independent or time-dependent covariates are commonly encountered in prognosis studies. Existing approaches mostly focus on the standard proportional hazards model, which may be too restricted in some applications. To obtain a comprehensive understanding of data, it is also important to identify variables that are associated with survival time. In this study, the investigators will develop nonparametric approaches for more flexible models with missing covariates, and investigate variable selection in this complicated setup.<br/><br/>The proposed research is expected to have broad impacts and application in biomedical studies, econometrics, environmental studies, behavioral and social sciences, where information is collected on time to an event of interest and multiple covariates. This proposed study will foster collaborations among investigators from different institutions/departments and backgrounds. It will promote teaching, training and learning in the Statistics Department and the newly founded Epidemiology and Biostatistics Department at the University of Georgia. Research conducted in this study will help develop advanced graduate courses in survival analysis, missing data and variable selection. It will create challenging statistical projects for graduate students that the investigators are supervising. Research results from this proposal will be disseminated through presentation at major statistical meetings. Software developed will be made publicly available, so that the proposed methods can be readily used in practice in various fields."
"1106695","Spatio-Temporal Dependence and Extremes with Applications to Networking and the Environment","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","08/01/2011","08/26/2013","Stilian Stoev","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","01/31/2015","$330,090.00","George Michailidis","sstoev@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1253, 1269","1515","$0.00","The goal of the project is to develop models and statistical inference techniques in the context of space-time computer network traffic and environmental data. The PIs propose to extend their joint work on global network traffic modeling via multivariate spatio-temporal processes and to address the network kriging, prediction, and optimal monitoring design problems. New problems involving extremal dependence in computer network traffic as well as environmental data will be also addressed. To do so, the PIs propose to use established techniques as well as to develop new tools involving max-stable processes, multivariate, functional, and hidden regular variation.<br/><br/>One of the main themes of the proposal is to understand and model the statistical aspects of traffic propagation in computer networks. This research would help predict, detect, monitor, and manage computer network traffic in a more principled way. The proposed methodology focuses on characterizing the global statistical behavior in both space and time, which would provide a more comprehensive picture of the entire network, namely the traffic loads on all links, routes, at concurrent as well as different points of time. This could enable the practitioners to predict the traffic load on an unobserved link or route by monitoring a select set of links or routes.  Another aspect of the proposed research involves applying the Extreme Value Theory to understand and model the statistical dependence of extreme delays and traffic loads in computer networks. This could help identify bottlenecks and ?weak links? when unusually extreme traffic volumes arise. Related important problems arise in environmental applications, where extremes play a critical role. For example, the adequate modeling of the probability of extreme precipitation events to occur at the same time over different spatial locations is essential to be able to quantify the risk of floods. The proposed research would help model and estimate such probabilities of concurrent extremes and  evaluate important environmental risks such as pollutions, floods, droughts, hot-spells, etc."
"1148991","Theory and Applications of Random Forests","DMS","STATISTICS","07/01/2011","04/22/2013","Hemant Ishwaran","FL","University of Miami School of Medicine","Continuing Grant","Gabor Szekely","06/30/2014","$159,999.00","","HIshwaran@med.miami.edu","1400 NW 10TH AVE","MIAMI","FL","331361000","3052843924","MPS","1269","","$0.00","This research develops theory for random forests specifically for the purpose of better facilitating its use in practical settings. Theoretical considerations include balancedness, subtrees, node distributions, node splitting, depth of variables, and other novel tree concepts.  These concepts are used to improve prediction and variable selection for random forests in both high and low-dimensional problems.  <br/><br/>One of the simplest techniques for improving the performance of a statistical method such as a tree is to take its average over multiple instances of the data. This averaging process is often referred to as ensemble learning and has attracted considerable attention as it has been widely observed that combining elementary learners can yield a predictor with superior prediction performance. One of the most successful tree ensemble learners is random forests.  Random forests has met with considerable empirical success, yet much is still unknown about it.  This research seeks to improve our understanding of random forests and utilize this knowledge to enhance its application in practical settings.  This research focuses on cardiovascular disease, the number one cause of death in the developed world, cancer staging and prognostication for cancer patients, and identifying and developing genotype signatures for myelodsyplastic syndromes, a heterogeneous diseases of blood stem cells having no current curative medical therapy."
"1106706","Coalescent-based Species Tree Inference Using Algebraic Statistics","DMS","STATISTICS, Cross-BIO Activities, MSPA-INTERDISCIPLINARY","10/01/2011","09/20/2011","Laura Kubatko","OH","Ohio State University","Standard Grant","Gabor Szekely","09/30/2016","$180,000.00","Julia Chifman","lkubatko@stat.osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269, 7275, 7454","8007","$0.00","Due to the increasing ease with which DNA sequence data can be obtained, much of current phylogenetics involves use of  the information contained in representative DNA sequences for a set of sampled organisms for estimation.  The sequence data available for a phylogenetic analysis often include samples taken from multiple genes within each organism, and thus it becomes necessary to model the evolutionary process at two distinct scales.  First, given an overall phylogeny representing the actual evolutionary history of the species, individual genes evolve their own histories, called gene trees.  Then, along each gene tree, sequence data evolve, leading to the observed data that is used for inference.  The coalescent model provides the link between the evolution of the gene trees given the species tree, and the evolution of the sequence data given the gene trees. Phylogenetic invariants have been proposed as a tool for inferring phylogenies using data from a single gene, but have not been studied in the multi-gene coalescent setting. The investigators use phylogenetic invariants to study the coalescent model for species tree inference by addressing questions such as the identifiability of the tree and associated model parameters. In addition, they develop and implement methods to utilize phylogenetic invariants to estimate species trees from empirical DNA sequence data.<br/><br/>The inference of the evolutionary history of a collection of organisms based on the information contained in their DNA sequences is a problem of fundamental importance in evolutionary biology. The abundance of DNA sequence data arising from genome sequencing projects has led to significant challenges in the inference of these phylogenetic relationships. Among these challenges is the inference of the evolutionary history of a collection of species based on DNA sequence information from several distinct genes sampled throughout the genome. The two primary goals of this project are: (1) to determine what aspects of the true phylogenetic history can be accurately identified given the information available in typical DNA sequence data sets; (2) to develop methods for extracting the available information from the DNA sequences in order to accurately and efficiently estimate the true evolutionary relationships.  Both of these objectives are approached using methods from algebraic statistics."
"1106815","Collaborative Research:  Semiparametric conditional graphical models with applications to gene network analysis","DMS","STATISTICS","07/01/2011","04/24/2013","Bing Li","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","06/30/2014","$180,000.00","","bing@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","The research proposed in this project is motivated by the following problem. In many genetic studies, in addition to gene expression data, other types f data are collected from the same individuals. The problem is how to make use of this additional information when construct gene networks. The investigators formulate this problem by a  conditional Gaussian graphical model (CGGM), in which the external variables are incorporated as predictors. They propose an estimation  procedure for this model by combining reproducing kernel Hilbert space with the lasso type regularization. The former is used to construct a model-free estimate of the conditional covariance matrix, and the latter is used to derive a sparse estimators of the conditional precision matrix, whose zero entry pattern correspond to a graph that describes the gene network. They propose to study the asymptotic properties, to introduce methods to determine the tuning constants, and to develop standardized and openly accessible computer programs for this model. Furthermore, the investigators propose to extend the CGGM in two directions. First, they propose to relax the Gaussian assumption by applying a copula transformation to the residuals and then using  pseudo likelihood to  estimate conditional correlations. These are then subject to the lasso-type  regularization to yield sparse estimator of the precision matrix. The second direction is the development of  sufficient graphical model, which is a mechanism to simultaneously reduce the dimension of the predictor and estimate the graphical structure of the response.<br/><br/><br/>High-throughput technologies that enable researchers to collect and monitor information at the genome level have revolutionized the field of biology in the past fifteen years. These data offer unprecedented amount and diverse types of data that reveal different aspects of the biological processes. At the same time, they also present many statistical and computational challenges that cannot be addressed by traditional statistical methods. In current genomics research it has become increasingly clear that statistical analysis based on individual genes may incur loss of information on the biological process under study. For example, a widely known study on identifying genetic patterns of diabetic patients show that no single gene could stand out statistically as responsible for the patterns, and yet clear signals emerged when genes were analyzed in groups. Motivated by this observation, greater attention has been paid to networks of genes. The investigators propose a class of new statistical methods, called conditional graphical models, for constructing gene networks that can take into account of a set of  covariates. They also plan to develop theoretical properties and computer programs for the proposed methods. Although their inquire began with gene networks, the investigators  envision conditional graphical models to have broad applications beyond genomics, such as in predicting asset returns  and in studying social networks, which are becoming all the more prevalent in this age of Internet."
"1056125","CAREER:  Bridging dynamical and statistical models of neural circuits --  a mechanistic approach to multi-spike synchrony","DMS","STATISTICS, MATHEMATICAL BIOLOGY, Robust Intelligence","03/01/2011","02/20/2011","Eric Shea-Brown","WA","University of Washington","Standard Grant","Junping Wang","02/28/2017","$469,114.00","","etsb@amath.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269, 7334, 7495","1045, 1187","$0.00","Questions of neural synchrony -- correlations in cell-to-cell spiking -- have <br/>driven decades of research.  However, recent technological and theoretical <br/>advances have thrust open two lines of inquiry.  The first is understanding <br/>the combinatorial scale of the correlations that occur in natural and model <br/>neural networks.  It is well known that describing neural activity requires <br/>pairwise statistical interactions -- but we do not yet understand when network <br/>dynamics produce patterns of correlations that extend beyond this pairwise <br/>description, when the pairwise descriptions will be complete, and what the <br/>overall implications are for neural coding and signal processing.  <br/>The Principal Investigator will address these questions for a set of ""canonical"" <br/>neural circuits, or motifs, and will build toward networks of gradually <br/>increasing complexity in their dynamics and architecture.  Further, extending <br/>beyond intrinsic network dynamics, he will ask how these basic network properties <br/>determine the ways in which patterns of synchrony can be controlled by external stimulation.   <br/>Answering these complementary questions requires synergies between methods of <br/>stochastic processes, dynamical systems, and statistical inference.<br/><br/>How do how networked neurons work together to produce the brain's astonishing <br/>computational ability?   Such coordinated neural dynamics are characterized by <br/>synchrony among different neurons.  One prospect is that this coordinated <br/>activity opens new channels for signal processing:  there is a combinatorial <br/>explosion in the number of possible multi-neuron patterns that can occur in <br/>increasingly large networks.  However, we only have the first hints at whether <br/>and when these patterns systematically occur in the brain's networks, and what <br/>information they might (or might not) carry.  Shea-Brown will study the <br/>fundamental properties of neural dynamics, connectivity, and noise that <br/>determine the level and impact of multi-neuron synchrony in a series of <br/>networks of gradually increasing complexity.  He will use <br/>interdisciplinary tools from both deterministic and statistical branches of <br/>applied mathematics to understand how levels of synchrony are created, <br/>destroyed, and manipulated by external stimulation.   These findings will <br/>contribute to experimental and clinical neuroscience:  working in <br/>collaboration with experimentalists, the investigator will make predictions for light stimuli <br/>that evoke higher-order correlations in the retina, and for electrical stimuli <br/>that suppress pathological synchrony in neurodegenerative disease.  These <br/>questions, as part of theoretical neuroscience -- an emerging field that is <br/>rich in open questions and highly varied interdisciplinary techniques -- <br/>present a strong opportunity for recruiting, engaging, and training <br/>undergraduates in the mathematical sciences.  Shea-Brown will direct this opportunity <br/>toward the underrepresented groups from which new scholars are most urgently <br/>needed, through an integrated four-year research pathway for undergraduates.  <br/>This will be developed together with newly designed units in the computational <br/>science and mathematical biology courses taught by the investigator."
"1103263","Doctoral Student Forum and Student Travel at the 2011 SIAM Data Mining Conference; Phoenix, AZ","DMS","STATISTICS","02/15/2011","02/14/2011","Marina Meila","WA","University of Washington","Standard Grant","Gabor Szekely","01/31/2013","$29,140.00","","mmp@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","7556","$0.00","The 2011 SIAM Data Mining (SDM 2011) Conference is being organized by the Society for Industrial and Applied Mathematics (SIAM) in cooperation with the American Statistical Association. SDM 2011 will be held in Phoenix (AZ), USA from April 28th to April 30, 2011. This conference is the foremost venue for researchers who are addressing problems in data mining to present their work in a peer-reviewed forum. It is widely attended by researchers and practitioners in the field. Funds are sought to provide travel support for approximately 16 Ph.D. students from universities in the United States to attend SDM 2011 and to participate in a Ph.D. student forum that is to be held as part of SDM 2011.  Funds are requested to provide travel support for approximately 15 graduate students from universities in the United States to attend SDM 2011. Special efforts will be made to attract candidates who are women or belong to minority groups that are currently under-represented within the data mining research community.<br/><br/>Participation in high quality research conferences is an integral component of the training of Ph.D. students in data mining and related fields. The SDM 2011 Ph.D. Student Forum is aimed at providing an opportunity for Ph.D. candidates to present their work and receive constructive feedback and mentoring from established researchers in data mining. Such feedback and mentoring is expected to improve the quality of the students thesis research. Similarly, students recipient of a travel award will be able to attend technical sessions, plenary talks, panels, tutorials and workshops. They will interact with peers who share similar interests from other universities, as well as hundreds of leading researchers in data mining from around the world. This experience will be extremely formative and fruitful towards the shaping of their Data mining is playing an increasingly important role in many emerging data-rich sciences and application domains, such as bioinformatics, computational biology, link analysis, counter-terrorism and security. The SDM 2011 Ph.D. student forum will enrich the education and training of student researchers at early stages in their careers. This is especially true for statistics students, since traditionally in statistics, conference publishing is considered less important than it is for instance in computer science. Experiencing and being mentored in this activity has the potential of culture shift, of improving overall communication between statistics and computer science and engineering, in a domain that lies at the boundary between the two. Both the student forum and travel awards will also help broaden the representation of women and members of underrepresented minority groups within the Data Mining research community."
"1106435","Recurrent Events, Dynamic Reliability Systems, and Multiple Decision-Making with High-Dimensional Data","DMS","STATISTICS, COFFES","09/01/2011","08/13/2011","Edsel Pena","SC","University South Carolina Research Foundation","Standard Grant","Gabor Szekely","08/31/2014","$267,498.00","James Lynch","pena@stat.sc.edu","915 BULL ST","COLUMBIA","SC","292084009","8037777093","MPS","1269, 7552","9150","$0.00","Project deals with the development of statistical theory and methodology for dealing with recurrent events occurring in many scientific fields; with complex systems characterized by dynamic changes incurred by the history of the component failures comprising the systems, such as load-sharing systems and networks; with the analysis of high-dimensional (HD) data sets such as microarray data sets in biology and medicine; and classification and prediction problems with HD predictors and many model classes. The development of the theory and methodology will use modern survival analysis techniques; coherent structure theory from reliability and engineering; Markov random fields and Gibbs measure; Neyman-Pearson paradigm, decision theory, and the Bayesian paradigm. Project goals are to obtain asymptotic properties of semiparametric estimators in the general class of dynamic recurrent event models of Pena and Hollander; to develop probabilistic models for complex systems incorporating component dependencies due to the load-sharing structure of the system; to devise and implement optimal decision-theoretic and Bayesian methodologies in multiple decision-making, classification, and prediction with HD data; and to provide training to students and junior faculty in performing statistical research.<br/><br/>Project results will be of high importance and impact since the class of dynamic recurrent events models considered is more realistic for complex systems, while those dealing with HD data will provide more efficient methodology for discovering relevant `genes.' Consequently, researchers and practitioners dealing with highly complex engineering systems will better predict system or component failures, thus leading to improved maintenance policies and decreased risk of catastrophic system failures; while those involved with HD data will improve discovery of important genes, with positive implications in chronic disease management and control. Finally, the training of graduate students and junior faculty in statistical research will benefit science since they are society's future researchers."
"1104832","Shape Restrictions, Empirical Processes, and Semiparametric Models","DMS","STATISTICS","07/01/2011","05/13/2013","Jon Wellner","WA","University of Washington","Continuing Grant","Gabor Szekely","06/30/2015","$366,097.00","","jaw@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","","$0.00","The investigator carries out research on problems involving shape restricted inference, empirical process methods and tools, and new theory for semiparametric and nonparametric models. In particular, the investigator and a University of Washington graduate student conduct research on new confidence set procedures in the context of convex function estimation and new semiparametric models involving shape restrictions.  The investigator and a University of Washington graduate biostatistics graduate student conduct research on improved estimation methods for semiparametric models with two-phase designs with missing data (by design).  This research also involves development of new asymptotic theory for a variety of complex sampling methods and multi-phase designs. Some of the research on empirical process methods and tools is carried out jointly with colleagues in the Netherlands. Some of the research on inference under shape constrained estimation is carried out jointly with colleagues in France, Switzerland, and Canada. These investigations involve nonstandard asymptotics for maximum likelihood estimators, likelihood ratio statistics, and new nonstandard limit distributions.  Part of the proposed research involves better understanding of bootstrap and other resampling procedures in high-dimensional settings.  The research also involves development of basic empirical process tools and methods, and applications of these new tools and methods to statistical problems concerning semiparametric models, shape restricted models, and high-dimensional data.  One key goal involves improved theory for empirical likelihood and generalized empirical likelihood estimation methods.  Another goal is to understand the effect of heavier tailed distributions in high-dimensional statistical problems.  <br/><br/>Applications include regression models with high-dimensional covariates, models for survival data with missing covariate data, and non- and semi-parametric maximum likelihood estimators used in HIV-AIDS research. The work on two-phase data dependent designs has application to new designs with increased efficiency for clinical trials and case-cohort sampling in epidemiology.  The tools of empirical process theory allow investigations of many problems of current interest in other areas of statistics involving high-dimensional data and parameter spaces. The research benefits education and human development by the training of graduate students and the inclusion of the resulting new statistical methods in graduate level courses for the Departments of Statistics and Biostatistics at the University of Washington."
"1237234","Efficient Modeling in Quantile Regression","DMS","STATISTICS","09/01/2011","08/06/2012","Xuming He","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","08/31/2014","$346,244.00","","xmhe@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Quantile regression has in recent years emerged successfully as a powerful supplement to the more conventional least squares regression. By modeling the conditional quantile functions, the researchers are often able to gain a much more comprehensive picture of how a response variable is associated with its covariates. The prevailing approach in quantile regression is to perform analysis of the conditional quantile functions one percentile level at a time. This approach offers great modeling flexibility at the cost of statistical efficiency. The Principle Investigator proposes to develop and study new approaches to efficient modeling of conditional quantile functions. By ""borrowing strength"" across neighboring quantiles and utilizing a Bayesian empirical likelihood approach, the investigator aims to advance the theory, methodology, and applications of efficient quantile regression. Efficiency gain is an important consideration of any statistical research, and the proposed modeling techniques are especially helpful in the analysis of quantiles in the data-sparse areas. The Bayesian empirical likelihood approach for quantile regression can be used in conjunction with optimal weighting for semiparametric efficiency, and with Markov chain Monte Carlo sampling for effective computation in a high dimensional parameter space.The proposed models, to be called semi-local quantile models, strike to balance bias and variance; when the models do not hold exactly, the proposed estimators follow the spirit of regularization.<br/><br/>Inference in data-sparse areas, including but not restricted to the analysis of high tails, is highly valuable in a wide range of scientific and social studies. The proposed research is motivated by the investigator's interdisciplinary research in climate studies and public health, and will provide researchers in statistics and other fields novel tools for better understanding and quantifying relationships between measurements. The proposed activities include new opportunities for graduate students to participate in transformative research, and will enable the investigator to continue integration of research with teaching and mentoring. The investigator pursues active academic exchanges through lecturers and collaborations, and free distribution of software, for broad dissemination of the research results."
"1105469","2011 International Conference on Probability, Statistics and Data Analysis (2011-ICPSDA)","DMS","PROBABILITY, STATISTICS","04/15/2011","04/08/2011","Subhashis Ghoshal","NC","North Carolina State University","Standard Grant","Haiyan Cai","03/31/2012","$20,000.00","Sujit Ghosh","subhashis_ghoshal@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1263, 1269","7556","$0.00","The Department of Statistics at North Carolina State University (NCSU) is hosting the ""2011 International Conference on Probability, Statistics and Data Analysis"" in the NCSU campus in Raleigh, North Carolina, from April 21-24, 2011.  All presentations will take place in the newly built state-of-the-art building named ""SAS Hall.""  The conference serves as the official meeting of the International Indian Statistical Association (IISA), and provides its members a unique opportunity to meet fellow statisticians and exchange ideas.  The conference is co-sponsored by the American Statistical Association (ASA), NC Chapter of ASA, National Institute of Statistical Sciences (NISS), SAS Corporation, GlaxoSmithKline (GSK), The College of Physical and Mathematical Sciences (PAMS) and the College of Agricultural and Life Sciences (CALS) of  NCSU.  The Department of Statistics at NCSU is providing equipment, staff and logistic support.  The conference includes 4 plenary talks given by Prof. S. R. S. Varadhan, Prof. Peter Hall, Prof. Michael Jordan and Prof. Sally Morton.  The conference also features 6 special invited talks, over 150 invited talks, a poster session and panel discussions on funding opportunities and statistical education.  Eminent speakers, women, minorities, under-represented groups, graduate students and young professionals will be supported by the funding received from federal granting agencies such as the National Science Foundation (NSF). <br/><br/>The main objective of the conference is to bring together both well established and emerging young researchers from around the world who are actively pursuing theoretical and methodological research in statistics and their applications in various allied fields.  The conference aims to provide a forum for leading experts and young researchers to discuss recent progress in statistical theory and applications, thereby provide new directions for statistical inference in various fields.  The conference thus provides tremendous opportunities and benefits to graduate students, young researchers and professionals from industry and government agencies by giving them an exposure to the cutting edge statistical methods and their applications and providing them a platform to showcase their own work to experts on the field through poster and oral presentations.  Moreover, interactive panel discussions on funding opportunities and statistical education will help them better prepare for their career ahead.  Women, minorities and under-represented groups have been encouraged to participate in the conference by proving them partial funding support, thus promoting diversity in our profession.  The conference website http://www.iisaconference.info will  disseminate conference products by providing links to all conference related material including abstracts and slides of the talks and hence this website will serve as a great resource for not only the conference participants but also to any professionals interested in statistical theory and applications."
"1106772","Statistical Methods for Network Data","DMS","STATISTICS","07/01/2011","06/18/2013","Elizaveta Levina","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","06/30/2015","$290,000.00","","elevina@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Network data have become common in a wide range of fields, and a large and diverse community of researchers have studied various aspects of networks, yet statistical methods are rarely applied. This project proposes new theory, methodology, and algorithms that take a principled statistical approach to these problems, assess uncertainty, and establish conditions for desirable properties such as consistency.  The focus is primarily on discovering community structure in networks, a common phenomenon in practice and a fundamental question in network analysis.   New pseudo-likelihood algorithms are proposed for fitting the block model for networks, as well as several generalizations that allow for non-uniform degree distribution within blocks, removing the main limitation of the classic block model.   The pseudo-likelihood based on aggregated data substantially speeds up computation, allowing fitting these models to larger and sparser networks than previously possible.   The asymptotic distribution of criteria used for community detection is also studied, which leads to development of significance tests for community structure, consistency conditions, and asymptotically correct partition thresholds, which have important practical implications.  New, more robust criteria are also proposed, consistent under weaker conditions.  The proposal also develops a formal non-parametric test for comparing two networks, a problem that arises frequently in practice but is currently addressed only through informal comparisons of summary statistics.   Finally, covariates on nodes and edges are incorporated into the models and used for predicting unobserved links in the networks.  Many of the proposed methods provide the first statistical solutions to the corresponding network problems.  <br/><br/>Development of statistical methods for community detection in networks, while contributing to the development of core statistical theory and methodology, has direct impact on the interdisciplinary field of network analysis and the study of complex networks.  The applications of these are wide-spread, covering such diverse areas as infectious disease modeling, national security, communications, sociology, and genomics.   The new statistical tools proposed take a more formal, rigorous approach, and have the potential to change how many scientists approach network analysis."
"1106564","Laplace Deconvolution and Its Application to Analysis of Dynamic Contrast Enhanced Computed Tomography Data","DMS","STATISTICS","07/01/2011","05/23/2011","Marianna Pensky","FL","The University of Central Florida Board of Trustees","Standard Grant","Gabor Szekely","06/30/2015","$250,000.00","","Marianna.Pensky@ucf.edu","4000 CENTRAL FLORIDA BLVD","ORLANDO","FL","328168005","4078230387","MPS","1269","","$0.00","The present proposal is motivated by analysis of Dynamic Contrast Enhanced Computed Tomography (DCE-CT)data.  DCE-CT provides a non-invasive measure of tumor angiogenesis and has great potential for cancer detection and characterization. It offers an  in vivo  tool for the evaluation and optimization of new therapeutic strategies as well as for longitudinal evaluation of therapeutic impacts of anti-angiogenic treatments. The difficulty of the problem stems from the fact that DCE-CT is usually contaminated by a high-level of noise and does not allows to directly measure the function of interest. Mathematically, the problem reduces to solution of a noisy version of Laplace  convolution equation based on discrete measurements, an important problem which also arises  in mathematical physics, population dynamics, theory of superfluidity and  fluorescence spectroscopy. However, exact  solution of the  Laplace convolution equation requires evaluation of the inverse Laplace transform  which is usually  found using  Laplace Transforms tables or partial fraction decomposition. None of these methodologies  can be used in stochastic setting. In addition, Fourier transform based techniques used for solution of a well explored Fourier deconvolution problem are not applicable here since the function of interest is defined on an infinite interval  while observations are available only on on a finite part of its domain and it may not be absolutely integrable on its domain. In spite of its practical importance,  the Laplace deconvolution problem was completely overlooked   by  statistics community. Only few applied mathematicians   took an effort to solve the problem but they  either completely ignored measurement errors  or treated themas fixed non-random values. For this reason,  estimation of a function given noisy observations on its Laplace convolution on an a finite interval requires development of a completely novel  statistical theory. The objective of the present proposal is to fill in this gap and to develop a path-breaking transformative statistical methodology for solution of various aspects of Laplace deconvolution problem:   formulation of fundamental theoretical results,  algorithmic developments and, finally, application of the  newly derived techniques to analysis of DCE-CT data.<br/><br/>The current proposal presents an integral effort of  merging applications and theory. Results of this effort will be greatly beneficial for<br/>a)    the medical practice  since  development of novel  path-breaking methodologies  for analysis of DCE-CT data will potentially  improve clinical outcomes by providing non-invasive tool for cancer detection and characterization as well as for longitudinal evaluation of therapeutic impacts of anti-angiogenic treatments. First, DCE-CT can be used  used for assessment of intra-tumor physiological heterogeneity, thus offering an   in vivo  tool for the evaluation and optimization of new therapeutic strategies.<br/>Second, DCE-CT provides a non-invasive tool for cancer detection and characterization as well as for longitudinal evaluationof therapeutic impact of anti-angiogenic treatments, and therefore, can act as a tool for improvement of those treatments.<br/>b)     the medical research   since   algorithmic developments and the software for interpretation of DCE-CT data will contribute to design of  new methodologies for non-invasive longitudinal evaluation of tumor angiogenesis, cancer detection and characterization. Software will be  freely available to anyone who carries out examination of such data and can be used   in cancer and medical imaging research.<br/>c)  various fields of science   since  data in the form of noisy measurements of the Laplace convolution of a function of interest with a known or estimated kernel appear in many areas of natural science. Analysis of decay curves  in fluorescence spectroscopy  is one but not the only example. However, due to the theoretical and methodological challenges associated  with the solution of Laplace deconvolution problem, these data are usually analyzed in an ""ad-hoc"" manner, or the formulation is abandoned overall in favor of a much less precise but easier treatable set-up.  Novel path-breaking methodologies which will be constructed as a result of this proposal will benefit all those applications.<br/>d)  training and development of the future work force and promoting interdisciplinary research by carrying out various educational activities, attracting and training Ph.D., M.S. and undergraduate students, teaching a Special Topics graduate course, organizing interdisciplinary seminars and promoting interdisciplinary research and diversity."
"1053252","Diagnostics and Experimental Design in Nonlinear Dynamics","DMS","STATISTICS","06/01/2011","05/19/2015","Giles Hooker","NY","Cornell University","Continuing Grant","Gabor Szekely","05/31/2017","$400,000.00","","ghooker@wharton.upenn.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","1045","$0.00","This grant is focused on developing statistical methodology for the parameters of ordinary differential equations and non-linear stochastic models that describe the evolution of physical and biological systems. The proposed methodological development can be broadly divided into two parts: (i) inferential methods and (ii)design of experiments. In inferential methodology, the investigator proposes diagnostic tests to assess mis-specification of the drift processes and the presence of state variables that have not been included in a proposed differential equation model. For the design of experiments, control-theoretic methods are suggested for experimental dynamical systems. Specifically, methods to choose system inputs so as to maximize the Fisher Information concerning the parameters of interest for the observed system are described. While models for rapid evolution in experimental ecologies provide real-world a real-world application, the proposed methods have much wider applicability, for example in chemical engineering, pharmacokinetics and neurodynamics.<br/><br/>The research in this grant will provide new, improved methods for making statistical inferences about real-world systems that change over time. As a specific example, we consider models that describe an experimental ecology in which different species of micro-organisms are grown together in a laboratory tank. However, this research has broader applications in areas as diverse as human immune systems, models for drug absorbtion by the body and chemical engineering. (I) A first task is to answer the question ""Is the proposed model adequate to describe the data observed from this system and, if not, how should it be improved?"" A particular question in experimental ecology is whether the models are missingspecies that are present in the system and which must therefore have evolved during the experiment. This research will provide the first statistical test of recently-advanced theories that evolution can occur at the same time-scales as ecological processes; theories that have profound implications for environmental protection, the management of algal ponds producing biofuels and many other domains. (II) A second focus of this work is to develop tools to improve the design of these experiments so as to obtain maximal information about the system from the data they generate. In ecological experiments, system inputs such as nutrients can be manipulated over time so as to produce the behavior that is most informative about the system's dynamics. Similar experimental design problems exist in neural dynamics, chemical engineering and pharmacokinetics. This grant will support the development of novel mathematical, statistical, and computational methods for addressing these issues."
"1106753","Statistical Problems in Closed-Loop Diabetes Control","DMS","STATISTICS","08/15/2011","08/05/2011","Cun-Hui Zhang","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","07/31/2015","$300,000.00","Larry Shepp","czhang@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","The project focuses on developing statistical models, methods and related theory for closed-loop diabetes control. An artificial pancreas with a closed-loop insulin delivery system, still in an early stage of its development, is expected to revolutionize the way diabetes is treated. As the PIs and many others recognized, a major impediment to the goal of developing an artificial pancreas is the unreliability of the glucose sensor. Sensor technology is not so new and it is also remarkably clever, but frequent recalibration is needed because of the physical processes underlying the way sensors work. The proposed project will solve the problem of proper recalibration by appropriate physical modeling of the metabolic processes involved.  Sensors measure current in interstitial space (subcutaneous fat) at fixed time increments all around the clock. The current is nominally proportional to the amount of glucose in the space, but there are two problems with the measurements. Firstly, there is a delay in diffusion of glucose from the bloodstream into fat so that the glucose density in the fat lags the glucose density in the bloodstream. Secondly, there is a defense mechanism (white blood cells) which surround the electrode as a foreign body and attempt to get rid of it via biofouling. The white blood cells interfere with current flow and produce erroneous measurements unless accurate recalibration is performed. The proposed approach has not been tried and is expected to significantly outperform the current implementation of the sensor technology based on more straightforward regression without gaining the benefit and insight of the proposed physical modeling. A new differential equation approach will be used to deal with the delay problem related to glucose sensor. The differential equation is widely accepted and the rate for the diffusion of glucose from the blood into interstitial space governs the relationship between the glucose densities in the bloodstream and interstitial spaces. The diffusion rate and the effect of biofouling will be estimated from finger stick metered measurements taken a few times a day. A main innovation in the proposed approach is statistical models using the physics of the delay and biofouling problems.<br/><br/>An artificial pancreas will be a godsend to the millions of Americans faced with the 24 hour a day tedium of having to decide when and how much insulin to inject. If this could be done automatically for them then the only task they would continue to have is to remember to replace insulin in the well of their insulin pump. An artificial pancreas would give them an effective treatment for their disease."
"1106576","Multi-armed Bandit Problems with Covariates","DMS","STATISTICS","06/01/2011","05/21/2013","Yuhong Yang","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","10/31/2014","$249,987.00","","yyang@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","Multi-armed bandit (MAB) refers to a class of sequential decision making problems where in each step one needs to choose a population from which a random reward will be generated. The goal is to maximize the total accumulated reward. The literature on MAB, with few exceptions, ignores available covariates. In this project, the PI will study MAB with covariates in general frameworks and develop methodologies as well as theories for various applications. The project will 1) provide methods for selecting key covariates; 2) establish consistency in variable selection; 3) establish consistency of the allocation rule in terms of the accumulated reward; 4) derive the rate of convergence of the accumulated reward relative to the oracle choices. In addition, nonparametric estimation of the mean reward functions and model combinations will be utilized for achieving higher expected reward. Strategies that simultaneously achieve high expected reward and also provide sufficient information for identifying the best arm (with high probability) will be sought.<br/><br/>In practice of medicine, treatments previously shown to be the best at population levels in clinical trials are given to new patients with minimal consideration of his/her own personal characteristics such as genetic profile. If practically feasible, there is every reason for a patient to be treated in a way that the outcomes of all previous treatments of patients with the same disease will have been taken into account and consequently the most promising individualized treatment is selected based on genetic information, clinical assessments, and all the accumulated trial/treatment results. The proposed research will set up statistical frameworks and build theories and methodologies for application of individualized medicine using the statistical machinery of sequential allocation with covariates.  Besides medicine, sequential allocation has applications in operations research, industrial engineering, economics and other fields. Due to the ease of getting and processing information furnished by the exponential growth of modern technology, with new research to bring effective use of key predictors, applications of sequential allocation with covariates will make a real impact, saving lives, improving health, promoting business, and reducing operating cost for the society."
"1107012","New Developments on Confidence Distributions (CDs) and Statistical Inference:  Theory, Methodology and Applications","DMS","STATISTICS","09/01/2011","07/15/2013","Minge Xie","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","08/31/2016","$179,141.00","","mxie@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","Basic statistical methods, such as point estimators, confidence intervals and p-values, are common inferential tools for analyzing information and data. Confidence distribution (CD), also known as a ""distribution estimator,"" contains a wealth of information and is a useful device for constructing all types of frequentists' statistical inferences. Some recent developments have highlighted promising potentials of the CD concept as an effective inferential tool. As an emerging new field of research, there are many important topics and interesting questions yet to be answered. The proposed research addresses several issues related to the theoretical framework of CD inference, and provides useful inference tools for a number of problems where frequentist methods with good properties were previously unavailable or difficult to obtain. Specifically, it proposes to: 1) Develop a general and new framework for resampling, utilizing a concept called CD random variables, and investigate theoretical issues related to the development. This resampling approach can be considered as an extension of the well-studied and widely-applied bootstrap methods, albeit one which is much broader. 2) Develop a new and effective meta-analysis approach to combine CDs of multivariate parameters. This development not only provides solutions to several existing problems in conventional meta-analysis, but also is extended to form a ""split and conquer"" strategy, with supporting asymptotic theory, which has potential applications in mining data of huge size and large dimensions. 3) Develop and generalize the theoretical framework for CD inference including: developing an exact CD concept and inference procedure for small samples from discrete distributions, and introducing a CD probability measure for infinite dimensional parameters (processes) and exploring its applications in survival analysis and growth curve models. <br/><br/>Advanced data acquisition and storage technologies have made it easy for gathering of data and information. The demand for effective statistical inference methods for processing and analyzing those information and data has never been greater. The proposal addresses several fundamental theoretical issues in statistical inference as well as a set of important practical problems arising from various disciplines. Advances in statistical theory and methodological developments are key aspects of the proposed activities. These advances not only solve the specific set of problems set forth in this proposal, but also bring about new perspectives to frequentist, fiducial and Bayesian approaches. Progress from this proposal should further advance theory of statistical inference and development of statistical methodology. It can also facilitate many applications in a variety of fields, including medical research, agriculture, industry, decision making, among others. The proposed research activities are also ideal for engaging student participation and training. Through these projects, students can acquire hands-on experience with real life problems. Such training is essential for them to become effective statisticians in the future."
"1105650","Sufficient dimension reduction of high-dimensional data through regularized covariance estimation","DMS","STATISTICS","07/01/2011","04/08/2013","Adam Rothman","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","06/30/2015","$195,311.00","Ralph Cook","arothman@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","Many statistical methods for dimensionality reduction, classification, and prediction, require an estimate of a covariance or precision matrix.  In high-dimensional settings, (where the number of variables is larger than the sample size), it is known that classical covariance estimation with the sample covariance performs poorly.  This has lead to a wealth of alternative regularized high-dimension covariance estimators, many of which have been proposed in the last decade. These estimators have been analyzed primarily in terms of how they perform when estimating the population covariance or precision matrix directly, rather than how they affect the performance of the statistical methods that require a regularized covariance estimate.  A particular class of statistical methods of interest is those that perform sufficient dimension reduction (SDR), a powerful approach to reduce the dimensionality of the predictor in regression problems. Most of the SDR methodology and theory requires the number of variables to be less than the sample size, preventing its application to high-dimensional data.  The PI, Co-PI, and their colleagues adapt sufficient dimension reduction methodology to high-dimensional settings via regularized covariance estimation.  Specifically, they develop alternative SDR methodology, high-dimensional asymptotic analysis (as both the number of variables and the sample size grow), efficient computational algorithms, and applications to data.<br/><br/><br/>Genetics, spectroscopy, climate studies, and remote sensing are a few examples of the many research fields that produce high-dimensional data; these are data with many more measured characteristics than subjects or cases.  Many standard statistical methods for prediction, classification, and data reduction are either inapplicable or perform poorly in this setting.  In response, statistical methods to extract a subset of the measured characteristics for use in predictive models have been developed; however, these methods operate under the assumption that a relatively small number of measured characteristics are relevant for prediction.  The investigators address this deficiency by developing new methods for the reduction of high-dimensional data for use in predictive modeling, which unlike many existing methods, are able to extract relevant predictive information from all of the measured characteristics.  In addition, the investigators develop publicly available computer software to implement these new methods, enabling their application by researchers and practitioners in many fields."
"1106609","Hierarchical models for Large  Geostatistical Datasets with Application","DMS","STATISTICS","06/01/2011","04/09/2013","Sudipto Banerjee","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","05/31/2014","$303,478.00","Andrew Finley","sudipto@ucla.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","This proposal lays down a comprehensive framework for carrying out statistical inference on point-referenced high-dimensional spatial data available from a large number of locations. The focus of the proposal is methodological rather than purely theoretical or purely applied. Thus, statistical theory is used to develop mathematically formal but computationally feasible methods that can have a broad range of applications. Theoretical derivations and new results that will enhance current methods (including  findings by the PI in prior NSF-funded research) will be explored, but always keeping in mind the practicing spatial analyst. The basic framework is to use a low-rank spatial process obtained by projecting the original process onto a lower-dimensional subspace. The PI intends to explore approximation properties of the low rank spatial process with regard to different metrics. The long-term goal of the PI is to develop a full suite of statistical methods that estimate spatial models in a wide variety of experiments in forestry, ecology and the broader environmental sciences. A recurrent underlying theme of the proposed methods that makes it different from existing methods is that the modeler does not need to sacrifice richness in modeling as a compromise for the large datasets. This resolves the statistical irony that large datasets are precisely where complex relationships can be detected effectively.<br/><br/>Modern spatial technologies such as Geographical Information Systems (GIS) and Global Positioning Systems (GPS) routinely identify geographical coordinates with a simple hand-held device. Consequently, scientists and researchers in a variety of disciplines today have access to geocoded data as never before. With data becoming increasingly high-dimensional both in terms of number of observed locations and the number of observations per location, scientists are seeking to hypothesize complex relationships. These, in turn, yield rather complex hierarchical models that are computationally expensive even for moderately sized datasets. This team recognises a need for statistical modeling of large multivariate spatial data and proposes a model-based setup to tackle a wide variety of large geostatistical datasets. Although some of the more serious statistical modeling will require multi-processor capabilities, the emphasis on this project is on methodology implementable with moderately powerful computing tools. The proposed methodologies would, therefore, be accessible to a large number of researchers. The broader impact of the proposed methods is best assessed by connecting the outcome of this research with the widely recognized impact of GIS on human society. From identifying spatial disparities in health standards to more precise weather predictions, GIS technology is used today in almost every sphere of society and the proposed methods can have far reaching beneficial effects in environmental research that potentially touch unexpected corners of society."
"1056996","CAREER: An integrated probabilistic approach to discrete and continuous extremal problems via information theory","DMS","PROBABILITY, STATISTICS","05/15/2011","09/24/2013","Mokshay Madiman","CT","Yale University","Continuing Grant","Tomek Bartoszynski","03/31/2014","$347,838.00","","madiman@udel.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1263, 1269","1045, 1187","$0.00","Mathematics  abounds  with extremal  problems  problems  where  the  goal  is  to  minimize some  functional  applied  to  a  class  of  objects  under  some constraint,  identify  the  extremal  objects,  and  investigate  the stability  of  extrema.  Relevant  examples  range  from  some  in the  continuous  world  (isoperimetric  phenomena  in  convex geometry,  functional  analytic  inequalities),  to  some  in  the<br/>discrete  world  (structural  phenomena  in  additive  combinatorics), and  some  in  both  (maximum  entropy  problems  in  statistics, limit  theorems  in  probability).  A  natural  language  for  all  of these  problem  classes  is  probability,  and,  although  not obvious,  information  theory.  The  project  will  develop  new formulations  of  extremal  problems  from  each  of  these  fields in  terms  of  information-theoretic  inequalities,  and  then  use  a variety  of  tools  from  analysis,  probability,  convex  geometry, combinatorics,  and  information  theory,  to  make  progress  on them.  The  unifying  nature  of  the  perspective  adopted  will bridge  discrete  and  continuous  problems  using  a  common  set  of tools,  and  enable  significant  cross-fertilization.  Furthermore, some  of  the  information-theoretic  inequalities  developed, combined  with  statistical  decision  theory,  will  be  applied  to novel  statistical  challenges  involving  multiple  players  that arise  in  engineering,  economics,  and  biology  (specifically, theoretical  foundations  for  the  problems  of  data  pricing  and distributed  inference).    The  project  will  use information-theoretic  thinking  to  make  advances  on  challenging mathematical  problems  from  the  three  seemingly  disparate  fields of  convex  geometry,  arithmetic  combinatorics,  and  probability.<br/><br/>Apart  from  the  intrinsic  significance  of  these  areas  within mathematics,  they  have  much  practical  significance - convex geometry  finds  applications  in  medical  tomography,  arithmetic combinatorics  in  computer  science,  and  probability  is ubiquitous  as  the  foundation  of  statistical  inference.  The interpretability  and  unifying  nature  of  the  proposed  research, and  the  diversity  of  tools  it  uses,  create  wonderful  opportunities   for  student  motivation.  Newly  developed  courses  and  a resource  website  on  information  theoretic  approaches  to extremal  problems  will  exploit  these  opportunities.  The investigator  will  disseminate  key  findings  through  survey articles,  organize  an  interdisciplinary  workshop,  and communicate  the  excitement  of  research  through  non-academic public  lectures  to  attract  promising  students  to  the mathematical  sciences.  The  applied  component  of  the  research would  also  have  broad  impact,  by  contributing  to  how  data collectors  and  vendors  come  up  with  pricing  mechanisms  (e.g., for  pricing  of  advertisements  by  search  engines),  and  by improving  the  way  networks  of  sensors  collect  and  use  data for  various  applications  (e.g.,  for  disaster  recovery coordination  or  smart  kindergartens)."
"1106738","Collaborative Research:  Semiparametric conditional graphical models with applications to gene network analysis","DMS","STATISTICS","07/01/2011","04/08/2013","Hongyu Zhao","CT","Yale University","Continuing Grant","Gabor Szekely","06/30/2015","$80,000.00","","hongyu.zhao@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","","$0.00","The research proposed in this project is motivated by the following problem. In many genetic studies, in addition to gene expression data, other types f data are collected from the same individuals. The problem is how to make use of this additional information when construct gene networks. The investigators formulate this problem by a  conditional Gaussian graphical model (CGGM), in which the external variables are incorporated as predictors. They propose an estimation  procedure for this model by combining reproducing kernel Hilbert space with the lasso type regularization. The former is used to construct a model-free estimate of the conditional covariance matrix, and the latter is used to derive a sparse estimators of the conditional precision matrix, whose zero entry pattern correspond to a graph that describes the gene network. They propose to study the asymptotic properties, to introduce methods to determine the tuning constants, and to develop standardized and openly accessible computer programs for this model. Furthermore, the investigators propose to extend the CGGM in two directions. First, they propose to relax the Gaussian assumption by applying a copula transformation to the residuals and then using  pseudo likelihood to  estimate conditional correlations. These are then subject to the lasso-type  regularization to yield sparse estimator of the precision matrix. The second direction is the development of  sufficient graphical model, which is a mechanism to simultaneously reduce the dimension of the predictor and estimate the graphical structure of the response.<br/><br/><br/>High-throughput technologies that enable researchers to collect and monitor information at the genome level have revolutionized the field of biology in the past fifteen years. These data offer unprecedented amount and diverse types of data that reveal different aspects of the biological processes. At the same time, they also present many statistical and computational challenges that cannot be addressed by traditional statistical methods. In current genomics research it has become increasingly clear that statistical analysis based on individual genes may incur loss of information on the biological process under study. For example, a widely known study on identifying genetic patterns of diabetic patients show that no single gene could stand out statistically as responsible for the patterns, and yet clear signals emerged when genes were analyzed in groups. Motivated by this observation, greater attention has been paid to networks of genes. The investigators propose a class of new statistical methods, called conditional graphical models, for constructing gene networks that can take into account of a set of  covariates. They also plan to develop theoretical properties and computer programs for the proposed methods. Although their inquire began with gene networks, the investigators  envision conditional graphical models to have broad applications beyond genomics, such as in predicting asset returns  and in studying social networks, which are becoming all the more prevalent in this age of Internet."
"1106940","Achieving Spatial Adaptation via Inconstant Penalization: Theory and Computational Strategies","DMS","STATISTICS","08/01/2011","08/08/2013","Yajun Mei","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","07/31/2014","$140,000.00","","yajun.mei@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","","$0.00","The investigator will study the relation between inconstant penalization and local adaptivity. The goal is to achieve location adaptive functional estimation when the underlying model (of the relation between a response and input variables) has inhomogeneous roughness. The investigator will (1) quantify the connection between a variable penalty function and the resulting local adaptivity, (2) generalize the methods for high-dimensional input variables, (3) study the selection of algorithmic parameters, (4) derive theoretical properties of the resulting estimators, and (5) design fast computational strategies.<br/><br/>Functional estimation under inhomogeneous roughness is a fundamental problem in many statistical applications. This study will contribute to the fundamental understanding to these problems, as well as providing deployable tools. Especially for large size datasets, better performance than the current state-of-the-art methodology is anticipated."
"1232424","Proportional Hazards Model for Various Types of Censored Survival Data with Longitudinal Covariates","DMS","STATISTICS","11/15/2011","02/29/2012","Jian-Jian Ren","MD","University of Maryland, College Park","Standard Grant","Gabor Szekely","08/31/2013","$59,465.00","","jjren@umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1269","0000, OTHR","$0.00","The interrelationships between time-to-event (survival time) variable and longitudinal covariates is often the primary research interest in medical and epidemiological studies. Due to the challenges encountered in some important clinical trials on AIDS and cancer research, recently the statisticians started modeling survival data and longitudinal data jointly via Cox's proportional hazards model. Such a joint modeling procedure or methodology has broad applications in many scientific research fields, but it is a considerably difficult problem due to censoring on the survival time and that the covariate process is only observed at some given time points. Up to now, statistical methods on this topic have not been fully or well developed, while the importance and needs for developing these methods have become more evident when the proposer and her collaborators recently encountered some more complicated problems which have not been studied in statistical literature; see examples listed below. Specifically, there have not been any modeling procedures that directly study the relationship between survival time and within-subject historic patterns of change in longitudinal covariates, nor have there been any works on joint modeling doubly censored or interval censored survival data together with (intensive or multi-phase intensive) longitudinal covariates, which is far more challenging than right censored data problem. In fact, there have been no published works on the Cox model with doubly censored data, not even for the case with time-independent covariates. In this research, asymptotic methods and simulations will be mainly used in the studies, and the issues under consideration include: (a) derivation of the empirical likelihood based MLE for the Cox model with longitudinal covariates for right censored, doubly censored and interval censored survival data, respectively; (b) computation algorithms for the MLE; (c) asymptotic properties of the MLE; (d) Wilk's theorem for the MLE; (e) goodness-of-fit tests for the Cox model; (f) comparison with alternative methods. At least two Ph.D. students of the proposer will be involved in and benefit from the proposed research.<br/> <br/>The new statistical methodology to be developed in this project has direct impact to medical research, epidemiology, social and behavioral sciences, etc. For instance, the data examples which we have encountered and motivate the research of this project include the following problems on joint modeling survival time and longitudinal covariates. In a prostate cancer study on mice, part of  the research focus is joint modeling interval censored survival time and longitudinal covariates. In a smoking cessation study, the research focus is joint modeling right censored survival time and intensive longitudinal covariates. In a recent study of child development, the research focus is joint modeling doubly censored survival time and multi-phase intensive longitudinal covariates."
"1129626","Conference on 'Imaging, Communications and Finance: Stochastic Modeling of Real-world Problems'","DMS","STATISTICS","07/01/2011","06/21/2011","Martin Lindquist","NY","Columbia University","Standard Grant","Gabor Szekely","06/30/2012","$12,000.00","Tze Lai, Cun-Hui Zhang, Zhiliang Ying","martin@stat.columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","7556","$0.00","The conference ""Imaging, Communications and Finance: Stochastic Modeling of Real-world Problems"" is scheduled to be held on June 24-25, 2011 at Columbia University in New York, NY. The purpose of the conference is to bring together scientists, from diverse research areas and career stages, in order to identify and discuss key emerging areas of interdisciplinary research where mathematical sciences promise to play an important role in the coming years. The diversity of the conference participants, which will include researchers from the fields of mathematics, statistics, radiology, engineering and finance, will help strengthen the connection between mathematical sciences and other science and engineering disciplines, as well as between established researchers and junior researchers in the start of their careers. This will have the effect of advancing knowledge both within and across different fields of research by sharing knowledge, identifying specific research problems of particular importance and helping to facilitate new collaborations between individuals and institutions.<br/><br/>Statistics and probability play an increasingly important role in a variety of interdisciplinary research areas including neuroscience, genetics, physics, finance and engineering. The goal of this conference is to focus on a number of important and emerging interdisciplinary areas of research, such as imaging, telecommunications and finance. Though the individual topic areas are diverse, a unifying theme for the conference is the use of probabilistic, combinatorial, and statistical analysis of models for problems arising in the real world. The conference will highlight important contributions already made through the use of statistics and probability, including the development of new reconstruction algorithms for medical imaging; stochastic models in risk analysis and finance; and methods for analyzing complicated network data arising from social, energy, traffic, communication, and computer problems. In addition, the conference will attempt to identify emerging issues where statistics and probability promise to play an important role in the future and help facilitate collaborations between researchers in these areas and the mathematical sciences."
"1104474","Saddlepoint and Bootstrap Accuracy with Applications to General Systems Theory","DMS","STATISTICS","07/01/2011","04/24/2013","Ronald Butler","TX","Southern Methodist University","Continuing Grant","Gabor Szekely","06/30/2015","$154,395.00","","rbutler@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1269","","$0.00","This project has two major goals: (a) to find explanations for the remarkable accuracy of saddlepoint approximations, and (b) to continue development of a framework for implementing nonparametric statistical inference in stochastic systems. Consideration of objective (a) uses two mathematical tools: the Ikehara-Weiner theorem, more commonly used in analytic number theory to prove the prime number theorem, and complex integration methods applied to inversion formulas of moment generating functions (MGFs). Both methods focus on the analytic continuation of MGFs and their properties outside of the convergence strip. These two tools will be used to streamline and extend what is known concerning the uniformly relative accuracy of saddlepoint approximations. Objective (b) continues previous work on the development of a framework for implementing bootstrap inference in stochastic models that are finite-state semi-Markov processes. These models include most of the commonly used stochastic models in reliability, multi-state survival analysis, epidemic modeling, and communication and manufacturing systems. Three tools are required to complete the framework: cofactor rules specifying the Laplace transforms for performance characteristics, saddlepoint approximations to invert these transforms, and the bootstrap to provide statistical inference in conjunction with the two previous tools.<br/> <br/>Modern statistical methods use models that involve complicated distributions from which the computation of probabilities can be a formidable task. This task is often simplified by using saddlepoint approximations. Such approximations generally provide probabilities with very little effort and most often achieve 2-3 significant digit accuracy. Explanations for this remarkable accuracy have continued to elude researchers. Part (a) of this proposal outlines two new approaches the investigator will consider to explain this accuracy. Among the modern methods that require probability computations from complicated distributions are the procedures the investigator considers in part (b) of the proposal. This work concerns the development of a framework for implementing nonparametric statistical inference in complex stochastic systems some of which began with the complex systems formulated in engineering during the cybernetics movement. These stochastic systems include most of the standard stochastic models used in reliability, multi-state survival analysis, epidemic modeling, and communication and manufacturing systems. No such general methodology currently exists for implementing statistical inference in the context of general stochastic systems models so the framework proposed by the investigator would provide tools that are currently unavailable. The proposal also addresses significant questions in other disciplines where answers are lacking due to certain computational difficulties. In ocean and electrical engineering accurate approximations are given for distributions of extreme hull stress during heavy seas and distributions for extreme responses in signal processing; in quantum physics, approximations are proposed for ""gauge"" functions, the computation of which are fundamental in quantum theory, and whose computation is difficult even in the simplest cases."
"1105191","Multidimensional Mixture Regression Models: Estimation and Inference","DMS","STATISTICS","08/01/2011","07/20/2011","Nicoleta Serban","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","07/31/2014","$100,000.00","","nicoleta.serban@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","","$0.00","The overarching objective of this proposal is to provide a unified framework for the study of a regression model described by a regression function that is a weighted sum of multi-dimensional unimodal functions. The multidimensional regression components are assumed similar in shape but identifiable through a set of parameters. The focus of the proposed research is to advance methodology that addresses fundamental statistical problems in estimation and inference of the proposed multidimensional mixture regression model.<br/><br/>The proposed statistical methodology will contribute to the field of biomolecular Nuclear Magnetic Resonance (NMR) studies, which will aid in the quantitative argumentation needed in discovery of biomelecule structures, but also to other applications such as identification and classification of lesions or tumor masses using breast computed tomography (CT) and identification and estimation of astronomical objects in images of the sky. The endpoint of the proposed research is stable protein structure predictions and determination of complex molecules using NMR technology along with more accurate detection of lesions or tumor masses using breast CT technology."
"1045153","EMSW21-RTG: Geometric, Topological and Statistical Methods for Analyzing Massive Datasets","DMS","APPLIED MATHEMATICS, TOPOLOGY, STATISTICS, COMPUTATIONAL MATHEMATICS, WORKFORCE IN THE MATHEMAT SCI","08/01/2011","06/24/2015","John Harer","NC","Duke University","Continuing Grant","Leland Jameson","07/31/2018","$1,839,327.00","Ingrid Daubechies, Scott Schmidler, Shayn Mukherjee, Mauro Maggioni, Paul Bendich","john.harer@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1266, 1267, 1269, 1271, 7335","7301, 9263","$0.00","In the past decade, the analysis of massive, high-dimensional, time-varying data sets has become a critical issue for a large number of scientists and engineers.  Observations across several disciplines, by researchers studying dramatically different problems, suggest the existence of geometrical and topological structures in many data sets, and much current research is devoted to modeling and exploiting these structures to aid in prediction and information extraction. Recent work by the investigators, among others, has shown that integrating statistical methodologies with ideas derived from computational topology and diffusion geometry often leads to strikingly superior results than by conventional means. The investigators now propose to bring these methods into the mathematics/statistics curriculum and departmental structure in a formal way, by establishing a vertically integrated program of undergraduate and graduate research and education. This activity has broad support from programs within the Division of Mathematical Sciences, including Applied Mathematics, Computational Mathematics, Statistics and Topology programs, as well as Division of Mathematical Sciences Workforce Program.<br/>This involves new undergraduate courses in the core theoretical areas, graduate topics courses, an extensive summer research program for undergraduates, as well as year long seminars aimed at both graduate and undergraduate students.  In addition, the investigators will disseminate their ideas via summer workshops aimed at small-college faculty, and methodology workshops directed to faculty from other large research institutions.<br/><br/><br/><br/>The need to analyze massive, complex data arises in a wide variety of scientific areas of national importance, including for example satellite image analysis, medical genomics, and internet security.<br/>The investigators propose a program of training future mathematical scientists to attack these new types of problems. The program will be vertically integrated, fostering extensive collaboration between post-docs, undergraduate and graduate students, and senior faculty, and will comprise a dynamic mixture of theoretical coursework and hands-on research activity. Participants in the program will gain valuable professional experience to distinguish them in the industrial and academic job-market."
"1107017","Spline-based Empirical Likelihood and Qausi-likelihood Estimation","DMS","STATISTICS","08/15/2011","08/13/2011","Jing Wang","IL","University of Illinois at Chicago","Standard Grant","Gabor Szekely","07/31/2014","$80,448.00","","jiwang12@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","","$0.00","Spline smoothing method contains substantial advantages for its simple implementation and fast computation. The method becomes one of the most prominent techniques in the area of semi-parametric and nonparametric regression modeling. The main objective of this proposal is to investigate the inferential aspects of two spline-based methods. One is the quasi-likelihood estimation for categorized response data in generalized regression, and the other the empirical likelihood estimation which brings efficiency properties analogous to parametric likelihood and retains distribution-free character of nonparametric procedures. Specifically, the PI proposes to i) develop robust estimation and testing procedures for generalized spline regression models; ii) employ the equivalence between linear mixed models and penalized splines for linearity tests in generalized additive models; iii) extend free-knots spline to generalized regression in order to improve the empirical behavior of polynomial spline estimators; and iv) investigate spline confidence region of linear coefficients in partially linear models via empirical likelihood by considering the number of constraints growing with the sample sizes.<br/><br/>The proposed projects are expected to be of broad interest to researchers from a wide range of applied and social science fields including biochemistry, biostatistics, epidemiology, and economics. For example, the research findings are applicable to a cancer research study for a dose-response relationship between ethanol and risk of cancer with binary outcomes, and a fauna study for relationship between the number of species on sea bed and the spatial coordinates where error distribution is not fully specified. The proposed procedures serve as new highly usable tools for curve estimation and model diagnosis in general regression model-related data analysis. For educational purpose, the PI plans to develop a new advanced topic course related with the proposed topics to mentor undergraduate or graduate students, and therefore involve them in proposed research and related projects."
"1107053","Collaborative Research: New directions in nonparametric inference on manifolds with applications to shapes and images","DMS","STATISTICS","07/01/2011","04/25/2013","Rabindra Bhattacharya","AZ","University of Arizona","Continuing Grant","Gabor Szekely","06/30/2015","$180,000.00","","rabi@math.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1269","","$0.00","Projective shapes of 3D configurations, not their Kendall type similarity shapes, are the most appropriate objects for general image analysis in machine vision and robotics. The present project will develop registration free nonparametric inference by constructing an appropriate equivariant embedding of the full projective shape manifold, and by providing two-sample and multi-sample inference procedures based on the extrinsic mean under the embedding. On the other hand, data analysis on size-and-shape reflection-similarity manifolds are important in virtual reconstructions of proteins and of various configurations of bone structures in humans. Another focus of the project is on certain special spaces which are not manifolds, but are spaces with manifold stratification, and which arise in many applications, e.g., in geometric representations of phylogenetic trees. Apart from the landmarks based shape analysis as described above, continuous shapes such as given by boundary contours in 2D will be investigated as elements of infinite dimemsional (Hilbert) manifolds. Finally, proposed nonparametric Bayesian procedures for density estimation, regression and classification on shape manifolds will be a significant point of departure from nonparametric inference based so far primarily on Fre'chet means and dispersions. Together these projects aim at providing comprehensive robust procedures for shapes which are of wide applicability in many fields of science and technology.<br/><br/>Digital images today play a vital role in science and technology, in intelligence gathering and defense, and in many aspects of everyday life. The present proposal seeks to advance the analysis of digital camera images via the statistical study of shapes and other non-Euclidean objects. Nonparametric statistical methods developed by the PIs and others over the past twelve years have had a significant impact on statistical inference for 3D scene recognition from regular digital cameras, on medical diagnostics, and on many other forms of image analysis. The proposal aims not only to consolidate this theory. The objective is also to develop new methodologies for machine vision and robotics, for dynamic scene recognition, for medical diagnostics from CT scans for planning reconstructive surgery for the severely injured, and for the detection of elusive health impairments from DNA sequences via shape configurations of proteins."
"1106535","Statistical Methodology and Applications to Economics, Engineering and Genetics","DMS","STATISTICS","07/01/2011","04/18/2013","Tze Lai","CA","Stanford University","Continuing Grant","Gabor Szekely","06/30/2015","$399,939.00","David Siegmund","lait@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","An important objective of the proposed research is to <br/>develop methods for gene mapping, i.e., the identification <br/>of genomic regions containing a gene (or genes) affecting <br/>a trait of interest in either humans or in experimental organisms. <br/>Searching a parameter set for the location of one or more  <br/>signals in a noisy background arise in gene mapping where the <br/>signals indicate the presence of a gene. Similar problems arise <br/>in brain mapping, astronomy, and bioinformatics. An important <br/>part of their solution involves the probability that a random field  <br/>exceeds a high threshold. A unified analytic approach and <br/>sequential Monte Carlo methods will be developed to evaluate <br/>these boundary crossing probabilities. Other methodological <br/>innovations of the proposed research are new dynamic empirical  <br/>Bayes models and methods, sequential surveillance procedures, <br/>and adaptive control schemes for input-output systems that may <br/>undergo occasional abrupt structural changes. <br/> <br/>The dynamic empirical Bayes approach under development has <br/>applications to insurance rate-making, dynamic panel data in <br/>economics, longitudinal data in biomedical studies, and risk <br/>management. Sequential surveillance and adaptive risk control <br/>are of timely relevance in the aftermath of the recent financial <br/>crisis and oil spill disaster. Sequential Monte Carlo methods have <br/>important applications to nonlinear filtering and to rare event <br/>simulation in communication networks and risk management. Gene <br/>mapping provides an important tool in the study of human diseases, <br/>and in agriculture and animal husbandry. The broader implications <br/>of the proposed research include (i) direct implications in genetics,  <br/>engineering, finance, insurance, risk management and surveillance,  <br/>and (ii) training of the next generation of scientists in academia,  <br/>industry, and government by developing new advanced courses and  <br/>involving graduate students in all phases of the research."
"1135257","MCQMC 2012","DMS","STATISTICS","09/15/2011","08/04/2011","Art Owen","CA","Stanford University","Standard Grant","Gabor Szekely","08/31/2012","$20,000.00","","owen@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","7556","$0.00","This project funds the travel expenses of US based researchers to the tenth International conference on Monte Carlo and quasi-Monte Carlo methods, to be held in Sydney Australia in February 2012.  The meeting will consider advanced methods for psuedo-random number generation, construction of low discrepancy point sets, and complexity of algorithms. There will be tutorial sessions on Monte Carlo, quasi-Monte Carlo and Markov chain Monte Carlo.<br/><br/>The Monte Carlo (MC) method is a computer based simulation using random number generators. The name was given by atomic researchers in the 1940s who likened their simulation methods to keeping score in a casino in order to learn some odds. Monte Carlo methods are used in every branch of science and engineering because they allow brute force computer power to be used on problems that are too complicated to solve mathematically. Quasi-Monte Carlo (QMC) methods replace simulated random numbers by strategically chosen ones. By leaving less to chance, large improvements in accuracy are possible. MC and QMC methods are widely applied in computer graphics to make animated movies and other images, in computational finance to control risks, in statistical inference to separate real findings from chance fluctuations, and in many other areas. Much of the top QMC work is done in Europe, Asia, Canada and Australia. This conference will bring together leading researchers from around the world to share results.  This project will support travel expenses of US based researchers to participate in this exchange of knowledge.  Two of the researchers to be supported are US experts giving major(plenary) talks. Of the other researchers to be supported priority will be given to academically young US researchers in the mathematical sciences, especially postdoctoral students and junior faculty, but also including PhD students."
"1106642","Collaborative Research: Objective Bayesian Model Selection and Estimation in High Dimensional Statistical Models","DMS","STATISTICS","10/01/2011","09/20/2011","Balakanapathy Rajaratnam","CA","Stanford University","Standard Grant","Gabor Szekely","09/30/2015","$99,191.00","","brajaratnam01@gmail.com","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","It is widely accepted that in many high dimensional situations, model selection has to be performed either before parameter estimation or simultaneously, in order to reduce the number of parameters under consideration. Indeed, model selection is one of the major challenges facing statisticians working with high dimensional data. Tools such as regularization and sparsity are some of the common notions employed to obtain parsimonious models to explain observed data. In recent years, the field of statistics has witnessed an explosion of frequentist and Bayesian methods for high dimensional problems. Despite these and other advances, Bayesian model selection in an ""objective"" sense in high dimensional problems remains an important problem that has yet to be solved satisfactorily. The need for objectivity translates into a need for specifying noninformative improper priors, which in turn renders the traditional Bayes factors unusable. The project proposes to derive objective Bayesian estimation and model selection procedures in a large class of high dimensional graphical models. The methodology that is proposed in this project therefore aims to contribute to much needed theory in the area of objective Bayesian model selection for high dimensional graphical models. In the process the methodology studies the benefits and shortcomings of objective Bayesian methods in this context. The theory that is developed feeds into developing algorithms and computational techniques for model selection/estimation in high dimensional settings.<br/><br/>The availability of throughput or high dimensional data has touched almost every field of science. The need to formulate correct models that explain observed high dimensional data permeates through many scientific fields. Indeed, such data where the number of variables is often much higher than the number of samples, referred to as the ""large p small n"" problem, is now more pervasive than it has ever been. Discovering statistical signals in high dimensional data, proposing correct models that can explain such data, and parameter estimation in these high dimensional settings are some of the major challenges that modern day statisticians have to contend with. Moreover, such challenges also feature in high stakes debates such as climate change, effectiveness of certain drugs in clinical trials, and relevance of various biomarkers in cancer studies. This project proposes to develop statistical methodology which is specifically targeted towards identifying models which explain high dimensional data in an objective manner. In particular the project is designed to develop better objective Bayesian model selection and parameter estimation methods in high dimensional problems, and has widespread applications. The PI and co-PI collaborate with scientists in applied fields, especially with faculty/researchers in their Medical Schools, Schools of Engineering and Environmental Sciences. Training of graduate students and mentoring is an integral part of this collaborative research. Scientific output from the project is intended for publication in high impact peer-reviewed journals."
"1106460","Asymptotic Inference for Locally Stationary Processes","DMS","STATISTICS","07/01/2011","04/18/2013","Michael Nussbaum","NY","Cornell University","Continuing Grant","Gabor Szekely","06/30/2015","$368,974.00","","nussbaum@math.cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","","$0.00","Stationarity is a crucial assumption in classical time series analysis. The theory of oscillatory spectra (Priestley, 1965) represents an attempt to overcome the resulting limitations for modeling nonstationary time series. However, it is the more flexible concept of locally stationary processes (Dahlhaus, 1993) which, by extending the theory of oscillatory spectra, provides a suitable framework for a general asymptotic theory of nonstationary processes. A fundamental characteristic of these processes is the time varying spectral density. While the literature on these models is already well developed, several questions of asymptotic inference remain open. Among them, two seem to stand out as most interesting: the possible asymptotic equivalence to a Gaussian white noise model, and the question of optimal exponential rates of large deviation type in testing and estimation problems. Since Le Cam developed the comparison of statistical experiments via their risk functions, many statistical models have been proved to be locally asymptotically normal, with the aim of establishing benchmarks for optimal procedures. For a parametric model of a time varying spectral density, local asymptotic normality has been established in the literature. However, for a better conceptual understanding of asymptotic inference, it is of interest to study the stronger property of asymptotic equivalence to a Gaussian white noise model, valid globally and over nonparametric function classes. As regards large deviation theory for locally stationary processes, some fragments are already available in the literature. A more fully developed theory can be envisaged, yielding not only testing results such as Stein's lemma and the Chernoff bound as special cases, but possibly also insights into the information geometry of these models based on the asymptotic Kullback-Leibler information.<br/><br/>Practitioners of statistics and data analysis very often assume that data show more or less similar behavior over different time periods, even when there is clear evidence to the contrary. For instance, this phenomenon can be observed in records of atmospheric turbulence, seismic signals from earthquakes, or speech signals analyzed in biological research. There is a need to develop more refined statistical methods for these cases. An ingenious theoretical solution to this problem has been proposed in the literature, based on the assumption that if data change over time, they often do not do so abruptly, but in a smooth way. This phenomenon is called local stationarity. If the series exhibits these ""smooth changes"", existing statistical methods can be adapted to smoothly change over time as well, considerably extending the scope of data analysis. The current proposal aims at a more thorough mathematical-statistical investigation of these locally stationary models. It also has a major educational component, as it is intended to accompany the collaboration between the principal investigator and a promising young scientist who has previously been supported with full tuition and stipend from the government of Mexico."
"1106956","Nonparametric Inference for Complex Physical Models","DMS","STATISTICS","08/15/2011","08/05/2011","Chad Schafer","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","07/31/2013","$100,000.00","Larry Wasserman, Christopher Genovese, Ann Lee, William Wood-Vasey","cschafer@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","The recent years have seen rapid growth in the depth, richness, and scope of<br/>scientific data, a trend that is likely to accelerate. At the same time, <br/>simulation and analytical models have sharpened to unprecedented detail<br/>the understanding of the processes that generate these data. But what has <br/>advanced more slowly is the methodology to efficiently combine the information <br/>from rich, massive data sets with the detailed, and often nonlinear, <br/>constraints of theory and simulations. This project will bridge that gap.<br/>The investigators develop, implement, and disseminate new statistical methods<br/>that can fully exploit the available data by adhering to the constraints <br/>imposed by current theoretical understanding. The central idea in the <br/>work is constructing sparse, possibly nonlinear, representations of both <br/>the data and the distributions for the data predicted by theory. These <br/>representations can then be transformed onto a common space to allow sharp <br/>inferences that respect the inherent geometry of the model. The methodology <br/>developed in this project will apply to a wide range of scientific problems.<br/>The investigators focus, however, on a critical challenge in astronomy: using <br/>observations of Type Ia supernovae to improve constraints on cosmological <br/>theories explaining the nature of dark energy, a significant, yet little-<br/>understood, component of the Universe.<br/><br/>Crucial scientific fields have enjoyed huge advances in the ability both to<br/>gather high-quality data and to understand the physical systems that generated <br/>these data. Nevertheless, the full societal and scientific value of <br/>this progress will only be realized with new, advanced statistical methods of<br/>analyzing the massive amounts of available data. The investigators develop <br/>statistical methods for combining theoretical modelling and observational evidence into <br/>improved understanding of these physical processes. The analysis of these data will <br/>requirenot only new methods, but also the use of high-performance computing resources. There is a particular need for these tools in cosmology and astronomy, and this project will bring together statisticians and astronomers to combine expertise, but this research is motivated by problems that are present in other fields, such as the climate sciences."
"1043903","EMSW21 - RTG: STATISTICS AND MACHINE LEARNING FOR SCIENTIFIC INFERENCE","DMS","STATISTICS, WORKFORCE IN THE MATHEMAT SCI","07/15/2011","07/01/2015","Robert Kass","PA","Carnegie-Mellon University","Continuing Grant","Gabor Szekely","06/30/2017","$2,250,982.00","William Eddy, Kathryn Roeder, Larry Wasserman, Christopher Genovese","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 7335","7301","$0.00","Statistics curricula have required excessive up-front investment in statistical theory, which many quantitatively-capable students in ``big science'' fields initially perceive to be unnecessary.  A research training program at Carnegie Mellon exposes students to cross-disciplinary research early, showing them the scientific importance of ideas from statistics and machine learning, and the intellectual depth of the subject. Graduate students receive instruction and mentored feedback on cross-disciplinary interaction, communication skills, and teaching. Postdoctoral fellows become productive researchers who understand the diverse roles and responsibilities they will face as faculty or members of a research laboratory.<br/><br/>The statistical needs of the scientific establishment are huge, and growing rapidly, making the current rate of workforce production dangerously inadequate.  The research training program in the Department of Statistics at Carnegie Mellon University trains undergraduates, graduate students, and postdoctoral fellows in an integrated environment that emphasizes the application of statistical and machine learning methods in scientific research. The program builds on existing connections with computational neuroscience, computational biology, and astrophysics.  Carnegie Mellon is recruiting students from a broad spectrum of quantitative disciplines, with emphasis on computer science.  Carnegie Mellon already has an unusually large undergraduate statistics program. New efforts will strengthen the training of these students, and attract additional highly capable students to be part of the pipeline entering the mathematical sciences."
"1106388","GOALI: Optimization of the marketing mix in the health care industry, with a view to reducing consumer costs","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","10/01/2011","09/27/2011","Dominique Haughton","MA","Bentley University","Standard Grant","Haiyan Cai","09/30/2012","$73,731.00","Eleanor Tipa","dhaughton@bentley.edu","175 FOREST ST","WALTHAM","MA","024524705","7818912660","MPS","1253, 1269","1504","$0.00","This project employs data mining techniques to model the return on investment from various types of promotional spending to market a drug, and then uses the model to draw conclusions on how the pharmaceutical industry might go about allocating marketing expenditures in a more efficient manner, thus reducing costs to the consumer. First, a model is built for the output variable (typically new prescriptions in a given time period) in terms of a number of relevant independent variables.  In this model building phase, attention is focused on issues such as the choice of the best set of variables to use in the model, as well as the best ways to measure predictive power, while taking full account of the time series nature of the data.  Techniques such as MARS and MART (Multiple Adaptive Regression Splines and Trees, respectively) are tested.  To handle the problem of correlated predictors, models are considered and compared that rely on partial least squares regression with or without the help of genetic algorithms or other algorithms to select the most predictive set of variables, as well as mixed models (with fixed and random effects). Extensions of the ridge regression method such as LASSO (Least Absolute Shrinkage and Selection Operator) and LARS (Least Angle Regression laSso) are tested. Directed Acyclic Graphs are employed to help unravel direct and indirect effects of predictors on new prescriptions. Another approach to be tested is that of using propensity score methods to improve on the industry practice of estimating effects of various marketing variables with matched samples.  Once built, the model is used to evaluate the contribution of each marketing activity to the new prescriptions. Once these contributions have been ascertained, simulations follow to test the effect of changes in the modeling mix on the expected prescription volume.  Linear or quadratic programming is then put in place to propose an optimal marketing mix, using as an objective function the equation obtained from the model.  Actionable recommendations can then be given to the pharmaceutical industry on how to achieve savings from a better optimized marketing mix.   <br/><br/>To summarize, the project proceeds in three phases. 1) A model is built for the number of new prescriptions to a drug in a given time period in terms of a number of relevant predictors, such as for example spending on promotional samples, or spending on journal advertising. 2) The model is then used to evaluate the contribution of each marketing activity to the new prescriptions and to define an optimal marketing mix. 3) Actionable recommendations to the pharmaceutical industry are then derived on how to achieve savings from a better optimized marketing mix. The project relies on strong synergies between the PI at a business university and a corporate co-PI with years of experience providing actionable database marketing advice to clients. The project will also provide valuable corporate exposure to a PhD student. Results from the project are expected to help lower the cost of drugs to the consumer and more generally to help control health care costs."
"1106516","Bayesian Models and Methods for Dynamic and Spatio-Dynamic Systems","DMS","STATISTICS","07/01/2011","04/22/2013","Mike West","NC","Duke University","Continuing Grant","Gabor Szekely","12/31/2014","$400,000.00","","Mike.West@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","","$0.00","The research concerns novel statistical theory, models and methods for structured dynamic and spatio-dynamic multivariate processes. The scope includes theoretical developments of new classes of stochastic process models for dynamic covariancestructures in multi- and matrix-variate time series, including novel classes of stationary Markov processes for multivariate volatility modeling. Theoretical and applied developments include new approaches to sparsity modeling for increasingly high-dimensional, time-varying parameter stochastic systems, applying to dynamic regression, time-varying vector auto-regression, dynamic factor models and covariance volatility models. Additional research focuses on new classes of spatial lattice and spatially-varying random field models, coupled with time series processes to define flexible models of spatio-temporal models for increasingly high-resolution lattice data observed through time. The investigator develops Bayesian simulation-based statistical computation-- including GPU-based parallelized algorithms-- for model implementations, and cross-disciplinary applications in financial time series as well as studies in atmospheric and biomedical sciences.<br/><br/>Faced with increasingly high-dimensional data sets generated in studies of temporal and spatial systems, statistical science research aims to substantially advance the ability to represent, analyze and use mathematical models of increasing dimension, realism and complexity. The investigator develops mathematical and statistical modelling theory and associated simulation-based computational methods for a range of contexts, motivated in part by collaborative cross-disciplinary applications in areas of finance, atmospheric science and the neurosciences. Innovations in statistical research include: (i) new and improved models for describing and predicting change in time of the complex patterns of relationships among several or many time series-- such as financial indicators, or nano-technology based recordings of neural signals in brain imaging; (ii) new theory and methods for inducing sparsity-- i.e., controlling complexity-- of applied stochastic models, to enable scaling to increasingly high-dimensional problems, such as arise in  high-resolution satellite imaging in atmospheric studies as well as large-scale financial time series;(iii) innovations in simulation techniques for statistical computing, including parallel desktop computing, to advance the ability to fit, explore and use models of increasing scale and complexity and with increasingly large data sets. With cross-disciplinary collaborators and students, the research advances core mathematical and statistical modeling theory and technology, and contributes new, refined and relevant approaches to modeling and dataanalysis in several specific applied contexts as well as generating methods for broader use."
"1106891","Advances in Bayesian Model Choice","DMS","STATISTICS","07/01/2011","07/28/2013","Merlise Clyde","NC","Duke University","Continuing Grant","Gabor Szekely","06/30/2015","$250,000.00","","clyde@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","","$0.00","The investigator considers new default prior distributions for<br/>Bayesian model selection and model averaging in linear and generalized<br/>linear models.  Prior choice for model specific parameters and for<br/>prior model probabilities is of critical importance, particularly for<br/>modeling high dimensional relationships where subjective<br/>specifications are impractical or where an ``objective'' analysis is<br/>desired.  New families of objective priors that have desirable risk<br/>properties, adapt to unknown degrees of sparsity, and also permit<br/>tractable computations for large scale model search are studied. In<br/>high dimensional problems, the dimension of the model space is<br/>astronomical. Innovative methodology and algorithms for large scale<br/>stochastic search and model averaging for high dimensional model<br/>spaces are developed, with an emphasis on algorithms that exploit the<br/>architecture of Graphical Processing Units (GPUs).  GPUs provide the<br/>computing power of a distributed cluster, but at a fraction of their<br/>cost and space/cooling requirements, but require care in the<br/>development of statistical algorithms that take advantage of their<br/>architecture.<br/><br/>Advances in technology have led to the collection of high dimensional<br/>data structures, spawning an increasingly complex array of statistical<br/>models for data. Goals of data analysis may include prediction and/or<br/>selection of a subset of models to test particular theories or to<br/>reduce attention from many speculative models to a few well chosen<br/>models; these are fundamental problems in statistics and throughout<br/>the sciences. In such settings, model uncertainty is ubiquitous.<br/>Bayesian methods offer an effective and conceptually appealing<br/>approach for addressing model uncertainty through Bayesian model<br/>averaging, incorporating both parameter and model uncertainty, while<br/>still permitting selection of a model via coherent decision-theoretic<br/>principles.  The methodological developments are driven by issues that<br/>arise from the following applications 1) identifying important factors<br/>to predict protein activity; 2) predicting risk of international<br/>conflict; 3) health effects of criteria pollutants; and 4) identifying<br/>single nucleotide polymorphisms that are associated with ovarian<br/>cancer risk.  As prediction and model selection are some of<br/>the most fundamental and widespread problems in the sciences and<br/>beyond, the project's impact extends beyond the applications listed<br/>above.  The development and distribution of software ensures that<br/>these methods are widely available."
"1106817","Collaborative Research:  Statistical Methods for Analyzing Complexity and Growth of Large Biological and Information Networks","DMS","STATISTICS","07/01/2011","06/30/2011","David Banks","NC","Duke University","Standard Grant","Gabor Szekely","06/30/2013","$90,000.00","","banks@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","","$0.00","Every network dataset poses unique challenges, but there is a growing toolkit of methods that can be adapted to specific situations. This research will extend that toolkit by continuing the development of machine learning ideas that perform aggressive local variable selection to fit local metrics, thus allowing nearby nodes to have similar models for edge formation, but distant nodes to have very different models.  Also, the research will provide a new approach to goodness-of-fit assessment for network models, based upon minimum description length inference.<br/><br/>Network modeling has emerged as a critical methodology across many fields of science, including biochemistry, sociology, and Internet communication.  Important applications include social interactions leading to fission in baboon troops, biochemical knowledge derived from protein-protein interactions, and insight into the growth and structure of the Wikipedia.  This research will develop novel statistical models for network growth and new ways to assess how well they explain a given dataset."
"1106691","Multivariate Depth and Quantile Functions: Foundations and Applications","DMS","STATISTICS","09/01/2011","07/19/2014","Robert Serfling","TX","University of Texas at Dallas","Continuing Grant","Gabor Szekely","08/31/2015","$299,452.00","","serfling@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1269","","$0.00","High dimension and/or complexity is now standard in applications of statistical data analysis, and typically data now is multivariate. Advances in computational resources make it feasible to implement quite sophisticated methods. This supports the development of powerful approaches that systematically take into account the special geometric features intrinsic to multivariate data sets. Especially important is the setting of nonparametric multivariate methods. This, of course, presents conceptual challenges. In particular, multivariate depth and quantile functions now provide a major approach that has become well-established in recent years but also is in active further development. In this project, the PI addresses significant open issues and directions in both the foundations and the applications of this approach. The latter inspire the former, and the former yields tools for the latter. This project advances core statistical science by developing useful extended foundations and underpinnings for multivariate depth and quantile functions. The results have even wider application and broadly enhance the role of statistical science in applications, permitting new kinds of problems to be treated more meaningfully and more powerfully. Central themes of the project are: I. Transformations to Produce Equivariance and Invariance of Statistical Procedures, II. Spatial Depth-Based Trimming to Produce Robustness without Undue Computational Burden, and III. Development and Exploitation of a New Synergy Between Depth Function Methods and Level Set Methods for Treating Contours. Topic I provides tools for the modification of statistical procedures so that they acquire desired certain equivariance or invariance properties that may not hold otherwise. Topic II investigates recent solutions to two related but different problems: (i) robustification of the spatial quantile and outlyingness functions, and (ii) simultaneously computationally easy, robust, and affine equivariant scatter estimators. Topic III investigates a promising but hitherto unexplored synergy between depth function methods on one hand and level set methods on the other. Besides these major thrusts, the project also addresses formulation of multivariate L-statistics, systematic exploration of a depth-outlyingness-quantile-rank paradigm, studies on integrated data depth, and studies on depth methods in functional data analysis. As a whole, the project is intended to have transformative impacts on modern approaches to data handling through statistical science.<br/><br/>Statistical data analysis and modeling now accommodates pressing new arenas of application involving data that is multivariate, using many variables taken together. All areas of science, engineering, government, and industry now routinely involve multivariate data, typically complex in structure and high in number of variables. The three key technical thrusts of this project address important and timely concerns arising in dealing with multivariate data. For example, in dealing with outliers in multivariate data, we need the classification of which points are outliers not to change simply when there is a simple change of coordinate system, such as metric to British. Also, for example, the contours that delineate the middle 50% or 75% or 90% of a data set should be determined efficiently and accurately without undue interference from extreme outlying data points not central to the data. Or, for example, when striking geometric features or patterns are discovered as in data mining, it is necessary to determine whether such findings are genuine features inherently meaningful or whether they are simply artifacts of the particular coordinate system that has been adopted and to be ignored. Another key effort of this project is to develop a new framework that brings together two different but related methodologies in multivariate analysis that have been recently developed independently (level sets and depth functions) and enables them to be applied together in a coordinated manner. This strengthens the understanding and the roles of these methodologies in their particular domains of application. The project also contributes to education and development of human resources in statistical science by involving graduate students and undergraduate students. Cross-stimulation among project participants, which may include visiting researchers, is achieved through regular meetings and a team approach. The participation of underrepresented groups and junior faculty and professionals is encouraged and fostered by the PI. The results and findings of the project are disseminated by the PI through high-profile conference presentations, journal publications, website postings, and introduction into the graduate curriculum."
"1107029","Collaborative Research: Penalization Methods for Screening, Variable Selection and Dimension Reduction in High-dimensional Regression via Multiple Index Models","DMS","STATISTICS","06/15/2011","06/06/2011","Peng Zeng","AL","Auburn University","Standard Grant","Gabor Szekely","05/31/2014","$100,000.00","","zengpen@auburn.edu","321-A INGRAM HALL","AUBURN","AL","368490001","3348444438","MPS","1269","9150","$0.00","The project aims to develop effective penalization methods for screening, dimension reduction, and variable selection in high dimensional regression. The investigators focus mainly on multiple index models, because this type of models combines the strengths of linear and nonparametric regression while avoiding their drawbacks. A novel penalization approach is employed for model fitting, which regularizes both the parametric and nonparametric components of a multiple index model.  A pilot study shows that this approach is more advantageous than other existing ones. When facing ultra-high dimensionality, the investigators use a forward variable screening procedure to reduce the dimension to a manageable size before applying the proposed penalization. The investigators plan to study the theoretical properties of this approach and develop fast and efficient computing algorithms for its implementation. The proposed approach is further extended to applications involving categorical responses or random effects.<br/><br/>Advances in science and technology have led to an explosive growth of massive data across a variety of areas such as bioinformatics, climate research, internet, etc. Traditional statistical methods for clustering, regression and classification become ineffective when dealing with a large number of variables. Lately, a tremendous amount of research effort has been dedicated to the development of statistical methods such as dimension reduction and variable selection for analyzing this type of massive data. The investigators join the effort by proposing a novel penalization approach and developing efficient computing algorithms. The results from this project not only advance statistical research but also help other scientists and researchers better understand and analyze their massive data and hence enhance their scientific discovery."
"1106789","Statistical Techniques for Regression Analysis of Censored Data","DMS","STATISTICS","09/01/2011","08/12/2011","Jianguo (Tony) Sun","MO","University of Missouri-Columbia","Standard Grant","Gabor Szekely","08/31/2015","$160,000.00","","sunj@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269","","$0.00","The main goal of this research project is to develop efficient statistical techniques for regression analysis of time-to-event data with complex censoring such as current status data or interval-censored data.  Specifically, the investigator will study three types of problems that often occur in medical and public health researches.  One is to develop efficient score function-based estimation procedures for univariate current status or interval-censored data that apply to more general situations and give practitioners more flexibilities in analyzing these data.  The second is to develop efficient score function-based estimation procedures for bivariate current status or interval-censored data and the third is to develop efficient sieve maximum likelihood estimation procedures for bivariate current status or interval-censored data. The second and the third parts will investigate the same problem but employ different modeling and inference strategy to allow one to focus on different aspects of the data analysis or different questions.<br/><br/>The results of this study will advance not only the theoretical research in statistics pertaining to survival analysis but also provide statistical methods for data analysis in a wide range of scientific disciplines including  public health, medicine, demographics, economics and psychology."
"1106717","Bayesian Variable Selection and Grouping","DMS","STATISTICS","08/15/2011","01/07/2014","Sounak Chakraborty","MO","University of Missouri-Columbia","Standard Grant","Gabor Szekely","07/31/2015","$121,279.00","Fei Liu","chakrabortys@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269","","$0.00","The recent advancements in science and technology present a huge challenge of handling large amount of data. Often the main objective is to retain the relevant features or covariates in the data and to filter out the redundant variables. In the statistical framework, this is known as variable selection. The problem becomes extremely difficult when there are a large number of covariates in comparison to the available sample size. Despite the great deal of research effort on variable selection, knowledge on modeling the dependence between the important variables is very limited and urgently needed in many fields. Penalization based techniques like Lasso can be used for variable selection purpose, but it cannot detect any grouping or dependency structure between the covariates. While methods like Elastic-Net and Octagonal Shrinkage and Clustering Algorithm for Regression (OSCAR) may be used to incorporate grouping, one apparent major drawback for such methods is that they are not based on any probabilistic framework. The project aims to develop probabilistic models or priors incorporating the dependency relationship, for simultaneous variable selection and grouping of closely related variables. The developed model will automate the process as much as possible using highly flexible Bayesian models characterized by a special dependency structure. The special dependency structure is formulated through an extension of the Laplace matrices of graphs (graph Laplacian) used in the machine learning and pattern recognition literature for finding good clusters. The proposed prior distribution for the graph Laplacian allows conjugacy and thereby greatly simplifies the computation. The graph Laplacian prior proposed in this research is very useful for small and moderately high dimension data sets. For data sets with a massive number of predictors, explicit modeling of the pair wise dependence through graph Laplacian is infeasible. Therefore, another goal in this research is to build a coherent Bayesian model which is capable of reducing the dimension and at the same time detecting the clusters of the nonzero coefficients through the graph Laplacian prior formulation (on the reduced dimension data set) for very high dimensional data sets. The proposed Bayesian Variable Selection and Grouping methods would be developed under continuous response data, as well as binary and count response data framework.<br/><br/>Due to enormous progress of computer technology, explosion of the internet based information, and emerging fields in biological sciences, high-dimensional complex data sets are now very common in our life. The first big challenge handling such data sets is to identify important features or covariates in them that are directly related and most important to the desired response or outcomes. In statistics this is commonly referred to as variable selection. This is the first objective of this project. Second goal of this project is to find any grouping pattern among the selected variables and enhance the understanding of how the features or covariates are related among themselves. The investigator proposes a new methodological framework to address these challenges. To account for any related uncertainty the proposed methodology is based on probabilistic or Bayesian framework. The practical implementation of the proposed models is done by developing fast computer algorithms, which are able to handle data sets of any size. The proposed work has enormous potential for real life applications, especially in the field of computer science, engineering, genetics, marketing research, and medicine. For example, the methods can be used to detect active genes and study the relationship between those active genes in biology. Another example arises in marketing segmentation for targeting a smaller market and helping the decision makers to effectively reach all customers. The principal investigator will distribute freely available and easy to use software along with a short tutorial, which will allow researchers from other disciplines to address their own scientific questions using the proposed methods. Based on the ideas developed in this project the principal investigator will develop new graduate courses, enhance the existing courses, and actively involve students in the research. This will train the current generation of students to deal with future challenges."
"1106935","Collaborative Research: New Directions in  Nonparametric Inference on Manifolds with Applications to Shapes and Images","DMS","STATISTICS","07/01/2011","06/15/2015","Victor Patrangenaru","FL","Florida State University","Continuing Grant","Gabor Szekely","06/30/2017","$197,037.00","","vic@stat.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","","$0.00","Projective shapes of 3D configurations, not their Kendall type similarity shapes, are the most appropriate objects for general image analysis in machine vision and robotics. The present project will develop registration free nonparametric inference by constructing an appropriate equivariant embedding of the full projective shape manifold, and by providing two-sample and multi-sample inference procedures based on the extrinsic mean under the embedding. On the other hand, data analysis on size-and-shape reflection-similarity manifolds are important in virtual reconstructions of proteins and of various configurations of bone structures in humans. Another focus of the project is on certain special spaces which are not manifolds, but are spaces with manifold stratification, and which arise in many applications, e.g., in geometric representations of phylogenetic trees. Apart from the landmarks based shape analysis as described above, continuous shapes such as given by boundary contours in 2D will be investigated as elements of infinite dimemsional (Hilbert) manifolds. Finally, proposed nonparametric Bayesian procedures for density estimation, regression and classification on shape manifolds will be a significant point of departure from nonparametric inference based so far primarily on Fre'chet means and dispersions. Together these projects aim at providing comprehensive robust procedures for shapes which are of wide applicability in many fields of science and technology.<br/><br/>Digital images today play a vital role in science and technology, in intelligence gathering and defense, and in many aspects of everyday life. The present proposal seeks to advance the analysis of digital camera images via the statistical study of shapes and other non-Euclidean objects. Nonparametric statistical methods developed by the PIs and others over the past twelve years have had a significant impact on statistical inference for 3D scene recognition from regular digital cameras, on medical diagnostics, and on many other forms of image analysis. The proposal aims not only to consolidate this theory. The objective is also to develop new methodologies for machine vision and robotics, for dynamic scene recognition, for medical diagnostics from CT scans for planning reconstructive surgery for the severely injured, and for the detection of elusive health impairments from DNA sequences via shape configurations of proteins."
"1104830","Theory and Applications of Random Forests","DMS","STATISTICS","07/01/2011","05/20/2011","Hemant Ishwaran","OH","Cleveland Clinic Foundation","Continuing Grant","Gabor Szekely","10/31/2011","$63,874.00","","HIshwaran@med.miami.edu","9500 EUCLID AVE","CLEVELAND","OH","441950001","2164456440","MPS","1269","","$0.00","This research develops theory for random forests specifically for the purpose of better facilitating its use in practical settings. Theoretical considerations include balancedness, subtrees, node distributions, node splitting, depth of variables, and other novel tree concepts.  These concepts are used to improve prediction and variable selection for random forests in both high and low-dimensional problems.  <br/><br/>One of the simplest techniques for improving the performance of a statistical method such as a tree is to take its average over multiple instances of the data. This averaging process is often referred to as ensemble learning and has attracted considerable attention as it has been widely observed that combining elementary learners can yield a predictor with superior prediction performance. One of the most successful tree ensemble learners is random forests.  Random forests has met with considerable empirical success, yet much is still unknown about it.  This research seeks to improve our understanding of random forests and utilize this knowledge to enhance its application in practical settings.  This research focuses on cardiovascular disease, the number one cause of death in the developed world, cancer staging and prognostication for cancer patients, and identifying and developing genotype signatures for myelodsyplastic syndromes, a heterogeneous diseases of blood stem cells having no current curative medical therapy."
"1053987","CAREER: Large Scale Stochastic Optimization and Statistics","DMS","STATISTICS","07/01/2011","07/01/2014","Philippe Rigollet","NJ","Princeton University","Continuing Grant","Gabor Szekely","05/31/2015","$296,881.00","","rigollet@math.mit.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","1045","$0.00","Stochastic optimization offers a general framework to study many fundamental statistical problems related to prediction such as regression, classification and density estimation. Furthermore, it is a natural framework to import powerful algorithms from numerical optimization, especially for large scale problems. The broad goal of this project is to understand the fundamental interactions between statistics and stochastic optimization. To accomplish this task the investigator (a) identifies new problems from statistics, especially with complex structure, that can be recast as stochastic optimization problems; (b) develops new algorithms that optimally and efficiently solve large scale problems; (c) determines essential characteristics of the problems that govern the performance of algorithms and their fundamental limitations; and (d) explores peripheral problems of stochastic optimization including stochastic optimization with stochastic constraints and stochastic optimization with limited feedback. <br/><br/>The information era has witnessed an explosion in the collection of data and large scale data sets are ubiquitous in a wide range of applications including biology, networks, environmental science, sociology and marketing. This results in an acute need of new statistical methods to analyze these data sets of unprecedented size. While techniques from numerical optimization can be used in several scenarios, their analysis remains largely dissociated from that of the statistical task at hand. This research aims at providing a unified treatment of a number of large scale problems emerging from statistical learning and from optimization under uncertainty in general. Therefore, the project will not only result in new and effective algorithms, but also in a novel theoretical framework that supports the analysis of stochastic optimization problems and enables further improvements of said algorithms."
"1136920","2012 Joint Statistical Meetings Diversity Workshop & Mentoring Program","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","08/15/2011","05/31/2012","Brian Millen","VA","American Statistical Association","Standard Grant","Jennifer Pearl","12/31/2013","$20,000.00","Keith Crank, Marcia Gumpertz, Rebecca Nichols","Millen.2@osu.edu","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1260, 1269","7556","$0.00","The 2012 Joint Statistical Meetings Diversity Workshop and Mentoring Program is a multi-day activity which will take place during the 2012 Joint Statistical Meetings (JSM) in San Diego.  It will consist of a series of interactive sessions and small group mentoring activities which aim to <br/><br/>1. Establish critical mentoring and networking relationships for minority statisticians at early- to mid-career levels (i.e., undergraduate students, graduate students, early statistics professionals);<br/>2. Motivate students to pursue graduate study and careers in statistics;<br/>3. Share best practices for recruiting and mentoring minority students and faculty; and<br/>4. Increase the active participation of minorities in the American Statistical Association.<br/><br/>Participants in the workshop and mentoring program include underrepresented minority graduate students in statistics or biostatistics programs; top minority undergraduates in statistics, mathematics, or related disciplines; minority statisticians in academia, government, and the private sector; key faculty from minority-serving institutions who advise and mentor undergraduates in math or related disciplines; and faculty influential in the faculty and/or student recruitment processes at their home institutions.<br/><br/>This program is unique in that it does not limit its focus to students only, but also provides opportunities to benefit early career professionals.  It is further unique in its holistic approach to career development which allows individuals to participate in the program as both mentor and protégé simultaneously.  These elements help create a community of support and accountability for the success of all involved.<br/><br/>This awarded is supported jointly by the Infrastructure Program and the Statistics Program within the Division of Mathematics."
"1125662","A NISS/ASA Writing Workshop for New Researchers","DMS","STATISTICS","07/01/2011","05/31/2012","Rebecca Nichols","VA","American Statistical Association","Standard Grant","Gabor Szekely","06/30/2012","$20,000.00","","rebecca@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","7556","$0.00","This award will support a workshop on effective technical writing for new researchers in the statistical sciences who seek to publish their research or to present their research plans in the form of grant proposals for federal funding. Researchers, especially new researchers, often have difficulty disseminating their research results not because of the quality of the research but rather because of inappropriate choices of publication venues for the particular research and/or because of poor presentation of technical material to the chosen audience. The National Institute of Statistical Sciences and the American Statistical Association will manage the Workshop. The workshop will be held at the Joint Statistical Meetings in Miami Beach, FL, July 31 - August 3, 2011.<br/><br/>This workshop will open with tutorial sessions on the organization of material for a technical article or grant application, on technical writing techniques and on the specific missions and audiences of key journals in the statistical sciences. Then each participating new researcher will work individually with an experienced journal editor as mentor to address these issues on an individualized basis for draft of the new researcher's work in progress. Revisions following this guidance will be critiqued by the mentor to assure that the new researcher's implementation of writing techniques has been successful before the article or the grant proposal is submitted for review. The workshop will be held at the Joint Statistical Meetings in Miami Beach, FL, July 31 - August 3, 2011."
"1114922","CASE STUDIES IN BAYESIAN STATISTICS AND MACHINE LEARNING","DMS","STATISTICS","07/01/2011","02/25/2011","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Haiyan Cai","06/30/2012","$15,000.00","","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","7556","$0.00","Case Studies in Bayesian Statistics and Machine Learning continues the tradition of  the workshop series Case Studies in Bayesian Statistics with a meeting October 14-15, 2011. The usual format for meetings and workshops in statistics and computer science emphasize methods over applications, which often stifles discussion about the impact of the methods on the substantive problem. The unique format of the case studies workshop series has allowed for substantive discussion of application-specific issues, most importantly a narrative of how the substantive scientific problem demanded either new methods or the novel application of existing approaches, the obstacles in the real problem that the researchers encountered, and the solutions that resulted.  <br/> <br/>Statistics and machine learning provide essential methodologies throughout the sciences, yet the connection between the science and the data analytic problem formulation is rarely emphasized in traditional conferences. This workshop fosters cross-pollination of ideas from the statistics  and computer science ommunities  and promotes work that takes on important challenges in scientific investigation.  The case studies highlight the way novel application of statistical machine learning methods are used to answer a scientific question. The workshop especially supports efforts by young investigators, in part by including a session of presentations exclusively by younger investigators."
"1104409","RUI - New Statistical Methods for Modeling 3-Dimensional Rotations with Advances in the Study of Human Motion","DMS","STATISTICS","07/01/2011","05/26/2011","Melissa Bingham","WI","University of Wisconsin-La Crosse","Standard Grant","Gabor Szekely","06/30/2016","$121,900.00","","bingham.meli@uwlax.edu","1725 STATE ST","LA CROSSE","WI","546013742","6087858007","MPS","1269","9229","$0.00","This research project focuses on the development of statistical methodology for 3-dimensional rotation data. As skeletal mammals move, their bones rotate around various joints, making data in the form of 3-dimensional rotations common in the study of biomechanics and human motion. While the investigator's previous work has made advances in modeling 3-dimensional orientations, there are many types of statistical inference that have not yet been studied for rotation data. This project models data from the study of human motion by developing (1) a nonsymmetric class of distributions, (2) a median estimator, (3) nonparametric methods, and (4) clustering methods for 3-dimensional rotations. The new statistical inference techniques produced provide answers to open questions in the study of biomechanics and human motion.<br/> <br/>As the statistical inference methods for 3-dimensional rotation data developed through this research project are used in the study of human motion, they will lend solutions to scientific problems in physical therapy.  Thus, this project promotes interdisciplinary collaboration between the mathematics and physical therapy departments at a predominantly undergraduate university.  Additionally, this project includes the mentoring of undergraduate student research experiences, exposing students to interdisciplinary research collaboration at an early stage in their education.  While the statistical methodology developed by this project is used primarily to answer open questions in the study of human motion, it has the potential to impact other areas where 3-dimensional rotation data arise, such as vectorcardiography and materials science."
"1107084","Distribution of Patterns and Statistics in Random Sequences","DMS","STATISTICS","08/01/2011","07/20/2011","Donald Martin","NC","North Carolina State University","Standard Grant","Gabor Szekely","09/30/2015","$100,000.00","","demarti4@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","","$0.00","This project targets the development and application of tools for efficient computation of distributions associated with patterns and statistics in random sequences, both realizations of observation sequences as well as hidden state sequences conditional on observed data.  Due to the massive size of data sets that are available, computational methods that are not only accurate but also efficient are important.  The proposed research seeks to contribute in this area.  Minimal deterministic finite automata, probability generating functions, the sum-product algorithm and matrix-vector updates are the primary tools used to form algorithms for efficiently computing distributions of patterns and statistics.  The goals of this research are threefold: (i) to further develop efficient methods for quantifying uncertainty in statistics of hidden state sequences of probabilistic graphical models; (ii) to efficiently compute exact probability distributions of complex patterns that have not previously been computed; (iii) to apply the probabilistic tools of (i) and (ii) to statistical tests and data analysis.  The quantification of uncertainty in statistics of hidden states is frequently dealt with by determining the state sequence that is optimal for a particular criterion, with the statistic then evaluated from the optimal state sequence in a deterministic fashion.  However, that approach does not account for uncertainty in the states.  An alternate approach is to sample from the conditional distribution of states given the observed data, and then approximate the distribution of the statistic empirically.  However, many samples are needed so that the approximation is accurate, leading to problems with scalability.  We give a way to compute exact distributions in an efficient manner.  The goals of the project are integrated, in that distributions of patterns and statistics are needed for statistical inference in applications, and in turn those applications drive the need for computing distributions in increasingly complex situations.  Objectives of the work include computing a model-based distribution of prediction error rates for protein-protein interactions, computing exact distributions of coverage of spaced seeds for homology searches in DNA sequences, and computing the exact distribution of the one-dimensional scan statistic for multi-state higher-order Markovian trials.               <br/>          <br/>The need for distributional properties associated with patterns and statistics in random sequences arises in many practical fields of study with massive data sets, such as bioinformatics, time series, information theory, economics, data mining, and quality control.  In this research computational tools are developed for computing such distributions.  Results for distributions of patterns and statistics may be applied to many practical problems, such as detecting genes, promoters, or other functionally significant patterns in DNA sequences, determining probabilities related to classifications of observations in health-related studies, change points that indicate new regimes in economic data, patterns that indicate an intrusion in a computer system, or patterns associated with surveillance work.  The theory will be used to compute distributions of statistics that are intractable by combinatorial or other means, and to provide exact probabilities in an efficient manner in situations that are typically handled by simulation of many data sets.  Thus this research facilitates new scientific studies that rely on results for distributions associated with patterns or statistics that have not been computed to date."
"1106975","Exploration, Modeling and Inference for Complex Data Objects","DMS","STATISTICS","09/01/2011","05/23/2011","Haonan Wang","CO","Colorado State University","Standard Grant","Gabor Szekely","08/31/2015","$159,398.00","","wanghn@stat.colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1269","","$0.00","This proposal aims to develop new statistical learning tools geared towards the challenging problem of understanding population level variation, extracting features and gaining knowledge from a set of complex data objects. Object Oriented Data Analysis (OODA) is an outgrowth of Functional Data Analysis, in which the basic elements of data analysis are curves. The basic elements of OODA are complex data objects including tree-structured objects. In medical image analysis, tree-structured objects are found to be efficient for data representation when the focus of the medical study involves variation in branching structures. The proposed work is driven by a data set of human brain artery systems and will clearly have an impact on many other scientific fields involving populations of tree-structured objects. Analysis of complex data objects, such as trees, general graphs, networks and shapes, poses serious challenges towards methodological development since traditional statistical models for multivariate data and functional data rely on linear operations in Euclidean spaces or vector spaces. Thus, it requires the development of novel and nontraditional techniques in a whole new statistical paradigm for extracting patterns and information from data objects. The proposed work is targeted to address some fundamental issues, including one-dimensional representation in tree space. The first goal of this project is to provide tools for data exploration and summarization. Next, the investigator will study probability distributions (mixture models), which can be used as the basis of statistical inference. The investigator will further study modeling of tree-structured data to explain the relationship between tree-structured covariates and numerical response, and/or between numerical covariates and tree-structured response. Kernel based methods and logistic regression will be implemented for classification in tree spaces.<br/><br/>Highly sophisticated data collection processes in science and technology from the last two decades motivate the study of complex data objects. The proposed work will open up a new area of statistical research, lay down a foundation and enrich the toolkit available for the analysis of object oriented data. The investigator will continue to implement newly developed modeling procedures to the human brain artery data, and help to improve existing brain tumor diagnosis procedure. This will also have a major impact on object oriented data analysis by developing interdisciplinary research among various scientific fields. It is expected that the ideas and methods resulting from this proposal will go beyond the motivating example of analyzing human brain artery data, and will provide researchers deeper insights in the discipline where the data were collected."
"1106912","Principal Component Analysis and Its Regularization","DMS","STATISTICS","08/15/2011","08/05/2011","Haipeng Shen","NC","University of North Carolina at Chapel Hill","Standard Grant","Gabor Szekely","07/31/2015","$250,000.00","","haipeng@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","The proposed research concerns Principal Component Analysis (PCA) and its various regularizations: Sparse PCA and Functional PCA.  The investigator proposes five research projects to address major challenges in these areas. PCA is a ubiquitous multivariate analysis technique for dimension reduction. Regularization of PCA becomes essential for large dimensionality, especially in the unconventional ``High Dimension-Low Sample Size'' (HDLSS) setting. HDLSS has become a common feature of data encountered in many divergent fields such as medical imaging and microarray analysis, but is outside of the domain of classical multivariate analysis. The first project studies asymptotic properties of PCA and Sparse PCA when the number of variables is much larger than the sample size; consistency and strong inconsistency regions will be characterized; various asymptotic frameworks will be considered. The results offer theoretical insights into appropriate understanding of the results from PCA and Sparse PCA. The next four projects add innovative and valuable analysis tools to the field of functional data analysis. The first two aim at developing two-way functional PCA techniques for non-standard data including those with exponential family distributions and hazard rates. The last two deal with dependent functional data such as spatial-temporal data and time series of curves.<br/><br/>The proposed research is motivated by and will have immediate beneficial impacts on cancer and neuro disorder research, medical imaging, and workforce management of labor-intensive service systems. In addition, the developed statistical methods will be useful in fields far beyond these motivating applications, such as demography, quantitative sociology, financial econometrics, and spatial-temporal modeling. Complementary activities are planned to foster the dissemination of the research results quickly and broadly. The problems addressed are also of broad interest to general society, in terms of pressing issues such as human risky behaviors, health care policies, social security planning, and worker productivity. The research activities are a natural venue for training graduate students in these exciting new research areas. The collaborative projects and the relevant data have natural second uses in the classroom. Methods proposed are useful for developing an advanced statistics research topic course. Strong mentoring of junior female scientists and minority students is another important component of the proposal."
"1107000","Sparse and structured networks: Statistical theory and algorithms","DMS","STATISTICS","07/01/2011","07/28/2013","Martin Wainwright","CA","University of California-Berkeley","Continuing Grant","Gabor Szekely","06/30/2015","$420,000.00","Bin Yu","wainwrigwork@gmail.com","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","","$0.00","The proposal focuses on Markov random fields and directed graphical<br/>models, classes of statistical models that are based on a marriage<br/>between graph theory and probability theory, and allow for flexible<br/>modeling of network-structured data.  The core of the proposal<br/>consists of multiple research thrusts, all centered around the goal of<br/>developing practical algorithms and theory for statistical estimation<br/>with network-structured data.  One research thrust concerns various<br/>issues associated with model selection in undirected graphical models,<br/>also known as Gibbs distributions or Markov random fields.  Problems<br/>include determining the information-theoretic limitations of graphical<br/>model selection in high dimensions (where the number of vertices may<br/>be larger than the sample size), not only for i.i.d. data but also<br/>dependent data; developing methods for tracking sequences of networks<br/>that evolve over time; and developing methods for data with hidden<br/>variables.  Another research thrust concerns various statistical<br/>problems associated with directed acyclic graphical structures (DAGs),<br/>including estimating equivalence classes of DAGs in the<br/>high-dimensional setting; estimating causal relationships via designed<br/>interventions; and efficient computational methods for DAG selection<br/>using the Lasso and related methods.  Overall, the proposed research<br/>is inter-disciplinary in nature, drawing on techniques from<br/>mathematical statistics, convex optimization, information theory,<br/>concentration of measure, and graph theory.<br/><br/>Science and engineering abounds with different types of networks.<br/>Examples include social networks such as FaceBook and Twitter,<br/>networks of genes and proteins in molecular biology, network models<br/>for economic and market dynamics, neural networks in brain imaging,<br/>networks of disease transmission in epidemiology, and information<br/>networks in law enforcement.  In the real-world, the structure of the<br/>underlying network is not known, but instead one observes samples of<br/>the network behavior (e.g., packet counts in a computer network;<br/>instances of infection at given time instances of an epidemic; emails<br/>or text messages sent among a group of people), and the goal is to<br/>infer the network structure.  Methods for solving this network<br/>inference problem have a broad range of applications.  Examples<br/>include inferring brain connectivity and disease etiology in<br/>neuroimaging studies, detecting terrorist cells in social networks,<br/>monitoring intrusions in computer networks, and understanding the<br/>basis of gene-protein interactions in systems biology."
"1106790","Covariance Matrix Estimation in Time  Series and Its Applications","DMS","STATISTICS","09/01/2011","06/05/2013","Wei Biao Wu","IL","University of Chicago","Continuing Grant","Gabor Szekely","08/31/2016","$276,879.00","","wbwu@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","","$0.00","The goal of this project is to establish a systematic asymptotic theory for estimates of large dimensional covariance matrices in time series; a fundamental problem in high-dimensional inference. In particular, the investigator plans to study properties of sample covariances and sample covariance matrices for stationary processes; deal with consistent estimation of covariance matrices of stationary processes and its applications in prediction and other problems; and explore non-Gaussian features of random processes by estimating higher order cumulant tensors.<br/><br/>Covariance matrices play a fundamental role in various fields including environmental science, engineering, economics and finance. Estimation of covariance matrices is needed in analyzing, testing, monitoring and predicting of seismic, economic and financial and other time series. Results developed from this project can provide a theoretical foundation for estimation of covariance matrices and can potentially improve time series processing algorithms that are used in various applications."
"1055815","CAREER: Subsampling Methods in Statistical Modeling of Ultra-Large Sample Geophysics","DMS","STATISTICS","09/01/2011","07/30/2013","Ping Ma","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Gabor Szekely","05/31/2014","$228,295.00","","pingma@uga.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","1045, 1187","$0.00","Remote sensing of the Earth's deep interior is challenging. Direct sampling of the Earth's deep interior is impossible due to the extreme pressures and temperatures. Our knowledge of the Earth?s deep interior is thus pieced together from a range of surface observations. Among surface observations, seismic waves emitted by earthquakes are effective probes of the Earth?s deep interior and are relatively inexpensively recorded by networks of seismographs at the Earth's surface. Unprecedented volumes of seismic data brought by dense global seismograph networks offer researchers both opportunities and challenges to explore the Earth?s deep interior. The key challenge is that directly applying statistical methods to this ultra-large sample seismic data using current computing resources is prohibitive. To facilitate geophysical discoveries that can enhance our understanding of the Earth?s deep interior using current computing resources, the investigator proposes a family of novel statistical methods under a subsampling framework. The proposed methods provide an opportunity to study various distinct statistical problems, such as function estimation and variable selection, in a unified framework. The investigator will establish asymptotic and finite sample theory to investigate the approximation accuracy and consistency of the proposed methods.<br/><br/>How to analyze ultra-large sample data creates a significant challenge in almost all fields of science and engineering.  Scientists and engineers develop various solutions to tackle the problem, such as developing cloud computing for aggregating a wide range of computing resources and building powerful supercomputers.  However, the high cost of these solutions creates an extraordinary budget barrier for researchers. The proposed subsampling methods provide alternative methods to surmount this challenge. The theory to be established will benefit a wide spectrum of research in science and engineering. They will offer a unique educational experience for both undergraduate and graduate students to participate in cutting-edge statistical and interdisciplinary research and inspire new lines of researches in three distinct fields: statistics, geophysics, and computational biology."
"1104545","Statistical Inference for Temporally Dependent Functional Data","DMS","STATISTICS","06/01/2011","05/23/2011","Xiaofeng Shao","IL","University of Illinois at Urbana-Champaign","Standard Grant","Gabor Szekely","05/31/2015","$316,245.00","","xshao@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","","$0.00","The PI develops a systematic body of methods and related theory on inference of temporally dependent functional data. The basic tool is the self-normalization (SN), a new studentizing technique developed recently in the univariate time series setting. The PI proposes to advance new SN-based methods in functional setup and develop (i) a class of SN-based test statistics to test for a change point in the mean function and dependent structure of weakly dependent functional data; (ii) a class of SN-based test statistics to test for white noise in Hilbert space and effective diagnostic checking tools for the AR(1) model in functional space; (iii) new SN-based tests in the two sample setup. The tests can be used to check if the two possibly dependent functional time series have the same mean and/or autocovariance structure. In this proposal, the SN is the foundation on which the body of connected and systematic inference methods for temporally dependent functional data is built.<br/><br/>The proposal is motivated by ongoing collaboration with atmospheric scientists on statistical assessment of properties of numerical model  outputs as compared to real observations. To study climate change, which is one of the most urgent problems facing the world this century, scientists have relied primarily on climate projections from numerical climate models.  There is currently a major interest to study how different the numerical model outputs are from real observations and the characterization of their difference. Analyzing these data are quite challenging because they are massive and highly complex with intricate spatial-temporal dependence. The SN-based inference methods that the PI develops in this proposal address these issues. With the assistance of functional principal component analysis, the SN-based methods are able to handle massive data sets with dependence, because the methods automatically take the unknown weak dependence into account, do not involve the choice of any tuning parameters (so are quite efficient computationally),   and are very straightforward to implement with asymptotically pivotal limiting distributions. A direct application of the SN-based methods to climate data is expected to help atmospheric scientists gain a better understanding of the ability of numerical model outputs in mimicking real observations.  In addition, the proposed methods will have broad direct applications to data that are obtained from very precise measurements at fine temporal scales which frequently  arise in engineering, physical science and finance. On the educational front, the PI will develop new advanced topic courses, mentor undergraduate and graduate students and expose them to the state-of-the-art research in this project."
"1106796","Sampling for Statistical Inference on Network Data","DMS","STATISTICS","07/01/2011","05/06/2011","Yuguo Chen","IL","University of Illinois at Urbana-Champaign","Standard Grant","Gabor Szekely","06/30/2015","$180,149.00","","yuguo@uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","","$0.00","Network structures arise in modeling a wide variety of systems in sciences and engineering. One of the most important classes of network models is random graphs. The goal of this project is to study two challenging problems in network analysis: one is on sampling random graphs with a given degree sequence, and the other is on detecting structures in networks. For the first problem, a new sequential sampling method is proposed which samples the networks from an approximate uniform distribution. These samples can be used to estimate the distribution of any test statistic and count the number of networks with given vertex degrees. For the second problem, new Monte Carlo algorithms are proposed to detect network structures, including highly connected subgraphs and network motifs.<br/><br/>The investigator develops innovative Monte Carlo techniques for simulating random graphs and detecting network structures. These methods can be used to analyze social interaction patterns, identify network motifs, extract densely connected molecular modules, and much more. Identifying network structures are scientifically important because they may correspond to a group of proteins that interact with each other at the same time or a functional unit that plays a key role in biological regulation networks. The research provides an ideal opportunity for involvement of students with a broad range of background and interests. The algorithms developed from this research will be incorporated into relevant courses."
"1106644","Oracle Inequalities in Sparse Regression and Low Rank Matrix Estimation","DMS","STATISTICS","08/15/2011","08/05/2011","Karim Lounici","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","07/31/2014","$100,001.00","","klounici6@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","","$0.00","The main research objectives of this proposal are related to the field of high-dimensional statistics such as sparse regression or low rank matrix estimation problems which have recently attracted a lot of attention. The investigator intends to develop new methodologies and novel applications and extend the scope of applications for penalized empirical risk minimization, empirical processes and exponential weights estimators. The investigator studies in particular the minimax rates in the noisy matrix completion problem and intends to adapt successful techniques from matrix completion problem to the covariance estimation problem and determine the minimax rate for this problem under the low rank assumption.<br/><br/><br/>The theoretical results developed in this research project are expected to have broader applications as well. The new research results can be applied in many fields: econometrics, marketing, data mining, quantum physics, cosmology, genomic, tomography, climatology and many other fields that require efficient tools for exploring high-dimensional data sets. In particular, a question of crucial interest in all these applications is to determine the set of active variables among a huge set of potential candidates. In genomic, micro-array chip contain the expression of thousands of genes and the goal is to find the few genes responsible for the synthesis of a particular molecule among the entire pool of tested genes. This difficult problem can be tackled efficiently through the techniques studied in this research project."
"1055286","CAREER: Sparse Modeling Driven by Large-Scale Genomic Data","DMS","STATISTICS","06/01/2011","05/28/2015","Qing Zhou","CA","University of California-Los Angeles","Continuing Grant","Gabor Szekely","05/31/2017","$400,000.00","","zhou@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","1045, 1187","$0.00","Massive and high-dimensional data arise frequently in biology and genomics. Regularization and sparsity are critical components for modeling such data and extracting information. Broad success of sparse modeling methods, such as the Lasso, has encouraged fast development in this area. However, most existing methods were developed under the frameworks of linear models and generalized linear models. The complex structures in genomic data require further development beyond existing methods. To this end, proposed are three novel sparse modeling methods with sophisticated model structures driven by large-scale gene expression data, protein binding data and DNA sequence data. The first method, motivated by modeling the relationship between protein binding and gene expression, constructs linear regression models on the terminal nodes of a decision tree. The decision tree partitions the population into subgroups according to the predictors. Each subgroup has its own sparse linear regression model between the response and the predictors. Two types of regularization, one on the regression coefficients and the other on the size of the tree, are used to encourage sparsity. The second method concerns the construction of tight clusters for gene expression data by penalizing the difference in grouped parameters between two tight clusters and between a tight cluster and the null cluster. Block-wise coordinate descent in conjunction with majorization is developed to maximize the regularized likelihood function. The third method, motivated by the motif finding problem, aims at sequence pattern discovery. A dictionary model is used to partition a sentence into words, which represent sequence patterns, and single letters. A novel regularization through the Kullback-Leibler divergence is developed for the product-multinomial model for words, which can achieve sparsity in estimating the cell probabilities. This regularization is used to construct a sparse dictionary that contains only a small number of words. A generalized EM algorithm is proposed for parameter estimation and solution path construction.<br/><br/>As efficient analysis of large-scale high-dimensional data is critical in many fields of science and engineering, the proposed research is of great current interest. Particularly, the proposed methods are ready for applications to front-edge research areas in genomics and molecular biology, where massive data sets have been continuously generated. To accelerate such applications, free computer packages and self-contained software are being developed for users to analyze their own data. On the other hand, this proposal contains many innovative statistical methodologies that may contribute significantly to statistics and computational sciences. Finally, the proposed research is integrated with educational activities by developing new and improving existing courses at both undergraduate and graduate levels."
"1055210","CAREER: New Statistical Methods for Classification and Analysis of High Dimensional and Functional Data","DMS","STATISTICS","09/01/2011","08/09/2015","Yichao Wu","NC","North Carolina State University","Continuing Grant","Gabor Szekely","12/31/2017","$400,000.00","","yichaowu@uic.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","1045, 1187","$0.00","Recent technology advances have generated data of unprecedented size and complexity across different scientific fields. To analyze such complex data, the principal investigator (PI) aims to develop new statistical methodologies. The PI proposes to study four interrelated research topics. First, the PI focuses on large-margin classification and proposes new large-margin classifiers to deliver competitive classification and conditional class probability estimation. He also proposes to address the question of whether a soft or hard classifier is preferred for a particular classification task and how to incorporate estimated conditional class probability to improve dimension reduction for data with a categorical response. Second, the PI proposes an extension of the least angle regression to deal with generalized linear models and, more generally, a strictly convex optimization problem. The new solution path is piecewise given by systems of ordinary differential equations and can be slightly modified to get the corresponding LASSO regularized solution path. Third, data with a sparse and irregular functional predictor are considered. New response-based dimensional reduction methods are proposed for such data using cumulative slicing and a viable scheme is also proposed to extend large-margin classifiers to analyze such data. Fourth, the PI focuses on the semi-parametric multi-index regression. By noticing that the Hessian operator filters out the effect of the linear component automatically, the PI provides a direct estimation scheme to estimate the space spanned by the multiple indices. The new scheme differs from existing methods in that it does not require estimating the nonparametric link while estimating the space spanned by the multiple indices as in other existing approaches.<br/><br/>The proposed statistical methodology innovations are widely applicable in various fields. For example, the proposed new large-margin classifiers can be applied to analyze genomic data with a categorical response such as cancer type; new ordinary differential equation based solution path algorithms can used to analyze survival or binary genomic data to identify important predictors; while analyzing longitudinal data of aging, the new proposed statistical methods for sparse and irregular functional data will be useful. In order to facilitate the use of the proposed new methods, the PI will implement them in R or Matlab and make new software available to the public along with the corresponding research reports. The success of the proposed research will help to improve public health."
"1055214","CAREER: A flexible design and modeling framework for computer experiments and beyond","DMS","STATISTICS","06/01/2011","07/15/2015","Peter Chien","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","05/31/2017","$400,000.00","","peterq@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","1045","$0.00","The primary objective of this proposal is to develop a flexible framework for design and modeling of large-scale simulations with broad applications to other areas of statistics. The investigator studies a multi-step method for fitting massive data from computer simulations that can simultaneously mitigate singularity and improve accuracy of interpolation. Theoretical bounds on numeric and nominal accuracy of this method will be derived. The investigator also proposes new designs inspired by Sudoku to efficiently pool data from multiple sources and employs sliced Latin hypercube designs to enhance stochastic optimization and cross-validation.<br/><br/><br/>Large-scale simulations are widely used for studying complex phenomena in sciences and engineering. The trend of replacing physical experiments with simulations to save cost and time has accelerated recently. The proposed research draws impetus from computer simulations but applies broadly to other areas of statistics for modeling massive data and for borrowing information from multiple sources. Beyond statistics, the research will make significant contributions to discrete mathematics, computer science and high-performance computing. Dissemination through journal publications, industrial collaborations and release of open source software will result in broad adoption of the developed research to significantly improve the use of complex simulations in U.S. industries. The research will be of considerable added value to rigorous uncertainty quantification efforts by national laboratories to support national security. Training students from under-represented groups will be accomplished through a puzzle based learning approach. Graduate students will benefit through multidisciplinary training on the interface between statistics and optimization. Ph.D. students will be supervised by following the Wisconsin model of balancing statistical theory and practice, and obtain first-hand research experience they can draw on for their careers."
"1106577","New Developments in Sufficient Dimension Reduction","DMS","STATISTICS","09/01/2011","08/17/2011","Yuexiao Dong","PA","Temple University","Standard Grant","Gabor Szekely","08/31/2014","$99,999.00","","ydong@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","MPS","1269","","$0.00","This proposal aims to develop new theory and methodology for sufficient dimension reduction. Through a series of well-defined research problems, the investigator develops specific dimension reduction methods for many important applications. In particular, this research proceeds in three main directions. First, the investigator proposes a general approach based on estimating equations to facilitate sufficient dimension reduction. Under this effective and flexible estimating equations framework, the investigator relaxes a restrictive distributional assumption for the classical dimension reduction techniques, and proposes a novel sufficient dimension reduction methodology that handles challenging problems such as missing observations and heteroscedastic modeling. Secondly, outliers are commonly observed in high-dimensional data and yet studied occasionally in the sufficient dimension reduction context. The investigator develops new sufficient dimension reduction procedures which are robust to outliers in the observations. Thirdly, most variable selection techniques in the current literature are model-based. The investigator proposes to extend sufficient dimension reduction methodology that can be applied to variable selection and testing the significance of subsets of predictors in a model-free fashion. The success of this project not only provides effective practical tools for high-dimensional data analysis but also represents an advance in the theory and methodology of semiparametric inference.<br/><br/>The scale and complexity of data sets have increased drastically in light of the development in modern technology. High-dimensional data that involve a large amount of variables are nowadays routinely generated and collected in areas such as environmental studies, human health and medical research, homeland security, government and business administration. High-dimensional data pose many challenges for statisticians and provide considerable momentum in the statistics community to develop new theories and methodologies. Sufficient dimension reduction methodology effectively transforms a high dimensional data problem to a low dimensional one, and thus facilitates many existing statistical methods which used to be hindered by the curse of dimensionality. The investigator proposes a new paradigm that synthesizes and broadens the theories and methodologies of sufficient dimension reduction. The results from this project can be widely applied for regression and classification problems in areas which involve a large number of variables, such as econometrics, finance and bioinformatics. The investigator also has the plan to provide free software packages to academic and industrial users of the proposed procedures. The support of the proposed research helps the investigator in the training of the students at Temple University, where few senior faculty members are available to supervise a large number of graduate students, including many minority and female students."
"1107070","Statistical Inference for Stochastic Processes, Analysis of MCMC Algorithms and Applications to Climate Science","DMS","STATISTICS","07/01/2011","06/30/2011","Natesh Pillai","MA","Harvard University","Standard Grant","Gabor Szekely","06/30/2014","$145,000.00","","pillai@fas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","","$0.00","This proposal consists of three complementary themes with a particular research focus on statistical inference for data arising from dynamical systems, partial differential equations (often representing physical phenomena) and diffusions. The research questions in the proposal are motivated by the genuine need for novel statistical inference in these areas where the data is naturally high dimensional. A highlight of this proposal is the interdisciplinary nature of the problems which requires the integration of techniques from a wide spectrum of fields in applied mathematics, probability and statistics. The main themes are 1) stability of Markov Chain Monte Carlo algorithms in high dimensions, 2) statistical inference for inverse problems from diffusions and dynamical systems, and 3) applications to climate science and temperature prediction. The first two themes aim at developing methods and improving the theoretical understanding of statistical inference procedures whilst the third will directly implement the insights gained from the first two to answer a few concrete relevant and open problems in climate science. The main thread connecting the above three themes of the research is the development and theoretical analysis of novel and efficient Markov Chain Monte Carlo techniques.<br/><br/>Advances in technology and computing power have made many historically intractable problems in statistics amenable to routine implementation using certain probabilistic algorithms.  Despite two decades of intense research, our theoretical understanding of the behavior of these complex algorithms in high dimensions is still primitive. The PI proposes to study these algorithms and quantify their behavior in high dimensions, and apply them to solve concrete problems in climate science."
"1107108","Statistical Inference for Random Recursive Equations","DMS","STATISTICS","08/01/2011","07/20/2011","Anand Vidyashankar","VA","George Mason University","Standard Grant","Gabor Szekely","07/31/2014","$100,002.00","Guoqing Diao","avidyash@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1269","","$0.00","This proposal is concerned with asymptotic and small sample inference related to random recursive equations and associated large deviation problems. Random recursive equations, also referred to as stochastic fixed point equations (SFPE), arise in several areas of contemporary science, including: (i) cell-biology, (ii) analysis of algorithms, (iii) financial time-series modeling, (iv) study of perpetuities, (v) actuarial science, (vi) risk management, (vii) ranking of web-pages, and (viii) processes on complex networks. This proposal is concerned with developing new statistical methods that integrate, refine, and sharpen ideas from large deviation theory, semi-parametric and non-parametric inference, efficient importance sampling. It addresses the following basic questions: (1) How to obtain confidence and prediction intervals for the tail probability in risk models that integrate complex financial and insurance processes? (2) How to efficiently estimate the page-ranks of web-pages and understand the factors that influence them? (3) How to provide statistical comparisons between running times of random recursive algorithms? The answer to question (1) is of significant interest to researchers in actuarial science and risk management. The answer to question (2) will enable the development of policies for better utilization of resources. The answer to (3) will yield quantitative methods for developing and comparing recursive algorithms which are used, for example, in computer science. <br/><br/>Random recursive equations allow one to unify a wide class of problems that arise in scientific investigations. In a range of applications, scientists are often interested in understanding the probabilities of occurrence of very rare event, which could nevertheless have catastrophic consequences. These rare events could be rare types of cancer whose prevalence rate is small, or the probability of bankruptcy of a financial institution, or beginning stages of resistance to a drug. A key issue is that, while extensive amounts of data are available to model and analyze the frequently occurring events, the amount of data available to study these rare events is perennially low, making the inferential problem challenging.  This proposal is concerned with mathematical, statistical, and computational methods to address these challenging issues and provide concrete answers to some of the problems concerning probabilities of rare events."
"1107206","Geometry, Shape and Objects","DMS","STATISTICS","07/01/2011","07/28/2013","Wolfgang Polonik","CA","University of California-Davis","Continuing Grant","Gabor Szekely","06/30/2015","$169,972.00","","wpolonik@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","","$0.00","Filaments, ridges, graphs, contours, and paths are examples of geometric objects in modern data analytic challenges. The development and the analysis of statistical methodology to tackle challenges related to such objects is the overarching goal of this project. In particular,  the principal investigator and his collaborators will (i) develop statistical methodology and algorithms for the estimation of ridge lines and/or filamentary structure involving path smoothing methodologies. Here, paths are estimates of integral curves of gradients or of appropriate eigenvectors of Hessians. (ii) They will also develop dimension reduction methodology for subsequent clustering of high-dimensional data, where projection onto non-linear manifolds will be considered. Further goals are (iii) the adaptation of model selection methodologies for high-dimensional graphical models to incorporate spatial and temporal smoothness, and (iv) the investigation of commonalities and differences between statistical density/regression level set methodology and depth contour approaches. Besides the intrinsic geometric nature of the problems under consideration, geometry also comes into play via the choice of an appropriate distance measure, for instance.<br/><br/>This project will lead to new statistical methodologies underpinned by relevant theory, and to corresponding numerical algorithms in the area of geometric statistics. In this field of statistics, the objects of interest itself are genuinely geometrically motivated. One important guiding instance of a relevant scientific problem is the analysis of the cosmic web, which consists of locations of galaxies in the space.  The web-like structure of these locations requires the use of nonstandard methodologies. Geometrically motivated objects also play a major role in several other scientific areas, such as medical imaging, fingerprint identification and remote sensing. The findings of this project will advance the field of statistics, and they will directly impact the relevant fields of application. The education of graduate and undergraduate students in a modern field of statistics is another important goal of this project."
"1106743","Martingale Control of mFDR in Variable Selection","DMS","STATISTICS","07/01/2011","05/19/2011","Robert Stine","PA","University of Pennsylvania","Standard Grant","Gabor Szekely","06/30/2014","$320,000.00","Dean Foster","stine@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","","$0.00","The investigators in this project develop methods that control the selection of predictive features from multiple sources when building statistical models.  A martingale representation of the number of spurious variables provides the underlying theoretic support.  This martingale defines a framework for testing a possibly infinite sequence of hypotheses.  This representation leads to methods for streaming feature selection that control the expected number of false discoveries (mFDR).  Extensions to be developed in this project generalize prior work of the investigators, extending their results to multiple streams of potential features while maintaining the martingale representation. Whereas the previous work of the authors was in the high-noise, low-signal setting in which few features are predictive (the nearly black setting), advances in this proposal push their methods into problems characterized by many predictive features with higher signal-to-noise ratios.  This proposal envisions replacing the original martingale by one directly related to the goodness of fit of the model.  The investigators plan to use this revised martingale to show that an auction-based system that combines several sources of features satisfies the mFDR condition.<br/><br/><br/>The investigators develop novel methods for building predictive statistical models that combine and learn from multiple sources of information.  A predictive statistical model is an empirical rule constructed from data that predicts a specific characteristic of observations, the response, based on the values of other characteristics.  The challenge of building these models is to identify characteristics that yield predictive insights.  While ever larger amounts of data are an essential input to a statistical model, the presence of vast numbers of characteristics lead to the problem of over-fitting.  Over-fitting occurs when one confuses a random coincidence among characteristics with a reproducible pattern.  Modern data mining produces such a plethora of characteristics that it becomes difficult to distinguish real from imaginary associations.  The investigators propose a system that makes these distinctions in the context of a common modeling paradigm. As a practical testbed, the investigators will analyze classic computational linguistic problems using regression analysis, the workhorse method of applied statistics. Given the extent of experience in linguistics, any deficiencies of a regression model will stand out. This will encourage innovations in regression that maintain their simplicity while competing with handcrafted methods in linguistics. These innovations should extend to other applications including fMRI, genetics, and more general data mining."
"1107067","High-dimensional Discrete Inference","DMS","STATISTICS","07/01/2011","06/22/2011","Luis Carvalho","MA","Trustees of Boston University","Standard Grant","Gabor Szekely","06/30/2014","$64,495.00","","lecarval@math.bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","MPS","1269","","$0.00","Recent advances in the last decade have brought attention to the analysis of high-dimensional data and, in particular, to estimation on high-dimensional spaces. Such spaces are often structured by either exhibiting constraints on specific space components or by the incorporation of prior information identifying co-dependence patterns between components in order to help carrying out the inference. Given the recent predominance of discrete inference problems in many influential fields, the investigator takes on the critically important task of discrete estimation in high-dimensional settings and aims at laying foundational principles upon which estimation and characterization of high-dimensional discrete spaces can be efficiently performed and where structural properties of the space are adequately taken into account. More specifically, the PI explores estimators formally derived from statistical decision theory and based on loss functions that more naturally capture the features of the discrete space and are thus arguably better representatives of the ensemble. If the discrete space is constrained, obtaining an efficient procedure for estimation is of prime concern given the large size of the space; to this end, the PI also proposes to develop a general framework that can be explored to design efficient procedures for inference, assess the computational complexity of the proposed estimation, and further derive approximation schemes when needed. In addition, the investigator applies the proposed foundations to highlight important features of the discrete space such as regions of high concentration of probability mass, and studies a method to jointly elucidate features and identify good subspace representatives.<br/><br/>Many problems from fields like genetics, social sciences, molecular biology, and environmental studies can be casted as statistical inference problems on a large number of unknowns. Even though modern, high-throughput technology has enabled the collection of large datasets, these problems remain hard since the number of parameters describing the data generating process grows with the number of observations. In this setting, it is helpful to associate structure to the model in order to guide the inference. The investigator studies novel, principled estimators that address two issues under this high-dimensional regimen: effectively capture structural relationships among variables in the model, and efficiently derive solutions through computationally feasible routines. The PI intends to implement and publish the resulting methods as open-source software that benefits both academia and industry, and further fosters the development of algorithms and practical implementations. Through this research project the PI also intends to promote the integration of research and education by developing new courses and raise awareness for statistical analysis of high-dimensional data and inference on discrete spaces with state-of-the-art methods. Finally, the PI expects to encourage collaborations between statisticians and researchers from other fields and promote statistical methods in interdisciplinary areas."
"1106483","Statistical Inference for Tree Models with Strong Hierarchical Autocorrelation","DMS","STATISTICS","07/01/2011","04/25/2013","Cecile Ane","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","06/30/2015","$206,505.00","","cecile.ane@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","Models with tree-structured, hierarchical autocorrelation are used when sampling units are related to each other. Their inheritance history is modeled by a tree, which is used to parametrize the residual correlation structure among observations. The project will develop an asymptotic theory for these autocorrelation models, arising from an Ornstein-Uhlenbeck process along the tree. As the number of tips in the tree grows indefinitely, the investigators will determine which parameters are microergodic and which parameters are not. The asymptotic consistency and the rate of convergence of the maximum likelihood estimator are expected to vary importantly depending on the microergodicity of the parameter and on topological properties of the tree. Analogies will be built between this asymptotic framework and the infill asymptotic framework in spatial statistics, when observations are collected on a dense set of locations within a bounded region of space. The project will refine the concept of effective sample size for hierarchically autocorrelated data and study optimal sampling designs. This work will provide important steps toward developing appropriate model selection tools for the detection of possibly many Ornstein-Uhlenbeck selection regimes, with a large number of model parameters compared to the sample size. <br/><br/>Tree models with hierarchical autocorrelation arose first in evolutionary biology and ecology, with the comparison of biological species. These models are now used in many other areas, ranging from the study of rapidly evolving viruses to the study of human language evolution. The Ornstein-Uhlenbeck model is used to detect selection as opposed to neutral evolution, to discover changes in selective regime and to determine driving factors of selection. The project will provide a unified statistical asymptotic framework for these models and will inform best practices for empirical studies.  Computational tools will be broadly disseminated, and opportunities will be provided for training at the interface between statistics and biology."
"1106980","Collaborative proposal: Statistical methods for analyzing complexity and growth of large biological and information networks","DMS","STATISTICS","07/01/2011","06/30/2011","Edoardo Airoldi","MA","Harvard University","Standard Grant","Gabor Szekely","06/30/2014","$75,000.00","","airoldi@temple.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","","$0.00","Every network dataset poses unique challenges, but there is a growing toolkit of methods that can be adapted to specific situations. This research will extend that toolkit by continuing the development of machine learning ideas that perform aggressive local variable selection to fit local metrics, thus allowing nearby nodes to have similar models for edge formation, but distant nodes to have very different models. Also, the research will provide a new approach to goodness-of-fit assessment for network models, based upon minimum description length inference. <br/><br/>Network modeling has emerged as a critical methodology across many fields of science, including biochemistry, sociology, and Internet communication. Important applications include social interactions leading to fission in baboon troops, biochemical knowledge derived from protein-protein interactions, and insight into the growth and structure of the Wikipedia. This research will develop novel statistical models for network growth and new ways to assess how well they explain a given dataset."
"1107025","Collaborative Research: Semiparametric Conditional Graphical Models with Applications to Gene Network Analysis","DMS","STATISTICS","07/01/2011","04/24/2013","HYONHO CHUN","IN","Purdue University","Continuing Grant","Gabor Szekely","06/30/2015","$90,000.00","","chunh@bu.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","","$0.00","The research proposed in this project is motivated by the following problem. In many genetic studies, in addition to gene expression data, other types f data are collected from the same individuals. The problem is how to make use of this additional information when construct gene networks. The investigators formulate this problem by a  conditional Gaussian graphical model (CGGM), in which the external variables are incorporated as predictors. They propose an estimation  procedure for this model by combining reproducing kernel Hilbert space with the lasso type regularization. The former is used to construct a model-free estimate of the conditional covariance matrix, and the latter is used to derive a sparse estimators of the conditional precision matrix, whose zero entry pattern correspond to a graph that describes the gene network. They propose to study the asymptotic properties, to introduce methods to determine the tuning constants, and to develop standardized and openly accessible computer programs for this model. Furthermore, the investigators propose to extend the CGGM in two directions. First, they propose to relax the Gaussian assumption by applying a copula transformation to the residuals and then using  pseudo likelihood to  estimate conditional correlations. These are then subject to the lasso-type  regularization to yield sparse estimator of the precision matrix. The second direction is the development of  sufficient graphical model, which is a mechanism to simultaneously reduce the dimension of the predictor and estimate the graphical structure of the response.<br/><br/><br/>High-throughput technologies that enable researchers to collect and monitor information at the genome level have revolutionized the field of biology in the past fifteen years. These data offer unprecedented amount and diverse types of data that reveal different aspects of the biological processes. At the same time, they also present many statistical and computational challenges that cannot be addressed by traditional statistical methods. In current genomics research it has become increasingly clear that statistical analysis based on individual genes may incur loss of information on the biological process under study. For example, a widely known study on identifying genetic patterns of diabetic patients show that no single gene could stand out statistically as responsible for the patterns, and yet clear signals emerged when genes were analyzed in groups. Motivated by this observation, greater attention has been paid to networks of genes. The investigators propose a class of new statistical methods, called conditional graphical models, for constructing gene networks that can take into account of a set of  covariates. They also plan to develop theoretical properties and computer programs for the proposed methods. Although their inquire began with gene networks, the investigators  envision conditional graphical models to have broad applications beyond genomics, such as in predicting asset returns  and in studying social networks, which are becoming all the more prevalent in this age of Internet."
"1106814","Collaborative Research: Applied Probability and Time Series Modeling","DMS","STATISTICS","06/01/2011","06/06/2011","Hernando Ombao","RI","Brown University","Continuing Grant","Haiyan Cai","04/30/2012","$31,169.00","","hombao@uci.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1269","9150","$0.00","An investigation of the properties of Levy-driven CARMA (continuous-time ARMA) processes will be undertaken and efficient methods of inference developed. The results will be applied to the study of stochastic volatility models with Levy-driven CARMA volatility that have applications that go beyond finance to turbulence and some neuroscience processes. Time series in which the parameters are constant over time-intervals between change-points constitute an important class of non-stationary time series which has been found particularly useful in hydrology, seismology, neuroscience, environmental science and finance. Properties and applications of a new estimation technique based on the minimization of the minimum description length of a model that includes the number of change-points and their locations as parameters will be developed and extended to cover a general class of processes with structural breaks. It is hoped that this technique can also be adapted for detection of both additive and innovational outliers. Linear and nonlinear models for multivariate time series, with a view towards modeling temporal brain dynamics, will also play a major role in this research proposal. These models include a mixture of possibly nonlinear vector autoregressions and a class of not necessarily causal vector autoregressions. The latter class, although linear, exhibits features previously only associated with nonlinear models and allows for the possibility of foresight in the sense of dependence of one or more components of future shocks. <br/><br/>In the last fifteen years, there has been a widely-recognized need for the development of new models and techniques for the analysis of time series data from scientific, engineering, biomedical, financial, and neuroscience applications. Some of the features required of these new models are nonlinearity, complex dependence structures, strong deviations from normality and non-stationarity. In neuroscience, environmental and financial modeling there is also a demand for continuous-time models which incorporate these features. The current proposal addresses these needs. It seeks to enhance understanding of the physical, biomedical, and economic processes represented by the models. The development of efficient estimation and simulation techniques will be an essential component of the research."
"1104426","Nonlinear Models for Functional Data Analysis","DMS","STATISTICS","07/01/2011","04/09/2013","Hans-Georg Mueller","CA","University of California-Davis","Continuing Grant","Gabor Szekely","06/30/2015","$309,998.00","","hgmueller@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","","$0.00","Nonlinear methods for Functional Data Analysis lead to flexible and versatile statistical models, inference and analysis methods for data that include samples of random functions. Such data accrue in the study of time-dynamic phenomena such as electricity consumption curves or biological trajectories and also in a large number of longitudinal studies across the sciences. Methods for functional data analysis have been rapidly evolving over the past few years and are increasingly viewed as essential for the analysis of time-dynamic phenomena. To date, linear models for functional data have been relatively well investigated, both in terms of theoretical and practical aspects, and statistical tools that are based on these methods are available for data analysis. However, the class of linear functional models is quite narrow and often not adequate in practical data analysis. In contrast, relatively little is known about more general and more flexible nonlinear approaches. This research seeks to remedy this situation by developing a class of nonlinear functional methods. The potential value of such models for applications is high, especially in scenarios where one observes repeated functions in time or space, or where one wishes to study regression relations that include functional components as predictors or responses. Nonlinear functional methodology includes representations of samples of trajectories by means of nonlinear components or through mixture models. These approaches are useful for applications where random time warping plays a role, and for the construction of quantiles in functional regression settings. The proposed methodology and associated software provides a sensible balance between increased flexibility and structural constraints.<br/><br/><br/>The investigator and his research group develop new methods aimed at the statistical analysis of repeatedly observed trends and trajectories. Such data are increasingly common due to new sophisticated sensors, measurement systems and the widening recognition that a deep understanding and interpretation of time trends and their patterns is often key to better individual and societal decision making. This new methodology is useful to gain insights into the dynamics of time-dependent processes such as human growth, characteristics of freeway traffic patterns, or the comparison of lifetables across countries and calendar years. The proposed nonlinear approaches to such functional data lead to improved and more compact descriptions and to better predictions of outcomes that are related to observed time trends."
"1106084","Collaborative Research: Objective Bayesian Model Selection and Estimation in High Dimensional Statistical Models","DMS","STATISTICS","10/01/2011","09/20/2011","Kshitij Khare","FL","University of Florida","Standard Grant","Gabor Szekely","09/30/2015","$100,809.00","","kdkhare@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","","$0.00","It is widely accepted that in many high dimensional situations, model selection has to be performed either before parameter estimation or simultaneously, in order to reduce the number of parameters under consideration. Indeed, model selection is one of the major challenges facing statisticians working with high dimensional data. Tools such as regularization and sparsity are some of the common notions employed to obtain parsimonious models to explain observed data. In recent years, the field of statistics has witnessed an explosion of frequentist and Bayesian methods for high dimensional problems. Despite these and other advances, Bayesian model selection in an ""objective"" sense in high dimensional problems remains an important problem that has yet to be solved satisfactorily. The need for objectivity translates into a need for specifying noninformative improper priors, which in turn renders the traditional Bayes factors unusable. The project proposes to derive objective Bayesian estimation and model selection procedures in a large class of high dimensional graphical models. The methodology that is proposed in this project therefore aims to contribute to much needed theory in the area of objective Bayesian model selection for high dimensional graphical models. In the process the methodology studies the benefits and shortcomings of objective Bayesian methods in this context. The theory that is developed feeds into developing algorithms and computational techniques for model selection/estimation in high dimensional settings.<br/><br/>The availability of throughput or high dimensional data has touched almost every field of science. The need to formulate correct models that explain observed high dimensional data permeates through many scientific fields. Indeed, such data where the number of variables is often much higher than the number of samples, referred to as the ""large p small n"" problem, is now more pervasive than it has ever been. Discovering statistical signals in high dimensional data, proposing correct models that can explain such data, and parameter estimation in these high dimensional settings are some of the major challenges that modern day statisticians have to contend with. Moreover, such challenges also feature in high stakes debates such as climate change, effectiveness of certain drugs in clinical trials, and relevance of various biomarkers in cancer studies. This project proposes to develop statistical methodology which is specifically targeted towards identifying models which explain high dimensional data in an objective manner. In particular the project is designed to develop better objective Bayesian model selection and parameter estimation methods in high dimensional problems, and has widespread applications. The PI and co-PI collaborate with scientists in applied fields, especially with faculty/researchers in their Medical Schools, Schools of Engineering and Environmental Sciences. Training of graduate students and mentoring is an integral part of this collaborative research. Scientific output from the project is intended for publication in high impact peer-reviewed journals."
"1107031","Collaborative Research: Applied Probability and Time Series Modeling","DMS","STATISTICS","06/01/2011","06/05/2013","Richard Davis","NY","Columbia University","Continuing Grant","Gabor Szekely","05/31/2015","$300,004.00","Peter Brockwell","rdavis@stat.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","","$0.00","An investigation of the properties of Levy-driven CARMA (continuous-time ARMA) processes will be undertaken and efficient methods of inference developed. The results will be applied to the study of stochastic volatility models with Levy-driven CARMA volatility that have applications that go beyond finance to turbulence and some neuroscience processes.  Time series in which the parameters are constant over time-intervals between change-points constitute an important class of non-stationary time series which has been found particularly useful in hydrology, seismology, neuroscience, environmental science and finance. Properties and applications of a new estimation technique based on the minimization of the minimum description length of a model that includes the number of change-points and their locations as parameters will be developed and extended to cover a general class of processes with structural breaks. It is hoped that this technique can also be adapted for detection of both additive and innovational outliers.  Linear and nonlinear models for multivariate time series, with a view towards modeling temporal brain dynamics, will also play a major role in this research proposal. These models include a mixture of possibly nonlinear vector autoregressions and a class of not necessarily causal vector autoregressions. The latter class, although linear, exhibits features previously only associated with nonlinear models and allows for the possibility of foresight in the sense of dependence of one or more components of future shocks.<br/> <br/>In the last fifteen years, there has been a widely-recognized need for the development of new models and techniques for the analysis of time series data from scientific, engineering, biomedical, financial, and neuroscience applications.  Some of the features required of these new models are nonlinearity, complex dependence structures, strong deviations from normality and non-stationarity.  In neuroscience, environmental and financial modeling there is also a demand for continuous-time models which incorporate these features.  The current proposal addresses these needs. It seeks to enhance understanding of the physical, biomedical, and economic processes represented by the models. The development of efficient estimation and simulation techniques will be an essential component of the research."
"1107047","Collaborative Research: Penalization Methods for Screening, Variable Selection and Dimension Reduction in High-Dimensional Regression via Multiple Index Models","DMS","STATISTICS","06/15/2011","06/06/2011","Yu Michael Zhu","IN","Purdue University","Standard Grant","Gabor Szekely","05/31/2014","$50,000.00","","yuzhu@stat.purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","","$0.00","The project aims to develop effective penalization methods for screening, dimension reduction, and variable selection in high dimensional regression. The investigators focus mainly on multiple index models, because this type of models combines the strengths of linear and nonparametric regression while avoiding their drawbacks. A novel penalization approach is employed for model fitting, which regularizes both the parametric and nonparametric components of a multiple index model.  A pilot study shows that this approach is more advantageous than other existing ones. When facing ultra-high dimensionality, the investigators use a forward variable screening procedure to reduce the dimension to a manageable size before applying the proposed penalization. The investigators plan to study the theoretical properties of this approach and develop fast and efficient computing algorithms for its implementation. The proposed approach is further extended to applications involving categorical responses or random effects.<br/><br/>Advances in science and technology have led to an explosive growth of massive data across a variety of areas such as bioinformatics, climate research, internet, etc. Traditional statistical methods for clustering, regression and classification become ineffective when dealing with a large number of variables. Lately, a tremendous amount of research effort has been dedicated to the development of statistical methods such as dimension reduction and variable selection for analyzing this type of massive data. The investigators join the effort by proposing a novel penalization approach and developing efficient computing algorithms. The results from this project not only advance statistical research but also help other scientists and researchers better understand and analyze their massive data and hence enhance their scientific discovery."
"1102482","International Conference on Design of Experiments (ICODOE-2011)","DMS","STATISTICS","04/01/2011","02/28/2011","Manohar Aggarwal","TN","University of Memphis","Standard Grant","Haiyan Cai","03/31/2012","$18,000.00","E Olusegun George","maggarwl@memphis.edu","Administration 315","Memphis","TN","381523370","9016783251","MPS","1269","7556, 9150","$0.00","The third International Conference on Design of Experiments (ICODOE-2011) will be hosted by the Department of Mathematical Sciences, The University of Memphis during May 10 ? 13, 2011. The principal aims of the conference are to bring together leading researchers from all over the world in the area of design and analysis of experiments and practitioners in the pharmaceutical, chemometrics, physical, biological, medical, social, psychological, economic, engineering and manufacturing sciences. The conference will also provide support and encouragement to junior researchers in the field of design and analysis of experiments. The focus of ICODOE-2011 is on emerging areas of research in experimental design, as well as novel innovations in traditional areas.  The conference will cover topics of importance to the academic and industrial community, such as design and analysis of experiments in Genomics, dose ? finding studies, adaptive designs for clinical trials, event related fMRI, designs for linear, non-linear and generalized linear mixed models, screening experiments, computer experiments and multi ? stratum designs.<br/><br/><br/>The design and analysis of experiments is at the foundation of the scientific method. It is used in applications across all scientific disciplines and engineering. It is used throughout business and industry to improve the reliability of processes and equipment and to identify and understand how process factors affect output. It is used in many scientific disciplines such as in medicine, especially in the design of clinical trials, dose ? finding studies and biomedical experiments. The advancement of statistical theory and methodology over the past years has resulted in the development of sophisticated methods capable of analyzing complex experiments. This has given rise to a demand for efficient designs for these experiments. At the same time, a new generation of enthusiastic and talented researchers in design of experiments is needed in the years to come to carry out original theoretical and applied research in the emerging areas. The junior researchers of today need to be nurtured and encouraged to assume this role. The researchers in the area of experimental design from the Universities and industry will also get an opportunity to meet together and discuss problems of mutual interest. ICODOE-2011 will broaden the participation of underrepresented groups, mainly minorities and women."
"1106668","New Dimension Reduction Approaches for Modern Scientific Data with High Dimensionality and Complex Structure","DMS","STATISTICS","07/15/2011","07/15/2011","Lexin Li","NC","North Carolina State University","Standard Grant","Gabor Szekely","06/30/2014","$100,000.00","","lexinli@berkeley.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","","$0.00","With the recent explosion of scientific data, and its unprecedented size and complexity, dimension reduction is becoming a central ingredient in any modern statistical analysis. This project aims to couple dimension reduction methodology with current statistical learning techniques, which results in an entirely new class of flexible and effective dimension reduction solutions for modern data with both high dimensionality and complex structure. From the coupling, the investigator establishes a framework for dimension reduction that incorporates prior information regarding the known structural relationships between the variables. Within this framework, the investigator plans to develop a family of dimension reduction solutions so that the results are more readily interpretable and accurate. Such a framework is to greatly facilitate the analysis of neuroimaging, climate, and genomic data where prior structural information is often available. <br/><br/>Modern technologies routinely produce massive amounts of data and such a data deluge now engulfs every branch of science and public life. As a result, scientific progress now heavily depends on the ability to process and analyze complex high-dimensional data. At the heart of these analyses are methods that reduce the dimensionality of the data, sometimes dramatically, by identifying a small set of variables that are important, or obtaining a few combinations of the original measurements. This project aims to develop a host of novel dimension reduction methods to address these pressing challenges in high-dimensional data analysis. The proposed research is expected to make significant contributions on two fronts: enabling scientists to quickly and effectively extract useful information from massive data, and at the same time, benefiting the discipline of statistics with advances in theory, methods and applications."
"1106395","Problems in Bayesian Model Selection and Development and Analysis of Markov Chain Sampling Algorithms","DMS","STATISTICS","09/01/2011","08/13/2011","James Hobert","FL","University of Florida","Standard Grant","Gabor Szekely","08/31/2015","$239,998.00","Hani Doss","jhobert@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","","$0.00","Bayesian methods are now routinely used in very complex models, with posterior distributions estimated by Markov chain Monte Carlo (MCMC) methods.  There are two consequences to this.  First, complex Bayesian models are virtually always governed by some hyperparameters, which have a large impact on subsequent inference.  Therefore, there is now a strong need for methods that enable selection of these hyperparameters.  Second, the Markov chains used to estimate the posterior distributions now run in non-standard spaces, for example large function spaces, and there is a need for the development of MCMC methods that will work well in non-standard spaces.  The investigators develop methods for efficiently estimating marginal likelihoods for large number of hyperparameter values.  This will enable implementation of the empirical Bayes method, and also enables users to determine classes of hyperparameter values which constitute reasonable choices.  The exploration of intractable posterior distributions resulting from complex Bayesian models often requires MCMC.  Unfortunately, in contrast with classical Monte Carlo, establishing central limit theorems (CLTs) for MCMC estimators is not straightforward.  This is a serious practical problem because the ability to choose an appropriate MCMC sample size hinges upon the existence of a CLT.  The investigators use spectral methods to develop checkable sufficient conditions for CLTs as well as methods for comparing the asymptotic efficiency of MCMC algorithms with the same target distribution.  They apply the theoretical results to very concrete problems of model selection and assessment.<br/><br/>Model selection in complex situations is an important and pervasive problem in scientific and medical research.  It includes in particular variable selection in regression, where a few important variables are to be selected from many candidates and used for understanding, prediction and decision making.  Different models can lead to different conclusions, with potential impact on public policy.  The investigators develop efficient computational methods for determining optimal models in complex settings.  The project has an educational component in that graduate students are involved in the research under the supervision of the investigators."
"1106608","Analysis of Survival Data using Copula Models","DMS","STATISTICS","07/15/2011","07/15/2011","Antai Wang","NY","Columbia University","Standard Grant","Gabor Szekely","03/31/2014","$140,000.00","","aw224@njit.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","","$0.00","The main objective of this research is to use Archimedean copula models to analyze survival data. The proposed strategies are useful in medical research and financial data analysis. The investigator will address three different problems. The first problem is to solve the identifiability problem for Archimedean copula models for dependent censored data. The second problem concerns left truncated bivariate data. The investigator will explore ways to best estimate the unknown parameters in Archimedean copula models using truncated bivariate data. A model selection procedure for Archimedean copula models will also be investigated. The last problem is to establish guidelines for selecting models belonging to Archimedean copula family based on survival data. Research will be conducted to demonstrate the performance of the proposed strategies.<br/><br/>The proposed methods and strategies are motivated by clinical trials involving dependent censoring problem and the study of the correlated bivariate survival data in AIDS research. The results of this project will be helpful for determining the underlying relationship between random variables when they are subject to different censoring patterns. The theoretic results will contribute to the advancement of the statistical theory on correlation studies and deepen the understanding of the dependence structure of Archimedean copula models."
"1106690","Application of Random Matrix Theory to Structured High-dimensional Data","DMS","STATISTICS","07/01/2011","05/06/2011","Debashis Paul","CA","University of California-Davis","Standard Grant","Gabor Szekely","06/30/2014","$169,987.00","","debpaul@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","","$0.00","The main goal of this application is to utilize spectral analysis techniques for dealing with high-dimensional inferential problems. Techniques of random matrix theory, especially Stieltjes transforms of spectral measures, will be utilized to enhance understanding the effects of dependencies among observations on commonly used statistical procedures in high-dimensional settings. As a key component, investigations on the spectral characteristics of large random matrices with dependencies among both rows and columns will be carried out. In addition, new regularization schemes will be developed that are tuned to the characteristics of the data, including possible non-stationarity of the observations, and make use of the intrinsic parsimonious structures in the data.<br/><br/>The proposed application is motivated by problems in a wide range of scientific fields such as wireless communication, spectrometry, genomics, environmental modeling, atmospheric science, brain imaging and econometrics. The emphasis of this proposal is to develop theoretical understanding and practical tools for analyzing complex and large-scale data arising in these disciplines. The research outputs from this project are expected to give wider access among scientists and practitioners in various disciplines to modern statistical tools and concepts for dealing with high-dimensional data. In addition, the tools and ideas developed through this project are likely to contribute towards downstream technologies that require sophisticated real-time data analysis techniques for complex time-varying signals."
"1106586","Dimension Reduction for Non-Regular Statistical Models with Applications","DMS","STATISTICS","08/15/2011","07/29/2011","Chunming Zhang","WI","University of Wisconsin-Madison","Standard Grant","Gabor Szekely","07/31/2014","$100,000.00","","cmzhang@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","The proposal aims to develop new statistical theory and methodology on dimension reduction for high-dimensional non-regular models which allow for discontinuity with respect to a subset of the parameters or covariates. Such models arise naturally from applications in various fields, such as statistics, biostatistics, climate, marketing research, management, economics and finance. They can capture many important features of the data structure and association between the explanatory and response variables which either low-dimensional or regular models alone cannot duplicate. This proposal focuses primarily on threshold models, an important class of non-regular models which has a wide variety of applications in statistics, biostatistics, and economics. While the literature on threshold models for low-dimensional data is comprehensive, the statistical theory and methods for threshold models applied to high-dimensional data are undeveloped due to four central challenges: (I) statistical nonregularities of the estimation, (II) increasing dimensionality, (III) unknown or incomplete distributions of response variables, (IV) computational difficulties. By introducing penalization techniques, a number of related research topics are proposed for investigation. New tools for statistical inference and computational algorithms of non-regular models applied to large and high-dimensional data, for example the brain imaging data, will be developed.<br/><br/>These new developments will allow scientists to efficiently analyze data with substantially increased flexibility, interpretability and reduced modeling biases. In addition, the investigator will integrate new mathematical, probabilistic and computational tools with those in sciences and engineering. Dissemination of these developments will enhance new knowledge discoveries, and strengthen interdisciplinary collaborations. The research will also serve an educational purpose through multi-disciplinary courses on the contemporary state-of-the-art data mining and machine learning, and benefit the training and learning of undergraduate, graduate students and underrepresented minorities."
"1107225","Short Memory in Long Memory Time Series","DMS","STATISTICS","09/01/2011","08/22/2011","Jaechoul Lee","ID","Boise State University","Standard Grant","Gabor Szekely","08/31/2014","$100,000.00","","jaechoullee@boisestate.edu","1910 University Drive","Boise","ID","837250001","2084261574","MPS","1269","9150","$0.00","Asymptotic properties and inference procedures for long memory processes have been extensively studied in the last 30 years.  However, when a long memory process involves short memory components, statistical inference methods are insufficient and need to be substantially enhanced.  Specifically, if a fractionally integrated autoregressive moving-average (ARFIMA) process contains autoregressive moving-average (ARMA) components, currently applied statistical methods frequently produce biases that result in significant inaccuracies.  Accordingly, there is a need for a more accurate investigation of short memory components pertinent in the ARFIMA process.  This project considers several statistical problems in time series settings where the data has both long memory and short memory characteristics.  The statistical problems considered include: (1) testing to determine if a long memory time series has short memory characteristics; (2) developing stochastic parameter regression models of long memory and short memory characteristics with a simpler autocorrelation structure; and (3) assessing biases in the sample autocorrelations and cross-correlations for long memory time series with short memory characteristics.<br/><br/>Studying long memory time series with short memory components is very important, as they are frequently observed in real-world contexts, such as stock returns and volatilities, inflation rates, temperatures, and river levels.  The project aims to develop accurate statistical models and inference methods to analyze such time series.  The development of this research will: (1) advance the theory and methods of long memory processes; (2) help the public better understand global warming issues with the proposed models and methods; and (3) benefit practitioners to use the research outcomes in their disciplines.  In addition, the investigator will contribute to the launch of Boise State's mathematical and statistical consulting center.  The center will be a hub of applied mathematics and statistics fused with other sciences, serving Boise and the State of Idaho where no such facility is currently available."
"1106854","Studies in Factorial and Response Surface Designs","DMS","STATISTICS","08/15/2011","08/05/2011","Hongquan Xu","CA","University of California-Los Angeles","Standard Grant","Gabor Szekely","12/31/2014","$100,000.00","","hqxu@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","","$0.00","The objectives of this proposal are to develop new design theories and methodology for factor screening and response surface exploration and to construct efficient factorial and composite designs  for practical use.  The first part of this proposal studies fractional factorial designs constructed via quaternary codes.  These designs often have a complex aliasing structure and a novel approach is introduced for finding defining relations among factors.  A general theory is developed for obtaining  important design properties such as resolution and wordlength pattern. Novel methods and algorithms are proposed for constructing optimal designs.  These designs are shown to have better statistical properties than existing ones in the literature in terms of aberration, resolution and projectivity.  The second part studies orthogonal array-based composite designs that consist of a two-level factorial design and a three-level orthogonal array.  These composite designs have many desirable features and are effective for factor screening and response surface modeling.  They can be used in a single experiment or in a sequential experiment.   Efficient composite designs are constructed for practical use and they are shown to be better than central composite designs and other existing designs.   This proposal employs a combination of mathematical, coding-theoretical and computational tools to tackle various important issues such as  design properties and construction methods. <br/><br/>Experimental design and analysis is an effective and commonly used tool in scientific investigations and industrial applications. Factorial designs are cost-efficient experimental plans for identifying important factors from a pool of variables; response surface designs are crucial for understanding a process or system and building empirical models.  Such designs have been successfully used in industrial manufacturing for improving quality and productivity.  Recent novel applications include biomedical experiments conducted at UCLA in order to find effective antiviral drug combinations or drug cocktails for treating herpes simplex virus and vesicular stomatitis virus.  This proposal aims at developing novel methodology for constructing new efficient factorial and response surface designs. The results of the proposed research can be quickly assimilated into undergraduate and graduate courses on design and analysis of experiments for course enrichment.  The proposed methods and designs can be applied in a wide variety of fields of application, including engineering, physical and chemical sciences, medicine and life sciences. The proposed research can lead to better practice in experimentation and help shorten investigation time and reduce experimental cost tremendously."
"1105127","Inference under Selection and Model Uncertainty","DMS","STATISTICS","09/15/2011","09/17/2012","Linda Young","FL","University of Florida","Standard Grant","Gabor Szekely","08/31/2015","$172,353.00","","LJYoung@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","","$0.00","This project has two distinct parts, each suggested by problems of inference in genomic experiments.  The first problem arises because, typically, thousands of genes are screened, and a smaller number are selected for further study.  Statistical inference must take this selection mechanism into account, otherwise the actual confidence coefficient is smaller than the nominal level, and approaches zero as the number of genes increases. The goal is to construct valid frequentist confidence intervals for the means of the selected populations. This will provide a confidence interval alternative to the False Discovery Rate.  The second problem deals with inference under model uncertainty, where the goal is to account for the variability induced by the collection of models.  Here a Bayesian approach is taken, seeking to construct intervals accounting for model uncertainty, investigate the impact of the choice of priors on model space, and construct new search algorithms that take advantage of parallel processing and can be used in the case when there are more covariates than observations.<br/><br/>The work will have impact in both genomic studies and high performance computing.  First, for inference from genomic studies, a valid statistical procedure to screen results will be provided. Insuring that the inferences are valid is of crucial importance, as illustrated by a recent NY Times article where a genomic disease therapy was found to be useless, because of faulty statistical inference (``How Bright Promise in Cancer Testing Fell Apart"", NY Times, July 7, 2011).  Second, parallel processing algorithms, using high performance computing, will be developed. These algorithms take advantage of the abundance of processors typically available, and split the large genomic selection problem across the many processors.  This results in answers from these statistical procedures that can be available in real time, and thus be relevant in a clinical setting."
"1244556","New Theory and Methodology for Large-Scale Multiple Testing","DMS","STATISTICS","10/31/2011","07/15/2012","Wenguang Sun","CA","University of Southern California","Continuing Grant","Gabor Szekely","07/31/2014","$163,344.00","","wenguans@marshall.usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","1269","","$0.00","Large-scale multiple testing is an important and rapidly growing area in modern Statistics. The proposed research focuses on new theories, methodologies and computational algorithms to address the fundamental questions and new challenges in this field. The investigator develops new concepts, data-driven schemes and solid theories that promise to improve the statistical efficiency and lay the foundation for simultaneous inferences in large-scale studies, especially when heterogeneity, dependence and other complex structures are present. The major components of the proposed research include: (i) the concept of simultaneously incorporating statistical significance and effect size in multiple testing and a new approach to identifying large non-null effects in heteroscedastic models; (ii) the strategy of exploiting spatial dependency and a new approach to testing correlated hypotheses in a hidden Markov random field; (iii) the strategy of grouping hypotheses in sets and a new approach to testing the significance of multiple groups of important variables; and (iv) the concepts of discovery boundary and effective screening, and a data-driven approach to reducing dimensionality by constructing subsets that are optimal in size and adaptive to unknown sparsity. <br/><br/>The proposed research has significant impact on many scientific applications such as genome-wide association studies, time-course microarray experiments, disease mapping in environmental studies, climate modeling, and medical imaging studies. The multiple testing and screening methods outlined in the proposal will improve the quality of simultaneous decision-making in complicated situations, yield more interpretable and reproducible scientific results, lead to great savings in costs in large-scale investigations, and hence help achieve the ultimate goal of understanding the underlying mechanisms in complex systems or human diseases in a precise, fast and cost-effective way. User-friendly software will be developed and made freely available for public use. Research results will be disseminated through publications, seminars and workshops. The investigator is committed to encouraging the participation of under-represented groups in science, and to integrating the proposed research into educational activities through developing new courses, and through mentoring and training students to work on the frontiers in Statistics with important health science applications."
"1106570","Bayesian methods for structure detection in analysis of object data","DMS","STATISTICS","06/01/2011","04/17/2013","Subhashis Ghoshal","NC","North Carolina State University","Continuing Grant","Gabor Szekely","05/31/2015","$250,000.00","","subhashis_ghoshal@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","","$0.00","Complex data objects such as images, functional data, random sets, shapes, graphs and trees are nowadays commonly used to represent information present in observations. Bayesian methods are well suited for detecting meaningful structures hidden underneath these complex data sets. The structure may come in the form of sparsity, setting many parameter values to a null value like zero, or by making adjacent values equal, thus effectively reducing the number of parameters to handle. This project will provide definitive guidelines for constructing prior distributions on the parameters controlling the distributions of object data, and for computing the resulting posterior distributions in an efficient manner. The main idea of the research is to use an auxiliary stochastic process to control ties in object data. The project connects diverse concepts such as multi-scale modeling, feature sharing, multiple testing, random geometry, machine learning and nonparametric Bayesian paradigm, and synthesizes these different concepts into a powerful approach for analyzing object data. The project also stimulates the development of computing strategies that exploit structural niceties such as conditional conjugacy, thus providing fast and accurate computational approaches.<br/><br/>This project develops methodologies that will provide a foundation for finding structures in collections of data, with a wide range of applications in astronomy, medical sciences, engineering, finance, and various other fields. The research will help process astronomical images in a more accurate and efficient manner, and thus help identify events in distant supernova remnants and other astronomical bodies.  The research will also have a significant impact in medical imaging, by providing a method of accurate processing of scans of sensitive organs with very little exposure to harmful rays. The project will impact human resource development in the form of graduate student advising. The project will have subprojects that will be suitable topics for undergraduate research projects. Efforts will be made to involve students from under-represented groups to promote diversity."
"1107004","Causal Inference from Two-level Factorial Designs","DMS","STATISTICS","10/01/2011","09/20/2011","Tirthankar Dasgupta","MA","Harvard University","Standard Grant","Gabor Szekely","09/30/2014","$200,000.00","Donald Rubin","tirthankar.dasgupta@rutgers.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","","$0.00","The investigators develop a framework for causal inference from two-level factorial and fractional factorial designs with particular sensitivity to applications to social, behavioral and biomedical sciences. The framework utilizes the concept of potential outcomes that lies at the center stage of causal inference and extends Neyman's repeated sampling approach for estimation of causal effects and randomization tests based on Fisher's sharp null hypothesis to the case of 2-level factorial experiments. The framework allows for statistical inference from a finite population, permits definition and estimation of parameters other than ``average factorial effects'' and leads to more flexible inference procedures than those based on ordinary least squares estimation from a linear model. It also ensures validity of statistical inference when the investigation becomes an observational study in lieu of a randomized factorial experiment due to randomization restrictions.<br/><br/>Factorial designs allow efficient and cost-effective assessments of the relative effects of several factors and their interactions on output variables of interest. Such designs have been successfully applied in several scientific, engineering and industrial endeavors, but not often used in the social, behavioral or biomedical sciences in spite of several potential applications in these fields. The proposed methodology addresses the complications associated with multi-factor experiments in the aforesaid fields and has a wide range of applications. It can be applied, for example, to assess the impact of several new initiatives on high-school education; or to conduct cost-effective clinical trials to study individual and combined effects of different treatments offered to patients suffering from a certain disease; or to identify critical factors that affect yield of complex physical processes in material science like synthesis of nanostructures. It can also be applied to comparative effectiveness research (e.g., in evidence-based medicine)."
"1106432","Right Censorship Model and Doubly-Censorship Model Allowing Dependent Survival Time and Censoring Times","DMS","STATISTICS","07/15/2011","07/15/2011","Qiqing Yu","NY","SUNY at Binghamton","Standard Grant","Gabor Szekely","06/30/2015","$160,001.00","","qyu@math.binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","MPS","1269","","$0.00","The properties of the non-parametric maximum likelihood estimator (NPMLE) of the cumulative distribution function (cdf) of the survival times of a population have been studied for a long time under the standard right censorship (RC) model, which assumes independent censoring. The independent assumption is sometimes not justified in applications.  In this project a new RC model that allows dependent censoring is introduced. The new model is more realistic in many applications including the analysis of survival times of cancer patients in a follow-up study that the investigator and his collaborators are engaged in. Under the new model, the PI plans to establish the asymptotic properties of the NPMLE of the survival function on the real line. New models that allow the dependent censoring for doubly censored data and other types of interval-censored data will also be investigated. <br/><br/>Right censored and doubly-censored data occur frequently in industrial experiments and medical research. The new models proposed by the investigator are motivated by the needs in the cancer research in which the investigator is analyzing certain breast cancer data provided by the Memorial Sloan-Kettering Cancer Center. The results from this project will be useful in the analysis of the data from clinical trials and engineering reliability studies. The theoretical results will contritute to the advancement of the statistical theory of survival analysis."
"1115654","Funding for Graybill 2011 Conference","DMS","STATISTICS","06/01/2011","03/03/2011","Mary Meyer","CO","Colorado State University","Standard Grant","Gabor Szekely","05/31/2012","$20,000.00","Jean Opsomer","mary.meyer@colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1269","7556","$0.00","The statistics department at Colorado State University has hosted the Graybill conference since 2001.     The theme of the conference differs from year to year, but the objective is to bring together researchers, practitioners, and graduate students, in a relaxed and stimulating  atmosphere focused on research and applications.   For the 2011 conference, the theme is ``Modern Nonparametric Methods.''   The program consists of a workshop, invited plenary talks, and a poster session.   The intimate nature of the conference allows for concentrated discussion and interaction among the participants, which can be especially valuable for young researchers and graduate students.   The conference will continue the series of topical conferences co-sponsored by the ASA Section on Nonparametric Statistics initiated in 2007, furthering the goal of establishing a biannual conference tradition on a topic of interest to this important segment of the statistical community.   <br/><br/>The continued development of practical nonparametric and semiparametric methods is crucial to the advancement of modern sciences, medicine, economics, agriculture, environment or global change, health and medicine, and many other fields.  In many disciplines, the increasing availability of large amounts of high quality data has outpaced the methods available to analyze them.  Flexible and robust tools such as modern nonparametric methods are increasingly necessary to address many questions encountered in practice, so that continued research in this area as well as education of future researchers will benefit the statistics discipline as well as the many disciplines that rely on it."
"1212325","Matrix estimation under rank constraints for complete and incomplete noisy data","DMS","STATISTICS","06/30/2011","04/09/2012","Florentina Bunea","NY","Cornell University","Continuing Grant","Gabor Szekely","05/31/2014","$220,263.00","","fb238@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","","$0.00","The central goals of this  proposal are:(a) to provide methods for the estimation of matrices of unknown rank from both completely and incompletely observed noisy matrices, using rank regularized risk minimization and (b) to establish novel oracle type risk bounds for the matrix estimates and the rank estimates, under minimal assumptions. The difficulty of the problem of recovering the underlying target matrix from an observed noisy matrix is that the number of independent parameters is large relative to the number of observations.  Special attention is given to multivariate response regression models. There is an interesting resemblance between matrix estimation under low rank assumptions and estimation in general regression models under sparsity assumptions, but matrix models pose different mathematical and computational challenges.<br/><br/><br/>High dimensional data arranged in matrix format are increasingly common in many scientific disciplines such as genetics, medical imaging, engineering, psychology and neuroscience. The matrices containing observed data in these areas tend to have high rank due to the presence of noise, but the signal matrix underlying the data may have significantly lower rank. Ignoring this in any inferential procedure may lead to poor recovery of the target, with severe repercussions on the interpretation of the results. Instances of targets that must be recovered with the highest possible precision include: faces against background, ensembles of genes that are associated with a disease, brain structures associated with cognitive processes, to name just a few example. Some of the challenges associated with the analysis of such data can be met via the methodological and  theoretical study of  the problem of matrix estimation under rank constraints. A second problem, which is substantially more difficult, is to perform the same task when only partially observed  noisy matrices are  available. Systematic investigation of these two problems is the focus of this proposal. The usefulness of these techniques will be immediately disseminated to the scientific community by applying them to data obtained from a study of the effects of HIV on brain structure and functions. Free software that implements the developed methodology will be made available on the web in a readily implementable form."

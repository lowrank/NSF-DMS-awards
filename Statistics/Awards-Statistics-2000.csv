"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"0071619","Inference in Heteroscedastic Nonlinear Time Series Under Long Memory With Applications to Finance","DMS","STATISTICS, Economics","08/01/2000","05/24/2002","Hira Koul","MI","Michigan State University","Continuing Grant"," Shulamith T. Gross","07/31/2004","$300,000.00","Richard Baillie","koul@stt.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269, 1320","0000, OTHR","$0.00","PROJECT ABSTRACT:<br/><br/>In physical sciences, economics, and finance many realizations of discrete time series exhibit long memory, i.e., their autocovariances as a function of lag decrease to zero at a hyperbolic rate as the lag approaches to infinity.  Such processes have unbounded spectral densities at the origin.  A part of this proposal is concerned with developing asymptotically optimal and robust estimators for heteroscedastic, non-smooth, non-linear time series models in the presence of regression or explanatory covariates that may have long memory, in a semi-parametric setting.  In particular, it is planned to obtain the limits of the experiments generated by the non-smooth autoregressive models when there are long memory explanatory variables present in these models and when the error distributions are unknown.  In the second part, the PI/Co-PI propose to develop asymptotically distribution free tests for fitting a parametric autoregressive mean and/or quantile function to a heteroscedastic stationary ergodic time series.  These tests are expected to be functions of certain martingale transforms of a partial sum processes that do notinvolve nonparametric curve estimation.  PI/Co-PI also plan to carry out a comparative study with some of the existing tests.  The results obtained will be used to estimate parameters of interest and test theories relevant to problems in financial economics.<br/><br/>A data set is said to have long memory if an association between distant observations is slowly decaying but persistent, as the distance between observations increases.  A data set observed over a period of time is called a time series.  A heteroscedastic time series is one where the conditional variability of an observation at the current time, given the past, depends on the past.  Such data often arises in economics, finance, and physical sciences.  In particular, an important example of long memory heteroscedastic time series is  the volatility process in spot returns. It is also known that this volatility increases with bank interventions in currency markets.  This intervention process is highly non-smooth time series since it is zero most of the times with certain bursts over some times.  Part of the emphasis of the proposal is on developing optimal inferential procedures in a class of non-smooth non-linear heteroscedastic time series models.  Another part emphasizes application of the results obtained to develop new tests of market efficiency and estimates of time dependent risk premium in financial economics and high frequency data mentioned in the proposal pertaining to German Mark and Swiss Frank vs. US Dollar exchange rates and commodity prices.  <br/><br/><br/>"
"0071642","Asymptotic Approaches to Bayesian and Likelihood Inference","DMS","STATISTICS, Methodology, Measuremt & Stats","08/15/2000","06/17/2003","Gauri Datta","GA","University of Georgia Research Foundation Inc","Standard Grant"," Shulamith T. Gross","07/31/2004","$67,631.00","","gauri@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269, 1333","0000, OTHR","$0.00","Abstract <br/><br/>Bayes and frequentist approaches are the two main paradigms for statistical inference.  Of late, likelihood-based methods are also being proposed in many inferential problems.  The proposed research falls in the interface and considers asymptotic inference in these paradigms.  Higher order asymptotic expansions play an important role in Bayes, frequentist and likelihood approaches to inference.  This proposal considers developing such expansions and using them in specific applications such as Bayesian experimental design, small area estimation and contingency tables.  Small area estimation is becoming increasingly popular in many federal and local government programs.  Both hierarchical Bayes and empirical Bayes approaches are receiving favorable attention from the users of small area statistics.  Hierarchical Bayes procedures rely to a great extent on the use of noninformative priors.  Indeed, the wider acceptance of Bayesian techniques in recent years both in the theory and in the practice of statistics is partly due to various noninformative priors.  Higher order asymptotics have been used by the PI in his past research to (i) develop second order accurate approximations to measure of uncertainty in small area estimation, (ii) calibrate naive EB confidence intervals, (iii) compare various adjustments to profile likelihoods in likelihood-based inference, and (iv) obtain frequentist validation of various noninformative priors.  Specifically, the following problems will be investigated: (a) Asymptotic comparison of adjusted likelihoods via expected volumes of confidence sets, mean squared errors of point estimates and other criteria; (b) Optimal Bayesian designs in variance components problem; (c) EB interval estimation with applications in small area estimation; (d) Frequentist validation of noninformative priors in the context of small area estimation; (e) Higher order expansion of null distribution of score tests in two-way contingency tables; (f) Robust estimation in small area estimation using survey weights.<br/><br/>Research on small area estimation has received considerable attention in recent years due to growing demand for reliable small area statistics by federal and local government agencies (e.g., the U.S. Census Bureau, U.S. Bureau of Labor Statistics, Statistics Canada, Australian Bureau of Statistics, Central Statistical Office of U.K.).  A small area usually refers to a subgroup of a population from which samples are drawn.  The subgroup may be a geographical region (e.g., county) or a group obtained by cross-classification of demographic factors.  Reliable small area statistics are needed in regional planning and fund allocation in many federal and local government programs.  Currently, the Census Bureau is engaged in developing small area estimates of number of poor children in school-going age at the county level, and developing adjustment factors to the census counts for various geographic and demographic classes.  Experimental designs play an important role in agriculture and industrial productions.  Optimal designs allow experimenters to derive maximum utility for a given budget.  Categorical data, which occur abundantly in every fields of quantitative study, especially in social sciences, consist of frequency of counts in various categories of interest to an experimenter.  Statistical solutions to be developed here are expected to lead to new and useful methodolgies on the research problems considered in this proposal.<br/>"
"0070176","International Biometric Conference - San Francisco","DMS","STATISTICS","06/01/2000","05/04/2000","Lynne Billard","GA","University of Georgia Research Foundation Inc","Standard Grant","William B. Smith","05/31/2001","$12,500.00","","lynne@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","DMS-0070176-Lynne Billard<br/><br/>The International Biometrics Conference is planned for July 2-7, 2000, at<br/>San Francisco.  The meeting will focus on the active areas of research in<br/>biostatistics.  Areas of emphasis are ""advancement of biological science<br/>through the development of quantitative theories and the application,<br/>development, and dissemination of effective mathematical and statistical<br/>techniques"".  The conference participants are concerned with<br/>cross-disciplinary and interdisciplinary interface of the theoretical and<br/>application oriented real-world problems.  In addition to the invited talks,<br/>ample time will be given for small group discussions in which junior<br/>participants will meet with senior mentors and other participants.<br/><br/>The NSF monies requested are to support about fifteen U.S.-based participants<br/>and five non-U.S.-based researchers with emphasis on young researchers<br/>and traditionally underrepresented groups.   Support will be limited to<br/>individuals at academic institutions and non-profit organizations.  <br/>"
"0071383","Periodic ARMA Modeling","DMS","STATISTICS","07/15/2000","07/20/2000","Robert Lund","GA","University of Georgia Research Foundation Inc","Standard Grant","Xuming He","06/30/2004","$76,542.00","","rolund@ucsc.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","Robert B. Lund<br/>Periodic ARMA Modeling, DMS 0071383<br/><br/>Abstract<br/><br/>Periodicities naturally arise in data involving tides, climatology,<br/>ecology, sociology, astronomy, economics, hydrology, etc..  This research<br/>studies general periodic time series modeling methods along with some<br/>applications to climatological problems.  The specific questions examined<br/>here include efficient computation of the autocovariance structure of<br/>periodic time series models, parsimonious periodic time series model<br/>development, changepoint and homogeneity methods, and trend properties in<br/>monthly average and extreme temperatures.  The mathematical and<br/>statistical methods center on periodic autoregressive moving average<br/>models (PARMA) time series models - the fundamental modeling vehicle for<br/>periodic series.  An efficient algorithm to compute the autocovariance<br/>structure of a PARMA model will be explored.  Changepoint and homogeneity<br/>methods for periodic series, an issue that arises when weather recording<br/>stations move, will also be investigated. <br/><br/>The developed mathematical and statistical results are used to settle<br/>some climatological problems.  A study of the trends in monthly<br/>temperature means and extremes from United States stations will be<br/>conducted.  This detailed study will enhance our understanding of<br/>climate change and global warming.  The changepoint aspect is perhaps<br/>the data's most important feature.  Periodicities in the autocovariances<br/>in the data need to be parsimoniously modeled to compute accurate<br/>standard errors.  Standard errors are needed to gauge the reliability of<br/>the computed trends.  The methods also yield improved forecasts of<br/>periodic series as a byproduct.<br/><br/><br/><br/><br/>"
"0072635","Semiparametric and Nonparametric Inferences","DMS","STATISTICS","07/01/2000","07/10/2000","Xiaotong Shen","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","Marianthi Markatou","06/30/2003","$74,646.00","","xshen@umn.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","Semiparametric and nonparametric inferences <br/><br/>Most scientific problems involving data and many medical research problems can be viewed as an investigation into the association between a response variable and a number of potential explanatory variables. Regression analysis, wavelet techniques, hierarchical Bayesian analysis, and neural networks are included in the set of tools that can be brought to bear on such problems. Recent advances in computer technology make it easier to collect large data sets, which in turn demand more complex scientific theories. Hence high-dimensional semiparametric and nonparametric techniques become powerful tools for analyzing data. <br/><br/>  This research concerns several related statistical problems ranging from processing signals to analyzing survival data. The research includes the further development of methodologies and computational tools for model fitting and inference in two areas. In the first area, the research develops new techniques of random-sieve methodology and deterministic sieve methodology, each of which expands existing techniques in novel ways. In the second area, the research studies foundational issues of the connection between semiparametric and nonparametric Bayesian and maximum likelihood inference, and constructs confidence bands andintervals for function inference. The areas of application include survival analysis and signal processing. The knowledge gained in this investigation is expected to bring great benefit to many other statistical areas and will be useful in a variety of complex scientific problems. <br/><br/>"
"0092659","Mathematical Methods for Small Sample Biostatistical Inference","DMS","STATISTICS","09/01/2000","08/29/2000","John Kolassa","NJ","Rutgers University New Brunswick","Standard Grant"," Shulamith T. Gross","08/31/2004","$125,000.00","","kolassa@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","A wide variety of techniques exist for conditional inference on exponential families arising from discrete distributions.  Normal theory methods, which rely on the approximate multivariate normality of the joint distribution of summary statistics from the data set, are often inaccurate for small data sets, and their quality can often be poor for summaries that indicate large parameter effects.  They also ignore discreteness in the data.  More sophisticated approximation techniques, known as saddlepoint techniques, are often used in cases when normal theory methods are inadequate.  These techniques often do not account for discreteness in data, and hence are suboptimal in their unmodified forms.  Exact inferential techniques are also available, but these techniques apply only to a limited number of models, require proprietary software, and fail when sample size reaches a moderate size.  Extensions to this software that employ Monte Carlo techniques for larger sample sizes are not yet commercially available.  These Monte Carlo techniques have the further disadvantage of delivering a variety of results for the same data set.  The techniques proposed use saddlepoint approximations in a way that accounts for discreteness in the data while avoiding most of the computationally intractable aspects of exact calculations.  Some of the projects proposed in this grant application involve new approximations, such as for approximating higher--dimensional distribution functions, and others involve modifications to existing approximations to avoid numerical instabilities.  Other projects involve formulating confidence regions to make accurate calibration easy, and modifying the conditioning event to obtain a more powerful analysis, and performing diagnostics to ensure that the proper approximations are used.  These methods will be general enough to apply to any canonical exponential family supported on a lattice, and hence to any generalized linear model with canonical link, observations supported on a lattice, and design matrix whose entries are confined to a lattice.  Examples of models that will be accommodated are logistic regression, Poisson regression including log linear models for contingency tables, and multinomial models.  Regression models with more exotic error structures, including positive Poisson and negative binomial distributions, will also be accommodated.<br/><br/>This proposed research is intended to aid in statistical inference on multiple parameters, in the presence of other nuisance parameters that are not of direct interest, when the distribution modeled is discrete.  For example, the probability that a cancer patient will stay in remission can be modeled as a function of a variety of factors.  Some of these effects, like which treatment a patient received or whether the patient had other cancer--related pathologies, may generalize to other populations, and others, like the effect of a particular center where the patient was treated, may not generalize.  Thus one might be interested in describing the possible values that the parameters of interested take on, without being required to simultaneously estimate the remaining parameters.  Typically one treats information associated with nuisance parameters as held fixed, and performs inference conditionally on this information.  That is, one assesses the the evidence concerning the parameter of interest by comparing experimental results to the population of possible results such that the information about nuisance parameters is held fixed.  The research agenda proposed here presents methods for doing these calculations, which balance high computational costs of exact methods against potential inaccuracies of approximations, and introduces and combines new methods for both<br/>exact and approximate calculations.  These new methods will make the analysis of small discrete data sets, commonly occurring in applied sciences, quicker and more accurate."
"0072571","Resampling Methods for Temporal and Spatial Processes","DMS","STATISTICS","08/15/2000","07/03/2002","Soumendra Lahiri","IA","Iowa State University","Continuing Grant","Xuming He","05/31/2004","$142,630.00","","s.lahiri@wustl.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","0000, OTHR","$0.00","ABSTRACT<br/><br/>The project consists of two parts, viz., (1) Developing a class bootstrap methods, called the  ""Transform Based Bootstrap"", for long-range dependent data and studying their properties; and (2) Developing a class resampling methods, called the ""Varying Probability Spatial Block Bootstrap"" and ""Varying Probability Spatial Subsampling"" for spatial data under some  (nonstandard) spatial sampling designs.  Although a number of resampling methods have been proposed and shown to be effective in dealing with weak dependence in time series data, an  earlier work of the PI reveals that these methods have only limited success under long range dependence. Since long-range dependent data appear naturally and frequently in many scientific studies (cf. Kuensch, H.R., Beran, J., and Hampel, F. (1993; Annals of Statistics)), developing effective resampling methods for such data is important. The transform-based-bootstrap, proposed here, holds some promise.  The other part of the project deals with spatial data.  Unlike the time-series case where the random process evolves only in one direction, processes with a continuous spatial index allow for more than one evolution pattern. This leads to different types of (often nonstandard) asymptotics for spatial data.  The proposed project seeks to develop new resampling methods for spatial data in such cases, particularly for irregularly spaced data-sites.<br/><br/>The emphasis of the proposed project is on development of suitable resampling methods for time-series and spatial data having a complex structure and on investigating their properties. Current statistical methodology for dependent data are predominantly parametric model based and are sensitive to model misspecification. The proposed research seeks to address this need and is aimed at removing some of the limitations of the current methodology.<br/>"
"0071818","Collaborative Research on Graphical Markov Models and Related Topics in Multivariate Statistical Analysis","DMS","STATISTICS","08/15/2000","05/28/2002","Michael Perlman","WA","University of Washington","Continuing Grant","John Stufken","07/31/2003","$156,000.00","","michael@ms.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, OTHR","$0.00","Perlman 0071818<br/>Andersson 0071920<br/><br/>Abstract<br/><br/>Statistical models based on acyclic directed graphs (ADGs) (also called directed acyclic graphs (DAGs), Bayesian networks, or influence diagrams) are particularly well behaved, easily interpretable, and computationally convenient.  In the late 1980s, ADG models were generalized to adicyclic graphs or chain graphs, which include both directed and undirected edges, hence can simultaneously represent dependences some of which are directional and some associative.  The investigators will study the Markov and statistical properties of a new class of chain graph models that retains many of the desirable properties of ADG models.  Problems to be investigated include the completeness and faithfulness of these new models, determination of their local Markov property, and characterization of their Markov equivalence classes by means of an appropriate essential graph.  The investigators will also study a very general class of Wishart distributions on homogeneous cones, in which transitive acyclic directed graphs (TADGs) play a central role. E. B. Vinberg's classical characterization of homogeneous cones has been found to reveal a fundamental relationship between normal models satisfying TADG Markov conditions and this general class of Wishart distributions.  This class includes all Wishart distributions previously known in multivariate statistical analysis, including the hyper-Wishart distributions and Wishart distributions associated with normal lattice conditional independence (LCI) models, as well as a great many new ones.  Additional topics to be investigated include the limitations of the Neyman-Pearson, likelihood ratio, and maximum likelihood criteria for multiparameter hypothesis-testing and estimation problems, and the efficacy of the likelihood ratio test for testing order-restricted and multivariate one-sided alternatives.<br/><br/>One of the most central ideas of statistical science is the assessment of dependences among a set of stochastic variables.  The familiar concepts of correlation, regression, and prediction are manifestations of this idea, and many aspects of causal relationships ultimately rest on representations of multivariate dependence.  Graphical Markov models  (GMM) use graphs i.e. networks, either undirected, directed, or mixed, to represent multivariate dependencies in a visual and computationally efficient manner.  A GMM is usually constructed by specifying local dependences for each variable, i.e. node of the graph, in terms of its immediate neighbors, parents, or both, yet can represent a highly varied and complex system of multivariate dependences by means of the global structure of the graph.  The local specification permits efficiencies in modeling, inference, and probabilistic calculations.  Among their many applications, GMMs have become prevalent in statistical science for the analysis of categorical data in contingency tables, for the modeling of spatially-dependent processes such as the spread of epidemics in human and animal populations, and for the development of early warning systems for severe weather conditions; in computer science (as Bayesian networks) for information processing and retrieval, for robotics, computer vision, and pattern recognition, for the debugging of complex programs (such as Windows 98), and for the representation of expert systems for medical diagnosis; and in decision science (as influence diagrams) as models for information flow and control and for combining the opinions of many decision-makers.  A crucial feature of these models is that they are designed for fast computational implementation, thereby facilitating the development of software that can ""reason"" about real world problems.<br/>"
"0084378","Nonstationary Spatial Modeling for Multiple Point Sources, with Applications to Enviromnental Data","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","10/01/2000","09/20/2000","Jacqueline Hughes-Oliver","NC","North Carolina State University","Standard Grant"," Shulamith T. Gross","09/30/2004","$200,000.00","Sujit Ghosh","hughesol@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1253, 1269","1386, 9188, EGCH","$0.00","This research develops, assesses, and provides convenient tools for implementing parametric modeling of processes that are driven by point sources. The project uses a hierarchical Bayesian approach to an extension of a process decomposition model that was recently introduced in the context of modeling the effect of point sources.  The process decomposition model decomposes the observed process into a trend surface, a baseline error process, and additional error processes (one for each point source) that may be viewed as shocks to the baseline. This approach allows flexibility and autonomy to modeling the individual sources. The Bayesian approach properly accounts for uncertainty in parameter estimates when evaluating prediction uncertainty, easily incorporates prior information, and is more flexible for assigning ranks according to the impact of several point sources. A Markov random field or conditional autoregressive model based on a distance-to-source neighborhood structure is used for the error processes. A multivariate distribution of weights describes the relative ranks of the sources.  Because all models are parametric, significance testing is easily accomplished. The project accomplishes the following technical goals: A. test the impact of several point sources, where exact locations of the point sources are known; B. rank several point sources according to impact; C. provide models that are useful for determining appropriate corrective action for altering an observed process or for optimizing a designed process; D. provide predictions, and appropriate measures of prediction uncertainties, at unsampled sites by accounting for the effects of point sources.  The educational component of the project involves teaching high school students the rudiments of the methodology developed and its relevance to major issues such as environmental equity.<br/><br/>The Clean Air Act of 1970 and its 1977 and 1990 amendments defined a pollution source as ""any place or object from which pollutants are released. A source can be a power plant, factory, dry cleaning business, gas station or farm. Cars, trucks and other motor vehicles are sources, and consumer products and machines used in industry can be sources too.""  The effects of these sources on human health are well-documented, as is the importance of modeling these effects. This project responds to this need by developing, assessing, and providing convenient tools for modeling processes that are driven by point sources. The resulting methodology is applied to address a public health and welfare concern of the Environmental Protection Agency, namely to determine which nitrogen oxide (NOx) and sulfur dioxide (SO2) emission sites are most responsible for site-specific ambient concentrations far from the emission sites. These site-specific ranks will help determine which emission sites require stricter regulations for controlling their impact at different spatial locations. More generally, project results may impact the regulation of point sources, the assignment of equitable consequences to several point sources within the vicinity of an ecological or environmental disaster, and help in determining appropriate corrective actions. Furthermore, the creation of teaching modules aimed at high school students will improve the readiness of these students for studies in the mathematical, physical, and biological sciences. <br/><br/>This project is jointly supported by the Statistics Program in the Division of Mathematical Sciences and the Office of Multidisciplinary Activities in MPS.<br/>"
"0072910","Computationally Aggressive Approaches to Adaptive Design","DMS","STATISTICS, ADVANCED COMP RESEARCH PROGRAM","09/01/2000","10/17/2003","Quentin Stout","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Xuming He","08/31/2004","$246,980.00","Janis Hardwick","qstout@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269, 4080","0000, OTHR","$0.00","  Abstract<br/><br/>This project involves interdisciplinary research in which algorithmic<br/>approaches are developed to design and analyze adaptive experiments. An<br/>adaptive (sequential) design is one whose characteristics change in<br/>accordance with information arising from the ongoing experiment, as <br/>opposed to classical statistical designs where such characteristics are<br/>set in advance and remain fixed throughout. Adaptive designs have a wide<br/>range of application in clinical trials, destructive testing, behavioral<br/>ecology, computer performance prediction, adaptive control, etc., where<br/>they have the potential to reduce the expenditure of experimental <br/>``resources'' such as time, money, or quality of life. Unfortunately,<br/>adaptive designs are difficult to analyze and optimize. Exact analytic<br/>solutions are rarely available, and thus, historically, such designs have<br/>been predominantly approached via asymptotic methods and ad hoc<br/>approximations. Computationally, adaptive designs require significant<br/>time and space that has often made exact calculations infeasible.<br/><br/>This project will expand the size and scope of solvable problems by<br/>developing new computational approaches for creating and evaluating<br/>designs and utilizing state of the art computational facilities.<br/>Attention is directed to problems that are important in applications,<br/>with a major emphasis on supplying researchers greater flexibility in<br/>modeling their statistical and cost objectives. For many of these<br/>problems, exact optimality will be unattainable, and thus techniques for<br/>producing near-optimal designs will also be pursued. Several of these<br/>techniques are based on optimizing smaller or simpler problems and<br/>extrapolating their solution structure to larger or more complex<br/>problems. This compliments analytical, asymptotic work and provides new<br/>insights into the structure of solutions. In other cases, a shift from<br/>serial algorithms to parallel ones will be used to address the<br/>additional complexity."
"0072489","Design and Analysis of Experiments for Screening, Optimization and Robustness","DMS","STATISTICS, PRODUCTION SYSTEMS, MANFG ENTERPRISE SYSTEMS","09/01/2000","06/23/2003","C. F. Jeff Wu","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant"," Shulamith T. Gross","04/30/2004","$300,000.00","","jeffwu@isye.gatech.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269, 1465, 1786","0000, 9147, MANU, OTHR","$0.00","Abstract:<br/><br/>The goal of this proposal is to study three important aspects of experimentation: screening, optimization and robustness. Section I proposes a novel approach to factor screening and response surface exploration by using a single design and experiment to achieve both objectives. This differs from the standard response surface methodology, which employs separate designs for factor screening and for response surface exploration.  New concepts, theory and analysis are proposed, which include a two-stage analysis and a projection-efficiency criterion.  Four problems are to be studied: (i) a theory for eligible projections in regular designs, (ii) combinatorial and algorithmic construction of optimal nonregular designs, (iii) connection with the maximum estimation capacity criterion, (iv) sensitivity of response surface exploration to errors in factor screening and a Bayesian alternative to the two-stage analysis. Section II addresses a fundamental and practically important issue of optimal assignment of factors to columns of a design matrix. Existing work can only be applied to regular fractional factorial designs and nonregular designs with two-level factors. By defining a B-contamination criterion and employing the Kronecker calculus, we propose an approach that can handle very general designs.  Three problems are to be studied: (i) Finding expressions for the contamination terms, (ii) characterization in terms of complementary designs, (iii) extensions to blocked designs. Section III addresses the issue of optimal selection of experimental plans for robust parameter design.  When the experimental cost is proportional to the total run size, the cross array format can be quite costly and the single array format becomes an attractive option.  An important question is how to select single arrays optimally and according to what criteria? By using an effect ordering principle, we propose to define new criteria and use them to select optimal single arrays.    <br/><br/>Statistical design and analysis of experiments is an effective and commonly used tool in scientific and engineering investigation. It has made significant impact in many areas of research and development such as manufacturing, electronics, materials, agriculture and energy. It will continue to make important contributions by innovation in methodological and theoretical development and applications in new areas such as biotechnology, drug discovery, and information technology. Potential gains from using the proposed new methods include savings in experimental runs, experimentation time, and discovery of new/better engineering designs and products. The results on factor assignment will provide clear guidelines on the assignment of factors and a substantial improvement over the prevailing practice of making arbitrary and often suboptimal assignment.  Parameter design has become a major tool for variation reduction and product and process improvement. The proposed work will develop new and more economical and efficient techniques for conducting such experiments. <br/><br/> <br/>"
"0004173","Statistical Modeling in Wavelet Domain with Application in Turbulence","DMS","STATISTICS","09/15/2000","09/21/2000","Marianna Pensky","FL","The University of Central Florida Board of Trustees","Standard Grant"," Shulamith T. Gross","08/31/2004","$60,000.00","","Marianna.Pensky@ucf.edu","4000 CENTRAL FLORIDA BLVD","ORLANDO","FL","328168005","4078230387","MPS","1269","0000, OTHR","$0.00","This collaborative proposal involves development of statistical concepts, theories and methods for multiresolution models and analyses of measurements and structures in atmospheric turbulence.  It draws on recent developments in  Bayesian multiscale modeling to understand the time-scale aspects of turbulence where the notions of scale and hierarchy are intrinsic.  Multiresolution statistical modeling approaches are applied to vast geophysical measurements arising from air quality field experiments and simulations in order to identify key structural properties of atmospheric turbulence responsible for the transport of scalars, such as ozone.  To achieve these objectives a team of researchers with expertise in statistical modeling and  multiscale methods (M. Pensky and B. Vidakovic) and measurement and modeling of atmospheric turbulence (G. Katul) are jointly collaborating together.<br/><br/>The aplicability of this project is improved fundamental understanding of atmospheric transport via novel statistical techniques.  Atmospheric transport, key to describing air-quality, is a complex phenomena involving modeling of turbulence which is responsible for the dispersion of polutants.  The applicability of the proposed methodology will be directly tested on high frequency  velocity, temperature, and ozone concentration  measurements collected at Duke Forest, Durham, North Carolina. <br/><br/>Partial support for this award is provided by the Physical Meteorology Program in the Division of Atmospheric Sciences.<br/>"
"0071832","Statistical Classification","DMS","STATISTICS, Methodology, Measuremt & Stats","07/01/2000","06/13/2002","John Hartigan","CT","Yale University","Continuing Grant"," Shulamith T. Gross","06/30/2004","$198,321.00","","hartigan@stat.yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269, 1333","0000, OTHR","$0.00","This proposal considers fundamental statistical research into the theories governing clustering of multivariate objects and variables, and the interrelation of classification and probability in prediction and explanation.  The emphases will be on classification probabilities, probability matching, probability searching algorithms, topological and block clustering, and multimodality.<br/><br/>Applications of these techniques include grouping and ultimate identification of biological entities, the analysis of block voting behavior and the determination of multimodal collections in large databases.<br/>"
"0010130","Nonparametrics in Large, Multidimensional Data Mining","DMS","STATISTICS","12/15/2000","12/13/2000","William Schucany","TX","Southern Methodist University","Standard Grant","John Stufken","05/31/2001","$10,000.00","","schucany@mail.smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1269","0000, OTHR","$0.00","Abstract:<br/>DMS-0010130 PI: William Schucany<br/><br/>This award provides funds to partially support the conference Nonparametrics<br/>in Large, Multidimensional Data Mining. The money is used to support junior<br/>researchers and Ph.D. students who will attend the conference. Women and<br/>minority young investigators and Ph.D. students will be actively recruited<br/>and encouraged to apply. The two-day conference will be held at Southern<br/>Methodist University, Dallas, Texas, on January 12-13, 2001, and will bring <br/>together a group of experts from industry, computer science and statistics to discuss<br/>cutting-edge research activities in the areas of data mining and knowledge<br/>discovery in databases. This holistic approach is necessary to push the<br/>research frontiers in these important disciplines. Leading researchers will<br/>present new nonparametric developments in an environment that is conducive<br/>to the development of new human resources.<br/>"
"0004131","Bayesian Modeling in the Wavelet Domain with Applications in Atmospheric Turbulence","DMS","STATISTICS, PHYSICAL METEOROLOGY","09/15/2000","09/21/2000","Brani Vidakovic","GA","Georgia Tech Research Corporation","Standard Grant"," Shulamith T. Gross","08/31/2004","$107,397.00","","brani@tamu.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269, 1522","0000, OTHR","$0.00","This proposal builds on a previous NSF sponsored project DMS-9626159, awarded to Duke University.  It involves development of statistical concepts, theories and methods for multiresolution models and analyses of measurements and structures in atmospheric turbulence.  It draws on recent developments in  Bayesian multiscale modeling to understand the time-scale aspects of turbulence where the notions of scale and hierarchy are intrinsic.  Multiresolution statistical modeling approaches are applied to vast geophysical measurements arising  from air quality field experiments and simulations in order to identify key structural properties of atmospheric turbulence responsible for the transport of scalars, such as ozone.  To achieve these objectives a team of researchers with expertise in statistical modeling and  multiscale methods (M. Pensky and B. Vidakovic) and measurement and modeling of atmospheric turbulence (G. Katul) is assembled.<br/><br/>The aplicability of this project is improved fundamental understanding of atmospheric transport via novel statistical techniques.  Atmospheric transport, key to describing air-quality, is a complex phenomena involving modeling of turbulence which is responsible for the dispersion of polutants.  The applicability of the proposed methodology will be directly tested on high frequency  velocity, temperature, and ozone concentration  measurements collected at Duke Forest, Durham, North Carolina. <br/><br/>Partial support for this award is provided by the Physical Meterology Program in the Division of Atmospheric Sciences."
"0072526","Nonparametric Bayesian Modelling","DMS","STATISTICS","07/01/2000","08/19/2004","Steven MacEachern","OH","Ohio State University Research Foundation -DO NOT USE","Continuing Grant","Xuming He","10/31/2004","$88,046.00","","snm@stat.osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","<br/>Nonparametric Bayesian Modelling<br/><br/>Nonparametric Bayesian models are motivated by the desire to more realistically<br/>model data.  They provide a means of escaping the strictures of parametric <br/>models, while, with their Bayesian nature, they allow the incorporation of some<br/>information about the process under investigation.  These philosophical <br/>advantages translate directly into superior performance for the models, which<br/>have had a particularly strong showing wherever random effects are involved.<br/>When these models are used in conjunction with the hierarchical model, they <br/>constitute a powerful modelling tool.  <br/><br/>The current state of the art in nonparametric Bayesian modelling allows one to <br/>propose and fit models, and a limited amount of work has been done on selection<br/>of a model from some small set of candidate models.  The greater field of data<br/>analysis in this context is almost untouched.  The reason for this is that the <br/>current set of models do not allow for a sophisticated data analysis:  the <br/>iterative process of proposing a model, assessing its fit, developing a <br/>modification of the model to improve its fit, and reducing the model if no <br/>substantial improvement in fit is found.  The main focus of this work is to <br/>formulate and implement sophisticated nonparametric Bayesian data analysis.  <br/><br/>To accomplish the above goal, this research proposal identifies five areas <br/>where nonparametric Bayesian models currently either perform poorly or cannot<br/>be used in a satisfactory, general fashion.  These areas are (i) examination<br/>of the relationship between covariates and response (ii) assessment of the fit<br/>of a model (iii) combination of information from related experiments (iv)<br/>modelling distributions with outliers and (v) working with small to moderate<br/>sample sizes.  <br/><br/>In the proposed research, the author will develop models that will show strong<br/>performance for each of the above problems.  The first class of models, <br/>dependent nonparametric processes, move beyond current models which are aimed<br/>at providing a description of a single random distribution.  They provide a <br/>means of modelling a collection of random distributions which exhibit strong<br/>local dependence and long-range independence.  This feature of the model makes <br/>it ideal for (i) - (iii) above.  The second class of models, contaminated <br/>models, directly targets the departure from a parametric model in a clear, <br/>easily interpretable fashion.  This type of model is ideal for describing <br/>distributions that contain outliers, and its close proximity to a parametric <br/>form will yield small sample performance that is nearly equivalent to the <br/>parametric model, providing an approach to problems (iv) and (v).  Additionally,<br/>the two types of models may be freely combined, resulting in a single, coherent <br/>approach to all five problems.  The basic theoretical properties of the models<br/>will be investigated, the computational strategies needed to fit the models<br/>will be devised, and the models will be applied in a variety of settings.  <br/>Throughout, the emphasis of the research will be on the development of sound<br/>data analytic strategies so that the theoretical and practical advantages<br/>of nonparametric Bayesian modelling can be realized.  <br/>"
"0071976","Finite Sample Performance of Multivariate Location and Scatter Estimators","DMS","STATISTICS","06/15/2000","04/12/2001","Yijun Zuo","AZ","Arizona State University","Continuing Grant","Marianthi Markatou","04/30/2003","$54,270.00","","zuo@msu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1269","0000, OTHR","$0.00","TITLE:<br/><br/>Finite Sample Performance of Multivariate Location <br/>and Scatter Estimators<br/><br/>ABSTRACT:<br/><br/>This research develops new methodology and theory in the <br/>performance evaluation of multivariate location and scatter <br/>estimators. The research is to propose finite sample performance<br/>criteria of multivariate location and scatter estimators based on <br/>their ``tail behavior'', to investigate and assess the performance <br/>of existing and new multivariate location and scatter estimators <br/>with respect to the proposed criteria, and to provide guides to <br/>statistical practices in applications. <br/> <br/>Given an estimator of some unknown parameter, a natural question<br/>is -- how good is the estimator? or how should one measure its <br/>performance? These questions are fundamental in statistical <br/>estimation and inference. To answer these questions, various <br/>performance criteria have been proposed and studied. Among the <br/>existing performance criteria, asymptotic approaches including <br/>Fisher consistency and large sample normality are the most prevalent <br/>ones. These approaches have been based on the behavior of the <br/>estimator as the sample size approaches infinity. This, however, <br/>raises some serious concern about the relevancy of the asymptotic <br/>results in practice, where the sample size is always fixed and <br/>finite. The study of the finite sample performance of multivariate <br/>location and scatter estimators thus is not only practically <br/>significant but also theoretically interesting. In sharp contrast <br/>with the asymptotic approaches, in this research the performance <br/>of multivariate location and scatter estimators is studied for <br/>fixed and finite sample size.   <br/><br/>The research develops new methodology and valuable insights <br/>into the comparison of estimator performance particularly from <br/>robustness standpoint. Inherent connections of our finite sample <br/>performance measures with two most crucial and promising notions <br/>in the robust and nonparametric statistical analysis and inference, <br/>the finite sample breakdown point and the data depth (especially<br/>Tukey-Donoho halfspace depth), are to be explored and illuminated. <br/>It is to be shown that the estimators with high breakdown point or <br/>halfspace depth possess remarkable finite sample tail performance. <br/>Findings like this in the research offer new insights into and deepen <br/>our understanding of the notions of breakdown point and halfspace depth, <br/>and establish the important role of the finite sample tail behavior as a <br/>quantitative assessment of robustness of estimators. The research has <br/>profound impact on statistical applications in various disciplines of <br/>sciences such as social, behavior and life sciences, environmental and <br/>biology sciences, industry, and economics. The research suggests, <br/>for example, that location estimators with appealing tail performance <br/>(e.g. depth-based multivariate medians) should be preferred in practice<br/>to traditional least-squares estimators (e.g. the multivariate mean) <br/>for robustness against the influence of extremities of observations in <br/>multivariate data analysis. The research also benefits education through <br/>the training of graduate students and the incorporation of the developed <br/>methodology in statistics courses.<br/>"
"0073601","Collaborative Research:  Sequential Monte Carlo Methods and Their Applications","DMS","STATISTICS","09/15/2000","05/28/2002","Rong Chen","IL","University of Illinois at Chicago","Continuing Grant"," Shulamith T. Gross","08/31/2003","$208,750.00","","rongchen@stat.rutgers.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","0000, 1616, OTHR","$0.00","Collaborative Research: Sequential Monte Carlo Methods and Their Applications<br/><br/>Jun Liu, Harvard University <br/>Rong Chen, Univ. Illinois at Chicago<br/>Xiaodong Wang, Texas A&M University<br/><br/>Abstract (Technical):<br/>Sequential Monte Carlo (SMC) methodology recently emerged in statistics and engineering fields promises to solve a wide class of nonlinear filtering and optimization problems, opening up new frontiers for cross-fertilization between statistics and many areas of applications. A distinctive feature of SMC is its ability to adapt to the dynamics of the underlying stochastic systems via recursive simulation of the variables involved.  Although special forms of SMC date back to 1950s, the general use of the method appeared only recently and its many key properties have yet been well understood.  This research group will focus on three major theoretical issues regarding the design of effective SMC-based computational tools and three important application areas, namely, wireless communications, computational biology, and business data analysis.  In the theory part, they will study approaches of generating better Monte Carlo samples for tracking system dynamics; investigate roles of resampling which is critical to the effectiveness of SMC; and propose system reconfiguration strategies for more efficient SMC algorithms.  In the application part, they plan to design novel signal processing and network control algorithms for wireless multimedia communications; develop better multiple sequence alignment models and SMC-based optimization method for protein structures; and build SMC-based modeling and analysis tools for business data. It is anticipated that the proposed research will culminate in the formulation of novel SMC methodologies and will bring the promise of the SMC paradigm into the practical arena of many emerging applications.<br/><br/>Stochastic dynamic systems are routinely used in many application fields such as automatic control, engineering, and finance.  The statistical analyses of these systems are crucial.  However, except for a few special cases, quantitative analyses of these systems still present major challenges to researchers.  Sequential Monte Carlo (SMC) technique recently emerged in the field of statistics and engineering shows a great promise on solving a wide class of nonlinear filtering, prediction, and optimization problems, providing us with many exciting new research opportunities.  The name ""Monte Carlo"" was coined in 1940s by scientists involved in designing atomic bombs and it refers to a technique in which computer is used to simulate and study a complex stochastic system. The technique was named after the famed gambling resort because its procedures incorporate the element of chance. A distinctive feature of SMC is its ability to sequentially simulate the system by considering one variable at a time.  The general use of SMC appeared recently and its invasion into many fields of science and engineering has just begun. Researchers including people in this research group have demonstrated that SMC can be successfully adapted to solve chemistry, engineering, and statistical problems. Understanding its theoretical properties and extending the use of SMC to other fields are the main focuses of this project. More specifically, this research group will focuse on three major theoretical issues regarding the design of effective SMC-based computational tools and three important application areas including wireless communications, computational biology, and business data analysis. These applications are not only important by their own merits, but also essential as the test ground for the new theories being developed and as the sources of stimulation for new research directions for SMC.  It is anticipated that this research will culminate in the formulation of novel SMC methodologies and will bring the promise of the SMC paradigm into the  practical arena of many emerging applications.  In particular, this research will bear fruits in the following areas: novel designs of signal processing and network control algorithms for wireless multimedia communications; developments of better algorithms analyzing biological sequence and structure data; and a SMC-based tool for business data analysis and prediction.<br/>"
"0072578","Block Thresholding Methods for Adaptive Wavelet Function Estimation:  Theory and Applications","DMS","STATISTICS","08/15/2000","08/07/2000","T. Tony Cai","IN","Purdue Research Foundation","Standard Grant","Marianthi Markatou","03/31/2002","$81,063.00","","tcai@wharton.upenn.edu","1281 WIN HENTSCHEL BLVD","WEST LAFAYETTE","IN","479064182","3174946200","MPS","1269","0000, OTHR","$0.00","This research studies two interrelated function estimation problems, <br/>nonparametric regression and linear inverse problems, using wavelet <br/>methods via the approach of block thresholding and ideal adaptation <br/>with oracle. The goals are to build a bridge between the traditional <br/>multivariate normal decision theory and the adaptive wavelet function<br/>estimation, and to develop a family of estimators that achieve <br/>simultaneously three objectives: adaptivity, spatial adaptivity, <br/>and computational efficiency. A major innovation and a consistent <br/>theme throughout the research is the use of block shrinkage<br/>methods which include the standard term-by-term thresholding as a<br/>special case. Block thresholding is studied via the approach of<br/>ideal adaptation with oracle. It will be demonstrated that block <br/>thresholding serves as a bridge between the classical normal decision <br/>theory and adaptive wavelet function estimation.  This leads to a <br/>systematic way of developing a coherent set of rate-optimal estimators<br/>with good empirical performance, all of which may be useful in different<br/>estimation problems.  <br/><br/>To fully understand why block thresholding works ``better''<br/>than the standard term-by-term thresholding, and more generally,<br/>separable rules, I will explore the connection between adaptability<br/>and information-pooling in general orthogonal series estimation, of<br/>which wavelets are a special case. Preliminary results show that<br/>separable rules lack adaptability; they are necessarily not fully <br/>rate-adaptive. A key to adaptively achieve the exact minimax rate is<br/>information-pooling. I will further carry out research in this topic<br/>and will derive a lower bound on the amount of information-pooling<br/>required for achieving full global adaptivity. These results together <br/>will offer a deeper understanding of the benefit of information-pooling <br/>in nonparametric function estimation, and also serve as a guide for<br/>the construction of fully adaptive estimators. Besides theoretical<br/>investigation, I am also interested in applications of the wavelet <br/>methods. I am collaborating with colleagues on using wavelet methods <br/>for archiving and retrieval of medical images from tomographic databases. <br/><br/>"
"0072523","Detection, Estimation and Optimization Problems in Stochastic Systems, Genetics and Economics","DMS","STATISTICS","07/01/2000","02/28/2002","Tze Lai","CA","Stanford University","Continuing Grant","Marianthi Markatou","06/30/2003","$450,714.00","David Siegmund","lait@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","Abstract: Statistical Problems in Quality control, Stochastic Systems and Genetic Analysis<br/><br/>This project will address a number of related statistical problems having applications in (i) industrial quality control and complex engineering systems, (ii) in molecular biology and genetics, and (iii) in financial economics.  (i) One objective of the proposed research is to develop a unified methodology of sequential change-point detection in industrial quality control and of automated fault detection in complex engineering systems.   Relatively simple algorithms that are not too demanding in computational and memory requirements for on-line implementation and yet are nearly optimal from a statistical viewpoint will be developed for a variety of practical applications.   This methodology will not only address the recognized discrepancies between the assumptions underlying conventional control charts and today's industrial processes, but it will also provide methodological advances for on-line detection and diagnosis of faults and potential failures of automated engineering systems. In this connection, estimation and forecasting problems in time series models and stochastic dynamical systems having parameters that may change with time will also be investigated. Although in practice abrupt parameter changes usually occur infrequently, the unknown times of their occurrence have led to detection algorithms of prohibitive complexity.  By using parallel recursive algorithms and combining new ideas in change-point detection with empirical Bayes methodology, it is anticipated that asymptotically efficient estimation and prediction schemes of manageable complexity will be developed.  (ii) Another direction of research involves fixed sample change-point problems and their applications to biomolecular sequence analysis and other problems of signal detection.  A comprehensive statistical methodology will be developed for genome scanning to map anonymous genes using data based on crosses of pure strains in experimental genetics or based on regions of identity by descent of related individuals in human genetics.  Related mathematical problems in boundary crossing probabilities of random fields will be investigated.  (iii) A third direction of research is financial time series and stochastic control problems of financial economics. New statistical models, computational algorithms, and methods for data analysis and forecasting  will be developed to address a variety of sequential decision, portfolio selection, and pricing problems in investments and financial markets.<br/><br/>The interdisciplinary research in financial economics and molecular biology not only leads to the development of new stochastic models and statistical methods, but it also provides valuable research opportunities for graduate and undergraduate students in these rapidly developing fields.<br/>"
"0072569","Confidence Regions for Trees","DMS","GEOMETRIC ANALYSIS, TOPOLOGY, STATISTICS, COMPUTATIONAL MATHEMATICS","09/01/2000","09/14/2000","Susan Holmes","CA","Stanford University","Standard Grant","John Stufken","02/28/2003","$75,510.00","","susan@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1265, 1267, 1269, 1271","0000, 9183, 9263, BIOT, OTHR","$0.00","This research proposal uses geometry and statistics to build meaningful confidence regions for tree structured parameters.  Binary trees appear as parameters in: (1) Hierarchical clustering problems, for instance for micro-array data from contemporary genetics; (2) Methods that use decision trees such as the Classification and Regression trees (CART); (3) Estimation of phylogenetic trees built from DNA data for the taxonomy of species, but also for building `gene trees'.  The embedding of tree estimation in a statistical framework allows one to see trees as a special type of parameter to be estimated.  However the estimation procedures have been developed without the ability to construct confidence regions for these paramters or ways of comparing various estimators of these parameters.  Currently biologists validate the tree estimated by perturbing the data through a simple bootstrap of the DNA sequences and then summarizing the collection of trees obtained by associating p-values to the branches of a consensus tree.  This reduces the problem to a collection of binomial simulations, losing much of the multivariate information of which groups appear simultaneously.  This new constructive geometrical approach uses the tools available from topology and algebraic combinatorics.  Collaboration with a combinatorialist, Louis Billera, and a topologist, Karen Vogtmann, on a more mathematical understanding of tree space allows a more natural definition of distances between points in the `tree polytope'.  This topological study of tree space is crucial in the definition of a notion of neighborhood that then allows a definition of continuity of the estimation function.  The space of trees is a space with negative curvature.  It is also possible to define geodesics on this space and convex hulls.  The notion of confidence region can be defined, and it is possible to combine trees from built from different datasets or to combine trees obtained from different genes or even compare trees with other data (biogeographic, for instance).<br/><br/>Understanding the geometry of ""tree space"" helps understand how to combine and compare trees.  Whether they are hierarchical clustering trees built for micro-array DNA data or family trees built from DNA sequences, binary tree data are abundant in today's genome era.  The current proposal aims to consider trees as a whole, instead of breaking them down into just sibling relationships as is done currently.  This problem uses recent results from topology and the geometry of spaces with negative curvature, like the surface of a hyperboloid.  In these spaces notions of average and distance have to be redefined, the hardest being ways of representing what is not naturally representable in our usual euclidean space.  The challenges are both computational and geometric.<br/>"
"9909141","Spatial Data and Scaling Methods for Assessment of Agricultural Impacts of Climate Managing Multiple Sources of Uncertainty Over Space","DMS","STATISTICS, Climate & Large-Scale Dynamics","06/01/2000","08/25/2003","Linda Mearns","CO","University Corporation For Atmospheric Res","Cooperative Agreement","Gabor Szekely","09/30/2005","$725,000.00","Timothy Kittel, Douglas Nychka","lindam@ucar.edu","3090 CENTER GREEN DR","BOULDER","CO","803012252","3034971000","MPS","1269, 5740","0000, 1317, OTHR","$0.00","9909141<br/>Mearns<br/> Support for this research is provided to develop a project focusing on agricultural assessment in the Southeastern United States that will formally quantify uncertainties in spatial assessments based on dataset sources and various methods of spatial scaling of the data sets and various means of calibrating and validating crop models over space. The objectives of this research are to<br/>1. Evaluate and characterize uncertainties associated with alternative input data sources and methods for scaling up crop model estimations for regional climate impact assessments;<br/>2. Identify appropriate spatial scale matches for observed crop data and the different types of crop model input data, and develop methods for aggregating or disaggregating data to a given spatial scale;<br/>3. Characterize the errors and uncertainties that result when applying spatial data sets and crop simulation models to assess regional-scale impacts of climate on agriculture;<br/>4. Provide a validated, gridded spatial data base of calibrated soil parameters, daily weather time series, representative management inputs, and historical crop yields in the Southeast US for climate impact assessment; and<br/>5. Extend and apply these methods to another region of agricultural importance, the central Great Plains.<br/>"
"0072661","New Statistical Challenges Posed by Multiscale and Adaptive Representations","DMS","STATISTICS, SIGNAL PROCESSING SYS PROGRAM","07/15/2000","06/16/2004","Iain Johnstone","CA","Stanford University","Continuing Grant","Rong Chen","06/30/2005","$850,000.00","David Donoho","imj@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269, 4720","0000, OTHR","$0.00","NEW STATISTICAL CHALLENGES POSED BY MULTISCALE AND ADAPTIVE<br/>REPRESENTATIONS  D.L. Donoho & I.M. Johnstone, PI's.<br/>Project period: 07/01/00 -- 06/30/05<br/><br/>The project will address the following specific topics:<br/>1. Estimation in tomography: the curvelet tight frame of<br/>representation seems, according to preliminary calculations, to<br/>achieve faster rates of convergence than traditional tomographic<br/>methods.<br/>2. Estimation and testing in time-frequency analysis: a new tight<br/>frame of chirplets has been built by deploying curvelets in the<br/>time-frequency plane.  The new representation has promise for detection and<br/>estimation of chirps in the presence of noise.<br/>3. Recent work in computational vision asks ``what is the best basis<br/>for representing natural images?''  It is proposed that many empirical<br/>results in the rapidly growing literature on this topic might be<br/>explained and improved using analytically-constructed representations:<br/>ridgelets and curvelets.<br/>4. Geometry-driven diffusions are popular in applied image processing<br/>-- but their quantitative performance is not well-understood. It is<br/>proposed to develop a quantitative statistical theory using recent<br/>tools such as multiscale ridgelets.<br/>5. Extensions of Sparsity-Based ideas. Problems of finding `edgels in<br/>white noise' and `subspaces in white noise' offer challenging and<br/>timely directions in which to generalize existing sparsity ideas.<br/>6. Testing Sparse Means.  An adaptive approach is suggested for<br/>testing if the mean of a random vector is nonzero, when the vector<br/>might exhibit an unknown degree of sparsity.<br/>7. Asymptotics of top eigenvalues of large covariance matrices.<br/>A program is set out to develop statistical theory for principal<br/>components of large data matrices based on recent progress in<br/>random matrix theory.<br/>Consistent with the principle of ``reproducible research'', software<br/>and figures from this project will be made available in future<br/>releases of the public domain WaveLab system.<br/><br/><br/>In recent years, research in wavelets and time-frequency methods has<br/>broadened to construct new systems of representation, including<br/>systems custom-tailored for specific phenomena.  Examples include<br/>wavelet packets and cosine packets, and very recently, systems like<br/>edgelets, ridgelets, chirplets, warplets, and curvelets.  In parallel,<br/>research in statistical analysis and cognate fields allows data<br/>themselves to dictate the design of their own optimal systems of<br/>representation.  Principal components (i.e.  Karhunen-Loeve<br/>decomposition) is the oldest example of such data-adaptive<br/>representation; many newer ideas have been proposed recently, such as<br/>independent components analysis.  The proposers have been<br/>active in both domains, creating new image and signal<br/>representations and developing statistical theory to underpin adaptive<br/>signal representations.  The current project will (a) pursue two<br/>opportunities arising from the recent introduction of curvelets, (b)<br/>address two active applied research areas, computational vision and<br/>geometry driven diffusions, and (c) attack some issues which are<br/>argued to be at the core of new developments in statistical decision<br/>theory. Topic (a) may have implications for applied work in<br/>tomography, image and signal processing, and (c) may impact applied<br/>uses of principal components in domains such as climate and global<br/>change studies.<br/><br/>"
"0072360","Statistical Theory and Methodology","DMS","PROBABILITY, STATISTICS","08/01/2000","06/01/2004","Bradley Efron","CA","Stanford University","Continuing Grant","Yazhen Wang","07/31/2005","$725,306.00","Persi Diaconis","brad@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1263, 1269","0000, OTHR","$0.00","Abstract:<br/><br/>This project aims to develop theoretical ideas in mathematical statistics and probability that will be of genuine use to scientific practitioners.  The current proposal concerns five major projects: a detailed look at Markov Chain Monte Carlo methods, in particular the eigenvalue theory that determines rates of convergence; the application of MCMC theory to truncated data; inferential theory for scatterplot smoothers, including the relative efficiencies of competing criteria adaptively selecting the ""window width"" of the smoother, and also the cost of adaptation on the accuracy of the smoother; random matrix theory and its connection with the zeros of Riemann's zeta function; and finally conversion curve methods for improving the accuracy of bootstrap confidence intervals.  The connecting theme for the five projects is the combination of mathematical theory with computer-intensive algorithms.<br/>"
"0072445","Statistical Numerics","DMS","STATISTICS","09/01/2000","07/23/2002","Art Owen","CA","Stanford University","Continuing Grant","John Stufken","08/31/2003","$209,952.00","","owen@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","Owen<br/>0072445<br/>Abstract<br/><br/>The focus of this project is the application of statistical ideas to high dimensional numerical problems, such as approximation and noisy or nonsmooth optimization.  This work follows on earlier successes in integration.  Standard Monte Carlo sampling integrates with a slowly decreasing error.  Deterministic quasi-Monte Carlo sampling can achieve a much more accurate answer, but without a practical error estimate.  Re-injecting some randomness allows one to estimate the error, and gave rise to a surprising further large improvement in the quality of the answer.  The first problem is to use integration methods on approximation problems.  One expands the function in a basis (polynomials, Fourier functions, or wavelets), and finds that the coefficients are high dimensional integrals.  Estimates of these coefficients, with statistical uncertainty attached, can be used to give approximations with error estimates.  It is also possible to address qualitative issues such as: effective dimension of the function, smoothness of the function, number of important inputs, and so on.  The second problem is to optimize the expected value of a function over some variables in the face of randomness in some others.  An example is how to design an experiment for a nonlinear model.  The third problem is to predict binary functions learned from data.  An example is whether to hold or exercise an American type option.<br/><br/>Computer codes that depend on a great many inputs are becoming ubiquitous.  They are used in the design of semiconductors, airplanes and automobiles, in climate models, and in financial risk management.  On any given task, it can be a great challenge to extract the relevant knowledge buried within this software.  It is also necessary to attach uncertainty estimates to the findings.  For even a few dozen input factors, it becomes necessary to employ statistical methods, of the type being researched in this project.  This project also considers functions that depend on one million or more input factors.  Advances in computer power will bring more attention to such functions, and new methods, such as those investigated in this project, will be required.<br/>"
"0073952","Framework for Statistical Evaluation of Complex Computer Models","DMS","STATISTICS","08/01/2000","07/31/2000","Jerome Sacks","NC","National Institute of Statistical Sciences","Standard Grant"," Shulamith T. Gross","07/31/2004","$900,000.00","","sacks@niss.org","19 TW ALEXANDER DR","RESEARCH TRIANGLE PARK","NC","277090152","9196859300","MPS","1269","0000, 1616, OTHR","$0.00","Though inherently a statistical issue, model evaluation lacks a unifying <br/>statistical framework; this project supplies such. The foundation of the <br/>framework is the use of Bayesian techniques to quantify the degree to <br/>which a model captures an underlying reality; develop theory and methods <br/>that allow dual use of data in both estimation of model inputs and <br/>evaluation of outputs; select evaluation functions, by which a model and <br/>reality are compared and through which flaws (causes of ""invalidity"") <br/>are found; and to design the collection of field and computer simulation <br/>data. The building of the framework relies on specific formulations of <br/>problems, motivated by testbed examples such as subsurface fluid flow <br/>models - where the computer model is deterministic but uncertainties are <br/>present in the model inputs and specifications - and traffic simulators <br/>- where the model is intrinsically stochastic, in addition to having <br/>uncertain inputs.  <br/>  <br/>Computer models are everywhere and evaluating their fidelity to reality <br/>is central to assessing their effectiveness in understanding real <br/>phenomena (such as flow of pollutants through soil and into groundwater) <br/>and predicting results of innovative technologies (such as new signal <br/>timing strategies to relieve traffic congestion in urban networks). This <br/>project develops a statistical structure and basis for such evaluations <br/>applicable across the scientific and technological landscape.  In the <br/>process, the project creates the ingredients for a virtual laboratory <br/>to disseminate results, broaden involvement of other researchers (and <br/>users), and establish a unique educational and training environment."
"0072162","Asymptotic Equivalence of Statistical Experiments","DMS","PROBABILITY, STATISTICS","06/01/2000","04/05/2002","Michael Nussbaum","NY","Cornell University","Continuing Grant","Marianthi Markatou","05/31/2003","$94,731.00","","nussbaum@math.cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1263, 1269","0000, OTHR","$0.00","NSF proposal DMS 0072162<br/>Abstract<br/>Asymptotic Equivalence of Statistical Experiments<br/><br/>Principal Investigator: M. Nussbaum, Department of Mathematics, Cornell<br/>University, Ithaca, NY<br/><br/><br/>The concept of asymptotic equivalence of experiments serves to compare<br/>properties of statistical models. An experiment is a family of probability<br/>measures; a distance is defined between these objects such that the<br/>informational content of experiments with respect to the parameter is<br/>similar if this distance is small. This basic deficiency pseudodistance <br/>(or Delta-distance) has been introduced by L. Le Cam, generalizing the<br/>concept of sufficiency. If experiments are equivalent via sufficiency<br/>then their Delta-distance is 0; here equivalence via sufficiency<br/>means that one experiment results from application of a sufficient statistic<br/>to the data of the other.<br/><br/>Based on the idea of the Delta-distance, a theory of local<br/>asymptotic normality (LAN-theory) of experiments has been developed which<br/>has found widespread application in statistics. The idea of data reduction<br/>which is at the heart of the sufficiency concept could thus be combined with<br/>limit theorems of probability, resulting in approximation of general<br/>statistical models by Gaussian shift families. These Gaussian shift or<br/>translation families allow explicit expression for risk bounds in many<br/>instances, and these risk bound then become valid in an asymptotic sense in<br/>the approximated models.<br/><br/>The limitation of the LAN-theory consists in its restriction to a<br/>local setting, in which the rate of localization is tied to the<br/>normalization rate in the central limit theorem (the classical root-n in<br/>most cases). This setting precludes application of the resulting risk bounds<br/>to the class of ill-posed function estimation problems. The emphasis of the<br/>current project is on the treatment of this problem class (which includes<br/>nonparametric density estimation) by methods involving the Delta-distance.<br/><br/>The restriction to a local setting (or alternatively, a finite dimensional<br/>setting) which is inherent in the LAN-theory has been overcome in recent<br/>years, by efforts of Brown and Low (1996) and by various<br/>results of the P.I. and collaborators, starting also in 1996 <br/>(asymptotic equivalence of density estimation and Gaussian white noise). <br/>The main tool has been approximation of general likelihood processes by Gaussian<br/>ones, using coupling methodology. The present project focuses on elaboration<br/>and extension of these results, with two main directions:<br/><br/>(i) constructive realization of equivalence in nonparametric models,<br/>enabling explicit equivalence mappings (direct transfer of decision<br/>functions)<br/><br/>(ii) equivalences in nonparametric models for dependent data (time series<br/>and diffusion process models).<br/><br/>"
"0072585","Bayesian Modeling in the Wavelet Domain with Applications in Atmospheric Turbulence","DMS","STATISTICS, PHYSICAL METEOROLOGY","09/15/2000","09/21/2000","Gabriel Katul","NC","Duke University","Standard Grant"," Shulamith T. Gross","08/31/2003","$18,590.00","Marianna Pensky","gaby@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269, 1522","0000, OTHR","$0.00","This proposal builds on a previous NSF sponsored project DMS-9626159, awarded to Duke University.  It involves development of statistical concepts, theories and methods for multiresolution models and analyses of measurements and structures in atmospheric turbulence.  It draws on recent developments in  Bayesian multiscale modeling to understand the time-scale aspects of turbulence where the notions of scale and hierarchy are intrinsic.  Multiresolution statistical modeling approaches are applied to vast geophysical measurements arising  from air quality field experiments and simulations in order to identify key structural properties of atmospheric turbulence responsible for the transport of scalars, such as ozone.  To achieve these objectives a team of researchers with expertise in statistical modeling and  multiscale methods (M. Pensky and B. Vidakovic) and measurement and modeling of atmospheric turbulence (G. Katul) is assembled.<br/><br/>The aplicability of this project is improved fundamental understanding of atmospheric transport via novel statistical techniques.  Atmospheric transport, key to describing air-quality, is a complex phenomena involving modeling of turbulence which is responsible for the dispersion of polutants.  The applicability of the proposed methodology will be directly tested on high frequency  velocity, temperature, and ozone concentration  measurements collected at Duke Forest, Durham, North Carolina. <br/><br/>Partial support for this award is provided by the Physical Meterology Program in the Division of Atmospheric Sciences.<br/>"
"0075302","Sixth World Meeting of the International Society for Bayesian Analysis","DMS","STATISTICS, WESTERN EUROPE PROGRAM","05/01/2000","05/09/2000","Robert Wolpert","NC","Duke University","Standard Grant","Joseph M. Rosenblatt","04/30/2001","$20,000.00","","rlw@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269, 5980","0000, OTHR","$0.00","This award provides support for Ph.D. graduate students and new investigators in the International Society for Bayesian Analysis attending the Sixth World Meeting, ISBA 2000, held 28 May - 1 June 2000 in Hersonissos, Crete.  More than 125 talks by over 200 presenters, and three poster sessions, are planned on the themes of Official Statistics, Public Policy, and Governmental Statistics.  This conference brings together academic and applied Bayesian statisticians throughout the world, many of them recent Ph.D.s or current Ph.D. candidates, due to the demographic consequence of the rapid growth of the Bayesian statistics.  The award funds are primarily devoted to defraying the expenses of graduate students, new researchers, and members of underrepresented groups from the U.S.  Partial support is provided from the Division of International Programs.<br/><br/>"
"9988445","Computing Science and Statistics: Symposium on the Interface","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS, LARGE-SCALE DYNAMIC METEOROLOG, Climate & Large-Scale Dynamics","05/01/2000","05/18/2000","Carey Priebe","VA","Interface Foundation of North America Inc","Standard Grant","Richard L. Berger","04/30/2001","$13,250.00","","cep@jhu.edu","P.O. Box 7460","Fairfax Station","VA","220397460","7039931691","MPS","1269, 1271, 1527, 5740","0000, 9263, OTHR","$0.00"," The Interface Foundation of North America (IFNA), a non-profit educational<br/> corporation, sponsors a yearly meeting on the Interface of Computing<br/> Science and Statistics. This proposal seeks funding for Interface 2000, to<br/> be held April 5-8, 2000, at the Hotel Monteleon, in New Orleans. The 2000<br/> Symposium is organized around the theme of 'Modeling the Earth's Systems:<br/> Physical to Infrastructural,' and is intended to bring together subject<br/> area scientists and statisticians to discuss large, complex,<br/> multidisciplinary, computationally demanding modeling problems.  The<br/> keynote speaker is Grace Wahba (University of Wisconsin) with short<br/> courses presented by Wahba, Lorraine Denby and William S. Cleveland.<br/> Among other invited presentators are David Hand (Imperial College),<br/> Douglas Nychka (NCAR), Sally Morton (RAND), Lorraine Denby (Bell Labs),<br/> Sallie Keller-McNulty (Los Alamos), Paul Black (Neptune) and Cathy Dippo<br/> (Labor Statistics).  This forum, a  smaller focused meeting with a less<br/> formal atmosphere, will allow graduate students and young researchers to<br/> interact strongly with experienced researchers.<br/> <br/>"
"0072207","Methods for Reliability Theory and Applications of Order-Restricted Inference","DMS","STATISTICS","09/01/2000","07/01/2002","Henry Block","PA","University of Pittsburgh","Continuing Grant"," Shulamith T. Gross","08/31/2004","$171,000.00","Thomas Savits, Allan Sampson","hwb@stat.pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","0000, OTHR","$0.00"," The question is addressed of when the bathtub shape of a failure rate function is preserved by various functions used to describe optimal behavior.  A second issue is when do mixtures have specified shapes such as a bathtub shape.  A third topic introduces isotonic logistic discrimination, which is developed as a nonparametric generalization of linear logistic discrimination taking into account order restrictions.  The fourth question addresses finding simultaneous confidence bands for isotonic response surfaces and the fifth topic develops an ordinal extension of the Pearson chi-squared for testing 2(k ordered tables.  A new metric on permutations is proposed and developed as the sixth topic.<br/><br/> Failure rates are a method used to describe the propensity of a system, either engineering or biological, to fail.  They are a function of time and typically at an early age they are high (more likely to fail), at middle age they are low, and then at an old age they tend to increase (called wearout).  This gives rise to the bathtub shaped failure rate.  For mechanical or electrical systems, which have a bathtub failure rate, there are many early failures.  Consequently, a product should not be released until this period of early failures is past.  A method of insuring this is called burn-in.  Typically this means testing such products for a length of time until this critical period is over.  An important consideration is how to optimally determine such a period and this is one question studied by the researchers. Another research area deals with a common phenomenon in many scientific problems where the experimental responses tend to get larger as various underlying experimental parameters are increased.  For example, an individual's likelihood of heart attacks within a five-year period will increases as baseline systolic blood pressure increases and as cholesterol levels increase.  In order to properly analyze the data arising from such phenomena, new and more general statistical methods need to be developed.  The researchers are considering several major new statistical methods to model and analyze such data."
"0090067","Asymptotic and Statistical Analysis of Volitility and its Implications for Derivative Pricing and Risk Management","DMS","APPLIED MATHEMATICS, STATISTICS","08/01/2000","08/25/2000","K. Ronnie Sircar","NJ","Princeton University","Standard Grant","Deborah Lockhart","07/31/2003","$81,034.00","","sircar@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1266, 1269","0000, OTHR","$0.00","Dear Professor Pang,<br/><br/> Here is the abstract in text form.<br/><br/>Ronnie Sircar.<br/><br/>------------------------------------------------------------------<br/><br/>Asymptotic and Statistical Analysis of Volatility and its Implications<br/>for Derivative Pricing and Risk Management<br/><br/>Proposal Number: 0090067<br/>PI: K. Ronnie Sircar<br/>    Department of Operations Research & Financial Engineering<br/> Princeton University.<br/><br/>In modern financial markets, investors are increasingly faced with<br/>exposure to changing and uncertain volatility. This project concerns<br/>mathematical models in which volatility is a stochastic process, and<br/>their use in derivative pricing and risk management.  The main aim is<br/>to develop an efficient and robust framework in which models are<br/>calibrated from observable market data and then used to design<br/>risk-minimizing strategies that hedge a portfolio against the<br/>potentially serious consequences of changing volatility. This problem<br/>is important for investors from large trading institutions to<br/>individuals with pension funds.<br/><br/>The spectacular growth in the size of the derivatives market over the<br/>last twenty-five years (currently it has a turnover of trillions of<br/>dollars in the US) plus recent infamous (and equally spectacular) risk<br/>(mis)management disasters, such as the Barings, Orange County and Long<br/>Term Capital Management fiascos, have created an urgent need for smart<br/>mathematical and computational models to quantify the respective risks<br/>and rewards of such investments. This project aims to build on the<br/>methodology introduced by Black, Scholes and Merton, to take into<br/>account the fluctuating nature of market volatility. Mathematical<br/>tools are combined with statistical analysis of past prices to produce<br/>formulas and software that accurately capture the potential losses and<br/>gains in today's vast derivative market.<br/><br/><br/>"
"0071920","Collaborative Research on Graphical Markov Models and Related Topics in Multivariate Statistical Analysis","DMS","STATISTICS","08/15/2000","08/10/2000","Steen Andersson","IN","Indiana University","Standard Grant","Grace Yang","07/31/2004","$80,000.00","","standers@indiana.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","MPS","1269","0000, OTHR","$0.00","Perlman 0071818<br/>Andersson 0071920<br/><br/>Abstract<br/><br/>Statistical models based on acyclic directed graphs (ADGs) (also called directed acyclic graphs (DAGs), Bayesian networks, or influence diagrams) are particularly well behaved, easily interpretable, and computationally convenient.  In the late 1980s, ADG models were generalized to adicyclic graphs or chain graphs, which include both directed and undirected edges, hence can simultaneously represent dependences some of which are directional and some associative.  The investigators will study the Markov and statistical properties of a new class of chain graph models that retains many of the desirable properties of ADG models.  Problems to be investigated include the completeness and faithfulness of these new models, determination of their local Markov property, and characterization of their Markov equivalence classes by means of an appropriate essential graph.  The investigators will also study a very general class of Wishart distributions on homogeneous cones, in which transitive acyclic directed graphs (TADGs) play a central role. E. B. Vinberg's classical characterization of homogeneous cones has been found to reveal a fundamental relationship between normal models satisfying TADG Markov conditions and this general class of Wishart distributions.  This class includes all Wishart distributions previously known in multivariate statistical analysis, including the hyper-Wishart distributions and Wishart distributions associated with normal lattice conditional independence (LCI) models, as well as a great many new ones.  Additional topics to be investigated include the limitations of the Neyman-Pearson, likelihood ratio, and maximum likelihood criteria for multiparameter hypothesis-testing and estimation problems, and the efficacy of the likelihood ratio test for testing order-restricted and multivariate one-sided alternatives.<br/><br/>One of the most central ideas of statistical science is the assessment of dependences among a set of stochastic variables.  The familiar concepts of correlation, regression, and prediction are manifestations of this idea, and many aspects of causal relationships ultimately rest on representations of multivariate dependence.  Graphical Markov models  (GMM) use graphs i.e. networks, either undirected, directed, or mixed, to represent multivariate dependencies in a visual and computationally efficient manner.  A GMM is usually constructed by specifying local dependences for each variable, i.e. node of the graph, in terms of its immediate neighbors, parents, or both, yet can represent a highly varied and complex system of multivariate dependences by means of the global structure of the graph.  The local specification permits efficiencies in modeling, inference, and probabilistic calculations.  Among their many applications, GMMs have become prevalent in statistical science for the analysis of categorical data in contingency tables, for the modeling of spatially-dependent processes such as the spread of epidemics in human and animal populations, and for the development of early warning systems for severe weather conditions; in computer science (as Bayesian networks) for information processing and retrieval, for robotics, computer vision, and pattern recognition, for the debugging of complex programs (such as Windows 98), and for the representation of expert systems for medical diagnosis; and in decision science (as influence diagrams) as models for information flow and control and for combining the opinions of many decision-makers.  A crucial feature of these models is that they are designed for fast computational implementation, thereby facilitating the development of software that can ""reason"" about real world problems.<br/>"
"0086688","International Conference on the Foundation of Statistical Inference: Applications in the Medical and Social Sciences and in Industry and the Interface with Computer Science","DMS","STATISTICS, Methodology, Measuremt & Stats, ARTIFICIAL INTELL & COGNIT SCI","11/15/2000","07/19/2002","Stephen Fienberg","PA","Carnegie-Mellon University","Standard Grant","Xuming He","04/30/2003","$25,000.00","","fienberg@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 1333, 6856","0000, 1269, 9145, OTHR","$0.00","Abstract:<br/>DMS-0086688  <br/>PI: Fienberg, Stephen E<br/><br/>A special international conference, ""Foundation of Statistical Inference: Applications in the Medical and Social Sciences and in Industry and the Interface with Computer Science"", will be held in Israel at the conference center at Kiryat Anavim near Jerusalem, on December 17-21, 2000.  This conference brings together leading experts in statistics and its applications to discuss both the issues in the foundations of inference and their relevance to the many and varied uses of statistics in the medical sciences, the social sciences, and in industry.  A similar conference was held in December 1985.  A special focus at this conference will be on the interface between statistics and computer science.  Funding will provide travel support for approximately 20 U.S. primarily new researchers and members of underrepresented groups, and invited speakers and participants.  The KCS Program in the CISE directorate and the MMS Program in SBE directorate is providing co-funding along with the Statistics Program in MPS.<br/>"
"0196041","Data-Analytic Modeling for High-Dimensional Data","DMS","STATISTICS","07/01/2000","11/09/2000","Jianqing Fan","NC","University of North Carolina at Chapel Hill","Continuing Grant","John Stufken","07/31/2002","$97,900.00","","jqfan@princeton.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","0000, OTHR","$0.00",""
"0084375","Spatial Modeling, Analysis and Prediction of Nonstationary Environmental Processess","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","09/15/2000","09/08/2000","Richard Smith","NC","University of North Carolina at Chapel Hill","Standard Grant","Xuming He","08/31/2004","$149,578.00","","rls@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1253, 1269","0000, 1386, OTHR","$0.00","Abstract:<br/><br/><br/>     SPATIAL MODELING, ANALYSIS AND PREDICTION OF <br/>      NONSTATIONARY ENVIRONMENTAL PROCESSES<br/><br/><br/>Montserrat  Fuentes, North Carolina State University<br/><br/>Richard  L. Smith, University of North Carolina, Chapel Hill<br/><br/>Spatial statistics is one of the major methodologies of environmental <br/>statistics; its applications include producing spatially smoothed or <br/>interpolated representations of air pollution fields, calculating <br/>regional average means or regional average trends based on data at a <br/>finite number of monitoring stations, and performing regression <br/>analyses with spatially correlated errors to assess the agreement <br/>between observed data and the predictions of some numerical model.  <br/>However, the most commonly used spatial statistics methodology, also <br/>known as geostatistics or kriging, is essentially based on the <br/>assumption of stationary and isotropic random fields. Such <br/>assumptions cannot be expected to hold in large heterogeneous fields. <br/>The research described here concentrates on nonstationary spatial <br/>models. Some new models are introduced, as well as new fitting <br/>methods based on spectral analysis. The applications include three <br/>real data sets: (i) monitoring data for nitrate fields compared with <br/>Models-3 output as part of the process for assessing compliance with <br/>the Clean Air Act Amendments of 1990; (ii) modeling the spatial <br/>distribution of particulate matter fields, as one of the components <br/>needed for an improved risk assessment of human health effects of <br/>particulate matter; (iii) developing statistical models for spatial <br/>temperature fields and applying them to the attribution of various <br/>""signals"" produced by climate models - in particular, this <br/>methodology will permit improved assessment of the extent to which <br/>observed global climate change may be attributed to anthropogenic <br/>influences.  <br/><br/>In more detail, the new statistical methodology concentrates on two <br/>approaches to nonstationary models: a spatial deformation approach <br/>due to Guttorp and Sampson, and an approach where the field is <br/>represented locally as a stationary isotropic random field, but the <br/>parameters of the stationary random field are allowed to vary <br/>continuously across space. Kernel functions are used to ensure that <br/>the field is well-defined but also continuous. Some combination of <br/>the two approaches may be needed for fields with are neither <br/>stationary nor isotropic. New fitting algorithms are developed, using <br/>both space domain and spectral approaches; in cases where the data <br/>are distributed exactly or approximately on a lattice, it is argued <br/>that spectral approaches have potentially enormous computational <br/>benefits compared with maximum likelihood. The methods are extended <br/>to prediction/interpolation questions using approximate Bayesian <br/>approaches to account for parameter uncertainty.  We develop <br/>applications to obtaining the total loading of pollutant <br/>concentrations and fluxes over different geo-political boundaries, to <br/>risk assessment for particulate matter random fields, and to the <br/>attribution of an observed climate record to various components <br/>produced by numerical climatic model, the latter forming a new <br/>approach to the fingerprint estimation technique developed by <br/>climatologists.  <br/><br/>This program is being jointly funded by the Division of Mathematical Sciences and the Office of Multidisciplinary Activities from the Directorate of Mathematical and Physical Sciences.<br/><br/>"
"0072510","Statistical Information in Genetic Studies:  Theory and Methods","DMS","STATISTICS","09/01/2000","09/27/2002","Dan Nicolae","IL","University of Chicago","Standard Grant"," Shulamith T. Gross","08/31/2004","$160,000.00","Xiao-Li Meng","nicolae@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00","This proposal is a comprehensive research plan for establishing a general framework for measuring available statistical information in gene mapping studies.  The key methodological challenge is to find a measure that (1) is a reliable index of the relative information specific to the purpose of a study, (2) conditions on particular data sets, (3) is robust in the sense of general applicability, including to small data sets, (4) is easy to compute, and (5) is subject to sensible combination axioms. Dealing with all these criteria simultaneously requires a careful combination of Bayesian and frequentist methods, especially for small samples. The PIs propose to investigate a large-sample framework involving likelihood functions only, and a small-sample framework from a robust Bayesian perspective. The robust Bayesian approach takes full advantage of the Bayesian formulation in deriving information measures with desirable coherence properties, and at the same time it seeks measures that are robust to various specifications and thus are more generally applicable. The PIs also propose to investigate several specific measures at two levels. At the more general level, the PIs will study and compare these measures in terms of their general behaviors and applicability, which are not restricted to the genetic setting. At the more specific level, the PIs plan to evaluate and apply these measures in specific genetic applications, including allele-sharing methods, methods for fine-scale genetic mapping (e.g., haplotype-sharing methods), map comparisons (e.g., SNPs verses microsatellites), gene-gene interaction and gene-environmental interaction studies. <br/><br/>Due to the huge potential benefit to the public health, geneticists and analytical researchers, including statisticians, have focused their efforts on finding genes affecting susceptibility to common, complex disorders such as diabetes, asthma, hypertension, cardiovascular and psychiatric diseases. The transmission of these disorders is complex, the etiologic complexity being increased by the action and interaction of multiple genes and environmental factors. There are other complications such as sporadic cases, incomplete penetrance (i.e., genetically predisposed individuals might not exhibit the disorder) and late age of onset. All these factors increase the difficulty of identifying the genetic components of the trait of interest.  Genetic linkage studies are often the first step in finding and cloning a disease gene. Their goal is to locate and, if possible, shorten regions on the genome that are very likely to contain disease susceptibility genes. In many studies, difficulty arises because most genetic data sets are incomplete and investigators want to know how much information in the data is available for the study relative to the amount of information that would have been available if the data were complete.  This relative information directly guides the investigator's follow-up strategies (e.g., using more genetic markers with existing DNA samples versus collecting DNA samples from more families), and a misleading measure can lead to a serious waste of human and financial resources as well as a delay in the progress of the underlying genetic studies. The goal of this proposal is to provide reliable measures of such relative information by using the current start-of-the-art statistical techniques. <br/>"
"0071726","Generalized Linear Models","DMS","STATISTICS","08/01/2000","06/13/2002","Peter McCullagh","IL","University of Chicago","Continuing Grant","John Stufken","07/31/2003","$225,000.00","","pmcc@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00","Peter McCullagh will investigate a number of issues, all bearing directly or indirectly on generalized linear models.  The main theme of the proposal is the development of a framework for constructing logically consistent statistical models, whose hallmark is extendability or scope.  Every sensible linear model for a two-way layout with 4 treatments and 7 blocks must ordinarily be one element, or vector space, in a sequence of logically related vector spaces, one such subspace for each layout.  Logical consistency demands that the restriction of one subspace to a subset of the blocks or treatments must coincide with the subspace associated with this subset.  In mathematical terms, not only must the model be invariant under permutation of treatments and permutation of blocks, but the sequence of vector spaces must be closed under restriction to subsets.  In algebraic terminology, such a sequence of vector spaces constitutes<br/>a representation of the category of injective maps on finite sets.  The standard factorial models (hierarchical interaction models) coincide precisely with the regular sub-representations of the product category.  The PI has developed this notion of extendability for designs, common in genetic studies of plant and animal breeding, in which two or more factors have the same set of levels.  In order to accommodate these new models, it is found necessary to extend the factorial model formulae<br/>by the inclusion of several new operators that are relevant only for homologous factors.  The aim is to develop a succinct way of specifying category-invariant subspaces, i.e.~models, in a way that is unambiguous and can be understood by the computer.<br/><br/>This proposal aims to study statistical models from the viewpoint of their logical structure, and to develop new models where appropriate.<br/>In each new area of application, whether it be genetics, biology or social science, new considerations relevant to the application emerge.<br/>For example, in experiments connected with plant breeding, the same set of plants may occur as males contributing pollen and as females contributing ova.  Likewise, in citation studies, a given journal may occur as a citing journal or as a cited journal.  Homologous factors of this sort rarely arise in agricultural field trials where factorial models were first developed.  As a result, standard statistical models are not well suited to such applications.  It is the aim of this proposal to study such structures from a logical viewpoint and to develop new statistical models where needed."
"0073651","Collabrative Research:  Sequential Monte Carlo Methods and Their Applications","DMS","STATISTICS","09/15/2000","09/15/2000","Xiaodong Wang","TX","Texas A&M Engineering Experiment Station","Standard Grant","John Stufken","02/28/2002","$195,135.00","","wangx@ee.columbia.edu","3124 TAMU","COLLEGE STATION","TX","778433124","9798626777","MPS","1269","0000, 1616, OTHR","$0.00","Collaborative Research: Sequential Monte Carlo Methods and Their Applications<br/><br/>Jun Liu, Harvard University <br/>Rong Chen, Univ. Illinois at Chicago<br/>Xiaodong Wang, Texas A&M University<br/><br/>Abstract (Technical):<br/>Sequential Monte Carlo (SMC) methodology recently emerged in statistics and engineering fields promises to solve a wide class of nonlinear filtering and optimization problems, opening up new frontiers for cross-fertilization between statistics and many areas of applications. A distinctive feature of SMC is its ability to adapt to the dynamics of the underlying stochastic systems via recursive simulation of the variables involved.  Although special forms of SMC date back to 1950s, the general use of the method appeared only recently and its many key properties have yet been well understood.  This research group will focus on three major theoretical issues regarding the design of effective SMC-based computational tools and three important application areas, namely, wireless communications, computational biology, and business data analysis.  In the theory part, they will study approaches of generating better Monte Carlo samples for tracking system dynamics; investigate roles of resampling which is critical to the effectiveness of SMC; and propose system reconfiguration strategies for more efficient SMC algorithms.  In the application part, they plan to design novel signal processing and network control algorithms for wireless multimedia communications; develop better multiple sequence alignment models and SMC-based optimization method for protein structures; and build SMC-based modeling and analysis tools for business data. It is anticipated that the proposed research will culminate in the formulation of novel SMC methodologies and will bring the promise of the SMC paradigm into the practical arena of many emerging applications.<br/><br/>Stochastic dynamic systems are routinely used in many application fields such as automatic control, engineering, and finance.  The statistical analyses of these systems are crucial.  However, except for a few special cases, quantitative analyses of these systems still present major challenges to researchers.  Sequential Monte Carlo (SMC) technique recently emerged in the field of statistics and engineering shows a great promise on solving a wide class of nonlinear filtering, prediction, and optimization problems, providing us with many exciting new research opportunities.  The name ""Monte Carlo"" was coined in 1940s by scientists involved in designing atomic bombs and it refers to a technique in which computer is used to simulate and study a complex stochastic system. The technique was named after the famed gambling resort because its procedures incorporate the element of chance. A distinctive feature of SMC is its ability to sequentially simulate the system by considering one variable at a time.  The general use of SMC appeared recently and its invasion into many fields of science and engineering has just begun. Researchers including people in this research group have demonstrated that SMC can be successfully adapted to solve chemistry, engineering, and statistical problems. Understanding its theoretical properties and extending the use of SMC to other fields are the main focuses of this project. More specifically, this research group will focuse on three major theoretical issues regarding the design of effective SMC-based computational tools and three important application areas including wireless communications, computational biology, and business data analysis. These applications are not only important by their own merits, but also essential as the test ground for the new theories being developed and as the sources of stimulation for new research directions for SMC.  It is anticipated that this research will culminate in the formulation of novel SMC methodologies and will bring the promise of the SMC paradigm into the  practical arena of many emerging applications.  In particular, this research will bear fruits in the following areas: novel designs of signal processing and network control algorithms for wireless multimedia communications; developments of better algorithms analyzing biological sequence and structure data; and a SMC-based tool for business data analysis and prediction.<br/>"
"0071757","Multivariate Analysis, Ranks, and Multivariate Ranks","DMS","STATISTICS","08/01/2000","05/22/2002","John Marden","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Grace Yang","07/31/2004","$74,903.00","","marden@stat.uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","0000, OTHR","$0.00","The project explores two areas that utilize ranked data. The first aims to <br/>develop statistical procedures that are robust to violations of assumptions <br/>while still working close to optimally when the assumption holds. There is <br/>a long history of such robust procedures based on ranking the data, that <br/>is, replacing the observed values in the data with the values' ranks. This <br/>process helps to ameliorate the effects of unusually wild observations that <br/>can ruin an analysis. A number of multivariate situations in which there <br/>has previously been little work using rank procedures will be the main <br/>focus of this project. These include certain structural models defining the <br/>relationship of variables, testing for runs in multivariate data observed <br/>over time, estimating variances and covariances, and testing whether <br/>certain variables are conditionally independent given some other variables. <br/>A proposed method for defining multivariate ranks (""iterated ranks"") so <br/>that their distribution is independent of the distribution of the <br/>underlying observations will be explored.<br/><br/>The second area looks at modeling rank data directly, where the data arise <br/>from judges ranking particular objects based on their preferences. One <br/>popular model posits that judges and objects can be arrayed along a line, <br/>where a judge is located nearest the judge's most preferred object, next <br/>closest to the second most preferred object, etc. A new model that also <br/>allows judges to locate themselves nearest the objects they prefer least <br/>will be considered. An extension of these models in which there is <br/>provision for a small percentage of judges to act not at all according to <br/>the model will also be considered. <br/><br/><br/>"
"0073044","Generalized Regression Modeling of Ordinal and Bounded Response Data","DMS","STATISTICS","08/15/2000","08/01/2000","Douglas Simpson","IL","University of Illinois at Urbana-Champaign","Standard Grant"," Shulamith T. Gross","07/31/2004","$83,552.00","","dgs@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>0073044<br/><br/>This research aims to develop new statistical methods, inference<br/>and theory for regression modeling of ordinal and bounded<br/>data. Recent work indicates a strong possiblility of unifying <br/>the most commonly used ordinal models and extending them to a <br/>broader class of models. A unified approach will lead to comprehensive<br/>and flexible ordinal regression modeling. Further this line of<br/>research is directed at developing improved methods for<br/>temporally and spatially correlated ordinal data. Bounded response <br/>data are common in many applications. The second part of this <br/>project will extend general regression methods and inference methods <br/>for bounded response data. Parametric and semi-parametric methods<br/>will be developed to model correlations among bounded responses.<br/>Modern computing algorithms such as Markov Chain Monte Carlo<br/>provide the means for developing software for correlated ordinal<br/>and bounded response data.   <br/><br/>Ordinal modeling has widespread applicability in the social <br/>sciences and medical studies. Moreover ordinal modeling has <br/>recently emerged as an important tool for risk assessment in <br/>environmental toxicology and in civil engineering applications <br/>where the probabilities of events of different severity <br/>need to be modeled. Bounded response data are common in <br/>infrastructure studies, which use bounded condition scores, <br/>e.g., 0-100 scale. The data may be correlated over time (e.g., <br/>the history of a road section) and space (e.g., neighboring road <br/>sections are correlated). The methods being developed will improve <br/>modeling and uncertainty assessments and will have applications <br/>in many fields such as environmental risk assessment, <br/>infrastructure management, transportation risk modeling and the <br/>assessment of treatments for depression and other social<br/>functioning disorders.<br/><br/>"
"9910238","Fourth International Conference on Forensic Statistics","DMS","STATISTICS, Methodology, Measuremt & Stats","01/01/2000","01/03/2000","Bruce Weir","NC","North Carolina State University","Standard Grant","Joseph M. Rosenblatt","12/31/2000","$10,000.00","","bsweir@u.washington.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269, 1333","9123, BIOT","$0.00","9910238<br/><br/>The Fourth International Conference on Forensic Statistics will be held at North Carolina State University on Dec. 5-8, 1999.  The conference is intended to allow statisticians, forensic scientists, lawyers, and scholars from related disciplines to discuss the many and varied uses of Probability and Statistics in legislative, administrative and judicial proceedings.  Forensic Statistics, the application of Statistics and Probability to legal matters, is an expanding field.  The current debates surrounding the presentation of evidence based on DNA profiling and epidemiological studies on the health effects of various drugs are just two examples.  There is a great need for statisticians to provide guidance to forensic scientists, and lawyers and the courts.  By bringing together statisticians, forensic scientists, lawyers and other scholars, this fourth international conference will further the interdisciplinary understanding of Statistics and Probability applied to legal matters.  The conference is the fourth in a series.  The previous conferences have been held at Univ. of Edinburgh and the Arizona State University.  Based on previous attendance, about 250 people are expected to attend the fourth conference.  This proposal provides funds to insure that U.S. participants will be able to hear and network with the best experts on this topic from this country and from abroad, and to provide access to the conference by graduate students, postdoctoral fellows, new researchers, and members of underrepresented groups.<br/>"
"9985738","Statistics:  Reflections on the Past and Visions for the Future","DMS","STATISTICS","01/01/2000","12/15/1999","Nandini Kannan","TX","University of Texas at San Antonio","Standard Grant","William B. Smith","12/31/2000","$12,000.00","Jerome Keating","nandini.kannan@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","MPS","1269","0000, OTHR","$0.00","9985738<br/><br/>An international conference ""Statistics:  Reflections on the Past and Visions for the Future"" will be held March 16-19, 2000 at the University of Texas - San Antonio.  Although the meeting is in honor of the 80th birthday of C. R. Rao, the primary focus of the conference will be to provide a forum for participants to meet and discuss the changing face of statistical research. The topics will include multivariate analysis, environmental and ecological statistics, biostatistics, genetics, image processing and other contemporary research areas that reflect the enormous contributions of C. R. Rao to the field.  The meeting encourages students and young researchers to engage in fruitful dialog with senior statisticians. This proposal provides funds to ensure that U.S. participants will be able to hear and network with the best experts on these topics from this country and from abroad, and to provide access to the conference for graduate students, postdoctoral fellows, new researchers, and members of underrepresented groups<br/>"
"0002790","Spatial Modeling, Analysis and Prediction of Nonstationary Environmental Processes","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","09/01/2000","09/03/2003","Montserrat Fuentes","NC","North Carolina State University","Standard Grant","Xuming He","08/31/2004","$149,754.00","","mfuentes@vcu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1253, 1269","0000, 1386, OTHR","$0.00","Abstract:<br/><br/><br/>     SPATIAL MODELING, ANALYSIS AND PREDICTION OF <br/>      NONSTATIONARY ENVIRONMENTAL PROCESSES<br/><br/><br/>Montserrat  Fuentes, North Carolina State University<br/><br/>Richard  L. Smith, University of North Carolina, Chapel Hill<br/><br/>Spatial statistics is one of the major methodologies of environmental <br/>statistics; its applications include producing spatially smoothed or <br/>interpolated representations of air pollution fields, calculating <br/>regional average means or regional average trends based on data at a <br/>finite number of monitoring stations, and performing regression <br/>analyses with spatially correlated errors to assess the agreement <br/>between observed data and the predictions of some numerical model.  <br/>However, the most commonly used spatial statistics methodology, also <br/>known as geostatistics or kriging, is essentially based on the <br/>assumption of stationary and isotropic random fields. Such <br/>assumptions cannot be expected to hold in large heterogeneous fields. <br/>The research described here concentrates on nonstationary spatial <br/>models. Some new models are introduced, as well as new fitting <br/>methods based on spectral analysis. The applications include three <br/>real data sets: (i) monitoring data for nitrate fields compared with <br/>Models-3 output as part of the process for assessing compliance with <br/>the Clean Air Act Amendments of 1990; (ii) modeling the spatial <br/>distribution of particulate matter fields, as one of the components <br/>needed for an improved risk assessment of human health effects of <br/>particulate matter; (iii) developing statistical models for spatial <br/>temperature fields and applying them to the attribution of various <br/>""signals"" produced by climate models - in particular, this <br/>methodology will permit improved assessment of the extent to which <br/>observed global climate change may be attributed to anthropogenic <br/>influences.  <br/><br/>In more detail, the new statistical methodology concentrates on two <br/>approaches to nonstationary models: a spatial deformation approach <br/>due to Guttorp and Sampson, and an approach where the field is <br/>represented locally as a stationary isotropic random field, but the <br/>parameters of the stationary random field are allowed to vary <br/>continuously across space. Kernel functions are used to ensure that <br/>the field is well-defined but also continuous. Some combination of <br/>the two approaches may be needed for fields with are neither <br/>stationary nor isotropic. New fitting algorithms are developed, using <br/>both space domain and spectral approaches; in cases where the data <br/>are distributed exactly or approximately on a lattice, it is argued <br/>that spectral approaches have potentially enormous computational <br/>benefits compared with maximum likelihood. The methods are extended <br/>to prediction/interpolation questions using approximate Bayesian <br/>approaches to account for parameter uncertainty.  We develop <br/>applications to obtaining the total loading of pollutant <br/>concentrations and fluxes over different geo-political boundaries, to <br/>risk assessment for particulate matter random fields, and to the <br/>attribution of an observed climate record to various components <br/>produced by numerical climatic model, the latter forming a new <br/>approach to the fingerprint estimation technique developed by <br/>climatologists.  <br/>This program is being jointly funded by the Division of Mathematical Sciences and the Office of Multidisciplinary Activities from the Directorate of Mathematical and Physical Sciences.<br/><br/>"
"0072292","Problems in Statistical Model Building","DMS","STATISTICS, Climate & Large-Scale Dynamics","08/15/2000","10/31/2003","Grace Wahba","WI","University of Wisconsin-Madison","Continuing Grant","Grace Yang","07/31/2006","$437,703.00","","wahba@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269, 5740","0000, 1303, OTHR","$0.00","PROJECT ABSTRACT<br/><br/>Problems in Statistical Model Building. Grace Wahba, PI.<br/><br/>This research is to further the development of Smoothing Spline ANOVA and related variational methods for multivariate function estimation and statistical model building, so that these methods may be used in analyses of very large complex heterogenous data sets as occur in demographic medical studies, environmental and climatic data analyses and classification problems in a variety of areas.  These methods are flexible nonparametric methods, but generally contain commonly used parametric families as special cases.  The general approach proceeds in the following steps:  (i)propose families of models that are appropriate for specific areas of application, (ii) develop new numerical algorithms as required for fitting the models, (ii) develop further methods for tuning the models and providing accuracy estimates, (iii) develop information concerning the properties of the methods, including testing on realistic simulated observations where the `truth' is known, (iv) and applying the resulting methods to important data sets, with the expectation of extracting information from these data that is not obtainable by standard parametric methods.<br/><br/>The goal of this project is provide to scientists in medical, environmental and atmospheric sciences and supervised machine learning, new and useful tools to more efficiently analyze their data.  Tasks are proposed to develop new methods that are appropriate for more efficient data analysis in complex demographic studies which follow populations over time, collecting information useful for understanding relationships between possible risk factors and the incidence and progression of various diseases.  Tasks are also proposed for the development of new methods that are appropriate for understanding relationships among various factors of interest in large environmental and atmospheric data sets with `non-standard' indirect observational data; and for exploiting some new methods in classification that have wide applicability for building classification algorithms based on learning from large data sets in very high dimensional spaces.<br/>"
"0071930","Statistical Analysis of Linkage/Association on Family-Based Studies in Human Genetics","DMS","STATISTICS","08/01/2000","05/28/2002","Shaw-Hwa Lo","NY","Columbia University","Continuing Grant"," Shulamith T. Gross","07/31/2004","$260,468.00","","slo@stat.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","0000, 9125, BIOT, OTHR","$0.00","The allelic association between disease genes and marker alleles allows predicting which marker allele is in coupling with the disease. There are novel statistical testing procedures proposed recently.  For example, the Transmission/ Disequilibrium Testing ( TDT ) has been both a popular and powerful method to evaluate the linkage/ association between the candidate genes with disease and the markers. This approach combines the information of association and linkage to yield greater power than conventional tests for detecting linkage when the association is present. It also has great potential to play a major role in future gene mapping of human disorder, by carrying out genomewide TDT screens. Recent literature has seen an increasing trend in the number of studies that have applied TDT or its generalizations to a variety of complex family-based data, but these methods are ad hoc. There is a great need for a systematic and sensible optimal analysis of these methods.  This work sets up a general statistical framework that can incorporate all procedures, different data sets and other related knowledge such as disease information, sampling designs and population information.  We develop a general statistical theory, useful as a guideline for future investigators, who can design and develop their own statistical procedures (following the framework) to meet their needs in incorporating different environments.  <br/><br/>The research provides a likelihood-based approach by introducing a useful 3x2 table which applies to each nuclear family. It is natural to adopt the conditionality principle to derive and to evaluate a variety of TDT type procedures and linkage tests, using ""Locally Most Powerful Testing"" (LMPT ) as a central optimality criteria.  This research outlines the likelihood approach and demonstrates how the method works in some special cases. The research provides the derivations and findings from the examples,which should shed light on a better understanding of the more challenging and broader problems arising from these data sets.  In training doctoral students working on this increasingly important area, the research provides the basis to develop a new research level course.  The proposed research intends to lead ultimately to routine guidelines for future investigators and to offer a theoretical basis for family-based studies."
"0094613","Collaborative Research:  Sequential Monte Carlo Methods and Their Applications","DMS","STATISTICS","09/15/2000","06/11/2002","Jun Liu","MA","Harvard University","Continuing Grant"," Shulamith T. Gross","08/31/2004","$335,393.00","","jliu@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","0000, 1616, OTHR","$0.00","Sequential Monte Carlo (SMC) methodology recently emerged in statistics and engineering fields promises to solve a wide class of nonlinear filtering and optimization problems, opening up new frontiers for cross-fertilization between statistics and many areas of applications. A distinctive feature of SMC is its ability to adapt to the dynamics of the underlying stochastic systems via recursive simulation of the variables involved.  Although special forms of SMC date back to 1950s, the general use of the method appeared only recently and its many key properties have yet been well understood.  This research group will focus on three major theoretical issues regarding the design of effective SMC-based computational tools and three important application areas, namely, wireless communications, computational biology, and business data analysis.  In the theory part, they will study approaches of generating better Monte Carlo samples for tracking system dynamics; investigate roles of resampling which is critical to the effectiveness of SMC; and propose system reconfiguration strategies for more efficient SMC algorithms.  In the application part, they plan to design novel signal processing and network control algorithms for wireless multimedia communications; develop better multiple sequence alignment models and SMC-based optimization method for protein structures; and build SMC-based modeling and analysis tools for business data. It is anticipated that the proposed research will culminate in the formulation of novel SMC methodologies and will bring the promise of the SMC paradigm into the practical arena of many emerging applications.<br/><br/>Stochastic dynamic systems are routinely used in many application fields such as automatic control, engineering, and finance.  The statistical analyses of these systems are crucial.  However, except for a few special cases, quantitative analyses of these systems still present major challenges to researchers.  Sequential Monte Carlo (SMC) technique recently emerged in the field of statistics and engineering shows a great promise on solving a wide class of nonlinear filtering, prediction, and optimization problems, providing us with many exciting new research opportunities.  The name ""Monte Carlo"" was coined in 1940s by scientists involved in designing atomic bombs and it refers to a technique in which computer is used to simulate and study a complex stochastic system. The technique was named after the famed gambling resort because its procedures incorporate the element of chance. A distinctive feature of SMC is its ability to sequentially simulate the system by considering one variable at a time.  The general use of SMC appeared recently and its invasion into many fields of science and engineering has just begun. Researchers including people in this research group have demonstrated that SMC can be successfully adapted to solve chemistry, engineering, and statistical problems. Understanding its theoretical properties and extending the use of SMC to other fields are the main focuses of this project. More specifically, this research group will focuse on three major theoretical issues regarding the design of effective SMC-based computational tools and three important application areas including wireless communications, computational biology, and business data analysis. These applications are not only important by their own merits, but also essential as the test ground for the new theories being developed and as the sources of stimulation for new research directions for SMC.  It is anticipated that this research will culminate in the formulation of novel SMC methodologies and will bring the promise of the SMC paradigm into the  practical arena of many emerging applications.  In particular, this research will bear fruits in the following areas: novel designs of signal processing and network control algorithms for wireless multimedia communications; developments of better algorithms analyzing biological sequence and structure data; and a SMC-based tool for business data analysis and prediction.<br/>"
"0072319","Problems in Model Selection, Mixtures and Weighted Likelihood","DMS","STATISTICS","08/01/2000","09/29/2003","Marianthi Markatou","NY","Columbia University","Standard Grant","Xuming He","12/31/2004","$123,621.00","","statdistance@gmail.com","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","0000, OTHR","$0.00","Abstract<br/><br/>Problems in model selection, mixtures and weighted likelihood<br/><br/>We study problems in model selection, mixtures and weighted likelihood.  In particular, we first discuss extensions of the weighted likelihood methodology introduced by Markatou et al (1997,1998) in the context of regression models and its connection to model selection issues with emphasis in the mixture model context.  We then generalize these ideas to study general model selection problems.  The role of the disparity measures that are associated with weighted likelihood, and several others, is studied in detail in connection with goodness of fit approach to model selection.<br/><br/>The point of view we take here is that parametric models can provide informative, parsimonious descriptions of the data.  Then, the test statistics for testing the model adequacy are constructed and the asymptotic distributions under the null hypothesis and under contiguous alternatives are studied.  The theory here is based on empirical processes.  To practically implement the methods we propose to bootstrap the test statistics to obtain appropriate p-values.  We consider bootstrapping from a hybrid between the model distribution stipulated by the null hypothesis and the data and study the consistency of it.  When the model is false we prefer nonparametric bootstrap.  The performance of Bickel's m out of n bootstrap will be examined.<br/>"
"0072840","Statistical Inference and Modeling for Complex Data","DMS","STATISTICS","08/15/2000","08/04/2000","Jiayang Sun","OH","Case Western Reserve University","Standard Grant","Xuming He","01/31/2004","$100,000.00","","jsun21@gmu.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","MPS","1269","0000, OTHR","$0.00","This project focuses on three  research areas related to complex data.<br/>The first research area is on high dimensional graphics, data analysis<br/>and related topics.  The second research area is on mixture models and<br/>bump hunting  problems.  The third  area is on models  and inferential<br/>procedures  for data  with  biases.  Several  variants of  likelihood,<br/>semi-parametric and  iterative procedures are sought  for modeling and<br/>making inferences about the data.<br/><br/>We live in the information  age.  Information often hides in data sets<br/>with different  appearances.  Mining and modeling  (massive) data sets<br/>include the  discovery of  patterns, subcomponents, bumps,  or special<br/>events, and  development of models  that explain the data  and predict<br/>the future.   This research  is aimed at  providing solutions  to some<br/>interesting  problems arising  from  the data  which  are either  high<br/>dimension,  or  complicated,  or  with  a  sampling  bias.   Theories,<br/>methods,   and  efficient  computing   algorithms  are   explored  for<br/>extracting  useful  information from  the  complex data.   Application<br/>areas  include  astronomy, neurosciences,  quality  control, and  some<br/>(other) observational studies.<br/><br/>"
"0196228","New Monte Carlo Methods for Scientific and Statistical      Computing","DMS","STATISTICS","09/01/2000","04/10/2001","Jun Liu","MA","Harvard University","Continuing Grant","Marianthi Markatou","08/31/2002","$54,467.00","","jliu@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","0000, OTHR","$0.00",""
"0072174","Efficient Estimation in Semiparametric Time Series Models","DMS","STATISTICS","08/01/2000","04/17/2002","Anton Schick","NY","SUNY at Binghamton","Continuing Grant","Xuming He","09/30/2003","$84,000.00","","anton@math.binghamton.edu","4400 VESTAL PKWY E","BINGHAMTON","NY","139026000","6077776136","MPS","1269","0000, OTHR","$0.00","Semiparametric models play a major role in many fields and have been extensively studied over the last two decades. Great emphasis has been placed on models with independent (and identically) distributed observations and quite some progress has been made in this case.  Models with dependent observations, however, been mainly neglected up to now from an efficiency point of view.  The proposed research will tackle open issues in the construction of efficient estimates and tests in semiparametric models with an emphasis on models with dependent observations.  Such models include stationary and ergodic Markov chains and other time series models which are plentiful in many fields such as econometrics and financial mathematics.  Efficient estimation of the finite-dimensional component as well as aspects of the infinite-dimensional component will be addressed.  The latter include innovation distributions in time series models, invariant distributions of ergodic Markov chains, and stationary distributions of several consecutive observations.<br/><br/>The main emphasis of the proposed research will be to develop a methodology for the construction of efficient estimates in semiparametric models with dependent observations.  In the process the proposed research will have to develop methods that deal with the difficulties associated with an efficient score function that cannot be calculated explicitly, a problem that is also of great interest for models with independent observations.  Finally, the proposed research will continue the work of the principal investigator in semiparametric regression models with an emphasis on improving existing methods of constructing root-n consistent and efficient estimates.<br/><br/>"
"0072400","Nonparametric Modeling for Nonlinear Time Series","DMS","STATISTICS","08/01/2000","08/25/2000","Zongwu Cai","NC","University of North Carolina at Charlotte","Standard Grant","Xuming He","07/31/2004","$49,999.00","","caiz@ku.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","MPS","1269","0000, OTHR","$0.00","This project covers a variety of methodological developments and foundational research.<br/>In particular, the object of this research is to develop some new nonparametric model building<br/>and identication procedures for high-dimensional nonlinear time series data. The investiga-<br/>tor studies both the varying-coefficient regression models and nonlinear additive regression<br/>models for nonlinear time series data and makes the models practically applicable. The first<br/>objectiveis to develop nonparametric estimation procedures for nonlinear time series. In<br/>particular, for varying-coefficient regression models, local polynomial regression techniques<br/>are used to estimate the coefficient functions and the asymptotic properties of the resulting<br/>estimators are studied. This procedure differs from the classical local linear regression for<br/>curve fitting where the response surface is of interest. Here, of interest is to estimate the<br/>coefficient functions. For nonlinear additive regression models, the investigator uses partial<br/>residual method to estimate unknown additive functions coupled with projection method<br/>and local polynomial fitting. The approach is divided into two steps. In the first step, the<br/>initial estimated values for all components are obtained by using the projection method. In<br/>the second step, a local polynomial technique is employed to estimate any one of compo-<br/>nents by using the initial estimated values of the rest of components. The second objective of<br/>this research is concerned with the model identification. For both models, a new goodness-<br/>of-fit test technique is proposed, based on the comparison of the residual sum of squares<br/>under the null and alternative models, to detect whether certain coefficient functions in the<br/>varying-coefficient models are constant or whether certain components in the additive mod-<br/>els are linear or whether any covariates are significant in the models. The null distribution<br/>of the test is estimated by a nonparametric bootstrap method. Also, the investigator studies<br/>the practical implementation of the methods, particularly the automatic bandwidth selection<br/>method based on a modified generalized cross-validation. A nonparametric version of Akaike<br/>information criterion is proposed for choosing the smoothing variable in varying-coefficient<br/>models and determining an appropriate model for given data.<br/>This research is concerned with model estimation and identification procedures in non-<br/>linear time series analysis. A time series is a set of data observed over a period of time. For<br/>example, daily ozone and pollutant levels from environmental study, quarterly earnings for<br/>a company or daily currency exchange rate from economical study, wild animals' popula-<br/>tion observed over years from ecological study, number of in uenza cases observed over time<br/>from epidemiology, and noisy telecommunication signals. The investigator has been active<br/>in this research area. Furthermore, the investigator has conducted preliminary work on the<br/>described problem area which has led to encouraging results. There are sufficient reasons<br/>to believe that by pursuing the topics outlined in this proposal, the results of this research<br/>should have significant contributions in nonlinear time series analysis, which has many impor-<br/>tant applications in the fields of economics, social science, medicine, epidemiology, biology,<br/>environment, physical sciences, engineering, and many others."
"0071438","Fractional Factorial Designs:  Minimum Aberration and Related Topics","DMS","STATISTICS","08/15/2000","05/28/2002","Ching-Shui Cheng","CA","University of California-Berkeley","Continuing grant","grace yang","07/31/2005","$180,000.00","","cheng@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","Cheng, Ching-Shui<br/>DMS-0071438<br/>Abstract<br/><br/>Minimum aberration has been a well accepted criterion for choosing good fractional factorial designs. This research studies variants of the minimum aberration criterion in several different settings including block designs in which the experimental units are grouped into more homogeneous blocks to improve the precision, and split-plot designs in which some factors are held constant within each block.  Split plots arise when some factors require larger experimental units than others, or when the effects of certain factors are not of major interest, but they are included in the experiment to study their interactions with other factors.  The latter has a very important application to robust parameter designs in quality improvement. These settings have their own special features that call for different optimality criteria.   Existing work did not address the issue that there are two different errors in split-plot structures.  The ultimate goal of this research is to obtain general results on the structures of optimal designs in various settings, and to develop useful algorithms for constructing designs which can incorporate user-supplied prior knowledge and requirements.  For the former, the Principal Investigator uses tools from coding theory and finite projective geometry.  Finally, nonregular designs, including supersaturated designs, are studied under a newly introduced criterion of generalized minimum aberration.<br/><br/>Experimental design is used extensively in a wide range of scientific and industrial investigations.  In industrial experiments, often a large number of factors have to be studied, but the experiments are expensive to conduct.  In this case, the so called fractional factorial designs, in which only a small fraction of all the possible combinations are observed, are particularly useful.  In recent years, factorial designs have received considerable attention, mainly due to the success in applying them to conduct experiments for improving quality and productivity in industrial manufacturing.  This research is to study the construction of efficient designs to extract more information, especially when systematic sources of variation (such as heterogeneity of experimental material or day-to-day environmental variations) need to be eliminated to improve the precision.  <br/>"
"0071681","Statistical Methodology in Astronomy","DMS","STELLAR ASTRONOMY & ASTROPHYSC, STATISTICS","09/01/2000","06/25/2002","John Rice","CA","University of California-Berkeley","Continuing grant"," Shulamith T. Gross","08/31/2004","$250,166.00","","rice@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1215, 1269","0000, OTHR","$0.00","                         ABSTRACT<br/><br/>Professor Rice proposes research on statistical problems arising in two large astronomical projects: the variable star database of the MACHO project (a search for dark matter in the halo of our galaxy) and Taiwanese American Occultation Survey (TAOS), a search for comets in the Kuiper belt.  The first involves statistical characterization and modeling of tens of thousands of light curves from variable stars. He proposes to approach these problems from the viewpoint of  functional data analysis, further developing and extending the methodology of this rapidly growing area of statistical research.   The TAOS project will monitor star fields for occultations by objects in the Kuiper belt, using dedicated telescopes in the interior mountains of Taiwan.  The primary statistical problems center around designing image processing and signal detection procedures that will operate in real time at high sampling rates to detect rare and faint signals.  Both projects are anticipated to make valuable contributions to both advancement of knowledge in astronomy and to the development of statistical methodology.<br/><br/>This is an interdisciplinary proposal involving mathematical statistics and astronomy, centering around two projects in astronomy: (1) The analysis of a large database of variable stars.  These are stars whose light is not constant, but changes in a periodic fashion.  Better understanding of this population of stars is important for models of stellar evolution and also for determining distances to remote objects in the universe. (2) The TAOS project will probe our solar system in the remote region beyond the orbit of Neptune.  It is thought that there may well be hundreds of millions of objects such as comets there, but because of their relatively small size and remoteness, they are very difficult to detect. Computationally intensive statistical methodology will play a key role in detecting these objects.  Partial funding for this project was provided by the Stellar Astronomy and Astrophysics (SAA) Program.<br/><br/>"
"0072827","Combining EM and Monte Carlo to Maximize Intractable Likelihood Functions","DMS","STATISTICS","07/15/2000","05/22/2002","James Hobert","FL","University of Florida","Continuing grant","Xuming He","05/31/2004","$345,946.00","James Booth","jhobert@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","0000, OTHR","$0.00","ABSTRACT<br/><br/>This proposal concerns parameter estimation methodology for an<br/>important class of statistical models.  These models, which we refer<br/>to as ``generalized hierarchical models'' (GHMs), have proven to be<br/>extremely useful in a wide range of scientific endeavors.  Specific<br/>examples of applications that have appeared in the scientific<br/>literature include: the impact of passive smoking, changes in voting<br/>behavior, clinical trials for a treatment for epilepsy, social surveys<br/>of households in rural China, sheep cloning experiments, the<br/>estimation of animal abundance and the analysis of multivariate<br/>survival data.<br/><br/>A general purpose algorithm for estimation called EM<br/>(expectation-maximization) is particularly suited to the GHM setting.<br/>However, implementation the algorithm is complicated because it often<br/>involves the calculation of intractable multi-dimensional integrals.<br/>Two methods for dealing with these integrals that have received some<br/>attention recently are Monte Carlo EM (MCEM) and Stochastic<br/>Approximation EM (SAEM).  Both methods involve replacing intractable<br/>integrals by Monte Carlo approximations.  Unfortunately, applications<br/>of the these Monte Carlo fitting algorithms to GHMs can often take<br/>hours or even days to converge.  This has limited their widespread use<br/>so far.  The situation will no doubt improve with the availability of<br/>faster computers.  However, the development of much faster algorithms<br/>would accelerate this process tremendously.  Hence, the proposed<br/>research concerns a detailed comparison of the MCEM and SAEM<br/>algorithms with a view towards substantially improving their<br/>performance.<br/>"
"0071363","Heat Kernel Analysis and Zeta Functions on Quotients of Symmetric Spaces","DMS","ALGEBRA,NUMBER THEORY,AND COM, STATISTICS","07/15/2000","08/01/2002","Jay Jorgenson","NY","CUNY City College","Continuing grant","Andrew D. Pollington","06/30/2004","$155,210.00","","jjorgenson@mindspring.com","Convent Ave at 138th St","New York","NY","100319101","2126505418","MPS","1264, 1269","0000, 9251, OTHR","$0.00","Abstract for proposal of Jorgenson<br/><br/>Jay Jorgenson proposes to continue ongoing research with his co-authors into applications of heat kernel analysis to number theory.  Jorgenson and Serge Lang will study spectral theory on finite volume quotients of symmetric spaces using generalizations of Eisenstein series defined using heat kernels, as opposed to classical Eisenstein series which are defined using automorphic forms.  Jorgenson and Lang plan to investigate questions involving analytic continuation, functional equations, and special values of their heat Eisenstein series.  Upon completion of this component of their work, Jorgenson and Lang then will investigate applications of heat Eisenstein series to Weyl's law as well as to constructions of zeta functions which will yield higher rank generalizations of Selberg zeta functions.  In collaboration with Jurg Kramer, Jorgenson will study analytic aspects of Arakelov theory.  In recent work, Jorgenson and Kramer study bounds on special values of Selberg's zeta function and asymptotic bounds through covers.  Jorgenson and Kramer propose to extend these results to study asymptotic behavior of Faltings's delta function through covers, thus extending results first obtained by Jorgenson in his 1989 Stanford Ph.D. thesis.  In collaboration with Jozek Dodziuk, Jorgenson plans to use Lang's definition of degenerating number fields (from Lang's 1971 Inventiones paper) to study spectral theory on the corresponding sequence of degenerating Hilbert modular varieties.  The proposed methods to employ involve generalizations of research previously obtained by Jorgenson with Dodziuk and with Huntley and Lundelius.  As time permits, Jorgenson plans to study proposed additional research problems with Carol Fan, Edward Jenvey and Peter Grabner.<br/><br/>Classically, the heat kernel is a function defined for positive values of time t and points x and y in a domain D, and the heat kernel measures the amount of heat at point x in D at time t when a unit burst of heat is introduced at point y in D at time zero.  Although the origin of the heat kernel lies in physics, the mathematics surrounding the function known as the heat kernel manifests itself in virtually every area of pure and applied mathematics. In addition, the heat kernel is present in the theoretical foundations of many fields of statistics as well as econometrics and, more specifically, financial mathematics.  Part of  the research undertaken by Jay Jorgenson and his collaborators involves understanding various ways in which the heat kernel appears in one area of mathematics and then translate the questions, theorems and techniques to other areas of mathematics, statistics, and economics.  Going beyond mathematical research, Jay Jorgenson proposes to extend his research endeavors to incluce applications of heat kernels to practical problems of financial mathematics and economics, which naturally includes developing ways in which one can program constructions of heat kernels in order to obtain precise, numerical evaluations.<br/><br/><br/>"
"9981741","First Midwest Conference for New Directions in Experimental Design to be held May 18-20, 2000, in Columbus, Ohio","DMS","STATISTICS","01/01/2000","11/03/1999","Angela Dean","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","William B. Smith","12/31/2000","$12,500.00","","dean.9@osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","Abstract:<br/>DMS-9981741  PI: Angela Dean<br/>             Co-PIs:  Kathryn Chaloner, Dennis Lin, Dibyen Majumdar<br/><br/>The First Midwest Conference for New Directions in Experimental Design is<br/>planned for May 18-29, 2000, at the Ohio State University.  The meeting will<br/>focus on the active areas of research in experimental design that have<br/>relevance to industry, especially engineering and manufacturing.  Areas of<br/>emphasis are design for nonlinear models, experimentation for factor<br/>screening, design for multiple linear responses, experimentation in the<br/>presence of trends, computer experiments, and various other approaches<br/>including Bayesian methods.  In addition to the invited talks, ample time<br/>will be given for small group discussions in which new researchers will<br/>meet with key speakers, mentors and other participants.  A panel discussion is<br/>planned for the final day of the conference to discuss the future of<br/>experimental design.  <br/><br/>Although the geographic area for inclusion of participants is restricted,<br/>the industrial use of experimentation in the face of time dependent,<br/>infrequent observations makes the topic of current major concern.  This venue<br/>will help focus part of the statistics community on these problems.<br/><br/>"
"9987821","SRCOS/ASA Summer Research Conference in Statistics, Wiliamsburg, Virginia, June 2000","DMS","STATISTICS","04/01/2000","04/12/2000","William Padgett","SC","University of South Carolina at Columbia","Standard Grant","Joseph M. Rosenblatt","03/31/2001","$4,000.00","","padgett@stat.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","MPS","1269","0000, OTHR","$0.00","Abstract<br/>Padgett, DMS-9987821<br/><br/>The Southern Regional Council on Statistics (SRCOS) and the American Statistical Association (ASA) are organizing a summer Research Conference in Statistics (SRCS) in June 2000 in Williamsburg, Virginia. The objectives of the conference are to bring together senior researchers, young researchers, and advanced doctoral students to focus on several current research areas in Statistics, as well as to discuss some current and future issues in statistical education and training, in a relaxed and informal setting. The conference is intended to be small, with approximately 50-60 participants mostly from the fourteen-state SRCOS region. This size allows for maximum participant interactions for the 3 1/2 days duration. Standard sessions consist of invited one hour talks with all speakers urged to give detailed background and review at the beginning of the talks, leading up to current results, problems, and issues on their topics. This format provides an entry point into the research topic for participants interested in beginning to investigate new areas, especially young researchers.<br/><br/>The SRCS in year 2000 is a special conference with the emphasis on topics in ""Industrial Statistics"" in honor of the well-known statistician, Professor Stuart Hunter. A poster session by advanced doctoral students and young researchers also reflect this theme. A special four-hour session on day one will feature Dr. Hunter and four to five other well-known researchers and is designated as ""Stu Hunter Day."" The special emphasis provides the doctoral students and young researchers with a unique opportunity to become acquainted with, and interact with these senior researchers over the entire conference. The students would most likely not have such an opportunity otherwise. and the NSF is funding support for eight doctoral students to participate."

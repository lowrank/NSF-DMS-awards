"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"2131821","CAREER: Statistical Inference of Tail Dependent Time Series","DMS","STATISTICS","07/01/2021","08/14/2023","Ting Zhang","GA","University of Georgia Research Foundation Inc","Continuing Grant","Yong Zeng","06/30/2024","$327,234.00","","tingzhang@uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","1045","$0.00","The project aims to develop a new theoretical framework and statistical inference methods for the analysis of tail dependent time series, and to educate future statisticians and data scientists on its theory and practice. Tail dependent time series, as an emerging data type, has been observed and recognized in various fields including actuarial science, climate science, economics, finance, hydrology, and internet traffic engineering, among others. The task of understanding and appropriately accommodating the phenomenon of tail dependence can be of significant importance to the modeling of extreme events such as earthquakes, hurricanes, and financial crises. The results from the project will make significant impacts in scientific areas such as climate science, economics, actuarial science, finance, hydrology and internet traffic engineering. The proposal also involves an integrated education plan to expose undergraduate and high school students to the topic, to equip graduate and advanced undergraduate students with a desirable level of statistical reasoning and analytical skills for analyzing tail dependent time series, and to mentor doctoral students to become future leaders in the education and research of the area. <br/><br/>Existing methods for studying tail dependent time series often rely on certain parametric models for describing the underlying tail dependence structure. This is particularly due to the lack of a convenient and rigorous framework that one can use to obtain desired limit theorems for a general class of tail dependent time series. The project aims to address this fundamental problem by proposing a new framework based on the causal representation and the technique of adversarial tail coupling. Using the newly proposed framework, the project will develop meaningful results toward a tail m-dependent approximation scheme, which can then be used as a powerful tool to obtain limit theorems for statistics of tail dependent data. Compared with the conventional m-dependent approximation, the current setting can be more challenging due to the double asymptotics where the quantile index is allowed to approach either zero or one as the sample size increases to reflect extreme risks. The project will study several statistical inference problems for tail dependent time series, including high quantile estimation and its associated confidence interval construction, tail dependence visualization and testing, inference of extremely high quantiles using the extreme value theory, extensions to high and extremely high quantile regression models, and high-dimensional nonstationary settings. The results to be developed are expected to be useful in identifying undiscovered features in certain climate science and economic data, and applicable to other scientific problems that involve the analysis of tail dependent time series.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2038080","RTG: Mathematical Foundation of Data Science at University of South Carolina","DMS","STATISTICS, WORKFORCE IN THE MATHEMAT SCI, Combinatorics, CDS&E-MSS, EPSCoR Co-Funding","08/01/2021","08/07/2023","Linyuan Lu","SC","University of South Carolina at Columbia","Continuing Grant","Stacey Levine","07/31/2026","$1,765,413.00","Wolfgang Dahmen, Qi Wang, Wuchen Li, Pooyan Jamshidi Dermani","lu@math.sc.edu","1600 HAMPTON ST # 414","COLUMBIA","SC","292083403","8037777093","MPS","1269, 7335, 7970, 8069, 9150","075Z, 079Z, 102Z, 7301, 9150, 9251","$0.00","This Research Training Group (RTG) project is a joint effort of Mathematics, Statistics, Computer Science and Engineering. It aims to develop a multi-tier Research Training Program at the University of South Carolina (UofSC) designed to prepare the future workforce in a multidisciplinary paradigm of modern data science. The education and training models will leverage knowledge and experience already existing among the faculty and bring in new talent to foster mathematical data science expertise and research portfolios through a vertical integration of post-doctoral research associates, graduate students, undergraduate students, and advanced high school students. A primary focus of this project is to recruit and train U.S. Citizens, females, and underrepresented minority (URM) among undergraduate and graduate students, and postdocs through research led training in Data Science. The research and training infrastructure implemented through this RTG program will not only support the planned majors and master?s degrees, but also provide systemic educational curricula for students and researchers from other areas whose research would benefit from Data Science within UofSC and in the vicinity. The training materials created by this RTG program will also be widely available to other institutions across the country. The RTG project will help build a highly educated workforce for academia, government and industry, in the area of data science, artificial intelligence, and machine learning. <br/><br/>This project is a response to emerging demands of modern technology-oriented societies for an innovative workforce with expertise in all areas related to Data Science. Based on a comprehensive view of Data Science, the program aims at providing students and postdocs with the necessary concepts that enable them to form their own research agenda. Our program covers, on the one hand, emerging developments in network science, artificial intelligence, machine learning, and optimization methodologies from computer science and statistical perspectives primarily for the Big-Data regime with applications such as autonomous systems. In addition, problems typically posed in a Small-Data regime can relate these concepts to relevant methodologies, such as Physics Informed Learning, needed to understand mathematical models, usually formulated in terms of Partial Differential Equations (PDEs), so as to understand key techniques for synthesizing models and data in the context of Uncertainty Quantification. Properly interrelating these activities in the broader Data Science landscape, will enable students to successfully tackle new problem areas at later stages of their career and address important challenges in sciences and engineering. The corresponding theoretical training is reinforced by accompanying practical training modules that are able to engage students across all levels as well as young researchers in synergistic activities, even reaching out to local industries. It is a feedback-loop between research and education that distinguishes the project. The educational component is designed with an ultimate goal of developing an innovative research training program to educate future workforce in a structured curriculum that offers a major, a master?s degree and a 4+1 dual degree in Data Science at UofSC. The project facilitates team-teaching by relevant experts and uses direct links to research projects that students will participated in. The built-in vertical and horizontal pedagogical synergies as well as the hierarchical mentoring scheme expose participating students to extensive educational and research experience offered by the program. This project is jointly funded by Computational and Data-enabled Science and Engineering in Mathematical and Statistical Sciences (CDS&E-MSS), the Established Program to Stimulate Competitive Research (EPSCoR), and the Workforce Program in the Mathematical Sciences, among others.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113779","Collaborative Research: Statistical Inference for High-dimensional Spatial-Temporal Process Models","DMS","STATISTICS","07/01/2021","06/10/2021","Wenpin Tang","NY","Columbia University","Standard Grant","Yong Zeng","06/30/2024","$120,929.00","","wt2319@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","079Z","$0.00","Spatial data science and other emerging technologies related to Geographic Information Systems are increasingly conspicuous in scientific discoveries. Scientists in a variety of disciplines today have unprecedented access to massive spatial and temporal databases comprising high resolution remote sensed measurements. Statistical modeling and analysis for such data need to account for spatial associations and variations at multiple levels while attempting to recognize underlying patterns and potentially complex relationships. Traditional statistical hypothesis testing is no longer adequate for these scientific problems and statisticians are increasingly turning to specialized methods for analyzing complex spatial-temporal data. However, there continue to remain substantial theoretical and methodological bottlenecks with regard to the interpretation of statistical models. This project will address these problems by developing probabilistic machine learning tools for spatial-temporal Big Data that can have far-reaching public health, economic, environmental, and scientific implications. Several innovations in statistical theory, methodologies and computational algorithms are envisioned that will inform basic science and policy questions arising in diverse disciplines using geographic information sciences. Key educational components include dissemination of technologies across the scientific communities including data scientists, engineers, foresters, ecologists, and climate scientists. The Principal Investigators will train the next generation of data scientists through dissemination efforts for graduate students in STEM fields.        <br/><br/>The PIs aim to blend innovative theory, methods and applications to advance knowledge of spatial-temporal stochastic processes with an emphasis on their properties for high-dimensional inference. This domain of spatial statistics has witnessed a burgeoning of models and methods for Big Data analysis. New classes of models have emerged from the judicious use of directed acyclic graphs (DAGs) that are being applied to massive datasets comprising several millions of spatiotemporal coordinates. Theoretical explorations envisioned in this project will focus upon statistical inference on the process parameters and the underlying spatial process. The PIs intend to perform rigorous investigations into statistical inference for high-dimensional spatio-temporal processes to derive micro-ergodic parameters for such models that will be consistently estimable and, at the same time, yield consistent predictive inference. The PIs will develop new methodologies that cast high-dimensional stochastic processes into computationally practicable frameworks by embedding graphical Gaussian processes within hierarchical frameworks for jointly modeling highly multivariate spatial data. Innovative statistical theory and methods will be developed and used to construct sparsity-inducing graphical spatio-temporal models to accommodate massive numbers of outcomes and capture complex dependencies among variables across massive numbers of locations. The planned theoretical explorations into the inferential properties of newly emerging scalable spatio-temporal processes will produce novel statistical contributions. The PIs will provide probability-based uncertainty quantification and will substantially enhance the understanding of physical and natural processes underlying various problems in the physical, environmental and biomedical sciences and in public health.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2109155","Learning with Confidence: Bootstrapping Error Estimates for Stochastic Iterative Algorithms","DMS","APPLIED MATHEMATICS, STATISTICS","09/01/2021","07/09/2021","Purnamrita Sarkar","TX","University of Texas at Austin","Standard Grant","Stacey Levine","08/31/2024","$299,966.00","Rachel Ward","purna.sarkar@austin.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1266, 1269","075Z, 079Z","$0.00","In this age of large-scale high-dimensional data analysis, stochastic iterative optimization methods constitute a popular class of algorithms, which look at one data point at a time. These algorithms are increasingly entrusted with decision-making tasks, from discarding emails as spam to recommending routes when one drives to work, to the future decisions made by self-driving cars and medical images produced for diagnostic purposes. These applications give rise to noisy data resulting from measurement errors and variability in the experimental setup or environment.  Despite this, almost all current theoretical results focus on the accurate estimation of underlying parameters. However, it is crucial to pay attention to confidence intervals that quantify the variability of the learned parameters, which allows one to make decisions with confidence. This research will develop a mathematical framework for the estimation of uncertainty of a broad class of stochastic iterative optimization methods. This project will also help train graduate students and postdoctoral scholars in uncertainty estimation for large-scale learning problems, thus making them better prepared for careers in both industry and academia.<br/> <br/>The investigators will establish central limit theorems and consistent online bootstrap procedures for fundamental stochastic iterative learning algorithms, incorporate these algorithms in software packages, and develop a framework for assessing confidence in point estimate predictions in a broad range of applications. These goals will be achieved using a variety of recently developed tools, including concentration inequalities for products of random matrices, high dimensional statistics, and theoretical advances in deep learning optimization methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113359","Collaborative Research: New Statistical Methods for Microbiome Data Analysis","DMS","STATISTICS","09/01/2021","07/26/2021","Xianyang Zhang","TX","Texas A&M University","Standard Grant","Yong Zeng","08/31/2024","$170,000.00","","zhangxiany@stat.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","068Z","$0.00","The human microbiome, the collection of micro-organisms associated with the human body, has been increasingly recognized as an important player in human health and disease.  Human microbiome research focuses on deciphering the intricate relationship between the microbiome and the host and identifying microbial biomarkers for disease prevention, diagnosis, and treatment.  Current technologies to study the human microbiome involve sequencing the microbial DNA in the sample, upon which the identity and the abundance of the micro-organisms can be determined.  Analysis of such microbiome sequencing data raises many statistical challenges. First, the data are zero-inflated. A typical microbiome dataset contains more than 75% zeros. Second, the data are compositional. The abundance change in one microbe will automatically lead to changes in the relative abundance of others, making identification of the ""driver"" microbe difficult. Third, the microbes are phylogenetically related. Closely related microbes usually share similar biological traits. Finally, the human microbiome is subject to many environmental confounders. Controlling these confounders is essential to make valid statistical inferences.  The project will develop novel statistical methods for analyzing microbiome data addressing these challenges. The research results will be disseminated through scientific publications as well as seminar and conference presentations. The PIs will develop, distribute, document, and maintain R software packages via GitHub and CRAN for developed methods, and provide tutorials with example datasets. The PIs will test the software in real-world settings thoroughly. Given the popularity of the multi-omics approach to study the human microbiome, the delivered software packages will be of particular interest to microbiome investigators. The PIs will train students at the intersection of high-dimensional statistics, optimization, and genomics.<br/><br/>The project has two research thrusts. In the first thrust, the PIs will develop a new statistical learning framework for microbiome data to simultaneously tackle the high-dimensionality, compositional effect, zero-inflation, and phylogenetic information. In particular, the new framework includes a novel zero imputation method based on a new Dirichlet mixture model, a general approach for handling compositional effect in supervised/unsupervised statistical learning, and a robust structure adaptive method to incorporate external information encoded in the phylogenetic tree. In the second thrust, the PIs will develop a two-dimensional false discovery rate (FDR) control procedure for powerful confounder adjustment in microbiome association analysis. The procedure uses the test statistics from the unadjusted analysis as auxiliary statistics to filter out a large number of irrelevant features, and false discovery rate control is then performed based on the test statistics from the adjusted analysis on the reduced set. The PIs will investigate both model-based and model-free approaches, and prove the asymptotic FDR control.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113079","Computational Methods for Structured Data and Models","DMS","STATISTICS","08/15/2021","08/02/2021","Maryclare Griffin","MA","University of Massachusetts Amherst","Standard Grant","Yong Zeng","07/31/2024","$149,982.00","","maryclaregri@umass.edu","COMMONWEALTH AVE","AMHERST","MA","01003","4135450698","MPS","1269","","$0.00","Scientists in many fields, including genetics, neuroscience, ecology, and economics, are obtaining richer measurements of more complex processes than ever before. This offers thrilling opportunities to answer scientific questions that were previously out of reach. For instance, measurements of disease prevalence collected at many locations over time provide the opportunity to estimate the spread of disease. Latent variable models, which model the observed data as a simple transformation of unobserved latent random variables, are a popular approach to extracting answers to scientific questions from measurements of complex processes. They are flexible, but they can also be computationally prohibitive. As a result, scientists using latent variable models may have to settle for approximations of unknown quality or ad-hoc simplifications that provide poor estimates or fail to answer questions of interest. This project aims to develop novel methods for fitting latent variable models that are more computationally efficient, reliable, and accessible. Involvement in the project will train statisticians at all levels, with a focus on statisticians from populations that are underrepresented in statistics research. Specifically, the investigator will supervise graduate student involvement in the research, guide the development of engaging outreach materials by undergraduate students, participate in outreach at local high schools, and lead writing groups for early-career faculty. <br/><br/>A variety of computational challenges arise when fitting latent variable models. It can be difficult to characterize and simulate from the conditional distribution of the latent variables given observed data, even when the small number of parameters characterizing the latent variable model are known. Furthermore, it can be difficult to estimate the unknown parameters of a latent variable model because the likelihood of the data corresponds to a high dimensional integral for which a closed-form expression may be unavailable or expensive to evaluate. Even when feasible methods are available, it can be difficult for practitioners to implement latent variable models without access to open-source software and detailed tutorials. Accordingly, this project aims to contribute (i) novel methods for simulating from the conditional distributions of high dimensional latent variables, (ii) improved methods for maximum likelihood estimation of latent variable model parameters, and (iii) versatile statistical software that allows practitioners to implement them.  Regarding (i), the PI plans to develop novel pathwise methods for simulating from the conditional distributions of high dimensional latent variables given data that leverage the relationship of the target conditional distribution to related or approximate distributions. Regarding (ii), the PI will develop improved methods for maximum likelihood estimation of latent variable model parameters that leverage the pathwise simulation methods introduced in the first aim. Regarding (iii), the PI will apply the new methods to disease mapping and genome-wide association studies and develop R packages that allow other practitioners to implement the methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2048028","CAREER: New Challenges in High-Dimensional and Nonparametric Statistics","DMS","STATISTICS","07/01/2021","07/04/2023","Mayya Zhilova","GA","Georgia Tech Research Corporation","Continuing Grant","Yulia Gel","06/30/2026","$213,242.00","","mzhilova7@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","1045","$0.00","Contemporary techniques for analysis of complex high-dimensional data sets give rise to numerous questions about fundamental concepts in statistics, data science, and related fields. In this project, the PI will address challenging open questions in high-dimensional and nonparametric statistics motivated by practical applications in finance, engineering, and life sciences. The project is focused on development of new methods of statistical inference for complex data sets providing high accuracy and explicit theoretical guarantees. This includes (i) development of a novel framework for statistical inference that will considerably extend the range of applicability of some of the major statistical methods; (ii) studies of performance of resampling methods in a high-dimensional framework; and (iii) studies of intrinsic properties of high-dimensional models that ensure good performance of the statistical methods. The educational component of the project includes mentorship of graduate and undergraduate students, summer camps in statistics and data science for STEM-oriented high school students, and a workshop/graduate school on high-dimensional statistics and learning theory for junior researchers. Special attention will be given to supporting students and researchers from underrepresented minorities.<br/><br/>The project is focused on two major research themes. The first theme is concerned with establishing non-asymptotic higher-order expansions for various distances between probability distributions, with a particular focus on problems and applications in a high-dimensional non-asymptotic setting. The PI will study characteristic properties that are crucial for establishing accurate approximation bounds in high dimensions, such as the normal approximation and bootstrapping and their relations and optimality properties. Another major theme of the project is development of a novel framework for statistical inference based on nonlinear modeling and its applications to nonparametric inference, functional estimation, and inference for models involving heavy-tailed distributions. The approach combines both parametric and nonparametric components, which can avoid severe model misspecification and establish good rates of approximation. The PI aims at conducting a comprehensive study of the new higher-order approximation bounds and the nonlinear modeling approach. The project includes development of new mathematical methods for statistical inference and studies of their connections with other areas of mathematics, such as high-dimensional probability, stochastic modeling, and uncertainty quantification.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2112907","Novel Missing Data Approaches for Corrupted Longitudinal Data","DMS","STATISTICS","07/15/2021","07/09/2021","Yen-Chi Chen","WA","University of Washington","Standard Grant","Yong Zeng","06/30/2024","$147,253.00","","yenchic@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","096Z","$0.00","Modern longitudinal databases, which can involve combined multiple datasets and varying measurements, may not be complete and clean. Because of the incompleteness, performing downstream statistical analysis is not straightforward. This project aims to develop novel statistical methods to handle incompleteness from missing data perspectives. The project will also deal with the data linkage issue from cancer research, where researchers have linked their clinical trial data to the Centers for Medicare & Medicaid database. The methods under development will be applied to the National Alzheimer?s Coordinating Center database and Prostate Cancer Prevention Trial data in the Southwest Oncology Group (SWOG) Cancer Research Network. The methods will also be used to resolve data collection problems caused by the COVID-19 pandemic and other infectious diseases that corrupt data collection. The project offers training for graduate students and research opportunities for undergraduate students.<br/><br/>The project focuses on three research questions. First, the PI aims to develop an inverse probability weighting (IPW) approach to handle linking of one longitudinal database with another database. This IPW method reweights observations based on the linking probability to account for the linking issue. The project will apply the IPW method to the data linkage issue and develop a new efficiency theory. The second part of the project considers the changing-measurement problem in a longitudinal database, in which a measurement is updated to a newer version during the collection of longitudinal data. The PI intends to formulate this as a missing data problem and introduce a new approach combining latent variable and quantile regression to create a conversion between the new and the old versions of the measurement. In the third part of the project, the PI plans to develop a ""doubly"" semi-parametric estimator for handling missingness in both responses and covariates and to study the efficiency theory. The PI will design a set of identifying assumptions on the missingness of covariates to ensure identifiability. A method can then be derived from the identifying assumptions to impute the missing covariates, converting the situation to a standard one in which responses alone are missing.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2112711","Measure of Heterogeneity for Complex Data Objects","DMS","STATISTICS","07/01/2021","06/11/2021","Heping Zhang","CT","Yale University","Standard Grant","Yong Zeng","06/30/2024","$299,993.00","","heping.zhang@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","075Z, 079Z","$0.00","Technological advances have led to the routine collection of large and complex data objects such as matrices, tensors, functions, and manifolds. Understanding and drawing conclusions from large and complex data sources with a sound scientific rationale is challenging. The level of difficulty generally increases with the size and complexity of the data. Most existing statistical tools are designed to deal with a number as the unit of information, and they become inadequate, because a human face, for example, can be digitalized into many numbers but must be analyzed as a whole, to retain the essential information. The goal of this project is to develop proper statistical methods and software for this need. The concept and methods to be developed will provide vital tools for the analysis of complex data, which have broad applications in both science and engineering. This project will not only advance statistical methodology and disseminate computing software but also offer solutions to important and challenging scientific and public health problems. Applications in understanding and diagnosing Alzheimer's disease and breast cancer are two examples of major public health significance.  Furthermore, by engaging doctoral and postdoctoral students, junior faculty members, and summer interns, the PI will take advantage of this project in training new generations of statisticians and data scientists. <br/><br/>To consider the complex data structures and to retain the essential information during the statistical analysis, it is important to treat certain structures as the observational point such as the functional resonance imaging (MRI) collected from a person at a given time. Such high dimensional points are referred to as tensors which may or may not fit in a traditionally defined Euclidean space. This project aims to develop statistical methods to analyze tensors as data objects, and classify such data objects in a possibly non-Euclidean space. In particular, this project introduces the concept of ball impurity as a measure of heterogeneity in the distribution of complex data objects and will investigate its use in developing tree-based methods to classify data objects in non-Euclidean spaces. The efforts will be made for an in-depth understanding of the theoretical and empirical properties of the ball impurity, and software will be developed and distributed simultaneously. The developed methods will be used to analyze large-scale, public databases from UK Biobank and dbGaP such as Human Connectome Project. The analyses of these important datasets can not only assess the utility of the novel methods, but also lead to insightful and new scientific discoveries.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113564","Homogeneity Pursuit in Regression Analysis: Statistical Theory, Integer Optimization, and Algorithms","DMS","STATISTICS","06/15/2021","06/08/2021","Peter Song","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yong Zeng","05/31/2024","$349,978.00","","pxsong@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","079Z","$0.00","The research project lies in a cross-disciplinary field that intersects statistics, operations research, and machine learning with a motivation to study unknown quantities and associated structures of the unknowns, referred to as homogeneity pursuit. The novelty of the statistical paradigm of homogeneity pursuit pertains to its capacity for simultaneous operation of parameter clustering and estimation in association analyses. The project expects to deliver new statistical tools to the hands of practitioners to generate new knowledge from data. The principal investigator will apply the developed methodology for the derivation of environmental exposure mixtures of toxic agents, DNA methylation integration in epigenetics, and survey questionnaire summarization in social sciences. The project also includes substantial educational initiatives involving graduate students and exposing trainees to state-of-the-art research in the topics related to the research activities. <br/><br/>The project develops a new statistical framework for association analyses in which similar model parameters are fused into subgroups while being estimated. The developed methodology harnesses mixed integer programming (MIO) to extend the best-subset regularization to perform a simultaneous operation of parameter clustering and estimation in regression analysis.  It also provides both analytic and algorithmic tools to improve the existing statistical solutions.  First, the project builds a new MIO formulation of simultaneous clustering and estimation for high-dimensional model parameters. The framework is flexible and efficient to fit a wide range of important statistical models, including generalized linear models (GLMs) for cross-sectional data, varying coefficient index models for nonlinear interactions, and mixed-effects models for longitudinal data. Second, to solve MIO problems, the project develops and implements a new fast and reliable algorithm, termed as Alternating Penalized Operator for L-zero Loss Optimization (APOLLO). Third, the project plans to establish both finite and large sample properties of the MIO estimator for group memberships and group parameters in GLMs and semiparametric models and investigate the theory of integer optimization to justify the MIO solver, APOLLO.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113404","Data Integration Via Analysis of Subspaces (DIVAS)","DMS","Genetic Mechanisms, STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2021","07/19/2021","James Marron","NC","University of North Carolina at Chapel Hill","Standard Grant","Yulia Gel","08/31/2024","$500,000.00","Jan Hannig","marron@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1112, 1269, 7454","068Z, 079Z, 7465","$0.00","A challenge In analyzing big data is that of multiple measurement types that are often generated simultaneously.  This research addresses this challenge from the perspective of multi-block (also known as multi-view) data.  In particular, the focus is on multiple types of measurements made on the same objects. One common example is that of multiple 'omics' measurements (e.g. gene and protein expression) in biology and medicine.  The data analytic challenge is to understand how the different measurements work together, in terms of modes of joint variation, and how they work independently in terms of individual modes of variation.  The proposed new methodology is named DIVAS, an acronym for Data Integration Via Analysis of Subspaces.  An important underlying theme is that the most useful new data analytic methods are invented in the context of interdisciplinary collaboration. The project also provides research training opportunities for graduate students. <br/><br/>DIVAS will be a breakthrough in analysis methods for multi-block data in several ways.  First, the algorithm is completely different from existing methods, using a new structure deliberately designed to facilitate partially shared blocks.  Second, statistical inference is deliberately incorporated into all aspects of DIVAS, instead of being mostly an after-thought as in most competing methods.  In particular, our applications motivate inference on both scores and loadings, which will be performed using novel principal angle based concepts.  Third, the algorithm is based on an innovative combination of perturbation bounds and random direction bounds which draws on ideas from all of probability theory, linear algebra, approximation theory and optimization.  Theoretical validation will be performed using an unusually broad range of asymptotics, that is motivated by the breadth of the driving applications. To ensure focusing on the most important aspect of the methodology, development of DIVAS will be done in direct collaboration with experts (including unfunded collaborators) in other scientific areas.  One will be breast cancer research, which typically involves very high dimensional data.  We will mathematically investigate that domain with High Dimension Low Sample Size asymptotics, where the dimensions go to infinity for fixed sample size.  The other will be Drosophila behavioral genetics with typically low dimension, which creates additional methodological challenges.  Here the completely different classical asymptotics, where the sample size grows for fixed dimension, provide the best insights into performance of the method.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2217007","Distance-Based Analysis for Complex High-Dimensional Data","DMS","STATISTICS","10/01/2021","01/26/2022","Debashis Mondal","MO","Washington University","Standard Grant","Yong Zeng","06/30/2024","$299,999.00","","mondal@wustl.edu","ONE BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","1269","079Z","$0.00","Throughout the course of the twentieth century, distances have played a significant role in important areas of statistics, which include classification, clustering, discriminant analysis, multidimensional scaling, sampling, spatial statistics, scoring rules, and kernel methods in machine learning. Distances are also central to the definition of divergence measures, relative entropy and information gain, some of which are fundamental to the concept of C.R. Rao's quadratic entropy and the analysis of diversity in ecology and other areas of science. Yet, at present there are significant gaps in our knowledge and in the emerging statistical literature on the use of distance-based tests and analyses for complex high dimensional data.  One example is analysis of similarity which is among the most cited and most widely used distance-based statistical methods but is limited by an absence of relevant mathematical knowledge. This research will derive new mathematical knowledge on various distance-based statistical methods, and apply this for providing answers to important scientific questions arising in a number of disciplines in forestry, ecology and marine science, such as: (1) how biodiversity changes in tropical forests? (2) how taxonomic and functional profiles of bacterial communities change with environmental conditions in different oceanic regions? The project establishes collaborations among several disciplines and between two US academic institutions and provides research and training to graduate and undergraduate students.<br/> <br/>The project develops a new body of knowledge on distance-based statistical methods and computation for analyzing complex, high dimensional data that arise in the form of compositions, trees, graphs, or networks. The distances considered here are all non-Euclidean -- either non-metric dissimilarities that do not satisfy any triangular inequalities or just discrete numbers -- but they all arise from conditionally positive definite kernels. Examples of distances include the squared Euclidean distance, the Bray-Curtis dissimilarity, the Jensen-Shannon distance, Unifrac or the Kantorovich-Rubinstein metric, the Aitchison distance, the edit distance, various graph kernel and spectral distances, and other distances based on optimal transport problems. Specifically, the project advances the mathematical theory and computation of exact distribution-free two and multi-sample runs tests, change points, and other related problems by counting runs along the shortest Hamiltonian path (or loop) of the pooled sample of data points. The project also considers analysis of similarity and related distance-based rank tests and derives new mathematical results that allow us to pursue more advanced statistical analyses. The project contributes to: (i) a deeper analysis of biodiversity in tropical forest; (ii)  an investigation of  how taxonomic and functional profiles of prokaryotic communities  change  with environmental conditions in different oceanic regions; (iii) a study of the variability of composition of rare earth elements in deep-sea muds of the Pacific Ocean;  and (iv) an understanding of the relationship of intertidal communities in the Oregon coast with respect to upwelling and nutrient delivery. The project integrates mathematics research, science and education and will provide opportunities for dissertation work for graduate students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113414","Mean-Field Models in Statistics","DMS","STATISTICS","07/01/2021","06/14/2021","Sumit Mukherjee","NY","Columbia University","Standard Grant","Yong Zeng","06/30/2024","$170,000.00","","sm3949@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","1269","$0.00","Frequently in high dimensional Bayesian models, the posterior distribution is complicated and exhibits non-trivial dependence. Understanding the behavior of such models, both theoretically and empirically, is a challenge. In these situations, variational inference provides an efficient and scalable method of inference, when more traditional methods based on Markov Chain Monte Carlo are computationally prohibitive. One of the most common techniques for variational inference is the naive mean field approximation. However, despite its wide usage, not much is known about rigorous guarantees for the naive mean field method. This project aims to address this question, by studying the validity of naive mean field methods for several examples of interest.<br/><br/>This project defines a formal notion of correctness of the naive mean field approximation, which requires that the leading order of the log normalizing constant of the high dimensional distribution is predicted correctly by the mean field prediction formula. This definition does not require the high dimensional distribution of interest to arise as a posterior distribution. If the naive mean field approximation is indeed asymptotically correct, the high dimensional distribution of interest should be well approximated by a mixture of product distributions. If further, the optimization in the mean field prediction formula has a unique maximizer, then essentially the high dimensional distribution should be close to a product measure, and it is natural enquire about Law of Large Numbers, Concentration, Fluctuations, and Asymptotic Properties of Estimators. The PI plans to study these questions for three concrete examples: (a) (Bayesian) Linear Regression, (b) (Bayesian) Mixture of Gaussians, and (c) Exponential Random Graph Models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113592","General Correlated Count Statistical Structures","DMS","STATISTICS","07/01/2021","06/11/2021","Robert Lund","CA","University of California-Santa Cruz","Standard Grant","Yong Zeng","06/30/2024","$220,000.00","","rolund@ucsc.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1269","1269","$0.00","This research project will focus on methods for analyzing data recorded in time and/or space that is count-valued, for example, daily county COVID-19 deaths, monthly state tornado counts, and annual North Atlantic severe hurricanes.  The data may be correlated in time, implying that counts observed today may be associated by counts occurring in the immediate past, or correlated in space, implying that adjacent observations are associated with each other.  The PI will develop statistical methods that take into account the particular type of distribution appropriate for the counts, for example, Poisson, geometric, and binomial, enabling the researcher to make more accurate forecasts and inferences. The models and methods developed here allow both positive and negative correlations in time and space, a feature not achievable with many current statistical count models.  For example, COVID-19 cases tend to cluster in groups, while oak and pine trees do not prefer to grow near one and other (negative correlation). The models to be developed in this project will permit the incorporation of covariates and allow for likelihood inference. The PI will also seek to construct the California Climate Center, a research station for climatic and environmental problems quantified by technical methods.<br/><br/>On a technical level, the PI will transform a Gaussian space-time process into a count process having the desired marginal distribution(s).  The work will prove that this setup produces the most flexible count structures achievable. Extensions involving non-Gaussian processes will be explored and methods to fit the spatio-temporal model via maximum likelihood techniques will be developed via particle filtering and sequential Monte Carlo methods.  Extensions to multivariate count time series and will be examined and asymptotic inference for some setups will be studied.  Space-time applications to snow presence/absence trends in the Northern Hemisphere will be conducted.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2046874","CAREER: Fast and Accurate Statistical Learning and Inference from Large-Scale Data: Theory, Methods, and Algorithms","DMS","STATISTICS","07/01/2021","07/13/2023","EDGAR DOBRIBAN","PA","University of Pennsylvania","Continuing Grant","Yong Zeng","06/30/2026","$240,000.00","","dobriban@wharton.upenn.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","1269","079Z, 1045","$0.00","This project will develop statistical methods for analyzing large datasets. Such massive datasets are emerging as an important challenge in many areas of science, engineering, and business. The research will pursue a multi-pronged approach to addressing several fundamental questions in the analysis of such datasets, focusing on three key areas. The first one is sketching and random projections, which is a powerful randomized approach to data analysis used when the data must be analyzed on a single machine. The second area is distributed statistical learning and inference, where datasets are spread across multiple locations, with limited communication among them. The third is model retraining, where statistical or machine learning models must be updated efficiently after data has been added or deleted from the original training set. In addition, the project will have a significant educational component, with the PI developing a new course on statistical machine learning. This project will also train a graduate student. The PI is committed to diversity and inclusion, including women and underrepresented minorities in all aspects of the project. The methods developed for the project will be made freely available as software, which will allow others to directly use and benefit from the results.<br/><br/>In the area of sketching, the project will leverage powerful tools from asymptotic random matrix theory and free probability to analyze fundamental problems, such as regression and clustering. In the area of distributed learning, the PI plans to develop and analyze statistical methods for distributed learning via gradient based optimization. For model retraining, the PI aims to study the connections between retraining and conformal prediction, with the goal of developing improved and broadly applicable methods for predictive inference. On a technical level, the work will involve advanced tools from probability theory, such as random matrix theory, as well as tools from numerical optimization. By carefully analyzing computational aspects of large-scale statistical analysis, the work will aim to bridge gaps between the statistical and computational perspectives.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113171","Advances in Causal Inference With Continuous Exposures","DMS","STATISTICS","07/15/2021","07/04/2023","Theodore Westling","MA","University of Massachusetts Amherst","Continuing Grant","Yulia Gel","06/30/2025","$177,828.00","","twestling@umass.edu","COMMONWEALTH AVE","AMHERST","MA","01003","4135450698","MPS","1269","","$0.00","Determining cause and effect is one of the fundamental goals of scientific inquiry. For instance, does a vaccine reduce risk of disease? Is a chemical such as lead or ammonia in drinking water harmful to human health? Causal inference is the area of statistical research concerned with developing methods for using data to answer such questions. The majority of causal inference research has focused on binary exposures; that is, exposures that can only take two values, such as treatment and control. However, many exposures of interest can take a large or even infinite number of values, such as the dose of a drug or vaccine received, or the concentration of a substance in drinking water. These are called ""continuous exposures"", and are commonplace in many disciplines, including biomedicine, epidemiology, public health, and economics. In this project, the PI will develop flexible statistical methods for assessing the causal effects of continuous exposures.<br/><br/>The PI will develop three methodological innovations for causal inference with continuous exposures. In the first two aims, the PI will focus on the causal dose-response curve, which describes how the causal effect changes as a function of the exposure level. In order to make valid statistical inference regarding the shape of this curve, the PI will develop a uniform confidence band for the causal dose-response curve and develop tools for assessing the fit of parametric models for the dose-response curve. These methods will permit researchers to understand the qualitative effect of a continuous exposure while making minimal assumptions. In the third aim, the PI will address nonparametric inference on the effect of incremental shift interventions, which provide useful one-number summaries of the causal effect of continuous exposures under weaker assumptions than those necessary for the dose-response curve. User-friendly software implementing the methods developed in each of these aims will be made freely available.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2111251","Mathematical and Statistical Modeling and Methodology for Topics in Diffusion Tensor Imaging","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2021","07/23/2021","Lyudmila Sakhanenko","MI","Michigan State University","Standard Grant","Yong Zeng","08/31/2024","$199,907.00","David Zhu","sakhanen@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269, 7454","068Z, 079Z, 8089","$0.00","Diffusion tensor imaging is a non-invasive magnetic resonance imaging technology that can be used to analyze the complex neuronal network of the brain. Currently, brain connectivity measurements are complicated and are difficult to validate due to a high level of noise. This project aims to model the noise in measurements and substantially improve the ability of scientific and medical end-users to make confident decisions about fiber tracts in the brain. Ultimately, this project aspires to improve early diagnostic tools for brain diseases and disorders such as Alzheimer's disease, traumatic brain injury, and multiple sclerosis. The results of the research are expected to help further develop diffusion tensor imaging technology as a reliable and practical routine clinical procedure. The investigators, with expertise in statistics, mathematics, imaging physics, engineering, and neuroscience, are also training an interdisciplinary team of young researchers, who will gain valuable exposure to both mathematical and neuroscience aspects of this project through regular meetings, graduate courses in nonparametric statistics and image processing, and other research group activities. The group also plans to stimulate interest of K-12 students on how to use ""math and stat"" to understand brain wiring structures. <br/> <br/>Integral curves are natural models for a variety of scientific phenomena, from axonal fibers in the brain, to jet streams in the atmosphere, to road outlines for self-driving cars. Traditionally, they are modeled as solutions to the orientation distribution functions defined on fields of direction vectors that are observed with noise in a 3D domain.  Advances in brain imaging technology can now provide highly complex directional information, such as longitudinal data, manifolds constructed out of integral curves, and graphic structures of the underlying axonal anatomy. Individual integral curves as well as their bundles traced using this enhanced directional data provide the potential to dramatically increase our understanding of biological phenomena such as the structural integrity of the axonal fibers and to assist in selecting an optimal scanning protocol in brain MRI. However, the estimators for the statistical properties of individual integral curves and their bundles based on the new data are not well understood. In this project, the PIs aim to provide a solid theoretical foundation for linking integral curve estimation in 3D-4D-6D fields of complex directional data with underlying graphical noise structures and to apply the new methodology to address several practical problems in diffusion tensor imaging (DTI) and high angular resolution diffusion imaging (HARDI), technologies commonly used in brain MRI. Specifically, the goals are (1) longitudinal modeling of the structural integrity of the axonal fibers, (2) modeling and assessing the uncertainty of a bundle of fibers, (3) searching for optimal numbers of diffusion directions and shells within a reasonable scan time, (4) equalizing methods for the fiber tracking characterization, and (5) deriving statistically optimal designs for data acquisition protocols. This project strives to improve DTI/HARDI not only as a reliable neuroscience research tool but also as a reliable and practical imaging technique for routine clinical applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113500","Collaborative Research: Development of Classification Theory and Methods for Objective Asymmetry, Sample Size Limitation, Labeling Ambiguity, and Feature Importance","DMS","STATISTICS","07/01/2021","06/11/2021","Xin Tong","CA","University of Southern California","Standard Grant","Yulia Gel","06/30/2024","$120,000.00","","xint@marshall.usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","MPS","1269","075Z, 079Z","$0.00","Classification is a popular data analytical technique in disciplines ranging from biomedical sciences to information technologies. This project will develop theory-backed statistical methods and algorithms to address pressing challenges in the application of classification. These challenges are related to imperfect aspects of training data, which are widespread in high-stake applications such as disease diagnosis and cybersecurity. In particular, this project will focus on the so-called asymmetric classification problems where a particular class is of greater importance than other classes, and the methods and algorithms will aim to control the classification error of missing the most important class in the population, not just in a particular dataset. This property will make the methods and algorithms powerful for medical diagnosis, for which the primary goal is diagnosis accuracy in the population. Moreover, this project will provide a suite of projects, ranging from theory to applications, that are suitable for training graduate and undergraduate students. The interdisciplinary nature of this project is expected to attract students from diverse background to join the PIs? efforts.<br/><br/>The PIs will develop a suite of application-driven, theory-backed methods and algorithms to address pressing data challenges including sample size limitations, sampling biases, and ambiguous class labels. The development will be primarily under the Neyman-Pearson (NP) classification paradigm, which was designed to control the population-level false-negative rate (p-FNR) under a desired level while minimizing the population-level false-positive rate (p-FPR). This project will integrate the NP classification into cutting-edge statistical learning tasks and enable it to address the aforementioned real-world data challenges. Specifically, this project will include the following four overarching goals. First, the PIs will use random matrix theory to address a long-standing problem in the NP classification methodology: whether NP classifiers can be constructed without a sample-splitting step to improve data efficiency. Second, because the NP paradigm has an invariance property to sampling bias, the PIs will develop NP classifiers to address the sampling bias issue in biomedical applications. These classifiers can be trained on biased samples but still achieve the p-FNR control. Third, the PIs will develop a model-free feature ranking framework to incorporate multiple classification paradigms including the NP paradigm and to reflect prediction objectives. Fourth, the PIs will develop the first NP umbrella algorithm under the label noise setting and the first information-theoretic criteria that combine ambiguous classes in multi-class classification. To disseminate the project outcomes, the PIs will give research talks, organize conference sessions, share open-source software packages with tutorials, and reach out to practitioners of classification methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113407","Collaborative Research: Efficient Bayesian Global Optimization with Applications to Deep Learning and Computer Experiments","DMS","STATISTICS","07/01/2021","07/16/2023","Chih-Li Sung","MI","Michigan State University","Continuing Grant","Yong Zeng","06/30/2024","$142,009.00","","sungchih@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","075Z, 079Z","$0.00","The primary objective of this research is to develop global optimization methods which will dramatically enhance the optimization efficiency in the studies of complex scientific problems. The research findings will significantly accelerate discoveries in numerous scientific disciplines involving artificial intelligence and numerical simulations such as mechanical engineering, energy, automated transportation, aerospace engineering, environmental science, and materials science. Integrated into the research is an education plan that emphasizes interdisciplinary training for a broad body of students and increasing participation from underrepresented groups. The PIs will recruit female students and undergraduate students from underrepresented groups and actively involve them in this research. Research findings will be disseminated at conferences. Furthermore, research findings will also be integrated into PIs? courses to have optimization and data analysis training for graduate and undergraduate students.  <br/><br/>This research focuses on Bayesian global optimization which refers to active learning strategies developed by stochastic process priors for the optimization of expensive ""black box"" functions. Motivated by the challenges emerged from global optimization in deep learning and computer experiments, two innovative Bayesian active learning methods will be developed which are applicable to problems with conditionally dependent inputs and non-Gaussian stochastic outputs. The first method will address an important but unresolved issue arising from the optimization of stochastic outputs in classification problems. The novelty lies in an expected improvement criterion developed based on a generalized Gaussian process which leads to a tractable objective function with an intuitive interpretation. The asymptotic convergence properties will be developed rigorously under the continuum-armed-bandit settings. The second method is based on a new correlation function for a branching and nested structure, which occurs commonly in practice. Sufficient conditions on the validity of the new correlation functions is expected to be derived and a new class of optimal initial designs will be systematically constructed. The innovative idea of automatic-tuning in deep learning by a rigorous Bayesian global optimization will shed light on new methodologies for the optimization of ""black box"" functions and inspire new research ideas in machine learning, optimization, and spatial statistics. Beyond the applications to computer vision and optimal controls in robotics, the design, modeling, and optimization strategies can open new avenues for studying complex optimization problems with expensive unknown functions and energize both theoretical and applied research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113768","Robust and Efficient Statistical Inference in Large Scale Semi-Supervised Settings","DMS","STATISTICS","08/01/2021","08/01/2022","Abhishek Chakrabortty","TX","Texas A&M University","Continuing Grant","Yulia Gel","07/31/2024","$169,977.00","","achakrabortty@tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","079Z","$0.00","This project will develop methods for robust statistical inference in semi-supervised settings. Unlike more traditional data settings, semi-supervised settings are characterized by two types of available data: 1) a typical small or moderate sized labeled (or supervised) data containing observations for a response (or outcome) and a set of covariates (or predictors), and 2i) a much larger sized unlabeled (or unsupervised) data having observations only for the covariates. Such settings arise naturally whenever the covariates are easily available for a large cohort, while the response may be difficult and/or expensive to obtain due to practical constraints. These are increasingly relevant in modern studies in the big data era with large unlabeled databases (often electronically recorded) becoming easily available (and tractable) on top of a labeled data. Examples are ubiquitous across many disciplines, including computer science, machine learning, econometrics, and biomedical applications like electronic health records and integrative genomics. Statistical inference in semi-supervised settings is therefore of substantial interest. The ultimate question here is to investigate when and how one can use the extra information available from the large unlabeled data to ?improve? upon a corresponding supervised approach, where improvement could be in terms of efficiency or robustness or both. This project aims to provide answers to such questions by developing a class of novel, provable and scalable semi-supervised inference methods for a range of fundamental problems in two fairly distinct and active research areas: 1) causal inference in semi-supervised settings, and 2) semi-supervised inference in the presence of selection bias in labeling. The research outlined in the project will lead to advances in bridging some major gaps in the existing literature and providing a much-needed unified understanding of semi-supervised inference and its subtleties. The methods will also have wide applicability to various domain areas, e.g. biomedical studies for precision medicine and causal inference. The project also has a significant education component, including mentoring of graduate students and curriculum development via short courses to raise awareness about these exciting new areas in modern statistics.<br/><br/><br/>In the first part of the project, the PI will consider causal inference in semi-supervised settings under the potential outcome framework, and explore semi-supervised inference for popular causal parameters, e.g. the average treatment effect and the quantile treatment effect, both of which have been widely studied in supervised settings but rarely so under semi-supervised settings. The PI will aim to develop semi-supervised methods for so-called doubly robust estimation of such parameters that can lead to improved (if not optimal) efficiency, as well as much stronger robustness properties than their best achievable supervised counterparts. The second part of the project will consider semi-supervised inference where the labeling mechanism has inherent selection bias, thus making the labeled and unlabeled data unequally distributed. Such settings, while of great practical relevance, have rarely been addressed so far, partly because their analysis is quite challenging since the labeling fraction decays to zero leading to a natural violation of the so-called positivity/overlap assumption. Under this setting, the PI will explore efficient and rate-optimal semi-supervised inference for various parameters, e.g. the mean response and the average treatment effect (under a causal framework), via doubly robust estimation methods, as well as modeling strategies for estimating the decaying propensity score which arises as an inevitable challenge and is of independent interest. Throughout, the PI's emphasis will be on developing methods with rigorous theoretical guarantees as well as efficient implementation that meets the scalability demanded by the intended applications on large modern datasets. The proposed methods will also bring together a synergy of tools and ideas from classical semi-parametric inference and modern high dimensional statistics theory.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113099","Neural Net Learning for Graph Data","DMS","STATISTICS","09/01/2021","06/23/2021","Cencheng Shen","DE","University of Delaware","Standard Grant","Yulia Gel","08/31/2024","$240,002.00","Joshua Vogelstein, Carey Priebe","shenc@udel.edu","220 HULLIHEN HALL","NEWARK","DE","197160099","3028312136","MPS","1269","079Z, 9150","$0.00","Graph structures are special data types that arise naturally in sociology, economics, public health, computer science, neuroscience, among other areas. In this project, we develop an innovative graph neural network architecture that is theoretically sound, computationally efficient, numerically superior, and versatile for a variety of graph structures. The development will incorporate many recent advances in graph embedding, dependence testing, and convolutional neural network. This project will significantly advance the theoretical foundation of graph neural networks, enable scalable and better graph learning for data scientists, and is uniquely poised to accelerate discoveries in a many graph-based applications. The project also provides research training opportunities for graduate students. <br/><br/>In the project, the PIs will start with graph adjacency, and investigate the difference among spectral embedding, standard neural network, and a novel graph convolutional neural network. Then the PIs plan to prove that under certain graph models, the graph convolutional layer can be asymptotically Bayes optimal in supervised learning. When the graph data is further coupled with node attributes, the PIs develop an attributed neural network architecture via a distance correlation screening layer. The project aims to prove its asymptotic optimality in the presence of node attributes, investigate the relationship between graph adjacency and node attributes to enable better machine learning, and demonstrate its superior performance against existing state-of-the-art methods in simulations and real data. Moreover, the project designs the algorithm in linear-time computation complexity, making it efficient and scalable to big data and sparse graphs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2112943","Automated Causal Discovery with Observational Data via Directed Graphical Models - New Theory and Methods","DMS","STATISTICS","07/01/2021","06/27/2022","Yang Ni","TX","Texas A&M University","Continuing Grant","Yulia Gel","06/30/2024","$179,960.00","","yni@stat.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","079Z","$0.00","Establishing causality is crucial in many fields of science including biology, psychology, neuroscience, climate science, robotics, and quantum mechanics. While the gold standard for establishing causality remains controlled experimentation, it can be expensive, unethical, and even impossible in many cases. Therefore, establishing causality from passively observed data (as opposed to experimental data) is often desirable and, sometimes, the only option. In this project, the PI will develop a series of causal discovery methods that are theoretically sound and practically useful for identifying causality with observational data. Efficient open-source software accompanying the proposed methods will be developed and the project also provides research training opportunities for graduate students. <br/><br/>The proposed methods will be based on directed graphical models (DGMs). Despite the popularity of DGMs across disciplines, using DGMs to establish causality from observational data remains difficult, both theoretically and methodologically, due to several prominent challenges. First, DGMs are generally non-identifiable due to Markov equivalence class in which all DGMs encode the same set of conditional independencies and hence are not distinguishable from each other without further assumptions. Second, the class of DGMs is not closed under marginalization and therefore the structure learning can be misled by unmeasured confounders. Third, the vast majority of existing methods rely on relatively strong distributional assumptions on the data generating mechanism, which can cause significant estimation biases when the assumptions are seriously violated. This project aims to address these three challenges by developing new DGMs for non-iid data and establishing their causal identifiability theories in the presence of confounders and model misspecification. As validation, the proposed methods will be used to reverse engineer gene regulatory networks from genomic datasets. Results will be disseminated through workshops, publications, and new graduate courses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2047444","CAREER: Advances in Modern Causal Inference: High Dimensions, Heterogeneity, and Beyond","DMS","STATISTICS","07/01/2021","07/28/2023","Edward Kennedy","PA","Carnegie-Mellon University","Continuing Grant","Yulia Gel","06/30/2026","$235,101.00","","Edward@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","1045","$0.00","Causality is at the heart of many of the most important questions in science and policy. Which cancer treatments are best for which patients?  Does incarceration impede or encourage recidivism? Causal inference is concerned with formulating such questions mathematically, exploring whether answers can be obtained from data, and if so, determining how well and with what methods. The classical setup in causal inference explores effects of simple interventions, and presumes confounding relationships are straightforward enough to estimate with relatively low error. However, simple ""all-or-nothing"" effects may mask underlying heterogeneity, and can be practically unrealistic or nearly impossible to estimate, for example if some subjects have no chance of receiving treatment. Further, in modern contexts, confounders are often high-dimensional and relate to exposures and outcomes in unknown and possibly very complex ways. Accommodating realistic confounding and heterogeneity are two of the most central challenges in modern causal inference. These pursuits yield myriad open questions, from understanding fundamental limits of causal inference in high dimensions to exploring entirely new effects altogether. This project aims to help address these questions by developing novel theory and methods and advancing the application of causal inference in fields such as public policy and medicine. Outreach is also a major component. New software will be made freely available in R. The PI will design an undergraduate course on quantitative causal reasoning, to help push data literacy forward from association to causation. A textbook will be written, and there will be numerous opportunities for broad participation, including summer programs, workshops, and short courses.<br/><br/>This project aims to develop new theory and methods for the study of more nuanced - yet practical - effect measures, accommodating the complex data structures often found in practice. The research will focus on (1) adjustment for high-dimensional confounding and (2) flexible estimation of heterogeneous treatment effects and optimal treatment regimes. Extensions will also be pursued for multivalued time-varying exposures subject to unmeasured confounding. For (1), the PI aims to develop novel non-asymptotic risk bounds for both classical and new propensity-based effects, as well as minimax lower bounds. This is accomplished in a high-dimensional discrete model (new to causal inference) as well as with continuous data. For (2), the PI plans to determine the fundamental limits of heterogeneous effect estimation in flexible nonparametric models, develop and analyze novel heterogeneous effect estimators, and study optimal treatment regimes under novel ""contact constraints."" This work has the potential to help transform our understanding of causal inference in the modern big data era. The projects will also directly contribute to research on specific applications in sociology, criminology, and medicine.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113570","New Techniques to Combine Measures of Statistical Significance from Heterogeneous Data Sources with Application to Analysis of Genomic Data","DMS","STATISTICS","08/01/2021","07/19/2021","Zheyang Wu","MA","Worcester Polytechnic Institute","Standard Grant","Yong Zeng","07/31/2024","$200,000.00","","zheyangwu@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","MPS","1269","068Z","$0.00","This project is motivated by integrative analysis of large-scale genomic data, where an important question is how to effectively combine statistical significances, or p-values, from heterogeneous data sources. Despite recent advances in theoretical and applied studies, statistical and computational challenges remain in addressing critical data features, such as complex correlations, discreteness of data, and availability of prior knowledge that could have been utilized to boost signal detection. This project will develop novel statistical methods to address the challenges and increase the statistical power for detecting valid signals. The research will facilitate innovations in statistical theory and methodology as well as in broad applications. The research activities will leverage project-oriented education, promote multi-disciplinary interactions, and benefit STEM education for the next generation of engineers and scientists, especially members of minorities underrepresented in the statistics field. <br/><br/>Specifically, the project will develop efficient and powerful p-value combination tests by following a new strategy different from common literature. Instead of designing and studying tests individually, the project will strategically resolve problems based on general families of tests. The project has three specific research aims. The first aim is to tackle the computational challenges in applying the p-value combination approach into analyzing complex heterogeneous data. The PI will develop fast and accurate algorithms to control the error rates of general families of tests under general correlations and the discreteness of the p-values. The second aim is to increase the power of p-value combination for integrative analysis of complex data through utilizing the correlation information, incorporating prior knowledge, automatically adapting to complementary procedures, and revealing asymptotic optimality properties. Finally, the PI will apply the developed methods into the integrative analysis of large-scale genomic data of neurodegenerative diseases. Results of the project are expected to advance global hypothesis testing methods for high-dimensional complex data analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113468","Causal Inference for Extremes via Tropical Geometry","DMS","STATISTICS","07/01/2021","08/19/2022","Ngoc Tran","TX","University of Texas at Austin","Continuing Grant","Yulia Gel","06/30/2024","$144,091.00","","tran.mai.ngoc@gmail.com","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1269","","$0.00","Monitoring and predicting extreme events such as flooding, financial collapses or engineering risks  are of huge importance to societies. However, extreme events by definition are rare and concern large, unlikely values, while traditional statistical techniques are based on averages and large numbers of observations. This project will create new, fast methodologies to uncover the causes and potential cascading failures when an extreme event hits a system, such as a river, computer network, or financial network. Concrete applications include flood risks predictions, tracing the source of contaminants in underground waterways, and modeling risks of airplanes runway overrun. This research will advance society?s ability to monitor, predict and prevent such adverse events. <br/><br/><br/>Extreme value statistics concerns the maxima of random variables and relations between the tails of distributions rather than averages and correlations. Unique challenges to this field are lack of data and lack of smoothness in the likelihood, which severely limits statistical learning and inference. Goal A of this research aims to advance causal inference for extreme value statistics with provably accurate algorithms that can handle datasets with thousands of variables and missing data.Goal B of this research aims to solve the Identification Challenge for deep neural networks with rectified linear (ReLU) activation, a difficult variant of the reverse-engineering problem. These problems are intimately connected and both will be tackled in this proposal via tropical algebraic and convex geometry. Preliminary work by the PI and co-authors on hydrology data have shown that the proposed methods achieve the state-of-the-art in the Hidden River Network, the benchmark problem in causal inference for extremes. The proposed research will advance society?s ability to monitor and predict extreme events in finance, engineering, and natural disasters. It will simultaneously advance both extreme value statistics and tropical geometry, widening their applications and create new interdisciplinary, data-driven research at their intersections.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113360","Collaborative Research: New Statistical Methods for Microbiome Data Analysis","DMS","STATISTICS","09/01/2021","07/26/2021","Jun Chen","MN","Mayo Clinic Rochester","Standard Grant","Yong Zeng","08/31/2024","$91,959.00","","chen.jun2@mayo.edu","200 1ST ST SW","ROCHESTER","MN","559050001","5072844715","MPS","1269","068Z","$0.00","The human microbiome, the collection of micro-organisms associated with the human body, has been increasingly recognized as an important player in human health and disease.  Human microbiome research focuses on deciphering the intricate relationship between the microbiome and the host and identifying microbial biomarkers for disease prevention, diagnosis, and treatment.  Current technologies to study the human microbiome involve sequencing the microbial DNA in the sample, upon which the identity and the abundance of the micro-organisms can be determined.  Analysis of such microbiome sequencing data raises many statistical challenges. First, the data are zero-inflated. A typical microbiome dataset contains more than 75% zeros. Second, the data are compositional. The abundance change in one microbe will automatically lead to changes in the relative abundance of others, making identification of the ""driver"" microbe difficult. Third, the microbes are phylogenetically related. Closely related microbes usually share similar biological traits. Finally, the human microbiome is subject to many environmental confounders. Controlling these confounders is essential to make valid statistical inferences.  The project will develop novel statistical methods for analyzing microbiome data addressing these challenges. The research results will be disseminated through scientific publications as well as seminar and conference presentations. The PIs will develop, distribute, document, and maintain R software packages via GitHub and CRAN for developed methods, and provide tutorials with example datasets. The PIs will test the software in real-world settings thoroughly. Given the popularity of the multi-omics approach to study the human microbiome, the delivered software packages will be of particular interest to microbiome investigators. The PIs will train students at the intersection of high-dimensional statistics, optimization, and genomics.<br/><br/>The project has two research thrusts. In the first thrust, the PIs will develop a new statistical learning framework for microbiome data to simultaneously tackle the high-dimensionality, compositional effect, zero-inflation, and phylogenetic information. In particular, the new framework includes a novel zero imputation method based on a new Dirichlet mixture model, a general approach for handling compositional effect in supervised/unsupervised statistical learning, and a robust structure adaptive method to incorporate external information encoded in the phylogenetic tree. In the second thrust, the PIs will develop a two-dimensional false discovery rate (FDR) control procedure for powerful confounder adjustment in microbiome association analysis. The procedure uses the test statistics from the unadjusted analysis as auxiliary statistics to filter out a large number of irrelevant features, and false discovery rate control is then performed based on the test statistics from the adjusted analysis on the reduced set. The PIs will investigate both model-based and model-free approaches, and prove the asymptotic FDR control.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113590","Collaborative Research: New Regression Models and Methods for Studying Multiple Categorical  Responses","DMS","STATISTICS","09/01/2021","07/12/2021","Xin Zhang","FL","Florida State University","Standard Grant","Yong Zeng","08/31/2024","$100,000.00","","henry@stat.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","079Z","$0.00","In many areas of scientific study including bioengineering, epidemiology, genomics, and neuroscience, an important task is to model the relationship between multiple categorical outcomes and a large number of predictors. In cancer research, for example, it is crucial to model whether a patient has cancer of subtype A, B, or C and high or low mortality risk given the expression of thousands of genes. However, existing statistical methods either cannot be applied, fail to capture the complex relationships between the response variables, or lead to models that are difficult to interpret and thus, yield little scientific insight. The PIs address this deficiency by developing multiple new statistical methods. For each new method, the PIs will provide theoretical justifications and fast computational algorithms. Along with graduate and undergraduate students, the PIs will also create publicly available software that will enable applications across both academia and industry.<br/><br/>This project aims to address a fundamental problem in multivariate categorical data analysis: how to parsimoniously model the joint probability mass function of many categorical random variables given a common set of high-dimensional predictors. The PIs will tackle this problem by using emerging technologies on tensor decompositions, dimension reduction, and both convex and non-convex optimization. The project focuses on three research directions: (1) a latent variable approach for the low-rank decomposition of a conditional probability tensor; (2) a new overlapping convex penalty for intrinsic dimension reduction in a multivariate generalized linear regression framework; and (3) a direct non-convex optimization-based approach for low-rank tensor regression utilizing explicit rank constraints on the Tucker tensor decomposition. Unlike the approach of regressing each (univariate) categorical response on the predictors separately, the new models and methods will allow practitioners to characterize the complex and often interesting dependencies between the responses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113373","A Transfer Learning Approach to Algorithmic Fairness","DMS","STATISTICS","08/15/2021","08/02/2021","Yuekai Sun","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yulia Gel","07/31/2024","$150,000.00","Moulinath Banerjee","yuekai@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","079Z","$0.00","In today's data-driven world, machine learning models are routinely used to make high-stakes decisions in criminal justice, education, lending, medicine, and many other areas. Although replacing humans with machine learning models appears to eliminate human biases in decision-making processes, they may perpetuate or even exacerbate biases in the training data. Such algorithmic biases are especially objectionable when they adversely affect underprivileged groups. In this project, we focus on detecting and mitigating algorithmic biases that are caused by sampling biases in the training data. The project also provides research training opportunities for graduate students. <br/><br/>There are three aims. First, the PIs identify gaps in the capabilities of existing algorithmic fairness practices for overcoming sampling biases in the training data. The PIs also study how current trends in the development of machine learning (ML) models (for example, data augmentation and overparameterization) can perpetuate and exacerbate algorithmic biases. Second, the PIs cast the fair machine learning problem as a transfer learning problem and leverage recent developments in transfer learning to detect and mitigate algorithmic biases caused by sampling bias. Third, the PIs consider how to collect training datasets that are more representative of the general population and beget ML models that are free from algorithmic biases. The ultimate goal is to broaden the appeal and adoption of algorithmic fairness practices among ML practitioners. The PIs plan to demonstrate that the transfer learning approach to algorithmic fairness avoids two barriers in the way of this ultimate goal: (i) it aligns the goal of algorithmic fairness with the goals of (possibly non-altruistic) ML practitioners by avoiding the apparent trade-off between accuracy and fairness, and (ii) it addresses the lack of consensus on the choice of algorithmic fairness practice in many ML tasks by providing an objective measure of the efficacy of such practices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113778","Collaborative Research: Statistical Inference for High-dimensional Spatial-Temporal Process Models","DMS","STATISTICS","07/01/2021","06/10/2021","Sudipto Banerjee","CA","University of California-Los Angeles","Standard Grant","Yong Zeng","06/30/2024","$260,000.00","","sudipto@ucla.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244201","3107940102","MPS","1269","079Z","$0.00","Spatial data science and other emerging technologies related to Geographic Information Systems are increasingly conspicuous in scientific discoveries. Scientists in a variety of disciplines today have unprecedented access to massive spatial and temporal databases comprising high resolution remote sensed measurements. Statistical modeling and analysis for such data need to account for spatial associations and variations at multiple levels while attempting to recognize underlying patterns and potentially complex relationships. Traditional statistical hypothesis testing is no longer adequate for these scientific problems and statisticians are increasingly turning to specialized methods for analyzing complex spatial-temporal data. However, there continue to remain substantial theoretical and methodological bottlenecks with regard to the interpretation of statistical models. This project will address these problems by developing probabilistic machine learning tools for spatial-temporal Big Data that can have far-reaching public health, economic, environmental, and scientific implications. Several innovations in statistical theory, methodologies and computational algorithms are envisioned that will inform basic science and policy questions arising in diverse disciplines using geographic information sciences. Key educational components include dissemination of technologies across the scientific communities including data scientists, engineers, foresters, ecologists, and climate scientists. The Principal Investigators will train the next generation of data scientists through dissemination efforts for graduate students in STEM fields.        <br/><br/>The PIs aim to blend innovative theory, methods and applications to advance knowledge of spatial-temporal stochastic processes with an emphasis on their properties for high-dimensional inference. This domain of spatial statistics has witnessed a burgeoning of models and methods for Big Data analysis. New classes of models have emerged from the judicious use of directed acyclic graphs (DAGs) that are being applied to massive datasets comprising several millions of spatiotemporal coordinates. Theoretical explorations envisioned in this project will focus upon statistical inference on the process parameters and the underlying spatial process. The PIs intend to perform rigorous investigations into statistical inference for high-dimensional spatio-temporal processes to derive micro-ergodic parameters for such models that will be consistently estimable and, at the same time, yield consistent predictive inference. The PIs will develop new methodologies that cast high-dimensional stochastic processes into computationally practicable frameworks by embedding graphical Gaussian processes within hierarchical frameworks for jointly modeling highly multivariate spatial data. Innovative statistical theory and methods will be developed and used to construct sparsity-inducing graphical spatio-temporal models to accommodate massive numbers of outcomes and capture complex dependencies among variables across massive numbers of locations. The planned theoretical explorations into the inferential properties of newly emerging scalable spatio-temporal processes will produce novel statistical contributions. The PIs will provide probability-based uncertainty quantification and will substantially enhance the understanding of physical and natural processes underlying various problems in the physical, environmental and biomedical sciences and in public health.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113409","Collaborative Research: Inference and Decentralized Computing for Quantile Regression and Other Non-Smooth Methods","DMS","STATISTICS","07/01/2021","06/14/2021","Wenxin Zhou","CA","University of California-San Diego","Standard Grant","Yong Zeng","06/30/2024","$173,126.00","","wenxinz@uic.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","075Z, 079Z","$0.00","Recent years have witnessed the transition of statistical analysis from a small- or moderate-scale data environment to a world involving massive data on parallel and distributed computing platforms. However, such a transition poses significant statistical and computational challenges for many important methods with non-smooth loss functions. As a representative example, quantile regression methods are building blocks for many advanced methods in statistics and econometrics and are frequently used to model financial data and medical data. The computational inflexibility makes quantile regression less favorable among various branches of the statistical learning tool kit. The project aims to develop a unified framework for large-scale learning with non-smooth loss functions to address the aforementioned problems. The developed methods will be applied to analyze complex biomedical data subject to censoring or privacy protocol and large-scale public health data. Both graduate and undergraduate students will receive training through research involvement in the project, ranging from developing new methods and theory to open-source software under different platforms. <br/> <br/>The principal investigators will use a combination of tools from statistics, optimization, and probability to develop a unified convolution smoothing framework and establish rigorous theoretical and algorithmic foundations for a class of statistical methods with non-differentiable loss, typified by quantile regression and support vector machine. The former is indispensable for understanding pathways of dependence and heterogeneous effects irretrievable through standard conditional mean regression analysis. However, most existing computational methods for quantile regression are based on generic algorithms, which are not scalable in large-scale machine learning applications when the number of variables is large. Convolution smoothing admits fast calibrated gradient-based algorithms without compromising the estimates' quality, therefore offering a balanced trade-off between statistical accuracy and computational precision. It also extends the applicability of quantile regression, from low to high dimensions, fully to partially observed samples, and linear to nonlinear structures, in modern big data analytics. The first part of the project will focus on three statistical problems: (a) high-dimensional sparse quantile regression, (b) large-scale censored quantile regression, and (c) robust regression with redescending M-estimation. The second part of the research focuses on developing efficient decentralized algorithms for methods with non-smooth loss functions under two modern data types: (i) parallel and distributed data, and (ii) online streaming data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113642","Deep Learning on Manifolds: New Architectures and Theoretical Foundations","DMS","STATISTICS","07/01/2021","07/25/2023","Lizhen Lin","IN","University of Notre Dame","Standard Grant","Yong Zeng","06/30/2024","$179,998.00","Dong Quan Nguyen","lizhen.lin@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","1269","075Z, 079Z, 1269","$0.00","Over the last couple of decades, deep learning approaches have achieved breakthrough performance in a broad range of learning problems from a variety of applications fields such as image recognition, speech recognition, natural language processing and others. Deep learning has also served as the main impetus for the advancement of recent artificial intelligence (AI) technologies. The main goal of this project is to develop new deep neural network (DNN) architectures, computational algorithms and foundational theory for deep learning in complex domains where the data are complex geometric objects.  The underlying geometry of the space will be utilized and exploited for developing the DNNs.  Such development will enable the application of deep learning models in medical imaging, computer vision, computer graphics, recommender systems and neuroscience to learning problems where the inputs are complex images, diffusion matrices, shapes or other geometric objects. In addition, results of the project will be integrated in a course on Geometry and Statistics for undergraduate and graduate students. The principal investigators will also be involved in the research training of students, possibly students from under-represented groups.<br/> <br/>The main goals of the research program are to develop general deep neural network architectures on manifolds and take some major steps toward understanding their theoretical foundations. Specifically, this project will (1) develop extrinsic deep neural networks (eDNNs) on manifolds by generalizing the popular feedforward neural networks in the Euclidean space to manifolds utilizing equivariant embeddings; (2) develop intrinsic deep neural networks (iDNNs) on manifolds employing a Riemannian structure of the manifold; (3) develop general retraction-based convolutional neural networks (RCNNs) on manifolds; (4) characterize theoretical properties of eDNNs, iDNNs and RCNNs on manifolds by studying their approximation properties as well as the estimation error for a class of empirical risk minimizers over the DNN class on manifolds; and (5) develop user-friendly software packages that can be readily used by the practitioners. The research program explores the interface among geometry, statistics and machine learning, and aims to broaden the paradigm of deep learning by extending geometric deep learning to manifolds.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2125524","The SRCOS Summer Research Conference in Statistics and Biostatistics","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","07/01/2021","05/21/2021","Katherine Thompson","KY","University of Kentucky Research Foundation","Standard Grant","Pena Edsel","06/30/2022","$49,966.00","Arnold Stromberg","katherine.thompson@uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","1260, 1269","7556, 9150","$0.00","The Southern Regional Councilon Statistics is a consortium of statistics and biostatistics programs in the South. It currently has member programs at 45 universities in 16 states in the region. This grant will provide support for graduate and undergraduate students to travel and participate in the 56th Summer Research Conference (SRC) in statistics and biostatistics at Jekyll Island Villas by the Sea in Jekyll Island, Georgia on October 3-6, 2021. The SRC is an annual conference sponsored by the SRCOS. The purpose of the SRC is to encourage the interchange and mutual understanding of current research ideas in statistics and biostatistics, and to provide motivation and direction to further research progress.  This project will focus on young researchers, giving them an opportunity to participate in the meeting and to interact closely with leaders in the field in a manner not possible at larger meetings. The SRC is particularly valuable for graduate students, undergraduate students, isolated statisticians, and faculty from smaller regional schools in the southern region at drivable distances without the cost of travel to distant venues. Speakers will present formal research talks with adequate time allowed for clarification, amplification, and further informal discussions in small groups. Under the travel support provided by this award, graduate students will attend and present their research in posters to be reviewed by more experienced researchers. <br/><br/>In addition to the graduate student participation, the 56th SRC will also include the second annual Statistical Undergraduate Research Experience (SURE), a conference within a conference aimed to encourage the participation of undergraduate students from under-represented groups to pursue graduate education and career opportunities in STEM fields.  SURE will include events specifically for undergraduate students and undergraduate mentors, such as a panel about career opportunities in statistics, a ?Real Data Analytics Workshop,? and a speed-mentoring session with current statistics and biostatistics graduate students. The support provided by this award will support SURE and encourage under-represented students to enter STEM fields, including statistics or biostatistics, and provide training to support this endeavor. The 56th SRC will strengthen the research of the statistics and biostatistics community as a whole and help bridge the gap for under-represented groups to pursue statistics or biostatistics, particularly in the sixteen states of the Southern Region. More information about the conference can be found at: https://www.srcos.org/conference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113242","Matrix Completion with Non-uniform Missing Patterns, a New Measure of Conditional Dependence, and Applications to Feature Selection","DMS","STATISTICS","07/15/2021","07/13/2021","Sourav Chatterjee","CA","Stanford University","Standard Grant","Yulia Gel","06/30/2024","$300,000.00","","souravc@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","This project aims to study three classes of problems in mathematical statistics. The first class of problems is about matrix completion. Suppose that we have an array of numbers with missing entries, such as a database of ratings from users of a product. Matrix completion is the problem of predicting the missing values. Much work has been done on this problem in the last ten years, but the vast majority of it is under the assumption that the matrix entries are missing uniformly at random. In practice, however, that is not usually the case. This project will implement a method where the matrix completion problem can be solved under more realistic assumptions. This will impact all areas of science and technology where matrix completion algorithms have applications, such as recommender systems, collaborative filtering, computer vision, and genetics, to name a few. The second class of problems concerns the development of an approach for measuring conditional dependence. Measuring conditional dependence is important in many applications of statistics, such as in the analysis of graphical and causal models, which are widely used in the social sciences. The third class of problems is about developing a new approach for selecting the right variables for performing regression analysis when presented with a large number of variables. This part of the project will impact all areas of science and technology where regression problems with many predictors are commonplace, such as biology, medicine, and genomics.<br/><br/>The project on matrix completion aims to solve the low rank matrix completion problem when the pattern of missing entries is deterministic. The PI has recently published an asymptotic solution of the problem. The project will yield a non-asymptotic version of the theory, and an algorithm for matrix completion when the probability of an entry to be missing is a function of the entry itself. The project on a new measure of conditional dependence will analyze the asymptotic properties of a coefficient proposed recently by the PI and one of his students. The results of the analysis may help in devising new tests for conditional independence. The project on feature selection will analyze the properties of a non-parametric feature selection algorithm proposed recently by the PI and one of his students. The results of the analysis may guide better implementation of the algorithm, as well as yield new and better selection algorithms.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113605","DMS-EPSRC Collaborative Research: Advancing Statistical Foundations and Frontiers for and from Emerging Astronomical Data Challenges","DMS","STATISTICS, ","07/01/2021","06/30/2021","Thomas Chun Man Lee","CA","University of California-Davis","Standard Grant","Yulia Gel","06/30/2024","$200,000.00","","tcmlee@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269, 1798","1206","$0.00","Statistical theory and methods play a fundamental role in scientific discovery and advancement, including in modern astronomy, where data are collected on increasingly massive scales and with more varieties and complexity. New technology and instrumentation are spawning a diverse array of emerging data types and data analytic challenges, which in turn require and inspire ever more innovative statistical methods and theories. This research is guided by the dual aims of advancing statistical foundations and frontiers, motivated by astronomical problems and providing principled data analytic solutions to challenges in astronomy. The CHASC (California-Harvard Astrostatistics Collaboration) International Center has an extensive track record in accomplishing both tasks. This research leverages CHASC?s track record to make progress in several new projects. Fitting sophisticated astrophysical models to complex data that were collected with high-tech instruments, for example, often involves a sequence of statistical analyses. Several projects center on developing new statistical methods that properly account for errors and carry uncertainty forward within such sequences of analyses. Additional work will focus on developing theoretical properties of novel statistical estimation procedures to address data-analytic challenges associated with solar flares and X-ray observations. Other projects involve fast and automatic detection of astronomical objects such as galaxies from 2D or even 4D data. The PIs will develop statistical theory and methods in the context of these projects, building statistical foundations and pushing the frontiers of statistics forward for broad impact that will extend well beyond astrostatistics. The PIs plan to offer effective methods and algorithms for tackling emerging challenges in astronomy, with the aspiration of promoting such principled data-analytic methods among researchers in astronomy. Its provision of free software via the CHASC GitHub Software Library will enable the distribution and impact of the proposed methods and algorithms. <br/><br/>The projects reflect three broad themes: (1) Exploring fundamental statistical theory with immediate impact in astronomy, including a general approach for obtaining confidence regions by leveraging the pivot-property of maximal product spacing, which is then applied to assess the power law of solar flares, and a statistically principled correction to the use of the popular C-stat in astrophysics; (2) Assessing the misspecification of models and prior distributions in multi-stage statistical analyses, and post processing posterior draws to correct for defects in prior modeling when redoing a Bayesian analysis is impractical; and (3) Identifying breakpoints in complex models, which includes a fast algorithm for identifying astronomical boundaries and identifying breakpoints in joint spatial, spectral, temporal models. Theme 1 is more theory driven, while Themes 2 and 3 are more methods and computation driven. Together they form a rich suite of case studies for developing statistical methods for astronomical problems, ranging from new theoretical foundations to innovative modeling strategies and to efficient computational techniques.  Consequently,  the research will impact both the fields of statistics and astronomy: spurring more interest and new problems for statisticians and resolving long standing problems in astronomy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2052964","FRG: Collaborative Research: Flexible Network Inference","DMS","STATISTICS","07/01/2021","06/30/2021","Yingying Fan","CA","University of Southern California","Standard Grant","Yulia Gel","06/30/2024","$200,000.00","","fanyingy@usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","MPS","1269","1616","$0.00","Scientists have been studying many natural systems by viewing them as networks, a term used to describe collections of entities and their interactions. Networks are ubiquitous in many fields, including epidemiology, economics, sociology, genomics and ecology, to name a few. A number of statistical models have emerged over the last two decades to describe network data. One common type of model is the latent space model, where each entity?s behavior is governed by its position in some unobserved (latent) space, and if we knew these positions, we could fully describe the statistical behavior of the network. While these models have been useful in many problems, real systems tend to be more complicated. The goal of this project is to fill this gap between models and reality by developing network analysis methods that still perform well even when the model does not fully match the data.   <br/><br/>The specific aims of this project are to improve our understanding of existing network analysis methods under model misspecification, heterogeneous noise, and incomplete or missing data, and to develop novel network methods that are robust to these sources of error.  We consider both these problems under one unified framework that represents the adjacency matrix of a network as its expectation plus entry-wise noise, which encompasses most popular network models.  Within this framework, the project will examine the effects of model misspecification on downstream inference, both for global inference tasks (e.g., network-level summary statistics) and local inference (e.g., node-level statistics). One core goal of the project is developing bootstrap and resampling algorithms for networks, two extremely useful tools in classical statistics that do not yet have full network analogues. Another core goal is developing more general notions of community membership and node similarity, allowing the extension of robust algorithms to a broader collection of network models. Finally, the methods developed will be extended to the analysis of multiple networks. Taken together, these tools will substantially expand the toolbox of network techniques, while accounting for the realities of noisy and incomplete network data and imperfect network models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113467","High-Dimensional Point Process Modeling with Applications to Large-scale Neuronal Activity Analysis","DMS","STATISTICS","07/01/2021","08/28/2023","Xiwei Tang","VA","University of Virginia Main Campus","Continuing Grant","Yong Zeng","06/30/2024","$124,930.00","","xt4yj@virginia.edu","1001 EMMET ST N","CHARLOTTESVILLE","VA","229034833","4349244270","MPS","1269","079Z","$0.00","Large-scale point process data is ubiquitous in a wide variety of scientific and business applications, for example in neuroscience.  This project aims to develop a sequence of novel statistical methods and machine learning algorithms for modeling high-dimensional point process data. These methods can be used to analyze the complex ensemble neuronal activity data, which is now of key interest in neuroscience to understand mechanisms of sensory coding, motor output, and cognitive function for groups of neurons. The methods will also be useful to address other important scientific questions, such as predicting the occurrence of earthquakes, capturing the spatial pattern of galaxies, characterizing social networks, and modeling marketing forecasts and supply chains. This project will stimulate interdisciplinary collaboration between data and domain scientists from disparate areas including neuroscience, environmental science, business, epidemiology, astronomy, ecology, and political science. The project will integrate research with the teaching and training of students at various levels. Software packages implemented in programming languages R and Python will be developed and distributed for broad use.<br/><br/>Analyzing multivariate neuronal spike trains data imposes significant challenges and requires new sophisticated tools for modeling high-dimensional point processes. This project aims to develop a general multivariate point process regression model, which allows both high-dimensional point-process-type responses and predictors, through utilizing the advanced tensor decomposition techniques. The method will enable researchers to model a large number of neurons jointly while having the model dimension substantially reduced?this turns the adversity of the high dimensionality of data into benefits. The project will also incorporate additional structures, such as sparsity, clustering, and spatial-correlation, into learning, and model interaction effects through higher-order convolutions. The research can greatly facilitate neuroscience studies regarding coordinating mechanisms among neurons in information transmission and encoding. More importantly, the models to be developed with effective and scalable computing procedures can adapt to both temporal and spatial processes and apply to other types of scientific applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113475","Collaborative Research: Efficient Bayesian Global Optimization with Applications to Deep Learning and Computer Experiments","DMS","STATISTICS","07/01/2021","07/16/2023","Ying Hung","NJ","Rutgers University New Brunswick","Continuing Grant","Yong Zeng","06/30/2024","$199,999.00","","yhung@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","The primary objective of this research is to develop global optimization methods which will dramatically enhance the optimization efficiency in the studies of complex scientific problems. The research findings will significantly accelerate discoveries in numerous scientific disciplines involving artificial intelligence and numerical simulations such as mechanical engineering, energy, automated transportation, aerospace engineering, environmental science, and materials science. Integrated into the research is an education plan that emphasizes interdisciplinary training for a broad body of students and increasing participation from underrepresented groups. The PIs will recruit female students and undergraduate students from underrepresented groups and actively involve them in this research. Research findings will be disseminated at conferences. Furthermore, research findings will also be integrated into PIs? courses to have optimization and data analysis training for graduate and undergraduate students.  <br/><br/>This research focuses on Bayesian global optimization which refers to active learning strategies developed by stochastic process priors for the optimization of expensive ""black box"" functions. Motivated by the challenges emerged from global optimization in deep learning and computer experiments, two innovative Bayesian active learning methods will be developed which are applicable to problems with conditionally dependent inputs and non-Gaussian stochastic outputs. The first method will address an important but unresolved issue arising from the optimization of stochastic outputs in classification problems. The novelty lies in an expected improvement criterion developed based on a generalized Gaussian process which leads to a tractable objective function with an intuitive interpretation. The asymptotic convergence properties will be developed rigorously under the continuum-armed-bandit settings. The second method is based on a new correlation function for a branching and nested structure, which occurs commonly in practice. Sufficient conditions on the validity of the new correlation functions is expected to be derived and a new class of optimal initial designs will be systematically constructed. The innovative idea of automatic-tuning in deep learning by a rigorous Bayesian global optimization will shed light on new methodologies for the optimization of ""black box"" functions and inspire new research ideas in machine learning, optimization, and spatial statistics. Beyond the applications to computer vision and optimal controls in robotics, the design, modeling, and optimization strategies can open new avenues for studying complex optimization problems with expensive unknown functions and energize both theoretical and applied research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2203207","Statistical Modelling and Inference for Next-Generation Functional Data","DMS","STATISTICS","10/01/2021","10/29/2021","Lily Wang","VA","George Mason University","Standard Grant","Yulia Gel","07/31/2023","$107,317.00","","lwang41@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1269","102Z","$0.00","With the rapid growth of modern technology, many large-scale imaging studies have been or are being conducted to collect massive datasets with large volumes of imaging data, thus boosting the investigation of ""next-generation functional data"". These enormous collections of imaging data contain interesting information and valuable knowledge, which has raised the demand for further advancement in functional data analytic approaches. Although functional data analysis (FDA) has gained widespread popularity in recent years, enhancing the capability of next-generation FDA remains a long-standing challenge. This research targets integrating state-of-the-art statistical modeling devices with modern computational and inferential techniques to develop a set of flexible and intelligent statistical tools to enable learning and discovery from next-generation functional data. The efficacy of the tools developed in this research will be tested by neuroimaging studies. The proposed methods and theory are also applicable to a broader range of fields that require modeling and analysis of images and other complex data types collected over space and/or time, such as geography, environmental science and remote sensing studies. The graduate student support will be used for day-to-day research activities, including parts of the theory/methodology developments and data analysis. <br/><br/>This research will enrich the methods for dealing with functional data observed from complex data objects (high-dimensional, correlated images or shapes), which commonly arise in imaging studies, such as, health/medical imaging or remote sensing imaging. The PI aims to address some challenging research problems in analyzing next-generation functional data by: (1) innovating a statistically sound framework to extract useful information from large-scale longitudinal imaging studies; (2) developing flexible and intelligent statistical models to delineate the association between massive imaging data and covariates of interest and to characterize and visualize the spatial variability of the imaging data; and (3) developing efficient, scalable algorithms with high-performance statistical software packages to meet the challenges posed by dynamic imaging studies. In particular, the proposed research involves four projects. Project 1 provides a unifying approach to characterize the varying association between imaging responses with a set of explanatory variables. Project 2 focuses on the interface between high-dimensional and next-generation functional data to address several fundamental bottlenecks in large-scale imaging genetics studies. Projects 3 and 4 deal with longitudinal/dynamic imaging studies, and a comprehensive functional regression framework to analyze repeated functional responses from these studies will be developed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113489","Modelling Covariance Structure Randomly, with Applications in Bootstrapping, Robust Statistics, and Deep Learning","DMS","STATISTICS","07/01/2021","06/23/2023","Xiucai Ding","CA","University of California-Davis","Continuing Grant","Yulia Gel","06/30/2024","$200,000.00","","xcading@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","High dimensional data with random covariance structure arises naturally in many fields, ranging from genetics and epidemiology to atmospheric and environmental sciences, medical sciences, social sciences and artificial intelligence. A thorough understanding of these large data matrices is in urgent demand in the era of Big Data. However, the current literature has mostly focused on the case when the population covariance matrix is deterministic. This project will change this by establishing novel results for data matrices with random population covariance structure. The research findings will demonstrate how the random covariance structure can help to enhance the understanding of complex and massive data sets. Moreover, this project will result in novel and better modeling and tools for analyzing high dimensional noisy data sets, which can provide more meaningful and interpretable information.<br/><br/>This project aims to study a general class of large dimensional matrices with random population covariance structure and address several challenges in high dimensional statistics and deep learning. It is the first time that the intersection between random matrix theory and extreme value theory is studied in full generality.  The goals include: (1).  Establishing novel theory and results for the top eigenvalues and principal components of large dimensional sample or separable covariance matrix with random covariance structure; (2). Answering the question whether bootstrapping is suitable for high dimensional inference, and how we can modify the standard bootstrapping procedure when it fails for massive data; (3). Constructing new statistics for statistical inference problems involving high dimensional elliptically distributed data in full generality, including heavy tailed data sets; (4). Providing novel insights on the phase transitions of fully connected two-layer neural networks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113713","Collaborative Research: Halfspace Depth for Object and Functional Data","DMS","STATISTICS","07/01/2021","07/15/2022","Xiongtao Dai","IA","Iowa State University","Continuing Grant","Yong Zeng","06/30/2023","$111,385.00","","xdai@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","","$0.00","Complex data objects are increasingly being generated across science and engineering. Non-Euclidean data such as wind directions, neural connectivity networks, and phylogenetic trees draw practical interest, but are challenging to analyze due to their intrinsic constraints. Functional data such as trajectories and images also provide examples of another type of data of high complexity, which are observed on a continuous domain in time or space. In general, practitioners are interested in first exploring the data distributions before any modeling analysis. For instance, given a sample of growth trajectories of children, a first step is to identify typical versus extreme growth patterns, where the latter can be non-trivial to uncover. Also, when analyzing brain connectivity matrices, it is important to find unusual brain networks and differences between healthy and diseased populations. Data-driven methods robust to anomalies are essential in these settings since little is known about the data generating process, and outliers can affect the analysis. Due to the lack of a natural ordering in data objects, exploratory tools such as boxplot and quantile are unavailable for these types of data. The project will address the lack of techniques for exploring non-Euclidean and functional data. Principled statistics and visualization methods will be developed based on a novel way of ranking the observations. The project will also provide training for graduate and undergraduate students. <br/><br/>The central research theme is to develop exploratory data analysis tools for non-Euclidean and functional data objects. To overcome the absence of a canonical ordering for object data, the PIs will develop suitable data depth notions to quantify the centrality of data points with respect to the distribution. This will provide a center-outward ranking of the data that will be used as a building block for outlier detection methods, rank tests, and robust classifiers. Analogous to Tukey's halfspace depth for the multivariate Euclidean case, the new depth notions for object data are expected to be intuitive and robust, and have desirable properties well-grounded in theory. Specifically, the research project will investigate a depth notion for non-Euclidean objects; a data visualization and an outlier detection procedure for non-Euclidean data; halfspace depth notions for functional data, one based on theory and another one from an algorithmic perspective; and a depth notion for sparsely observed longitudinal data. Key challenges that will be addressed include a lack of vector space structure when dealing with non-Euclidean objects; the infinite dimensionality and degeneracy when defining depth notions for functional data; detecting outlying trajectories and images in shape and not just at any time point; and the sparsity and irregularity of observations in longitudinal data. Method and theory development will draw from metric geometry, functional data analysis, empirical process, and M-estimation. Software implementing a suite of depth-based methods will be made available to the public as an outcome of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113646","Statistical Learning and Inference for Single-Cell RNA Sequencing","DMS","Genetic Mechanisms, STATISTICS, MSPA-INTERDISCIPLINARY","08/01/2021","07/20/2021","Jingshu Wang","IL","University of Chicago","Standard Grant","Yong Zeng","07/31/2024","$230,000.00","","jingshuw@uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1112, 1269, 7454","068Z, 079Z, 7465","$0.00","Single-cell genomics is an emerging technique that has become an indispensable tool for understanding cellular diversity and cell activities. Among the various single-cell sequencing technologies, single-cell RNA sequencing (scRNA-seq), simultaneously measuring tens of thousands of RNAs inside each individual cell, is the most mature and widely used technology. This project aims to develop new analytical tools for scRNA-seq with explicit and coherent statistical frameworks to provide reliable uncertainty quantification and inference. At the same time, the new tool will retain the scalability and user-friendly features in existing algorithmic-based methods. The PI will focus on building probabilistic models for machine learning frameworks such as deep learning and address new challenges to account for biological randomness and technical noise in scRNA-seq. The PI will develop open-source software for analyzing scRNA-seq data to help scientists understand cell development, the mechanism of gene regulation, and cell-type-specific features of common diseases. Because of the interdisciplinary feature of this project, it will also train both graduate and undergraduate students within and outside statistics to become future scientists in the fast-evolving area of applied statistics and computational biology.<br/><br/>The PI plans to focus on three research problems that are unique to the analysis of single-cell data: trajectory inference, cell type deconvolution, and gene-gene co-expression / co-bursting. For trajectory inference, the PI will incorporate a hierarchical mixture model into a deep neural network to infer trajectories shared by cells from multiple sources. In the cell type deconvolution problem where scRNA-seq data are used as references to estimate cell type proportions in bulk samples, the PI will derive asymptotically valid confidence intervals of the estimated cell type proportions without parametric assumptions and account for three major uncertainty-inflation factors: the technical noise, biological heterogeneity across individuals, and dependence across genes. Finally, in the gene-gene co-expression / co-bursting analysis, the PI will estimate the true gene-gene correlation and co-bursting pattern from noisy observed data and design a scalable multiple testing framework that can efficiently find gene pairs that are co-expressed or co-bursted. The PI also aims to link the co-expression and co-bursting signals with the enhancer-promoter contacts in the three-dimensional genome structure to understand causal mechanisms of transcriptional regulation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113707","Advancing Statistical Methods for Multi-Study Predictions","DMS","STATISTICS","09/15/2021","08/23/2023","Giovanni Parmigiani","MA","Dana-Farber Cancer Institute","Continuing Grant","Yong Zeng","08/31/2024","$350,000.00","Lorenzo Trippa","gp@jimmy.harvard.edu","450 BROOKLINE AVE","BOSTON","MA","022155418","6176323940","MPS","1269","079Z","$0.00","Scientific predictions are more valuable when their accuracy is stable across studies. Steps towards more replicable predictions would increase public confidence in the scientific process, facilitate dissemination of results, and enhance public engagement with science and technology. As many areas of science and technology are becoming data-rich, multiple datasets are more commonly available for training prediction models. This project aims to develop new general strategies for multi-study ensemble learning and prediction that enhance replicability. The long-term goals are to help improve the well-being of individuals in society (for example, via algorithms for individualized disease prevention and medical treatment), improve national security, and benefit industries that use prediction approaches as key elements of their business plans (including for example finance, marketing, and real estate). The investigators plan to build free, open-source software to implement the successful strategies and to provide research training opportunities to graduate students. <br/><br/>Prior work developed a novel strategy for multi-study prediction, which groups together prediction models, each trained on a single study, and weights them to reward those that perform well outside their training study. This technique, called multi-study ensembling, shows promise to substantially improve prediction replicability. In this project, the investigators plan to generalize this approach in two ways. The first goal is to extend the study to include resampling concepts tailored to the multi-study setting. Specifically, they will consider the study strap ensemble, which fits models to bootstrapped datasets, or ""pseudo-studies."" These are generated by resampling from multiple studies with a hierarchical resampling scheme that generalizes the randomized cluster bootstrap. The study strap is controlled by a tuning parameter that determines the proportion of observations to draw from each study. When the parameter is set to its lowest value, each pseudo-study is resampled from only a single study. When it is high, the study strap ignores the multi-study structure and generates pseudo-studies by merging the datasets and drawing observations like a standard bootstrap. The second goal is to extend the concept of weight by building statistical models on the weights themselves to both handle high dimensionality and exploit useful structure of the multi-study collection. The work will be carried out within the framework of multi-study stacking, where predictions generated by study-specific models are used as features in a second-stage analysis (typically a regularized regression) performed on the merged dataset collection. Coefficients in this step reflect cross-study replicability. The research will evaluate a range of specific prediction techniques within this paradigm, investigate their statistical properties theoretically and empirically, and compare them to existing alternative multi-study statistical methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113072","Testing and Estimation for Multi-Modality Single Cell Genomics","DMS","Genetic Mechanisms, STATISTICS, MSPA-INTERDISCIPLINARY","08/01/2021","07/20/2021","Eugene Katsevich","PA","University of Pennsylvania","Standard Grant","Yong Zeng","07/31/2024","$184,999.00","","ekatsevi@wharton.upenn.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","1112, 1269, 7454","068Z, 7465","$0.00","This project aims to develop new statistical analysis methodologies for data produced by a new technology known as single cell CRISPR screens, which promise to help unravel the molecular mechanisms of human disease and guide drug development. This technology discovers the functions of crucial but poorly understood genomic regions by knocking them out and then measuring the impact on the cell.  Given these possibilities, this technology has attracted enormous interest in both academia and industry, leading to its rapidly growing adoption. However, the statistical analysis of the data produced by CRISPR screens presents a major challenge to realizing their promise. The PI will tackle several of the most important statistical analysis challenges presented by this technology and implement the resulting methodologies in software tools. These tools will help scientists draw reliable conclusions from their single cell CRISPR screen data and therefore accelerate progress in the prognosis, diagnosis, and treatment of human disease. The project will provide training opportunities to graduate students by involving them in the research. <br/><br/>The PI recently leveraged the distilled conditional randomization test (dCRT) methodology to design SCEPTRE, a computationally efficient, well-calibrated, and powerful analysis method for single cell CRISPR screens. In this project, the PI will expand his recent work on association testing for single cell CRISPR screens to more challenging problem settings (non-interventional screens) and more challenging inferential tasks (estimation and interaction detection). First, the PI will extend the ideas underlying SCEPTRE to address the problem of association testing in multi-omics screens, the observational counterparts of CRISPR screens. Second, the PI will develop a procedure to estimate the effect of knocking out a regulatory element on the expression of a gene based on single cell CRISPR screens. This will require more carefully accounting for the indirect measurement mechanism of CRISPR perturbations. Third, the PI will design a test of interaction between two regulatory elements, also based on CRISPR screens. This problem is even harder but critical to capturing the complexity of gene regulation. Finally, the resulting methods will be implemented in an integrated software package designed for practical application to real single cell data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113346","Collaborative Research: Inference and Decentralized Computing for Quantile Regression and Other Non-Smooth Methods","DMS","STATISTICS","07/01/2021","06/14/2021","Kean Ming Tan","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yong Zeng","06/30/2024","$174,759.00","","keanming@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","075Z, 079Z","$0.00","Recent years have witnessed the transition of statistical analysis from a small- or moderate-scale data environment to a world involving massive data on parallel and distributed computing platforms. However, such a transition poses significant statistical and computational challenges for many important methods with non-smooth loss functions. As a representative example, quantile regression methods are building blocks for many advanced methods in statistics and econometrics and are frequently used to model financial data and medical data. The computational inflexibility makes quantile regression less favorable among various branches of the statistical learning tool kit. The project aims to develop a unified framework for large-scale learning with non-smooth loss functions to address the aforementioned problems. The developed methods will be applied to analyze complex biomedical data subject to censoring or privacy protocol and large-scale public health data. Both graduate and undergraduate students will receive training through research involvement in the project, ranging from developing new methods and theory to open-source software under different platforms. <br/> <br/>The principal investigators will use a combination of tools from statistics, optimization, and probability to develop a unified convolution smoothing framework and establish rigorous theoretical and algorithmic foundations for a class of statistical methods with non-differentiable loss, typified by quantile regression and support vector machine. The former is indispensable for understanding pathways of dependence and heterogeneous effects irretrievable through standard conditional mean regression analysis. However, most existing computational methods for quantile regression are based on generic algorithms, which are not scalable in large-scale machine learning applications when the number of variables is large. Convolution smoothing admits fast calibrated gradient-based algorithms without compromising the estimates' quality, therefore offering a balanced trade-off between statistical accuracy and computational precision. It also extends the applicability of quantile regression, from low to high dimensions, fully to partially observed samples, and linear to nonlinear structures, in modern big data analytics. The first part of the project will focus on three statistical problems: (a) high-dimensional sparse quantile regression, (b) large-scale censored quantile regression, and (c) robust regression with redescending M-estimation. The second part of the research focuses on developing efficient decentralized algorithms for methods with non-smooth loss functions under two modern data types: (i) parallel and distributed data, and (ii) online streaming data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113611","Central Limit Theorems and Inference in High Dimensions","DMS","PROBABILITY, STATISTICS","07/01/2021","06/25/2021","Arun Kuchibhotla","PA","Carnegie-Mellon University","Standard Grant","Yong Zeng","06/30/2024","$300,000.00","Alessandro Rinaldo","arunku@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1263, 1269","079Z, 1263, 1269","$0.00","The Central Limit Theorem (CLT) is a fundamental result in probability theory, asserting that the aggregate behavior of large ensembles of small and approximately independent stochastic quantities follows a universal law, informally known as the bell curve. The CLT approximation is a cornerstone of statistical inference, as it provides the theoretical underpinning of the vast majority of statistical methods for estimation, hypothesis testing, and confidence intervals. It is also routinely used for uncertainty assessment across the sciences and in many industrial applications. Despite the immense popularity, most existing CLT results are unable to fully express mathematically the degree of complexity that is typical of modern, large datasets. In addition, they are inadequate for high-dimensional statistical modeling. As a result, reliance on classic CLT approximations to verify the validity of statistical procedures is no longer justifiable in high-dimensional settings. Instead, new, more refined CLT guarantees are in order. Recent breakthrough advances have led to the formulation of new high-dimensional CLTs (HDCLTs), whose validity holds but only under fairly restrictive assumptions. The broad goal of this project is to develop new HDCLT approximations that are applicable across a significantly wider range of conditions and settings and to elucidate their uses in high-dimensional statistical problems involving large and complex data.<br/><br/>The research components of this project include five main research aims: (1) to produce HDCLTs for independent observations under weak conditions that allow for heavy-tail data and singular covariances; (2) to derive new high-dimensional Edgeworth expansions; (3) to obtain new HDCLTs for sums of dependent random vectors and time series processes; (4) to study HDCLTs for high-dimensional random matrices, such as the sample covariance matrix, and their spectra; and (5) to deploy HDCLTs in multiple-testing problems targeting FDR and FWER control. The research outcomes of this project will advance in important ways the theory and applications of HDCLT approximations and will lead to novel, practicable tools for inference in high-dimensional statistics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113375","Default Bayesian Analysis of Spatial Data","DMS","STATISTICS","08/01/2021","06/23/2021","Victor DeOliveira","TX","University of Texas at San Antonio","Standard Grant","Yong Zeng","07/31/2024","$159,996.00","","victor.deoliveira@utsa.edu","1 UTSA CIRCLE","SAN ANTONIO","TX","782491644","2104584340","MPS","1269","079Z, 1269","$0.00","The collection and analysis of spatial data are ubiquitous in most natural and earth sciences, such as ecology, epidemiology, geology and hydrology. This research project will develop statistical methodology for automatic Bayesian analysis of Gaussian models that does not require any subjective input. These models play a prominent role due to their versatility to model spatially varying phenomena, and because they serve as building blocks for the construction of more elaborate models. The Bayesian approach is especially appealing when the main goal is spatial interpolation, but implementing it for such models faces two big challenges: (i) Automatic formulation of sensible prior distributions that are adapted to the scale of the data under investigation and (ii) Analysis of massive data sets that are the norm nowadays. The project aims at developing theory and practice to overcome both challenges, which will make practicably feasible the automatic Bayesian analysis of large spatial data sets. In addition, the project will train graduate students in spatial statistics in general, and the topics of this project in particular. The results derived from the project will be disseminated in diverse outlets, and software to implement the methodology will be made publicly available.<br/><br/>The research project will make practicably feasible default Bayesian analyses of large spatial data sets, by contributing innovations to the two parts of the Bayesian model. First, approximate reference priors for the parameters of covariance models will be developed that allow carrying out Bayesian analyses for these models in an automatic fashion, not requiring subjective elicitation. These will be based on the spectral approximation of stationary random fields. Second, likelihood approximations feasible for large spatial data sets will be elaborated by developing strategies to tune a recently proposed approximation for stationary covariance functions. The tuning of the approximation aims at striking a balance between accuracy and computational effort. Both approximations rely on the spectral density, rather than the covariance function, of the model. Together, the reference prior and likelihood approximations will make possible carrying out default Bayesian analyses that include model selection and assessment. In addition, the project will critically assess the common practice of fixing the smoothness of the random field at a value, chosen by convention or tradition, that bears no relation to the data under analysis. The project will investigate methods to quantify the information content in spatial data about smoothness parameters, and uncover how this depends on the sample design. The methodology will be tested on diverse data sets from the earth sciences, with special focus on spatial rainfall data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113599","Slow Kill for Big Data Learning","DMS","STATISTICS","09/01/2021","06/11/2021","Yiyuan She","FL","Florida State University","Standard Grant","Yong Zeng","08/31/2024","$170,000.00","","yshe@stat.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","1269","$0.00","Big-data applications typically involve large numbers of samples and features and are often contaminated with outliers, posing challenges for variable selection and parameter estimation.  Fitting a sparse model with a prescribed cardinality is a common request in practice, but it is associated with solving a highly nonconvex and discrete problem. Using multiple starting points in such nonconvex optimization is common, but is often computationally prohibitive on big data; new cost-effective techniques are needed to alleviate the starting point requirement and ensure the best statistical accuracy. Moreover, how to adjust an arbitrarily given loss function to guard against gross outliers and achieve a high break-down point poses a major challenge for modern-day data analysis. The project will study innovative and efficient statistical methods and perform rigorous theoretical analysis to answer these questions. In this project, education is tightly coupled with research, consisting of course development, student mentoring, outreach, and recruiting underrepresented students.<br/><br/>The project will propose a novel slow-kill technique for large-scale variable selection, motivated by a scalable optimization algorithm with iteration-varying threshold and simultaneous L2-regularization. The three main elements of progressive quantile control, growing learning rate and adaptive L2-shrinkage in slow kill have solid theoretical support, and its ability to reduce the problem size during the iteration, as opposed to boosting and forward pathwise algorithms, makes it attractive for big data. The interplay between statistics and optimization in the project will reveal tight error rates and fast convergence under some regularity conditions, without the need to pursue a globally optimal solution. Furthermore, a framework of outlier-resistant estimation will be introduced to robustify a given method beyond the standard likelihood setup. It has a close connection to the method of trimming but includes explicit outlyingness parameters for all samples, which in turn facilitates computation and theory. With slow kill, the number of data resamplings will be substantially reduced, and the obtained resistant estimators can enjoy minimax rate optimality in both low and high dimensions. Overall, the proposed research will create a new-generation high dimensional tool for robust sparse learning that can accommodate coherent designs and gross outliers in big data applications, to deepen and broaden existing methods and theory in statistics and optimization.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2052955","FRG: Collaborative Research: Dynamic Tensors: Statistical Methods, Theory, and Applications","DMS","STATISTICS","09/01/2021","08/17/2021","Ming Yuan","NY","Columbia University","Standard Grant","Yulia Gel","08/31/2024","$300,000.00","","ming.yuan@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","075Z, 079Z, 1616","$0.00","Dynamic tensor data, represented by multidimensional arrays that vary in time, has become increasingly important to society at large. It is collected in a wide range of applications, from biology and medical research, natural sciences, and engineering to social sciences, economics, and finance. This research aims to develop novel statistical theory, methods, and algorithms for analyzing large dynamic tensor data. The work also includes analysis of the computational efficiency and utility of the methods under development. The results will provide state-of-art statistical tools for effectively extracting useful information from such data and aiding practical decision making in a wide spectrum of applications. The project will apply the new methods to important examples, including motion behavior modeling and crime data analysis. The project will foster collaborations among students and young researchers through involvement in cutting-edge research. Software and other tools will be made publicly available, enhancing scientific progress and data driven decision-making processes in practical applications.<br/><br/>The objectives of the research are to develop statistical theory, methods, and algorithms for analyzing large dynamic tensor data and to demonstrate their feasibility, effectiveness, and utility in interesting applications. Dynamic tensor data, an area with opportunities for systematic methodological and theoretical treatment from a statistical point of view, is creating new challenges and opportunities for researchers. The project will develop autoregressive and dynamic factor models for continuous tensor time series data, and generalized dynamic tensor models for binary, count, and other non-Gaussian data; produce new tools for forecasting, parameter estimation, and statistical inferences for such models; and study the theoretical and empirical properties of the new methods. The project findings are expected to have impact in other fields of statistics, including discrete tensor analysis, video analysis, inference of high-dimensional tensors, and analysis of high dimensional dynamic systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2046393","CAREER: Geometric and Combinatorial Methods for Distribution-Free Inference and Dependent Network Data","DMS","STATISTICS","07/01/2021","06/26/2023","Bhaswar Bhattacharya","PA","University of Pennsylvania","Continuing Grant","Yong Zeng","06/30/2026","$240,000.00","","bhaswar@wharton.upenn.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","1269","1045","$0.00","Modern statistical applications often involve multivariate data that violate the convenient assumptions of independent sampling and tractable parametric forms. For instance, parametric methods are often inadequate in the analysis of complex high-dimensional data arising from genomics, epidemiology, and bioinformatics. This necessitates the development of procedures that are agnostic to the distribution of the data, computationally efficient, and yet statistically powerful for large nonparametric classes. Similarly, the classical assumption of independence is routinely violated in combinatorial datasets arising from social networks, making it increasingly important to develop realistic and mathematically tractable methods for modeling structure and dependence in high-dimensional distributions. This project leverages ideas from recent developments in optimal transport theory, random geometric graphs, and statistical physics to gain a deeper understanding of (1) multivariate distribution-free inference and (2) dependent network data. The educational and outreach component of this project will aim to foster undergraduate research and prepare graduate students in mentoring, through curriculum development, directed reading groups, and summer programs. <br/> <br/>The first component of this project will study the efficiency properties of nonparametric, distribution-free two-sample tests based on the emerging theory of multivariate ranks, which include, among others, the rank analogue of the celebrated energy distance test. The project will also explore the asymptotic properties of tests based on optimal matchings and their applications to detecting balance in observational studies. The second component of this project will focus on modeling dependence in complex relational data, using the Ising model and, more generally, higher-order (tensor) Markov random fields. The goal here is to build a framework for simultaneously modeling the network dependency (arising from neighborhood interactions) and the individual node effects, and to develop a holistic theory of parameter estimation in these models using recent advances on random tensors and tools from statistical physics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2044823","CAREER: Next-Generation Methods for Statistical Integration of High-Dimensional Disparate Data Sources","DMS","STATISTICS","06/01/2021","04/26/2023","Irina Gaynanova","TX","Texas A&M University","Continuing Grant","Yong Zeng","05/31/2026","$245,638.00","","irinag@stat.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","1045","$0.00","Multi-view data (collected on the same samples from multiple sources) are increasingly common with advances in multi-omics, neuroimaging and wearable technologies. For example, wearable devices such as physical activity trackers, continuous glucose monitors and ambulatory blood pressure monitors are worn concurrently to provide measurements of distinct subjects? characteristics. There is enormous potential in integrating that concurrent information from the distinct vantages to better understand between-view associations and improve prediction of health outcomes. Existing tools for data integration are sensitive to outliers, and are not designed for mixed data types (e.g. continuous skewed glucose measurements, zero-inflated activity counts, binary indicators of sleep/wake). The PI will develop a more robust framework for multi-view data integration that is better able to account for outliers, better match the mixed types of data actually collected, and be more accurate in separating common from view-specific signals. The new methods will be implemented in open-source software accompanied by reproducible workflow examples, providing immediate and easy access for other researchers. The educational component centers on the development of structured research experiences (SRE) for students. SRE enhances students written communication, software development and reproducible research skills, all of which are lacking in traditional curriculum. This will improve students? preparation for conducting research, and widen their STEM employment opportunities. The involvement of students from traditionally underrepresented groups will positively impact their retention rate and will broaden the participation of underrepresented groups in STEM.<br/><br/>Popular dimension reduction methods, such as principal component analysis and discriminant analysis, are tailored for single-view data, and thus fail to discover coordinated multi-view signals on a global level. On the other hand, existing multi-view dimension reduction methods suffer from reliance on the Gaussianity assumption, an inability to capture joint functional signals, and a lack of theoretical guarantees. The PI will address these drawbacks by (i) developing a joint dimension reduction framework for skewed continuous, binary and zero-inflated view types; (ii) a joint dimension reduction framework for mixed functional multi-view data and (iii) a new paradigm for simultaneous extraction of signals across views based on hierarchical low-rank constraints.  This work will lead to critically needed new statistical methods for data integration with direct relevance for researchers working with wearable monitors, microbiome and multi-omics data through interdisciplinary collaborations of the PI. The proposed structured research experiences will center on the design and reproducibility of simulations studies, and align with computational components of the proposed research, including direct students? involvement in multiple simulation studies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2045981","CAREER: Beyond Conditional Independence: New Model-Free Targets for High-Dimensional Inference","DMS","STATISTICS","07/01/2021","07/25/2023","Lucas Janson","MA","Harvard University","Continuing Grant","Yong Zeng","06/30/2026","$233,144.00","","ljanson@fas.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269","1045","$0.00","The field of statistics has seen great success over many decades drawing scientific insights from simple, easy-to-understand models. But progress in computing is now allowing researchers to measure and store huge amounts of data at once, opening the door to understanding and manipulating much more complex systems than ever before. Indeed, the field of machine learning has been very successful at fitting predictive models to such data sets, but the black-box nature of these methods makes it hard to draw scientific insights from them using the usual statistical approach. In fact, not only do classical statistical methods fail in this modern big data setting, but the questions they answer no longer even make sense because the models they are based on do not hold even approximately. In this research, the PI will first come up with new statistical ways of posing scientific questions that make sense in complex data but still have interpretable answers. Second, the PI will find new statistical methods to answer those questions in a rigorous way, and study the mathematical and computational properties of these methods so that they can be used as effectively as possible. And finally, the PI will work with experts in the areas of genomics, the microbiome, and political science to use these methods to gain new scientific insights in these fields. Throughout the project, the PI will also run a free statistical consulting service to help the broader research community, develop new curricula with high school teachers, and provide enriching educational and research experiences for undergraduate and graduate students.<br/><br/>To understand the importance of a covariate in a high-dimensional regression, it has become increasingly popular to perform a hypothesis test for conditional independence with the response. The appeal of such a test is that it provides statistically rigorous insight that is well-defined and scientifically interpretable no matter how the response depends on the covariates, including the case when their relationship is highly nonlinear and includes interactions, possibly of high order. However, conditional independence as a model-free inferential target only provides a type of scientific insight that can be of little use in some applications. The PI will extend conditional independence to new model-free targets for two types of data on which it does not provide a useful inferential target, namely, data with highly-locally-dependent covariates and data with compositional covariates. Then, the PI will move past conditional independence entirely to propose novel model-free targets that instead of just identifying (as in variable selection) actually numerically quantify relationships in the data, such as the relationship between a covariate and a response or the interaction between two covariates in the response's conditional distribution. Along with each new target, the PI will develop entirely new methods for powerful and provably valid inference. The novelty of the proposed targets and associated methods will provide for new connections with other fields of statistics including Bayesian computation, measure theory, statistical physics, experimental design, causal inference, and graphical model estimation. Ultimately, this research aims to provide a suite of new tools for researchers to move beyond the constraints of parametric targets and instead leverage state-of-the-art machine learning tools to answer novel and important questions about their data in a statistically principled way.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2112938","Post-Selection Inference for Survival Outcomes in Precision Medicine","DMS","STATISTICS","08/01/2021","07/21/2021","Min Qian","NY","Columbia University","Standard Grant","Yulia Gel","07/31/2024","$249,989.00","Ian McKeague","mq2158@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269","","$0.00","Recent breakthroughs in biomedical technology produce massive amounts of data on individual patients. Typically, however, only a relatively small number of features, if any, may be predictive of the clinical outcome, especially when the outcome is a survival time. A central aspect of scientific discovery in this scenario is to detect significant predictors among a large set of covariates. In addition, in studies where treatment assignments are observed, an essential goal is to develop strategies for precision medicine. To achieve this goal, it is important to identify covariates that interact with the treatment. As the resulting model fits play a role in informing clinical decisions and guiding future research, it is crucial to provide inferential guarantees for the selected covariates. This is the post-selection inference problem in a nutshell. The overarching goal of this project is to develop a unified hypothesis testing procedure that can be used to detect variables that are predictive of survival outcomes under right censoring, as well as to identify significant treatment-by-covariate interactions of survival outcomes that can be used in making optimal treatment decisions.<br/><br/>    This project will develop new methods of post-selection inference for screening high-dimensional predictors of survival outcomes and use those methods to design new classes of treatment selection policies. The problem is challenging, not only because of nonregular asymptotic behavior (of test statistics and estimators), but also because of the presence of censoring. The plan involves construction of a semi-parametrically efficient estimator of the slope parameter (in an accelerated failure time model) corresponding to the maximal marginal correlation between each predictor and the survival outcome, and devising a calibration of a regularized version of this statistic to furnish a formal screening test that will detect significant associations. Further, methods of constructing and assessing the effectiveness of optimal treatment policies based on the detected associations will be developed. The resulting procedures are expected to be more powerful and efficient than existing methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113696","Collaborative Research: Halfspace Depth for Object and Functional Data","DMS","STATISTICS","07/01/2021","06/02/2021","Sara Lopez-Pintado","MA","Northeastern University","Standard Grant","Yong Zeng","06/30/2024","$174,999.00","","s.lopez-pintado@northeastern.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173733004","MPS","1269","","$0.00","Complex data objects are increasingly being generated across science and engineering. Non-Euclidean data such as wind directions, neural connectivity networks, and phylogenetic trees draw practical interest, but are challenging to analyze due to their intrinsic constraints. Functional data such as trajectories and images also provide examples of another type of data of high complexity, which are observed on a continuous domain in time or space. In general, practitioners are interested in first exploring the data distributions before any modeling analysis. For instance, given a sample of growth trajectories of children, a first step is to identify typical versus extreme growth patterns, where the latter can be non-trivial to uncover. Also, when analyzing brain connectivity matrices, it is important to find unusual brain networks and differences between healthy and diseased populations. Data-driven methods robust to anomalies are essential in these settings since little is known about the data generating process, and outliers can affect the analysis. Due to the lack of a natural ordering in data objects, exploratory tools such as boxplot and quantile are unavailable for these types of data. The project will address the lack of techniques for exploring non-Euclidean and functional data. Principled statistics and visualization methods will be developed based on a novel way of ranking the observations. The project will also provide training for graduate and undergraduate students. <br/><br/>The central research theme is to develop exploratory data analysis tools for non-Euclidean and functional data objects. To overcome the absence of a canonical ordering for object data, the PIs will develop suitable data depth notions to quantify the centrality of data points with respect to the distribution. This will provide a center-outward ranking of the data that will be used as a building block for outlier detection methods, rank tests, and robust classifiers. Analogous to Tukey's halfspace depth for the multivariate Euclidean case, the new depth notions for object data are expected to be intuitive and robust, and have desirable properties well-grounded in theory. Specifically, the research project will investigate a depth notion for non-Euclidean objects; a data visualization and an outlier detection procedure for non-Euclidean data; halfspace depth notions for functional data, one based on theory and another one from an algorithmic perspective; and a depth notion for sparsely observed longitudinal data. Key challenges that will be addressed include a lack of vector space structure when dealing with non-Euclidean objects; the infinite dimensionality and degeneracy when defining depth notions for functional data; detecting outlying trajectories and images in shape and not just at any time point; and the sparsity and irregularity of observations in longitudinal data. Method and theory development will draw from metric geometry, functional data analysis, empirical process, and M-estimation. Software implementing a suite of depth-based methods will be made available to the public as an outcome of the project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2153669","Markov Random Fields, Geostatistics and Matrix-Free Computation","DMS","STATISTICS","10/01/2021","10/19/2021","Debashis Mondal","MO","Washington University","Standard Grant","Yong Zeng","08/31/2023","$73,789.00","","mondal@wustl.edu","ONE BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","1269","","$0.00","In the past few decades, spatial statistics has become increasingly important in agriculture, epidemiology, geology, image analysis and environmental science. PI's prior research provided new perspectives in connecting two major branches of spatial statistics, namely the Markov random fields and geostatistics and in advancing fast statistical computations.  At present, many important scientific applications demand use of complex spatial models and their multivariate and spatial-temporal versions. However, statistical computations of these complex spatial models have remained a challenge. The project derives new mathematical understanding on these complex spatial and spatial-temporal models, which then opens up the possibility of advancing various scalable statistical computations with minimal storage.  The project will contribute to obtaining enhanced scientific understanding in studies such as arsenic and magnesium contamination and hydro-chemical analysis of groundwater and spatial and spatial temporal variations in opioid overdose cases in the United States.<br/><br/>The project brings together mathematical and computational knowledge from different scientific fields to develop principled frameworks for spatial statistics and inference. The research aims to provide new understanding on (i) constructions of higher neighborhood order Gaussian Markov random fields, (ii) joint modeling of two or more spatial variables, and (iii) complex spatial-temporal models. Novel matrix-free computations are proposed to advance statistical inference. These computations include not just best linear unbiased predictions and residual maximum likelihood estimation, but also scalable Hamiltonian Monte Carlo methods. Applications will include mapping (1) heavy metal contamination in groundwater and (2) geographic variations in drug overdose cases across the United States. The project also aims to integrate research and educational activities through developing short courses and case studies on spatial statistics and scalable computation, and through providing valuable training and learning opportunities for graduate students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113458","Theory and Methods for Tree-Informed High-Dimensional Compositional Data Analysis","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","08/01/2021","06/14/2023","Shulei Wang","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Yong Zeng","07/31/2024","$149,999.00","","shuleiw@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269, 7454","068Z, 079Z","$0.00","Compositional data, that is, quantitative measurements of the parts of some whole, is subject to constraints that necessitate its analysis be distinct from that of standard unconstrained multivariate statistical analysis. High-dimensional compositional data naturally arises in a wide range of modern scientific applications, including human microbiome studies, nutritional science, genomics studies, and geochemistry. In these scientific applications, a hierarchical relationship represented by a tree structure is often available for the compositional data?s different components. Because of compositional nature and tree structure, these data pose a unique challenge to gaining reliable and scientifically meaningful insights in a data-driven way. Current efforts on analyzing such tree-informed compositional data are primarily designed for individual applications; there is need for new methodology and theory in a unified framework. Motivated by this need, this project aims to develop novel statistical theories, methodologies, and computational tools for more robust and efficient analysis. The project will provide interdisciplinary research opportunities for students who aim to work on the intersection between statistics and other scientific areas. The project will also develop user-friendly open-source software implementing the new statistical methods to benefit a broad scientific community. <br/><br/>This project aims to study how statistical analysis should take data's compositional nature and tree structure into account reliably and efficiently. Through a unified framework, the project will develop novel and principled methodologies and provide a deep understanding of the tree structure's role in tree-informed compositional data analysis. Specifically, this research will study three fundamental topics in tree-informed compositional data analysis: 1) independence and conditional independence test for tree-informed compositional data, 2) testing for differential components in tree-informed compositional data, and 3) metric learning for tree-informed compositional data. The resulting methods and theories from the project will lead to more robust and powerful practical tools for tree-informed compositional data analysis in different scientific fields, ultimately helping advance knowledge in science and health.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113426","Inference for Functionals in High-Dimensional Regression","DMS","STATISTICS","07/01/2021","06/22/2023","Pragya Sur","MA","Harvard University","Continuing Grant","Yong Zeng","06/30/2024","$170,000.00","","pragya@fas.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269","1269","$0.00","Modern science and engineering applications involve large datasets with a multitude of variables or features. A key challenge in this context is to distinguish the scientifically relevant variables from the irrelevant ones - in other words, the signal from the noise. The challenge is compounded by subtle nonlinear relationships among these variables. Generalized linear models are the most often used tools in classical statistics for discovering such nonlinear relationships and they are routinely employed, even in contemporary big data settings. Unfortunately, classical statistical theory, traditionally used to justify the validity of these methods, fails in this regime. This project will develop novel approaches for inferring scientifically relevant parameters in the framework of generalized linear models, adapted to the setting of high-dimensional or big data. The theory developed will facilitate principled inference regarding the relations among observed variables in applications such as genomics, computational neuroscience, signal and image processing. The principal investigator will also engage graduate students in the project by mentoring them and develop courses that will incorporate results from this project.<br/><br/>This research project will develop statistical theory and methods for inferring scientifically relevant low-dimensional functionals in high-dimensional generalized linear models, organized around two broad themes: (1) frequentist inference for signal-to-noise ratio type functionals; (2) Bayesian inference for functionals under continuous shrinkage priors. The first theme will develop novel estimators for the signal-to-noise ratio and the  genetic relatedness, a generalization of the signal-to-noise ratio that measures the shared genetic basis between multiple traits in statistical genetics. The second thrust will construct data-driven credible intervals for components of the underlying signal under computationally tractable continuous shrinkage priors. Both thrusts will develop inference procedures agnostic to sparsity level of the underlying signal. To achieve this, the research will focus on the proportional asymptotics high-dimensional regime and utilize novel insights  from  approximate  message  passing theory, developed originally in probability, information theory, and statistical physics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2053188","CDS&E: Point Process Models for Traffic Risk Analysis and Crash Prevention","DMS","STATISTICS, CDS&E-MSS","08/15/2021","08/02/2021","Matthew Heaton","UT","Brigham Young University","Standard Grant","Yong Zeng","07/31/2024","$199,941.00","Richard Warr, Philip White, Grant Schultz","mheaton@stat.byu.edu","A-153 ASB","PROVO","UT","846021128","8014223360","MPS","1269, 8069","079Z, 9263","$0.00","Traffic and crash databases collected by state and federal departments of transportation contain a wealth of information that can be used to increase highway safety.  For example, crash databases can be used to identify locations where crashes frequently occur as well as the underlying factors that contributed to those crashes.  Such analysis of crash databases can subsequently lead to identifying countermeasures that can be enacted to decrease the frequency of crashes at high-risk locations. While many efforts are ongoing to use the information contained in these databases to increase traffic safety, modern traffic datasets contain more information than can be currently extracted using traditional data analysis techniques.  The most glaring shortcoming of common statistical techniques for crash data is that such techniques focus only on small segments of the road (e.g. intersections) rather than analyzing the entire roadway network simultaneously.  In this project, the researchers are developing statistical methodology that will analyze an entire roadway network to capture important relationships between roadway features that may lead to an increase in crashes.  Ultimately, the goal of this project is to analyze traffic network data so as to identify ways to create a safer roadway network for all travelers.  Beyond research activities, mentoring activities associated with this project include student mentoring on advanced topics in data science and traffic safety engineering.  Educational activities will include STEM career presentations to high school students as well as the development of a novel interdisciplinary research group.<br/><br/>Historically, statistical models for traffic crashes have analyzed aggregated crash counts along with roadway segments, where aggregated data forfeit the use of any within-segment information.  Modern crash databases, however, contain data on the exact locations of crashes (referred to as point pattern data) which, if analyzed appropriately, can give richer statistical inferences than aggregated data.  This project seeks to fully utilize the information in modern traffic databases by considering the continuous nature of roadway traffic rather than relying on arbitrarily aggregated count data over roadway segments.  Specifically, this project will develop easily implementable and computationally feasible approaches to modeling crash point pattern data to determine where crashes are likely to occur (referred to as hot spot identification) as well as how features of the roadway influence the potential for a crash (referred to as risk factor identification).  Specifically, this project has the following goals related to statistical and civil engineering science: (1) develop piece-wise linear point process models on roadway networks and (2) develop a hierarchical point process approach to model multiple crash types simultaneously.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2046880","CAREER: Design and analysis of experiments for complex social processes","DMS","STATISTICS, Methodology, Measuremt & Stats","06/01/2021","07/31/2023","Alexander Volfovsky","NC","Duke University","Continuing Grant","Yong Zeng","05/31/2026","$232,601.00","","alexander.volfovsky@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269, 1333","1045","$0.00","Modern scientific inquiry from the social to the health sciences centers around answering foundational ?what if?? questions with a special emphasis on understanding the effects of changing complex social processes such as the arrangement of individuals into networks or groups and communication of information via text. The causal inference literature has heralded the role of randomization in getting answers to such ?what if?? questions, treating randomized controlled experiments as a gold standard for testing simple causal hypotheses. However, hidden behind this powerful tool are a series of assumptions and design decisions that are difficult to control for and are untenable in the context of complex and changing social processes. The PI will develop novel theory and methodology that will directly address the role of these social processes in causal inference. The new tools can be used across disciplines to study interventions whenever social processes are present such as in the study of important societal questions relating to vaccines, non-pharmaceutical interventions, implications of different education policies and the like. The PI's education plan integrates the research products from this project into courses that will engage students from a wide range of academic backgrounds, presenting and linking the methodological contributions to substantive applications. Research products will be widely disseminated to the scientific community and the general public through popular and scientific publications, presentations, and open-source software.<br/><br/>This project addresses the nascent areas of causal inference in the presence of network information and text data. While much of the work in these areas has concentrated on the analysis of existing experimental designs, little work has gone into designing experiments specifically for such complex social processes. The research will demonstrate the inadequacy of classical designs and the importance of developing specialized experimental designs that adapt to underlying complex social processes. For network and text data, the PI will provide a comprehensive framework for defining causal quantities of interest that exploit the structure in such data. The PI will work on three main thrusts: (1) Conditional design in the presence of network information: this thrust will develop restricted randomizations to target the testing and estimation of peer effects, total effects and other network quantities; (2) Unconditional design where the experimenter can control the social process: this thrust will draw on results from graph sampling to develop experimental designs that simultaneously design an interaction graph (that can represent how study participants will be allowed to interact) and a treatment allocation; and (3) Text as a social process: this thrust will provide guidance and tools for extracting causal quantities from text data that can play the role of treatment, outcome, confounder or mediator in a causal analysis. The output of the research will include practical guidelines for experimental design as well as adaptable tools and algorithms that can be deployed to address a wide range of social processes and applied problems beyond those studied in this project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113121","Estimation of Functionals of High-Dimensional Parameters of Statisical Models","DMS","STATISTICS","07/01/2021","07/12/2021","Vladimir Koltchinskii","GA","Georgia Tech Research Corporation","Standard Grant","Yong Zeng","06/30/2024","$219,999.00","","vlad@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","1269","$0.00","Estimation of low-dimensional features of high-dimensional parameters is an important subject in contemporary statistical analysis of complex, high-dimensional data. While information-theoretic limitations often make impossible the reliable estimation of the whole unknown parameter due to its high dimensionality, estimation of low-dimensional features could be done efficiently with much faster error rates, common in classical statistics. Such problems often occur in applications, in particular, when the unknown parameter is a large matrix such as the density matrix of a quantum system, and the features of interest are various spectral characteristics of such matrices. Despite the fact that these problems have been studied for many years, there are few general approaches to statistical estimation of functionals representing the features of interest. The main goal of this project is to study functional estimation problem in a general mathematical framework and to develop general estimation methods as well as a comprehensive theory showing how the error rates in functional estimation depend on the underlying properties of the target functional such as its smoothness. The project provides new opportunities for training  graduate students in the areas of high-dimensional statistics, in particular, by developing graduate level courses and seminars.           <br/><br/>The main focus of the project is on the development of a higher order bias reduction method (bootstrap chain bias reduction) in estimation of smooth functionals of unknown high-dimensional parameter of statistical model. It is based on iterative bootstrap and it could be viewed as a method of approximate solution of certain integral equations on high-dimensional parameter spaces. In the case of high-dimensional Gaussian models, this method yields estimators of smooth functionals with optimal error rates. This research project will study the properties of such estimators for a variety of important high-dimensional statistical models, including log-concave models, models on manifolds, sparse models and density matrix estimation models in quantum statistics. The goal is to determine minimax optimal error rates in functional estimation and to study the phase transition between fast parametric and slow nonparametric rates depending on the degree of smoothness of the functional and complexity parameters of the problem. This requires solving a number of challenging analytic and probabilistic problems, including the study of approximation of bootstrap Markov chains by superpositions of independent stochastic processes (random homotopies), the development of high-dimensional normal approximation and coupling methods as well as of concentration bounds for statistical estimators. The project will result in much deeper understanding of functional estimation problems in high dimensions and in the development of a variety of new probabilistic tools in high-dimensional statistical inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2114143","New Frontiers in Time Series Analysis","DMS","STATISTICS","09/01/2021","07/09/2021","David Matteson","NY","Cornell University","Standard Grant","Yong Zeng","08/31/2024","$299,999.00","","dm484@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","1269","$0.00","Big data are now prevalent in nearly every domain. While shrinkage and sparse estimators are essential to mitigate the volume issues posed by big datasets, innovative models are needed to address their variety and velocity demands. Indeed, enhanced monitoring and measurement systems now provide data at high enough resolutions that observations can often be considered intrinsically continuous or functional. Massive environmental, biological, industrial, and computational networks are being dynamically recorded such that their intricate evolution might be succinctly monitored, studied, and maintained. Vast datasets are generated every day, ranging from large-scale satellites and remote sensing instruments, emergency systems, and energy infrastructure, to nanoscale electromagnetic sensors, medical devices, and imaging devices. These systems provide rich information about our human-natural world, and most is sequential, or time-ordered and exhibit complex trends, transitions, and dependencies. Standard methods in multivariate statistics are unsuitable and insufficiently adaptable. The Principal Investigator (PI) will develop new methods and computational tools to aid data-driven scientific discovery and industry applications. The PI will also train and mentor graduate and undergraduate student research, freely disseminate new software and methodology across application areas, and foster collaboration between statistics and a wide range of fields, including economics, ecology, physics, finance, space weather, hydrology, agriculture, energy, environmental engineering, biophysics, mathematics, electrical engineering, human development, and computer science.<br/><br/>Most time-indexed data exhibit heteroskedastic noise, anomalies, change points, local and global trends, and both linear and nonlinear dependence, and there is a striking shortage of analytical tools suitable for modeling such complexity. Shrinkage, sparse, and adaptive estimators are exceptions and have become vital tools. Global-local and regularized estimation, via adaptive sparsity/smoothness-inducing penalties/priors, is essential ? it allows computationally tractable estimation of complex models, with greater interpretability and reduced estimation uncertainty. The PI will develop: (i) new methods and computational frameworks for change-point detection with increased flexibility by allowing global and segment-specific parameters; (ii) new theoretical and application-driven investigations into less explored aspects of hidden Markov models; (iii) extensions of dynamic shrinkage process for robust and adaptive estimation of change-points in the presence of outliers, spillover effects and causal inference on dependent network time series, and distributional trend filtering; (iv) new methods for simultaneous modeling and inference of dynamic functional data with complex features such as long range dependence and stochastic volatility.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113676","Inference for Stationary Processes: Optimal Transport and Generalized Bayesian Approaches","DMS","STATISTICS","07/01/2021","05/12/2023","Andrew Nobel","NC","University of North Carolina at Chapel Hill","Standard Grant","Yong Zeng","06/30/2024","$299,944.00","Kevin McGoff, Shayn Mukherjee","nobel@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","1269","$0.00","This project will address the problem of making inferences about sequences of observations that exhibit dependence arising from physical or other interactions.  Observations of this sort occur in many fields, including finance, ecology, natural language processing, and biology.  We will explore ways to fit a sequence of observations to a family of statistical models using ideas from the theory of optimal transport.  Informally, we will identify models in the family into which the generating mechanism of the observations can be transformed with the least overall cost.  We will address both the theory and efficient computation of these transformation costs, and will consider applications to biomedicine and computer science. The project will involve collaborations with graduate students and more senior researchers working in genomics and bioinformatics.  Both undergraduate and graduate students will receive training through involvement in supported research projects.<br/><br/>On a more technical level, this project will address inference for stochastic processes, in particular, how to fit a family of stationary processes to an observed ergodic process, revealed sequentially. Research will focus on the use and extension of ideas from optimal transport, including stationary couplings of stationary processes and related variational quantities, with a focus on methods development and supporting theory.  The research has two primary aims.  The first aim is to investigate minimum divergence estimation based on joinings, including the use and properties of entropy regularization.  The second aim is to investigate the efficient computation of optimal transition couplings of Markov chains, with applications to graph distances, graph alignment, and hidden Markov models.  The project will involve collaborations with graduate students and more senior researchers working in genomics and bioinformatics.  Both undergraduate and graduate students will receive training through involvement in supported research projects.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113364","Topics in Threshold Models: Efficient Procedures under Endogeneity in Regression Discontinuity Designs and Distributed and Robust Estimation of Change-Points","DMS","STATISTICS","07/01/2021","06/22/2021","Ya'acov Ritov","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yulia Gel","06/30/2024","$400,000.00","Moulinath Banerjee","yritov@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","The project concerns two different kinds of challenges. The first challenge involves a class of regression discontinuity designs where the question is whether a treatment, like a grant, scholarship, or being accepted to a lucrative program, directly impacts the outcome, for example, the future income of the student. The difficulty is that the treatment is given precisely to those expected to have a better outcome. Hence, it is not easy to sort whether the treatment made an impact, or, simply, the better candidate got it. A standard approach uses only the data about students near the threshold, comparing those students who barely got the scholarship to those just below the threshold. The method considered in the project uses all the data. The second problem in this project concerns a situation in which the distribution of the observations changes abruptly. This can happen if, for example, there is a change in the type of infectious agent in the environment. The project's main concerns are when this change is monitored in different sites, in each of them, the change occurs at approximately the same time. However, we need to strongly constrain the amount of information passed from each site to the central control because of privacy or traffic concerns.<br/><br/>The project deals with new estimation and inference techniques for several classes of problems that exhibit uni- or multi-dimensional discontinuities as key features of interest. The studied problems present two different kinds of challenges: The first is determining whether there is a tangible treatment effect in a class of regression discontinuity design (RDD) models, where the treatment group is determined by a pre-fixed threshold value of a core covariate. The model will be addressed via a novel point of view that introduces new estimating equations allowing the statistician to take advantage of the entire data at hand to propose semiparametric efficient estimates of the treatment effect in the presence of endogeneity. The second problem involves estimating single or multiple change-points in parallel data sequences/data-streams. Part of this plan deals with distributed computing for change-points, where the data sequence for each entity is stored on a single platform, and one has hard constraints on exchanging data across platforms. The modeling involves misaligned change points across the various data sequences, and the solutions involve computationally efficient methods with tractable statistical properties. The other part of this agenda aims to develop deeper theoretical insights into robust estimation of change-points in the presence of heavy-tailed response variables for canonical models and to develop effective methodologies in more complex incarnations of such problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2112988","Statistical Investigations in Ranking from Pairwise and Multi-wise Comparisons","DMS","STATISTICS","07/01/2021","06/16/2023","Ye Zhang","PA","University of Pennsylvania","Continuing Grant","Yong Zeng","06/30/2024","$220,000.00","","ayz@wharton.upenn.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","1269","1269","$0.00","Ranking from comparisons is a central problem in a wide range of learning and social contexts. Researchers in various disciplines including psychology, economics, and computer science have made significant contributions to the ranking problem. Despite much progress, many fundamentally important statistical tasks remain unclear. One example arises in quantifying the uncertainty of ranking procedures and in analyzing multi-wise comparison data, which appears naturally in recommendation systems, web search, social choice, and many other areas. This project aims to address these challenges by developing an in-depth understanding of the ranking problem through a systematic statistical and computational investigation. The wide range of important applications of the ranking problem ensures that the progress we make towards our objectives will have a great impact on a broad scientific community. The techniques and methods developed will further advance the interplay between a wide range of areas including statistics, optimization, and machine learning. New courses will be developed incorporating results from the project and graduate students will also be involved in the project through their research work supervised by the PI.<br/><br/>This project focuses on the following two directions. First, the PI will quantify the uncertainty in ranking from pairwise comparisons. To achieve this goal, the PI will carry out a thorough fine-grained and entrywise statistical investigation. Second, the PI will deepen theoretical and methodological understanding of the multi-wise comparisons by investigating various tasks including developing optimal procedures, carrying out entrywise analysis, and others. Advances along these directions will be made towards the goal of laying out a firm theoretic foundation that feeds back into the development of practical methodology and informed applications for ranking problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113684","Foundations of High-Dimensional and Nonparametric Hypothesis Testing","DMS","STATISTICS","07/01/2021","06/14/2021","Sivaraman Balakrishnan","PA","Carnegie-Mellon University","Standard Grant","Yong Zeng","06/30/2024","$250,000.00","Matey Neykov, Larry Wasserman","sbalakri@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","1269","$0.00","Statistical inferential tools are the main export from the discipline of statistics to the empirical sciences, serving as the primary lens through which natural scientists interpret observations and quantify the uncertainty of their conclusions. However, in the analysis of modern large datasets the most common inferential tools available to us are fraught with pitfalls, often requiring various technical conditions to be checked before their valid application. This in turn has led to misuse of the inferential tools and subsequent misinterpretation of results. This research project will aim to address this issue by developing and analyzing new user-friendly methodologies for statistical inference in complex settings. The methods we develop will be broadly applicable to a wide variety of challenging inferential problems in the physical and biological sciences, will eliminate the need to verify technical conditions, and will ultimately be robust in their application. The principal and co-principal investigators will be involved in advising and mentoring graduate students, in curricular and course development, and in integrating the project with a research group on Statistical Methods in the Physical Sciences (STAMPS).<br/><br/>This project will advance our understanding of high-dimensional and non-parametric inference along three frontiers. Firstly, we aim to develop statistical inferential tools for irregular models, which are valid under weak conditions. Our particular focus will be on mixture models, and on methods which use sample-splitting to avoid strong regularity conditions. Secondly, we will show that our methods achieve these strong guarantees at a surprisingly small statistical price. To rigorously quantify the statistical price paid for avoiding strong regularity conditions we will use minimax theory. However, standard minimax theory, in many cases, does not adequately capture the difficulty of statistical inference since the difficulty of inference can vary significantly across the parameter space. A more refined theory -- called local minimax theory -- leads to a more accurate picture, and we will study our methods via this lens. Finally, we will address the problem of conditional independence (CI) testing. Despite its central role in regression diagnostics, and in the study of probabilistic graphical models, the task of CI testing and its intrinsic difficulty is poorly understood. We will address two fundamental aspects of CI testing, by studying methods to appropriately calibrate CI tests, and by developing and analyzing powerful new CI tests.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2042473","CAREER: New Statistical Paradigms Reconciling Empirical Surprises in Modern Machine Learning","DMS","STATISTICS","07/01/2021","06/04/2023","Tengyuan Liang","IL","University of Chicago","Continuing Grant","Yulia Gel","06/30/2026","$240,000.00","","tengyuan.liang@chicagobooth.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","075Z, 079Z, 1045","$0.00","Exciting empirical breakthroughs have emerged in data science and engineering through combination of large-scale datasets, increasingly complex statistical models, and advanced computational power. The success also promises new directions in statistics and econometrics, among other scientific disciplines. Nevertheless, the empirical phenomena exhibited by modern Machine Learning (ML) challenge the core mathematical concepts in statistics and computation: (a) Why can complex over-parametrized models enjoy excellent statistical performances even with interpolating the training examples? (b) Why can seemingly simple stochastic optimization methods optimize such complex models effectively? (c) What kinds of structures or representations of data are responsible for modern ML models? efficacy over classical statistical models when the dimension becomes moderately large? This project aims to develop new statistical and computational paradigms that bridge the gap between theory and practice for learning from data. The project will also significantly impact undergraduate and graduate students? training in data science research through synergetic educational and research activities to be hosted under a new initiative that integrates and enhances resources across the fields of statistics and economics.<br/><br/>The project will investigate the role of regularization, statistical performance, and optimization algorithms in modern ML models, including kernel machines, boosting, random forests, and neural networks. In particular, the PI will focus on the following three modules. (a) Learning functions in the interpolation/overfitting regime: The PI will study the statistical performance of minimum-norm interpolated solutions, which fall beyond the realm of the classical empirical risk minimization analysis. The PI also plans to develop a rigorous mathematical framework to quantify the adaptive representation aspects of specific ML models. (b) Learning distributions with generative models and simulation-based inference: The PI will investigate the statistical foundations of generative models for learning implicit probability distributions and study new simulation-based inference procedures. (c) Optimization algorithms motivated by stochastic approximation and online learning: The PI will study the interplay between optimization and statistical performance of gradient-based stochastic approximation methods for learning complex ML models with non-convex landscapes. The research intends to challenge conventional wisdom in statistics and computation, modernize nonparametric statistics and learning theory education, and further shed light on devising the next generation nonparametric models with algorithms and computation in mind.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113637","Offline Statistical Reinforcement Learning with Applications in Precision Health","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","08/15/2021","09/20/2022","Wenbin Lu","NC","North Carolina State University","Standard Grant","Yulia Gel","07/31/2024","$200,000.00","","wlu4@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269, 7454","068Z, 079Z","$0.00","Precision medicine seeks to tailor medical treatment to the individual characteristics of each patient to achieve the goal of better patient outcomes. As a broader conceptualization that includes precision medicine, precision health involves approaches that everyone can do on their own to protect their health as well as steps that public health can take. Reinforcement Learning (RL) is a powerful technique that allows an agent to learn and take actions in a given environment in order to maximize the cumulative reward that the agent receives. The interest in developing new statistical RL methods for precision health is emerging. The potential impacts of this work can be summarized in the following four goals. First, the project contributes to both the fields of semiparametric inference and RL. The theoretical results include non-asymptotic distribution, risk bounds with novel empirical process technical tools. These results will be fundamentally important and generally applicable for studying semiparametric inference empowered by RL. Second, the clinical findings based on analyzing the electronic medical record (EMR) data will lead to major progress in addressing important clinical questions on the treatment recommendations for patients.  Third, although EMR and mobile health (mHealth) data are the main applications of this project, the developed methods are general enough to apply to a variety of data sources including clinical data and economic data. The developed methods are expected to greatly enhance the acquisition and analysis of large-scale data with population heterogeneity for medical, scientific and engineering communities. Fourth, the integration of research and education is a key aspect of this project. The PI will develop new courses and improve existing courses on RL and semiparametric inference, will train graduate students, and will reach out to the K-12 education levels by training high school teachers and students.<br/><br/>Despite the tremendous impacts that RL has achieved in areas such as games and robots, a direct deployment of RL algorithms in precision health can be costly, risky or even infeasible, due to significant real-world challenges. The main objective of this proposal is to develop new statistical offline RL methods to handle real-world challenges by developing flexible and efficient off-policy learning and robust and efficient off-policy evaluation methods. The off-policy learning in RL refers to the problem of finding the best target policy that maximizes the value, given samples collected from a possibly different policy. In Aim 1, we will develop an efficient advantage learning framework in order to efficiently use pre-collected data for policy optimization. In Aim 2, we consider the problem of off-policy evaluation where the objective is to learn the value under a target policy with data collected under a possibly different policy. There is a growing literature on estimating the value under a given policy in off-policy settings. However, very limited work have been considered regarding statistical inference such as hypothesis testing and confidence intervals (CIs) of the value, which is the focus of this aim. In Aim 3, we will discuss the plan of analyzing EMR and mHealth data with policy learning and policy evaluation based on the proposed methods in Aims 1 and 2.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2052918","FRG: Collaborative Research: Flexible Network Inference","DMS","STATISTICS","07/01/2021","06/30/2021","Elizaveta Levina","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yulia Gel","06/30/2024","$270,000.00","","elevina@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","1616","$0.00","Scientists have been studying many natural systems by viewing them as networks, a term used to describe collections of entities and their interactions. Networks are ubiquitous in many fields, including epidemiology, economics, sociology, genomics and ecology, to name a few. A number of statistical models have emerged over the last two decades to describe network data. One common type of model is the latent space model, where each entity?s behavior is governed by its position in some unobserved (latent) space, and if we knew these positions, we could fully describe the statistical behavior of the network. While these models have been useful in many problems, real systems tend to be more complicated. The goal of this project is to fill this gap between models and reality by developing network analysis methods that still perform well even when the model does not fully match the data.   <br/><br/>The specific aims of this project are to improve our understanding of existing network analysis methods under model misspecification, heterogeneous noise, and incomplete or missing data, and to develop novel network methods that are robust to these sources of error.  We consider both these problems under one unified framework that represents the adjacency matrix of a network as its expectation plus entry-wise noise, which encompasses most popular network models.  Within this framework, the project will examine the effects of model misspecification on downstream inference, both for global inference tasks (e.g., network-level summary statistics) and local inference (e.g., node-level statistics). One core goal of the project is developing bootstrap and resampling algorithms for networks, two extremely useful tools in classical statistics that do not yet have full network analogues. Another core goal is developing more general notions of community membership and node similarity, allowing the extension of robust algorithms to a broader collection of network models. Finally, the methods developed will be extended to the analysis of multiple networks. Taken together, these tools will substantially expand the toolbox of network techniques, while accounting for the realities of noisy and incomplete network data and imperfect network models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113397","DMS-EPSRC Collaborative Research: Advancing Statistical Foundations and Frontiers for and from Emerging Astronomical Data Challenges","DMS","STATISTICS, ","07/01/2021","06/30/2021","Yang Chen","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yulia Gel","06/30/2024","$160,000.00","","ychenang@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269, 1798","1206","$0.00","Statistical theory and methods play a fundamental role in scientific discovery and advancement, including in modern astronomy, where data are collected on increasingly massive scales and with more varieties and complexity. New technology and instrumentation are spawning a diverse array of emerging data types and data analytic challenges, which in turn require and inspire ever more innovative statistical methods and theories. This research is guided by the dual aims of advancing statistical foundations and frontiers, motivated by astronomical problems and providing principled data analytic solutions to challenges in astronomy. The CHASC (California-Harvard Astrostatistics Collaboration) International Center has an extensive track record in accomplishing both tasks. This research leverages CHASC?s track record to make progress in several new projects. Fitting sophisticated astrophysical models to complex data that were collected with high-tech instruments, for example, often involves a sequence of statistical analyses. Several projects center on developing new statistical methods that properly account for errors and carry uncertainty forward within such sequences of analyses. Additional work will focus on developing theoretical properties of novel statistical estimation procedures to address data-analytic challenges associated with solar flares and X-ray observations. Other projects involve fast and automatic detection of astronomical objects such as galaxies from 2D or even 4D data. The PIs will develop statistical theory and methods in the context of these projects, building statistical foundations and pushing the frontiers of statistics forward for broad impact that will extend well beyond astrostatistics. The PIs plan to offer effective methods and algorithms for tackling emerging challenges in astronomy, with the aspiration of promoting such principled data-analytic methods among researchers in astronomy. Its provision of free software via the CHASC GitHub Software Library will enable the distribution and impact of the proposed methods and algorithms. <br/><br/>The projects reflect three broad themes: (1) Exploring fundamental statistical theory with immediate impact in astronomy, including a general approach for obtaining confidence regions by leveraging the pivot-property of maximal product spacing, which is then applied to assess the power law of solar flares, and a statistically principled correction to the use of the popular C-stat in astrophysics; (2) Assessing the misspecification of models and prior distributions in multi-stage statistical analyses, and post processing posterior draws to correct for defects in prior modeling when redoing a Bayesian analysis is impractical; and (3) Identifying breakpoints in complex models, which includes a fast algorithm for identifying astronomical boundaries and identifying breakpoints in joint spatial, spectral, temporal models. Theme 1 is more theory driven, while Themes 2 and 3 are more methods and computation driven. Together they form a rich suite of case studies for developing statistical methods for astronomical problems, ranging from new theoretical foundations to innovative modeling strategies and to efficient computational techniques.  Consequently,  the research will impact both the fields of statistics and astronomy: spurring more interest and new problems for statisticians and resolving long standing problems in astronomy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113674","Sparse and Efficient Estimation with Semiparametric Models in Meta-Analysis","DMS","STATISTICS","08/01/2021","07/31/2023","Sunyoung Shin","TX","University of Texas at Dallas","Continuing Grant","Yong Zeng","07/31/2024","$149,482.00","","sunyoung.shin@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1269","","$0.00","In many scientific fields, such as genomics, epidemiology, and economics, combining large-scale datasets of multiple studies is a valuable approach to fully utilizing the collected data. However, such studies often have privacy policies that prevent individual-level data sharing. In biomedical research, for example, while data integration can boost the power of evaluating risk factors of a disease, study protocols typically prohibit sharing participant-level genomic and clinical data among the studies. This project will investigate meta-analysis that combines studies using compressed information in summary statistics without requiring individual-level data. The PI plans to establish a broad framework with semiparametric regression models and to develop concrete and computationally efficient methods with theoretical guarantees. Both undergraduate and graduate students will receive training through involvement in the research project.<br/><br/>The PI will study the general likelihood theory for meta-analysis with semiparametric regression. The theoretical framework to be established will embrace meta-analysis of studies with different observation schemes that generate various data types. The project will deal with both homogeneous and heterogeneous structures of meta-analysis. The PI will develop semiparametric methods based on summary statistics with the aim of efficient estimation and sparse structure recovery. In developing the methods, the research will focus on using and extending techniques such as least-squares approximation and regularization. Statistical properties and optimization algorithms of these methods will be studied under both structure types of meta-analysis. The resulting methods will be applicable to various studies such as large-scale public health studies, prognostic signature studies, and genome-wide association studies.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113589","Collaborative Research: New Regression Models and Methods for Studying Multiple Categorical  Responses","DMS","STATISTICS","09/01/2021","07/31/2023","Aaron Molstad","FL","University of Florida","Continuing Grant","Yong Zeng","08/31/2024","$150,000.00","","amolstad@ufl.edu","1523 UNION RD RM 207","GAINESVILLE","FL","326111941","3523923516","MPS","1269","079Z","$0.00","In many areas of scientific study including bioengineering, epidemiology, genomics, and neuroscience, an important task is to model the relationship between multiple categorical outcomes and a large number of predictors. In cancer research, for example, it is crucial to model whether a patient has cancer of subtype A, B, or C and high or low mortality risk given the expression of thousands of genes. However, existing statistical methods either cannot be applied, fail to capture the complex relationships between the response variables, or lead to models that are difficult to interpret and thus, yield little scientific insight. The PIs address this deficiency by developing multiple new statistical methods. For each new method, the PIs will provide theoretical justifications and fast computational algorithms. Along with graduate and undergraduate students, the PIs will also create publicly available software that will enable applications across both academia and industry.<br/><br/>This project aims to address a fundamental problem in multivariate categorical data analysis: how to parsimoniously model the joint probability mass function of many categorical random variables given a common set of high-dimensional predictors. The PIs will tackle this problem by using emerging technologies on tensor decompositions, dimension reduction, and both convex and non-convex optimization. The project focuses on three research directions: (1) a latent variable approach for the low-rank decomposition of a conditional probability tensor; (2) a new overlapping convex penalty for intrinsic dimension reduction in a multivariate generalized linear regression framework; and (3) a direct non-convex optimization-based approach for low-rank tensor regression utilizing explicit rank constraints on the Tucker tensor decomposition. Unlike the approach of regressing each (univariate) categorical response on the predictors separately, the new models and methods will allow practitioners to characterize the complex and often interesting dependencies between the responses.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113342","Reusing Data Efficiently for Iterative and Integrative Inference","DMS","STATISTICS","08/01/2021","07/25/2023","Snigdha Panigrahi","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Yulia Gel","07/31/2024","$150,000.00","","psnigdha@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","075Z, 079Z","$0.00","Drawing knowledge and reproducible results from complex data drives a broad range of scientific disciplines. From a statistical viewpoint, model selection and inference are the two fundamental tasks, the latter often pursued only after models are chosen through data-driven procedures. Naively using the same data for both tasks creates complicated correlations between the selected models and their inferential properties, which inevitably affects the reproducibility of findings from these models. The investigator develops methods for reusing data from selection to compensate for these correlations while not squandering away information from the full data. Finding immediate use in biomedical problems, observational studies in the behavioral sciences, and engineering applications, the methods will aid discoveries even when analyses rely on scarce samples. This research has a broader outreach component in creating opportunities for interdisciplinary engagement, training statisticians, and contributing to a new graduate curriculum.<br/><br/>The project is geared towards efficient and reproducible inference through a reuse of data from the model selection steps.  Combining ideas from convex optimization, probability theory, and statistical learning, the project seeks solutions for two main thrusts. In the first thrust, the investigator develops methods to integrate fresh samples available at a later point in time with information from selection. This workflow is realized in modern applications such as online streaming of data, which demand iterative inference on the fly. In the second thrust, the investigator explores integrative inference by combining selected models from different batches or splits or sources of data. Aggregating inference from multiple sources through a reuse of samples will have the potential for new discoveries that any single dataset may fail to report due to a lack of power.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113662","Network Time Series: From Dynamics to Coevolution","DMS","STATISTICS","09/01/2021","08/20/2021","Vladas Pipiras","NC","University of North Carolina at Chapel Hill","Standard Grant","Yulia Gel","08/31/2024","$219,990.00","Sreekalyani Bhamidi","pipiras@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","079Z","$0.00","The last few years have seen a large increase both in the amount of data on real-world networks in numerous research areas and the impact in people?s daily lives. One increasingly important field is in an area called network time series. Some examples include networks that evolve over time (dynamic or temporal networks), or time series over nodes in a network whose dynamics is intricately tied to the underlying network structure; and time series over dynamic networks where the two structures coevolve. Applications specific to this project include social networks with social connections changing owing to social dynamics, vertex specific streams such as text influenced by other vertices, neuroscience with brain functional connectivity networks from fMRI signals and brain structural connectivity networks or sociology and urban planning with migration and economic flows over spatial networks. Despite concerted activity over the last decade, rigorous understanding of network time series models and their applicability in various domains is still challenging owing to the complex emergence of macroscopic structure through microscopic interaction rules between individual network components. The aim of this project is to develop general theoretical foundations for network time series to inform the application of statistical methodology as well as computational techniques in practice whilst being guided by PIs? collaborations with domain scientists in the areas mentioned above. Additionally, the project will contribute to the training of students with an envisioned data science lab, populated in part by projects from this work providing vertical integration of research experiences.  <br/><br/><br/>There are three major pillars to this project, arranged sequentially in order of complexity. (1) Network modulated time series. The focus is on multivariate nodal time series with an underlying, possibly latent static network structure. Motivated by recent work on network vector autoregressions, network factor and propagation of chaos models are studied as superior alternatives and extensions. Special cases of the models include opinion dynamics in social networks and network versions of Hodgkin-Huxley and FitzHugh-Nagumo models in neuroscience. Motivated by applications in urban planning, spatial versions of these models will be studied through large network asymptotics. (2) Dynamic networks driven by possibly latent multivariate time series.  The PIs will work on their systematic analysis leveraging general time series methods and approaches, especially for discrete-valued time series. Temporal migration and economic flow networks form one targeted area of applications. (3) Co-evolving networks. The PIs study Network models in the scenario where multivariate time series are affected by the underlying network, which itself is affected by the multivariate time series. The PIs will develop mathematical techniques to understand various salient phenomena including the role of self-excitation (the greater the number of times a node interacts with a neighbor the higher the influence this neighbor has in the future including the creation of new connections) and information decay.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113568","New Frontiers of Robust Statistics in the Era of Big Data","DMS","STATISTICS","07/01/2021","06/11/2021","Zhao Ren","PA","University of Pittsburgh","Standard Grant","Yulia Gel","06/30/2024","$235,948.00","","zren@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","","$0.00","Modern technologies have facilitated the collection of an unprecedented amount of features with complex structures. Although extensive progress has been made towards extracting useful information from massive data, the statistical analysis typically assumed that data are drawn without any contamination. However, in reality the data sets arising in applications such as genomics and medical imaging are usually more inhomogeneous due to either data collection process or the intrinsic nature of the data in the era of big data. For instance, in gene expression data analysis, outliers frequently arise in microarray experiments due to the array chip artifacts such as uneven spray of reagents within arrays. Compared to the recent advances in the era of big data, research in modeling and theoretical foundations for robust procedures under contamination models has fallen behind. To bridge this gap, this project seeks to develop new robust estimation and inference procedures which are rate-optimal for various contamination models as building blocks to address the modeling, theory and computational challenges.  Upon completion, this work will lead to a comprehensive understanding of contamination models and have an immediate impact on various disciplines such as biology, genomics, astronomy and finance. The project also provides training opportunities for undergraduate and graduate students, and is used to enrich courses and outreach educational materials in statistics and data science.<br/><br/>This project aims to address some of the most pressing challenges that are faced by robust procedures in high-dimensional and nonparametric contamination models. Specifically, (I) the research begins with statistical inference of low-dimensional parameters in both increasing-dimensional and high-dimensional regressions under contamination models. The PI will study the influence of contamination proportion in obtaining the root-n consistency results. Robust large-scale simultaneous inference under contamination models are also considered. (II) Next, the PI will revisit some classical nonparametric density estimation problems both under arbitrary and structured contamination distributions. The PI plans to propose rate-optimal procedures and carefully study the effect of contamination on estimation through various model indices, including contamination proportion, the structure of contamination and the choice of loss function. (III) The PI will develop a U-type robust covariance estimator under structured contamination models and provide rigorous theoretical guarantees on its rate optimality. This general robust estimator can serve as building blocks for establishing many rate-optimal procedures for structured large covariance/precision matrix estimation problems. User-friendly R packages will be developed to implement the proposed methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113557","Collaborative Research: Covariate-Driven Approaches to Network Estimation","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","08/15/2021","08/12/2021","Christine Peterson","TX","University of Texas, M.D. Anderson Cancer Center","Standard Grant","Yulia Gel","07/31/2024","$120,000.00","","cbpeterson@mdanderson.org","1515 HOLCOMBE BLVD","HOUSTON","TX","770304000","7137923220","MPS","1269, 7454","068Z, 079Z","$0.00","In this research project, the PIs will develop new statistical methods for estimating networks from scientific data. In statistical network inference, each node in the network corresponds to a variable, and each edge represents a dependence relation. The project will address challenging scenarios where a set of external covariates may influence either the values of the nodes within the network or the strength of the connections. The PIs will develop new Bayesian modeling approaches for learning both directed and undirected networks and their dependence on covariates, including methods that can handle data that are not normally distributed, and implement the proposed methods using efficient computational algorithms. The PIs will apply the developed statistical models to high-dimensional data, including functional brain imaging and microbiome profiling. This work is significant, as it will break new ground in Bayesian modeling and computation. The broader impacts of this project include the public sharing of software, training of graduate students, and the application of the methods to real-world neuroimaging and microbiome data.<br/><br/>This project will break new ground in the simultaneous estimation of graphical models and covariate effects. The PIs will develop a framework to infer directed graphs based on vector autoregressive models for time series data and will develop a novel formulation where covariates may influence the strength of an edge in a non-linear fashion. This framework will allow the determination of how key covariates modulate network relations. The PIs will also develop Bayesian methods for the simultaneous selection of covariates and edges in an undirected graph, focusing on models for non-Gaussian data. They will implement these models using efficient Variational Inference approaches, enabling scalability to real-world applications. This project achieves innovation both in terms of the Bayesian modeling approaches and the computational methods employed to enable efficient inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2052949","FRG: Collaborative Research: Dynamic Tensors: Statistical Methods, Theory, and Applications","DMS","STATISTICS","09/01/2021","08/17/2021","Cun-Hui Zhang","NJ","Rutgers University New Brunswick","Standard Grant","Yulia Gel","08/31/2024","$600,000.00","Rong Chen, Han Xiao","czhang@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","075Z, 079Z, 1616","$0.00","Dynamic tensor data, represented by multidimensional arrays that vary in time, has become increasingly important to society at large. It is collected in a wide range of applications, from biology and medical research, natural sciences, and engineering to social sciences, economics, and finance. This research aims to develop novel statistical theory, methods, and algorithms for analyzing large dynamic tensor data. The work also includes analysis of the computational efficiency and utility of the methods under development. The results will provide state-of-art statistical tools for effectively extracting useful information from such data and aiding practical decision making in a wide spectrum of applications. The project will apply the new methods to important examples, including motion behavior modeling and crime data analysis. The project will foster collaborations among students and young researchers through involvement in cutting-edge research. Software and other tools will be made publicly available, enhancing scientific progress and data driven decision-making processes in practical applications.<br/><br/>The objectives of the research are to develop statistical theory, methods, and algorithms for analyzing large dynamic tensor data and to demonstrate their feasibility, effectiveness, and utility in interesting applications. Dynamic tensor data, an area with opportunities for systematic methodological and theoretical treatment from a statistical point of view, is creating new challenges and opportunities for researchers. The project will develop autoregressive and dynamic factor models for continuous tensor time series data, and generalized dynamic tensor models for binary, count, and other non-Gaussian data; produce new tools for forecasting, parameter estimation, and statistical inferences for such models; and study the theoretical and empirical properties of the new methods. The project findings are expected to have impact in other fields of statistics, including discrete tensor analysis, video analysis, inference of high-dimensional tensors, and analysis of high dimensional dynamic systems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113602","Collaborative Research: Covariate-Driven Approaches to Network Estimation","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","08/15/2021","08/12/2021","Marina Vannucci","TX","William Marsh Rice University","Standard Grant","Yulia Gel","07/31/2024","$149,994.00","","marina@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269, 7454","068Z, 079Z","$0.00","In this research project, the PIs will develop new statistical methods for estimating networks from scientific data. In statistical network inference, each node in the network corresponds to a variable, and each edge represents a dependence relation. The project will address challenging scenarios where a set of external covariates may influence either the values of the nodes within the network or the strength of the connections. The PIs will develop new Bayesian modeling approaches for learning both directed and undirected networks and their dependence on covariates, including methods that can handle data that are not normally distributed, and implement the proposed methods using efficient computational algorithms. The PIs will apply the developed statistical models to high-dimensional data, including functional brain imaging and microbiome profiling. This work is significant, as it will break new ground in Bayesian modeling and computation. The broader impacts of this project include the public sharing of software, training of graduate students, and the application of the methods to real-world neuroimaging and microbiome data.<br/><br/>This project will break new ground in the simultaneous estimation of graphical models and covariate effects. The PIs will develop a framework to infer directed graphs based on vector autoregressive models for time series data and will develop a novel formulation where covariates may influence the strength of an edge in a non-linear fashion. This framework will allow the determination of how key covariates modulate network relations. The PIs will also develop Bayesian methods for the simultaneous selection of covariates and edges in an undirected graph, focusing on models for non-Gaussian data. They will implement these models using efficient Variational Inference approaches, enabling scalability to real-world applications. This project achieves innovation both in terms of the Bayesian modeling approaches and the computational methods employed to enable efficient inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2201136","CAREER: Stable and Scalable Estimation of the Intrinsic Geometry of Multiway Data","DMS","STATISTICS, Division Co-Funding: CAREER","10/01/2021","06/27/2022","Eric Chi","TX","William Marsh Rice University","Continuing Grant","Yulia Gel","06/30/2024","$185,456.00","","echi@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269, 8048","1045","$0.00","Two important challenges of the proposed work are in high-throughput bioinformatics studies and neuroscience studies, which generate data typical of research aligned with two recent White House Initiatives, namely the Precision Medicine and BRAIN Initiatives respectively. Both initiatives are investing heavily in generating high-resolution data, which once analyzed properly, can yield insights that will pave the foundations of advanced treatments for genetic and nervous system disorders. To fully maximize the potential impact of collecting such data, this proposal develops a new framework for identifying complicated underlying patterns in multiway arrays. Specifically, the project fills a gap in nonparametric estimation of low-dimensional structure and geometry in big and noisy data arrays. The PI plans to develop a training program based on applications of the proposed research to recruit and retain talented high school, undergraduate, and graduate students from underrepresented minority (URM) groups for potential careers as innovative data scientists. <br/><br/>Modern data matrices present two challenges to their analyses: (1) they are transposable in the sense that both their rows and columns are often of interest and may contain non-trivial dependencies among them, and (2) they may be very large. The first challenge has only partially been addressed by existing biclustering or co-clustering methods. These methods can identify only very simple coupled structures that organize the rows and columns. In order to flexibly model and extract more complicated patterns in large data matrices, in which both rows and columns are high-dimensional, one requires a new co-manifold learning framework that can discover a wider range of intrinsic geometries of the rows and columns. To meet the first challenge, this project develops a framework for performing joint nonlinear dimension reduction on the rows and columns.  The proposed methods construct multiscale distances that are invariant to row and column permutations, equipping practitioners with a means to estimate the intrinsic organization of the rows and columns of a data matrix without prior information on any row or column orderings. To meet the second challenge, this project formulates the key computations as optimization problems that admit distributed parallel algorithms with nearly linear speed-up. The framework also generalizes naturally to the higher-order generalization of matrices, multiway arrays or tensors. Finally, the estimated intrinsic geometries possess stability guarantees, namely small perturbations in the data due to noise or adjustments to input parameters cannot lead to disproportionately large variations in the estimated intrinsic geometry. The proposed procedures have the potential to enable practitioners to extract complicated patterns stably from massive data tensors with non-trivial dependencies along their modes or axes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2052632","FRG: Collaborative Research: Flexible Network Inference","DMS","STATISTICS","07/01/2021","06/30/2021","Keith Levin","WI","University of Wisconsin-Madison","Standard Grant","Yulia Gel","06/30/2024","$200,000.00","","kdlevin@wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","1616","$0.00","Scientists have been studying many natural systems by viewing them as networks, a term used to describe collections of entities and their interactions. Networks are ubiquitous in many fields, including epidemiology, economics, sociology, genomics and ecology, to name a few. A number of statistical models have emerged over the last two decades to describe network data. One common type of model is the latent space model, where each entity?s behavior is governed by its position in some unobserved (latent) space, and if we knew these positions, we could fully describe the statistical behavior of the network. While these models have been useful in many problems, real systems tend to be more complicated. The goal of this project is to fill this gap between models and reality by developing network analysis methods that still perform well even when the model does not fully match the data.   <br/><br/>The specific aims of this project are to improve our understanding of existing network analysis methods under model misspecification, heterogeneous noise, and incomplete or missing data, and to develop novel network methods that are robust to these sources of error.  We consider both these problems under one unified framework that represents the adjacency matrix of a network as its expectation plus entry-wise noise, which encompasses most popular network models.  Within this framework, the project will examine the effects of model misspecification on downstream inference, both for global inference tasks (e.g., network-level summary statistics) and local inference (e.g., node-level statistics). One core goal of the project is developing bootstrap and resampling algorithms for networks, two extremely useful tools in classical statistics that do not yet have full network analogues. Another core goal is developing more general notions of community membership and node similarity, allowing the extension of robust algorithms to a broader collection of network models. Finally, the methods developed will be extended to the analysis of multiple networks. Taken together, these tools will substantially expand the toolbox of network techniques, while accounting for the realities of noisy and incomplete network data and imperfect network models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113389","Flexible Statistical Modeling","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","09/01/2021","08/20/2021","Robert Tibshirani","CA","Stanford University","Standard Grant","Yulia Gel","08/31/2025","$500,000.00","","tibs@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269, 7454","068Z, 079Z","$0.00","Statistical learning techniques have made significant progress in the past 15-20 years. Some representative areas include neural networks, applied regression, classification, and clustering. As a result of these developments, a powerful collection of adaptive regression and classification techniques are now available and can be applied to a wide range of important science and engineering areas. Some typical applications include medical diagnosis, bioinformatics, chemical process control, and face recognition. The focus of this project is on high-dimensional statistics and data science. This work will help scientists working in biotechnology and other areas to interpret and uncover important patterns in large-scale data sets. This research will also help scientists and doctors discover the biological bases of many diseases, and improve prognosis and treatment selection for patients. The project will provide research training opportunities for graduate students. <br/><br/>This project includes four main thrusts in the area of supervised learning. In the first thrust, the investigator will build feature-efficient, or lean, models that depend on only a small number of unique features. The investigator will develop COVID-19 case forecasting through customized training and collaboration with a team in the second thrust. In the third thrust, the investigator will develop a model-free approach to the challenge of local feature importance via building a convex region around a point for prediction. The fourth thrust focuses on improving the large-scale computation of l1- regularized models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113671","Inferring the Past on Markovian Models of Networks","DMS","STATISTICS","07/01/2021","07/01/2022","Min Xu","NJ","Rutgers University New Brunswick","Continuing Grant","Yulia Gel","06/30/2024","$199,999.00","","mx76@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","079Z","$0.00","A major challenge in statistics and data science is the analysis of network data. Network data describe interactions and relationships between individual entities. The most prominent example is social network data, but other important examples include internet hyperlink networks, protein interaction networks, air route networks between cities, and disease transmission networks between people. These interaction networks generally start with a few individuals and, as time goes on, they attract, infect, or recruit more members and create more interactions. The goal of this project is to develop probabilistic models that accurately describe the growth process of real-world networks and to use these models to extract important information from large scale network data. Algorithms and software packages will be developed that enable users to answer questions such as, which individuals were the earliest members of a social network, or does the network contain one growing community or multiple? The results of this project will have applications in public health, social science, computer science, and national security. The project also provides research training opportunities for graduate students. <br/> <br/>The framework developed by the PI models a random network as a combination of a preferential attachment (PA) tree and Erdos-Renyi (ER) random edges. The PA tree describes the growth process of a network and may be regarded as the signal and the ER random edges can be interpreted as the noise. This framework includes many existing network models as special cases and allows practitioners to trade-off model complexity and computational complexity. Scalable methodology based on Gibbs sampling will be developed to tackle inference problems such as constructing confidence sets for the root nodes or inferring the community membership of the nodes of a network. Theoretical analysis, based on existing probabilistic properties of preferential attachment models, will also be conducted to assess the quality of statistical inference as a function of the signal-to-noise ratio and to understand the information limits of these problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2112887","Large Sample Analysis of Markov Chain Monte Carlo Methods in Bayesian Statistics From a Frequentist Perspective","DMS","STATISTICS","07/15/2021","06/12/2023","Qian Qin","MN","University of Minnesota-Twin Cities","Continuing Grant","Yulia Gel","06/30/2024","$199,354.00","","qqin@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","This project concerns Markov Chain Monte Carlo (MCMC), which is a class of computer algorithms that are widely used to simulate complicated probability distributions. In the field of statistics, probability distributions are often used to model uncertainty about unknown parameters that one wishes to estimate, for example, the average household income of a large nation, or the difference in average life expectancy between two demographic groups. The estimation procedure is based on a data set, which is usually a sample collected from an underlying population through a survey or experiment. MCMC is widely regarded as an extremely powerful tool for high-quality estimation, but not enough is known about its reliability when the sample is massive. This project aims to study the mathematical properties of various MCMC algorithms in large sample settings. Results of the research are expected to advance understanding in the performance of MCMC algorithms that are applied to modern data sets in fields such as economics, biology, and astronomy. Undergraduate and graduate students involved in the research efforts of this project will receive training in probability theory as well as mathematical and applied statistics.<br/><br/>More specifically, this project focuses on MCMC algorithms that are used to explore posterior distributions in Bayesian statistical models. From a frequentist perspective, the data set associated with a Bayesian model is assumed to be generated from an underlying distribution. The Markov transition kernel of an MCMC algorithm can thus be regarded as a statistic, i.e., observable random element, no different from a classical vector-valued statistic, e.g., a sample mean. Just like with any statistic, it is important to understand the asymptotic behavior of an MCMC transition kernel when the sample size of the data set grows. This project aims to develop general theories for the problem using techniques from classical large sample theory in conjunction with those from Markov chain theory. Special attention will be given to data augmentation algorithms, which are a broad class of practically relevant MCMC algorithms that exhibit rich and meaningful large sample properties. The project will also involve studying the mixing times of MCMC algorithms in the large sample regime, which is crucial to the effectiveness of MCMC-based inference in practice.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2112918","Overparameterization, Global Convergence of the Expectation-Maximization Algorithm, and Beyond","DMS","STATISTICS","07/01/2021","06/14/2021","Huibin Zhou","CT","Yale University","Standard Grant","Yulia Gel","06/30/2024","$370,000.00","","huibin.zhou@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","075Z, 079Z","$0.00","The expectation-maximization (EM) algorithm is among the most popular algorithms for statistical inference. Despite a wide range of successful applications in both statistics and machine learning, there is little finite-sample theoretical analysis explaining the effectiveness of EM and its variants. Recently, there have been some encouraging successes on the global convergence guarantee of the EM algorithm, but often under unrealistic and impractical assumptions.  The PI will integrate the recent success of overparametrization in deep learning with EM to overcome the aforementioned limitations. The research presented in this project will significantly advance the celebrated algorithms in statistics and machine learning including EM, mean-field variational inference, and Gibbs sampling by providing guarantees of global convergence and statistical optimalities.  The research will help address the non-convex optimization challenges for a range of important and classical statistical models and shed light on the recent successes of deep learning. The wide range of applications of EM, mean-field variational inference, and Gibbs sampling and the importance of clustering ensure that the progress we make towards our objectives will have a great impact on the broad scientific community which includes neuroscience and medicine. Research results from this project will be disseminated through research articles, workshops, and seminar series to researchers in other disciplines. The project will integrate research and education by teaching monograph courses and organizing workshops and seminars to support graduate students and postdocs, particularly women, underrepresented minorities, domestic students, and young researchers, to work on this topic.<br/><br/>The PI will develop methods for obtaining global convergence under possibly the weakest assumptions for a general class of latent variable models? estimation with an unknown number of clusters. The PI will address the following questions: 1) can we show that the overparameterized EM converges globally to the true parameters without any separation condition and any knowledge of the number of clusters and cluster sizes under a certain distance (such as Wasserstein)? 2) how fast does the algorithm converge? 3) what are the parameter estimation and clustering error rates and how do they compare to the optimal statistical accuracy? and 4) if not optimal statistically, can we achieve the optimality by adding a second stage EM initialized by the output of the overparameterized EM? There are three aims to develop a comprehensive theory to analyze the overparameterized EM and go beyond: 1) studying the global convergence of overparameterized EM for Gaussian Mixtures for both parameter estimation and latent cluster recovery and statistical optimality of the two-stage EM, 2) extending the two-stage EM to its variants including two-stage mean-field variational inference and Gibbs sampling and considering a unified analysis for a class of overparameterized algorithms, and 3) extending the analysis for Gaussian mixtures to general location mixture models and Stochastic Block Models and possibly a unified framework of latent variable models. In addition, the PI will work closely with the Yale Child Study Center and Yale Therapeutic Radiology Department to explore the appropriate EM algorithm and its variants for neuroscience, autism spectrum disorder, and cancer risk stratification.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2136034","CAREER:   Big Computation and the Management of Emerging Infectious Diseases","DMS","STATISTICS, Division Co-Funding: CAREER","01/01/2021","05/02/2022","Eric Laber","NC","Duke University","Continuing Grant","Yulia Gel","11/30/2022","$142,524.00","","eric.laber@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269, 8048","1045","$0.00","Emerging infectious diseases (EIDs) account for more than 25% of global disease burden and more than 32% of global deaths. Current EIDs like Middle East Respiratory Syndrome Coronavirus (MERS) and antibiotic-resistant superbugs have the potential to make devastating impacts on public health. The methodologies under development in this project can be used to translate real-time data on EIDs into recommendations about where, when, and to whom to apply interventions so as to minimize negative impacts of the disease while reducing overall resource consumption. Furthermore, these recommendations are designed to be immediately interpretable in a subject matter context, thereby empowering decision makers to incorporate information from complex and heterogeneous data streams into disease management. Application of these methodologies has the potential to reduce mortality and morbidity at lower cost than existing management plans. Furthermore, models underpinning intervention recommendations will generate new knowledge about EID dynamics. <br/><br/>This research project aims to make fundamental contributions to online sequential decision making and to create a new statistical framework for data-driven management of EIDs. We conceptualize the EID as spreading across a finite set of locations, which might be physical locations in space or nodes in a network. An allocation strategy formalizes management of an EID and is represented by a sequence of functions, one per intervention decision, that map up-to-date information on an EID to a subset of locations recommended for treatment. An optimal allocation strategy maximizes some mean utility function over the duration of the EID. Construction of an optimal allocation strategy from data on an EID is challenging because: (i) the number of allocations is exponential in the number of locations; (ii) estimation and management must occur simultaneously; (iii) spatial proximity induces causal interference; and (iv) an allocation strategy must be interpretable to subject matter experts. We integrate ideas from statistics, computer science, optimization, and disease ecology to overcome these challenges. We combine simulation-optimization with policy-search algorithms to construct an online estimator of the optimal allocation strategy; this strategy trades off exploring allocation choices that improve estimates of disease dynamics with exploiting current estimated dynamics to immediately slow spread of the EID. We show that the treatment allocation problem can be recast as an infinite-dimensional bandit problem. We leverage this connection to derive estimation algorithms that scale to very large allocation problems and are amenable to theoretical study. We combine our policy-search and bandit-based estimators with a novel class of allocation strategies that can be expressed as a sequence of if-then statements that are immediately interpretable to subject-matter experts and can be readily adjusted based on expert judgment. We derive a non-parametric lower bound on the approximation error of an estimated allocation strategy within this class; this bound is used to perform goodness-of-fit tests for the estimated optimal allocation strategy."
"2045068","CAREER: Robust and Efficient Algorithms for Statistical Estimation and Inference","DMS","STATISTICS","07/01/2021","06/20/2023","Stanislav Minsker","CA","University of Southern California","Continuing Grant","Yulia Gel","06/30/2026","$204,203.00","","minsker@usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","MPS","1269","079Z, 1045","$0.00","Statistical and machine learning methods are useful for making data-driven decisions. These methods are particularly advantageous in the presence of uncertainty originating from measurement errors or randomness inherent to the data collection process itself. The principal investigator (PI) will pursue a research program to develop statistical and machine learning methods possessing two important characteristics, robustness and efficiency. Robust algorithms are characterized by their ability to perform well even when some of the data are not accurate and clean but instead are ""outliers,"" such as completely irrelevant or grossly corrupted measurements. Robust techniques can help to reduce the amount of resources spent on human-supervised data cleaning and preprocessing. On the other hand, methods that are efficient are able to extract most of the useful information contained in the data, therefore reducing the amount of uncertainty in the data-guided decision. This research program will be integrated with educational activities that, among other things, will expose undergraduate and graduate students to cutting edge approaches in statistics and machine learning, and give students an opportunity to serve as individual tutors and mentors at local K-12 schools.<br/><br/>One part of the research program is devoted to investigation of the connections between self-normalized sums and robust statistical techniques. In particular, the PI will demonstrate that, unlike many existing approaches, algorithms based on the self-normalized sums often give rise to efficient methods, for instance in the context of univariate and multivariate mean estimation. Analysis of such algorithms is closely related to the theory self-normalized processes. Another part of the research focuses on the asymptotic properties of U-statistics of growing order, and the implications of these properties for robust and efficient empirical risk minimization (ERM), one of the key principles underlying modern mathematical statistics and machine learning algorithms. The PI will introduce a new approach to robust ERM and will relate questions about efficiency of resulting algorithms to purely mathematical questions in the theory of U-statistics. Finally, the research will address uncertainly quantification in robust statistics using Bayesian methods. Specifically, the PI aims to develop new robust analogues of the standard posterior distribution based on U-statistics of growing order, and will investigate the asymptotic behavior of these robust posteriors as well as the asymptotic frequentist properties of the corresponding credible sets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2203741","CAREER: Inference for High-Dimensional Structures via Subspace Learning: Statistics, Computation, and Beyond","DMS","STATISTICS","10/01/2021","06/13/2023","Anru Zhang","NC","Duke University","Continuing Grant","Yulia Gel","06/30/2025","$230,127.00","","anru.zhang@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","1045","$0.00","High-dimensional arrays commonly arise from modern scientific and technological research and have been a central topic in modern statistics and data science. Some areas such as genetics, microbiome studies, brain imaging, hyperspectral imaging, etc., yield a large amount of high-dimensional array data; while in some other areas, data can be recast into high-dimensional array form to facilitate analysis. In these situations, the target parameter is often high-dimensional/high-order, but the important information may lie in dimension-reduced subspaces induced by various structural conditions. How to efficiently exploit these subspaces poses significant statistical and computational challenges. This project aims to address these challenges from a perspective of subspace learning. By taking into account dimension-reduced and low-order subspaces, the PI aims to address a series of statistical and machine learning questions by developing new methodologies and theories with statistical and computational advantages. <br/><br/>This project will progress along three major directions: (i) fast estimation and inference for high-dimensional arrays via important subspace sketching; (ii) high-order clustering with theoretical guarantees; (iii) ultrahigh-order tensor singular value decomposition via a tensor-train parameterization. The research will be applicable to a variety of topics involving high-dimensional matrix and tensor data, such as genetics and genomes, reinforcement learning, neuroimaging analysis, material science, recommender design, etc. The PI will also develop user-friendly software packages for the new algorithms and make them available for public use. The PI is committed to training students, especially those from groups underrepresented in STEM, through involvement in the research project.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2052926","FRG: Collaborative Research: Flexible Network Inference","DMS","STATISTICS","07/01/2021","06/30/2021","Jianqing Fan","NJ","Princeton University","Standard Grant","Yulia Gel","06/30/2024","$230,000.00","","jqfan@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","1616","$0.00","Scientists have been studying many natural systems by viewing them as networks, a term used to describe collections of entities and their interactions. Networks are ubiquitous in many fields, including epidemiology, economics, sociology, genomics and ecology, to name a few. A number of statistical models have emerged over the last two decades to describe network data. One common type of model is the latent space model, where each entity?s behavior is governed by its position in some unobserved (latent) space, and if we knew these positions, we could fully describe the statistical behavior of the network. While these models have been useful in many problems, real systems tend to be more complicated. The goal of this project is to fill this gap between models and reality by developing network analysis methods that still perform well even when the model does not fully match the data.   <br/><br/>The specific aims of this project are to improve our understanding of existing network analysis methods under model misspecification, heterogeneous noise, and incomplete or missing data, and to develop novel network methods that are robust to these sources of error.  We consider both these problems under one unified framework that represents the adjacency matrix of a network as its expectation plus entry-wise noise, which encompasses most popular network models.  Within this framework, the project will examine the effects of model misspecification on downstream inference, both for global inference tasks (e.g., network-level summary statistics) and local inference (e.g., node-level statistics). One core goal of the project is developing bootstrap and resampling algorithms for networks, two extremely useful tools in classical statistics that do not yet have full network analogues. Another core goal is developing more general notions of community membership and node similarity, allowing the extension of robust algorithms to a broader collection of network models. Finally, the methods developed will be extended to the analysis of multiple networks. Taken together, these tools will substantially expand the toolbox of network techniques, while accounting for the realities of noisy and incomplete network data and imperfect network models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113724","Learning Algorithms for Inverse Problems from Data: Statistical and Computational Foundations","DMS","STATISTICS","07/01/2021","06/11/2021","Venkat Chandrasekaran","CA","California Institute of Technology","Standard Grant","Yulia Gel","06/30/2024","$200,000.00","","venkatc@caltech.edu","1200 E CALIFORNIA BLVD","PASADENA","CA","911250001","6263956219","MPS","1269","079Z","$0.00","Methods for the solution of inverse problems arising in domains such as image analysis, the geosciences, computational genomics, and many others are designed based on a detailed understanding by a human analyst of the structure underlying the problem.  This project aims to develop new data-driven approaches to learning solution methods for inverse problems and to develop the associated statistical foundations. Specifically, the project will provide a new approach to data-driven design of learning regularizers, which can be computed or optimized within a specified computational budget, and come with statistical guarantees. The research will engage both graduate and undergraduate students and will be disseminated to a broader audience through the development of new courses.<br/><br/>Regularization techniques are widely employed in the solution of model selection and statistical inverse problems because of their effectiveness in addressing difficulties due to ill-posedness, access to only a small number of observations, or the high dimensionality of the signal or model to be inferred. In their most common manifestation, these methods take the form of penalty functions added to the objective in optimization-based formulations. The design of the penalty function is based on prior domain-specific expertise about the particular model selection or inverse problem at hand, with a view to promoting a desired structure in the solution. This project will develop a framework for the construction of algorithms for inferential problems so as to address the following questions ? What if we do not know in advance the structure we seek in our solution due to a lack of detailed domain knowledge? Can we identify a suitable regularizer directly from data rather than human-provided expertise? What are the fundamental limitations in terms of sample complexity and the amount of computational resources required in such a framework? Statistically, how do we provide confidence bounds for point estimates that lie in a collection of regularizers?<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113754","Collaborative Research: Development of Classification Theory and Methods for Objective Asymmetry, Sample Size Limitation, Labeling Ambiguity, and Feature Importance","DMS","STATISTICS","07/01/2021","06/11/2021","Jingyi Jessica Li","CA","University of California-Los Angeles","Standard Grant","Yulia Gel","06/30/2024","$120,000.00","","jli@stat.ucla.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244201","3107940102","MPS","1269","075Z, 079Z","$0.00","Classification is a popular data analytical technique in disciplines ranging from biomedical sciences to information technologies. This project will develop theory-backed statistical methods and algorithms to address pressing challenges in the application of classification. These challenges are related to imperfect aspects of training data, which are widespread in high-stake applications such as disease diagnosis and cybersecurity. In particular, this project will focus on the so-called asymmetric classification problems where a particular class is of greater importance than other classes, and the methods and algorithms will aim to control the classification error of missing the most important class in the population, not just in a particular dataset. This property will make the methods and algorithms powerful for medical diagnosis, for which the primary goal is diagnosis accuracy in the population. Moreover, this project will provide a suite of projects, ranging from theory to applications, that are suitable for training graduate and undergraduate students. The interdisciplinary nature of this project is expected to attract students from diverse background to join the PIs? efforts.<br/><br/>The PIs will develop a suite of application-driven, theory-backed methods and algorithms to address pressing data challenges including sample size limitations, sampling biases, and ambiguous class labels. The development will be primarily under the Neyman-Pearson (NP) classification paradigm, which was designed to control the population-level false-negative rate (p-FNR) under a desired level while minimizing the population-level false-positive rate (p-FPR). This project will integrate the NP classification into cutting-edge statistical learning tasks and enable it to address the aforementioned real-world data challenges. Specifically, this project will include the following four overarching goals. First, the PIs will use random matrix theory to address a long-standing problem in the NP classification methodology: whether NP classifiers can be constructed without a sample-splitting step to improve data efficiency. Second, because the NP paradigm has an invariance property to sampling bias, the PIs will develop NP classifiers to address the sampling bias issue in biomedical applications. These classifiers can be trained on biased samples but still achieve the p-FNR control. Third, the PIs will develop a model-free feature ranking framework to incorporate multiple classification paradigms including the NP paradigm and to reflect prediction objectives. Fourth, the PIs will develop the first NP umbrella algorithm under the label noise setting and the first information-theoretic criteria that combine ambiguous classes in multi-class classification. To disseminate the project outcomes, the PIs will give research talks, organize conference sessions, share open-source software packages with tutorials, and reach out to practitioners of classification methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113615","DMS-EPSRC Collaborative Research: Advancing Statistical Foundations and Frontiers for and from Emerging Astronomical Data Challenges","DMS","STATISTICS","07/01/2021","06/30/2021","Xiao-Li Meng","MA","Harvard University","Standard Grant","Yulia Gel","06/30/2024","$240,000.00","","meng@stat.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269","","$0.00","Statistical theory and methods play a fundamental role in scientific discovery and advancement, including in modern astronomy, where data are collected on increasingly massive scales and with more varieties and complexity. New technology and instrumentation are spawning a diverse array of emerging data types and data analytic challenges, which in turn require and inspire ever more innovative statistical methods and theories. This research is guided by the dual aims of advancing statistical foundations and frontiers, motivated by astronomical problems and providing principled data analytic solutions to challenges in astronomy. The CHASC (California-Harvard Astrostatistics Collaboration) International Center has an extensive track record in accomplishing both tasks. This research leverages CHASC?s track record to make progress in several new projects. Fitting sophisticated astrophysical models to complex data that were collected with high-tech instruments, for example, often involves a sequence of statistical analyses. Several projects center on developing new statistical methods that properly account for errors and carry uncertainty forward within such sequences of analyses. Additional work will focus on developing theoretical properties of novel statistical estimation procedures to address data-analytic challenges associated with solar flares and X-ray observations. Other projects involve fast and automatic detection of astronomical objects such as galaxies from 2D or even 4D data. The PIs will develop statistical theory and methods in the context of these projects, building statistical foundations and pushing the frontiers of statistics forward for broad impact that will extend well beyond astrostatistics. The PIs plan to offer effective methods and algorithms for tackling emerging challenges in astronomy, with the aspiration of promoting such principled data-analytic methods among researchers in astronomy. Its provision of free software via the CHASC GitHub Software Library will enable the distribution and impact of the proposed methods and algorithms. <br/><br/>The projects reflect three broad themes: (1) Exploring fundamental statistical theory with immediate impact in astronomy, including a general approach for obtaining confidence regions by leveraging the pivot-property of maximal product spacing, which is then applied to assess the power law of solar flares, and a statistically principled correction to the use of the popular C-stat in astrophysics; (2) Assessing the misspecification of models and prior distributions in multi-stage statistical analyses, and post processing posterior draws to correct for defects in prior modeling when redoing a Bayesian analysis is impractical; and (3) Identifying breakpoints in complex models, which includes a fast algorithm for identifying astronomical boundaries and identifying breakpoints in joint spatial, spectral, temporal models. Theme 1 is more theory driven, while Themes 2 and 3 are more methods and computation driven. Together they form a rich suite of case studies for developing statistical methods for astronomical problems, ranging from new theoretical foundations to innovative modeling strategies and to efficient computational techniques.  Consequently,  the research will impact both the fields of statistics and astronomy: spurring more interest and new problems for statisticians and resolving long standing problems in astronomy.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2128589","Statistical Modelling of Multivariate Functional and Distributional Data","DMS","STATISTICS","06/01/2021","03/22/2021","Alexander Petersen","UT","Brigham Young University","Standard Grant","Yulia Gel","06/30/2022","$8,383.00","","petersen@stat.byu.edu","A-153 ASB","PROVO","UT","846021128","8014223360","MPS","1269","8091","$0.00","Modern recording devices are collecting data of greater complexity and, when these data are measured over space or time, also at ever-increasing resolution.  While providing more detailed information about the associated physical phenomena occurring around us, they also pose statistical challenges related to interpretable modeling and feasible computation for such data.  Data measured over space, time, or some other continuum, are fittingly termed functional data, and constitute an important subfield of modern statistics.  This project will develop important methodology for the analysis of two types of functional data.  The first set of projects aim at the estimation of dependency patterns between components of so-called multivariate functional data, where a common set of features is measured over time for each subject in a study, such as neuroimaging scans where signals are recorded over time at a variety of spatial locations.  Another important class of functional data are samples of distributions or histograms, which regularly arise in the analysis of demographic data as mortality distributions, for example, but also in other important fields such as neuroscience and finance.  This project outlines methods for dimension reduction and regression that respect the well-known positivity and area-under-the-curve constraints for distributions, yielding interpretable data summaries and model fits that provide the practitioner with a clearer understanding of the information contained in their data.<br/><br/>Both fMRI and EEG yield time-dependent signals at multiple brain locations, resulting in multivariate functional data.  Quantifying connectivity patterns to define brain networks, for example in order to identify normal and pathological characteristics, is an important neuroscientific problem that can be addressed using multivariate functional data techniques.  This project seeks to advance the use of functional graphical models to estimate underlying brain dependency networks, including improved computational efficiency compared to existing methods. These methods are equally applicable in other domains that produce data of similar structure, such as longitudinal medical studies, where a common set of measurements is recorded repeatedly over time.  Also considered in this proposal are methods for distributional data, which can be thought of as collections of curves or surfaces, each corresponding to a probability distribution.  For example, neuroimaging data naturally provide such distributional samples, as levels of myelination or signal correlations within brain regions are high-dimensional data that can be effectively summarized at the subject level by a histogram or distribution.  Given a sample of such distributional data, this project investigates statistical methods of interpretable dimension reduction and dependency of distributional response functions on relevant covariates through distributional regression.  A key tool is the Wasserstein metric for distributions, which has been widely successful in applied settings, but has not been utilized to its full extent in statistics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2131233","Development of  a General Framework for Nonlinear Prediction Using Auto-Cumulants: Theory, Methodology, and Computation","DMS","STATISTICS","04/15/2021","04/21/2021","Soumendra Lahiri","MO","Washington University","Continuing Grant","Yulia Gel","07/31/2022","$127,461.00","","s.lahiri@wustl.edu","ONE BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","1269","","$0.00","Data exhibiting nonlinear characteristics appear routinely in many areas of applications, such as weather forecasting, signal processing, etc. These features are also present in many economic and demographic time series collected by various national agencies for policy formulations that have important implications for the public and the society. However, the current methodology is heavily reliant upon linear approaches and some ad hoc methods are often used to handle nonlinear data, rendering the final results of analysis difficult to interpret. As a result, there is acute need for systematic development of new theoretical and methodological framework for improved prediction that takes into account the nonlinear features of the time series data. The proposed research seeks to address this need directly by developing new capabilities that will build on the existing linear theory for Gaussian and provide substantially improved prediction. In addition to advancing the statistical science and related scientific applications, it will also have potential impact on the practice of seasonal adjustments for better public policy formulation in the US and other nations.<br/><br/><br/>This project seeks to develop new theory and methodology for prediction for non-Gaussian, nonlinear processes, utilizing the tools of higher-order auto-cumulant functions and polyspectra. Specifically, the goals of the project include : (i) developing quadratic and higher order nonlinear predictors, with demonstrable improvements, (ii) extending forecasting approaches for a new class of so-called quadratically predictable processes; (iii) developing nonlinear models-fitting via an appropriate generalization of the Whittle likelihood, derived from the mean squared error of the one-step ahead quadratic forecasting filter, (iv) developing theoretical foundations of auto-cumulants for multi-linear forms that are paramount to derive third and higher order polynomial predictors,(v) developing algorithms and supporting software in R for implementation of the methodology. The results from the project are expected to provide tools for substantially improved forecasting and signal extraction for univariate and multivariate time series data exhibiting nonlinear characteristics that are prevalent in many areas of sciences (e.g., Astronomy, Atmospheric sciences, Finance, Signal Processing) and real life applications.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2113771","Distance-Based Analysis for Complex High-Dimensional Data","DMS","STATISTICS","07/01/2021","06/23/2021","Debashis Mondal","OR","Oregon State University","Standard Grant","Pena Edsel","02/28/2022","$299,999.00","Bhaswar Bhattacharya","mondal@wustl.edu","1500 SW JEFFERSON ST","CORVALLIS","OR","973318655","5417374933","MPS","1269","","$0.00","Throughout the course of the twentieth century, distances have played a significant role in important areas of statistics, which include classification, clustering, discriminant analysis, multidimensional scaling, sampling, spatial statistics, scoring rules, and kernel methods in machine learning. Distances are also central to the definition of divergence measures, relative entropy and information gain, some of which are fundamental to the concept of C.R. Rao's quadratic entropy and the analysis of diversity in ecology and other areas of science. Yet, at present there are significant gaps in our knowledge and in the emerging statistical literature on the use of distance-based tests and analyses for complex high dimensional data.  One example is analysis of similarity which is among the most cited and most widely used distance-based statistical methods but is limited by an absence of relevant mathematical knowledge. This research will derive new mathematical knowledge on various distance-based statistical methods, and apply this for providing answers to important scientific questions arising in a number of disciplines in forestry, ecology and marine science, such as: (1) how biodiversity changes in tropical forests? (2) how taxonomic and functional profiles of bacterial communities change with environmental conditions in different oceanic regions? The project establishes collaborations among several disciplines and between two US academic institutions and provides research and training to graduate and undergraduate students.<br/> <br/>The project develops a new body of knowledge on distance-based statistical methods and computation for analyzing complex, high dimensional data that arise in the form of compositions, trees, graphs, or networks. The distances considered here are all non-Euclidean -- either non-metric dissimilarities that do not satisfy any triangular inequalities or just discrete numbers -- but they all arise from conditionally positive definite kernels. Examples of distances include the squared Euclidean distance, the Bray-Curtis dissimilarity, the Jensen-Shannon distance, Unifrac or the Kantorovich-Rubinstein metric, the Aitchison distance, the edit distance, various graph kernel and spectral distances, and other distances based on optimal transport problems. Specifically, the project advances the mathematical theory and computation of exact distribution-free two and multi-sample runs tests, change points, and other related problems by counting runs along the shortest Hamiltonian path (or loop) of the pooled sample of data points. The project also considers analysis of similarity and related distance-based rank tests and derives new mathematical results that allow us to pursue more advanced statistical analyses. The project contributes to: (i) a deeper analysis of biodiversity in tropical forest; (ii)  an investigation of  how taxonomic and functional profiles of prokaryotic communities  change  with environmental conditions in different oceanic regions; (iii) a study of the variability of composition of rare earth elements in deep-sea muds of the Pacific Ocean;  and (iv) an understanding of the relationship of intertidal communities in the Oregon coast with respect to upwelling and nutrient delivery. The project integrates mathematics research, science and education and will provide opportunities for dissertation work for graduate students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."

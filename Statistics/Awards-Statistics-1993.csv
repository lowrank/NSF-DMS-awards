"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"9300918","Mathematical Sciences: Inference for Nonparametric Function Estimators","DMS","STATISTICS","07/01/1993","03/27/1995","Randall Eubank","TX","Texas A&M Research Foundation","Continuing Grant","James E. Gentle","06/30/1996","$60,000.00","","eubank@math.asu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","     A number of inference problems in nonparametric function  estimation will be investigated during the course of this  project. The problems to be considered include i) estimation of  breakpoints for regression functions or their derivatives, ii)  development of practical confidence bands for use in  nonparametric regression, iii) construction of diagnostic  goodness-of-fit tests using nonparametric function estimation  methodology in a variety of settings, including censored data and  additive models, and iv) spline smoothing with correlated errors.       It is often reasonable to believe in many areas of science,  such as biology, engineering, psychology, etc., that the data  being collected is produced by two components: a nonrandom  component, representing a characteristic of nature  common to all  individuals or subjects, and a random component that accounts for  individual variation. This nonrandom component represents the  understandable or predictable aspect of the phenomenon being  studied and is therefore of considerable interest. Nonparametric  function estimation is an area of statistics concerned with the  development of flexible methods for estimating or describing the  nonrandom components of data.  The current research project deals  with a variety of problems in nonparametric function estimation  including i) the development of procedures for estimating a point  where an abrupt change has occurred in some phenomenon (e.g.,  locating a point of sudden change in the discharge of water from  a river or prices of a commodity) ii) the construction of  accuracy measures (similar to those associated with popularity  polls, etc.) to accompany nonparametric function estimators and  iii) the development of statistical methods to assess the  accuracy of models that have been postulated by scientists to  describe various processes they are studying."
"9301193","Mathematical Sciences:  Nonlinear Time Series Analysis","DMS","STATISTICS","06/01/1993","04/30/1993","Rong Chen","TX","Texas A&M Research Foundation","Standard Grant","James E. Gentle","05/31/1996","$40,000.00","","rongchen@stat.rutgers.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","","$0.00","     This project is concerned with nonlinear time series                       analysis.  It has four main objectives.  The first objective is                 to develop new methods for modeling nonlinear time series  via                  nonparametric smoothing techniques.  This research is a                         continuation of the work on functional coefficient AR models and                nonlinear additive AR models by the PI and Tsay over the last                   several years.  See Chen and Tsay (1993a, 1993b). The results                   obtained under this research will increase the applicability of                 nonlinear time series models.                                                        The second objective of the proposed research is to                        investigate binary-process driven switching regression models.                  A unified treatment is proposed for the general switching                       structures and a testing procedure is introduced for                            discriminating a random (independent) switching regression model                from a Markov-chain driven model.  The third objective is to                    study new approaches in finding the indicator variable for an                   open-loop threshold AR model.  Several approaches are suggested                 to overcome the difficulties encountered in using the open-loop                 threshold AR models.  In particular, two algorithms and some                    graphical tools are proposed to identify an appropriate indicator               variable.  The final objective is to investigate the ergodicity                 conditions of some nonlinear time series models.  The results                   obtained will provide a better understanding of the nonlinear                   models as well as a foundation on which asymptotic properties of                various nonparametric statistics for nonlinear time series                      analysis can be established."
"9224798","Mathematical Sciences:  Inference for Nonlinear Time Series and Spatial Models","DMS","STATISTICS","06/01/1993","05/26/1993","Ishwarasa Basawa","GA","University of Georgia Research Foundation Inc","Standard Grant","James E. Gentle","05/31/1997","$60,000.00","","ishwar@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","","$0.00","     This proposal is concerned with problems related to                        statistical inference in nonlinear time series, state-space                     models, random fields and spatial point processes.  The emphasis                will be on nonlinear and non-Gaussian models.  A unified approach               will be used to study the asymptotic efficiency properties of the               estimators and tests.  The local asymptotic normality will be                   investigated for the  likelihood-based models.  the quasi-                      likelihood framework will be used for the cases where the                       likelihood function is unavailable.  Empirical Bayes methods will               also be investigated.                                                                The statistical analysis and the models to be studied in                   this proposal will have an impact on a large number of diverse                  applications in Basic Sciences, Engineering, Health Sciences,                   Economic and Social Planning, among other.  Statistical data such               as the unemployment figures, various economic indicators, stock                 market fluctuations, opinion polls, currency exchange rates and                 the like are all subject to inherent random variations, and                     possibly trends.  These aspects involving uncertainty can                       usefully be represented by appropriate probability models such as               the ones proposed here.  Various methods of forecasting and                     assessing the precision of the forecasts will also be studied.                  Techniques of statistical estimation and methods of testing the                 validity of the models proposed will be investigated."
"9306738","Mathematical Sciences:  Likelihood Functions for Estimating Equations","DMS","STATISTICS","05/15/1993","05/26/1993","Bing Li","PA","Pennsylvania State Univ University Park","Standard Grant","James E. Gentle","10/31/1996","$60,000.00","","bing@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","     My research focuses on the construction  of  likelihood                    functions for estimating equations.  The need for such functions                arises when the estimating equations do not integrate to                        potential functions.  The likelihood functions can be either                    obtained from the projection of the true score function  onto a                 class of  conservative estimating equations or from the                         projection of a pair of centered likelihood ratios.   The                       projections are developed for the quasi-likelihood method, and                  for the generalized estimating equations, as well as some other                 classes of estimating equations.  They can be applied to both                   independent and dependent situations.  These functions will be                  used to construct invariant confidence intervals, to distinguish                between consistent and inconsistent solutions to estimating                     equations, and to test statistical hypotheses.  I also have                     introduced a notion of the information contained in a testing                   function,  and will use it to study some commonly used testing                  functions. In relation to this, a property of the first order                   ancillarity is obtained.                                                             The  key issue in statistics is to establish a relation                    between the truth and the data, and to use the relation to make                 inference about the truth  based  on the  data.  This  is  often                achieved by what we call the likelihood methods.  Sometimes the                 traditional likelihood method faces difficulties:  it may need to               make assumptions that are difficult to satisfy; it may be                       computationally difficult to realize; it may not be rich enough                 to accommodate certain applications. Meeting these challenges,                  the theories and methods of quasi-likelihood and estimating                     equations have been developed and widely used during recent                     years.  Extending  and  enriching the likelihood method, the new                methods can be applied under more realistic assumptions, more                   conveniently, and in wider situations.   My research is on                      certain aspects of  quasi-likelihood  and estimating equations,                 and in particular the construction of  likelihood functions for                 estimating equations and their use in statistical inference."
"9305877","Mathematical Sciences:  Adaptive Sampling","DMS","STATISTICS","05/15/1993","05/27/1993","Steven Thompson","PA","Pennsylvania State Univ University Park","Standard Grant","James E. Gentle","10/31/1996","$60,000.00","","skt@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","     Adaptive sampling designs are designs in which the procedure               for selecting the units to include in the sample may depend on                  observed values of the variable of interest.  In previous                       research supported by NSF grants DMS-8705812 and DMS-9016708, the               methodology of adaptive sampling was advanced to the point where                now there are a number of practical adaptive designs, giving                    remarkably high efficiencies for sampling rare, clumped, or                     aggregated populations compared to conventional designs of                      equivalent sample size.  Theoretical results suggest, however,                  that greater potential still exists.  It is proposed to further                 develop the basic theory of adaptive sampling and look for new                  classes of adaptive sampling designs.                                                Adaptive sampling designs are designs in which the procedure               for selecting the units or sites to include in the sample depends               on observations made during the survey.  For example, in a survey               to estimate the abundance of a rare animal species, additional                  observations may be made in the vicinity of sites with high                     observed abundance.  In a survey of an environmental pollutant,                 additional observations may be made in the vicinity of discovered               ``hot spots.''  For such surveys, adaptive sampling designs can                 provide estimates of far greater precision than can be obtained                 with conventional designs of equivalent sampling effort.                        Research is proposed to develop the theory and methods of                       adaptive sampling. The proposed research  has immediate,                        important applications to many fields, including environmental,                 ecological, and natural resource studies."
"9303705","Mathematical Sciences:  Inference and Curve-Fitting in      Generalizations of Isotonic Regression Models","DMS","STATISTICS","08/15/1993","04/14/1995","Thomas Hettmansperger","PA","Pennsylvania State Univ University Park","Continuing Grant","James E. Gentle","01/31/1997","$135,000.00","Thomas Ryan, Frank Deutsch","tph@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, OTHR","$0.00","In many scientific hypotheses the researcher can specify a  directional effect:  a treatment is believed to improve health, a  new teaching method is believed to improve learning, an increase  in dosage is believed to have an increasing effect up to a  certain (possibly unknown) toxicity point and then a decreasing  effect.  Many statistical methods ignore this directional  information, which results in a loss of statistical power and  efficiency.  Our major goal in this research is to develop very  general approaches to the computation of estimates and tests  along with assessments of the standard errors of the estimates  and the error rates of the tests.  The research has three main  thrusts:  First, the analysis of general algorithms such as  Dykstra's generalization of von Neumann's alternating projections  method.  Second, the development and implementation of smoothing  and fitting algorithms to use on data that arise in experiments  with order restrictions on the parameters.  And third, the  construction of statistical estimation and testing methods with  attendant error rates for research hypotheses that entail  directional effects.  Often in scientific investigations the researcher expects a  directional effect, but cannot justify a specific mathematical  function as a relationship.  For example, the yield of a crop can  reasonably be expected to increase if the amount of fertilizer  applied is increased (at least within a certain range).  The fact  that there is a directional relationship is obvious here, but the  exact nature of the relationship is far from obvious.  Our goal  is to develop methods that make use of directional effects so  that more efficient use may be made of experimental data.  We  will develop computer algorithms to fit data in ways that obey  directional effects, and use statistical and mathematical theory  to study the properties of our fitting methods."
"9307255","Mathematical Sciences: Random Effects Models in Survival    Analysis","DMS","STATISTICS","07/01/1993","05/26/1993","Susan Murphy","PA","Pennsylvania State Univ University Park","Standard Grant","Stephen M. Samuels","12/31/1996","$60,000.00","","samurphy@fas.harvard.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","     This research is concerned with the generalization of the                  two well known regression models for duration times, the                        proportional hazards model ant the accelerated failure time                     model, to a mixed model with both fixed effects and a random                    effect.  In particular, the objective of this research is to                    consider score and likelihood ratio tests for the presence of the               random effect in the proportional hazards model and to consider                 both a method of estimation for the regression coefficient and                  nonparametric likelihood estimation of the error distribution in                the mixed effects, accelerated failure time model.                                   Many areas of scientific endeavor, such as demography,                     duration analysis in economics, reliability in manufacturing, and               survival analysis in medical studies, are confronted with                       dependent duration times.  This research is concerned with a                    generalization of two often used models to a model which is more                realistic."
"9300973","Mathematical Sciences: Statistical Inferences, Decision     Theory, and Asymptotics of Eigenvalues and Eigenprojections","DMS","STATISTICS","07/01/1993","06/08/1993","Morris Eaton","MN","University of Minnesota-Twin Cities","Standard Grant","James E. Gentle","12/31/1996","$60,000.00","","eaton002@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","     Research is proposed on a variety of problems in statistical               decision theory.  The main focus of this research is the                        evaluation of inferential methods with admissibility as one                     evaluative criterion.  The connection between the admissibility                 of formal Bayes rules and the recurrence of symmetric Markov                    chains forms an important portion of the theoretical background                 for this work.  In a nonparametric setting an alternative method                of inducing inferences is proposed.  This alternative is easily                 implemented numerically and has connections with both frequentist               and Bayesian bootstrap procedures.  Another area of proposed                    research concerns questions related to the asymptotic                           distribution of eigenvalues, eigenprojections, and singular                     values.  Previous work contains new techniques which provide a                  unified treatment of eigenalues and singular values.                            Applications of these results to concrete examples and extension                of the basic results to eigenprojection problems is proposed.                        Statistics deals with the activity of making statements                    about populations on the basis of available samples or data.                    Since information is most often incomplete, such statements are                 typically in error.  The quantification of this error is a                      standard statistical activity and is often expressed in terms of                probability.  The research supported by this grant concerns                     various methods for quantifying statistical errors and the                      properties of such methods.  The ultimate goal of the research is               the development of new statistical methods for obtaining                        probabilistic quantification of errors in inferential statements."
"9308772","Mathematical Sciences:  New Directions in Time Series","DMS","STATISTICS","07/01/1993","04/20/1995","Andrew McDougall","NJ","Rutgers University New Brunswick","Continuing Grant","James E. Gentle","12/31/1996","$60,000.00","","","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","     Three research projects that explore some recent and  innovative methodologies of active interest in time series  analysis are investigated. The first focuses on the problem of  obtaining robust recursive estimates for ARMA models, and will  initially address a number of important issues that arise when  redescending robust criteria are employed in the recursive  procedure. In the second project, applications and finite sample  properties associated with the spectral envelope methodology for  analyzing categorical time series are investigated. The extension  of this methodology to real-valued time series is also explored.   The development and integration of exploratory time series  analysis methods within the dynamic graphical environment is  considered in the final project, and provides an effective means  to perform the research involved in each project concurrently.           The development of new procedures for analyzing time series  data is an important and active area of research across a variety  of disciplines.  Typically, time series analysis is equated with  data that is observed sequentially over time (eg. stock market  prices, temperature), but in other disciplines ""time"" may simply  be a positional index that defines, for example, the order of the  base-pair codings (a,t,g,c) in DNA sequences.  In this project,  three innovative approaches to time series analysis are  investigated.  The first concerns the problem of retaining the  ability to obtain useful information about a time series process  when aberrant observations may be present.  The second approach  deals with the concept of a ""spectral envelope"" which provides an  objective criterion for assessing periodic patterns in  qualitative series such as DNA sequences.  A large component of  the research involved in both these approaches will be  incorporated into the last project where the development of  highly interactive exploratory graphical methods for time series  analysis is pursued."
"9224839","Mathematical Sciences: Topics in Statistical Inference","DMS","STATISTICS","06/01/1993","06/10/1993","Kesar Singh","NJ","Rutgers University New Brunswick","Standard Grant","James E. Gentle","11/30/1996","$60,000.00","","kesar@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","     This proposal contains ten problems related to various                     statistical topics which the investigator wishes to study during                the period for which this funding is sought.  A dominant topic in               this proposal is resampling, i.e., bootstrap, jackknife, etc, an                area which has been the author's main focus of research.  Other                 areas of statistics covered here include: data depth and                        multivariate analysis, confidence regions and bands, directional                data analysis survival analysis, robustness, ANOVA and                          experimental designs.  The proposal includes some proposed                      notions and procedures yet to be explored, mathematically and                   numerically.                                                                         The proposed research mainly focuses on a relatively new                   topic in statistics named the bootstrap.  This new methodology in               statistics is computer intensive, it is often more accurate and                 accessible to more users, since it replaces mathematical labor by               computer work.  One of the thrusts of this research is to                       investigate how well this methodology can evaluate its own                      accuracy.  Among other topics of research proposed here, is the                 development of a very general approach in the field of survival                 analysis, which is a key tool in pharmaceutical researches.  The                research will also include new concepts of statistical                          robustness, an area pertaining to the lack of sensitivity (which                is desired) of data-conclusions to a relatively few                             unrepresentative data values."
"9311477","Mathematical Sciences: Asymptotically Efficient             Semiparametric Estimation","DMS","STATISTICS","07/01/1993","06/05/1995","Yuly Koshevnik","TX","Southern Methodist University","Continuing Grant","James E. Gentle","12/31/1996","$60,000.00","","","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1269","0000, OTHR","$0.00","The proposal is focused on estimation problems involving infinite  dimensional parameters, such as cumulative distribution  functions, characteristic functions, error distributions in  regression models etc. Methodologically, a geometric approach has  been successfully developed to solve nonparametric estimations  problems from i.i.d. (independent and identically distributed)  data. This approach is also applicable to a wide range of  nonparametric and semiparametric problems, in which the  traditional restrictive i.i.d. assumptions can be relaxed. The  main emphasis is on the estimation of differentiable statistical  functionals. For a general differentiable transformation of the  unknown abstract parameter, the asymptotically efficient  estimates will be constructed using the geometric approach.  Complex statistical data that are collected in areas such as  medical, econometric, and engineering reliability studies often  cannot be analyzed using traditional statistical methods. The  diversity of people in clinical trials, the multitude of inputs  affecting economic outputs, and the intricate interaction of  components affecting the performance of sophisticated electronics  or equipment do not permit the use of simple statistical modeling  techniques. These applications require instead a new methodology  that is more flexible and yet does not sacrifice information  unnecessarily. The proposed research will develop methods for  estimating key features of complex statistical data.  These  methods do not depend on the validity of traditional assumptions.  Results of this work will be applicable to a rich variety of  areas, including the estimation of drug efficacy, a population  profile of potential patients, proficiency characteristics for  industrial output, and the probability of successful operation of  electronic equipment."
"9303925","Mathematical Sciences:  Methods for Ranked Data","DMS","STATISTICS","07/01/1993","08/10/1993","Georgia Thompson","TX","Southern Methodist University","Standard Grant","Sallie Keller-McNulty","12/31/1995","$30,000.00","","","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1269","","$0.00","In ranked data each observation is a permutation of n items.   Because the permutations do not have a natural linear ordering,  many common statistical methods are inappropriate for ranked  data.  However, the permutations of n items can be naturally  placed on the vertices of a polytope inscribed in a sphere in n-1  dimensional Euclidean space.  This geometry suggests new  graphical and analytical methods for analyzing full and partially  ranked data.  Of particular interest are probability density  histograms, spectral analysis, kernal smoothing methods, and  confidence regions.  Statistical methods are proposed to analyze ranked data.  Ranked  data occur when a group of ""judges"" are asked to evaluate a set  of items and then to rank them in order of preference.  Hence,  each ""judge"" states his first, second, etc., and last choices  among the items that he has evaluated.  Proper methods to  graphically display and analyze ranked data are very important.  This type of data is encountered frequently in the social  sciences, in market research, and in elections.  A less well  known, but equally important area in which ranked data are  encountered is expert systems for military applications."
"9307567","Mathematical Sciences: Experiment Design for Variance       Functions","DMS","STATISTICS","06/01/1993","06/11/1993","Sharon Lohr","AZ","Arizona State University","Standard Grant","Sallie Keller-McNulty","11/30/1994","$15,700.00","","sharon.lohr@asu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1269","9221","$0.00","     The proposed investigations include properties of balanced                 designss in the general random effects model, development of                    Bayesian optimal experimental designs for some simple examples of               variance functions and computer implementation of a Bayesian                    method for choosing sample sizes."
"9306979","Mathematical Sciences:  Extra-Variation Models for Cross-   Classified Data","DMS","STATISTICS","07/01/1993","04/01/1994","Jeffrey Wilson","AZ","Arizona State University","Continuing Grant","Sallie Keller-McNulty","12/31/1995","$50,800.00","","ATJRW@ASUACAD","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1269","0000, 9135, OTHR","$0.00","     The research entails methods of modelling extra-variation in               complex survey data. Approaches include complete probability                    distributions with applications to unequal cluster sizes and                    longitudinal data. Overall, the intention is to establish models                which are applicable to national surveys such as the `High School               and Beyond' and `National Crime Victimization Survey' projects."
"9304014","Mathematical Sciences:  Building Designs for Collecting     Better Data and Making Finer Conclusions","DMS","STATISTICS","06/01/1993","05/20/1993","Sam Hedayat","IL","University of Illinois at Chicago","Standard Grant","James E. Gentle","11/30/1996","$90,000.00","","hedayat@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","9146","$0.00","     We plan to identify optimal and efficient statistical                      designs in several commonly used scientific studies. These areas                include, but not limite to, two-level factorial designs for main                effects and selected two-factor interactions; resolution III                    and more two-level fractional factorial designs with (weak)                     minimum aberration; repeated measurements designs for comparative               bioavaliability studies.                                                             Inference based on data collected via poorly designed                      studies is generally misleading and could be invalid altogether.                Good science calls for good data and good data can come only                    through statistically designed studies. This proposal plans to                  identify and build statistically efficient designs/per unit cost                so that better data can be collected and valid scientific                       inference can be made. Our discoveries will have immediate                      applications to many areas of scientific  studies including                     medicine and engineering. We plan to solve design problems which                have been posed to us by scientists at US FDA, and many US                      industries."
"9306271","COLLABORATIVE RESEARCH:  Nonlinear Demographic Dynamics:    Mathematical Models, Biological Experiments, Data Analyses","DMS","ECOSYSTEM STUDIES, APPLIED MATHEMATICS, STATISTICS, COMPUTATIONAL MATHEMATICS","08/01/1993","08/11/1995","Jim Cushing","AZ","University of Arizona","Continuing Grant","Michael Steuerwalt","01/31/1997","$320,000.00","Robert Costantino, Brian Dennis","cushing@math.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1181, 1266, 1269, 1271","0000, 1305, 9169, 9178, 9251, 9263, EGCH, OTHR, SMET","$0.00","9306271  Cushing       The investigators conduct an interdisciplinary research  program to test nonlinear population theory: they construct and  analyze mathematical models, design and implement biological  experiments, develop and apply statistical techniques for the  analysis of data.  For the biological experiments an organism is  used that is easy to culture, has a short generation time (i.e.  yields long time series data) and allows an accurate census of  animal numbers: flour beetles of the genus Tribolium.  In the first  part of the study the objectives are model identification and  parameter estimation.  In the second part, the concern is to  document transitions in the qualitative behavior of the  demographic dynamics.  Rates of reproduction and adult mortality  are manipulated in order to cross boundaries in parameter space  from stable equilibria, to periodic cycles, to chaos.  In phase  three the objectives are to test hypotheses concerning the  existence of these unusual demographic dynamics and develop  methods for identifying these phenomena in experimental data.  A  major contribution of this project is an unequivocal example of  experimentally manipulated transitions between qualitatively  different dynamical behaviors of a biological population as  predicted by a mathematical model.       In the last ten years or so, the recognition that simple  equations can generate complex dynamics has led to an outpouring  of fascinating theoretical possibilities for the explanation of  population time series data.  Understanding the observed  fluctuations in animal population numbers is a central question in  population biology; it has far-reaching applications in areas  ranging from food production and pest control, to the management  of renewable resources, to the conservation of species diversity.  The hypothesis that fluctuations are the result of nonlinear  dynamic forces has proved to be elusive to test due to the  difficulties of gathering adequate ecological data, of  experimentall y manipulating ecological systems, and of evaluating  complex mathematical models with ecological data.  In this research  project the investigators' approach to testing nonlinear  population theory is to connect mathematical models rigorously  with experimental biological data by means of newly developed  statistical methods for nonlinear time series.  The project is  unique in its interdisciplinary approach because it involves both  theory and experimentation and utilizes the talents of the  biologist, statistician, and mathematician.  It is unusual in the  field of population biology to have an interdisciplinary effort in  which investigators from all of these disciplines are involved in  all aspects of the project, from experimental design and  implementation, through theoretical modeling and analysis, to  statistical testing and verification.  The ultimate goal is to  demonstrate the usefulness and importance of nonlinear mathematics  in gaining a rigorous understanding of the dynamics of animal  populations and in particular of fluctuations in population  numbers, be these fluctations regular or ""chaotic."""
"9303557","Mathematical Sciences:  Bayesian Inference and Computing","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS, Methodology, Measuremt & Stats","07/01/1993","06/19/1997","Joseph Kadane","PA","Carnegie-Mellon University","Continuing Grant","Joseph M. Rosenblatt","12/31/1999","$1,040,000.00","Robert Kass, Luke-jon Tierney, Larry Wasserman","kadane@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 1271, 1333","0000, 9263, OTHR","$0.00","Our research is oriented toward implementation of Bayesian  inference. There has been increasing interest recently in the  Bayesian approach to statistics, in part because advances in  computational ability have made it feasible in many settings, and  in part because Bayesian analysis of data can make use of  information from additional sources. Our work will build on our  previous research in Bayesian statistics, part of which has been  funded by NSF. Our main concerns are: (1) review and assessment  of methods for choosing prior probability distributions by formal  rules, and further development of methods for assessing  sensitivity to the choices; (2) investigation of approximate and  exact computational methods for Bayesian hypotheses testing; (3)  modification and enhancement of numerical integration techniques  and Monte Carlo simulation of posterior distributions; also,  improvement of statistical computing environments including use  of animation and three dimensional rendering for visualization of  uncertainty in higher dimensions; (4) further work on the  foundations of subjective probability; and (5) several other  topics related to our previous work on elicitation of priors and  asymptotic approximations.  When analyzing data, it is important  to combine all sources of information effectively.  Bayesian  statistical methods are tailored to this purpose.  Our research  focuses on finding practical ways to implement Bayesian methods  and on investigating the theoretical basis for these methods.  We are concerned with the development of computational and  graphical techniques that make Bayesian inference feasible in  complicated problems.  These include: simulation, animation and  the construction of statistical computing environments.  We will  also investigate theoretical issues that support Bayesian  techniques. These issues include the foundations of subjective  probability and the development of mathematical approximations."
"9301316","Mathematical Sciences: Strategies for Bayesian Data Analysiswith Application to Quantal Bioassay and Geographic Disease Occurrence Models","DMS","STATISTICS","07/01/1993","02/10/1995","Alan Gelfand","CT","University of Connecticut","Continuing Grant","James E. Gentle","12/31/1996","$120,000.00","","alan@stat.duke.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","0000, OTHR","$0.00","     Recently there have been major advances in the development                 of monte carlo strategies to obtain samples from high dimensional               nonnormalized joint densities.  These techniques have proved                    especially valuable in fitting structured random effects models.                In such settings,  Analytic examination of the posterior                        distribution is infeasible but sampling enables arbitrarily                     accurate estimation of any features of the posterior with the                   promise of fitting an enormous range of previously inaccessible                 models attention must now turn to the development of tools for                  bayesian data analysis.  Consistent with the informality of the                 art of model development we propose to develop informal tools                   with an eda flavor to mesh naturally with the sampling based                    approaches used to fit the model.  Application will be in two                   Biometric areas, quantal bioassay and geographic disease mapping.                    CONTEMPORARY STATISTICAL WORK IS DRIVEN BY SERIOUS                         Applications.  Realistic Models in such applications will                       necessarily be complex incorporating many unknowns.  Indeed,                    there may be several plausible models with collected data, the                  question arises of how to fit such models with several choices                  of models we might ask if any of them are good and which one we                 prefer.  The role of the data analyst is to provide answers to                  these questions.  The proposed research is intended to help the                 data analyst by providing a unifying strategy for carrying out                  the fitting and an associated set of tools for studying adequacy                of and choice of models at the heart of the entire approach is                  the most elementary of statistical ideas - drawing samples to                   learn about model unknowns.  We propose to utilize this approach                in two biometric areas of application.  One considers geographic                patterns of disease occurrence.  The other involves the change in               toxicity to a population due to change in exposure to a toxin."
"9307727","Mathematical Sciences:  Studies in Bayesian Inference       Design and Bayes - Classical Synthesis","DMS","STATISTICS","07/01/1993","04/10/1995","Anirban Dasgupta","IN","Purdue Research Foundation","Continuing Grant","James E. Gentle","12/31/1996","$60,000.00","Jayanta Ghosh","","1281 WIN HENTSCHEL BLVD","WEST LAFAYETTE","IN","479064182","3174946200","MPS","1269","0000, OTHR","$0.00","In this project, we propose to consider problems in Bayesian asymptotics, design,  inference, and the question of synthesis with classical procedures. In  asymptotics, we propose to consider the limiting behavior of the posterior in  nonregular problems. Design of experiments will be considered in nonlinear  models, when prior information is available about parameters of the model. Such  nonlinear models typically are hard to deal with because of dependence of the  design on the parameters themselves. In particular, we will try to use the rich  linear theory by approximating nonlinear response functions by linear functions,  such as polynomials. We also will derive designs that give a preexperimental  guarantee of pos data accuracy, uniformly in the data whenever possible. Finally,  in canonical Exponential family problems, we will derive priors for which the  implied Bayes procedure has classically satisfactory property.  Much of existing statistical methodology has evolved from methods of so called  classical statistics. In comparison, the alternative methods known as Bayesian  methods are still growing.  Nevertheless, it is now recognized by the community  as a whole that Bayesian methods can be very useful as tools for statistical data  analysis. In this project, our goal is two fold:  first, to study alternative  Bayesian methods in important classes of statistical problems, such as designing  statistical surveys, and second, to bridge the two methods of statistical  analysis together by studying Bayesian methods which are also acceptable  according to classical criteria."
"9303556","Mathematical Sciences:  Investigations in Bayesian Analysis,Statistical Decision Theory, and Computation","DMS","STATISTICS","06/15/1993","06/16/1998","James Berger","IN","Purdue Research Foundation","Continuing Grant","Joseph M. Rosenblatt","11/30/1999","$758,922.00","","berger@stat.duke.edu","1281 WIN HENTSCHEL BLVD","WEST LAFAYETTE","IN","479064182","3174946200","MPS","1269","0000, OTHR","$0.00","     The project will focus on research in the six areas of                     Robust Bayesian Analysis, Default Prior Bayesian Methodology,                   Conditional Inference in Decision Theory, Multivariate                          Estimation, Hierarchical Modelling, and Bayesian Computation. In                the first two areas, special emphasis will be paid to problems                  involving testing and model selection.  Bayesian approaches to                  these problems are severely underutilized, primarily because                    robust or default prior analyses in these areas have not been                   developed.  In Conditional Inference, the primary focus will be                 the study of situations in which Bayesian answers have a valid                  conditional frequentist interpretation.  Identification of such                 situations is not only valuable foundationally, but also                        indicates situations in which adopting a conditional frequentist                approach can be highly advantageous.  In Bayesian Computation,                  emphasis will be on use of Markov Chain and other simulation                    techniques for carrying out Bayesian integrations.                                   Bayesian analysis is perhaps the most rapidly growing                      approach to statistical analysis, because of its ability to model               and analyze even extremely complex situations, and because it                   readily allows combination of multiple information sources. The                 chief limitations of Bayesian analysis have been concern about                  sensitivity to modelling assumptions, and the difficulty in                     carrying out Bayesian computations in high dimensional problems.                In this project we will attack the sensitivity problem by                       developing Bayesian models that are inherently ""robust,"" and by                 creating powerful methods for selecting from among competing                    Bayesian models.  On the computational side, we will extend new                 techniques based on simulation that have the promise of handling                situations with hundreds or thousands of unknown model                          parameters."
"9301511","Mathematical Sciences: Optimal Design of Experiments and    Spline Smoothing","DMS","STATISTICS","07/01/1993","02/14/1995","William Studden","IN","Purdue Research Foundation","Continuing Grant","James E. Gentle","06/30/1997","$69,000.00","","studden@stat.purdue.edu","1281 WIN HENTSCHEL BLVD","WEST LAFAYETTE","IN","479064182","3174946200","MPS","1269","0000, OTHR","$0.00","We seek to develop reliable automatic structural nonparametric  estimators of densities and hazards on multidimensional domains.   The structure, ANOVA decomposition of functions, can be built in  via tensor product splines.  For density estimation, the  structure allows conditional independence to be built into  multivariate densities; for hazard estimation, it yields survival  models more general than, but reducible to, proportional hazard  models.  The targeted results are asymptotic theory and automatic  algorithms implemented in portable public domain code.  Our  research is on the estimation (reconstruction) of curves/surfaces  based on imperfect data collected in designed experiments or  other scientific studies.  The curves/surfaces to be  reconstructed may represent geographical distribution of acid  rain deposition as is useful for environmental monitoring, or may  characterize the effectiveness of medical treatments in clinical  trials, etc.  Prior results of the research have found  applications in modeling weather data, environmental data, and  AIDS data."
"9303713","Mathematical Sciences:  Large-Sample Approximations for     Analysis of Qualitative Data","DMS","STATISTICS","06/01/1993","05/27/1993","Shelby Haberman","IL","Northwestern University","Standard Grant","James E. Gentle","05/31/1996","$60,000.00","","haberman@casbah.acns.nwu.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1269","","$0.00","     Large-sample approximations used in the analysis of                        qualitative data are to be examined.  Problems to be considered                 involve both large numbers of parameters and large numbers of                   observations.  Log-linear models, conditional log-linear models,                latent-class models, canonical models, and association models are               to be explored.  Large-sample approximations are to be considered               for distributions of maximum-likelihood estimates, Pearson and                  likelihood-ratio chi-square statistics, and measures of                         predictions.  Analyses are to be developed without the assumption               that the underlying model is correct.                                                In large-scale surveys and censuses, statistical analyses                  are commonly performed on very large samples.  In such analyses,                models used to approximate the observations often involve very                  large numbers of unknown quantities which must be estimated from                the sample in order to apply the model.  In the proposed                        research, approximations are developed for the variability                      expected from the estimates of unknown quantities and for                       measures of the value of the model for construction of predictors               of future observations.  In contrast to usual statistical                       analyses, the methods developed apply even if the proposed model                is not entirely consistent with the data.  It is shown that                     approximate models can be quite valuable in statistical work even               when they do not fully describe the data."
"9217866","Mathematical Sciences:  Estimation and Inference for Noisy  Nonlinear Systems","DMS","STATISTICS","07/15/1993","06/05/1995","Douglas Nychka","NC","North Carolina State University","Continuing Grant","James E. Gentle","06/30/1997","$119,876.00","Stephen Ellner, A. Ronald Gallant","nychka@mines.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","0000, OTHR","$0.00","One important property of a system that changes over time, such  as an economy or an ecosystem, is the extent to which its future  behavior can be predicted. This work will develop statistical  methods for quantifying predictability, and apply these methods  to address some open questions in ecology, epidemiology and  macroeconomics.  The result of this work will be an understanding  of how a complex system's dynamics can be divided in two parts: a  part that is a possibly complex function of the previous history  of the system and a random component that is unrelated to the  system's past behavior.  The ability to make predictions depends  on both of these components; moreover, knowing the relative  contributions of the components is necessary for understanding  how the system responds to external shocks.  In the past 20 years there has been much interest in the use of  nonlinear models to explain seemingly unpredictable or random  phenomena.  Most techniques for analyzing data from a nonlinear  dynamic system have been based on large data sets and properties  of deterministic models.  Such methods are not useful for  biological and economic systems that are subject to random  perturbations and observed over a limited amount of time.   Statistical methods will be developed that address these  situations by combining techniques of nonparametric regression  and time series analysis.  These statistical techniques will  enable researchers to reliably estimate the rules governing a  system's evolution over time (law of motion) and the average  response of the system to small perturbations (Lyapunov  exponent).  Also, at a more theoretical level, the properties of  artificial neural networks for approximating systems of many  variables will be studied.  The methods will be applied to  empirical, substantive time series in ecology and epidemiology in  order to quantify the predictability of the systems from past  history and to identify the system's response to exogenous (e.g.  environmental) shocks.   These methods, coupled with general  equilibrium economic models, will provide new evidence for  resolving a longstanding controversy in macroeconomics: Are  extreme fluctuations in financial markets natural phenomena or  are they aberrations requiring government regulation?"
"9301366","Mathematical Sciences: Multi-dimensional Statistical        Analysis","DMS","STATISTICS","07/01/1993","04/28/1995","Ingram Olkin","CA","Stanford University","Continuing Grant","James E. Gentle","12/31/1996","$99,027.00","","IOLKIN@STAT.STANFORD.EDU","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","The large number of univariate distributions that have been thoroughly studied  sometimes have been developed for use in statistical analyses of data; other  distribution arise from natural probabilistic or physical considerations, from  statistical properties or as limiting distributions.  Most of the well-known  distributions arise in more than one way.  Although univariate distributions provide information about individual  measurements, they cannot facilitate studies of relationships between  measurements.  In many situations in the physical, biological, economic sciences  it is just these relationships that are of interest, for which multivariate  distributions are required.  The goal of this proposal is to contribute to the understanding of multivariate  versions of the well-known univariate theory.  This subject is particularly  timely because recent increases in computational and computer graphics  capabilities have made the use of multivariate distributions much more broadly  practical.  Multivariate models are now particularly understood in terms of  underlying structures that unite many multivariate families into cohesive units.   Meaningful derivations of multivariate distributions can shed light on which  models to use in different applications.  A consequence of this research is that  flexible models useful in describing multivariate data will become available."
"9208758","Mathematical Sciences: Fellows for Cross-Disciplinary       Research in Statistics","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","03/01/1993","07/21/1999","Jerome Sacks","NC","National Institute of Statistical Sciences","Continuing Grant","Lloyd E. Douglas","07/31/1999","$1,394,000.00","","sacks@niss.org","19 TW ALEXANDER DR","RESEARCH TRIANGLE PARK","NC","277090152","9196859300","MPS","1260, 1269","0000, 9179, 9187, 9188, EGCH, ENVI, OTHR","$0.00","                                                                                     This project will provide support for postdoctoral fellows                 at the National Institute of Statistical Sciences (NISS), to                    engage in cross-disciplinary research with significant                          statistical research components.  Jerome Sacks is Director of                   NISS, located in Research Triangle Park.  NISS is a recently                    formed institute whose inspiration came initially from an NSF                   sponsored workshop on cross-disciplinary research.  Subsequent                  meetings in the statistical community produced a site selection                 procedure and, ultimately, the Research Triangle site.  The                     creation of the Institute responded to the fact that in order to                address major national problems or ""Grand Challenges"",                          substantial progress has to be made in the methods and                          applications of statistical science.  A critical component of a                 continuing effort in this direction is the involvement and                      encouragement of junior researchers.                                                 The present project will support junior fellows to work on                 dedicated projects of high importance.  It will provide                         postdoctoral fellowship support for projects of a cross                         disciplinary nature.  The scope of the project is closest to the                role of the institutes in mathematics (IMA, MSRI, IAS).  However,               the outlook is quite different.  While the mathematics institutes               support postdoctoral fellows, their focus is on active                          collaboration with researcher from different disciplines, to                    solve important ""national problems"" which involve new statistical               research, in a manner fully consistent with recommendations of                  the Commission the Future of the NSF. ( See: A Foundation for the               21st Century, 1992.)//"
"9357646","Mathematical Sciences: NSF Young Investigator","DMS","STATISTICS","07/15/1993","07/22/1997","Larry Wasserman","PA","Carnegie-Mellon University","Continuing Grant","James E. Gentle","12/31/1998","$130,000.00","","larry@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, 9178, 9251, 9297, OTHR, SMET","$0.00","                                                                                     This research, supported by the National Science Foundation                Young Investigator award, will address theoretical and                          methodological issues in Bayesian statistical inference.  The                   first topic concerns information-theoretic diagnostics to assess                and construct prior distributions.  The second topic involves                   using function space derivatives to perform sensitivity analysis.                    The National Science Foundation Young Investigator award                   recognizes outstanding young faculty.  This award recognizes the                recipient's strong potential for continued professional growth as               a research mathematician and for significant development as a                   teacher and academic leader.//"
"9301344","Mathematical Sciences: Robust Estimation of Multivariate    Location and Shape in High Dimension","DMS","STATISTICS","07/01/1993","07/02/1993","David Rocke","CA","University of California-Davis","Standard Grant","James E. Gentle","06/30/1996","$90,000.00","David Woodruff","dmrocke@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","     This project will develop computationally efficient, robust,               affine-equivariant estimators of multivariate location and shape                for use in multivariate analysis and for identification of high                 leverage points in robust regression.  This problem has proved to               be computationally quite challenging---reliable solutions were                  possible with previous methods only for small samples and low                   dimension.  We have developed methods that greatly increase the                 effectiveness of these procedures and propose to develop these                  methods further, including the adaptation to massively parallel                 architectures.  The goal, which seems to be in reach, is to                     obtain reliable, estimates in a few seconds, compared to the many               hours of computation time now required.                                              The detection of unusual observations and clusters of                      observations is an important part of any effective statistical                  analysis.  Such observations may be mistakes or may represent                   important new phenomena.  For data that consists of many                        simultaneous measurements, this goal has to date been out of                    reach of the best statistical methods.  We have developed                       techniques that make this goal practical, and, under this                       project, we will further develop them, together with their                      computer implementations."
"9300137","Mathematical Sciences:  Polya Trees for Nonparametric       Bayesian Analysis","DMS","STATISTICS","07/01/1993","05/07/1993","Michael Lavine","NC","Duke University","Standard Grant","James E. Gentle","12/31/1996","$60,000.00","","lavine@math.umass.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","","$0.00","     Polya trees are a class of probability models intermediate                 between the more specific Dirichlet processes and the more                      general tailfree distributions.  The proposed research project                  will investigate whether Polya trees provide workable solutions                 in some problems where Dirichlet processes are unsatisfactory.                  The hope is that the generality of Polya trees can be used to                   create models that reflect prior and posterior opinions more                    accurately while avoiding the computational complexity of the                   full class of tailfree distributions.  The proposed research                    concentrates in four areas:                                                       1. Modelling opinions of smoothness, monotonicity and                         unimodality through the joint distributions of the components of                the Polya tree,                                                                   2. Modelling predictive distributions through mixtures of Polya               trees,                                                                            3. Assessing the robustness of parametric models by exploring                 nonparametric models concentrating around parametric ones;                      assessing the consistency of the nonparametric models and                         4. Generalizing the Dirichlet process to yield more flexible                  priors for the sizes of the point masses in the random                          distribution.                                                                        A constant problem in statistics is constructing                           mathematical models for real world phenomena.  To be useful the                 models must be sufficiently sophisticated to represent essential                features of the phenomena accurately but must also be                           computationally tractable.  The proposed research project                       investigates a class of models called ""Polya trees"" that                        generalize the well known, tractable, but somewhat limited class                of models called ""Dirichlet processes.""  The hope is that Polya                 trees can represent real world phenomena more accurately than                   Dirichlet processes while remaining tractable.  The work will                   proceed partly through theoretical calculation, partly through                  computer simulation and partly through examples of Polya trees                  applied to real data."
"9304250","Mathematical Sciences:  Statistical Inference on Synaptic   Release Mechanisms","DMS","STATISTICS, NEURAL SYSTEMS CLUSTER","09/01/1993","08/05/1993","Mike West","NC","Duke University","Standard Grant","James E. Gentle","02/29/1996","$40,000.00","Dennis Turner","Mike.West@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269, 5500","","$0.00","This project will develop methods of Bayesian statistical  analysis for mixture distributions in studies of mechanisms of  neuronal synaptic activity.  Physiological analysis of synaptic  mechanisms (such as short- or long-term potentiation) is based on  presumptions regarding activity at individual synaptic sites, yet  such individual sites can rarely be analyzed directly.  Thus, to  infer baseline activity and dynamic changes at individual sites  from physiological recordings, statistical/neurophysiological  models have been developed which include the output from multiple  sites and cater for background noise. These models assume that  the recorded cell output represents a random mixture distribution  composed of synaptic signal 'components,' where each 'component'  represents one or more synaptic sites. Bayesian mixture models  and methods of analysis using (uncertain) mixtures of (uncertain  numbers of) components will be explored in this context. This  pilot project will aim to benchmark and validate the novel  Bayesian approaches in the context of experimental data from  synapses with small numbers of neural transmitter sites.  Technically, research will formulate prior distributions for  model parameters based on available synaptic noise data,  physiological evidence, and experimental conditions;  study  simulations to examine operating characteristics and  explore the sensitivity and robustness to prior and model  specifications; derive inferences about physiological parameters;  determine methods of assessment of goodness of fit; make  comparisons with existing and traditional approaches; and develop  computer software development for implementation of the modelling  techniques.    The project represents a collaborative, cross-disciplinary  initiative to develop novel methods for analysis and statistical  inference concerning the mechanisms governing electrochemical  signal transmission at neural junctions in animal nervous  systems. Interaction between the theoretical development  and  physiological data will play a critical role in identifying  and resolving uncertainties about physiological processes and  mechanisms in nervous systems, assessing site to site variability  in neural signals, synaptic output, identifying critical synaptic  characteristics, and assessing changes under differing  experimental and physiological conditions.  The project will  involve substantial analysis of existing and forthcoming  physiological data sets in order to provide assessments of the  validity of the statistical models and the derived inferences  with respect to underlying physiology.  The immediate objective  is to validate the new statistical approaches proposed by  demonstrating their application and improvements over existing  statistical analyses in synaptic response analysis. Ultimately  these methods are expected to contribute to fundamental  understanding of signalling mechanisms in human and other  nervous systems."
"9308444","Mathematical Sciences: Topics in Nonparametric Function     Estimation","DMS","STATISTICS","07/15/1993","06/23/1997","Paul Speckman","MO","University of Missouri-Columbia","Continuing Grant","James E. Gentle","12/31/1997","$59,586.00","","speckmanp@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269","0000, OTHR","$0.00","Professor Paul L. Speckman will investigate several statistical  problems in nonparametric curve and surface estimation, the  problem of recovering smooth curves and surfaces from noisy data.   There are a number of general methods for curve estimation which  have been extensively studied including kernel estimators and  smoothing splines.  This project is aimed at applications of  these methods in two specific areas, constructing confidence sets  for curves and estimating change-points (or breakpoints) in  otherwise smooth curves.  The first part of the project is aimed  at developing reliable, practical confidence bands for unknown  smooth curves.  The investigator will derive the necessary  asymptotic theory to construct confidence bands based on  cross-validated or otherwise data-driven smoothers, and he will  study their finite sample properties by computer simulation.  The  second part of the project will employ related distibutional  theory to develop methods for detection and estimation of  change-points and estimation of the magnitude of the change.  In  this context, change-points are defined to be points where jump  discontinuities in an otherwise smooth response function or its  derivatives occur.  These change-point models are useful in a  variety of applications including longitudinal studies of  patients in certain kinds of clinical trials.  Professor Paul L. Speckman will investigate several statistical  problems in an area known as nonparametric curve and surface  estimation, the problem of recovering smooth curves and surfaces  from noisy data.  Such problems arise in a wide variety of areas  of science such as economics, atmospheric sciences, engineering  and medicine.  There are a number of methods currently available  for estimating such curves.  The first problem to be addressed by  Professor Speckman will be that of giving realistic and practical  error bounds for these estimates.  This will enable investigators  using nonparametric curve estimators to know how good their   results are.  Theoretical results will be obtained and their  applicability will be studied through computer simulation.  The  second problem to be studied, known as a change-point problem,  pertains to situations where there is a sudden change in an  otherwise smooth process. One example is an industrial process  when there is a sudden shift in the quality of the output.  Other  examples come from the study of certain diseases in humans.  One  specific case in which the new methods will be tested is a  certain form of muscular dystrophy where muscle strength abruptly  begins to decline at the onset of the disease.  Techniques to be  developed in this project will enable researchers to detect when  such change-points have occurred and to estimate the size of the  change."
"9308756","Mathematical Sciences: Limit Distributions of GL-Related    Statistics and Testing Independence of Circular Data","DMS","STATISTICS","07/01/1993","06/11/1993","Shwu-Rong Shieh","MO","University of Missouri-Columbia","Standard Grant","Sallie Keller-McNulty","08/31/1995","$18,000.00","","Shieh@statb.cs.missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269","9221","$0.00","     The limit distributions of GL-statistics with kernels which                are degenerate, of infinite order, or with estimated parameters                 are proposed.  The underlying invariance principle of ``averaging               Kendall's tau on its marginals'' is also proposed. Furthermore,                 an approach to derive new test statistics from some distances for               independence of circular data is proposed. Finally, a unifying                  theme for many circular rank statistics will be investigated."
"9221304","Mathematical Sciences: 49th Session of the International    Statistical Institute, Florence, Italy 8/25-9/3/93","DMS","STATISTICS, Methodology, Measuremt & Stats","04/01/1993","03/26/1993","Barbara Bailar","VA","American Statistical Association","Standard Grant","Jean Thiebaux","03/31/1994","$20,000.00","","BarbaraB@ASA.MHS.Compuserve.Com","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269, 1333","","$0.00","     This award is for travel grants for United States                          participants to attend the 49th Session of the International                    Statistical Institute (ISI) in Florence, Italy, from August 25 to               September 3, 1993.  The funds, which will be administered by the                American Statistical Association (ASA), will provide partial                    support for transportation costs for individuals selected from                  institutions and non-profit associations.  A priority of the ASA                is to provide opportunities for junior researchers who have no                  other funding opportunities, to attend and participate in this                  important meeting.                                                                   The sections of the ISI include  (1) the Bernoulli Society                 for Mathematical Statistics and Probability, with specific groups               or committees in stochastic processes, statistics in the physical               and engineering societies, European statisticians, and Latin                    American statisticians;  (2) the International Associations for                 Survey Statisticians which holds seminars, work-shops, and                      tutorials preceding the ISI meeting;  (3) the International                     Association for Statistical Computing;  (4) the International                   Association for Official Statistics; and  (5) the International                 Association for Statistical Education.  Also meeting in                         conjunction with ISI are specific groups with interests in                      international training centers, problems of industrial                          statisticians, and other special interest groups.                                    The convergence of this broad spectrum of statisticians,                   with participants from many countries, provides an excellent                    forum to present and discuss research results and educational                   activities in statistics."
"9300997","Mathematical Sciences:  Workshop on Bayesian Statistics in  Science and Technology","DMS","INFRASTRUCTURE PROGRAM, PROBABILITY, STATISTICS","05/15/1993","05/26/1993","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Jean Thiebaux","04/30/1995","$15,000.00","Nozer Singpurwalla, James Hodges, Constantine Gatsonis","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1260, 1263, 1269","","$0.00","     As increasingly much background information becomes                        available to scientists undertaking an investigation, it is                     important to utilize previous knowledge effectively in designing                studies and analyzing data.  Bayesian statistical methods are                   tailored to this computation, but scientific meetings rarely                    spend substantial time discussing applications of Bayesian                      statistics.  The goal is to elucidate the interplay between                     theory and practice and thereby identify successful methods and                 indicate important directions for future research."
"9310322","Mathematical Sciences: Statistical Image Models","DMS","STATISTICS","07/01/1993","08/31/1993","Chuanshu Ji","NC","University of North Carolina at Chapel Hill","Standard Grant","James E. Gentle","12/31/1996","$60,000.00","","cji@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","     The proposed research focuses on statistical aspects of two  classes of image models: Gibbs random fields (GRFs) on discrete  lattices (Part I) and deformable templates on continuous domains  (Part II).  For the first class, the purpose is to fit a GRF  which generates images with analytical and visual characteristics  similar to those of natural textures.  This includes parameter  estimation and model selection: the former is an inference  problem of estimating the unknown parameter (maybe high  dimensional) contained in the energy function that induces the  GRF; the latter is a multiple decision problem of choosing an  energy function from a finite set of candidates.  Some  complicated issues, such as possible long-range dependence and  indirect observations, will also be considered.  The second class  is studied in the context of object detection from laser radar  sensors.  The particular objects considered are human faces in an  unconstrained environment.  The face detection is formulated as a  Bayesian inference problem, which consists of the prior (shape  models), the likelihood (data models) and the algorithms  (simulation from the posterior and inference based on the  posterior). In particular, the simulation will be carried out by  jump-diffusion processes and the inference is to identify the  number of object(s) corresponding to the maximal posterior  probability.       Texture is a dominant feature in representation of various  kinds of images.  The first part of the proposed research  attempts to render textures from statistical point of view.  For  instance, wood grain shows strong directional tendency but sand  patterns appear isotropic and random. Such a difference can be  represented in images created by computer graphics using  different parameters for the same model.  The second part of the  proposed research will try to answer the questions ""Is there any  specific object in the picture?"" ""If yes, how many?""  Here a  model-based approach is taken which not only provides an swers to  those questions based on the observed images but also quantifies  the chance associated with each answer.  Therefore, we should  seek answers that are more likely (with greater chance) to be  correct."
"9305588","Mathematical Sciences:  New Methodology for Predictive      Inference","DMS","STATISTICS","07/01/1993","08/09/1993","Francoise Seillier-Moiseiwitsch","NC","University of North Carolina at Chapel Hill","Standard Grant","James E. Gentle","12/31/1996","$57,000.00","","seillier@georgetown.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","A non-parametric approach is developed to select models for  categorical and survival data, with a view to helping the  elicitation of predictors and prognostic factors. The tests put  forward take the form of central limit theorems for normalized  scoring functions. The sequential aspect of the data collection  is taken into account. The goal of the research is to establish  sound theoretical foundations for these diagnostics and to  demonstrate their practicality. For survival models, the  distributional properties of residuals from the probability  integral transform conditional on sufficient statistics will also  be studied. In the event that these properties are easily  characterized, conditions, under which a sequential application  of the transformation leads to a test of predictive validity of  the model, will be sought.  The construction of empirical Bayes  confidence intervals based on structured priors is set in a  predictive framework and will be addressed via both analytical  methods and bootstrap samples. Comparison with respect to several  validation criteria will discriminate between existing and  proposed techniques. Focussing on observables, the  characteristics of prediction intervals based on empirical Bayes  distributions will be looked into.  A novel approach for the selection models for categorical and  survival data, is developed. This methodology will help in the  elicitation of predictors and prognostic factors. The sequential  aspect of the data collection is taken into account. The goal of  the research is to establish sound theoretical foundations for  these diagnostics and to demonstrate their practicality. The  construction of empirical Bayes confidence intervals based on  structured priors in a predictive framework. The construction of  these intervals will be addressed via both analytical methods and  bootstrap samples. Comparison with respect to several validation  criteria will discriminate between existing and proposed  techniques.  Focussing on observables, t he characteristics of  prediction intervals based on empirical Bayes distributions will  be looked into."
"9305045","Mathematical Sciences:  Adaptive Forcasting and Bayesian    Time Series Analysis","DMS","STATISTICS","07/01/1993","03/31/1995","Ruey Tsay","IL","University of Chicago","Continuing Grant","James E. Gentle","06/30/1996","$60,000.00","","ruey.tsay@gsb.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00","Traditional forecasting methods often assume that there is a  ""true"" model for the process under study.  Under such an  assumption, one can use maximum likelihood method to estimate the  model and then use the estimated model for forecasting. In  practice, however, there is no ""true"" model.  Consequently, the  traditional forecasting method may fare poorly in long-term  forecasts.  To overcome this difficulty, we proposed in this  project an adaptive forecasting procedure that can produce  accurate long-term forecasts. The proposed procedure selects a  statistical model for each forecasting horizon. In other words,  under the proposed procedure, different models are used to  produce different multi-step ahead forecasts. Our preliminary  results suggest that such an adaptive procedure outperforms  substantially the traditional forecasting 1 methods. In this  project, we plan to investigate properties of such an adaptive  forecasting procedure, to provide theoretical justifications of  the procedure, and to develop statistical criteria of model  selection in adaptive forecasting. The second part of the  proposed research is concerned with Bayesian time-series  analysis.  Its main objective is to develop new methodologies for  Bayesian inference that (a) make use of the recent developments  in repeated stochastic substitution such as the Gibbs sampler,  (b) allow for various structural changes in a process, and (c)  provide a flexible framework for estimating common features in a  vector process.  For instance, we consider a Markov switching  framework for modeling macroeconomic time series and for  discriminating non-nested non-linear time series models. We also  augment a probit model to a random variance-shift model to relate  the probability of a variance change to a set of chosen  explanatory variables.  These preliminary results are very  encouraging as they successfully describe many features that  cannot be captured by the traditional time-series analysis.  The proposed research consists of  two main projects. In the first  project, we propose to develop new methods that can produce  accurate long-term forecasts. The basic idea of the proposed  research is that different models are selected to produce  forecasts of different horizons. Under the proposed research,  statistical models are used adaptively in long-term forecasts.  Advantages of the proposed new forecasting methods include (a)  they relate directly model selection to the objective of  forecasts, (b) it relaxes many ""unrealistic assumptions"" commonly  imposed by the traditional forecasting methods, and (c) they make  use of the recent advances in statistical  computing. Our  preliminary experience on forecasting the U.S. monthly consumer  price index for foods indicates that the proposed methods  outperform substantially other forecasting methods available in  the literature. The second project is concerned with time-series  analysis. It emphasizes the recent developments in statistical  algorithms and the advances in computing.  The main objective is  to develop new methodologies that can incorporate prior  acknowledge on subject matters into data analysis in an efficient  manner.  It also widens the applicability of many statistical  methods by enlarging the class of possible statistical  distributions such as using scale mixture of traditional   distributions."
"9224370","Mathematical Sciences: Custom-Designed Confidence Set       Construction","DMS","STATISTICS","06/01/1993","05/05/1993","Anthony Hayter","GA","Georgia Tech Research Corporation","Standard Grant","James E. Gentle","11/30/1996","$60,000.00","","ajh@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","","$0.00","     The objective of this project is to improve the quality of                 statistical inference methods available to an experimenter                      through the use of improved and custom-designed confidence sets.                While it is clear that confidence bounds on a set of parameters                 are more informative to an experimenter than the answer to a                    hypothesis test formulation of the problem, there are many                      essentially basic statistical inference problems where current                  techniques for confidence set construction are either crude,                    inefficient or nonexistent.                                                          This may be because of a complicated distributional set-up,                because the real parameters of interest are functions of other                  basic parameters, or because the experimenter has chosen to                     emphasize certain kinds of information such as directional                      decisions.  In many settings, the experimenter has some clearly                 defined objectives which can rightly and sensibly be used to                    determine a sensitive and powerful inference method, yet will                   find out that confidence sets are currently only available for                  standard less specific inference approaches. The construction of                custom-designed confidence sets associated with these specific                  inference procedures will enable the experimenter to make a                     fuller and more complete inference than is often currently                      possible.  It is important to develop these techniques and to                   make them widely available since many decisions type inference                  are currently being made, such as in comparative drug trials, in                which the decision results are being reported (e.g., drug A is                  more effective than drug B) and where, in fact, additional free                 information (such as a confidence interval for the difference in                the efficacies of drugs A and B) is also available but is not                   being utilized.                                                                      This research will consider ways to improve current data                   analysis techniques.  The measurement of data and the subsequent                analysis of such data inevitably involves some uncertainty, and                 the objective of this research is to develop improved ways of                   dealing with this uncertainty through the science of statistical                inference.  This investigation will consider both the design of                 experiments or data collection schemes, together with the                       analysis of the data.  The main thrust of the research will be to               develop new ways of presenting the results of statistical                       analyses which are designed to be clear and intiutively                         comprehensible to the lay person.  Moreover, these methods must                 be accurate and they must efficiently summarize all the available               information.   These new techniques will be useful for                          researchers and experimenters in areas such as engineering,                     quality control, clinical trials, medical studies and other                     important fields."
"9312686","Mathematical Sciences:Collaboration Between Statistical and Atmospheric Sciences on Modeling the Climate System","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","07/15/1993","07/14/1998","Richard Katz","CO","University Corporation For Atmospheric Res","Continuing Grant","K Crank","06/30/1999","$3,600,000.00","Roland Madden","rwk@ucar.edu","3090 Center Green Drive","Boulder","CO","803012252","3034971000","MPS","1260, 1269","0000, 1303, 1577, EGCH, OTHR","$0.00","9312686  Katz       The modeling and analysis of the climate system is a complex  problem that challenges the statistical, mathematical, and  geophysical sciences.  Increased collaboration among these  sciences would result in more efficient analyses and more  accurate interpretation of data from the climate system and its  models, and in a more reliable assessment of accompanying  uncertainties.  It is proposed that a team of statistical  scientist be assembled at the National Center for Atmospheric  Research (NCAR) to help foster such a collaboration. NCAR,  basefunded by the National Sciences Foundation, is a focal point  for basic research on the atmosphere and has strong links to  universities throughout North America.  With the addition of more  statistical and mathematical expertise, it could serve as well as  a focal point for promoting collaboration among geophysical,  statistical, and other mathematical sciences.       The general objective is to develop and utilize new  techniques of data analysis that arise from the collective  understanding of physical scientists, mathematicians, and  statisticians.  The climate system has a very large number of  degrees of freedom and exhibits some chaotic behavior.   Consequently, the specific focus of the collaborative research  would be on the development of inferential methods appropriate  for high-resolution analysis of the climate system.  Such  analyses are appropriate both because realistic models of the  climate system require the treatment of smaller scale phenomena,  and because assessments of societal impact of climate require  information on regional or smaller scales.  Besides the immediate  benefits to the scientific fields directly involved, improved  understanding of the climate system would result in potential  benefits to society that would be accrued indefinitely into the  future.  For instance, it could lead to improved weather and  climate forecasts over a wide range of time scales from hours to  seasons in advance, as well as  to more realistic scenarios of  future changes in climate.  Such improvements would be of  substantial value to decision makers and policy analysts."
"9309101","Mathematical Sciences:  Model Building for Linear and Non-  linear Mixed Effects Models","DMS","STATISTICS","07/15/1993","05/08/1995","Douglas Bates","WI","University of Wisconsin-Madison","Continuing Grant","James E. Gentle","06/30/1997","$69,929.00","","bates@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","0000, OTHR","$0.00","Mixed effects models are used with data where several  observations are made on each of several different subjects and  the observations are assumed to be generated by a model function.   Some of the parameters in the model function are assumed to be  common to all subjects (fixed effects) and some vary between  subjects (random effects).  The purpose of the analysis is to  estimate the fixed effects and characterize the distribution of  the random effects. Linear mixed effects models, in which all the  parameters occur linearly in the model function, provide an  explicit objective function to define maximum likelihood or  restricted maximum likelihood estimates.  We will investigate  properties of these estimates including the asymptotic  uncorrelation of the fixed effects and the variance-covariance of  the random effects.  This work includes determining effective  parameterizations of the variance-covariance of the random  effects.  Using this we can assess the effectiveness of the  two-stage algorithm for nonlinear mixed effects models proposed  by Lindstrom and Bates.  The type of mathematical modelling which statisticians call  ""mixed effects"" has been widely applied in pharmacokinetics, the  study of the rate at which the body absorbs and eliminates drugs.   This statistical tool allows, a pharmacologist to build a  mathematical model of the pharmacokinetics of a drug in the  ""average"" person, then update that model with a few concentration  measurements on a particular person.  In this way, we augment  limited information on the individual with general information  about the population to establish more accurate dosage schedules.   This saves both time and money.  We will be developing  statistical techniques to make the development of mixed effects  models easier and more accurate."
"9309990","Mathematical Sciences: Reparameterization of Multiparameter Statisical Models","DMS","STATISTICS","07/01/1993","06/23/1993","Elizabeth Slate","NY","Cornell University","Standard Grant","Sallie Keller-McNulty","12/31/1994","$18,000.00","","eslate@fsu.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","9221","$0.00","     The research activities proposed entail studies in                         reparameterization of statistical models. Progress in this                      direction is expected to lead to efficiency in applying                         techniques of numerical optimization and integration algorithms                 to asymptotic normal approximation methods used in statistical                  analysis."
"9310228","Mathematical Sciences: Investigations in Mathematical       Statistics","DMS","STATISTICS","07/01/1993","09/09/1994","Lawrence Brown","NY","Cornell University","Continuing Grant","Sallie Keller-McNulty","06/30/1995","$93,000.00","","lbrown@wharton.upenn.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","0000, OTHR","$0.00","Statistical analysis is an essential component of scientific  and technical research and development. These analyses have broad  mathematical links in common, although many problems also require  specific techniques adapted to the particular application. This  research proposal involves investigations into aspects of this  common mathematical structure. One focus of the research is  nonparametric function estimation, which encompasses varied  applications in image processing and in electronic signal  processing and analysis.  The current research will establish  desirable general properties for such procedures. It will then  examine to what extent the procedures in common use have these  properties and, where they do not, how they should be modified.  Another goal of the research is a general theory which explains  how to construct the smallest possible confidence sets of given  confidence. The implications for specific cases will also be  investigated. A third focus is on general methods for evaluating  probabilistic forecasts, such as the predicted chance of rain.  Considerable theory has been developed for the case of a single  forecaster predicting a binomial (=""yes"" or ""no"") event.  Extensions will be proposed for the case of multinomial events  and for forecasts produced by group consensus rather than by one  individual."
"9306196","Mathematical Sciences:  Problems in Statistical Modeling","DMS","STATISTICS","06/01/1993","06/08/1993","David Ruppert","NY","Cornell University","Standard Grant","James E. Gentle","05/31/1997","$100,000.00","","dr24@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","9146","$0.00","     The project will involve work in five areas of statistical                 modeling:  (1) efficient bandwidth selectors for nonparametric                  estimation by local polynomial regression, (2) risk assessment by               orthogonal array sampling, (3) variance modeling for quality                    improvement and productivity in engineering, (4) robust                         regression with errors-in-variables, and (5) adaptive spline                    approximations in dynamic programming.                                               The principle investigator and his graduate students will                  investigate several applications of statistics to engineering and               environmental science.  The first application is to modeling the                relationship between two quantities, for example, percent carbon                in an alloy and hardness, when there is no known mathematical                   form for this relationship so that the mathematical form must be                derived from data analysis.  The second is the assessment of                    risks (say of cancer) from environmental policies when many of                  the relevant quantities (say the dose response between cancer and               dioxin) are known only with uncertainty.  The third is the design               of engineering experiments to learn how to reduce variation in                  manufacturing processes.  The fourth is in the area statistical                 modeling when some of the data are subject to gross measurement                 errors---the object is to identify the gross errors or at least                 to eliminate their effect upon the estimation of quantities of                  interest.  The fifth is a new computational method for finding                  the best method of controlling systems, for example, the ``best''               inventory policy for a business where demand is uncertain.  Here                the ``best'' policy properly balances the cost of excess                        inventory with the loss of sales when demand exceeds                            inventory."
"9304274","Mathematical Sciences:  Saddlepoint Methods in Statistics","DMS","STATISTICS","07/15/1993","04/10/1995","Ronald Butler","CO","Colorado State University","Continuing Grant","James E. Gentle","12/31/1996","$60,000.00","","rbutler@smu.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1269","0000, OTHR","$0.00","The first portion of this research is concerned with the  computation of Bayesian predictive densities in settings where  the predictive density does not have a tractable form.  Such  settings occur in stochastic network models where flowgraph  methods can be used to derive their moment generating functions.  Saddlepoint methods are then used to convert these generating  functions into densities and cumulant generating functions.   Subsequent mixing over the posterior distribution is accomplished  with either simulation or the analytic method of Laplace.  We  also develop importance sampling methods for p-value computation  in multivariate analysis.  Virtually all the common tests in this  subject area will be computable with the simulation schemes we  shall develop and the run times should amount to less than five  minutes on a work station.  Additional work dealing with  approximation of responses in non-linear stochastic systems is to  be considered.  Systems theory encompasses such things as the study of production  management, where the flow of products along an assembly line is  of interest, queuing theory, reliability of electrical networks,  flow of traffic on a highway system, etc.  The mathematical study  of these systems when they are subject to randomness is based on  an area of mathematics called transform theory.  So far transform  theory has not proven to be a particularly useful tool to the  more practical engineer for extracting practical answers about  systems.  Engineers are mostly inclined to use very  time-consuming computer simulations for this analysis.  This  research is concerned with applying new mathematical tools called  saddlepoint approximations to these problems that will allow  practical answers to be extracted from the transform theory.  Where a random system might now take 10 hours of computer  simulation to analyze, it should take only minutes on the  computer to analyze when the full power of these saddlepoint  approximations are combined with the existing pow er of transform  theory.  This research is concerned with using saddlepoint  approximations to analyze systems and for extracting practical  answers about such systems. Statistical analyses that involve  more than one variable often require that the data are analyzed  using multivariate methods.  In this setting testing the  significance of various hypotheses is not easily accomplished  since the computation of attained levels of significance is not  exact and requires approximation.  The proposed research offers  special computer simulation methods that will lead to virtually  exact computation of attained significance levels for the  majority of multivariate tests."
"9217655","Mathematical Sciences:  Mathematical and Computational      Problems in Object Recognition","DMS","PROBABILITY, STATISTICS, COMPUTATIONAL MATHEMATICS, SIGNAL PROCESSING SYS PROGRAM, ROBOTICS","07/01/1993","04/28/1995","Stuart Geman","RI","Brown University","Continuing grant","Michael Steuerwalt","08/31/1997","$510,001.00","Ulf Grenander, Basilis Gidas, Donald McClure","Stuart_Geman@Brown.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1263, 1269, 1271, 4720, 6840","0000, 9148, 9218, 9263, HPCC, MANU, OTHR","$0.00","     The research program focuses on mathematical aspects of                    object recognition.  There are two classes of problems. The first               is the recognition of rigid objects positioned in a scene at                    arbitrary rotations, locations, and scales.  A large repertoire                 of shapes is assumed known in advance, and the problem is to then               devise computationally efficient algorithms for recognizing                     which, if any, of these objects are present in a given scene.                   Sequential and adaptive strategies will be explored, in which a                 sequence of image-based observations is made, with the choice of                an observation depending upon the results of previous                           observations.  There are close connections to coding theory,                    sequential design of experiments, multi-armed bandit problems,                  the game of ""twenty questions,"" and, of course, previous work in                machine vision.                                                                      The second class of problems is the recognition of nonrigid,               or deformable, objects.  Examples include handwritten numerals                  and various biological shapes, such as leaves, hands, organelles,               etc. Here the issue of shape modeling appears to be central.  An                approach through  deformable templates is proposed.  Templates                  are prototypes which capture global characteristics, whereas                    deformations are random transformations, satisfying certain                     regularity constraints, that act upon templates to produce the                  possible presentations of an object.  The proposed shape models                 suggest certain recognition algorithms, and these will be                       explored in a variety of application areas."
"9300721","Mathematical Sciences:  Models and Structures of            Multivariate Distributions","DMS","STATISTICS","07/01/1993","04/21/1995","A. Marshall","WA","Western Washington University","Standard Grant","Stephen M. Samuels","12/31/1996","$58,232.00","","","516 High Street","Bellingham","WA","982259038","3606502884","MPS","1269","0000, 9178, 9229, OTHR","$0.00","     The large number of univariate distributions that have been                thoroughly studied sometimes have been developed for use in                     statistical analyses of data; other distributions arise from                    natural probabilistic or physical considerations, from                          statistical properties, or as limiting distributions.  Most of                  the well-known distributions arise in more than one way.                             Although univariate distributions provide information about                individual measurements, they cannot facilitate studies of                      relationships between measurements.  In many situations in the                  physical, biological, and economic sciences, it is just these                   relationships that are of interest, for which multivariate                      distributions are required.                                                          The goal of this proposal is to contribute to the                          understanding of multivariate versions of the well known                        univariate theory, with emphasis on life distributions.  This                   subject is particularly timely because recent increases in                      computational and computer graphics capabiliteies have made the                 use of multivariate distributions much more broadly practical.                  Multivariate models are now particularly poorly understood in                   terms of underlying structures that unite many multivariate                     families into cohesive units.  Meaningful derivations of                        multivariate distributions can shed light on which models to use                in different applications.  A consequence of this research is                   that flexible models useful in describing multivariate data will                become available."
"9224868","Mathematical Sciences: Asymptotic Theory for                Computationally Intensive Statistical Methods","DMS","STATISTICS","06/01/1993","05/08/1995","Pressley Millar","CA","University of California-Berkeley","Continuing grant","James E. Gentle","05/31/1997","$594,095.00","Rudolph Beran, Deborah Nolan","","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","     The advent of modern computing power has greatly enlarged                  the notion of a practical statistical procedure.  On the other                  hand, important advances in statistical theory over the past                    decade (e.g. modern semiparametrics, empirical processes,                       bootstrap theory) offer extraordinary opportunity for creativity                in the invention of effective statistical methods.  This proposal               is based upon the fact that a strong theoretical approach,                      combined with creative use of computing, can be an effective dual               attack on a number of substantive problems.  Specific problems                  proposed here include: regression with random coefficients                      (Beran, Millar), random sets arising in statistics (Millar,                     Nolan), adaptation and model selection (Beran, Nolan),                          probability forecasting via martingale methods (Nolan), the local               stochastic search method (Millar), directional data (Beran).                    Collaboration is intended when more than one investigator is                    listed.  Other topics are discussed in the individual proposals.                Our apparently disparate statistical topics are related in part                 by means of the methods with which we intend to solve them.                     These methods include: iterated bootstraps, applied empirical                   process theory, semiparametrics, minimum distance methods,                      Hajek-LeCam asymptotics, and the theory of stochastic procedures.                    The team of investigators intends to provide useful                        statistical tools for several substantive fields.  For example,                 our proposed estimation procedures in the random coefficient                    regression model are intended for use in econometrics, the                      studies of directional data are intended for use in geophysics,                 several of our proposed procedures in stochastic distance                       statistics will be used in the reliability analysis of electrical               systems.  Our proposed non-linear variants of the random                        coefficient regression model apply to the analysis of patient                   response curves to new pharmaceuticals.  Our statistical                        applications of random set theory were motivated by problems in                 optics.  Several of our graduate students will participate in the               project; indeed, two of them will incorporate their work on the                 last two subjects in their forthcoming PhD theses.  The approach                taken in this proposal involves both a relatively abstract                      analysis and a computer-intensive implementation; consequently,                 these graduate students will receive a broad and sound training                 as young statistical scientists."
"9212419","Mathematical Sciences: ""Computer Intensive Methodology in   Classification and Regression""","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS","04/01/1993","06/09/1995","Leo Breiman","CA","University of California-Berkeley","Continuing grant","Michael Steuerwalt","03/31/1997","$267,873.00","","leo@stat.Berkeley.EDU","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269, 1271","0000, 9139, 9216, 9218, 9263, HPCC, OTHR","$0.00","     This is a three year research project which has as its                     purpose the study of computationally intensive methods in                       classification (pattern recognition) and regression (noisy                      prediction) where the data is high-dimensional and nonlinear.                   The research involves new approaches to binary tree structured                  methods, with the goal of significantly improving on the accuracy               and flexiblity of the present, widely used tree structured                      methods initiated by the book and software presented by Breiman                 et. al. (1984).                                                                      The advances in methodology will be useful in the many and                 diverse areas that make use of classification and prediction.                   Current areas of interest including speech recognition, image                   processing, medical diagnosis, and handwritten and printed                      charactor recognition.                                                               There will be five areas of concentration.  Two of these are               based on recent work by Breiman (1991) for efficiently fitting                  very high dimensional noisy data by continuously joined                         hyperplane segments.  Similar algorithms will be used to                        construct trees by fitting hyperplanes in the nodes, and by using               multivariate ""ramp"" functions to construct continuous                           approximations to prediction surfaces.                                               The other three areas include a promising method for                       optimizing trees, making them multi-step optimal instead in one-                step at present, hyperplane splitting for multiple response trees               and a new method for doing penalized linear prediction.  This                   latter method, called Bridge Regression, offers hope of providing               uniformly better prediction than ordinary least squares."
"9307403","Mathematical Sciences:  Topics in Nonparametric Analysis andModel Building","DMS","STATISTICS","07/01/1993","07/09/1993","Kjell Doksum","CA","University of California-Berkeley","Standard Grant","James E. Gentle","06/30/1997","$78,000.00","","doksum@stat.wisc.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","","$0.00","It is proposed to address the question of how intuitive and  concise linear model concepts and techniques can be extended to  nonparametric settings and it is also proposed to develop  nonparametric techniques for model diagnostics that can be used  for dimensionality reduction and to address the question of  adequacy of particular models.  More precisely, it is proposed to  focus on procedures that are counterparts to such commonly used  linear model ideas as regression coefficients, correlation  coefficients, ANOVA decompositions, and principal components.  In  addition, it is proposed to consider nonparametric tools for  model building, identification, and diagnostics including tests  for linearity, partial linearity, additivity, etc. All estimators  to be considered depend on smoothing parameters needed in  estimation of curves and surfaces.  A large part of the research  will address the problem of developing reliable data-based  methods for smoothing parameter selection.  With the advent of computer data bases of unprecedented size and  complexity and with dramatic increase in computer power, it has  become increasingly more desirable and possible to develop  general models, concepts, and procedures that can be used to  study relationships between variables and to construct models  less dependent on specific assumptions. It is proposed to extend  commonly used linear model ideas and techniques to this more  general setting  and to assess the adequacy of models with  simpler structure."
"9300002","Mathematical Sciences:  Statistical Inference for Some      Dynamic and Spatial Phenomena","DMS","STATISTICS","07/01/1993","04/14/1995","David Brillinger","CA","University of California-Berkeley","Continuing grant","James E. Gentle","06/30/1997","$96,000.00","","brill@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","The research will involve a combination of theoretical  investigation and scientific data analysis."
"9305573","Mathematical Sciences:  Maximum Likelihood Methods in       Complex Sample Surveys","DMS","STATISTICS, METHOD, MEASURE & STATS","07/15/1993","09/03/1993","Abba Krieger","PA","University of Pennsylvania","Standard Grant","James E. Gentle","07/31/1996","$60,000.00","","krieger@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269, 1333","1269","$0.00","9305573  The proposed research is to study efficient ways of applying maximum likelihood  (ML) inference to complex survey data.  The methodology we intend to apply is  based on the ideas of weighted distributions.  It consists of expressing the  joint probability density functions (pdf) for units in the sample as products of  the sample inclusion probabilities, expressed as functions of the observed data,  and the marginal pdf holding in the population.  These products are normalized.   ML estimators are obtained by maximazing the resulting weighted pdf with respect  to the unknown model parameters.  Survey data are often used in the social sciences and by government agencies to  estimate parameters of interest (e.g. cell frequencies; relationships among  variables).  Often the survey is complex in the sense that the chance that an  individual is in the sample depends on many factors.  As illustrated in the  statistical and econometric literature, failure to account for the features of  the sampling design in the inference process may yield misleading results.  The  significance of the proposed research is that it will hopefully provide a unified  approach for drawing inferences from complex survey data that will account for  the known features of the sampling design."
"9308373","Mathematical Sciences: Some New Bootstrap Methods for SampleSurveys","DMS","STATISTICS","06/01/1993","05/13/1993","James Booth","FL","University of Florida","Standard Grant","James E. Gentle","05/31/1996","$34,658.00","","jb383@cornell.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","","$0.00","     Numerous resampling methods for variance estimation and                    confidence intervals in sample surveys have been proposed in the                statistical literature.  In most cases, their theoretical                       properties are not well understood and there is little evidence                 that the methods offer a significant improvement over standard                  approaches based on linearization and normal approximation.  The                proposed research involves extending recent foundational work on                resampling techniques for data obtained by stratified random                    sampling to more complex survey designs. In particular, this                    involves the development of Edgeworth expansion theory                          appropriate for double sampling and two-stage cluster sampling                  plans. A second component of the proposal describes a                           nonparametric prediction-based approach to inference in sample                  surveys which utilizes existing bootstrap methods for estimating                conditional distributions. In addition to theoretical work, a                   thorough numerical investigation of the proposed methods will be                conducted and applications to real data will be presented.                           In today's ""Information Age"" an ever-increasing quantity of                data is collected and the need for reliable data summary                        techniques has never been more critical.  Unfortunately, much of                the data encountered in practical problems does not satisfy the                 conditions necessary for standard statistical methods to work                   well.  The sheer quantity of information often prohibits or, at                 least, inhibits the identification of these potential                           difficulties and hence blind application of many statistical                    procedures is commonplace.  In addition, there is a constant need               to develop new statistical methodology which can be used to                     analyze increasingly complex sampling designs. The proposed                     research is part of an ongoing effort to develop widely                         applicable and ""robust"" statistical methods for analyzing survey                data. The proposed methods will utilize widespread access to                    faster computers allowing statistical techniques which were not                 feasible only a few years ago to be used routinely."
"9306245","Mathematical Sciences:  Topics in Nonparametric Analysis    and Model Building","DMS","STATISTICS","07/01/1993","07/13/1993","Alexander Samarov","MA","Massachusetts Institute of Technology","Standard Grant","James E. Gentle","12/31/1996","$60,000.00","","samarov@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1269","","$0.00","     It is proposed to address the question of how intuitive and                concise linear model concepts and techniques can be extended to                 nonparametric settings and it is also proposed to develop                       nonparametric techniques for model diagnostics that can be used                 for dimensionality reduction and to address the question of                     adequacy of particular models.  More precisely, it is proposed to               focus on procedures that are counterparts to such commonly used                 linear model ideas as regression coefficients, correlation                      coefficients, ANOVA decompositions, and principal components.  In               addition, it is proposed to consider nonparametric tools for                    model building, identification, and diagnostics including tests                 for linearity, partial linearity, additivity, etc. All estimators               to be considered depend on smoothing parameters needed in                       estimation of curves and surfaces.  A large part of the research                will address the problem of developing reliable data-based                      methods for smoothing parameter selection.                                           With the advent of computer data bases of unprecedented size               and complexity and with dramatic increase in computer power, it                 has become increasingly more desirable and possible to develop                  general models, concepts, and procedures that can be used to                    study relationships between variables and to construct models                   less dependent on specific assumptions. It is proposed to extend                commonly used linear model ideas and techniques to this more                    general setting  and to  assess the adequacy of models with                     simpler structure."
"9305484","Mathematical Sciences:  Break Curves and Isoklines in       Curves Estimation Models","DMS","STATISTICS","07/15/1993","02/10/1995","Hans-Georg Mueller","CA","University of California-Davis","Continuing grant","James E. Gentle","06/30/1996","$60,000.00","","hgmueller@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","0000, OTHR","$0.00","In applications like image analysis, signal detection and  ecological spatial data smooth and non-smooth parts for curves  and surfaces occur simultaneously. In the higher dimensional  case, isolated discontinuities occurring in otherwise smooth  surfaces correspond to boundaries or ""break curves"". Methods for  analyzing break curves and to estimate smooth-non-smooth surfaces  and regression functions will be developed, using curve  estimation, least squares and (local) maximum likelihood  techniques.  Applications include confidence regions for break  curves and isoklines, edge detection in image analysis, the  Poisson forest problem, and dimension reduction techniques for  high dimensional regression.  Many phenomena in biological, environmental and economic sciences  can be described by the occurrence of sharp changes when either  time or location varies. Statistical tools will be developed for  the analysis of data describing such changes. These will be of  particular interest for the detection of edges in images and for  parsimonious descriptions of high-dimensional data.  In this  context, confidence regions will be constructed which allow for  instance to find ""safe"" areas where pollutant levels are below  critical levels. The proposed methods will work under minimal  assumptions on the nature of the data."
"9310079","Mathematical Sciences:  Topics in Multivariate Survival     Analysis and Counting Processes","DMS","STATISTICS","07/01/1993","05/27/1993","Dorota Dabrowska","CA","University of California-Los Angeles","Standard Grant","Sallie Keller-McNulty","07/31/1995","$40,000.00","","dorota@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","","$0.00","The project concerns three topics:                                                                                                                              (1)  Concepts of dependence and asymmetry of multivariate                            distributions and product integrals: we consider a                              class of tests for exchangeability of multivariate                              random vectors subject to censoring.                                                                                                                            The tests are based on symmetrized hazard functions and                         derive their form from the product integral                                     representation of multivariate survival functions in                            neighborhoods contiguous to the hypothesis of                                   exchangeability. The aim of the project is to develop                           asymptotic properties of these tests.                                                                                                                      (2)  Resampling methods in estimation and prediction from                            Markov renewal processes:  we examine two different                             resampling methods in estimation of the renewal matrix                          from a censored Markov renewal process and apply them                           towards setting confidence and prediction intervals.                                                                                                       (3)  L-estimation in the accelerated failure time model: we                          develop an extension of L-estimates in a semiparametric                         linear regression model with censored and truncated                             data.                                                                                                                                                           The aim of the project is to develop new inference                              methods in models designed for statistical analysis of                          data arising in application to areas such as medicine,                          econometrics and astronomy.  Three interrelated topics                          will be considered and will deal with testing                                   hypotheses, estimation and prediction based on                                  incomplete data."
"9306658","Mathematical Sciences: Multivariate Nonparametric           Methodology Studies","DMS","STATISTICS","05/15/1993","05/27/1993","David Scott","TX","William Marsh Rice University","Standard Grant","James E. Gentle","06/30/1996","$130,500.00","","scottdw@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","9218","$0.00","     Nonparametric methodology is widely used in one and two                    dimensions, but not in high dimensions.  This research proposal                 focuses on the mid-range dimensions in an attempt to foster a                   deeper understanding of the implications of the curse of                        dimensionality.  Particular emphasis will be given to                           multivariate regression and density estimation problems, and                    closely related applications.  Anecdotal evidence suggests a gap                exists between the apparent successes of nonparametric                          methodology and the poor performance predicted by theory.  We                   will examine new points of view, especially related to adaptive                 estimation.  Higher quality estimation has often required use of                negative kernels, but recent research and shown that equivalent                 gains are possible in regions where the Hessian is indefinite,                  often in the tails which dominate in higher dimensions.  Other                  recent work suggests that cross-validation algorithms which are                 considered of marginal practical value in one dimension, improve                dramatically in the multivariate case.  We have found the many                  bandwidth selection algorithms cluster into two cases, and                      propose to characterize and investigate these classes. Dealing                  with medium dimensional data gives rise to many problems in data                visualization which we propose to investigate. Multivariate                     visualization requires aids and guides such as cognostics.  We                  plan to extend our density estimation visualization capabilities                to regression surfaces as well as applications such as visual                   clustering and discrimination. We propose to extend univariate                  ideas of mode estimation and testing based on the mode tree and                 simulation to several dimensions.  Algorithmic development for                  multiprocessor and parallel architectures will be briefly                       considered. Nonparametric methodology seems to work well in the                 hands of experts, and this research is designed to not only aid                 the expert but to facilitate the use of the methodology by a                    wider audience.                                                                      The proposer has recently completed a book on the topic of                 multivariate density and regression estimation, and related                     applications, particularly focusing on histograms and their                     logical extensions (Scott, 1992).  Difficult theoretical problems               with practical consequences still abound.  However, widespread                  application reflects the general acceptance of nonparametric                    methodology.  The growth in the field of scientific visualization               is fertile ground for these exploratory procedures.  This project               attempts to capitalize on existing investments in large data                    bases, by developing flexible techniques that attempt to extract                the maximum amount of information and structure hidden in the                   high dimensional data."
"9317464","Random Function Models and Applications","DMS","STATISTICS","07/15/1993","07/28/1994","Dennis Cox","TX","William Marsh Rice University","Continuing grant","Sallie Keller-McNulty","03/31/1996","$60,000.00","","dcox@stat.rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","0000, OTHR","$0.00","9317464  Cox    Stochastic models and methodologies for statistical inference  will be developed in the context of modern instrumentation-  intensive experimentation.  Bayesian, likelihood-based, semi- and  non-parametric methods will be investigated for prediction,  calibration, testing and constructing confidence intervals.   Efficient and numerically stable computational algorithms will be  developed and statistical properties will be investigated both  theoretically and via computer experimentation.  Much modern science relies on instrumentation-intensive  experimentation.  With instrument-generated data, the problem may  be prediction of responses for new inputs or untried experimental  conditions, or calibration of the responses for the test  settings, or understanding which components of the response bear  a scientific relationship to the input variables.  A variety of  processes may influence each observed response:  the simplest is  simple random error, other include deliberate or accidental  censoring, and the limitations due the precision of the measuring  instrument.  Proper, efficient and valuable inferences require  statistical models and inferential methodologies with correct  understandings of all these processes."
"9307499","Mathematical Sciences: Statistical Intervals","DMS","STATISTICS","08/01/1993","04/10/1995","J.T. Hwang","NY","Cornell University","Continuing grant","James E. Gentle","07/31/1997","$60,000.00","","jth5@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","0000, OTHR","$0.00","Hwang will work on two areas in statistics: a theory  concerning  the conditional analysis and a theory about prediction based on a  measurement error model. In the first part of the proposal, the  principal investigator proposes to examine the estimated  confidence approach by applying it to the Fieller's problem and  by relating it to Fisher's exact tests for contingency tables.  The second part of the research will focus on construction of  prediction intervals based on measurement error models, which  arise in many applications.  The examples that motivate the  principal investigator in this research are about the prediction  problems of compressive strengths of concrete based on  nondestructive tests.  The principal investigator will work on two areas:  conditional  inference and prediction.  For the first area, Hwang shall  determine which statistical procedures for evaluating the  probability of a particular event can take advantage of lucky  data better than others; in other words, which procedures work  better conditionally.  The event involved refers to whether a  particular interval (based on data) covers some ratios of unknown  parameters.  In the second area, Hwang's focus will be on  prediction by intervals, where the studies are more applicable to  U.S. society. The aim of the research is to construct a  prediction interval for the compressive strength of a wall, a  bridge, or a building based on a nondestructive method such as  pulse velocity of penetrating sound waves.  Success of this  project may help to determine, without destroying any part of the  bridge, whether the bridge is weak enough that it causes concern."
"9305547","Mathematical Sciences: Assessing Robustness of Inference","DMS","STATISTICS","07/01/1993","05/28/1993","George Casella","NY","Cornell Univ - State: AWDS MADE PRIOR MAY 2010","Standard Grant","James E. Gentle","06/30/1996","$126,000.00","","casella@stat.ufl.edu","373 Pine Tree Road","Ithica","NY","148502488","6072555014","MPS","1269","","$0.00","All statistical procedures result in an inference, a statement that is based on both the observed data and the assumed model. Since there are often model assumptions that are difficult (or impossible) to check, it is desirable for inferences to be robust against model assumptions. The inferences that we are most concerned about are post (data accuracy measures for confidence sets and hypothesis tests. Such measures are often derived through decision theoretic or Bayesian arguments, hence are based on assumptions about sampling distributions, loss functions, and prior distributions. Here we want to investigate the performance of accuracy estimators, using both frequentist and Bayesian criteria, as the underlying assumptions are relaxed. We are most interested in procedures constructed from default priors, for these tend to perform well under both frequentist and Bayesian scrutiny. One way that we will address the sampling robustness is to combine the default priors with an empirical likelihood to obtain a (somewhat) automatic inference robust procedure. Robustness of inference will be judged using a variety of criteria such as ranges of posterior probabilities, multiple and distance penalizing loss functions and a new (and promising) theory of dilation of probabilities."

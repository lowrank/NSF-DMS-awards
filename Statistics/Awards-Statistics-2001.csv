"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"0093208","Some Applications of Wavelets in Statistics","DMS","STATISTICS","01/01/2001","10/19/2004","Marina Vannucci","TX","Texas A&M Research Foundation","Continuing Grant","Grace Yang","12/31/2006","$250,000.00","","marina@rice.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, 1045, OTHR","$0.00","This project extends current interests of the P.I. in new directions, in both research and education. The project aims at the development of wavelet methods as well as at their use in interdisciplinary research efforts. Wavelets, the P.I.'s Ph.D. thesis subject, have become increasingly popular in many scientific fields since the discovery of orthonormal wavelet bases by<br/>Daubechies and Mallat in 1989. The work that the P.I. intends to do over the next five years addresses the application of wavelet methods in four different but interrelated areas:<br/>i) nonparametric density estimation;<br/>ii) time series;<br/>iii) dimension reduction in curve regression, and<br/>iv) interdisciplinary research in biology.<br/>Specific contributions include the development of nonparametric wavelet-based hypothesis tests; the wavelet estimation of parameters of random processes, with emphasis on long-memory;  Bayesian wavelet component selection techniques and, finally, applications of wavelet<br/>methods in the analysis of proteins and genomes.<br/><br/>In addition to the research component, the P.I. will develop a course on wavelet methods. Theory will be interlaced with applications in many areas of practical interest and lectures integrated with computer demonstrations. The course will be at the advanced graduate level, for statistics<br/>and non-statistics majors. The goal will be to train students that may subsequently do their dissertation work on wavelets. The course will also produce students capable of bringing<br/>wavelet methods in research fields other than statistics."
"0102306","Workshops: Pathways to the Future","DMS","STATISTICS","07/01/2001","06/19/2001","Lynne Billard","GA","University of Georgia Research Foundation Inc","Standard Grant"," Shulamith T. Gross","12/31/2003","$30,000.00","","lynne@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","The funding is for the workshop ""Pathways to the Future"", to be held August 10-11, 2002, in New York, NY. The workshop provides a venue for approximately 20 young female researchers in probability and statistics to interact with each other and with approximately 5 invited established researchers. The young researchers must have received their Ph.D. within the five years prior to the workshop, and will give a short presentation of their research. All participants must also have the intention to attend the Joint Statistical Meetings that will be held August 11-15, 2002, in New York, NY."
"0104195","Block Designs: Advances in Theory and Use","DMS","STATISTICS","09/01/2001","07/06/2001","John Morgan","VA","Virginia Polytechnic Institute and State University","Standard Grant","Grace Yang","08/31/2005","$172,500.00","","jpmorgan@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1269","0000, OTHR","$0.00","A problem common to many disparate fields of scientific and industrial enquiry is<br/>that of comparing a set of experimental conditions, or `treatments', when faced with<br/>heterogeneity in the units of material on which the experiment is to be performed. The<br/>technique of blocking specifically addresses this problem. Typically using values of some<br/>identifiable nuisance factor, the experimental units are partitioned into homogeneous<br/>subsets called blocks. Inferences can then be based on the more precise comparisons of<br/>measurements from the same block. The design problem is to determine which treat-<br/>ments are assigned to which units in which blocks, said assignment being driven by<br/>the desire to maximize the quality of information that will ultimately be produced.<br/>The problem becomes more complicated as more nuisance factors, with various inter-<br/>relationships, are introduced or identified. This project addresses the design problem<br/>for a variety of commonly encountered experimental settings, with two broad goals:<br/>(i) to extend the known theory for determining optimal designs for a wider range of<br/>settings than is now known or available; and (ii) to produce a comprehensive catalog<br/>of designs to be incorporated into a larger, web-based resource that will include a vari-<br/>ety of downloadable combinatorial designs useful for statisticians, mathematicians, and<br/>other scientists in academia and industry. By collecting optimal designs in a single,<br/>easily accessible resource, the use of good designs can be increased and the practice of<br/>experimental science consequently sharpened.<br/><br/>When comparing v treatments, the most commonly encountered situation across a<br/>range of scientific endeavors is that of a single blocking variable partitioning bk experi-<br/>mental units into b blocks of k units each. This project will examine optimality problems<br/>for these simple block designs, and the attendant combinatorial issues, augmenting the-<br/>ory by computation where needed, to produce designs for a practical range of parameter<br/>combinations (v; b; k), including settings where equireplication is not possible. The most<br/>frequently used designs with two blocking factors are row-column designs, in which two<br/>blocking factors can be visualized as row and columns in a rectangular array, and re-<br/>solvable designs, in which a second blocking factor partitions a simple block design into<br/>subsets of blocks each consisting of a single replicate. This project will also address<br/>problems in optimality and construction of designs in both these classes, with special<br/>emphasis on plans most demanded in practice: those with few replicates."
"0096528","Fifth North American Meeting of New Researchers in Statistics and Probability","DMS","STATISTICS","03/01/2001","08/29/2001","Victoria Chen","GA","Georgia Tech Research Corporation","Standard Grant","John Stufken","02/28/2002","$15,000.00","","vchen@uta.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","0000, OTHR","$0.00","This award provides funds to partially support the Fifth North American Meeting of New Researchers in Statistics and Probability, sponsored by the Institute of Mathematical Statistics. The primary objective is to provide a venue for interaction among new researchers in statistics and probability. The conference will take place from July 31 - August 3, 2001, on the Georgia Institute of Technology campus in Atlanta, Georgia, just prior to the annual Joint Statistical Meetings to be held in Atlanta. Participants will include approximately 50 statisticians and probabilists who have received their Ph.D. within 5 years of the conference or who expect to receive their Ph.D. within 1 year of the conference. Presentations by 4 senior speakers and roundtable discussions on the processes of publishing and grant funding (with participants from NSA, NSF, ONR, and NIH) are also part of the schedule."
"0104290","QEIB:  Statistical Issues in Combining Data for Phylogenetic Analysis","DMS","STATISTICS","07/15/2001","07/23/2001","Laura Kubatko","NM","University of New Mexico","Standard Grant","Grace Yang","06/30/2005","$125,000.00","","lkubatko@stat.osu.edu","1700 LOMAS BLVD NE STE 2200","ALBUQUERQUE","NM","871063837","5052774186","MPS","1269","0000, 1649, OTHR","$0.00","This research involves the development of methods for combining data sets from different sources with the goal of obtaining robust estimates of phylogenetic relationships.  As an example, consider the case in which evolutionary information in the form of DNA sequence data is available from several distinct genes sampled throughout the genome.  Phylogenetic trees estimated individually from each of these genes will yield estimated gene trees, trees which illustrate the evolutionary history of that particular gene.  What is often of most interest is the estimation of the species history, which may be different from the gene history.  Thus, the genetic information must be combined appropriately so that species relationships can be estimated. This research will address three main issues associated with this problem.  The first is the study of currently available tests for assessing combinability of the individual data sets, and the improvement of these existing tests.  The second component of this research involves the development of new procedures for testing for similarity in underlying evolutionary history in the datasets, and for development of methods for testing which evolutionary mechanisms (i.e., hybridization, horizontal gene transfer, etc.) might be responsible for differing underlying histories.  These newly developed tests are based on likelihood ratio statistics which compare the likelihood of a tree-like structure estimated under the assumption of a particular evolutionary force to an unrestricted likelihood.  The final component of this research is the development of appropriate methods for combining data from different sources in order to estimate the species tree.  This is achieved by modeling the probability that a set of observed gene trees would have arisen from a given species trees.  The estimated species tree is then that tree which maximizes this probability.<br/><br/>The inference of the evolutionary history of a collection of organisms based on the information contained in their DNA sequences is a problem of fundamental importance in evolutionary biology.  The abundance of DNA sequence data arising from genome sequencing projects has led to significant challenges in the inference of these phylogenetic relationships. Among these challenges is the inference of the evolutionary history of a collection of species based on DNA sequence information from several distinct genes sampled throughout the genome.  This research will address numerous aspects of this problem, including (1) the assessment of existing procedures for combining data from different genes, and the improvement of such procedures; (2) the development of methods for testing for the cause of differences in the evolutionary histories of distinct genes; and (3) the development of new procedures for combining DNA information from distinct genes with the goal of inferring species relationships.  This work has applications in the understanding of much-debated species relationships, such as the evolutionary relationships between placental mammals, marsupials, and monotremes."
"0108882","A Conference in Honor of Wayne A. Fuller","DMS","STATISTICS, Methodology, Measuremt & Stats","04/01/2001","03/21/2001","Sarah Nusser","IA","Iowa State University","Standard Grant","Marianthi Markatou","03/31/2002","$12,000.00","Yasuo Amemiya, Alicia Carriquiry","nusser@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269, 1333","0000, OTHR","$0.00","The main objective of the conference is to encourage intellectual discourse on emerging statistical topics<br/>that honor Wayne Fuller's forty plus years of contributions to statistics and fields of application. To do<br/>so, we have several goals for the conference:<br/>1. To advance statistical knowledge in the areas of time series analysis, measurement error models,<br/>and survey sampling. To achieve this goal, we have invited a distinguished slate of statisticians who<br/>will present their work in three sessions.<br/>2. To highlight the importance of applications and the impact that statistical developments have in other<br/>fields of inquiry. For this purpose, we have invited a set of researchers who will discuss the<br/>influence of Fuller's work in various areas, including dietary assessment, small area estimation, econometrics and economics.<br/>3. To provide a friendly and open environment in which young statisticians can present their work and<br/>interact with senior statisticians. We plan to achieve this goal by inviting researchers in the early<br/>stages of their careers to present a talk in one of the invited sessions or in the special poster area<br/>that will be dedicated to young investigators.<br/>4. To gather statisticians from all over the world for a two-day celebration of statistics in general and of<br/>Fuller in particular. A lively exchange of ideas and knowledge, a definition of future directions for<br/>research in these important areas, and a fostering of communication and collaboration among<br/>researchers at different stages of their careers and from different geographic locations are lasting<br/>results from the conference that we hope to produce."
"0094323","Adaptive Regression for Dependent Data by Combining Different Procedures","DMS","STATISTICS","06/01/2001","10/19/2004","Yuhong Yang","IA","Iowa State University","Continuing Grant"," Shulamith T. Gross","04/30/2005","$250,000.00","","yyang@stat.umn.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","0000, 1045, 1187, OTHR","$0.00","This proposal concerns research and education on adaptive regression<br/>when the random errors are dependent. Many procedures have been <br/>(and will be) proposed for nonparametric regression based on<br/>different assumptions. In applications, a difficulty a user often<br/>faces is the choice of the best method for the data at<br/>hand. This is specially the case for high-dimensional function<br/>estimation, where to overcome the curse of dimensionality, various <br/>parsimonious models such as projection pursuit, CART, neural nets, <br/>additive models, MARS, etc. are proposed according to different <br/>characterizations of the target function. A main interest in this <br/>research is to construct adaptive estimators by combining a<br/>collection of candidate procedures. The goal for the combined<br/>procedure is to perform automatically as well as (or nearly as well<br/>as) the best original procedure without knowing which one it is.  <br/>The random errors will be assumed to be generally dependent,<br/>including both short- and long-range cases. The effects of<br/>dependence on adaptation capability will be studied. It is<br/>anticipated that theoretically proven and computationally feasible <br/>algorithms will be proposed to combine regression procedures<br/>targeted at various characteristics of the regression function and <br/>different dependence structures for the random errors.<br/><br/>Function estimation is an important statistical tool that tries to<br/>understand accurately the functional relationships between variables based on <br/>data and it has applications in many disciplines for successfully  <br/>addressing scientific questions. In reality, observations are always<br/>subject to random noise (error) from different sources. When the<br/>random errors are dependent on each other, the dependence may<br/>disguise the functional relationship of interest. Long-range<br/>dependence refers to a situation where the errors are still highly<br/>correlated even when they occur at times or locations that are far<br/>away from each other. It is known that such a long-range dependence makes<br/>the estimation of the target function much harder. In applications,<br/>the degree of dependence between the errors is usually unknown,<br/>which makes the function estimation problem even harder. In this<br/>proposal, we intend to develop methods that adaptively handle<br/>different degrees of dependence among the errors so that the<br/>function of interest can be estimated optimally without knowing the<br/>dependence structure of the errors. The research results and related<br/>work by others on long-range dependent data will be brought to<br/>students at various levels in several statistics<br/>courses. Collaborations will be conducted with several professors at<br/>Iowa State University and their students in atmospheric science, <br/>electrical engineering, agronomy and possibly other fields to<br/>appropriately address long-range dependence phenomena, which have<br/>been encountered often and known to cause problems in data analysis <br/>with the existing statistical methods. <br/>"
"0102870","Adaptive Goodness-of-Fit Tests, Recurrent Event Models, and Models with Alternative Time-Scales","DMS","STATISTICS, EPSCoR Co-Funding","08/01/2001","07/15/2002","Edsel Pena","SC","University of South Carolina at Columbia","Continuing Grant","Xuming He","07/31/2004","$165,000.00","","pena@stat.sc.edu","1600 HAMPTON ST # 414","COLUMBIA","SC","292083403","8037777093","MPS","1269, 9150","0000, 9150, OTHR","$0.00","Abstract<br/>DMS 0102870<br/>Edsel Pena<br/><br/><br/>This project will address statistical research problems in hazard-based models in the face of incomplete data and optimal adaptive tests for these models, by investigating conditions for their existence, construction and application.  Additionally, stochastic problems in recurrent events applications will be considered.  A new and general class of recurrent events analyses that will encompass the special cases now in the literature is proposed.  This class features models that simultaneously incorporate interventions as they occur and account for the effects of concomitant variables.  The major contributions of this project are expected to enhance the theoretical understanding of inferences in a variety of hazard-based models.<br/><br/>Hazard-based models have wide application.  They are used to understand reliability of many operations, including industrial, biomedical and economic.  The procedures to be developed under this grant are primarily for the purpose of understanding the fundamental aspects of these experiences and experiments.  These models may model rare events such as flooding and industrial accidents, but also be appropriate for modeling hospital admissions and stock market cycles.  Thus, these statistical methods very useful for further research.  The Principal Investigator will enable many disadvantaged students from a large number of disciplines to participate in his education/research efforts.  <br/>"
"0102411","Developments on Quantile Regression","DMS","STATISTICS","07/01/2001","08/24/2004","Adam Martinsek","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Grace Yang","06/30/2006","$274,257.00","Stephen Portnoy","martinse@uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","0000, OTHR","$0.00","Francis Galton, the progenitor of modern regression, chided those of his statistical colleagues who ""limit their inquiries to Averages and do not revel in more comprehensive views"". Arguing that any complete analysis of the full variety of experience requires the entire distribution of a trait, not just a measure of its central tendency, he introduced the empirical quantile function as a convenient graphical device for this purpose. Unfortunately, the very success of least squares methods throughout applied statistics has obscured the need for a more complete analysis of the statistical relationship among variables. The least squares regression limits its inquiries to the conditional mean function and thus can fail to find when structural relationships in the data may depend on the size of the response.  For example, patients with long survival times may respond to treatment differently from those with average survival times; or persons with long periods of unemployment may respond to training differently from those with shorter unemployment periods.  Such differences could not be seen in standard analyses that model only the mean response. The investigators propose to extend conditional quantile functions to more complex situations, specifically to parametric and semiparametric regression quantiles for correlated or censored response variables (which are common in both examples mentioned above). The computation of the conditional quantile functions is facilitated by modern linear programming algorithms, and appropriate statistical inference can be developed through traditional large sample theory or Markov Chain Marginal Bootstrap being developed by the PI and his colleagues.<br/><br/>Conditional quantile functions help data analysts understand general heterogeneity in the population. They are often of direct interest in applications ranging from biomedical research, economic and business analyses to infrastructure studies. The proposed research is to establish a firm statistical theory for regression quantiles and provide a complete toolkit for their applications in complex problems with correlated and/or censored data."
"0102268","Limit Theorems and Statistical Inference for Ergodic Processes","DMS","STATISTICS","07/01/2001","05/06/2003","Michael Woodroofe","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Grace Yang","06/30/2005","$239,958.00","","michaelw@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","Abstract 0102268<br/>Limit Theorems and Statistical Inference for Ergodic Processes<br/><br/><br/>A major goal of the project is to develop a new approach to the change point problem in which the abrupt change of the latter is replaced by an arbitrary monotonic change.  The new procedure uses a penalized likelihood ratio statistic for testing equality of means against a non-decreasing trend, derived for independent normal observation errors.  The properties of the test can be studied in the more general context of dependent, but stationary and ergodic errors.  Applications of such procedures should be evident in the analysis of climate changes results from cataclysmic events or legal intervention, such as the required reduction on vehicle emissions.<br/><br/>Current work by the principal investigator and students has determined the asymptotic null distribution of the test statistic for stationary ergodic errors under modest conditions, thus allowing application to historical data sets, like weather data.  Remaining questions include developing a sequential analogue for applications to quality control, alternative penalizations, and estimating a variance parameter after an isotonic regression.  A second major goal of the project is to develop asymptotic distribution theory in a context that is applicable to the first.  The central limit theorem will be studied for additive functionals of a Markov chain with special attention to chains in which the current state is a function of the previous state and an independent variable.  Many linear and non-linear time series models are of this form.  Conditions for the existence of a stationary distribution have been widely studied for such processes, but there is much less work on central limit theory for their additive functionals.  The principal investigator plans to develop central limit theory in this context.  Previous work has shown that additive functionals can be written as a martingale plus a remainder term of smaller order in many cases, and then asymptotic normality can be deduced from the martingale central limit theorem.  This approach does not require Harris recurrence or other strong forms of asymptotic independence.  It will be developed, and statistical applications explored, especially applications to the modified change point problem. Other statistical applications include setting approximate confidence intervals.  In some cases, approximate confidence intervals may be obtained from a multivariate central limit theorem.  For others, it is necessary to develop tightness of empirical processes, and this question will be studied.  In highly structured models, it is possible to go beyond asymptotic normality to (Edgeworth like) asymptotic expansions from which corrected confidence intervals can be formed, intervals whose actual coverage probability converges to the nominal value at a fast rate.  A third major objective of project is to develop such expansions.  Previous work by the principal investigator, co-workers, and students has developed expansions of this nature for adaptively designed linear models and auto regressive processes.  This work will be extended to processes whose finite dimensional distributions form exponential families, a large class of processes that includes Markov Chains and many semi-Markov processes.<br/>"
"0101360","Multivariate Statistical Methodology for the Virtual Observatory","DMS","STATISTICS","06/15/2001","04/28/2003","Gutti Babu","PA","Pennsylvania State Univ University Park","Standard Grant","Grace Yang","05/31/2005","$1,026,289.00","Eric Feigelson","babu@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, 1616, 9178, 9251, OTHR, SMET","$0.00","The NAS Taylor/McKee Decadal Report on astronomy for 2000-2010 recommends as a top priority the formation of a National Virtual Observatory (NVO) to link archival data sets and catalogues from many existing astronomical surveys.  The effective use of such integrated massive data sets involves more than just access and extraction of information -- scientific understanding requires sophisticated statistical modeling of the selected data.  This effort falls under the rubric of statistical inference and includes the fields of multivariate analysis, nonparametrics, Bayesian analysis, spatial point processes, density estimation and data mining.<br/>Large-scale multiwavelength astronomical surveys present a variety of new challenging statistical and algorithmic problems that require methodological advances. The principal investigator and his colleagues address some of the critically important statistical challenges raised by the NVO. Specific approaches include: low-storage percentile estimation for large data sets, multi-resolutional K-Dimensional trees for clustering and outlier detection, and multi-dimensional goodness-of-fit tests for comparison of multivariate astronomical data sets with astrophysical models and simulations. Such an endeavor needs close collaboration of statisticians, astronomers and NVO specialists who reside at different institutions.  Developing a statistical toolkit within the NVO software environment implementing both new and existing methods is one of the central goals of this project.<br/><br/>As the data volume and complexity of astronomical findings have enormously increased in recent decades, a paradigm shift is underway in the very nature of observational astronomy.  While in the past a single astronomer might observe a handful of objects, today data mining of large digital sky archives obtained at all wavelengths of light is becoming a major mode of study.  The astronomical community thus faces a key task: to enable efficient and objective scientific exploitation of enormous multifaceted data sets.  In recognition of this need, the National<br/>Virtual Observatory (NVO) initiative has recently emerged to federate numerous large digital sky archives and develop tools to explore and understand these vast volumes of data.  The investigation here aims at developing statistical and computational methods to achieve these goals. The cross-disciplinary team, of astronomers and statisticians, brings advances in these fields into the toolbox of observational astronomy. The project seeks not only to formulate effective techniques to address NVO problems, but also to code these methods into statistical toolkits within NVO software environments for the entire astronomical community. The collaboration includes two institutions skilled in astrostatistics (Penn State and Carnegie Mellon) and an institution at the center of the NVO effort (California Institute of Technology).  The participation by graduate students and postdocs give them a rare opportunity to develop skills needed for cross-disciplinary work."
"0103886","Design and Analysis of Fractional Factorial Split-Plot Experiments in Industry","DMS","STATISTICS","07/15/2001","06/05/2003","Derek Bingham","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","06/30/2004","$128,392.00","","dbingham@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","The goal of this research is a theoretical and practical study of split-plot designs in industrial applications.  While split-plot designs maintain many of the fundamental features of industrial experimental design such as effect sparsity, effect hierarchy and effect heredity, the more complicated randomization structure impacts both the design and the analysis of the experiment. Regular and non-regular fractional factorial split-plot designs are studied.  In both cases, the fundamental problems considered are (i) defining optimality criteria for split-plot designs in industrial applications; (ii) constructing optimal split-plot designs; and (iii) analysis of split-plot designs.  Novel approaches for constructing optimal regular and non-regular fractional factorial split-plot designs will be proposed. To analyze non-regular fractional factorial split-plot designs, new Bayesian variable selection procedures are entertained.  Lastly, the connection between split-plot designs and other designs (i.e., compound orthogonal arrays and robust parameter designs) will be addressed.<br/><br/>The design and analysis of experiments have been successfully used in efforts to improve products and processes.  They have made important contributions to scientific discovery and innovation and will continue to do so for the foreseeable future.  Most recent advancements relate to experiments where the trials are performed as completely randomized designs. However, many industrial processes take place in multiple.  This experiment structure induces correlation between observations and a ""split-plot"" design results. The aim of the proposed research is a theoretical and practical study of the design and analysis of fractional factorial split-plot experiments in industrial applications.  The proposed work will develop new techniques for constructing and analyzing such experiments. The potential gains realized by performing an experiment as a split-plot design include more efficient and cost effective designs and increased power to detect some significant effects of interest. <br/> <br/><br/> <br/>"
"0096490","Statistical Challenges in Modern Astronomy III","DMS","STELLAR ASTRONOMY & ASTROPHYSC, STATISTICS","03/01/2001","01/10/2001","Gutti Babu","PA","Pennsylvania State Univ University Park","Standard Grant","John Stufken","02/28/2002","$20,000.00","Eric Feigelson","babu@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1215, 1269","0000, OTHR","$0.00","This award provides funds to support graduate students and young<br/>researchers to attend the conference Statistical Challenges in Modern<br/>Astronomy III, to be held from July 18-21, 2001, at the Penssylvania<br/>State University, University Park, PA. The primary focus of this<br/>cross-disciplinary conference is on applying advanced statistical<br/>techniques for the analysis of observational data in astronomy. One of<br/>the topics of the conference will be statistical methods for extremely<br/>large data sets and information systems. Another topic is statistical<br/>aspects of dealing with data resulting from the current revolution in<br/>observational cosmology. The organizing committee and speakers represent<br/>both communities and cross-disciplinary discussion is encouraged."
"0104443","Statistical Distances, Estimating Functions, and Mixture Models","DMS","STATISTICS","08/15/2001","03/10/2003","Bruce Lindsay","PA","Pennsylvania State Univ University Park","Continuing Grant","Grace Yang","07/31/2005","$270,000.00","","bgl@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, OTHR","$0.00","The principal investigator and his colleagues study important topics in three major areas of statistics. The work on statistical distances, the first area, is focused on developing understanding of distances as measures of loss when one is building a statistical model. Important considerations in developing this theory include the tradeoff between statistical sensitivity and robustness of interpretation. The results have wide implications in model building. The research on estimating equations, area two, targets the development of the generalized method of moment methodology into wider statistical contexts than current knowledge allows. A second topic in this area concerns the reconciliation of fixed and random effects modeling. The final area is mixture models, where a number of important applications in bioinformatics are leading to demands for improved mixture model tools.<br/><br/>This research work focuses on both theoretical and applied problems. Statistics has always relied heavily on the building of appropriate statistical models. The use of computational power has allowed statisticians to handle larger and larger data sets with increasingly sophisticated models.  In the process, there is increased need for understanding the extent to which a simplified but restrictive model might be a very good description of reality without being strictly correct; this is the basis for the investigation of statistical distances. Although theoretical in nature, this has implications throughout statistics and could lead to a more meaningful use of models. A second investigation is focused on a class of statistical methods that allow one to reduce the restrictiveness of model building assumptions; the goal is to expand the range of applicability of these methods. There are many potential applications in social sciences and medicine. The final subject of investigation springs from an increased demand for statistical tools in the area of biology, especially in genomic research; the principal investigator is applying expertise in his modeling area to three different applications. In each application the statistical model used, the mixture model, has direct scientific meaning as a tool for measuring the heterogeneity in biological processes."
"0102505","Variable Selection in High-Dimensional Modeling and Its Oracle Properties","DMS","STATISTICS","07/01/2001","06/08/2001","Runze Li","PA","Pennsylvania State Univ University Park","Standard Grant","Grace Yang","05/31/2005","$96,769.00","","rzli@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, OTHR","$0.00","High-dimensional data, such as, biotech and genetic data, financial data, satellite imagery and hyper-spectral imagery,  are commonplace in our daily life.  Indeed, high-dimensional data analysis has become an important research topic in statistics.  Variable selection is fundamental to high-dimensional statistical modeling.  Many approaches currently in use are stepwise selection procedures, which are expensive in computation and  ignore stochastic errors in the stage of selection process. This research involves a variety of data-analytic techniques for developing a unified effective variable selection procedure in high-dimensional statistical modeling. The goal of this project is to significantly enhance the availability of tools for analyzing complicated high-dimensional data.<br/><br/>In this project, penalized least squares and a penalized likelihood approach are proposed to select significant variables for various models used in high-dimensional data analysis. The proposed approach is distinguished from others since it deletes insignificant covariates by estimating their coefficients to be zero.  In the other words,  it simultaneously selects significant variables and estimates their regression coefficients, and thereby enables one to construct confidence intervals for the estimated parameters.  An algorithm is proposed for finding solutions to optimization problems involved in the penalized least squares and penalized likelihood. The rates of convergence and the sampling properties of the resulting estimators are investigated and presented.<br/>"
"0102529","Statistical Methods for Some Applied Problems","DMS","STATISTICS","07/01/2001","05/06/2003","Yehuda Vardi","NJ","Rutgers University New Brunswick","Continuing Grant","Grace Yang","06/30/2005","$172,207.00","Cun-Hui Zhang","vardi@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","<br/><br/> Abstract DMS 0102529 ----------<br/><br/><br/>           STATISTICAL METHODS FOR SOME APPLIED PROBLEMS<br/><br/>Stochastic models and methods for a number of real-life problems will be developed together with appropriate statistical inference tools. Results of the proposed research will be applicable and directly relevant to important areas of applications.  The first topic concerns statistical methodology for problems originating by users' command streams in a multi-users computer network.  Statistical modeling of such data has important applications in networks' intrusion-detection, in designing intelligent computer/internet environments with learning capabilities, and more.  The goal is to develop a practical methodology for profiling individual users or, more generally, random sequences of commands originating from a fixed given source. Our methods allow the system to recognize 'statistical signature' of users, and to flag out masqueraders who might assume the e-identity of a legitimate user. The data pose huge practical challenges, because of its shear size and complexion and our current method has very good operating characteristics<br/>(false- and missing- alarms) on two publicly available test data. <br/><br/>The second topic concerns the development of a new concept of data-depth functions, based on multivariate medians. We provided a new algorithm for calculating an important multivariate median function, and a closed form formula for the associated data depth.  The methodology will lead to robust, practical tools for multivariate data analysis, inference, regression, image processing, and more.  The third topic concerns regression analysis and  nonparametric methods for the comparisons of growth curves under informative heterogeneous censoring. In many tumor growth inhibition studies, tumor sizes are recorded over a period of time, forming a growth curve for each experimental subject. The usual noninformative-censoring model is often not applicable, because subjects could be censored out of the study due to treatments toxicity effects. We propose to develop statistical tests for the comparison of tumor growth rates and estimates of regression <br/>coefficients, in the presence of informative heterogeneous censoring. The proposed testing procedures are expected to be widely used, since they naturally correct the censorship bias, retain high efficiency and <br/>require no distributional assumption of the growth curves or the censoring mechanism. <br/><br/>"
"0103832","Nonparametric Estimation and Inference Methods for the Analysis of Longitudinal Data","DMS","STATISTICS","08/01/2001","03/14/2002","Colin Wu","MD","Johns Hopkins University","Continuing Grant","John Stufken","07/31/2002","$35,000.00","","colin@mts.jhu.edu","3400 N CHARLES ST","BALTIMORE","MD","212182608","4439971898","MPS","1269","0000, OTHR","$0.00","The aim of this project is to develop a series of statistical methods for the analysis of longitudinal data. The investigator studies the theoretical and practical properties of these methods through a series of asymptotic and simulation studies. This type of data involves either equally or unequally spaced repeated measurements over time from a collection of independent subjects. Because of the possible intra-subject correlations, two major tasks involved in a typical longitudinal analysis are: (1) to model and estimate the mean time-varying covariate effects on the response variables of interest; and (2) to quantify the possible correlations and individual effects in a statistical estimation and inference process. The investigator provides a range of nonparametric tools for accomplishing the above tasks through the investigation of four research topics: (a) developing and comparing the large sample properties of several local smoothing methods for the estimation of coefficient curves in varying coefficient models; (b) developing a class of local and global inference and model diagnostic procedures to assess the validity of parametric and semi-parametric regression models; (c) evaluating the theoretical and practical properties of the ""leave-one-subject-out"" cross-validation and other procedures for the selection of smoothing parameters; and (d) investigating the theoretical and practical properties of global approximation through a class of nonparametric mixed-effects models. In addition to the methodological publications, results of this project also include algorithms that allow for easy implementations of the developed methods. The theoretical results of this project provide useful insights for guiding the development of new statistical procedures in longitudinal analysis. The investigator and his collaborators demonstrate the usefulness and the potential impacts of their methods by applying them to a number of biomedical and epidemiological studies.  <br/><br/>The rapid development of computing technology has enabled scientists in various fields of social and natural sciences easy access to large datasets involving variables repeatedly observed over time. This type of data, known as longitudinal data, is common in biomedicine, epidemiology, economics, and sociology, among others. Statistical research plays the crucial role of providing theoretically sound and practically feasible tools for extracting useful information from the data. In biomedical and epidemiological studies, such useful information may include, for example, the effects of treatments on disease progression over time, the potential association between a mother's habit of cigarette smoking and the fetal growth pattern during pregnancy, and other findings that are of biomedical and public health interests. Despite considerable progress made by many talented researchers, there is still a large demand for more reliable and efficient modeling and diagnostic techniques, particularly in the area of initial data exploration, that are capable to handle repeated measurements. Systematic theoretical development is also needed for building a solid foundation to judge the adequacy of some existing methods and providing insights that lead to future methodological development. In the current project, the investigator evaluates the theoretical properties of a class of flexible and useful statistical models known as the varying coefficient models and, by extending his theoretical results, develops a class of new modeling approaches that are potentially superior to the existing ones in many longitudinal settings. Because the statistical theory, models and algorithms can be applied to situations where there does not exist a pre-specified parametric model, they provide valuable tools that are capable to derive statistical inferences entirely based on the data. These tools allow scientists, policy makers and researchers to draw adequate conclusions from their data without depending on pre-specified assumptions that maybe too restrictive to their settings. In a collaborative effort with other statistical and biomedical researchers, the investigator and his colleagues demonstrate the application potential of their methods by applying them to a number of biomedical and epidemiological studies and discuss the biological implications of their findings."
"0103983","Foundations of Dimension Reduction and Graphics","DMS","STATISTICS","07/01/2001","05/06/2003","Ralph Cook","MN","University of Minnesota-Twin Cities","Continuing Grant","Grace Yang","06/30/2005","$274,000.00","Sanford Weisberg","dennis@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","Regression analysis is the general area of study of how a response variable changes as one or more predictors are varied over their possible values. Regression is one of the most widely applied areas in statistical analysis, and is used for monitoring the performance of assembly lines, for determining the success or failure of social innovations, to predict the future outcomes based on passed data. Regression analysis has a long history, dating back at least 200 years. A myriad of methods for specific types of problems (e. g., problems in which the response is a survival time, or a binary variable) have been developed.  The work proposed in this project looks at regression in a very general way. It is founded on asking two questions. First, how much can be learned about dependence through using graphs? And the second question: how far can one push regression methodology without making any limiting assumptions about the nature of the problem at hand?<br/><br/>Over the last decade, substantial progress has been made on the first of these questions, summarized in two books, a theoretical summary of the area in Cook (1998a) and an applied approach to regression through graphics in Cook and Weisberg (1999a). Both theoretical and applied issues must be understood to develop methodology for regression based on graphics. The second question is important because all the existing methodology for regression through graphics is based on a few assumptions, generally concerning the distribution of the predictors. The methodology to be developed in this project will overcome the limitations that are imposed by making assumptions at the outset. In particular, the assumption that predictors must be at least approximately linearly related is not required. In addition, the method can be extended to qualitative predictors like factors.<br/>"
"0103727","Topics in Design of Scientific Experiments","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","06/15/2001","06/22/2005","Sam Hedayat","IL","University of Illinois at Chicago","Continuing Grant","Grace Yang","05/31/2007","$355,171.00","","hedayat@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1260, 1269","0000, OTHR","$0.00","ABSTRACT<br/>DMS 0103727<br/>S. Hedayat<br/><br/><br/>The investigator together with his co-researchers study and employ a combination of mathematical, statistical, and computational tools which are useful in discovering new orthogonal arrays useful for multifatorial experiments.  Proper and relevant small Chebyshev-systems and maximum principles will be developed for identifying and constructing optimal designs for non-linear models. When needed genetic algorithms will be developed for the purpose of explicitly constructing optimal and near optimal designs useful for linear and non-linear models.<br/><br/>This proposal develops new methods and theory for the purpose of collecting informative and cost effective data.  Two broad cases are considered.  Those cases where data are controlled by many factors, and those cases where medicinal chemists are seeking for natural products with chemotherapeutic and chemopreventive properties among thousands of agents.  In both cases the goal is to collect the minimum amount of data with maximum info/cost value.  The proposed research has important applications in collecting and analyzing raw optical density data used by medicinal chemists.  This research provides a mechanism for integrating, coordinating and expanding interdisciplinary interaction between statisticians and researchers in the areas of pharmaceutical and medical sciences.  The emerging uses of natural products in preventing and treating disease add urgency to a portion of the proposed research. The outcomes of this proposal will also assist scientists working on research projects sponsored by NIH Office of Dietary  Supplements (DOS) and National Centers for Complementary and Alternative Medicines (NCCAM).<br/> <br/>"
"0103607","GARCH, Diffusion, Stochastic Volatility and Wavelets","DMS","STATISTICS","07/01/2001","06/29/2001","Yazhen Wang","CT","University of Connecticut","Standard Grant","Grace Yang","06/30/2005","$121,632.00","","yzwang@stat.wisc.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","0000, OTHR","$0.00","  There are two independent strands of financial stochastic modeling: continuous-time models centered in the modern finance literature and discrete-time models in the empirical finance literature.  The continuous-time models are dominated by the diffusion which elegantly accommodates finance theory such as arbitrage and option pricing but is very hard for statistical inference.  Most of the discrete-time models are the autoregressive conditionally heteroscedastic (ARCH) and stochastic volatility (SV) models which often provide parsimonious representations for the observed discrete-time data and are relatively easier for statistical inference. It is natural to ask whether the discrete-time model can be compatible with the continuous-time model.  Not until  recent years did researchers begin to bridge the gap between the two modeling approaches and establish the weak convergence of the discrete-time ARCH model to continuous-time diffusion. Because of the weak convergence linkage, there is a general belief in financial economics and financial mathematics that the ARCH model and its diffusion limit are ``equivalent'' at all respects. Since both types of models involves unknown parameters, their practical implementation requires to estimate and test the parameters from the data. Because of the belief and ARCH's easier statistical inference, it is a common practice toapply statistical procedures derived under the ARCH model to the corresponding diffusion.  However, the claimed statistical equivalence and the employed practice are much based on blind faith and lack of adequate statistical justification. In particular, they can not be rigorously justified by the weak convergence linkage.  In this proposal PI will initiate a new research topic: study the statistical relationship between these discrete-time and continuous-time models.  Three interrelated problems will be investigated. <br/>Whether the experiment formed by observations from the ARCH model is asymptotically equivalent in terms of Le Cam's deficiency distance to an experiment comprised by observations<br/>from the diffusion limit ? Study model equivalence or nonequivalence at different frequencies (e.g. daily, weekly and monthly); Propose a wavelet stochastic volatility model for widely available high-frequency data.  The proposed research bears important computational and practical consequence.  For example, if the two models are asymptotically equivalent at certain lower frequencies, the easily obtained statistical inference based on the ARCH model can be applied to the subsample that are sampled from the diffusion data at the corresponding frequencies; because ARCH and SV models describe stationary processes and fail to account for local sharp peaks and long-memory founded in high-frequency data, the proposed wavelet model is expected to fit high-frequency data better and easily pick up high frequency features like sharp peaks, local shock, and non-stationarity as well as low frequence phenomenon such as long-memory and long term trend.<br/><br/><br/>   Stock market modeling has two types of approaches in the literature. One is continuous-time modeling that assumes a stock price to change with time continuously and obey a continuous-time stochastic process. Historically, continuous-time models based on stochastic differential equations have been developed in financial economics. Because of elegant accommodation of finance theory such as arbitrage and option pricing, modern finance theory is much  based on the continuous-time modeling. However, in reality all data are recorded only at discrete intervals. Unknown parameters in the continuous-time models need to be estimated and tested from the observed discrete-time data.  Due to the difficulty in statistical inference for the continuous time model based on the discrete data, the validity of the continuous-time modeling is not straightforward to check.  Another approach is discrete-time modeling of available discrete data. Successful discrete-time models are the autoregressive conditionally heteroscedastic (ARCH) and stochastic volatility (SV) models.  These discrete-time models often provide parsimonious representations for the observed discrete-time data, and their statistical inference is relatively easier. But the discrete-time models are statistical models in nature and are not easy to accommodate finance theory. This proposal will study the statistical compatibility of the two types of models and investigate wavelet modeling for high-frequency data. The research bears important theoretical and practical consequences. For example, the research can yield a picture on when continuous-time and discrete-time models are statistically equivalent; if equivalent, the easily obtained statistical inference procedures for thediscrete-time models can be applied to the continuous-time models; the wavelet based model is expected to fit high-frequency data better and easily pick up high frequency features like sharp peaks, local shock, and non-stationarity as well as low frequence phenomenon such as long-memory and long term trend.<br/><br/>"
"0104059","Computer-intensive Methods for Nonparametric Time Series Analysis","DMS","STATISTICS","07/01/2001","06/14/2001","Dimitris Politis","CA","University of California-San Diego","Standard Grant"," Shulamith T. Gross","06/30/2004","$94,500.00","","dpolitis@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","0000, OTHR","$0.00","Resampling and subsampling offer viable approaches to obtaining valid distributional approximations in the context of dependent data while assuming very little about the underlying stochastic mechanism.  Many important questions still need to be addressed in order for these modern approaches to be applied safely and accurately in practice.  The main issues the investigator wishes to tackle include the following:  (a) Extend the realm of applicability of subsampling by considering self-normalized statistics and/or extrema of time series with possibly heavy tails together with extrapolation/interpolation of subsampling estimators.  (b) Investigate the performance of the Local Bootstrap in forming confidence bands for conditional moments and prediction intervals for future values of a Markov process, as well as constructing hypothesis tests for time-reversibility.  (c) Show that nonparametric estimation of conditional moments via flat-top kernel smoothing is not appreciably affected by the curse of dimensionality when the underlying function is ultra-smooth.  (d) Investigate the performance of the newly proposed Local Block-Bootstrap in the case of a nonstationary series with a slowly-changing stochastic structure.  (e) Propose the Tapered Block-Bootstrap algorithm, and show that it achieves superior performance as compared to the well-known Block-Bootstrap.  (f) Propose a new block/bandwidth choice estimator with superior rate of convergence.  And finally  (g) consider the issue of a possibly integrated time series, and propose a new computer-intensive procedure, the Continuous-Path Block-Bootstrap, for statistical inference.<br/> <br/>Correlated data, such as time series and spatial data, are often encountered in many diverse scientific disciplines including economics, meteorology, electrical engineering, etc.  The general goal of this project is to further the development of computer-intensive statistical analysis methods that are applicable in the setting of correlated data but do not rely on unrealistic or unverifiable model assumptions.  Addressing this issue fruitfully will have many practical applications.  For example, in a daily series of exchange rates or stock returns spanning a decade (or more), there may be evidence that the stochastic structure of the series has not been invariant over such a long stretch of time.  Creating a practical way to model such nonstationarities and devising appropriate resampling methods for inference would be most helpful for economic applications.  For a different application, consider the problem of stochastic simulation of manufacturing systems or a Gibbs-type sampler simulation; the development of subsampling/resampling for `almost' stationary time series would be most helpful in order to assess convergence and accuracy of the simulation.  In the context of spatial statistics (e.g., mining and geostatistics, atmospheric and environmental science, etc.), the data typically correspond to measurements obtained at spatial points that are irregularly spaced.  For example, a measurement may indicate the quality or quantity of the ore found in some location X, or a measurement of precipitation or air quality at location Y during a fixed time interval.  The irregular nature of the measurement locations presents an added complication that, however, can be by-passed by specially designed versions of resampling/subsampling.<br/>"
"0102636","Boosting for Regression and Classification:  Some Views from Analogy","DMS","STATISTICS, THEORY OF COMPUTING","08/01/2001","07/25/2001","Wenxin Jiang","IL","Northwestern University","Standard Grant","Grace Yang","07/31/2004","$74,468.00","","wjiang@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1269, 2860","0000, 9216, HPCC, OTHR","$0.00","The principal investigator will study theoretical properties of boosting algorithms.  Topics include the assumption of weak hypotheses, the behavior of generalization error in the large time limit and during the process of boosting, a comparison to the optimal Bayes error, the performance in noiseless and noisy situations, overfitting and regularization, and the analogy between regression and classification boosting algorithms.  The following goals will be addressed: <br/><br/>(I).  Provide conditions and examples for the assumption of weak hypotheses to be valid, as well as some implications of the assumption on the generalization error.<br/>(II). Further understanding of the overfitting behavior and regularization methods in boosting.<br/>(III). Bring together the important recent developments in the areas of regression  (e.g., thresholding) and classification (e.g., boosting), where increasingly different sets of tools have been developed. <br/><br/>Boosting algorithms are very useful tools for combining simple prediction rules sequentially and adaptively into more powerful prediction rules, and are of mutual interest to the fields of computer science, machine learning and statistics.  A popular version of the algorithms, called AdaBoost, is shown to improve the fit on the existing data very quickly when more and more relatively simple ""rules of thumb"" are incorporated.  In addition, the algorithm also improves the prediction of new outcomes very effectively.  On the other hand, recent empirical evidence has shown that combining too many simple rules can `overfit' the existing data and deteriorate the performance in predicting new, unseen outcomes, when data are `noisy'.  This project studies important theoretical properties of boosting algorithms, based on an analogy between the regression situation (when the outcomes are continuous numbers) and the classification situation (when the outcomes are discrete classes).  This will be helpful in understanding how boosting works, in what situations, to what degree, and how to prevent `overfitting' and improve the performance when treating noisy data. <br/>"
"0102274","Applications and Extensions of Likelihood Methods","DMS","STATISTICS","08/15/2001","08/02/2001","Thomas Severini","IL","Northwestern University","Standard Grant","Grace Yang","07/31/2005","$83,919.00","","severini@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1269","0000, OTHR","$0.00","The proposed research considers several problems in the higher-order asymptotic theory of likelihood-based inference. Many higher-order approximations apply only to the case in which the underlying data have a continuous distribution. The proposed research considers the extension of these results to the case in which the underlying data have a lattice distribution. A second aspect of the research is the development of methods for models with a hierarchical structure. Likelihood methods are generally derived under the assumption that the likelihood function is correctly specified. Of course, in practice, the probability models used are often only an approximation to the true, but unknown, models. Hence, the proposed research considers the development of methods that are based on more limited assumptions, such as moment conditions.  <br/><br/>Statistical methods based on the likelihood function play a central role in statistical theory and methodology. Many of these methods are based on approximations which may have questionable accuracy in certain cases. The proposed research develops methods of approximation with generally higher accuracy.  The result is statistical methods that offer an improvement over those currently available."
"0103926","Approximate and Exact Inference Via Computer-Intensive Methods","DMS","STATISTICS","08/01/2001","08/03/2001","Joseph Romano","CA","Stanford University","Standard Grant","Grace Yang","07/31/2005","$186,000.00","","romano@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","The investigator will continue the development of inferential methods that do not rely on unrealistic or unverifiable model assumptions. Standard inferential methods rest upon strong assumptions, especially in the analysis of time series, random fields, or whenever complex dependencies must be taken into account. In contrast, resampling, subsampling, and other computer-intensive methods offer viable approaches to obtaining valid distributional approximations while assuming very little about the stochastic mechanism generating the data. In part 1 of this proposal, the investigator will address several important problems so that these bootstrap and subsampling methods can serve as good approximate methods in statistical practice. The main issues we wish to tackle include the following: further relaxing of conditions (such as slower mixing rate for long memory data and allowing for nonstationarity); more theory for irregularly spaced data; studying delicate problems where the rate of convergence depends on unknown parameters, such as in the notoriously difficult problem of autoregressive type processes with unit roots; improve the accuracy of distribution estimation by techniques such as Richardson extrapolation, and by optimal choice of block size; and pursue the development of goodness-of-fit tests in the dependent data case. These methods are especially useful in modelling of economic time series, due to the inherent difficulties caused by nonlinearity and nonstationarity. In part 2 of the proposal, the investigator will pursue the development of methods that have exact finite sample validity, such as the construction of conservative confidence regions, without the expense of losing efficiency, at least in large samples. Typical nonparametric methods are based on approximations or limit theorems, so that finite sample behavior is always an issue, and is often typically addressed by small scale simulations. In contrast, the goal here is to construct nonparametric procedures with guaranteed finite sample behavior and good efficiency.<br/><br/>The statistical analysis of data is vital in many diverse scientific disciplines: physics, engineering, acoustics, geostatistics, medicine, econometrics, seismology, law, ecology, and others. The scope of modern statistical analysis is continually expanding, as is the need for inferential methods that are valid without imposing strong model assumptions.  The investigator will continue the pursuit of the development of statistical methods that can be applied safely in practice, keeping in mind the many applications toward which such methods can fruitfully be applied.  The philosophical approach of the investigator is to develop practical methods that have a robustness of validity so that they may be applied in increasingly complex situations. The impact of this work is potentially quite large because strong inferential statements can be made without imposing strong assumptions."
"0101364","FRG: Topological methods in data analysis","DMS","PROBABILITY, TOPOLOGY, STATISTICS, COMPUTATIONAL MATHEMATICS","07/01/2001","06/19/2001","Gunnar Carlsson","CA","Stanford University","Standard Grant","Joanna Kania-Bartoszynska","06/30/2006","$996,396.00","Persi Diaconis, Joshua Tenenbaum","gunnar@math.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1263, 1267, 1269, 1271","0000, 1616, 9263, OTHR","$0.00","DMS-0101364<br/>Gunnar Carlsson<br/><br/>The overall goal of this project is to develop flexible <br/>topological methods which will allow the analysis of data <br/>which is difficult to analyze using classical linear methods. <br/>Data obtained by sampling from highly curved manifolds or <br/>singular algebraic varieties in Euclidean space are typical <br/>examples where our methods will be useful.  We intend to <br/>develop and refine two pieces of software which have been <br/>written by members of our research group, ISOMAP (Tenenbaum) <br/>and PLEX (de Silva-Carlsson).  ISOMAP is a tool for dimension <br/>reduction and parameterization of high dimensional data sets, <br/>and PLEX is a homology computing tool which we will use in <br/>locating and analyzing singular points in data sets, as well <br/>as estimating dimension in situations where standard methods <br/>do not work well.  We plan to extend the range of <br/>applicability of both tools, in the case of ISOMAP by <br/>studying embeddings into spaces with non-Euclidean metrics, <br/>and in the case of PLEX by building in the Mayer-Vietoris <br/>spectral sequence as a tool  Both ISOMAP and PLEX will be <br/>adapted for parallel computing. We will also begin the <br/>theoretical study of statistical questions relating to <br/>topology.  For instance, we will initiate the study of <br/>higher dimensional homology of subsets sampled from <br/>Euclidean space under various sampling hypotheses.  <br/>The key object of study will be the family of Cech <br/>complexes constructed using the distance function in <br/>Euclidean space together with a randomly chosen finite <br/>set of points in Euclidean space.  <br/><br/>The goal of this project is to develop tools for <br/>understanding data sets which are not easy to understand <br/>using standard methods.  This kind of data might include <br/>singular points, or might be strongly curved.  The <br/>data is also high dimensional, in the sense that each <br/>data point has many coordinates.  For instance, we might <br/>have a data set whose points each of which is an image, <br/>which has one coordinate for each pixel.  Many standard <br/>tools rely on linear approximations,  which do not work <br/>well in strongly curved or singular problems.  The kind <br/>of tools we have in mind are in part topological, in <br/>the sense that they measure more qualitative properties <br/>of the spaces involved, such as connectedness, or the <br/>number of holes in a space, and so on.  This group of <br/>methods has the capability of recognizing the number <br/>of parameters required to describe a space, without <br/>actually parameterizing it.   These methods also have the <br/>capability of recognizing singular points (like points <br/>where two non-parallel planes or non-parallel lines <br/>intersect), without actually having to construct <br/>coordinates on the space.  We will also be further <br/>developing and refining methods we have already <br/>constructed which can actually find good parameterizations <br/>for many high dimensional data sets.  Both projects will <br/>involve the adaptation for the computer of many methods <br/>which have heretofore been used in by-hand calculations <br/>for solving theoretical problems.  We will also initiate <br/>the theoretical development of topological tools in a setting <br/>which includes errors and sampling.  <br/><br/>"
"0104016","Complex Statistical Models: Theory and Methodology for Scientific Applications","DMS","STATISTICS","07/01/2001","05/06/2003","Larry Wasserman","PA","Carnegie-Mellon University","Continuing Grant","Xuming He","06/30/2004","$390,000.00","Robert Kass, Kathryn Roeder, Christopher Genovese","larry@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, OTHR","$0.00","Complex Statistical Models: Theory and Methodology<br/>for Scientific Applications<br/><br/>Larry Wasserman, Christopher Genovese, Robert E. Kass<br/>and Kathryn Roeder<br/><br/>ABSTRACT<br/><br/>This project is aimed at developing statistical theory and methodology for highly complex, possibly infinite dimensional models.  Although the methodology and theory will be quite general, we will conduct the research<br/>in the context of three scientific collaborations.  The first is  ``Characterizing Large-Scale Structure in the Universe,''  a joint project with astrophysicists and computer scientists.  The main statistical challenges are nonparametric density estimation and clustering, subject to highly non-linear constraints. The second project is ``Locating Disease Genes with Genomic Control.''  We aim to locate regions of the genome with more genetic similarity among cases (subjects with disease) than controls. These regions are candidates for containing disease genes. Finding these regions ina statistically rigorous fashion requires testing a vast number of hypotheses.  We will extend and develop recent techniques for multiple hypothesis testing.<br/>The third projects is ``Modeling Neuron Firing Patterns.'' The goal is to construct and fit models for neuron firing patterns, called spike trains. The data consist of simultaneous voltage recordings of numerous neurons which have been subjected to time-varying stimuli. The data are correlated over time and a major effort is to develop a class of models, called inhomogeneous Markov interval (IMI) process models, which can adequately represent the data.<br/><br/>Statistical methods for simple statistical models with a small number of parameters are well established.<br/>These models often do not provide an adequate representation of the phenomenon under investigation.<br/>Currently, scientists are deluged with huge volumes of high quality data. These data afford scientists the opportunity to use very complex models that more faithfully reflect reality. The researchers involved in this proposal are developing methodology and theory for analyzing data from these complex models. The methods are very general but they are being developed for applications in Astrophysics, Genetics and Neuroscience."
"0103265","Bayesian Analysis and Frequentist Interfaces","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","06/01/2001","03/18/2005","James Berger","NC","Duke University","Continuing Grant","Grace Yang","05/31/2007","$643,150.00","","berger@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1260, 1269","0000, OTHR","$0.00","James O Berger<br/><br/>Four areas in Bayesian analysis and its interfaces with frequentist statistical reasoning will be pursued.  The first is model selection, in particular the greatly needed development of automatic Bayesian methodology for choosing between models.  Other problems in model selection that will be considered include the question of optimal choice of a model when prediction is the goal, and study and comparison of large sample approximations in Bayesian model selection.  The second area of research is objective Bayesian analysis, which will culminate in the preparation of a research monograph on the subject (with associated software for practical implementation).  Included in the research that must be performed to reach this goal is the development of objective prior distributions for covariance matrices and the practically very important class of hierarchical models.  The third area is nonparametric Bayesian analysis, especially involving use of wavelets.  The final area of research that will be addressed is conditional frequentist testing, and its unification with Bayesian testing.<br/><br/>The advances in model selection and nonparametric Bayesian methodology will be utilized in the study of Cepheid variable star oscillations, which are key to establishing astronomical distances.  The work on objective Bayesian inference will impact the setting of confidence limits in physics.  The development of conditional frequentist tests will be undertaken in a variety of biostatistical settings, especially involving clinical trials.  In addition to these interdisciplinary impacts, the research will benefit education and human development through intensive training of graduate students and the incorporation of the developed methodology in statistics courses at Duke University and elsewhere.<br/><br/>"
"0102227","Research in Bayesian Analysis:  Large-scale Regression and Prediction Models with Applications in Bioinformatics and Applied Time Series","DMS","STATISTICS, Methodology, Measuremt & Stats","08/15/2001","05/01/2005","Mike West","NC","Duke University","Continuing Grant","Grace Yang","01/31/2007","$475,005.00","","Mike.West@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269, 1333","0000, OTHR","$0.00","The PI, collaborators and students develop statistical models and methods for problems involving latent structure in large and complex data sets.  Specific research topics include: new models for regression and prediction with latent variables; a range of developments involving latent structure in Bayesian regression models and associated algorithmic implementation using simulation methods; statistical aspects of machine learning algorithms for regression and classification; generalized linear and multivariate latent factor models; models with errors in predictor variables; empirical factor models. Research in these areas is partly motivated by applications in bioinformatics in the area of gene expression profiling, and related research and applications in time series in finance and communications signals processing.<br/><br/>This research is concerned with creating and implementing improved methods for the statistical analysis of large and complex data sets arising in fields such as functional genomics, finance and communications engineering.  The research goals include creating new mathematical models that can adequately represent the complex structure of increasingly large data sets, together with appropriate computational tools and algorithms for the analysis of such data using these models.  The need for substantial advances in this area of statistical research are highlighted by the analysis challenges posed by the increasingly large and complex gene expression data sets that are becoming standard in biomedical research due to rapid advances in genome technology, and major aspects of this research involve applications in functional genomics.  Additional applications of models arising from this research lie in analysis of time series data arising in areas including finance and communications. <br/>"
"0102511","Statistical Methods in the Frequency Domain","DMS","STATISTICS","07/01/2001","05/07/2003","David Stoffer","PA","University of Pittsburgh","Continuing Grant","Xuming He","06/30/2004","$269,999.00","Hernando Ombao","stoffer@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","0000, OTHR","$0.00","Abstract DMS-0102511  Stoffer & Ombao<br/><br/><br/>In this proposal, we concentrate on topics relating, in general, to statistical methods in the frequency domain. First, we propose to extend the spectral envelope methodology for stationary time series to the notion of evolutionary spectral envelope for nonstationary series.  In another project, we will direct our attention to analyzing nonstationary multiple time series and their principal components using transforms based on smooth localized complex exponentials (SLEX). In a third project, we will consider spectral analysis of time series collected in experimental designs with covariates.  The spectral envelope was first proposed as a method to analyze stationary categorical-valued time series in the frequency domain. The motivation for that research was the analysis of DNA sequences.  A common problem in analyzing long DNA sequence data is in identifying coding sequences that are dispersed throughout the sequence and separated by regions of noncoding. It is well known that DNA sequences are heterogeneous, and even within short subsequences of DNA, one encounters local behavior.  In this project, we are interested in extending the spectral envelope methodology to capture the local behavior of such sequences. To address this problem of local behavior in categorical-valued time series, we will explore using the spectral envelope in conjunction with a dyadic tree-based adaptive segmentation (TBAS) method for analyzing locally stationary processes. Our hope is that this methodology will help emphasize any harmonic feature that exists in a categorical sequence of virtually any length in a quick and automated fashion. Projects such as the human genome project have produced large amounts of data. We believe our methods will prove to be useful as a data mining technique for help in the analysis of the vast quantities of data being produced by various genome projects. <br/><br/>While the first project focuses on Fourier based methods, the second project concentrates on other techniques that will give spatial (or time) and frequency localization. Our goal, as always, is to develop computationally efficient algorithms for the analysis of large data sets. In our initial investigations, we will focus on the SLEX transform for analyzing categorical-valued nonstationary time series, but our goal is eventually to apply the technique to multiple time series (and their principal components) in general. The SLEX transform has special properties that make it ideal for analysis of nonstationary time series.  The SLEX transform is based on the SLEX basis functions which are localized in both the time and frequency domains. The SLEX transform yields a decomposition in both time and frequency and allows a choice among many orthogonal transforms. Orthogonality leads to computationally efficient procedures for automatic segmentation of nonstationary time series and will hopefully facilitate in our investigation of the theoretical elements of our proposed methodology. An orthogonal representation allows one to store the coefficients and later process them by methods such as nonlinear thresholding. Our feeling is that if the data can be reduced to a relatively small number of meaningful coefficients then these coefficients might be useful in some type of secondary statistical analysis.  In our collaborations with other scientists and physicians, we frequently encounter settings where time series, and covariates, are recorded for several subjects in an experimental design. There is an absence of a core of statistical procedures for analyzing such data, and we typically run across techniques that are cooked up in an ad hoc manner by researchers who have little technical skill or knowledge for analyzing correlated data and estimating (spectral) functions.  Our goal in this project is to develop a general, user friendly, statistical methodology that will incorporate the relevant information obtained from time series data sets recorded from several units from many groups, and where covariates may also be measured.  Our initial approach will be to exploit the relationship between spectral density estimation and generalized linear models. <br/><br/>"
"0103698","Nonparametric and Robust Multivariate Analysis via Quantile Functions","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","07/01/2001","02/26/2007","Roderick Heelis","TX","University of Texas at Dallas","Continuing Grant","Dean Evasius","06/30/2008","$304,302.00","","heelis@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1260, 1269","0000, OTHR","$0.00","This project develops concepts, perspectives, and tools leading toward a conceptually well-founded theory and methodology of nonparametric and robust data analysis in arbitrary dimension. The general framework being developed includes extensions of the traditional tools of one-dimensional analysis as well as tools unique to the higher-dimensional context.  A theory of ""median oriented quantile functions"" having probabilistic interpretations similar to univariate quantiles is being pursued.  A central approach is based on statistical depth functions.  A major secondary objective is to bring the depth-based approach to a definitive degree of completion.  Topics receiving special focus include multivariate quantile functions, depth functions, vector-valued L-statistics, matrix-valued scale statistics, generalized quantile processes, and generalized L-statistics. Linearization techniques via functional representations are used. Overall, the project advances nonparametric and robust multivariate analysis using quantile methods and addresses a range of specific applications.<br/><br/>As the scope of application of multivariate statistical modeling has widened, the treatment of multivariate probability distributions and data has become increasingly important and central.  A great deal of univariate statistical analysis is carried out in terms of percentiles, which lend themselves easily to interpretation.  The use of percentiles has become fundamental.  This project extends such methodology to higher dimension, to support a coherent and meaningful ""percentiles"" approach to the analysis and interpretation of multivariate data.  Users of multivariate statistics in diverse fields of application thus acquire a tool that is of fundamental importance and conceptually well understood.<br/>"
"0101429","FRG: Development of Geometrical and Statistical Models for Automated Object Recognition","DMS","TOPOLOGY, STATISTICS","08/01/2001","05/06/2003","Anuj Srivastava","FL","Florida State University","Continuing Grant","Grace Yang","07/31/2005","$522,037.00","Eric Klassen, David Banks, Gordon Erlebacher","anuj@stat.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1267, 1269","0000, 1616, OTHR","$0.00","Anuj Srivastava 0101429<br/><br/>Abstract<br/><br/>The proposed research will focus on developing methods for automated object recognition using tools from statistics, differential geometry and computer graphics.  The main objective is to design algorithms for recognizing (3D) objects from their (2D) camera images, with an emphasis on automated face recognition. The biggest challenge comes from the variability manifested in the images. How do we model it and what efficient procedures can be used to analyze it?  Many current methods seek dominant subspaces (e.g. PCA, ICA, Fisher discriminant) of the observed images to capture and characterize this variability. Although the hardware technology has advanced significantly for both computing and imaging,the current mathematical techniques and algorithms for computer vision remain  limited in their ability to fundamentally handle <br/>the image variability. Recent technological advances, such as 3D imaging, super fast graphics, and high-performance computing, make this project both feasible and timely.<br/><br/>Our approach builds upon the physical considerations that will lead to representations in stochastic geometry.  We highlight the physical factors behind the image variability  and propose methods to model them. A distinct advantage of modeling the physical factors is the ability to incorporate the contextual information in the resulting recognition algorithms.  In particular, we will develop (i) geometric models for facial shape variability, (ii) tools for synthetic illumination and facial rendering, and (iii) algorithms for statistical inference on these models/parameters. We use coordinate and differential geometry to characterize object shapes, pose, motion, reflectance, illumination, and their time variations, and show that these variables take values on the Lie groups and their quotient spaces.  Following the ""analysis by synthesis"" paradigm, where the observed images are statistically compared to the synthesized images, we propose inferences over the nuisance variables to seek the best match, and thus perform recognition.  In a Bayesian framework, the contextual knowledge of these physical representations can be incorporated as a prior model, to add to the observed information. The inference engine is based on the Monte-Carlo methods particularized to these representations. These stated goals require expertise from distant areas of statistics, geometry, computing, and graphics.  Through this FRG collaboration, we will create an atmosphere for synergistic, multi-disciplinary research that will support many future endeavors.<br/><br/>"
"0097981","International Statistical Institute Travel Grant","DMS","STATISTICS","01/01/2001","07/13/2002","Ray Waller","VA","American Statistical Association","Standard Grant","John Stufken","03/31/2002","$15,000.00","","rwalter@calpoly.edu","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","0000, OTHR","$0.00","Abstract<br/>DMS-0097981 PI: Ray A. Waller<br/><br/>This award provides funds for a travel grant to partially support United<br/>States participants to attend the 53rd Session of the International<br/>Statistical Institute (ISI) in Seoul, Republic of Korea, from August 22-29,<br/>2001. The ISI meeting is an umbrella meeting of a number of international<br/>statistical associations with sessions of interest to thousands of<br/>statisticians. The travel grant provides partial support to defray<br/>transportation costs for applicants from institutions and non-profit<br/>associations. An emphasis of the award is to encourage and provide the<br/>opportunity for younger statisticians to participate in the meeting. Special<br/>consideration will be given to statisticians who have received their Ph.D.<br/>in 1991 or later and to women and minorities. <br/>"
"0102961","Symposium on Case Studies in Bayesian Statistics - VI","DMS","STATISTICS","07/01/2001","07/05/2001","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Marianthi Markatou","06/30/2002","$15,000.00","","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, OTHR","$0.00","A symposium entitled ""Bayesian Statistics in Science and Technology: Case Studies VI"" will be held at Carnegie-Mellon University in Pittsburgh, Pennsylvania, on Friday September 28 and Saturday September 29, 2001.  The symposium will include two extended presentations of applications of Bayesian methods in problems in which the statistician was an integral member of the research team, and one case study of statistical methods analyzed by a panel of three experts.  Two contributed poster sessions will also be held.  The objectives of the symposium are to (i) Highlight the close interplay of statistical theory and applications in the context of substantive scientific research, (ii) Contribute to the development of Bayesian statistics, by identifying problems without standard solution, and encouraging the extension of the theory and its implementation so that possible approaches to analyses may be found. (iii) Bring to the fore the topic of reporting of Bayesian statistical analyses to the scientific community, and discuss effective and relevant means of communicating both the methods used in, and the conclusions drawn from quantitative analyses. (iv) Provide a small meeting atmosphere for young researchers and graduate students to present their work and to interact with senior colleagues, and to learn about the recent advances in implementation of Bayesian method in substantive problems. (v) Encourage the collaboration between statisticians and researchers in subject mater disciplines, by emphasizing the many challenging statistical problems that arise in the course of scientific research. (vi) Disseminate the results of the research presented at the workshop by publishing a volume containing well-documented and peer-reviewed case studies and data sets, and other selected workshop presentations. <br/><br/>As increasingly much background information becomes available to scientists undertaking an investigation, it is important to utilize previous knowledge effectively in designing studies and analyzing data. Bayesian statistical methods are tailored to this purpose. There have been many recent advances in Bayesian statistical theory and computation, but scientific meetings rarely spend substantial time discussing applications. The purpose of this symposium is to concentrate attention solely on applications of Bayesian statistics. The goal is to elucidate the interplay between theory and practice and thereby identify successful methods and indicate important directions for future research.<br/>"
"0102131","Ill-Conditioned Generalized Estimating Equations","DMS","STATISTICS, EPSCoR Co-Funding","08/01/2001","07/19/2002","Brian Marx","LA","Louisiana State University","Continuing Grant","Grace Yang","07/31/2005","$154,456.00","","bmarx@lsu.edu","202 HIMES HALL","BATON ROUGE","LA","708030001","2255782760","MPS","1269, 9150","0000, 9150, OTHR","$0.00","This project will investigate the behavior of ill-conditioned regressors with correlated response generalized estimating equations (GEE) framework.  This ill conditioning occurs when the Fisher's Information matrix is nearly singular, a situation that leads to traditional problems of 'multicollinearity'.  The resulting effect is poor prediction, large variability in the population parameters that are being estimated and poor testing.  The Principal Investigator will subdivide the alternative estimators and will then propose well-defined estimations techniques for the subclasses.<br/><br/>The estimators are used to understand regression under ill-posed conditions.  The procedures to be developed under this grant are primarily for the purpose of understanding the fundamental aspects, but also will have application to large longitudinal data sets and provide a unifying framework for generalized models and the expectation/maximization algorithm, chemometrics and genomics. The Principal Investigator will enable many students from a large number of disciplines to participate in his education/research efforts.  <br/>"
"0090166","Computational Inference, Monte Carlo, and Scientific Applications","DMS","STATISTICS","01/01/2001","05/13/2003","Wing Hung Wong","MA","Harvard University","Continuing Grant","Grace Yang","12/31/2004","$514,464.00","","whwong@stanford.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","0000, OTHR","$0.00","Title: Computational inference, Monte Carlo methods, and scientific applications<br/><br/>With the advent of automated, high-throughput experimental protocols and data collection techniques, research and discoveries in many areas of science and technology have become increasingly data driven and computation intensive. The applications motivating the research in this project arise from molecular biology, biotechnology and neural science. The rapid accumaulation of experimental data in these areas have outstriped scientists' ability to analyze them, and advanced statistical methods are needed to automate the analysis process and to exploit the complex data structure and extensive scientific knowledge underlying such studies. Computational inference refers to statistical modeling and inference procedures that rely on intensive computation to extract information from large scale data and knowledge-based models. The board, long term goal of this project is to advance the methodologies of computational inference and apply them towards the solution of several important problems in the aforementioned scientific areas. <br/><br/>A critical step in almost all large scale computational inference procedure is the study of the posterior density through Monte Carlo sampling (or the related problem of studying the likelihood function). Successful sampling leads immediately to the inference of any parameter or prediction of interest to the investigator. Thus the first specific goal of this project is to develop Monte Carlo simulation methods that are effective in sampling complex, multimodal distributions. Advances in this core computational problem will not only facilitate effective computational inference, but will also be of interest to other scientific tasks such as simulation of molecular structures and combinatorial optimization. Three approaches will be investigated: a) an evolutionary Monte Carlo approach where a population of structures are evolved and individual structures, including recombinant ones, are continuously competing for survival in the population, b) further development of sequential importance sampling and dynamic importance sampling through better methods to handle skewed weight distributions, c) multi-level computational models. Hybrid algorithms combining the above approaches will also be investigated. Some of these methods will be used to investigate the grand-challenge problem of understanding the energy landscape of protein conformation. <br/><br/>The second specific goal of this project is the development of computational inference tools for two further scientifc problems: i) multiple alignment and clustering of DNA and protein sequences based on hidden Markov models, and the use of these in the analysis of human genome coding regions, ii) the development of hierarchical computational models for low-level vision task such as texture recognition and primal sketching. <br/><br/>If successful, the methods developed in this project will enable the wider application of computational inference and will also result in direct contributions to three problems of considerable importance in the current scientific frontier."
"0074276","Mathematical Analysis of the Compositional Structure of Images","DMS","GEOMETRIC ANALYSIS, STATISTICS, COMPUTATIONAL MATHEMATICS, ROBOTICS","07/01/2001","07/08/2003","David Mumford","RI","Brown University","Continuing Grant","Junping Wang","06/30/2006","$1,830,000.00","Ulf Grenander, Stuart Geman, Basilis Gidas, Donald McClure","mumford@dam.brown.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1265, 1269, 1271, 6840","0000, 9263, OTHR","$0.00","The investigator and his colleagues study mathematical, statistical, and computational questions motivated by the problem in computer vision of defining and computing ""structural scene description,"" descriptions of the objects in a scene and their relations to each other.  The first goal is to obtain a theoretical understanding of the problem of inferring objects and their compositional structure, both in standard optical images and in laser range imagery and motion images.  The second goal is to test this understanding by developing effective algorithmsfor the statistical inference of this structure, algorithms that work with real data at reasonable speed on current computers.  The general approach is to formulate mathematical representations of patterns, typically compositional with a hierarchy of structures, to encode the variability of these structures in stochastic models, and to use the models to infer information about the image.  Standard stochastic models are rarely adequate, prompting the development of new classes of probability measures or completely new directions for traditional models.  Additionally, in the context of this application the team aims to develop mathematical, statistical, and computational ideas that are of broader use.  For example, compositional issues in vision are similar to ones in the grammars of language.  One aspect of the project studies this connection.<br/><br/>The investigator and his five colleagues continue their mathematical, statistical, and computational investigations of a range of problems motivated by image processing and computer vision.  The underlying question is simple enough: Here's an image, what is it an image of? Despite its simplicity, this is a hard question to answer.  The approach taken here is to decompose the image into different components in some hierarchical structure and to use statistical analysis to infer information about the image from the relationships of the components within the structure.  There are similarities with the grammatical structure of language, which the project explores.  Recognition of objects in an image is a fundamental problem for both computer systems and biological systems.  Advances are important for engineers developing computer vision applications, computer scientists seeking efficient algorithms in problems related to intelligent behavior of machines, and cognitive scientists studying human vision and language skills.  Results of the project are published on CD-ROM, allowing demonstration of the dynamic behavior of algorithms in ways impossible through traditional publication modes and offering new ways to exploit multi-media communications for both education and research purposes.<br/>"
"0096701","Grostat V:  Workshop on Grobner Bases and Statistics","DMS","ALGEBRA,NUMBER THEORY,AND COM, STATISTICS, EPSCoR Co-Funding","09/01/2001","03/21/2001","Ian DINWOODIE","LA","Tulane University","Standard Grant","John Stufken","08/31/2002","$10,450.00","","ihd@pdx.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","MPS","1264, 1269, 9150","0000, OTHR","$0.00","This award provides funds to partially support the international workshop Grostat V on applications of computational commutative algebra to statistics. The workshop will take place from September 4 - 6, 2001, at Tulane University in New Orleans. This is the first workshop of its kind in the United States, and will bring together researchers from Europe and the United States from the fields of algebra and statistics. The use of Grobner bases in data analysis and design of experiments forms the main focus of this interdisciplinary workshop. Approximately 20 talks will be given, including two invited lectures. A primary objective of the workshop is to promote this area of research in the United States. The workshop is also expected to have an immediate positive impact on graduate education in the Gulf South region."
"0102223","Imputation Methodology for Complex Survey Problems","DMS","STATISTICS, Methodology, Measuremt & Stats","08/15/2001","08/02/2001","Jun Shao","WI","University of Wisconsin-Madison","Standard Grant","Grace Yang","07/31/2005","$97,897.00","","shao@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269, 1333","0000, OTHR","$0.00","The proposed research focuses on imputation and variance estimation after imputation for survey data with nonresponse.  Marginal imputation (such as random hot deck imputation, nearest neighbor imputation, and random regression imputation) will be studied for the purpose of estimating population totals and quantiles.  The investigator will also study joint imputation (for estimating parameters such as the coefficients of correlation or the cell probabilities in a contingency table) and imputation under nonignorable response.  For each imputation method, variance estimation that takes nonresponse and imputation into account will be studied, using a direct derivation approach or a replication method such as the jackknife, the balanced half samples, and the bootstrap.<br/><br/>Many statistics and government agencies collect data through surveys.  Most surveys have nonresponse. Item nonresponse occurs when some sampled units cooperate in the survey but fail to provide answers to some questions. Imputation techniques, which insert values for nonrespondents, are commonly used compensation procedures for item nonresponse.  In some cases, when auxiliary information is properly used, imputation increases statistical accuracy.  An essential requirement for an imputation method is that one can obtain unbiased (or approximately unbiased) survey estimators by treating the imputed values as observed data and using the standard estimation formulas designed for the case of no nonresponse.  This requires developments on imputation methodology and statistical analysis procedures to take nonresponse and imputation into account.  Since most of the proposed research topics are motivated by problems in survey agencies such as the Census Bureau, the Bureau of Labor Statistics, Westat, and Statistics Canada, results obtained from the proposed research will have significant impacts on the imputation and variance estimation methodology for these survey agencies.<br/>"
"0114537","SRCOS/ASA Summer Research Conference in Statistics 2001","DMS","STATISTICS","04/01/2001","03/21/2001","Roger Berger","NC","North Carolina State University","Standard Grant","John Stufken","09/30/2001","$4,000.00","","roger.berger@asu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","0000, OTHR","$0.00","This award provides funds to partially support the participation of up to eight Ph.D. students at the SRCOS/ASA Summer Research Conference in Statistics, to be held June 3-6, 2001, in St. Augustine, FL. This relatively small conference will have about 50 to 60 participants, mostly from the 14-state SRCOS (Southern Regional Council on Statistics) region. The format of the meeting, with a detailed background of the presented research at the beginning of each talk, provides junior researchers and Ph.D. students with a great entry point into the various areas of research. Areas of emphasis for this year's meeting are statistics education (including distance education), outlier detection, and Markov chain Monte Carlo methods in generalized linear mixed models. The meeting facilitates interaction between junior and senior researchers, and junior researchers are encouraged to present their research either in the form of a talk or a poster session."
"0124182","Workshop:  Developments and Challenges in Mixture Models, Bump Hunting and Measurement Error Models","DMS","STATISTICS","09/01/2001","09/17/2001","Jiayang Sun","OH","Case Western Reserve University","Standard Grant","John Stufken","12/31/2002","$14,000.00","Hemant Ishwaran","jsun21@gmu.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","MPS","1269","0000, OTHR","$0.00","This award will provide funds to partially support 10 junior researchers and 5 invited speakers to participate in an international research workshop on ""Developments and Challenges in bump hunting, mixtures and measurement error models"" to be held in Cleveland, June 2-4, 2002.  The workshop has multiple purposes.  It brings together statisticians and a collection of scientists from different fields, yet all working towards similar scientific goals, to exchange new techniques and explore merging ideas.  The workshop provides a unique opportunity for young statisticians to interact with leaders in these related fields.  It invites experts with a national reputation from astronomy, biology and medicine, to discuss challenges to statistics, particularly in bump hunting, mixture modeling, clustering and measurement error models."
"0104129","Efficient Computation in Multi-level Models","DMS","STELLAR ASTRONOMY & ASTROPHYSC, OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","08/01/2001","05/09/2003","David van Dyk","MA","Harvard University","Continuing Grant","Xuming He","07/31/2004","$452,913.00","Jun Liu","dvd@uci.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1215, 1253, 1269","0000, OTHR","$0.00","EFFICIENT COMPUTATION IN MULTI-LEVEL MODELS<br/><br/>In recent years, a new trend has been growing in applied statistics---it is becoming ever more feasible to build application specific models which are designed to account for the structure inherent in any particular data generation mechanism.  Such models have long been advocated on theoretical grounds, but recently the development of new computational tools (e.g., hardware, software, and algorithms) for statistical analysis has begun to bring such model fitting into routine practice.  Of course, much work remains to be done. The flexibility of such methods comes at a cost---they require problem specific coding, long computation times, and present difficulties in ascertaining convergence.  This proposal aims to tackle some of these difficulties using newly developed efficient Monte Carlo techniques.  The PIs plan to study a number of outstanding theoretical questions concerning the behavior and extended application of these efficient methods by developing new algorithms for a number of important models which are prime candidates for these methods.  The PIs are involved in several on-going substantive data analytic projects (e.g., in computational biology and high energy astrophysics) which both help to clarify the relevant theoretical questions and stand to benefit from the new methodology.  The computational goals of this research are by no means an end unto themselves, but rather a means to improved data analysis and statistical inference. As has been so clearly illustrated in recent years improved computational tools can open up whole new areas of statistical application, as well as increase reliability, thus improving statistical inference.<br/><br/>Research will focus on such newly developed Monte Carlo techniques as multi-point Metropolis and the methods of conditional, joint, and marginal data augmentation.  Multi-point Metropolis generalizes the Metropolis-Hastings algorithm by allowing multiple dependent proposals at each iteration. As a consequence the multi-point method is more able to jump further, is less likely to be caught in a local mode, and thus can substantially improve mixing.  The methods of conditional, joint, and marginal augmentation have already substantially improved performance of the EM and Data Augmentation algorithms in a wide range of models (e.g., mixed-effects models, finite mixture models, multivariate t-models, probit generalized linear models and generalized linear mixed model, Poisson image models, etc.). In particular, these new algorithms maintain the stable convergence properties of EM and DA while sometimes reducing the required computation time by over 99%.  These methods, especially in tandem, have the potential to significantly improve and extend Markov Chain Monte Carlo in statistical practice.  This program is being jointly funded by the<br/>Division of Mathematical Sciences and Astronomical Sciences and the Office of Multidisciplinary<br/>Activities from the Directorate of Mathematical and Physical Sciences.<br/>"
"0196353","Algorithms, Approximations, and Valid Statistical Inference","DMS","STATISTICS","02/01/2001","07/03/2003","George Casella","FL","University of Florida","Continuing Grant","Xuming He","06/30/2004","$165,002.00","Martin Wells","casella@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","0000, OTHR","$0.00",""
"0103513","Semiparametric Models for Correlated Data: The Quadratic Inference Function Approach","DMS","STATISTICS, Methodology, Measuremt & Stats","07/15/2001","07/09/2001","Annie Qu","OR","Oregon State University","Standard Grant"," Shulamith T. Gross","06/30/2004","$79,141.00","","aqu2@uci.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","MPS","1269, 1333","0000, OTHR","$0.00","This research focuses on a new statistical method for the analysis of correlated data, the quadratic inference function (QIF) approach (Qu, Lindsay & Li 2000). The QIF is built on a semiparametric framework defined by a set of mean zero estimating functions, but differs from the standard estimating function approach in that there are more equations than the number of unknown parameters. The QIF has advantages compared to the estimating function approach, such as not requiring the specification of the likelihood function. It also overcomes limitations of the estimating function approach such as a lack of objective functions and likelihood functions for testing. One of the main goals of the proposed project is to explore the QIF for robustness with respect to the consistency of estimators when mean zero assumptions are not satisfied. A second goal focuses on the missing data problem, which occurs often in longitudinal data. Testing whether missing data are ignorable is still a challenging problem in general. The goodness-of-fit test for the QIF appears to be a valid test for nonignorable missing data. The third goal is to test order restricted alternative hypotheses for correlated data using the QIF. Current existing testing tools are not satisfactory and are mainly based on the likelihood function for parametric models, and therefore are not applicable for correlated data where the likelihood function is difficult to formulate. The QIF is related to the empirical likelihood (Owen, 1988) which is popular for nonparametric models. The proposed project also illustrates the Edgeworth expansion of QIF and explores how to apply the bootstrap strategy to improve testing accuracy for small samples of correlated data.<br/><br/>This research will have a significant impact and many applications in biostatistics, econometrics, and the environmental and social sciences where correlated data arise often. In particular, the QIF method substantially improves the estimation of regression parameters in generalized estimating equation settings (Liang & Zeger, 1986). Considering a real world example of air pollution for health impact assessment, even a slight difference in the regression parameter estimates can have a major impact on our health and environmental policies. Further, it is also the first effort to connect the generalized method of moments (Hansen, 1982) in econometrics to estimating functions in the statistics field. It attempts to answer a question frequently asked by econometricians: how to choose the most informative moment conditions with the lowest dimension possible. The research will also serve an educational purpose through developing a new course on longitudinal data and training of graduate students."
"0103606","Multivariate Nonparametric Methods Using Mass Concentration","DMS","STATISTICS","07/15/2001","05/06/2003","Wolfgang Polonik","CA","University of California-Davis","Continuing grant","grace yang","06/30/2005","$173,734.00","","wpolonik@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","0000, OTHR","$0.00","Nonparametric statistical methods are used in practice so far mainly for low dimensional data.  A major reason for this is the so-called ``curse of dimensionality'', meaning that the statistical performance of methods get worse with increasing dimension.  On the other hand, the steep increase in complexity when passing from dimension one to higher dimensions might not be caught adequately by parametric models.  Hence, there is a need for non- and semiparametic methods that on the one hand do not suffer too much from the curse of dimensionality, and on the other hand are computationally feasible.  The goal of this project is to develop such types of nonparametric statistical methods.  Central for this project is the observation that many important statistical problems can be formulated in terms of ``mass concentration'', thereby providing a unifying view to diverse problems with potential applications in various scientific fields.  The intuitive idea of mass concentration becomes explicitly expressed in the statistical methods developed in this project.  This makes the proposed methods transparent and intuitively accessible which supports interpretation of the outcomes.<br/><br/>Included in the project is problem of ``investigating multivariate modality''.  Different approaches will be considered.  One approach is based on a local fitting procedure, and another is based on some concavity property of a certain concentration function.  Another problem included in this project that admits a natural formulation in terms of mass concentration is ``measuring volatility or risk in financial time series'' which is a central problem of stochastic finance.  Regions with high volatility can be interpreted as regions where the volatility function is highly concentrated.  Investigating more than one explanatory variable simultaneously leads to a nontrivial multivariate problem.  Surprisingly, these quite diverse problems can be treated by closely related methods.  This underlines the usefulness of our methodology whose propagation is another inherent goal of this project.<br/>"
"0104075","Adaptive Methods for Nonparametric Classification and Regression/Supervised Learning, Inference in HMM and State Space Models and Inference in Semiparametric Models","DMS","STATISTICS","08/01/2001","06/19/2005","Peter Bickel","CA","University of California-Berkeley","Continuing grant","grace yang","07/31/2007","$630,001.00","","bickel@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","In non and semiparametric inference Bickel, in collaboration with <br/>Ritov and others, proposes to study how an increasing but <br/>proportionally vanishingly small cross validation sample can be used <br/>systematically to optimize supervised learning (classification and <br/>regression) procedures, for example ADA BOOST.  Further they propose <br/>to study a unified theory for testing of semiparametric hypotheses <br/>and develop efficient tests for bioequivalence.  In dependent data <br/>models, they propose to extend previous results on Hidden Markov <br/>models to state space models and study how procedures obtained by <br/>fitting and use of approximate likelihoods such as particle filters <br/>behave.<br/><br/>The investigator and collaborators propose to analyze and develop new <br/>effective methods for identifying (by machine) the type of a newly <br/>perceived object or predicting some feature from historical <br/>information.  This ranges from machine reading of hand written zip <br/>codes to predicting travel times of cars from one destination to <br/>another to predicting tumor type from microarray data.  In a similar <br/>direction they propose to see how well computer simulation based <br/>approximations to ideal prediction methods work in very complicated <br/>models applying to situations such as voice recognition.  Further <br/>they propose to study methods of inference bearing on questions such <br/>as whether a new drug which may be more expensive and have <br/>side-effects is sufficiently better than drugs currently in use to be <br/>authorized for distribution.<br/>"
"0112734","ITR-AP: A Program for Predicting and Understanding Data","DMS","STATISTICS, ITR SMALL GRANTS","09/15/2001","09/07/2001","Leo Breiman","CA","University of California-Berkeley","Standard Grant","grace yang","08/31/2005","$422,147.00","","leo@stat.Berkeley.EDU","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269, 1686","0000, 1686, OTHR","$0.00","<br/>ABSTRACT DMS 0112734-<br/><br/><br/>The two main uses of much of the scientific data currently being collected are first to use the data to predict the types of future objects observed by means of computerized algorithms.  For instance, one such problem is to develop algorithms that will classify the millions of stellar objects recorded on photographic images by optical and radio telescopes.  Second: to understand which variables are discriminating between different types of objects.  An example is in locating the gene activity that discriminates between cancerous and non cancerous DNA.  This project builds on a recently discovered algorithm called random forests which, when further developed and combined with interactive graphical displays, will provide an advanced tool for answering these questions.<br/><br/>Random forests is a new prediction algorithm coming from the Machine Learning context that functions by combining hundreds of randomly generated binary decision.  It has demonstrated state-of-the-art prediction accuracy on large data sets with thousands of variables.  It generates a wealth of information about the data other than the prediction.  This information can be used to estimate variable importance, clustering, density estimation etc.  To make this information more readily understandable to the user, interactive graphics such as parallel coordinates and hierarchical cluster diagrams linked together will be incorporated into the design of a program using a more highly developed random forests algorithm as the underlying engine. <br/>"
"0130819","Bayesian Formulations for Model Uncertainty","DMS","STATISTICS","08/15/2001","08/20/2001","Edward George","PA","University of Pennsylvania","Standard Grant","Rong Chen","07/31/2004","$124,744.00","","edgeorge@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, OTHR","$0.00","The investigator and his colleagues consider new directions for the development of default model uncertainty input specifications for Bayesian model selection and model averaging.  For the problem of model space prior specification, distributions are developed that dilute probability within neighborhoods of redundant models, thereby providing a more appropriate representation of ignorance.  For the problem of parameter space prior specification, the predictive properties of model averaging are investigated for various prior formulations.  In particular, focusing on the frequentist consequences of prior misspecification, new formulations are developed that maintain near-minimax behavior with only a minor degradation of predictive potential.  For Bayesian modeling of large data sets, new adaptive formulations are developed that accommodate local as well as global structure.  This includes new adaptive hierarchical modeling formulations as well as a new framework for simultaneous model and data selection.  The goals of theoretical optimality and practical feasibility are considered throughout. <br/><br/>The ultimate objective of this research is to enhance the potential of Bayesian statistical methods for discovering and modeling systematic relationships between variables in large multi-variable data sets.  The explosive growth of information technologies has led to the proliferation of such data sets across widely diverse fields in business and science.  Such methods offer a general approach towards improving explanations and predictions of many varied phenomenon such as, for example, consumer behavior, disease incidence, financial turbulence, industrial pollution and school efficiency.  Bayesian statistical methods, in particular, offer the promise of optimally distinguishing systematic structure from random noise, which is of critical importance for effective mining of large, detailed data sets.  The main thrust of this research is on the development of automatic implementations and richer formulations of these methods that will more fully exploit their statistical potential."
"0296215","Block Thresholding Methods for Adaptive Wavelet Function Estimation:  Theory and Applications","DMS","STATISTICS","07/01/2001","02/07/2002","T. Tony Cai","PA","University of Pennsylvania","Standard Grant","Marianthi Markatou","07/31/2003","$59,225.00","","tcai@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, OTHR","$0.00",""
"0103821","Differentiable Statistical Functionals and Bayes Asymptotics","DMS","PROBABILITY, STATISTICS","07/15/2001","03/10/2003","Richard Dudley","MA","Massachusetts Institute of Technology","Continuing grant"," Shulamith T. Gross","12/31/2004","$111,003.00","","rmd@math.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1263, 1269","0000, OTHR","$0.00","<br/>The investigator and co-workers are studying differentiability of functionals of empirical measures and distribution functions.  In one dimension, p-variation norms work well for Frechet differentiability. Extensions to several dimensions are being pursued.  In Bayes asymptotics, normal approximations with small relative errors are being found even for small probabilities of intermediate deviations.<br/><br/>In Bayes asymptotics, one of the goals is to choose the best of several statistical models, possibly for multiple data sets.  For example, multiple clinical trials may be done of a treatment for a disease.  Three models are that the treatment is helpful, is harmful, or makes no difference.  Further models incorporate the possibility that the treatment may have substantially different effects in different study populations.  The procedure is to begin with a noninformative prior probability distribution on each model, then adjust it based on the likelihoods from each data set.  Improved approximations of the updated probabilities are being investigated. For differentiable statistical functionals, a data set gives an approximation to a probability distribution.  If a nonlinear transformation is applied both to the true distribution function and to its approximation, one looks for a linear transformation that approximates the nonlinear one as well as possible in the neighborhood of the true distribution.  One is also looking for effective ways of bounding the discrepancy between the true and approximate distributions and their transformations. <br/>"
"0103792","Best Predictor Methods for Correlated Data","DMS","STATISTICS","07/01/2001","08/12/2004","Charles McCulloch","CA","University of California-San Francisco","Continuing grant","grace yang","06/30/2005","$139,001.00","","chuck@biostat.ucsf.edu","1855 Folsom St Ste 425","San Francisco","CA","941034249","4154762977","MPS","1269","0000, OTHR","$0.00","ABSTRACT 0103792<br/><br/>This project proposes to develop statistical methods useful for the analysis of correlated, non-normally distributed data.  Such data arises commonly in clinical trials, in familial genetic studies, in ecological studies, and in a variety of other contexts when binary or count data is gathered on the same subject, on the same family, at the same site, or from the plot of land.  Failure to account for correlations in the analysis of such data can readily lead to misleading conclusions.<br/><br/>The methods will be based on deriving unbiased estimating equations for both variance components and regression parameters within the context of a generalized linear mixed model.  The goal is to derive methods of wide utility with good small sample performance.  The performance of the newly developed methods will be assessed on their own with regard to ease of computation, lack of bias, smallness of mean square error, the ease with which accurate standard errors can be computed, and the ease of calculating accurate confidence intervals and performing hypothesis tests.  The methods will also be compared to extant methods (maximum likelihood, higher order Laplace approximations, penalized quasi-likelihood and generalized estimating equations) using the same criteria.  <br/><br/><br/>"
"0104038","Effective Dimension Reduction for Both Input and Output Variables","DMS","STATISTICS","08/15/2001","03/14/2003","Ker-Chau Li","CA","University of California-Los Angeles","Continuing grant","grace yang","07/31/2005","$235,000.00","","kcli@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","0000, OTHR","$0.00","This proposal is concerned with the analysis of large data with many dimensions. For a variety of reasons, it is often desirable to reduce the dimensionality first. Using the techniques of sliced inverse regression and principal Hessian directions as building blocks, new methods are developed for more complex applications involving many input and output variables simultaneously. When the variables consist of time series or curves, automatic basis searching systems are derived for modeling both the deterministic trends and the stochastic patterns.  <br/><br/>Scientific data from a variety of disciplines have accumulated in unprecedented volume and complexity. This is exemplified by the massive gene expression profiles generated by microarray technologies. Hidden under many public accessible rich databases is a gold mine of biological messages, awaiting genomic researchers' exploration. Powerful statistical methods from clustering and classification have been successfully applied to dig them out. But the variety of information that can be distilled is so diverse that the pursuit of new paths is more than warranted. The new methods developed in this project will meet this demand. In particular, they can be used to visualize both the local and the global interaction in gene expression, to infer metabolic circuitry and enzyme functionality, to shed light on the multi-task coordination at different stages of the cell cycle, and to explore the relationship between drug responsiveness and gene profiles."

"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"0606580","Collaborative Research: Statistical Learning and Object Oriented Data Analysis","DMS","STATISTICS","07/01/2006","04/18/2006","Jianhua Huang","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","06/30/2009","$99,165.00","","jianhua@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","This research is in the related areas of Statistical Learning and Object Oriented Data Analysis (OODA). There are major challenges in these areas that are addressed by a team of researchers, who bring different but complementary skill sets to explore. Statistical Learning is widely recognized as a very active area of interdisciplinary research, which lives between statistics, computer science, and optimization. With state-of-art optimization tools, this research offers a set of new approaches for statistical learning, including new penalties for regularization, further developments of large margin classifiers both theoretically and numerically, as well as nonparametric-based probability calibration for hard margin classifiers. In addition, new visualization and analytical tools for ``High Dimension-Low Sample Size'' (HDLSS) data are developed. Such development is extremely important since HDLSS has become a common feature of data encountered in many divergent fields such as medical imaging and micro-array analysis for gene expression but is outside of the domain of classical statistical multivariate analysis. OODA is a generalization of the recently very productive area of Functional Data Analysis (FDA). In FDA, curves are data points and variation in a family of curves is the focus of analysis. OODA extends this notion to populations where the data points are more complex objects, such as images, shape representations, and even tree-structured objects. The proposed research offers a set of new tools for FDA, including exponential family functional principal components analysis (PCA), robust functional PCA, curve discrimination, and forecasting and dynamic updating of time series of curves. Proposed research will also advance OODA for data on smooth manifolds and tree-structured objects.<br/><br/>The main application area of the research is in health and medicine and civil infrastructure. The research is motivated by and will have beneficial impacts on cancer research, medical imaging, call center management, and network traffic modeling.  However, the developed statistical methods will be useful in fields far beyond those motivating this research, such as demography/epidemiology, financial economics and spatial-temporal modeling.  The team consists of a good mix of well established senior researchers and young junior researchers. Strong mentoring at several levels is an important component of this project. First, there is  strong training of graduate students, in these exciting new research areas, with the goal of giving them the background, and skills needed to start their own research careers. Second, there is strong mentoring of the junior researchers, by the more experienced members of the research team.  In addition to working closely together on research projects, the junior researchers will learn the skills of advising PhD students, through joint supervision together with the more senior members. The team  continues to disseminate the research results quickly and broadly through collaborative work, academic presentations, and journal publications. Web pages are created to enable quick access to user-friendly and accessible software implementations of new methods as well as technical reports and relevant references.<br/>"
"0604801","Cluster-Based Bootstrapping in Multiple Hypotheses Testing","DMS","STATISTICS","06/01/2006","04/28/2006","Jeffrey Hart","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","05/31/2010","$97,000.00","","hart@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","<br/>       Cluster-Based Bootstrapping in Multiple Hypotheses Testing<br/><br/>  The subject of this investigation is nonparametric methodology designed to provide sound inferences in large-scale multiple testing problems. In particular, situations are considered in which one observes a large<br/>number of small data sets and wishes to test as many hypotheses as there are data sets. A primary concern is ensuring validity of tests when the data generating mechanism is largely unknown. A fundamental model considered is one wherein the distribution of data within small data sets is the same, up to location and scale, for all data sets. This distribution is assumed to determine the sampling distributions of all test statistics, and hence, given an estimate of the within-data-sets distribution, the bootstrap can be used to estimate the requisite sampling distributions. Cluster analysis is investigated as a means of nonparametrically estimating the common within-data-sets distribution and also the joint distribution of location and scale parameters across data sets. Asymptotic properties of such estimates are investigated. These asymptotics allow the number of small data sets to tend to infinity, but bound the sizes of individual data sets. Extensions to models where the distributions across data sets differ with respect to more than just location and scale are also<br/>considered.  A key idea in these extensions is defining and consistently estimating one or a small number of reference distributions that define critical values for all test statistics. This allows one to use existing technology to control the false discovery rate even though all test statistics have different sampling distributions, none of which can be estimated consistently.<br/><br/>  Important areas of application for the research funded by this grant are microarray analysis and proteomics, both of which provide enormous insight into the study of genetics.  The methods investigated have the potential of improving methods of analyzing microarray and proteomics data. Genetics has had and will continue to have a tremendous impact on society, particularly in the area of medicine. Therefore, any method that improves upon existing technology for analyzing genetics data has the potential of enhancing the general<br/>quality of life.<br/><br/><br/><br/>"
"0605132","Long Memory Time Series Modelling: Computational and Statistical Efficiency, Nonstationarity/Noninvertibility and Goodness of Fit","DMS","STATISTICS","08/01/2006","02/28/2006","Willa Chen","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","07/31/2010","$116,292.00","","wchen@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","In view of the rapid expansion of the application of long memory models and growing interest in high frequency data, the research plan of this project stresses both statistical and computational efficiency in modern long memory modelling. The goal is to develop statistically sound and computationally efficient methods for long memory time series in estimation and goodness-of-fit testing. To reach this goal, the investigator focuses on the three lines of research.  The first line of research concerns the semiparametric estimation of memory parameter of nonstationary time series. Nonstationarity is a very common feature in economics, finance and network traffic data. Our goal is to develop estimators that allow nonstationarity in series and retain the same efficiency as the stationary case without compromising the computational efficiency.  The second line of research focuses the estimation of a full parametric long memory model, namely ARFIMA which is potentially noninvertible. In practice, time series are often differenced one or more times to induce stationarity. Sometimes overdifferencing may occur. The investigator considers both frequency and time domains MLE and derives the asymptotic properties for them. The investigator also provides an efficient algorithm to evaluate the likelihood function so that the exact MLE can be applied on a large data set. The availability of high-frequency data on returns of financial assets has intrigued a great amount of research in volatility modelling. In the last line of research, the investigator extends a few statistical procedures which were previously developed for the linear processes to stochastic volatility models. The investigator proposes a class of goodness-of-fit tests for stochastic volatility models. These tests are periodogram-based statistics that not only circumvent the computation of fitted residuals but also can be evaluated via the fast Fourier transform algorithm. Furthermore, they are asymptotically normal under the null and consistent against a wide class of alternatives.  Lastly, the investigator studies the relation between two or more long memory stochastic volatility series, a relationship called fractional cointegration. The investigator proposes an estimator for the cointegration parameter and derives its consistency.  The investigator studies the finite sample properties of the proposed procedures through simulation studies.<br/><br/><br/>This research is motivated by the emerging trend to analyze high frequency time series including economic and financial data. The proposed methods not only provide practitioners feasible statistical procedures in modelling time dependent data across a number of disciplines, but also lead a direction of future research in time series toward computational efficiency. The up-to-date developments in the research plan will be modified and transform into the applied time series course for the new coming online graduate program which will be launched in year 2006 by the Department of Statistics for serving the higher education needs of professionals who work in industrial statistics, biostatistics and statistical teaching, including minorities, women and older students with families and full-time jobs. The programs can help students, especially those who might otherwise not be able to do so, achieve their goals of personal enrichment and career advancement.<br/>"
"0605001","Wavelet-based Statistical Modeling and Applications","DMS","STATISTICS","09/01/2006","06/26/2007","Marina Vannucci","TX","Texas A&M Research Foundation","Continuing Grant","Grace Yang","07/31/2008","$79,041.00","","marina@rice.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","This proposal summarizes current interests of the P.I. and future directions, in both research and education.  Topics all involve the development of wavelet-based methods and represent natural extensions of the P.I.'s previous work. The three main areas of interest are: (1) Bayesian Clustering of Functional Data. The objective is to develop novel Bayesian methods for clustering of functional data. The approach proposed by the P.I. is model-based and uses infinite mixture models together with the selection of wavelet coefficients describing discriminatory features of the data.  (2) Analysis of Protein Mass Spectra. The overall goal of the P.I. is to develop methodologies for extracting important features of proteomic data whileincorporating dimension reduction wavelet techniques. The P.I. has a growing interest in the area of Bioinformatics and has <br/>established collaborations with a number of investigators at Texas A&M.  (3) Wavelet-based Methods for Long Memory Data.  This project relates to the development of wavelet methods for time series modelling.  The P.I. plans to build on her previous work on long memory estimation and on change-point detection and to explore novel applications to functional Magnetic Resonance Imaging (fMRI) data. <br/><br/>The novel methodologies developed in this proposal constitute advances in the theory and practice of wavelet-based methods. Applications to data arising from interdisciplinary collaborations demostrate the practical usefulness of the proposed methods, and confirm the success of wavelets as a tool for analysing data. The proposed clustering methods are quite general and can be applied to a number of different contexts that involve functional data. The P.I. has previous experience with the analysis of data from studies involving Near Infrared spectra and of biomedical data. She has also established several collaborations in the area of Bioinformatics and plans to develop wavelet methods for the analysis of high-throughput protein mass spectra.  Broader impacts of this proposal are in the collaborative nature of the proposed research but also in its educational and training objectives and in its efforts to disseminate results. The P.I. is engaged in several collaborations with investigators in the life sciences, both at Texas A&M and at other universities. She continues her engagement in the mentoring of graduate students and in training activities. She also maintains an updated webpage on her research activities where papers and accompanying software are posted in a timely <br/>manner.<br/>"
"0600933","Robust Statistics","DMS","STATISTICS","06/01/2006","04/18/2006","David Olive","IL","Southern Illinois University at Carbondale","Standard Grant","Gabor Szekely","05/31/2009","$89,162.00","","dolive@siu.edu","900 S NORMAL AVE","CARBONDALE","IL","629014302","6184534540","MPS","1269","0000, OTHR","$0.00","This research considers the development of computationally practical robust multivariate location and dispersion estimators, robust multiple linear regression estimators and resistant dimension reduction estimators along with the corresponding theory. Regression is the study of the conditional distribution of the response variable given a vector of the predictor variables.  Dimension reduction searches for a lower dimensional vector of predictors that carries all the information relevant to the regression.  A 1D regression is a special case of dimension regression and can be visualized in a plot of the estimated sufficient predictor versus the response.  Many of the most used statistical procedures, including multiple linear regression and generalized linear models, are special cases of 1D regression. Robust estimators are needed since existing methods for dispersion, regression and dimension reduction such as ordinary least squares and sliced inverse regression often perform poorly in the presence of outliers.<br/><br/>Statistics is the science of extracting useful information from data and is used by industry and government and in the fields of engineering, biological sciences, geological and environmental sciences, information technology, medicine, physical sciences, and the social sciences.  Applications of the methods under investigation include biomedical research, predicting future observations based on previous data, and the analysis of economic and social data.  Increasingly complex high dimensional data sets are being collected for scientific, social and strategic purposes. These data sets tend to contain outliers which are observations that differ from the bulk of the data. Typing and recording errors (e.g., 1000 or 10 pound women) may create outliers, but often there is no simple explanation for a group of data that differs from the bulk of the data. Robust statistics combined with dimension reduction is perhaps the most promising technique for making the most used methods of statistics simultaneously easier and more effective.  Hence the research is also likely to have a great impact on statistical education.<br/><br/>"
"0604997","Symmetry and Asymmetry in Experimental Design","DMS","STATISTICS","09/01/2006","06/01/2006","John Morgan","VA","Virginia Polytechnic Institute and State University","Standard Grant","Gabor Szekely","08/31/2010","$144,276.00","","jpmorgan@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1269","0000, OTHR","$0.00","Blocking of experimental material is a fundamental device for variance reduction in the design of comparative experiments. The investigator is exploring efficient applications of this principle from three distinct perspectives based on symmetry/asymmetry notions in the treatments to be compared, in the available experimental material, and in external restrictions on the set of permissible designs. Conventional design optimality theory rests strongly on the assumption of equality of treatment interest, expressed through summary measures of experimental information that are invariant to treatment permutation. The first goal is to develop a coherent optimality theory for evaluating designs when permutability is not tenable, that is, when experimenters are faced with asymmetry in treatment interest. Equality of treatment replications is another conventional symmetry that, depending on the number of blocks available and their sizes, can result in underutilization of resources and consequent reduction of information. The second goal is to develop theory for dealing with asymmetry of treatment replication in a comprehensive fashion. In some experiments it is demanded that blocks be partitioned into subsets so that each treatment occurs exactly once in each subset; a design for this situation is said to be resolvable. Resolvability is an added symmetry in treatment assignment, over that ordinarily required of a block design.  The third goal is to extend theory for optimal resolvable designs to cover numbers of treatments, replicates and block sizes for which there are currently no available results.<br/><br/>Collectively, the investigator is pursuing an approach to the design of comparative experiments that, by virtue of wider applicability, can compile significantly expanded design catalogs as resources for experimenters in a wide range of disciplines. A component of each of the problems described above, as the optimality theory evolves, is the building and compiling of designs for many combinations of blocks and treatments, and cataloging these online for free access by researchers, experimenters, and statistical practitioners. Making good designs easily available provides a basic tool for more efficient experimentation, aiding progress in scientific advancement and industrial innovation by increasing the quality of information that experiments can produce.<br/><br/> <br/><br/> <br/><br/>"
"0702277","Gene Tree-Species Tree Relationships Under the Coalescent Process","DMS","STATISTICS","10/01/2006","01/22/2007","Laura Kubatko","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","Gabor Szekely","07/31/2009","$54,454.00","","lkubatko@stat.osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, 9150, OTHR","$0.00","Abstract <br/><br/>Prop ID:  DMS-0505265  <br/>Prev Awd:  0104290  <br/>PI:  Salter, Laura    <br/>Institution:  University of New Mexico  <br/>Title:  Gene Tree-Species Tree Relationships Under the Coalescent Process   <br/><br/>In this proposal, incongruence in gene trees and species trees is<br/>examined using data from multiple genes in the context of the<br/>coalescent process.  First, the investigator will show<br/>that one currently advocated approach for the analysis of data<br/>from multiple genes, the concatenation approach, can be<br/>statistically inconsistent, even when a consistent method of<br/>phylogenetic tree estimation is used. Second, an algorithm for<br/>maximum likelihood (ML) estimation of species trees from data on<br/>multiple genes under the coalescent model will be developed and<br/>implemented, and will be made freely available via the internet.<br/>The availability of a method for ML species tree estimation will<br/>allow for likelihood-based hypothesis testing of phylogeographic<br/>and population genetic hypotheses.  Further, methods for assessing<br/>uncertainty in the species tree estimates will be developed by<br/>extending traditional bootstrapping methods in phylogenetics to<br/>the case in which data have been collected for multiple genes<br/>sampled randomly throughout the genome.  Finally, tests for the<br/>adequacy of the coalescent model will be developed by examining<br/>whether the observed gene trees are consistent with a given<br/>species tree using several metrics to measure levels of<br/>incongruence.<br/><br/>The inference of the evolutionary history of a collection of organisms <br/>based on the information contained in their DNA sequences is a problem<br/>of fundamental importance in evolutionary biology. The abundance of DNA <br/>sequence data arising from genome sequencing projects has led to <br/>significant challenges in the inference of these phylogenetic <br/>relationships. Among these challenges is the inference of the evolutionary <br/>history of a collection of species based on DNA sequence information<br/>from several distinct genes sampled throughout the genome.  This project <br/>studies the effect of the coalescent process on the inference of species <br/>phylogenies using data from multiple genes.  This work will first <br/>demonstrate that failure to model the coalescent process can lead to <br/>incorrect inferences of species relationships.  The investigator will then <br/>develop methods that can accurately estimate species phylogenies through <br/>explicitly modeling the coalescent process, and will apply these <br/>estimation procedures to construct techniques for hypothesis testing and <br/>for measuring uncertainty in estimated species phylogenies.<br/><br/>--<br/><br/>"
"0604670","Monitoring Structural Changes in Dynamic Time Series Models","DMS","STATISTICS, International Research Collab","07/01/2006","05/30/2007","Lajos Horvath","UT","University of Utah","Standard Grant","Gabor Szekely","06/30/2009","$180,960.00","Alexander Aue","horvath@math.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","MPS","1269, 7298","0000, 5916, 5979, OTHR","$0.00","Structural stability is one of the principal objects in modern time series analysis and is of interest in fields as diverse as econometrics, geoscience, engineering, climatology, computer science and signal processing. Clearly, statistical analyses based on estimates derived from unstable relationships under the false assumption of stability are meaningless and will doubtlessly have far-reaching consequences. It is well-known that a variety of test statistics used to detect structural changes of a certain type (ie, level shifts) are also sensitive to other phenomena (ie, long memory). Examples are abundant in the literature. This indicates the need to develop new and more sophisticated procedures that not only detect existing changes but that can also identify the specific underlying mechanism that governs the observed data. Motivated by this need, the investigator's research is aimed at developing new up-to-date statistical methods that allow for a deeper understanding of the phenomenon under consideration. The framework is general enough to include ramifications to a wide variety of applications. There are two main objectives, econometrics and climatology. (1) One of the major concerns in econometrics is to validate or reject the random walk hypothesis. Commonly, test statistics used in the context have low power and are often sensitive not only to non-stationarity but also to level shifts and long memory. New methods are proposed that are able to distinguish between these phenomena. (2) In climatology, there is a great controversy how to interpret weather related data such as hurricanes, precipitation and temperatures (global warming, greenhouse effect). The investigator proposes new methods involving the detection of multiple breaks that will help to gain further insight. <br/><br/>The investigator's research is concerned with detecting time dependent changes in environment. He believes to be able to contribute to the broad scientific discussion by developing new and nonstandard statistical methods which will have broad impacts in climatology and econometrics, and which will be of strategic interest for the federal government. Many problems invite the question if a previously assumed model is still valid and accurate or if a structural change took place, and model assumptions, hence, have to be adapted towards a new situation. Answering this questions certainly goes along with a demand for a more detailed and diversified understanding of the nature of the particular structural changes and the evolution of the competing models used to describe them."
"0604920","Empirical Likelihood and Censored Quantile Regression","DMS","STATISTICS","06/01/2006","04/15/2008","Mai Zhou","KY","University of Kentucky Research Foundation","Continuing Grant","Gabor Szekely","05/31/2009","$199,993.00","Arne Bathke, Mi-Ok Kim","mai@ms.uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","1269","0000, 9150, OTHR","$0.00","This project is concerned with the development of statistical methodologies in the regression analysis of censored data that can model both the average and extreme responses. The investigators will develop a novel empirical likelihood (EL) approach for the accelerated failure time (AFT) model and censored quantile regression. Empirical likelihood is a recently developed nonparametric inference method with asymptotic properties similar to the parametric maximum likelihood. The novel EL approach uses sample points casewise and is less stringent than previously proposed residual based empirical likelihood: the sample points are not required to be identically distributed, and a more general form of heteroscedasticity is permitted. An interesting application of the proposed approach is to censored quantile regression. While quantile regression has appeared as an alternative to the least squares with uncensored data as it provides more complete information about the conditional distribution of the response, its application to censored data has been limited due to the lack of an efficient inference method, among other reasons. The proposed EL approach will advance quantile analysis in censored regression and extend the domain of EL inference in general.<br/><br/>The proposed research is of high federal strategic interest because the results will accelerate many health, medical, and economic research projects where data is incomplete and abnormal cases rather than the average are of interest, such as low birth weight, high ozone concentration, cancer survival rates, or high yield stock, among others. Traditional analyses focus on the center of the data and implicitly assume that the results found for the average group are generalizable to the entire patient group. However, this is usually not the case. For example, an investigator in a survival study of cancer patients may find that certain molecular biomarkers are prognostic factors only for those ovarian cancer patients who die exceptionally early compared to others in the same reference group. The quantile regression technique with the proposed inference procedure permits the investigation of the biomarkers without constraining their effects for different subpopulations to be same as for the average group. Therefore, the proposed method can better inform health professionals of the effects of physiological and molecular biomarkers on different subpopulations and provide a useful prognostic tool for the overall and progression free survival times.<br/>"
"0604394","Inference and Prediction in a Complex Discovery Process","DMS","STATISTICS","07/01/2006","08/28/2008","Xiaotong Shen","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","06/30/2010","$234,657.00","","xshen@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, 1269, OTHR","$0.00","                    Abstract<br/><br/>  With advances of computing technology, scientific and engineering investigation becomes increasingly complex. Powerful statistical techniques therefore are needed to exploit data with complex structure. The proposed project will be centered at design and analysis of inferential and prediction methods for problems involving complex statistical modeling that arise in, for instance, machine learning and data mining. The problems to be investigated include inference about a modeling process to account for modeling uncertainty in a modeling process, as well as prediction and inference in the contexts of multi-class margin classification and semi-supervised learning. The specific aims of the project, motivated by characteristics of complex modeling processes in our targeted applications, are focused on (1) the development of a novel theory of inference, as well as inferential procedures and computational tools, for comparing complex modeling processes; (2) the development of multi-class margin classification techniques; (3) the development of novel techniques for semi-supervised learning; and (4) the specific development of techniques for the targeted applications including object tracking, cancer genomics classification, and text categorization. For the targeted applications, the PI will collaborate with other scientists and engineers.<br/><br/>  The proposed educational program will train graduate students for research in statistics. Success of this project will bring tremendous benefits to fundamental scientific and engineering research, particularly<br/>in automatic machine processing to combine humans' intelligence with machine's speed, in mining data with complex structure, and in biomedical research.<br/>"
"0604563","Statistical Inference for Continuous-Time Stochastic Processes","DMS","STATISTICS","08/15/2006","05/31/2007","Song Chen","IA","Iowa State University","Standard Grant","Gabor Szekely","07/31/2010","$205,016.00","Liang Peng","songchen@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","0000, OTHR","$0.00","<br/>This project considers parameter estimation and model testing  for a wide range of continuous-time processes used  in stochastic modeling.  They include processes which have discontinuous sample paths, are multivariate, only partially observed and are subject to long range dependence.  Despite being wide ranging, a common opportunity offered by these processes is that the conditional characteristic functions are available for commonly used processes in stochastic modeling.  The investigators will develop a framework of  statistical inference for stochastic processes based on the conditional characteristic functions. They will consider both parameter estimation and model testing by employing three modern nonparametric tools of statistical inference:  the kernel smoothing method, the empirical likelihood  and the bootstrap resampling <br/>method.  Developing efficient parameter estimators and robust test procedures for stochastic processes are the goals of this project.<br/><br/>Continuous time stochastic processes defined by stochastic differential equations have long been used to model dynamic stochastic systems arising in physics, biology and other natural sciences.   One latest surge of interest on these processes comes from molecular biology in modeling the dynamics of proteins as part of an effort to understand how energy transfer and conversion happen within a biological cell.  Perhaps the most eminent use of the continuous time stochastic processes in the last two decades has been in finance following the works of Merton (1971) and Black and Scholes (1973) which established the foundation of option pricing theory for modern finance. Analysts and practitioners of these continuous-time stochastic models are <br/>increasingly aware that no matter how rich and powerful these models are for modeling a stochastic system, their applicability will be  largely limited if model parameters cannot be  estimated and the validity of the <br/>models cannot be confirmed with empirical data.  The intellectual merit of the project is in establishing a statistical inference framework for stochastic processes commonly used in stochastic modeling these days.  The broader impacts of the proposed research are (1) being  on the cutting edge of statistics, probability and stochastic system modeling, (2) producing opportunities for collaboration and partnerships with practitioners from a range of disciplines, and (3) extending  the educational experience of students with an opportunity to be deeply involved with stochastic modeling and applications.<br/><br/><br/><br/>"
"0603931","Collaborative Research: Empirical Likelihood Based Statistical Methods for Diagnostic Systems","DMS","STATISTICS","07/15/2006","07/11/2009","Xiao-Hua Zhou","WA","University of Washington","Standard Grant","Gabor Szekely","06/30/2010","$70,000.00","","azhou@u.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, OTHR","$0.00","The receiver operating characteristic (ROC) curve methodology is the statistical methodology for assessment of the accuracy of prediction rules. It is commonly used in a wide range of scientific fields such as signal detection theory, medical imaging, weather forecasting, real and false alarms testing for anti-terrorist system, and diagnostic medicine. Motivated by assessing diagnostic accuracy of Dermoscope, prostate cancer screening test, and audio test and the need to find tractable and easily implemented solution to complex statistical problems in ROC studies, in this project the investigators develop new semi-parametric and nonparametric statistical methods for ROC analysis, particularly on the statistical inferences for the partial areas or the full areas under the ROC curves (AUC) and ROC and AUC regression models. In this project, the investigators extend the traditional empirical likelihood (EL) to the setting of ROC analysis in three directions: (1) to develop EL-based semi-parametric methods for the partial areas under the ROC curves, (2) to develop EL-based nonparametric methods for the partial or full areas under the ROC curves, and (3) to develop EL-based statistical methods for ROC and AUC regression models.  The new methods are expected to be more robust and more accurate than the existing methods in evaluation of competing diagnostic systems. They have potentially better small sample performances than the existing methods. A software is developed for the actual use of the newly proposed methods for ROC analysis. The methods developed from this project are great addition to existing medical diagnostic testing and applicable to a wide range of scientific fields. <br/><br/>High-tech diagnostic systems of various kinds around us have been made tremendous progress in the last two decades. Many new diagnostic systems have been used to reveal diseases in people, malfunctions in nuclear power plants, flaws in manufactured products, collision courses of aircraft, threatening activities of terrorists, etc. However, costs of diagnostic systems can be high. To select more accurate diagnostic system for widespread use, it is important to develop appropriate statistical methods for evaluating the diagnostic accuracy of competing systems. The use of attractive statistical methods like the semi-parametric and nonparametric methods for ROC analysis developed in this project is going to help diagnostic systems users make informed choices of the most reliable diagnostic systems. This may contribute to the reduction of health care costs in the long run.<br/>"
"0604963","Space-time models, methods, and applications","DMS","STATISTICS","07/01/2006","04/28/2006","Peter Craigmile","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","Gabor Szekely","09/30/2010","$95,264.00","","pfc@stat.osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","Many phenomena in nature are measured across space and through time. Investigating possible space-time interactions, in the presence of uncertainty, is key to understanding the science.  There is no opportunity to study these interactions when marginalized over time or space. Space-time statistical methodologies should be faithful to methodology in time series analysis and spatial statistics.  The models the investigator studies are defined as spatially-dependent filterings of space-time innovation processes that are realizations of geostatistical processes. The innovations do not need to be invariant with respect to time. These models include the usual class of linear time series models, as well as standard geostatistical models, both Gaussian and non-Gaussian. The processes do not need to be stationary in time or in space, and build on the growing literature concerning nonstationary models.  Leveraging both the innovations, as well as the filtering operations used to define these space-time processes, this research will springboard from spatial statistics and time series to develop methods for inference and prediction in a space-time context.<br/><br/>Ice core paleoclimatology involves the study of the physical and chemical properties preserved in the Earth's glaciers and ice sheets. These properties, analyzed from ice cores, are used as proxies for various climatological processes. In collaboration with other researchers at The Ohio State University, the investigator studies proxy measures of accumulation, extracted from chemical analyses of ice cores distributed over and around Greenland.  Using the models developed under this research the investigator will examine relationships between these accumulation records and known drivers of climate variability and change.  In addition to climate research, the results will be immediately applicable to other scientific areas, such as speech and hearing sciences.  Findings will be communicated via articles in statistical and subject-matter areas. Education of a diverse cross-section of students (statistical and non-statistical), in spatial and time series methodologies, will be achieved in carrying out this research.<br/>"
"0604702","North American Meeting of New Researchers in Statistics and Probability","DMS","STATISTICS","04/01/2006","03/28/2006","Peter Hoff","WA","University of Washington","Standard Grant","Grace Yang","03/31/2007","$21,110.00","","peter.hoff@duke.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, 7556, OTHR","$0.00","<br/> An abstract:<br/><br/>The investigator and his colleagues will organize the New Researchers<br/>Conference, a scientific conference for roughly ninety new researchers<br/>in statistics and probability. This consists of organizing an application<br/>process, selecting participants from a pool of applicants, coordinating<br/>talks and poster sessions, arranging for senior speakers, and a variety of<br/>other administrative tasks (arranging for travel, housing, food, etc.).<br/><br/>The New Researchers Conference (NRC) is a conference for recent<br/>recipients of doctoral degrees in Statistics and Probability. The primary <br/>purpose of this conference is to provide a much needed venue<br/>for interaction among new researchers. At present, researchers develop<br/>contacts with others in their field either through their own<br/>institutions or at meetings of professional societies. It is often the<br/>case that junior researchers have limited<br/>contact with senior colleagues, even at their own institutions.<br/>Meetings of professional societies are rather large and junior<br/>researchers often find them overwhelming. The proposed meeting of new<br/>researchers will provide a unique opportunity for these junior<br/>researchers to exchange ideas and initiate contacts among themselves<br/>as well as provide them an opportunity to interact with the invited senior <br/>participants.<br/><br/><br/>"
"0604229","Inferential Methods for Quantile Regression","DMS","STATISTICS","08/01/2006","04/13/2009","Xuming He","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Gabor Szekely","07/31/2010","$374,543.00","","xmhe@umich.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","0000, OTHR","$0.00","While least squares regression targets the conditional mean function in a regression model, quantile regression provides more complete information on the conditional distribution of the response variable. It is especially valuable when there is heteroscedasticity or general heterogeneity in the population. To facilitate quantile regression modelling in a wider areas of applications, this proposal aims to develop inferential procedures for quantile regression models to account for the presence of random-effects or random censoring in the observations. Although random-effects and censoring have been well studied under linear models equipped with parametric, and often Gaussian, likelihoods, the conventional inference procedures do not have straightforward extensions to the quantile regression model when standard minimal assumptions are made on the conditional distributions. The principle investigator aims to make focused attempts in developing new ideas and tools to make possible appropriate inference in quantile regression models with random-effects or with censoring. The proposed research will build upon the recent developments in quantile regression modelling and incorporate some innovative ideas to develop appropriate inferential methods that are mathematically justified, mainly through large sample theory, and statistically meaningful at realistic sample sizes.<br/> <br/>Currently available methods for statistical inference in quantile regression models are not well-developed to handle random-effects or random censoring. For example, the analysis of GeneChip data in genomics would result in inflated false discovery rates without taking the array effect as random. The proposed research will develop new methods that preserve statistical confidence in a wider range of quantile regression based applications. The PI will pursue collaboration with other scientists to ensure that the methodologies under development are valuable to researchers in the biological sciences, health sciences, engineering, economics, and finance. The proposed activities will also involve training of graduate students for future researchers in statistics as well as providing selected undergraduate students with research experience.<br/><br/>"
"0630950","A Virtual Center to Promote Collaboration between US- and China-based Researchers in Statistical Science","DMS","STATISTICS","08/01/2006","08/11/2006","Xuming He","IL","University of Illinois at Urbana-Champaign","Standard Grant","Gabor Szekely","07/31/2010","$70,262.00","Feng Liang","xmhe@umich.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","0000, 7556, OTHR","$0.00","In this project, the PIs set up a virtual center for promoting and supporting exchanges and collaborations in statistical science between US-based and China-based researchers and educators. The center will be guided by a scientific committee. The goal is to enable research collaborations in the areas where the two countries have complementary strengths. The center will sponsor or co-sponsor Ambassador Lectures in China given by prominent researchers from the US on cutting-edge topics in statistics, invite prominent Chinese scholars to give theme lectures in the US and to collaborate with US researchers, co-sponsor workshops in China on frontiers of statistics and assist US participants to attend those workshops, and support collaborations between researchers from the two countries, especially junior researchers. <br/> <br/>The virtual center recognizes the importance of enabling US researchers and educators to advance their work through international collaboration. The activities supported by this proposal also help ensure that future generations of US researchers in statistical science have substantial interaction with their counterparts in China, a country that is accelerating its research and education at a rapid speed.<br/>"
"0604488","Further Studies on Weighted Empirical Likelihood","DMS","STATISTICS","07/01/2006","04/24/2008","Jian-Jian Ren","FL","The University of Central Florida Board of Trustees","Continuing Grant","Gabor Szekely","06/30/2010","$151,892.00","","jjren@umd.edu","4000 CENTRAL FLORIDA BLVD","ORLANDO","FL","328168005","4078230387","MPS","1269","0000, OTHR","$0.00","<br/>ABSTRACT:<br/><br/>Since Owen (1988), the empirical likelihood method has been developed to construct tests and confidence sets based on nonparametric likelihood ratio. Studies have shown that the empirical likelihood ratio inferences are of comparable accuracy to alternative methods. However, so far the applications of empirical likelihood method to censored data are relatively few and are mainly focused on the right censored data. In this context, the PI discovered in her 2001-paper that a new likelihood function, called weighted empirical likelihood<br/>function, has the potential to facilitate the research on a broad class of nonparametric and semiparametric statistics for various types of incomplete data, including doubly censored data, interval censored data, and partly interval-censored data. It is shown that weighted empirical likelihood may be viewed as the asymptotic version of Owen's empirical likelihood function for censored data. In the past few years, the PI's investigation<br/>shows that the weighted empirical likelihood indeed provides a useful tool to deal with statistical inference problems with complicated types of censored data which are otherwise quite difficult to handle. The objective of this current project is to further study the applications of weighted empirical likelihood in providing solutions for several important nonparametric or semiparametric statistical inference problems in survival<br/>analysis with various types of censored data. The issues under consideration include: (1) extension of the weighted empirical likelihood to deal with p-dimensional variables; (2) further applications of the weighted empirical likelihood to estimation or model assessment problems associated with estimating equations, profile likelihood and some important survival models; (3) coverage accuracy of weighted empirical likelihood ratio confidence intervals; (4) comparison with alternative methods.<br/><br/>Incomplete data are frequently encountered in medical follow-up and reliability studies. Recently, statisticians are paying more attention to some more complicated types of incomplete data, such as doubly<br/>censored data, interval censored data, partly interval-censored data, truncated data, etc., as these data occur in important clinical trials and scientific research. For instance, doubly censored data were encountered in a recent study of primary breast cancer, interval censored data were encountered in AIDS research, partly interval-censored data were encountered in heart disease and diabetes studies,<br/>and doubly truncated data were encountered in astronomical research. Up to now, the statistical research on these more complicated types of incomplete data still generally lags behind that on right censored data,mainly because it is technically much more challenging. The expected results of this project are better understanding<br/>of the technique of weighted empirical likelihood, and providing solutions for several important statistical inference problems associated with some widely used survival models in biomedical research and epidemiological studies when observed data are right censored, doubly censored, interval censored, or partly<br/>interval-censored.<br/>"
"0604571","Multi-Way Semilinear Methods with Applications to Microarray Data","DMS","STATISTICS","07/01/2006","04/27/2006","Cun-Hui Zhang","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","06/30/2010","$139,586.00","","czhang@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","This research focuses on developing statistical models, methods, and theory for normalization and significance analysis of microarray data and similar applications. Microarray technology has become an important tool for quantitatively monitoring gene expression patterns and has been widely used in functional genomics. The investigator and his collaborators have proposed and developed a two-way semilinear model for normalization and analysis of cDNA microarray data. The purpose of this research is to extend and further develop this methodology and investigate its theoretical justifications. The project covers a wide range of specific problems including multi-way semilinear models, normalization and analysis of high-density oligonucleotide arrays, incorporation of control/spike genes and biological pathway information, location-scale normalization, estimation of noise level, incorporation of data quality measurements, invertibility of information operator, asymptotic equivalence to ideal/oracle estimators of gene effects, and more.<br/><br/>The multi-way semilinear model is an extension of widely used analysis of variance and semilinear regression models. Its applications to microarray experiments present challenging methodological and theoretical problems with high-dimensional complex datasets. These datasets have the following features: large number of unknown parameters for gene effects and small number of samples, many nonparametric components, co-linearity between bases for the estimations of nonparametric and parametric components, stochastically dependent covariates with spatially inhomogeneous distributions, interactions between observed gene expressions and possibly unobservable gene clusters and biological pathways, inhomogeneous noise level, and more. This research is motivated and will be directly applicable to functional genomic studies using microarray and similar technologies. It will also be directly applicable to high-throughput screening of chemical compounds in drug discovery experiments. This research will have significant educational impact.<br/><br/>"
"0604596","Invariant Coordinate Selection (ICS): A Robust Statistical Perspective on Independent Component Analysis (ICA)","DMS","STATISTICS","06/01/2006","05/01/2008","David Tyler","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","09/30/2009","$138,000.00","","david.tyler@rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","ABSTRACT<br/><br/>Independent component analysis (ICA) is an increasingly popular method for analyzing multivariate data within many diverse disciplines, such as computer vision, psychology, electrical engineering, physics and meteorology. Although essentially a multivariate statistical method, the primary development of ICA<br/>has come from outside the area of statistics. Methods developed for ICA presume somewhat specialized models for multivariate data. Nevertheless, these methods tend to possess some natural robustness properties, and consequently, have been used for exploratory data analysis in general and for cluster analysis or unsupervised learning in particular.  At present, the statistical properties of ICA methods are not well understood. One goal of the proposed research is thus to conduct a study of the statistical and robustness properties of ICA methods. Another goal of the proposed research is to study the use of ICA methods as general multivariate statistical methods rather than simply as solutions to the ICA problem.  To accomplish this, a new model free interpretation of ICA methods, together with some extensions of these methods, is to be introduced and developed. The principal investigator refers to this model free approach as invariant coordinate selection (ICS).<br/><br/>The proposed research should have an impact on the many scientific and engineering disciples using ICA models by providing a better statistical understanding of their methods, consequently leading to improved methods and applications.  An important application of ICA models, for example, is in the unraveling of convoluted signals or sources. The proposed research will also help call further attention to these important statistical problems to the general statistics research community. In addition, it is anticipated that the proposed introduction and development of the ICS methodology will provide important and potentially very popular new methods for exploring and analyzing multivariate data, perhaps similar to the importance of principal component analysis or discriminant analysis. These more general ICS methods should have impact not only within the statistics community but also within the varied disciplines, such as psychology, biology, geology, and others, that routinely deal with multivariate data.<br/>"
"0706806","Spatial Point Pattern Analysis Using Composite Likelihood","DMS","STATISTICS","07/01/2006","06/08/2007","Yongtao Guan","CT","Yale University","Standard Grant","Gabor Szekely","06/30/2009","$96,496.00","","yguan@miami.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","0000, OTHR","$0.00","The proposed research introduces a new likelihood based method in fitting spatial point process models using the idea of composite likelihood. Composite likelihood (CL) has been successfully applied in numerous settings where a full maximum likelihood is not feasible or is not available. Its use in spatial point process modeling, however, has never been studied. This research intends to show that a CL can be formed for any spatial point process whose second-order intensity function can be explicitly defined. The proposed likelihood is easy to obtain and can be used in many different spatial point pattern analyses. In particular, it can be used to fit both homogeneous and inhomogeneous spatial point process models to data. Furthermore, it can also be used to select the bandwidth used in estimating the pair correlation function, which is an extremely exploratory and model fitting tool in spatial point pattern analysis.<br/><br/>This research is motivated by the red oak borer data that were collected by entomologists at the University of Arkansas. The data consist of mapped locations of attack holes caused by larvae of red oak borers when they eat their way into the trees. The main objective is to understand what affects adult red oak borers to decide where to lay their eggs. The proposed model fitting procedures will be applied to link the locations of attack holes to important tree characteristics such as side and height of the tree. This work will have both important biological and practical significance. In particular, the results will provide biological insight on the breeding habit of the adult borers. This biological insight can in turn be used in practice to guide the development of more effective trapping techniques that can be used to control the population of the red oak borers. Thus the proposed research may play a critical role in the effort to save millions of red oak trees in the US that are being threatened by the outbreaks of red oak borers.<br/><br/> <br/>"
"0600537","Conference on Asymptotic Analysis in Stochastic Processes,  Nonparametric Estimation, and Related Problems","DMS","PROBABILITY, STATISTICS","06/15/2006","06/16/2006","Pao Liu Chow","MI","Wayne State University","Standard Grant","Tomek Bartoszynski","05/31/2008","$16,000.00","Gang George Yin, Boris Mordukhovich","plchow@math.wayne.edu","5700 CASS AVE STE 4900","DETROIT","MI","482023692","3135772424","MPS","1263, 1269","0000, 7556, OTHR","$0.00","This is a proposal on conference for asymptotic analysis in stochastic processes, nonparametric estimation, and related fields. The proposal presents a concerted effort of researchers from multiple institutions to assemble an impressive list of invited speakers, who are renowned leaders in the fields of probability theory, stochastic processes, stochastic differential equations,<br/>as well as in the nonparametric estimation theory, and related fields. Their works have stimulated much of subsequent progress in these fields. In addition,<br/>a number of invited speakers were early developers of the fields of probability and stochastic processes, establishing the foundation of contemporary probability theory. The conference focuses on stochastic systems. The main theme is asymptotic stochastic analysis, which is a powerful tool for studying stochastic processes and nonparametric estimation problems. Leading experts around the globe will present state-of-the-art results on such topics as stochastic ordinary and partial differential equations, large deviations, homogenization, asymptotic methods in nonparametric inference, the interplay between stochastic differential equations and nonlinear partial differential equations, and stochastic control and numerical methods. Not only will the conference further our understanding in stochastic processes and nonparametric methods, but also it will be a trend setting event, leading to an acceleration of research efforts, and making broader impacts on mathematics, education, and different branches of science and engineering as a whole.<br/><br/>In the new era of global economy, any scientific innovation will add to <br/>the national competitiveness. The asymptotic analysis in stochastic processes<br/>and nonparametric estimation extends in many areas of human endeavor, and it is a very powerful mathematical tool for solving many complex problems arising from engineering and sciences. In recent years, research in these areas has witnessed a tremendous progress, which has resulted in a significant impact on mathematics, physical, social and biological sciences, as well as on engineering and technology. The rapid progress has necessitated fast and timely communications among researchers in the field. The proposal aims at bring leading experts in the areas together to exchange ideas, to interact with young researchers, and to update the most recent progress. The proposal presents a concerted effort by researchers from multiple institutions to assemble an impressive list of invited speakers, who are renowned leaders in the fields of probability theory, stochastic processes, stochastic differential equations,<br/>as well as in the nonparametric estimation theory, and related fields. Their important scientific contributions have stimulated much of subsequent progress in these fields. In addition, a number of invited speakers were early developers of the fields of probability and stochastic processes, establishing the mathematical foundation of contemporary probability theory and its applications.<br/>Special emphasis will be given to encourage broad participation, which is important to the diversification of the research community. The organizing committee will make every effort to promote the participation of junior researchers, graduate students, and researchers from under-represented groups.<br/>Not only will the conference further our understanding in stochastic processes and nonparametric statistical methods, but also it will be a trend-setting event, leading to an acceleration of research efforts, and making broader impacts on mathematics and applications, education, and different branches of science and engineering as a whole.<br/>"
"0604736","Statistical Problems in Detectability","DMS","STATISTICS","07/15/2006","07/13/2006","Xiaoming Huo","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","06/30/2009","$94,998.00","","xiaoming@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","0000, OTHR","$0.00","The PI will study statistical problems in detectability, which is to answer whether a detection problem is solvable for a given data set (e.g., an imagery). There will be two major components: (a) what is the fundamental threshold to determine when a detection task is doable, and (b) when a detection problem is solvable, what is the adequate order of computational complexity to solve it. Based on the current state-of-the-art, the PI proposes to derive more accurate results, and to compare different formulations and their influence on the theory of detectability. The proposed works have three main thrusts. (1) Limit distributions of the test statistics at the asymptotic rate of detectability will be derived. This will advance<br/>the detectability theory. (2) Application-driven models will be adopted in the detectability theory. In many cases, these application-driven models are complex, and the adaptation and the possible generalization of the detectability theory are not trivial. (3) Influences of different statistical formulations on the theory of detectability will be characterized. The proposed works are rooted in two of PI's prior works: (1) the project of multiscale geometric detection (MGD), which derived the asymptotic rate of detectability for detecting a range of geometric objects, and (2) the project of multiscale significance run algorithms (MSRA) and the<br/>consequent results on limit distributions. In the second project, after knowing the asymptotic rate in MSRA, the limit distribution of the test statistic is derive (in a simpler situation), so that the detectability right at the asymptotic rate is characterized. The limit distribution also explains the robustness of the detection<br/>algorithm that have been demonstrated in simulations.<br/><br/>Detection is a fundamental problem in many image processing applications. Some applications include (1) particle detection in cryo-EM images, which plays an important role in automated reconstruction of a molecular structure, (2) automatic target recognition (ATR), which has many military and civil surveillance applications, and (3) crater detection in geomorphology, which is utilized in extraterrestrial mapping and planetary chronological research. Proposed theoretical problems are fundamental in these<br/>applications. Graduate students will get involved.<br/>"
"0605041","Statistical Inference under Subjective and Not-Fully-Quantifiable Information on Experimental Units","DMS","STATISTICS","08/01/2006","06/12/2006","Steven MacEachern","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","Gabor Szekely","08/31/2010","$200,000.00","Omer Ozturk","snm@stat.osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","In many scientific investigations, there is a wealth of subjective information about experimental material--which agricultural plots are more fertile, which patients are healthier, etc.--that is difficult to quantify as a formal, numerical variate for further use in statistical analysis.  Currently, statistical science is unable to effectively exploit this information.  This research develops a body of techniques that allow one <br/>to recover the bulk of this information and to use it in a fully objective fashion.  Importantly, the same mathematical techniques that apply to recovery of subjective information also allow one to exploit ``lesser quality'' covariates which may be subject to substantial measurement error or other biases.  These lesser quality covariates are used to create an artificial stratification among the potential experimental units before their responses are measured.  The body of work that underlies these techniques is known as ranked set sampling.  The current status of ranked set sampling provides a theoretical foundation under very strong assumptions, such as perfect ranking or other precisely specified probability and judgment ranking models. <br/>Even a small departure from these assumptions may result in inconsistent estimators that contain substantial bias, even asymptotically.  There are reservations within the research community regarding the use of ranked set sampling when either the cost recruiting a unit for a study is substantial or when the number of available experimental units is limited.  In these situations, it is desirable to use all available experimental units to perform the experiment.  To alleviate these concerns, this research looks at ranked set sampling from a different perspective and identifies three areas where the current state-of-the-art ranked set sampling is either not appropriate, performs poorly, or cannot be used in a satisfactory general fashion.  These areas are (i) the use of ranked set sampling in the design of experiments, (ii) the development of low- and medium structure parametric estimation under minimal judgment modeling assumptions, and (iii)<br/>the development of models for the ranking process.<br/><br/>This research will have a substantial impact on statistical analysis, on several established areas of scientific research, on the quality of life of the U.S. populace, and on the scientific infrastructure of the country. The research, through the mechanisms described in the preceding paragraph, will allow researchers to squeeze more information out of their experiments with novel statistical analyses.  The flip side of squeezing more <br/>information out of an experiment is the ability to obtain a given amount of information with a smaller experiment.  The research will develop better designs to perform experiments, impacting a wide variety of disciplines. As a prime example, with the new designs, a clinical trial, used to establish the benefits of a new drug, will require a smaller number of subjects.  The cost of the trial will be reduced and the trial will be completed in a shorter time span.  The triple benefit of a smaller, cheaper and quicker trial will expedite the development and approval of new drugs, including those for terminal diseases, such as cancer.  In addition to economic benefits, there is the ethical benefit of moving promising new therapies through the development process as quickly as possible, without sacrificing current safeguards in the approval process.  The scientific infrastructure of the country will be enhanced by the increased collaboration between statisticians and medical researchers, and by the rigorous training of graduate students.  Particular emphasis will be given to the training of women and minorities in the mathematical sciences.<br/>"
"0603761","Information-Rich and Cost-Effective Designs for Scientific Experiments","DMS","STATISTICS","06/01/2006","09/16/2008","Sam Hedayat","IL","University of Illinois at Chicago","Continuing Grant","Gabor Szekely","05/31/2011","$272,004.00","","hedayat@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","0000, OTHR","$0.00","<br/><br/>ABSTRACT<br/><br/>The scientific goals of the project are to provide new statistical theory and related methodology (i) for aiding medicinal chemists in seeking better models for pharmacokinetic and pharmacodynamic (PK/PD)<br/>experimental outcomes; (ii) for reducing the variability and cost of experiments in biopharmaceutical (BE) and drug abuse liability trials and (iii) in drug stability studies, including shelf life estimation of drugs. In the process, potential applications of cross-over designs and trade-off theory will be strengthened in a wide array of pharmaceutical, biotechnological, agricultural and engineering investigations.  The research activities employ a combination of mathematica, statistical and computational tools (partly available and partly to be developed).<br/><br/>The aim of this project is to guide experimenters (in areas such as pharmaceutical, biotechnological, agricultural and engineering investigations) towards collection of information-rich and cost-effective<br/>data while running planned experiments. The problems proposed in the broad area of design of experiments and pharmaceutical investigations center around some key issues for which very little theoretical<br/>contributions are available in the published literature. Every research problem proposed herein is characterized by its distinctiveness with respect to the innovative tools to be developed and analytical arguments to be adopted.  The anticipated outcomes of this proposal will have substantial useful applications and will be utilized by scientists and engineers in a wide array of scientific investigations.<br/>"
"0604593","Computing Environments for Statistics","DMS","STATISTICS","06/01/2006","07/16/2008","Luke-jon Tierney","IA","University of Iowa","Continuing Grant","Gabor Szekely","05/31/2010","$299,632.00","","luke@stat.uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1269","0000, OTHR","$0.00","The investigator explores and develops new principles for the design of statistical software to take advantage of modern computing power. Particular emphasis is placed on exploring the effective use of compilation, code analysis, and exception handling for statistical languages, and on developing effective tools and frameworks for parallel computing in statistics.  Pilot implementations are incorporated in open source statistical software systems.<br/><br/>Effective statistical methodology made available through statistical software is critical to the ability of researchers to make maximal use of experimental and observational data, and for the ability of instructors to teach good research practices.  The design principles developed by this research lead to software that improves the ability of researchers, instructors, and other users of statistical methodology to apply this methodology more effectively in scientific research and teaching and to take full advantage of modern high-performance computational resources.  These principles also lead to software frameworks that can be used to more rapidly deliver new statistical methodology to end users.  Applications in brain imaging analysis serve as a testbed for methods and principles developed in this research.<br/>"
"0603913","Collaborative Research: Empirical Likelihood Based Statistical Methods for Diagnostic Systems","DMS","STATISTICS","07/15/2006","06/16/2009","Gengsheng Qin","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Gabor Szekely","12/31/2009","$79,935.00","","gqin@gsu.edu","58 EDGEWOOD AVE NE","ATLANTA","GA","303032921","4044133570","MPS","1269","0000, OTHR","$0.00","The receiver operating characteristic (ROC) curve methodology is the statistical methodology for assessment of the accuracy of prediction rules. It is commonly used in a wide range of scientific fields such as signal detection theory, medical imaging, weather forecasting, real and false alarms testing for anti-terrorist system, and diagnostic medicine. Motivated by assessing diagnostic accuracy of Dermoscope, prostate cancer screening test, and audio test and the need to find tractable and easily implemented solution to complex statistical problems in ROC studies, in this project the investigators develop new semi-parametric and nonparametric statistical methods for ROC analysis, particularly on the statistical inferences for the partial areas or the full areas under the ROC curves (AUC) and ROC and AUC regression models. In this project, the investigators extend the traditional empirical likelihood (EL) to the setting of ROC analysis in three directions: (1) to develop EL-based semi-parametric methods for the partial areas under the ROC curves, (2) to develop EL-based nonparametric methods for the partial or full areas under the ROC curves, and (3) to develop EL-based statistical methods for ROC and AUC regression models.  The new methods are expected to be more robust and more accurate than the existing methods in evaluation of competing diagnostic systems. They have potentially better small sample performances than the existing methods. A software is developed for the actual use of the newly proposed methods for ROC analysis. The methods developed from this project are great addition to existing medical diagnostic testing and applicable to a wide range of scientific fields. <br/><br/>High-tech diagnostic systems of various kinds around us have been made tremendous progress in the last two decades. Many new diagnostic systems have been used to reveal diseases in people, malfunctions in nuclear power plants, flaws in manufactured products, collision courses of aircraft, threatening activities of terrorists, etc. However, costs of diagnostic systems can be high. To select more accurate diagnostic system for widespread use, it is important to develop appropriate statistical methods for evaluating the diagnostic accuracy of competing systems. The use of attractive statistical methods like the semi-parametric and nonparametric methods for ROC analysis developed in this project is going to help diagnostic systems users make informed choices of the most reliable diagnostic systems. This may contribute to the reduction of health care costs in the long run.<br/>"
"0603890","Theory and practice of nonparametric detection","DMS","STATISTICS","07/01/2006","12/27/2010","Ery Arias-Castro","CA","University of California-San Diego","Standard Grant","Gabor Szekely","12/31/2011","$119,999.00","","eariasca@math.ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","0000, OTHR","$0.00","The scan statistic, or matched filter, has been extensively used for detection tasks. In many instances, a parametric model is assumed for simplicity and computational limitations. Also, various parametric settings for detection have been studied over the years. In particular, the investigator has done a study that establishes minimax rates for a large variety of problems and proves that the scan statistic achieves the optimal rate. However, most detection problems are nonparametric in nature, making it important to gain similar insight into nonparametric detection problems. By studying the problem of detection on graphs and drawing connections with Statistical Mechanics, the investigator brings a new understanding of such problems.  The investigator also designs flexible graphical structures based on multiscale ideas to be the corner stone for developing new tools in various applied settings.<br/><br/>The problem of detection arises in a wide variety of settings such as in analyzing galaxy catalogs; object tracking; road tracking; satellite imagery, which includes detecting man-made objects, detecting ships and fires; environment monitoring, which includes controlling pollution levels of water sources, detection of disease outbreaks and detection/localization of whales in the Ocean from acoustic data. The investigator studies the theoretical properties of the scan statistic in more complex (nonparametric) models for such detection tasks.  Though the scan statistic has been extensively used in virtually all such problems, its behavior is mostly known under rather simplified (parametric) assumptions. Further, the investigator develops new tools for implementing the scan statistic in such complex detection settings. Such efficient tools are currently needed in many Scientific and Engineering fields, such as Astrophysics, in detecting galaxy clusters.<br/><br/>"
"0604722","Statistical Analysis of the Filtering Models with Marked Point Process Observations: Applications to Ultra-High Frequency Data","DMS","STATISTICS","06/01/2006","07/11/2009","Yong Zeng","MO","University of Missouri-Kansas City","Continuing Grant","Gabor Szekely","05/31/2010","$94,454.00","","zengy@umkc.edu","1011 E 51ST ST","KANSAS CITY","MO","641102201","8162355839","MPS","1269","0000, OTHR","$0.00","With a new view of ultra-high frequency data, strong motivation from financial market microstructure theory, and the powerful machinery of stochastic calculus, this proposal studies two lines of the filtering problems with marked point process observations. Line One has Markov signals and Line Two has long-memory signals driven by fractional Brownian motion. For each line, the investigator systematically develops the statistical analysis and its applications. The investigator establishes the statistical foundations for inference: the likelihoods, the posterior distribution, the likelihood ratio and the Bayes factors. Typically, they are all infinite-dimensional and are characterized by stochastic differential equations such as filtering equations. Such equations are derived and the focus is on developing Bayesian inference via filtering for the two models. Moreover, the investigator studies two approaches for constructing high-performance computing algorithms for the on-line implementation of the statistical analysis. One approach is the Markov chain approximation method, and the other is particle filtering or sequential Monte Carlo method. The mathematical foundations for the consistency of these carefully-designed algorithms are established.<br/> <br/>Ultra-high frequency (or trade-by-trade) data are widely available on increased market variables in all major world financial markets and the filtering models are motivated from market microstructure theory. <br/>With the filtering models and their established statistical theory and tools, the investigator develops substantial financial economic applications. These help market microstructure researchers to better<br/>understand and model the real-time market dynamics, to better assess market quality, and to better regulate financial markets.<br/>"
"0604123","Integrated Likelihood Functions for Non-Bayesian Inference","DMS","STATISTICS","07/01/2006","06/07/2006","Thomas Severini","IL","Northwestern University","Standard Grant","Gabor Szekely","06/30/2009","$117,951.00","","severini@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1269","0000, OTHR","$0.00","The problem of dealing with nuisance parameters is a fundamental aspect of statistical theory and methodology, particularly in likelihood-based inference.  Commonly used approaches to eliminating a nuisance parameter from a statistical model include marginal and conditional inference and the use of the profile likelihood function.  An alternative approach is to use an integrated likelihood, in which the nuisance parameter is eliminated from the likelihood function by integration with respect to a given weight function. Integrated likelihoods have the advantage that they are always available and, unlike the profile likelihood, they are based on averaging rather than maximization, which has been shown to be a more reliable approach in many models of interest. The primary drawback of the integrated likelihood approach is that weight function needed for its implementation must be chosen. The goal of this research is to study the use of integrated likelihoods in non-Bayesian, likelihood-based, inference. The most important aspect of this is the construction of the weight function so that the resulting integrated likelihood function is useful for non-Bayesian inference. Other topics considered in the research include development of higher-order asymptotic theory, development of computational algorithms, comparisons with existing methods, applications to models with a high-dimensional nuisance parameter, and the application of the methodology<br/>to models used in practice.<br/><br/>This research develops a new approach to statistical theory and methodology, based on the use of an integrated likelihood function.  These methods are used in the analysis of virtually all statistical models and in all fields<br/>of application. In particular, integrated likelihood methods have been used in applications ranging from the reliability of computer software to the analysis of genetic data. In contrast to some other recently-developed methods, which require considerable background in advanced statistical theory, the integrated likelihood approach is relatively straightforward to understand and to implement. Thus, the results of this proposed research are useful for researchers in a wide range of fields. The results also further our understanding of the properties of statistical models and, hence, play an important role in the education of researchers in statistics and related fields.<br/><br/>"
"0604499","Statistical Analysis for Censored Data and Long Memory Data Using Wavelet Method","DMS","STATISTICS, EPSCoR Co-Funding","07/01/2006","06/20/2006","Linyuan Li","NH","University of New Hampshire","Standard Grant","Gabor Szekely","06/30/2008","$28,218.00","","linyuan@math.unh.edu","51 COLLEGE RD SERVICE BLDG 107","DURHAM","NH","038242620","6038622172","MPS","1269, 9150","0000, 9150, OTHR","$0.00","The wavelet method is widely used in many applications, such as signal processing, medical imaging, pattern recognition, and many others. In this project, the investigator focuses on applications of wavelet methods to analyze censored data and estimate non-parametric curves with long range dependence data. The project is to establish asymptotically optimal and robust estimators for nonparametric function estimations with censored data and long memory data. It extends beyond the standard Gaussian error assumption to non-Gaussian error structures, with long memory. <br/><br/>This project studies a class of important statistical tools called 'wavelet methods' for censored and long memory data. It has wide range of applications in signal process, medical imaging, pattern recognition and many others. It will provide more reliable and flexible methods to examine unknown structures of the underlying functional relationship between different features encountered in many applications.  <br/>"
"0605679","Conference on Methodological Developments of Statistics in Social Science","DMS","STATISTICS, Methodology, Measuremt & Stats","06/01/2006","04/18/2006","Lori Thombs","MO","University of Missouri-Columbia","Standard Grant","Gabor Szekely","05/31/2007","$20,000.00","Nancy Flournoy, Steven Osterlind, Douglas Steinley, Stanislav Kolenikov","thombsl@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269, 1333","0000, 7556, OTHR","$0.00","The investigator and her colleagues propose to host a national interdisciplinary conference in October 2006 to address key methodological issues related to social science statistics.  These areas are: structural equation modeling with latent variables, multilevel modeling, test theory and item response theory, social networks, survey sampling, cluster analysis and program evaluation.  Speakers include both prominent researchers in statistical and social science areas, along with beginning assistant professors, post-doctoral researchers, and graduate students.   The format of this conference provides ample opportunities for participation of young researchers through contributed papers, posters and roundtable panel discussion.  Another unique aspect of this conference is its interdisciplinary nature, providing an environment of exchange of ideas between persons that do not share common conferences and hence do not often interact. <br/><br/>Research in the social sciences is a dynamic area that has been traditionally driven by applications in psychology, sociology, education and economics.   This conference on cutting-edge Methodological Developments of Statistics in the Social Sciences brings together both established and young researchers from a diverse set of fields -- mathematical statistics, psychology, education, government, sociology, and political science. The results of this endeavor will enhance our understanding of the contemporary issues in quantitative methodologies of social sciences, and has enormous potential for the betterment of society.    <br/><br/>  <br/><br/>"
"0604396","Robust Functional Data Analysis","DMS","STATISTICS","06/01/2006","03/27/2009","Daniel Gervini","WI","University of Wisconsin-Milwaukee","Continuing Grant","Gabor Szekely","05/31/2010","$119,894.00","","gervini@uwm.edu","3203 N DOWNER AVE","MILWAUKEE","WI","532113153","4142294853","MPS","1269","0000, OTHR","$0.00","The analysis of samples of curves is a field of growing importance in Statistics. Samples of curves arise in longitudinal studies, where random processes are observed on groups of individuals. Often some of these curves are atypical compared with the rest of the sample, due either to individual peculiarities or to measurement errors. The most common techniques for functional data analysis are very sensitive to outlying curves, which may lead to invalid statistical inference. Outlier-resistant multivariate techniques are, in most cases, not directly applicable to functional data, where the number of observations per curve is usually larger than the sample size. Therefore, the investigator's goal is to develop robust methods for functional data analysis that provide valid statistical inference even in presence of a significant proportion of outlying curves. In particular, outlier-resistant estimators for the mean and the variance components are proposed and studied. The properties of these estimators (such as consistency, asymptotic distribution and breakdown point) are studied theoretically and empirically, the latter by simulation and analysis of real datasets. Algorithms and computer software implementing these methods are being developed.<br/><br/>Examples of functional data are human growth curves, gene expression profiles, and daily weather and environmental indicators (such as precipitation, temperature, pressure, pollution level), to mention just a few. Thus, detection of atypical growth curves can provide new  insights into the effect of diseases or other unusual circumstances on human growth, and detection of unusual gene expression profiles can help understand the genetic causes of abnormal biological processes or diseases. These examples illustrate the potential for application of the methods being developed by the investigator to areas beyond Statistics, such as public health and environmental sciences.<br/>"
"0604939","Monte Carlo and Quasi-Monte Carlo Methods for Statistics","DMS","STATISTICS","07/01/2006","08/21/2007","Art Owen","CA","Stanford University","Continuing Grant","Gabor Szekely","06/30/2009","$250,000.00","","owen@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","This project extends and improves Monte Carlo sampling techniques.  The main idea is to incorporate ideas that originated in quasi-Monte Carlo sampling into Monte Carlo sampling. In particular this project embeds quasi-Monte Carlo sampling into Markov chain Monte Carlo simulations, a combination that until recently was not known to be possible. The combination can be very effective, bringing variance reductions of over 100 fold in some problems.  The project personnel are identifying when quasi-Monte Carlo brings a large improvement in Markov chain Monte Carlo, as well as finding new ways to combine the methods.  Another area in which this project is improving Monte Carlo sampling is in integration of unbounded functions. There are versions of randomized quasi-Monte Carlo sampling that attain a better convergence rate than the original quasi-Monte Carlo sampling, at least for well behaved integrands. Unfortunately, problems with unbounded integrands don't see much improvement. This project extends the benefit of randomized quasi-Monte Carlo sampling to unbounded integrands by applying a change of variable formula to bound the integrand while endeavoring to prevent the integrand from becoming too spiky. This project is also investigating tempering methods for speeding up the mixing of Markov chain Monte Carlo as well as applications of Monte Carlo and quasi-Monte Carlo ideas to problems in bioinformatics.<br/><br/>Monte Carlo sampling is used in just about every branch of science and engineering. At its simplest it involves simulating a system using random number generators, and recording what happens.  In practice Monte Carlo methods are used to solve by brute force computation some problems that are too hard to do mathematically.  Real world complications that can easily be introduced into a simulation often make a problem too hard for exact mathematical treatment. Quasi-Monte Carlo methods can be used to drive a simulation byt replacing the random number sequence by very carefully chosen numbers that are much more balanced than random numbers are.  The result is often a tremendous speedup for a given level of accuracy, or a tremendous increase in accuracy for a given computation time. This project pushes quasi-Monte Carlo methods into simulations that had hitherto been thought incapable of benefiting from them. Those simulation techniques, known as Markov chain Monte Carlo, are used in many areas including materials science, analysis of educational testing data sets, biomedical research, robotics, computer graphics, and marketing. This project is also looking at other ways to improve Monte Carlo methods with similarly broad potential for benefit.<br/>"
"0603808","Travel Support  for the 8th Valencia/ISBA World Meeting on Bayesian Statistics","DMS","STATISTICS","04/01/2006","03/28/2006","Alan Gelfand","NC","Duke University","Standard Grant","Grace Yang","03/31/2007","$10,000.00","","alan@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, 7556, OTHR","$0.00","Abstract<br/><br/>The Valencia/ISBA Eighth World Meeting on Bayesian Statistics is jointly organized by the Universitat de Valencia and The International Society for Bayesian Analysis (ISBA). Valencia/ISBA 2006 will be held in Benidorm, Spain, from June 1 to June 6, 2006.  The University of Valencia has a strong tradition in Bayesian statistics, and has organized seven previous international meetings. The International Society for Bayesian Analysis was established in 1992 to promote the development and application of Bayesian statistical theory and methods. The co-operation of these two entities ensures a well-run meeting with strong intellectual content and good attendance from around the world.<br/>The ``Valencia"" meetings are the pre-eminent Bayesian gatherings in the world; attendance is predicted at over 500 participants. Previous meetings have been attended by participants from approximately 50 different countries, creating a truly international conference. This funding is exclusively to provide support to assist with travel expenses for junior statistical investigators from US institutions,<br/>i.e., persons pursuing a PhD in statistics or a closely related field, or who have received such a degree within the five years preceding the conference. Such investigators are often doing research that is among the most novel, interesting, and important for international dissemination. Yet, often they lack the travel funds necessary to attend such a conference because they have not yet established themselves sufficiently to attract external travel and other funding for their work.<br/><br/><br/>Statisticians are playing an increasingly indispensable role in interdisciplinary research, contributing as vital team members to the entire scientific enterprise. Scholarly conferences where new ideas can be exchanged are vital for statistical science to move forward. International meetings are particularly beneficial since they permit exposure to ideas and colleagues attendees might not ordinarily encounter in their usual worlds. The benefit for attendance at such meetings by junior statistical researchers is particularly important; these meetings provide a special, unique environment to enable connection with internationally-regarded researchers, to foster international collaborations, and to assist in the recruitment of outstanding graduate students to complete their studies in the U.S.  The Valencia/ISBA Eighth World Meeting on Bayesian Statistics is the pre-eminent Bayesian meeting in the world. This funding is exclusively to provide support to assist with travel expenses for junior statistical investigators from US institutions to attend this meeting.<br/>"
"0605141","Bayesian Analysis of Shapes and Curves with Applications in Structural Bioinformatics","DMS","STATISTICS","09/01/2006","08/31/2006","Scott Schmidler","NC","Duke University","Standard Grant","Gabor Szekely","08/31/2007","$39,870.00","","schmidler@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, OTHR","$0.00","<br/>This research involves development of new probabilistic and Bayesian statistical models for the analysis of three-dimensional structures of biological macromolecules. A secondary focus is the development of statistical methods for characterizing the biomechanical behavior of molecules under applied forces such as arise in the study of cellular proteins (e.g. molecular motors) and in problems of bioengineering (e.g. nano-scale sensing"") and materials science (e.g. smart materials and biologically inspired materials). The emphasis is on comparative modeling and classification of protein structures and protein force-response curves. Stochastic models of three-dimensional shapes are explored motivated by the difficulties in quantitative comparison of complex structural information of proteins. Bayesian techniques are developed for unsolved problems of matching unlabeled point sets using extensions of techniques from the area of statistical shape analysis. Computational algorithms for making these techniques practical for high-throughput searching by biomolecular scientists are also a major focus of this research, and several directions are developed involving exact algorithms, Monte Carlo sampling techniques, and rapid approximations based on geometric algorithms. A second component of this research involves Bayesian models for errors-in-variables regression and functional data analysis techniques for force-extension curves measured on single molecules. The development of Bayesian hierarchical random-effects models for functional data analysis, along with development of efficient algorithms for Bayesian computations, is carried out. <br/><br/>This statistical methodology research is of broad applicability, but all work is clearly motivated by and directly relevant to significant interdisciplinary applications in biomedical science and biomolecular engineering. The research constitutes important advances in stochastic modeling for applications in computational biology, bioinformatics, and computational chemistry, and will provide both unified theoretical frameworks and practical data analysis techniques and software tools for scientific researchers and engineers involved in these areas. Such advances will be important to statisticians working in this area, as well as domain scientists in need of new methodology and tools for data analysis and predictive modeling.<br/><br/>"
"0605167","Frechet differentiation of functions of operators with application in functional data analysis","DMS","STATISTICS","06/01/2006","04/19/2006","Frits Ruymgaart","TX","Texas Tech University","Standard Grant","Gabor Szekely","05/31/2008","$37,468.00","","ruymg@math.ttu.edu","2500 BROADWAY","LUBBOCK","TX","79409","8067423884","MPS","1269","0000, OTHR","$0.00","<br/>This research deals on the one hand with a statistical problem in functional data analysis and on the otherwith a mathematical tool from perturbation theory that might also be of independent interest.  More specifically the statistical problem concerns the limiting distribution of properly defined functional canonical correlations and their corresponding canonical variates.  Functional canonical correlations are of interest in their own right, but may also help organize high-dimensional data sets.  The mathematical tool to be derived is the Frechet derivative of an analytic function of a compact, nonnegative, Hermitian operator (in the sense of functional calculus), tangentially to the space of all compact Hermitian operators.  This Frechet derivative allows one to obtain, for instance, the asymptotic distribution of a function of the covariance operator by applying the ensuing delta-method.  This type of result provides the major ingredient for deriving the aforementioned asymptotic distributions.<br/> <br/>In practice the theory applies to data that are independent copies of a certain infinite dimensional signal.  A well-known instance of such a signal in the literature is the gait example where one part of the signal represents the variability in the knee angle cycle and the second part that in the hip angle cycle.  Canoncial correlations and their corresponding canonical variates may capture a significant part of the relation between the two cycles.  When applied to this example the proposed research should lead to the asymptotic distributions of these objects and in particular to confidence intervals for the canonical correlations."
"0643684","Theory and Applications of Sharp Nonparametric Estimation and Learning","DMS","STATISTICS","08/10/2006","08/14/2006","Sam Efromovich","TX","University of Texas at Dallas","Standard Grant","Grace Yang","08/31/2007","$15,776.00","","efrom@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1269","0000, 9150, OTHR","$0.00","abstract<br/><br/>PI:  SAM EFROMOVICH<br/>proposal number:  0243606<br/><br/>The primary focus of this research is to develop general methods of data-driven statistical estimation and learning motivated by and tested on environmental, medical and biological applications.  The main intellectual objectives are threefold:<br/>(A) In the case of settings with known sharp asymptotics (like censored or biased datasets), develop the theory of the onset of the sharp optimality and the equivalence between statistical models for small datasets; <br/>(B) In the case of models with indirect observations and nuisance functions (like error density estimation in heteroscedastic nonparametric regression or recovery of a hidden component in time series), develop the theory of sharp estimation and sampling with fixed accuracy; <br/>(C) In the case of inverse problems with unknown operator, develop data-driven learning machines implying sharp estimation. <br/>Practical problems include statistical modeling of temporal and spatial structures of plants in Sevilleta National Wildlife Refuge, modeling of arsenic concentration in Albuquerque water basin, the study of municipal wastewater treatment plants, statistical modeling of spreading hantavirus, and learning machines for recovery magnetic resonance images.<br/><br/>The primary focus of this research is to develop, in collaboration with Sandia National Laboratories and the UNM Medical School, algorithms and software for adaptive statistical estimation and learning motivated by and tested on the following environmental, medical and biological applications:  Statistical modeling of temporal and spatial structures of plants in Sevilleta National Wildlife Refuge; Modeling of arsenic concentration in Albuquerque water basin; Study of municipal wastewater treatment plants; Statistical modeling of spreading hantavirus; Learning machines for recovery magnetic resonance images.  The broader impact of the research is defined by the well-understood applications that can encourage students to study mathematics and can help a broader audience to understand the importance of statistics.  The impact is based on the following activities:<br/>(i) Developing a new course on adaptive statistical estimation taught via the UNM web-based program;<br/>(ii) Weekly scientific seminars (supported in part by private grants) held for undergraduate and graduate students, and talks during the UNM mathematical awareness weeks for high-school students;<br/>(iii) Regular presentations at outreach seminars conducted by the UNM Valencia campus to broaden participation of under-represented groups;<br/>(iv) Posting the developed software, databases, and practical findings, that can be of interest to a broader audience, on the investigator's webpage;<br/>(iv) Publishing of medical, environmental and biological findings, benefiting the society, in non-technical journals.<br/><br/>"
"0638468","Nonparametric Curve Estimation: Theory and Practice","DMS","STATISTICS","07/01/2006","08/29/2008","Sam Efromovich","TX","University of Texas at Dallas","Continuing Grant","Gabor Szekely","07/31/2011","$176,144.00","","efrom@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1269","0000, 9150, OTHR","$0.00","<br/>ABSTRACT<br/><br/>   The proposal focuses on developing statistical theory, methodology and methods of data-driven nonparametric curve estimation motivated by and tested on biological, psychological, medical, and <br/>environmental applications, with the main aim to bridge the asymptotic theory and applications. The main intellectual objectives are: (i) For models with indirect observations (like censored and biased regression, hidden components, modeling of time series in the presence of nonparametric trend and volatility) develop the theory of sharp minimax estimation as well as of mimicking of oracles that know direct observations and/or nuisance functions unavailable to the statistician; (ii) For controlled experiments, develop the theory, methodology and methods of sequential sampling and estimation for the case where optimal (under more common criteria) solution depends on estimated and/or nuisance functions; (iii) Develop the theory and methods of a local aggregation of wavelet estimates. Practical problems include the study of municipal wastewater treatment plants, modeling of levels of contaminants and residual disinfectants in Albuquerque water basin, analysis of drinking patterns and behavior change initiation during pregnancy, study of temporal and spatial structures of plants in Sevilleta National Wildlife Refuge, learning machines for recovery magnetic resonance images and the analysis of satellite data.<br/><br/>   The primary focus of this research is to develop, in collaboration with Sandia and Los Alamos National Laboratories as well as with the UNM Medical and Engineering Schools, algorithms and software for statistical learning and adaptive estimation motivated by and tested on the following environmental, medical and biological applications: Statistical modeling of temporal and spatial structures of plants in Sevilleta National Wildlife Refuge which will allow to study global weather changing; The study of municipal wastewater treatment plants; Modeling and analysis of change points in levels of contaminants and residual disinfectants in Albuquerque water basin with applications to a homeland security drinking water monitoring; Learning machines for recovery magnetic resonance images and the analysis of environmental satellite data including temperature and humidity; Analysis of drinking patterns and behavior change initiation during pregnancy which reduces likelihood of relapses. The broader impact of the research is defined by well--understood applications that will benefit the society and help students and a broader audience to understand the importance of mathematical sciences. The impact will be based on the following<br/>proposed activities: (i) Graduate students will participate in the research; (ii) To promote learning, scientific seminars will be held for undergraduate and graduate students, and talks by the proposer and the students will be presented during the UNM mathematical awareness weeks for high-school students. (iii) To broaden participation of under-represented groups, regular presentations will be held at outreach seminars conducted by the UNM Gallup and Valencia campuses. (iv) The developed software will be freely available. Medical, environmental and biological findings, benefiting the society,  will be published in not-technical journals.<br/>"
"0604726","Classification Based on Data Depth Ordering","DMS","STATISTICS, EPSCoR Co-Funding","07/01/2006","06/30/2006","Asheber Abebe","AL","Auburn University","Standard Grant","Gabor Szekely","06/30/2009","$119,977.00","Nedret Billor","abebeas@auburn.edu","321-A INGRAM HALL","AUBURN","AL","368490001","3348444438","MPS","1269, 9150","0000, 9150, OTHR","$0.00","The problem of classifying entities into one of several groups has been one of the main goals of many scientific investigations. This is a very important statistical problem with many applications in science and engineering. This is an inherently multivariate problem since measurements are made on several aspects (variables) of the entity as an attempt to best capture its place among others. As it is not generally possible to obtain a measure all variables pertaining to an entity we wish to classify or even get perfect measurements of the measured variables, classifications are usually performed in the presence of uncertainty. It is important that this activity is done in a manner that minimizes the misclassification error rate with efficiency and in a way that is robust to outlying cases.  In this project the investigators develops a new classification technique that is based on statistical depth function, an extension of the robust rank-based nonparametric procedures. Issues of asymptotic optimality, robustness, computational algorithms and finite sample performance are dealt with step by step and methodically throughout the duration of the project. The project also includes implementations of the newly developed classifier using genetic data sets that are publicly available and comparing its performance to existing classifiers.<br/><br/>The problem of classifying entities into one of several groups has been one of the main goals of many scientific investigations. For instance, identifying a tumor or a case of flu as one of the many different possibilities is potentially life-saving and hence is indespensable to physicians. This is an inherently multivariate problem since measurements are made on several aspects (variables) of the entity as an attempt to best capture its place among others. As it is not generally possible to obtain a measure all variables pertaining to an entity we wish to classify or even get perfect measurements of the measured variables, classifications are usually performed in the presence of uncertainty. It is important that this activity is done in a manner that minimizes the misclassification error rate with efficiency and in a way that is robust to outlying cases.  In this research the investigators develops a new classification technique that is insensitive to erroneous or extraordinary data.  Such methods are very important to for researchers in areas such as genetics and oncology where these qualities are of the utmost importance, as well as cancer classification using gene expression data where a precise and accurate classification of tumors is essential for successful treatment of cancer.<br/><br/> <br/>"
"0652353","Collaborative Research: Statistical Analysis on Manifolds: A Nonparametric Approach for Shapes and Images","DMS","STATISTICS","08/09/2006","11/17/2006","Victor Patrangenaru","FL","Florida State University","Continuing Grant","Gabor Szekely","08/31/2007","$15,277.00","","vic@stat.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","0000, OTHR","$0.00"," The shape or image of an object may be recorded digitally by a finite <br/>number k of landmarks or positions on the object, called a k-ad. The space <br/>of orbits under rotation and translation of k-ads is the size-and-shape <br/>space. If one includes scaling with rotation and translation, then the <br/>space of orbits under the resulting group of transformations on the k-ads<br/>is the shape space. These are examples of Riemannian manifolds, some with <br/>singularities. One approach proposed here for inference about a <br/>distribution on a general differentiable or Riemannian manifold M, based <br/>on a random sample from it, is to carry out a multivariate analysis of the <br/>image under the so-called Log map on one or more tangent spaces to M. <br/>Apart from this intrinsic approach, less computation intensive extrinsic <br/>procedures based on embeddings in Euclidean spaces are investigated. The <br/>project explores consistency and asymptotic distribution theory on <br/>manifolds for robust tests and confidence regions from both points of <br/>view--intrinsic and extrinsic.<br/><br/>   One motivation for this study comes from the need to identify <br/>deformations or shape changes for purposes of medical diagnosis and <br/>biomorphology. Immediate applications also arise to machine vision and <br/>pattern recognition. There are significant impacts of this research on <br/>these and many other fields. Another important goal of this project is to <br/>train students in the newly developed methodologies, leading to the <br/>dissemination of knowledge gained through this research and the creation <br/>of a body of technicians and experts to apply it.<br/>"
"0531839","Workshop on Frontiers of Statistics: Nonparametric Modeling of Complex Data","DMS","STATISTICS","04/01/2006","06/15/2005","Jianqing Fan","NJ","Princeton University","Standard Grant","Grace Yang","03/31/2007","$16,000.00","","jqfan@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","0000, OTHR","$0.00","Prop ID:  DMS-0531839  <br/>PI:  Fan, Jianqing  <br/>Institution:  Princeton University  <br/>Title:  Workshop on Frontiers of Statistics: Nonparametric Modeling of Complex Data   <br/><br/> This is an extremely well conceived worshop that will bring together some of the <br/>The frontiers of statistics are dynamic and vibrant.  The aim of this<br/>proposal is to organize a workshop on the frontiers of statistics that<br/>involve large and complex data. The conference will focus on data-analytic<br/>nonparametric techniques in diverse fields of disciplines such as<br/>computational biology, financial econometrics, machine learning, and<br/>industrial engineering.  It will also include various topics on<br/>statistical theory and methods.  A distinguished feature of the conference<br/>is that it addresses emerging statistical problems that arise from various<br/>frontiers of science. It will be very helpful for shaping the future<br/>directions of statistical research.<br/><br/><br/>The purpose of this proposal is to bring together top and junior<br/>researchers to define and expand these research frontiers of statistics.<br/>The workshop provides a focal venue for top and junior researchers to<br/>gather, interact and present their new research findings, to discuss and<br/>outline emerging problems in their fields, and to lay the groundwork for<br/>fruitful future collaborations.  Moreover, it will provide an overview for<br/>graduate students who are interested in seeking for an interesting area to<br/>work.  The workshop focuses on various nonparametric statistical problems<br/>from various frontiers of science and engineering such as computational<br/>biology, financial econometrics, machine learning, and industrial<br/>engineering. It will be very helpful for shaping the future directions of<br/>statistical research that addresses problems of high societal impact.<br/><br/><br/>"
"0610295","Workshop on Statistical Analysis of Neuronal Data","DMS","STATISTICS","03/01/2006","02/22/2006","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Rong Chen","02/28/2007","$10,000.00","","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, 7556, OTHR","$0.00","Studies of the neural basis of behavior typically use time-varying stimuli and produce time-varying neuronal responses. Statistically, the setting involves both continuous multiple time series and inhomogeneous point processes, sometimes dozens or hundreds of them observed simultaneously. There are many challenging analytical issues, including that of combining information obtained from multiple modalities (EEG, fMRI, MEG, and extracellular recordings). The third workshop Statistical Analysis of Neuronal Data will be devoted to discussion of these issues.<br/><br/>The Statistical Analysis of Neuronal Data (SAND) workshops are held in even years in Pittsburgh, PA. The third workshop is planned for May 11--13, 2006. SAND3 will bring together neurophysiologists, statisticians, physicists, and computer scientists who are interested in quantitative analysis of neuronal data. In addition to four scientific sessions, we will run a pair of half-day short courses to provide relevant background on neurophysiological data, for statisticians, and in state-of-the-art statistics for experimentalists.  This workshop series aims to define important problems in neuronal data analysis and useful strategies for attacking them; foster communication between experimental neuroscientists and those trained in statistical and computational methods encourage young researchers, including graduate students, to present their work; expose young researchers to important challenges and opportunities in this interdisciplinary domain, while providing a small meeting atmosphere to facilitate the interaction of young researchers with senior colleagues.<br/><br/><br/>"
"0605165","High-Dimensional Challenges in Statistical Machine Learning: Theory, Models and Algorithms","DMS","STATISTICS","08/15/2006","08/10/2006","Bin Yu","CA","University of California-Berkeley","Standard Grant","Gabor Szekely","07/31/2010","$450,000.00","Martin Wainwright","binyu@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","TECHNICAL SUMMARY: This research proposal consists of four closely related research thrusts, all centered around the common goal of an integrated treatment of statistical and computational issues in dealing with high-dimensional data sets arising in information technology (IT).  The first two research thrusts focus on fundamental issues that arise in the design of penalty-based and other algorithmic methods for regularization.  Key open problems to be addressed include the link between regularization methods and sparsity, consistency and other theoretical issues, as well as structured regularization methods for model selection.  Sparse models are desirable both for scientific reasons including interpretability, and for computational reasons, such as the efficiency of performing classification or regression. The third research thrust focuses on problems of statistical inference in decentralized settings, which are of increasing importance for a broad variety of IT applications such as wireless sensor networks, computer server ``farms'', traffic monitoring systems.  Designing suitable data compression schemes is the key challenge. On one hand, these schemes should respect the decentralization requirements imposed by the system (e.g., due to limited power or bandwidth of communicating data); on the other hand, they should also be (near)-optimal with respect to a statistical criterion of merit (e.g., Bayes error for a classification task; MSE for a regression or smoothing problem).  The fourth project addresses statistical issues centered around the use of Markov random fields, widely-used for modeling large collections of interacting random variables, and associated variational methods for approximating moments and likelihoods in such models.<br/><br/>BROAD SUMMARY: The field of statistical machine learning is motivated by a broad range of problems in the information sciences, among them remote sensing, data mining and compression, and statistical signal processing.  Its applications range from homeland security (e.g., detecting anomalous patterns in large data sets) to environmental monitoring and assessment (e.g., estimating changes in Arctic ice).  A challenging aspect to such applications is that data sets tend to be complex, massive (frequently measured in terabytes of data), and rich in terms of possible features (hundreds of thousands to millions). These characteristics presents fundamental challenges in the design and application of statistical models and algorithms for testing<br/>hypotheses and performing estimation.  Whereas classical statistical methods are designed separately from computational considerations, dealing effectively with extremely high-dimensional data sets requires that computational issues be addressed in a more integrated manner during the design and testing of statistical models; and that issues of over-fitting and regularization, while always statistically relevant, become of paramount importance.<br/>"
"0606577","Collaborative Research: Statistical Learning and Object Oriented Data Analysis","DMS","PROBABILITY, STATISTICS","07/01/2006","04/18/2006","James Marron","NC","University of North Carolina at Chapel Hill","Standard Grant","Gabor Szekely","06/30/2010","$250,834.00","Yufeng Liu, Haipeng Shen","marron@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1263, 1269","0000, OTHR","$0.00","This research is in the related areas of Statistical Learning and Object Oriented Data Analysis (OODA). There are major challenges in these areas that are addressed by a team of researchers, who bring different but complementary skill sets to explore. Statistical Learning is widely recognized as a very active area of interdisciplinary research, which lives between statistics, computer science, and optimization. With state-of-art optimization tools, this research offers a set of new approaches for statistical learning, including new penalties for regularization, further developments of large margin classifiers both theoretically and numerically, as well as nonparametric-based probability calibration for hard margin classifiers. In addition, new visualization and analytical tools for ``High Dimension-Low Sample Size'' (HDLSS) data are developed. Such development is extremely important since HDLSS has become a common feature of data encountered in many divergent fields such as medical imaging and micro-array analysis for gene expression but is outside of the domain of classical statistical multivariate analysis. OODA is a generalization of the recently very productive area of Functional Data Analysis (FDA). In FDA, curves are data points and variation in a family of curves is the focus of analysis. OODA extends this notion to populations where the data points are more complex objects, such as images, shape representations, and even tree-structured objects. The proposed research offers a set of new tools for FDA, including exponential family functional principal components analysis (PCA), robust functional PCA, curve discrimination, and forecasting and dynamic updating of time series of curves. Proposed research will also advance OODA for data on smooth manifolds and tree-structured objects.<br/><br/>The main application area of the research is in health and medicine and civil infrastructure. The research is motivated by and will have beneficial impacts on cancer research, medical imaging, call center management, and network traffic modeling.  However, the developed statistical methods will be useful in fields far beyond those motivating this research, such as demography/epidemiology, financial economics and spatial-temporal modeling.  The team consists of a good mix of well established senior researchers and young junior researchers. Strong mentoring at several levels is an important component of this project. First, there is  strong training of graduate students, in these exciting new research areas, with the goal of giving them the background, and skills needed to start their own research careers. Second, there is strong mentoring of the junior researchers, by the more experienced members of the research team.  In addition to working closely together on research projects, the junior researchers will learn the skills of advising PhD students, through joint supervision together with the more senior members. The team  continues to disseminate the research results quickly and broadly through collaborative work, academic presentations, and journal publications. Web pages are created to enable quick access to user-friendly and accessible software implementations of new methods as well as technical reports and relevant references.<br/>"
"0605434","Optimal Design of Experiments for Correlated Observations","DMS","STATISTICS","07/01/2006","05/27/2009","Zhengyuan Zhu","NC","University of North Carolina at Chapel Hill","Continuing Grant","Gabor Szekely","06/30/2010","$218,961.00","Richard Smith","zhuz@iastate.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","0000, OTHR","$0.00","<br/><br/>ABSTRACT:<br/><br/>Much research has been done on the optimal design of experiments for independent observations, while for correlated observations there are still many challenging problems that remain to be solved. The investigators develop optimality criteria which quantify the uncertainty for complex models of correlated observations, including spatial Gaussian and non-Gaussian random field models and extreme value processes. Algorithms based on newly developed optimization techniques and extensions of design measures to correlated observations are investigated as well. The intellectual merit of the research is new theory, methodology, and <br/>computational techniques for design, in particular spatial and space-time designs where the correlations between observations cannot be ignored. This includes theories for quantifying uncertainties in spatial <br/>prediction, design algorithms using quadratic and semidefinite programming, and novel ways of defining design measure for correlated observations and their application in space-time design.<br/><br/>In many applications, data are collected from a network of monitors in space and time to make predictions. The investigators study the problem of how to optimally place the monitors so that one can have the most accurate prediction. The direct motivation of this work is from environmental science, where networks are used to monitor the pollutants. Air pollution is known to be associated with human health, and the methods developed in this proposal can help researchers quantify the uncertainties in the pollution estimates, which may contribute to better understanding of the association between pollution and human health. In particular interactions with EPA researchers may lead to improved monitoring of atmospheric pollutants. The investigators also intend to study possible applications to Project BioWatch, which provides an early warning system for bio-threats. The impact of this research is not limited to the environmental science, as the methods and algorithm are in principle applicable to many other fields such as climatology and chemical kinetic models.<br/><br/>"
"0604758","Statistical Inference for High Frequency Data","DMS","STATISTICS","09/01/2006","07/15/2010","Per Mykland","IL","University of Chicago","Continuing Grant","Gabor Szekely","08/31/2012","$220,000.00","","mykland@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00","The project will investigate the estimation of volatility-like objects in the context of the hidden semi-martingale model. Apart from volatility, we are concerned with co-variations, ANOVA, leverage effect, and related quantities. The project uses ideas from contiguity and unbiased estimation to find such estimators. Data are assumed to have high frequency, so that small-interval asymptotics will be used. A main part of the project is concerned with the applications of such estimators.  The investigator's earlier findings on nonparametric, trading based, risk management for options will be interfaced with the estimators to find complete procedures for safely unwinding dangerous positions. The estimators will also be combined with forecasting techniques to provide high-frequency based competitors to latent volatility models like GARCH. We can here draw on the martingale type error structure in the high frequency estimation. The economic value of the estimators in terms of portfolio management will also be investigated.<br/><br/> A main background for the project is the increasing availability of high frequency data for financial securities prices. This permits, in principle, very precise determination of volatility and similar characteristics of prices. The investigator's finding, however, that prices behave as if they have measurement error, raises a number of questions about how the statistics is carried out. This project will be concerned with both estimation, and applications to risk management, forecasting, portfolio management, and regulation. The results are of interest to investors, regulators, and policymakers.<br/><br/>"
"0631501","Advancing Statistical Thinking and Practice through Advocacy and Assessment","DMS","STATISTICS","06/15/2006","07/03/2006","George Tiao","IL","University of Chicago","Standard Grant","Gabor Szekely","05/31/2007","$16,500.00","","george.tiao@chicagogsb.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, 7556, OTHR","$0.00","The principal investigator and his colleagues have designed this conference to reach out to faculty and professionals interested in statistics for business and economics, particularly in the context of business schools. The primary aims of the conference are to improve the teaching of statistics and the level of statistical thinking in business education and practice.  In addressing these themes, the conference has sought to attract faculty from both teaching and research oriented schools.  In addition, major aims of the conference are to draw in faculty members from small schools and to encourage the participation of doctoral students in statistics who are planning to become faculty members in a business school.  The organizing committee recognizes that funding is often not available for these groups and so will provide grants to enable these individuals to participate.<br/><br/>The conference themes reflected in the title will be addressed by special sessions on statistical advocacy, recent research developments, the use of technology to stimulate active learning, and a panel discussion on how to improve statistical practice in business schools.  The conference will also include poster sessions for participants, and software and textbook exhibits.<br/><br/><br/>"
"0604576","Some New Developments in Competing Risks Models -- Extensions and Applications","DMS","STATISTICS","07/01/2006","04/07/2008","Yanqing Sun","NC","University of North Carolina at Charlotte","Continuing Grant","Gabor Szekely","06/30/2010","$119,999.00","","yasun@uncc.edu","9201 UNIVERSITY CITY BLVD","CHARLOTTE","NC","282230001","7046871888","MPS","1269","0000, OTHR","$0.00","In this project, the investigator studies an extension of the competing risks model to allow a continuum of competing risks, in which the cause of failure is replaced by a continuous mark variable only observed at the uncensored failure times, and its applications in the HIV vaccine efficacy studies. The investigator develops statistical methods for the mark-specific proportional hazards model, allowing the regression parameters to depend nonparametrically on the mark and the baseline hazard to depend nonparametrically on both time and mark. This research is motivated by the need to assess  HIV vaccine efficacy, while taking into account the divergence of infecting HIV viruses in trial participants from the HIV strain that is contained in the vaccine, and adjusting for covariate effects. The vaccine efficacy is expressed in terms of one of the regression functions in a mark-specific proportional hazards model. The research can find applications in other medical researches as well. The mark-specific proportional hazards model and its applications to vaccine efficacy trials is investigated when the mark variable is dsicrete/continuous and univariate/multivariate. It is studied under the case-cohort designs where some of the covariates may only be observed for a subset of the sample. The semiparametric mark-specific proportional hazards model is also studied. The statistical procedures concerning the mark-specific hazards functions naturally extend the scope of methods that have been developed for competing risks data with discrete marks and for failure time data with single cause of failure.  Since the mark-specific relative risks measure not only the relative risks of developing the end-point event given the marks, but also depend on differential exposure to the marks, one needs to be careful in their interpretations. To allow for greater flexibility, direct modeling of the conditional hazard function of failure time given the mark and covariate is considered.  The new methods are justified theoretically, evaluated in simulations and applied to the HIV vaccine efficacy trials.<br/><br/>The investigator studies an extension and new applications of the classical competing risks model. The goal of the reseach is to develop statistically efficient and biologically interpretable methods for evaluating and achieving efficacious HIV vaccines. The theoretical justifications for the statistical methods are very challenging. By pursuing this research, significant progress could be made in the theory of competing risks models and its applications. The methods developed in this research will be used to analyze the data collected from the HIV vaccine efficacy trials and provide useful input for developing more effective vaccines. The research of the problems proposed here will also generate many research topics at different levels suitable for graduate and undergraduate studies, therefore promotes involvements of students in the research of current sciences.<br/><br/>"
"0604931","Measure of Dependence, Model Selection and Multiple Testing in Regression","DMS","STATISTICS","07/15/2006","06/10/2008","Kjell Doksum","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","06/30/2010","$140,001.00","Kam-Wah Tsui","doksum@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","0000, OTHR","$0.00","This proposal considers multiple regression procedures for analyzing the relationship between a response variable and a vector of covariates.  It introduces an approach which deals with the dilemma that with high<br/>dimensional data the sparsity of data in regions of the sample space makes estimation of nonparametric curves and surfaces virtually impossible by choosing procedures whose relative variability (noise) is minimized.  This is accomplished by abandoning the goal of trying to estimate true underlying curves and instead introducing measures of dependence that may be able to determine important relationships between variables.  These dependence measures are expressed in terms of tuning constants that are<br/>chosen to maximize a signal to noise ratio.  More precisely, the tuning parameter is a vector which gives the window size of local regions in the covariate space where we do local parametric fits of the response variable<br/>to the covariates.  The signal is a local estimate of a dependence parameter which depends on the window size, and the noise is the standard error (SE, an estimate of the standard deviation) of this estimate.  This approach of choosing the window size to maximize a signal to noise ratio lifts the curse of dimensionality because for regions with sparsity of data the SE is very large. It includes model selection where the variables that contribute insignificant signals compared to their SE's are eliminated.<br/><br/>It is proposed to develop procedures that can be used to determine relationships between factors in studies involving a large number of factors and complex relationships. The proposed methodology is applicable<br/>generally without any restrictive conditions. It involves the discovery of key relationships in studies where there are a great number of factors that need to be screened for their relevance. The dimension reduction and<br/>discovery techniques in this proposal are useful in a variety of scientific and engineering contexts including genetics and bioinformatics. In summary, it is proposed to develop methods that will be useful in studies involving large and complex data sets that are common in many areas including studies of health, the environment, and finance.<br/>"
"0604486","Assessing the Readability of Documents and Statistical Tools for Non-Euclidean Data","DMS","STATISTICS","06/01/2006","03/06/2007","Guy Lebanon","IN","Purdue University","Continuing Grant","Gabor Szekely","05/31/2008","$112,277.00","","lebanon@cc.gatech.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","0000, OTHR","$0.00","Documents are written with a specific audience in mind that varies across several dimensions. One such dimension is the readability level, which may vary from elementary child readability to adult readability. The investigator developd statistical models for readability prediction and experiment with different alternatives. As most standard representations of documents are not well described using Euclidean geometry, the investigator directd his research at non-Euclidean modeling of the word histogram or term-frequency representation. Specifically, the task is that of non-linear regression where the covariates are points in the simplex, but do not obey Euclidean geometry.<br/> <br/>The task of predicting the readability of documents is an important one. A likely implication of advances in this area is improvement in matching readability level with documents retrieved by search systems. This in turn will positively effect children and non-native speakers of English in their internet searches and other<br/>automated textual efforts. As the research is interdisciplinary it is expected to bring together and foster future collaboration between the communities of statistics, machine learning and information retrieval.<br/><br/><br/>"
"0604572","A New Paradigm for Classification Based on Dissimilarity Information via Regularized Kernel Estimation","DMS","STATISTICS","08/01/2006","07/01/2008","Grace Wahba","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","10/31/2010","$277,128.00","","wahba@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","0000, OTHR","$0.00","        ABSTRACT<br/><br/>A New Paradigm for Classification Based on Dissimilarity Information <br/>     Via Regularized Kernel Estimation<br/><br/>     Grace Wahba, PI<br/><br/><br/>The objective of this research is to develop improved methods for classification and clustering  <br/>when attribute vectors for the objects of interest are either not known or or are of a much <br/>higher dimension than is useful, but when dissimilarity information between pairs of objects is <br/>available. In the work being proposed, this dissimilarity information may be subjective, crude, noisy, <br/>incomplete, confined within a nonlinear manifold, come from multiple sources and/or be inconsistent.<br/>The approach is to build on some preliminary work by the PI and collaborators, who have initiated two <br/>new robust nonparametric methods for obtaining positive definite kernels (a.k.a ""reproducing kernels"")<br/>from  noisy dissimilarity data under various circumstances. These kernels generate  ""pseudo-attribute"" vectors which may be used for clustering, for outlier detection, or in a support vector machine with copiously labeled data, or with sparsely labeled data (""semi-supervised learning"") for classification. <br/>Tasks are proposed to build a series of optimized classification  systems under a variety of scientifically important scenarios regarding the nature of the data available, which combine robustly estimated kernels with support vector machines to effect classification based on dissimilarity information. It is proposed  to develop theoretically valid  and practically useful optimization procedures and efficient algorithms<br/>for these systems, test the results in carefully designed test beds where the answer is known, apply them to <br/>a variety of different classification tasks, compare the results with related systems, and publicize the results.<br/><br/>With the availability of extremely large amounts of data and high speed computing, modern classification tools are doing impressive things in speech recognition, text classification, image analysis, and classification of proteins and microarray data, among other things. However there is still much room for improvement<br/>in certain areas.  This work will provide a unique and novel contribution to the theory and practice of classification when the data available  may be subjective, crude, noisy, incomplete, satisfy complex constraints, come from multiple sources and may be inconsistent. It is anticipated that the proposed work will provide improved methods of statistical analysis that have the potential to seriously impact essentially any engineering or scientific endeavor that collects data to be classified.<br/>"
"0603868","Multiple Testing:  Further Development Of Theory And Methodology","DMS","STATISTICS","09/01/2006","05/22/2006","Sanat Sarkar","PA","Temple University","Standard Grant","Gabor Szekely","08/31/2009","$169,907.00","","sanat@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","MPS","1269","0000, OTHR","$0.00","<br/>MULTIPLE TESTING: FURTHER DEVELOPMENT OF THEORY AND <br/>METHODOLOGY<br/><br/> The primary goal of the investigator in this project is further development of statistical theory and methodology for problems in multiple hypothesis testing. Having realized that the traditional idea of controlling the familywise error rate (FWER) is too stringent to use when large number of hypotheses are tested, researchers have recently focused on defining less stringent error rates and developing methods that control them. The false discovery rate (FDR) is the first of these receiving considerable attention, even though there are still a number of important related issues that are yet to be answered. The probabilities of rejecting at least a number more than one and the false discovery proportion exceeding a certain threshold are the two most recently introduced concepts that generalize the FWER and represent meaningful alternatives to the FDR. While some methods controlling them have been suggested very recently, the potential to developing newer and more powerful procedures is revealed by some recent work by the investigator. The development of such new procedures and investigative studies comparing them to related methods, theoretically as well as empirically, will be one major thrust of the present research. When testing multiple null hypotheses against two-sided alternatives, making directional decisions for the alternative hypotheses corresponding to the rejected ones with a control of false directional rejections is often desired. While such a procedure has been recently put forward in the framework of the FDR, there is a good potential of developing newer procedures in light of some work of the investigator together with students and colleagues. The development of such new procedures will be the second major thrust of this research.<br/><br/>The results from this research will be of importance to virtually any statistical investigation where questions are posed in terms of testing several hypotheses. One particular area of application is DNA microarrays which are a new and promising biotechnology that can monitor expression levels in cells for thousands of genes simultaneously. The identification of differentially expressed genes is an important and common question in these experiments. The biological question of differential expression is framed as a statistical problem of multiple hypotheses testing: the simultaneous test for each gene of the null hypothesis that the expression levels do not associate with the responses or covariates of interest. Developing a statistical procedure of discovering the genes that are differentially expressed by controlling statistical measures of false discoveries or false non-discoveries at a desired level is one of the central issues in such a problem. Another area of application is pharmaceutical investigations where multiple testing techniques are routinely used in dose-response study or in evaluating a drug's efficacy over a standard drug or placebo. This research has the potential to generate collaborations with researchers in the biopharmaceutical industry and medical schools. It would also benefit education through training of graduate students, incorporation of the developed methodologies in statistics courses, and writing textbooks.<br/>"
"0604942","Space-Time Variograms and  Covariance Models","DMS","STATISTICS, EPSCoR Co-Funding","08/15/2006","08/14/2006","Chunsheng Ma","KS","Wichita State University","Standard Grant","Gabor Szekely","07/31/2009","$120,000.00","","chunsheng.ma@wichita.edu","1845 Fairmount","Wichita","KS","672600007","3169783285","MPS","1269, 9150","0000, 9150, OTHR","$0.00"," <br/>The investigator studies a variety of space-time random fields, including Gaussian random fields, non-Gaussian random fields, and Poisson point processes, and uses variograms and covariance functions as main tools to describe  space-time interaction and  dependence. The  aims of the project are to provide valuable techniques for constructing non-Gaussian random fields;  to introduce new spatio-temporal variograms and covariance models with flexibility and  physical interpretablity; to investigate their important properties so  that practitioners would more easily choose appropriate models with effective and efficient practical usage; to develop algorithmically efficient methods for simulating space-time random fields with such variograms and covariance functions; to perform suitable statistical inference of the models; and to demonstrate the practicality of the  developed models by applying our models to various practical cases.<br/><br/>The world is dynamic at many scales in space and time, and the space-time interaction is prevalent in almost every field in the  environmental, informational, and geophysical sciences. For example, uncertainty of environment or global change mostly results from variability of geophysical locations at different times (season, month, day, etc.). A human being's health may be affected by where s/he lives over periods of time. Phenomena in  homeland security also  evolve over space and time. This project has the potential applications in diverse fields, such as applications in atmospheric science, environmental science, geophysical science, agriculture, biology, health and medicine, hydrology, and others. Its success  would  impact not only the realm of statistical modeling, but also help advance research in the fields of the applications.<br/><br/><br/>"
"0604776","Bayesian models and Monte Carlo strategies in identifying protein or DNA sequence motifs","DMS","STATISTICS","07/01/2006","04/07/2006","Jun Xie","IN","Purdue University","Standard Grant","Gabor Szekely","12/31/2009","$160,246.00","Daisuke Kihara","junxie@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","0000, OTHR","$0.00","This project pursues new probability models and Monte Carlo strategies to detect functionally relevant sequence motifs within protein or DNA sequence information. Protein motifs are defined as local segments (20-50 amino acids) that are critical for protein structures and functions. DNA motifs are referred to as specific sequence elements (6-20 nucleotides) in the genome that bind to transcription factors. Despite many advances, current computational tools for aligning sequence motifs are often lack of convergence and produce too many false positives. The investigator proposes: 1) To develop Bayesian models for protein sequence motifs that combine distributions of amino acids with distributions of sequence-derived secondary and tertiary characteristics. 2) To develop a parallel tempering procedure that runs multiple Markov chains in parallel to improve the convergence of motif alignment algorithm. 3) To develop new probability models and statistical methods that describe modules of transcription factor binding sites (TFBSs) and combine genomic sequence information with gene expression information.<br/><br/>The rapid progress of the human genome project and development of biotechnologies have created a rich source of sequence data. Making sense of these data, however, is an ongoing challenge and in part depends on efficient computational and statistical approaches. This proposal focuses on computational detection of functionally relevant sequence motifs. Knowledge of protein motifs gives indication of protein functions, and knowledge of DNA motifs provides information for controlling gene expression. The proposed statistical and computational approaches will help to interpret and model the biochemical processes that regulate gene activity and the responses of cells to diverse stimuli. The eventual applications could lead to better understanding of diseases and drug developments.<br/><br/>"
"0611059","Travel Support: Brazilian Probability School and IMS Meeting, 2006","DMS","PROBABILITY, STATISTICS","03/01/2006","02/27/2006","Gregory Lawler","NY","Cornell University","Standard Grant","Dean Evasius","02/28/2007","$15,000.00","","lawler@math.uchicago.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1263, 1269","0000, 7556, OTHR","$0.00","This award provides funds to support US participants at the joint Brazilian School of Probability and summer meeting of the Institute of Mathematical Statistics in Rio de Janeiro, July 2006. This tenth meeting of the Brazilian School of probability features two mini-courses conducted by renowned experts in actively developing areas of probability, and a set of invited lectures emphasizing recent results that strives to present frontline research to experts, young researchers, and advanced graduate students. The joint summer meeting of the Institute of Mathematical Statistics features a symposium on probability and stochastic processes and collection of special sessions in statistics. The mix of the two programs promotes interaction between international groups in the probability and statistics communities."
"0719508","Bayesian Analysis and Prediction of Gaussian Random Fields","DMS","STATISTICS","09/15/2006","03/27/2007","Victor DeOliveira","TX","University of Texas at San Antonio","Continuing Grant","Gabor Szekely","05/31/2010","$73,434.00","","victor.deoliveira@utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","MPS","1269","0000, 9150, OTHR","$0.00","This project develops methodology for objective Bayesian analysis <br/>of spatial data, both geostatistical and lattice data, that arise in <br/>many of the social and earth sciences, such as economy, epidemiology, <br/>geography, geology and hydrology, as well as computationally efficient <br/>algorithms to perform Bayesian analysis and prediction based on <br/>moderate to large spatial datasets. <br/>On the methodological side, the investigator derives new automatic <br/>prior distributions for the parameters of different kinds of Gaussian <br/>random fields, specified either by their covariance matrices or by <br/>their precision matrices. <br/>The research explores the main statistical properties of Bayesian <br/>inferences based on these automatic priors, such as conditions for <br/>posterior propriety, frequentist properties of parameter and<br/>predictive inferences, and existence of predictive summaries.<br/>A point of particular interest is the study of the pros and cons of <br/>the dependence of these automatic priors on the sampling design.<br/>On the computational side, the investigator derives methods to <br/>approximate these automatic priors distributions, since evaluation <br/>of these priors is in most cases computationally expensive, and <br/>develops new computationally efficient algorithms for Bayesian <br/>inference and prediction of spatial data that would make feasible <br/>Bayesian analysis based on moderate to large spatial datasets.<br/>The methodology proposed in this project serves as an initial step <br/>toward the development of objective Bayesian analysis for spatial <br/>hierarchical models used to describe non-Gaussian data, since most <br/>of these models use Gaussian random fields as building blocks.<br/><br/><br/>The statistical methodology developed during this project has practical<br/>impacts in many social and earth sciences, such as economy, epidemiology, <br/>geography, geology and hydrology, where the collection and analysis of <br/>spatial data have become common tasks.<br/>A paradigm of statistics called the Bayesian approach possesses several <br/>conceptual and methodological advantages when compared to traditional <br/>approaches for the analysis of spatial data, but technical and <br/>computational difficulties that arise during implementation have <br/>hindered its more widespread use among practitioners.<br/>This is particularly so for the analysis of some types large spatial <br/>datasets where current implementations of the Bayesian approach <br/>are too cumbersome or unfeasible to be carried out.<br/>The statistical methodology developed in this project would contribute <br/>to overcome some of these technical and computational hurdles, and<br/>consequently to bridge the gap between methodology and practice for <br/>Bayesian analysis of spatial data.<br/>Graduate students would be engaged in the project, contributing to <br/>their statistical training as well as the enhancement of the Statistics <br/>program at the University of Arkansas.<br/>"
"0604666","Joint modeling and analysis of longitudinal markers and recurrent events","DMS","STATISTICS","07/01/2006","04/12/2006","Elizabeth Slate","SC","Medical University of South Carolina","Standard Grant","Gabor Szekely","06/30/2010","$200,000.00","","eslate@fsu.edu","171 ASHLEY AVE","CHARLESTON","SC","294258908","8437923838","MPS","1269","0000, 9150, OTHR","$0.00","This project addresses modeling and inference for joint longitudinal and recurrent event outcomes. Although there are a variety of models for analysis of joint longitudinal and (single event) survival outcomes, the recurrent event setting raises unique features that require nontrivial generalization. Among these features is the interplay that may occur between the longitudinal and recurrent event processes due to interventions at each event occurrence. The investigator develops and implements a flexible class of models and associated inferential procedures for this situation, building upon a latent class approach. The properties of these methods are characterized relative to alternative modeling approaches, such as generalizations of selection and shared random effects models often used in the joint longitudinal-survival context. Application to real data demonstrates practicality of the methods and that new insight can be obtained.<br/><br/>This project addresses the situation where a series of event times is observed for each subject under study, together with a series of readings on a quantity (marker) that provides information about the risk for subsequent events. Examples in the biomedical sciences include recurrent cancer and an associated biological marker that conveys information about the risk for subsequent cancers, and recurrent cardiovascular disease events and markers such as stress, cholesterol or blood pressure levels. In engineering and reliability, the event times may be machine breakdowns, with the marker being part fatigue. A context relevant to national defense is the occurrence of terrorist acts with chatter an associated marker. This project develops modeling and prediction methods for this context that allow for the effects, on both the marker and event processes, of interventions following event occurrences. In the example contexts given, such interventions could be medical treatments, machine repairs, and changes in defense postures, respectively. The methods combine information from both the marker and observed event occurrence times to strengthen predictions about each. The new methods are applied to real data to demonstrate their practicality and that new insight can be obtained. The project also contributes to research infrastructure by training graduate students.<br/>"
"0604318","Saddlepoint and Bootstrap Methods in Stochastic Systems and Related Fields","DMS","STATISTICS","07/01/2006","06/11/2007","Ronald Butler","CO","Colorado State University","Continuing Grant","Grace Yang","10/31/2007","$95,753.00","","rbutler@smu.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1269","0000, OTHR","$0.00","Abstract <br/><br/>    The author proposes to develop a complete framework for implementing nonparametric statistical inference in stochastic systems. These stochastic systems are semi-Markov processes and include most of the commonly used stochastic models in reliability, multi-state survival analysis, epidemic modeling, and communication and manufacturing systems. Three tools are required to complete the framework: cofactor rules for transforms, saddlepoint approximations to invert the transforms, and the bootstrap to provide statistical inference in conjunction with the two previous tools. With any one of the three tools missing, statistical inference is no longer generally possible. Such a complete theory was the goal of the cybernetics movement during the late 40s to mid 70s which devoted a great deal of effort into developing a Laplace transform approach to such modelling. Ultimately this approach failed due to the difficulty of inverting the <br/>transforms involved, a task very successfully performed by using saddlepoint approximations. It also <br/>lacked a statistical theory of inference, a need that is filled admirably by the bootstrap. The work of this <br/>proposal takes a step towards achieving the ultimate aims of the cybernetics movement: to facilitate probability computations and nonparametric (bootstrap) inference for stochastic systems that cannot be easily achieved by other means. Bootstrap simulation for inference without saddlepoint assistance is beyond <br/>computational feasibility for systems of even modest size and complexity.<br/> <br/>This project proposes to develop a complete framework for implementing nonparametric statistical inference<br/>in complex stochastic systems. These stochastic systems include most of the commonly used stochastic models<br/>used in reliability, multi-state survival analysis, epidemic modelling, and communication and manufacturing<br/>systems. The proposal also addresses significant questions in other disciplines where answers are lacking due to certain computational difficulties. In population genetics, solutions are provided for statistical inference problems dealing with natural selection, mutation and genetic drift; in ocean and electrical engineering accurate approximations are given for distributions of wave crest heights in models used for sea surfaces and in signal processing; in biological models for the transmission of pain through the nervous system, methods are given to allow inferences about the underlying mechanisms that drive the fluctuating polarities of these ion channel models with the ultimate aim of helping to reveal the mechanisms that control pain sensation."
"0605169","Random Matrices in Multivariate Statistics: Theoretical Developments and Applications","DMS","STATISTICS","07/01/2006","06/30/2006","Noureddine El Karoui","CA","University of California-Berkeley","Standard Grant","Gabor Szekely","06/30/2010","$240,000.00","","nkaroui@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","This research program is currently focused on the development of data analysis methods for the new paradigm of high-dimensional problems. The associated theoretical problems are concerned with eigenvalues of large dimensional random matrices. More precisely, three related directions seem of particular interest: <br/>1) further our understanding of the spectral properties of the relevant random matrices; 2) make practical use of the results obtained, combined with some more classical results from random matrix theory; 3) find and contribute to area of applications where this framework is relevant. More specifically, it is now very often that statisticians are faced with ``n times p"" data matrices X, for which p, is of the same order of magnitude as n, and p and n are both large. The sample covariance matrix computed from this data is of great importance to a number of applications, as it underlies widely used methods like principal components analysis. However, the theoretical results which underly the method fail to apply in the ""large n, large p"" setting just described. Hence, a thorough study of sample covariance matrices in this setting is needed. Eigenvalues of such large dimensional matrices are of particular interest. The largest and smallest eigenvalues of these matrices are, from the point of view of applications, particularly interesting. The aim of the study is to obtain central limit type theorems for these extreme eigenvalues and use them in Statistics for, for instance, hypothesis testing, having a notion of power, etc... A more applied part of this work concerns efficiently using results from random matrix theory - new and old - to better estimate the eigenvalues of the population covariance with the ultimate aim of better estimating the whole covariance matrix when p and n are both large.<br/><br/>Technological progress allows us to store and use massive amounts of data about many aspects of our daily lives. An interesting problem is to use this data to understand how certain traits depend on each other. In the stock market, we might be interested in how the behavior of one stock affects the behavior of another stock;<br/>understanding all these interrelationships leads to having a measure of the risk taken by investing in portfolios that use the corresponding stocks. Statisticians have a number of tools to deal with all these interrelationships. We can discover ways to look at the data so that, even if all interrelationships are small or weak, so each trait ""should"" not help us learn too much about any other trait, we might find combinations of the traits that carry enormous amounts of information. We also know what are typical values for these combinations, so we might be able to detect unusual things in the data by looking at it the right way.  Those statistical techniques have very wide applications in various fields of science, ranging from climatology to genetics, image recognition etc... Thousands of research papers are published each year that use these techniques. However, the theory that underlies these statistical techniques was created in an era where massive datasets just did not exist, as they were not storable. This research project is focusing on theories and their applications that are better suited to handle our current massive datasets. The applications should allow us to see structure where the classical tools fail to see any and tell us when there is no structure when the classical tools tell us there is. We also have increasing evidence that our standard tools give us often very inaccurate results about our standard measures of risk or amount of information carried in combination of traits. It seems that risks might be underestimated and amount of information might be overestimated. Part of this research program will be dedicated to measuring how inaccurate the classical results are for large datasets and how can a more relevant theory be used for correcting these inaccuracies.<br/>"
"0606609","SRCOS - Summer Research Conference 2006","DMS","STATISTICS","06/01/2006","03/16/2006","Jack Tubbs","TX","Baylor University","Standard Grant","grace yang","05/31/2007","$5,000.00","","jack_tubbs@baylor.edu","One Bear Place #97360","Waco","TX","767987360","2547103817","MPS","1269","0000, 7556, OTHR","$0.00","    <br/><br/>Summer Research Conference in Statistics, 2006<br/><br/>The Southern Regional Council on Statistics (SRCOS) together with the American Statistical Association is organizing a Summer Research Conference (SRC) in Statistics be held June 4-7, 2006, in Kerrville, Texas. The conference objective is to bring together statistics researchers of all academic levels in a relaxed and stimulating atmosphere.  The program chairs for the 2006 Summer Research Conference (SRC) on Statistics are Dr. Jack Tubbs, Department of Statistical Science, Baylor University, and Dr. Jiayang Sun, Department of Statistics, Case Western Reserve University. The session topics include; Bayesian Methods in Pharmaceuticals, Adaptive Clinical Designs, Multiple Tests of Hypotheses, Bioinformatics, Data Confidentiality and Security, and Text and Data Mining. The principal Senior speakers include; Stacy Lindborg, Eli Lilly, Andre Rogatko, Emory University, Don Berry, M D Anderson, Jeff Hart, Texas A&M, Rudy Guerra, Rice University,  Simon Sheather, Texas A&M, Alan Karr, NISS/SAMSI, and Ed Wegman, George Mason University.  A poster session is open for graduate students and other attendees to present their research. The sessions of the conference are not concurrent. <br/>The research topic areas are broad, with applications of the methodologies in a spectrum of fields (such as medicine, homeland security, business, etc.) The individual 90-minutes sessions, featuring a senior speaker and two junior speakers, are structured to provide in-depth coverage of the topic, from an overview by the senior speaker to specific, current research problems of the junior researchers. Advanced graduate students, postdoctoral fellows, and junior researchers are encouraged to attend and present in the judged poster session. Support to offset travel expenses for students and junior researchers by NSF is gratefully acknowledged. Detailed information about the conference can be found at http://www.baylor.edu/statistics/srcos/<br/>"
"0604639","Advanced Bayesian Approaches for Heterogeneous Temporal Genomic MetaData","DMS","STATISTICS","06/01/2006","04/03/2006","Yulan Liang","NY","SUNY at Buffalo","Standard Grant","Gabor Szekely","02/28/2009","$144,999.00","","ylian001@umaryland.edu","520 Lee Entrance","Buffalo","NY","142282567","7166452634","MPS","1269","0000, OTHR","$0.00","<br/>Recent genomic and proteomic data sets are so disparate and complex that not too many studies have provided robust and sophisticated modeling for the latent information.  The principal research of the underlying project strives to make efficient and maximal use of the generated information from heterogeneous temporal genomic sources where the special features are present. The investigator aims to develop, implement and validate the innovative advanced Bayesian modeling techniques, such as Bayesian state space models for studying the dynamics of heterogeneous temporal genomic metadata for (i) inferring and predicting the genomic profiles associated with diseases and treatments; (ii) estimating the important hidden biological parameters; (iii) constructing gene-time-gene and protein-protein interaction networks and pathways for hybrid biological systems. This should be sufficient to explain causal and probable relations about the interactions of genes-treatments-diseases and gene-environment. The investigator evaluates the efficacy and sensitivity of the proposed models through detailed study of specific diseases, such as time course of lymphocyte gene expression data from interferon-beta-1a treated multiple sclerosis patients and multiple tissues polygenic data such as kidney and liver data of animals sacrificed at 17 time points following administration of a bolus dose of MPL. Graphic display of the results from each model are provided to explore the dynamics of the modeling processes, which marks an important intermediate goal that allows visual examination of the degrees of heterogeneity between models. Through the research and educational activities the heterogeneous raw temporal genomic data are converted into scientific knowledge that advances our understanding of today's common complex diseases, biological processes and potentially identify new modalities of treatment.<br/><br/>This project describes a novel area for the field of statistical genomics and bioinformatics, which is driven by the over-availability of a variety of heterogeneous temporal genomic data and methodologies. Through the research and teaching activities, systematic understanding and overall knowledge is generated for efficient data exploration methodology, primarily contributing to the areas of statistics and computer science. If fully successful, its contribution to the fields of biology, pharmaceutical sciences, and medical sciences could be invaluable, potentially speeding up research, diagnosis, drug development, and medical decision making ultimately improving human and other life. Biomedical/genomic applications of the developed methodologies would support biologists and medical researchers to better understand the underlying causes of diseases, the risks and offer a more powerful diagnostic tool and predictive treatment and provide customized solutions to genomic data analysis. Both the theoretical and the practical foundations of the activity will make an impact on higher education, especially in training the current and next generation of statisticians and computational scientists to tackle challenges involved in the human genome research. The techniques developed in this project are be used to augment and develop related courses and made available on the internet for outreach at large."
"0605236","Construction and Analysis of Methods for Making Appropriate Use of Low Dimensional Structure in Data and Models When Apparent Dimension is Very High","DMS","STATISTICS","07/15/2006","07/18/2008","Peter Bickel","CA","University of California-Berkeley","Continuing grant","Gabor Szekely","06/30/2010","$450,000.00","","bickel@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","The investigator develops and analyzes asymptotically methods for inference and prediction for high dimensional data. His research with collaborators includes estimation of intrinsic dimension, estimation of high dimensional covariance matrices and prediction using  low dimensional approximate representations of  the data and/ or  approximate low dimensional models.<br/><br/>The goal of the investigators research is to improve prediction and inference in situations ranging from numerical weather prediction to  computational biology  when high dimensional data is the basis   of action and understanding. In fact, such data nowadays arise everywhere. The underlying principle the investigator and collaborators are following is that the data  is essentially  low dimensional when properly represented and/or modeled  and that appropriate representations can be found in computationally feasible ways. <br/>"
"0604896","A Synthesis of Objective Bayes Factors for Model Selection and Hypothesis Testing","DMS","STATISTICS, EPSCoR Co-Funding","08/01/2006","07/07/2006","Luis Pericchi","PR","University of Puerto Rico-Rio Piedras","Standard Grant","Gabor Szekely","07/31/2008","$71,549.00","","luarpr@gmail.com","18 Ave. Universidad, Ste.1801","San Juan","PR","009252512","7877634949","MPS","1269, 9150","0000, 9150, OTHR","$0.00","Objective Bayesian testing and model selection has become an important research subject recently, due to the widely accepted Bayesian principle and lack of practical tools for testing and model selection under the Bayesian framework. In this project the investigator develops 'synthesis Bayes Factor', taking into account the main contributions so far, and develops substantial new theory, methodology and computer software to reach the level of everyday practice. Practical and specific guidelines with sound priors are developed for the most common tests, like the Student-t test, F-Test, Goodness of Fit, test for homoscesdasticity or test for correlation among variables. There is no easy solution: To comply with desiderata of Objectivity and Large and Small sample consistency, the Bayes Factors can not be based on Conjugate Priors, like g-priors. The main tool to develop these tests is the Theory of Intrinsic Priors, which digs out the implicit priors associated with several objective Bayesian methods, like Intrinsic Bayes Factors, Fractional Bayes Factors, EP-Priors, Bayes Factors based on Tests Statistics, etc. The guiding principle is to accept an Objective Bayesian Test, only when the associated prior is sensible and the procedure is consistent and objective. In this project, Objective Bayesian Student-t tests, F-Tests, Homoscesdasticity tests, Goodness of Fit test and a small sample Bayes Factor approximation, an improved BIC, are developed and justified. <br/><br/>The University of Puerto Rico (UPR) is one of the major educational institutions in the USA, that is comprised of underrepresented groups. Thus this project will naturally integrates diversity into the NSF programs. The Rio Piedras (RP) Campus in San Juan is the largest and oldest campus of the UPR University System. The Department of Mathematics is affiliated to the College of Natural Sciences, along with the Departments of Biology, Chemistry, Computer Science, Physics, Environmental Studies and the Institute of Tropical Ecosystems Studies. It is notorious that Statistics needs reinforcement in the College, both in terms of courses given and research output. Bayesian Statistics is undergoing an important development worldwide. It is seen with interest by several researchers in the UPR, particularly in Environmental Sciences, Biology, the Campus of Medical Sciences and the Engineering School at the Mayaguez Campus. This project serves as a focal point to the development of Bayesian Statistics in the University, and this development will influence the university as well as some Federal Agencies as USGS. This project also strengthens the PhD program in Mathematics and the PhD program in Computer Science, and will serve as support of one Ph.D student whose thesis would be directly developed under this grant. <br/> <br/>"
"0604954","Theory And Methodology For Sparse Inference","DMS","STATISTICS","09/01/2006","04/10/2006","T. Tony Cai","PA","University of Pennsylvania","Standard Grant","Gabor Szekely","08/31/2009","$353,380.00","Mark Low","tcai@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, OTHR","$0.00","<br/> <br/>ABSTRACT: <br/><br/>The analysis of massive data sets now commonly arising in scientific investigations poses many statistical challenges not present in smaller scale studies. Many of these data sets exhibit sparsity where most of the data corresponds to noise and only a small fraction is of interest. In such situations mixture models can provide an effective and  convenient framework for a wide variety of problems. The investigators propose to develop a comprehensive methodology  for mixture models in sparse settings. There are four main goals to be pursued in  moderately sparse and  super sparse environments. The first is to make precise how well sparsity can be estimated as well as to develop a general methodology for estimating sparsity. A second goal is to develop a data dependent thresholding rule which with high probability yields a collection of cases almost all of which correspond to signal. The investigators also plan to develop a theory which makes precise the possible tradeoffs between discovering signal and including noise. A third goal is to develop a theory of optimal detection for sparse mixture models. A final goal is to provide a theoretical basis for connecting mixture models with sequence models another useful framework for analyzing sparse data.<br/><br/>The proposed research on sparse inference will provide technical tools as well as methodology, to researchers in other scientific fields who collect and analyze large data sets with sparse signals. These fields include astronomy, bioinformatics, biostatistics, and genetics. The  procedures and algorithms will be implemented in Splus or Matlab and made available on the Internet along with the associated research reports so as to facilitate comparisons with other approaches.<br/> <br/> <br/>"
"0605102","High Dimensional Bayesian Model Discovery, Inference and Prediction","DMS","STATISTICS","07/01/2006","04/28/2006","Edward George","PA","University of Pennsylvania","Standard Grant","Gabor Szekely","06/30/2009","$99,634.00","","edgeorge@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, OTHR","$0.00","This research is concerned generally with new methods for regression analysis in high dimensions, where the number of parameters is very large compared to the number of observations.  The first part of this research focuses on the development and application of methods that identify and draw inference about low dimensional structure in this setting.   For this purpose, the investigator further develops a new nonparametric effects model called BART (Bayesian Additive Tree Models) that uses a dimensionally adaptive dynamic random basis of trees.  In particular, BART opens up a variety of new ways to perform variable selection followed by model selection.  As opposed to traditional variable selection methods that rely on a preselected parametric model class, these new methods will select variables before the selection of a model, thus eliminating a major limitation of previous methods.   The second part of this research focuses on the development of new Bayesian procedures for predictive density estimation in multiple regression.  The minimaxity of such density estimates under Kullback-Leibler loss are established for various classes of scaled superharmonic priors.  Based on these estimates, the investigator further develops minimax multiple shrinkage predictive density estimates that exploit variable selection uncertainty to achieve risk reduction.  <br/><br/>The discovery of structure in complex settings and the prediction of future uncertain events are fundamental statistical challenges in most areas of academic research including business, the hard and social sciences, medicine, humanities, computer science and engineering.  By modeling and predicting economic conditions, the economist can better direct the economy; by modeling and predicting climate changes, the scientist can better manage the environment; by modeling and predicting health care needs, the policy analyst can better allocate resources to meet demand.  This research develops brand new procedures that establishes high dimensional model discovery and predictive estimation as highly visible and valuable areas of theoretical and methodological research, one that will attract the brightest students and the wisest seasoned scholars.  This will entail broad dissemination of the work in pinnacle journals, through public lectures around the world, and through new collaborations with colleagues and graduate students. To further facilitate the use of these new methods, the investigator will continue to create and make publicly available open-source software implementations along with full documentation and examples.   <br/><br/> <br/>"
"0608306","Algorithms for Applied Multivariate Statistical Analysis","DMS","APPLIED MATHEMATICS, STATISTICS","07/01/2006","08/10/2009","Alan Edelman","MA","Massachusetts Institute of Technology","Standard Grant","Henry A. Warchall","09/30/2009","$137,792.00","Plamen Koev","EDELMAN@MATH.MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1266, 1269","0000, OTHR","$0.00","This project concentrates on the development, analysis, and implementation of algorithms for computing the distributions of the eigenvalues (and functions thereof) of the classical random matrix ensembles -- Wishart, Jacobi, and Laguerre.  The focus is on achieving high practical efficiency by exploiting the combinatorial and algebraic properties of multivariate orthogonal polynomials as well as utilizing structured matrix computations and dynamic programming techniques.<br/><br/><br><br><br/><br/>This research will provide new efficient tools for using multivariate statistical techniques in practice. Such techniques are critical in many areas and applications, including bioinformatics, genomics (population classification), wireless communications (network capacity optimization), and military applications (automatic target classification).<br/>"
"0604558","Nonparametric Curve Estimation: Theory and Practice","DMS","STATISTICS","06/01/2006","05/01/2006","Sam Efromovich","NM","University of New Mexico","Continuing grant","grace yang","10/31/2006","$53,316.00","","efrom@utdallas.edu","1700 Lomas Blvd. NE, Suite 2200","Albuquerque","NM","871310001","5052774186","MPS","1269","0000, 9150, OTHR","$0.00","<br/>ABSTRACT<br/><br/>   The proposal focuses on developing statistical theory, methodology and methods of data-driven nonparametric curve estimation motivated by and tested on biological, psychological, medical, and <br/>environmental applications, with the main aim to bridge the asymptotic theory and applications. The main intellectual objectives are: (i) For models with indirect observations (like censored and biased regression, hidden components, modeling of time series in the presence of nonparametric trend and volatility) develop the theory of sharp minimax estimation as well as of mimicking of oracles that know direct observations and/or nuisance functions unavailable to the statistician; (ii) For controlled experiments, develop the theory, methodology and methods of sequential sampling and estimation for the case where optimal (under more common criteria) solution depends on estimated and/or nuisance functions; (iii) Develop the theory and methods of a local aggregation of wavelet estimates. Practical problems include the study of municipal wastewater treatment plants, modeling of levels of contaminants and residual disinfectants in Albuquerque water basin, analysis of drinking patterns and behavior change initiation during pregnancy, study of temporal and spatial structures of plants in Sevilleta National Wildlife Refuge, learning machines for recovery magnetic resonance images and the analysis of satellite data.<br/><br/>   The primary focus of this research is to develop, in collaboration with Sandia and Los Alamos National Laboratories as well as with the UNM Medical and Engineering Schools, algorithms and software for statistical learning and adaptive estimation motivated by and tested on the following environmental, medical and biological applications: Statistical modeling of temporal and spatial structures of plants in Sevilleta National Wildlife Refuge which will allow to study global weather changing; The study of municipal wastewater treatment plants; Modeling and analysis of change points in levels of contaminants and residual disinfectants in Albuquerque water basin with applications to a homeland security drinking water monitoring; Learning machines for recovery magnetic resonance images and the analysis of environmental satellite data including temperature and humidity; Analysis of drinking patterns and behavior change initiation during pregnancy which reduces likelihood of relapses. The broader impact of the research is defined by well--understood applications that will benefit the society and help students and a broader audience to understand the importance of mathematical sciences. The impact will be based on the following<br/>proposed activities: (i) Graduate students will participate in the research; (ii) To promote learning, scientific seminars will be held for undergraduate and graduate students, and talks by the proposer and the students will be presented during the UNM mathematical awareness weeks for high-school students. (iii) To broaden participation of under-represented groups, regular presentations will be held at outreach seminars conducted by the UNM Gallup and Valencia campuses. (iv) The developed software will be freely available. Medical, environmental and biological findings, benefiting the society,  will be published in not-technical journals.<br/>"
"0605052","Computational Issues in Model Elaboration, Diagnostics and Estimation","DMS","STATISTICS","09/01/2006","02/23/2006","Mario Peruggia","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","Gabor J. Szekely","08/31/2010","$150,000.00","","peruggia@stat.ohio-state.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","The degree of complexity and sophistication in modern statistical models has various consequences.  This project addresses two important, related aspects.  First, how can one assess if specific features of a model provide an accurate representation of the phenomenon under study?  Further, if some features are identified as problematic, in what direction should they be modified to improve the fit of the model?  Second, as models become more complicated, fitting the models also becomes harder.  It would be a mistake to think that raw computing power is all that is needed to keep pace with the increasing complexity.  Novel algorithmic approaches are needed to tackle the challenges posed by modern modeling practices.  Old techniques used on more powerful computers might only run longer, without producing the desired output.  This project establishes some important and novel connections between the theoretical properties of Bayesian hierarchical models and some other areas of statistics, such as time series analysis and cluster analysis, leading to advances in the area of model diagnostics and elaboration and to the development of an effective class of algorithms for fitting Bayesian mixture models.<br/><br/>Recent advances in computational resources have afforded modelers unprecedented opportunities to describe real life and natural phenomena in very realistic terms.  As reality is inherently complex, realistic models tend to be highly sophisticated.  The need for accurate and reliable modeling cannot be overemphasized.  Public<br/>policy decisions are routinely made on the basis of probabilistic models that forecast economic and social indicators, predict environmental factors, assess the potential for disease outbreak, etc. Small deficiencies in the models and little estimation inaccuracies can have consequences that might impact on the welfare of large<br/>numbers of people.  The proposed research will develop translational methodology that cuts across disciplines and can be used to improve modeling and forecasting in a variety of settings.  Suggested areas of application include those with which the PI is most familiar because of his ongoing collaborations (quantitative psychology, marketing, and patient oriented medical investigation) but there is clear potential for the research to have an impact on other areas as well.  For example, finite mixture distributions models, one of the specific research themes, are used in applied settings as diverse as human genetics and the monitoring of worldwide nuclear testing to tell explosions apart from earthquakes.  The project also has a clear educational focus, both in terms of training of the graduate students who will assist the PI in the research activities and in terms of alerting the broader research community to the need for sound and modern modeling strategies.<br/>"
"0603873","Semiparametric Statistical Inferences for ROC Curves and Surfaces under Density Ratio Models","DMS","STATISTICS","07/01/2006","02/16/2006","Biao Zhang","OH","University of Toledo","Standard Grant","Gabor J. Szekely","06/30/2009","$73,241.00","","biao.zhang@utoledo.edu","2801 W Bancroft St., MS 218","TOLEDO","OH","436063390","4195302844","MPS","1269","0000, OTHR","$0.00","Receiver operating characteristic (ROC) curves are commonly used to measure the accuracy of diagnostic tests in discriminating disease and nondiasease. The investigator studies four important statistical applications of the density ratio model in semiparametric ROC curve and surface analyses. Analogous to the nonparametric kernel-based ROC curve analysis, the investigator studies the semiparametric kernel estimators of the ROC curve and its area on the basis of the maximum semiparametric likelihood estimators of the underlying distribution functions under a two-sample density ratio model. Furthermore, the investigator proposes three approaches for comparing the accuracy of two diagnostic tests with paired or unpaired data. Moreover, the investigator studies the maximum semiparametric likelihood estimator of the best combination of two or more diagnostic tests by directly modeling the likelihood ratio function under a density ratio model. In addition, as a generalization of semiparametric ROC curve analysis to semiparametric ROC surface analysis, the investigator studies maximum semiparametric likelihood estimation of the ROC surface and its volume in the context of multiple-class diagnostic problems by extending the two-sample density ratio model to a multiple-sample density ratio model. As an alternative to the Cox proportional hazards model and the Lehmann alternative model, the natural connection between the semiparametric density ratio model and the logistic regression model has enhanced its recent popularity. It is anticipated that statistical inferences based on the density ratio model would be more robust than a fully  parametric approach and would be more efficient than a fully nonparametric approach.<br/><br/>An important role of research in diagnostic medicine is to estimate and compare the accuracies of diagnostic tests, enabling one to determine if a new diagnostic test is as good as the standard reference test or if an inexpensive test has an acceptable inferiority in sensitivity or specificity. In clinical practice, several medical diagnostic tests are often available, yet they may not be perfect in the sense that no single test is sufficiently sensitive and specific on its own for the purpose of population disease screening. One approach to improving the performance of screening is to combine multiple diagnostic tests so as to obtain an optimal composite diagnostic test with higher sensitivity that detects presence of the disease more accurately. The proposed activity has important applications in the evaluation of medical diagnostic tests, all of which are beneficial to practitioners in biological and medical communities. In particular, the proposed activity provides a more robust and efficient statistical methodology for assessing the accuracy of diagnostic tests used in the practice of medicine, thereby enhancing the statistical evaluation of medical diagnostic tests for classification and prediction. Thus, the proposed activity would be widely applicable in the field of diagnostic medicine and other related interdisciplinary problems. In addition, the proposed research activity has greater educational impacts on statistics teaching and learning, in that much of the proposed material can be utilized in classroom teaching and incorporated into textbooks on semiparametric models for master and doctoral students in statistics and biostatistics.<br/>"
"0604698","Some Problems in Nonparametric Statistics","DMS","STATISTICS","07/01/2006","04/07/2006","Peter Hall","CA","University of California-Davis","Standard Grant","Gabor J. Szekely","06/30/2010","$345,000.00","","pghall@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","0000, OTHR","$0.00","ABSTRACT<br/><br/>Motivated by today's extraordinary opportunities for computer-intensive statistical analysis, and by the myriad new problems that new data types pose, new statistical techniques are being introduced at an extraordinary pace.  Many of them lead directly to novel, nonparametric approaches to data analysis, where  <br/>the conception and appreciation, and even the applications, often demand sophisticated and complex theoretical methods.  The new, nonparametric techniques involve minimal structural assumptions and use the data in highly adaptive ways.<br/><br/>In this context, the research supported by this grant will address six projects, motivated by potential applications to astronomy, economics, engineering, defence, health science and image analysis.  This diversity illustrates the substantial importance and potential, and strategic significance, of nonparametric statistics.  The research program will draw together new theoretical methods, a wide range of ideas for developing solutions to diverse practical problems, and the intellectual perception and insight that these attributes will bring.<br/>"
"0604176","Inference Problems in Extreme Value Statistics","DMS","STATISTICS, COLLABORATIVE RESEARCH","07/15/2006","05/01/2008","Yongcheng Qi","MN","University of Minnesota Duluth","Continuing grant","Gabor J. Szekely","06/30/2010","$159,542.00","","yqi@d.umn.edu","1049 University Drive","Duluth","MN","558123011","2187267582","MPS","1269, 7298","0000, OTHR, 9251","$0.00","The study of extreme-value theory has been paid much attention in recent years. Estimating probabilities of rare events is one of the primary interests. This has motivated many researchers to develop new methodologies in extreme-value statistics. One of the difficulties in applying the extreme-value theory is that the sample fraction has to be carefully chosen such that the estimation has a convergence rate as <br/>fast as possible while its bias is negligible. This proposal consists of five topics, as follows. First, the investigator proposes a data tilting method to construct confidence intervals for extreme tail probabilities when the underlying distribution belongs to the domain of attraction for one of the extreme-value distributions. The proposed method is expected to generate more accurate confidence intervals in terms of coverage probabilities and to be more robust against the choice of the sample fraction. Second, the investigator develops new methods for estimation of dependence structures in bivariate extremes. Estimation of the dependence structures, such as the spectral measure, in bivariate extreme-value statistics is an important issue. The spectral measure, together with the two marginal limits, determines the limiting distribution of the bivariate extremes. Third, the investigator proposes smooth estimators for the first partial derivatives of the dependence function in order to construct confidence intervals for the dependence function based on the normal approximation. Fourth, the investigator studies how to construct confidence bands for the spectral measure and tail dependence functions in bivariate extreme-value statistics. Special bootstrap techniques are applied to solve the problems. This allows one to obtain asymptotically correct confidence bands without estimating the derivatives of the dependence function globally. Fifth, the investigator proposes new estimators for the Pickands dependence function of high dimensional extreme-value distributions with unknown marginal distributions.<br/><br/>Extreme-value statistics have found applications in many fields such as meteorology, hydrology, climatology, environmental sciences, telecommunications, insurance, and finance. The investigator develops more accurate and effective methodologies for risk analysis in both univariate and multivariate extreme-value statistics. Progress of the projects in this proposal enhances the collaboration between the investigator and researchers in these fields. The proposed activities also involve teaching graduate students to use extreme-value statistics in their future research. The new methods developed in this proposal are expected to have broader applications as well. For example, they can be used by actuaries to calculate and insure against the probability of rare but financially devastating events, or be employed by statisticians to calculate the required height of sea walls to prevent flooding. They can also be used to tell engineers how strong to build bridges or oil rigs and to model excessively high pollution levels."
"0603673","Spatial Point Pattern Analysis Using Composite Likelihood","DMS","STATISTICS","06/01/2006","05/30/2006","Yongtao Guan","FL","University of Miami","Standard Grant","grace yang","07/31/2007","$121,230.00","","yguan@miami.edu","1320 S. Dixie Highway Suite 650","CORAL GABLES","FL","331462926","3052843924","MPS","1269","0000, OTHR","$0.00","The proposed research introduces a new likelihood based method in fitting spatial point process models using the idea of composite likelihood. Composite likelihood (CL) has been successfully applied in numerous settings where a full maximum likelihood is not feasible or is not available. Its use in spatial point process modeling, however, has never been studied. This research intends to show that a CL can be formed for any spatial point process whose second-order intensity function can be explicitly defined. The proposed likelihood is easy to obtain and can be used in many different spatial point pattern analyses. In particular, it can be used to fit both homogeneous and inhomogeneous spatial point process models to data. Furthermore, it can also be used to select the bandwidth used in estimating the pair correlation function, which is an extremely exploratory and model fitting tool in spatial point pattern analysis.<br/><br/>This research is motivated by the red oak borer data that were collected by entomologists at the University of Arkansas. The data consist of mapped locations of attack holes caused by larvae of red oak borers when they eat their way into the trees. The main objective is to understand what affects adult red oak borers to decide where to lay their eggs. The proposed model fitting procedures will be applied to link the locations of attack holes to important tree characteristics such as side and height of the tree. This work will have both important biological and practical significance. In particular, the results will provide biological insight on the breeding habit of the adult borers. This biological insight can in turn be used in practice to guide the development of more effective trapping techniques that can be used to control the population of the red oak borers. Thus the proposed research may play a critical role in the effort to save millions of red oak trees in the US that are being threatened by the outbreaks of red oak borers."

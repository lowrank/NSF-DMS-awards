"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"0706963","Collaborative Research: Nonparametric Methods for Emerging Technologies in Bioinformatics","DMS","STATISTICS","09/01/2007","09/04/2009","Huixia Wang","NC","North Carolina State University","Continuing Grant","Gabor Szekely","08/31/2011","$179,798.00","","judywang@gwu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","0000, 9263, OTHR","$0.00","The proposed research targets two important statistical problems in protein and gene expression microarray experiments: (I) quantification of the protein lysate arrays, an emerging technology for directly measuring protein contents of different lysed tissue samples simultaneously; (II) modeling probe level gene expression<br/>data, in particular, the exon tiling arrays to detect alternative splicing, which is an essential process resulting in much of the human diversity.  The investigators provide a statistical framework that allows for unknown regressor values in a nonparametric regression model, with applications to the quantification of protein lysate array data. The investigators also develop a quantile regression approach for mixed-effect models that are appropriate for detecting treatment and/or interaction effects without parametric distributional assumptions on the model. The investigators propose to make use of information across genes to enhance performance of the inferential methods in small sample problems. The new principles developed in the proposal are statistically interesting beyond their direct applications to gene and protein expression data.<br/><br/>Findings from the Human Genome Project highlight the intricacy of interactions between cell regulation,<br/>proteins and genes. It is generally understood that biological functions and biological activities are controlled by subsets of genes interacting with proteins in a highly controlled manner.  High throughput technologies such as microarrays are valuable for studying a large number of biological components simultaneously.  In particular, the protein lysate and exon tiling arrays have begun to show their important roles in cancer study and other biomedical research.  However, sound conclusions from these technologies depend on appropriate statistical analysis of the proteomic and genomic data. The statistical methods developed in the proposal are timely and important for proper quantification of the protein lysate arrays and for detecting alternative splicing through the exon tiling arrays.  The nonparametric approach proposed  is especially appealing due to its flexibility and adaptivity in modeling probe level gene expression data as well as protein lysate array data. <br/><br/>"
"0707139","Resampling methods for temporal and spatial processes and their higher order accuracy","DMS","STATISTICS","09/01/2007","06/17/2009","Soumendra Lahiri","TX","Texas A&M Research Foundation","Continuing Grant","Gabor Szekely","08/31/2011","$319,294.00","","s.lahiri@wustl.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","The  project focuses on  investigating  higher order asymptotic properties of  common  resampling methods for time-series and spatial data and on development of new  ones. Specifically, this project concentrates on<br/>(i) investigating higher order properties of resampling methods for infinite dimensional parameters of time series data; (ii) developing Edgeworth expansion theory for  statistics under long range dependence; (iii) developing new  resampling methods for spatial  data on a regular grid with an  aim towards achieving higher order accuracy, (iv)  investigating ways to extend resampling methodology to irregularly spaced spatial data<br/>and study their higher order properties.<br/><br/>Data exhibiting temporal and spatial dependence appear in many areas of sciences, such as Astronomy, Atmospheric Sciences, Economics, Geology, Hydrology, Physics, etc. Analyses of such data sets using current statistical methodology face some limitations. This is primarily due to the fact that  the existing statistical  methodology mostly rely on strong  structural  (i.e.,  parametric model) assumptions that are often  inadequate to capture all important  features of the data generating process.  This project seeks to (i) develop new methodology (based on what are known as Resampling Methods) that provide  valid assessment of uncertainty  without strong structural assumptions and (ii) develop theoretical tools to investigate  optimality   properties of statistical methods for  time- and space-dependent data.<br/>"
"0706518","Reduction of Infinite Data Dimension via B Spline Smoothing","DMS","STATISTICS","06/01/2007","03/24/2009","Lijian Yang","MI","Michigan State University","Continuing Grant","Gabor Szekely","05/31/2010","$221,525.00","","yang@stt.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","0000, OTHR","$0.00","This research project develops B spline smoothing methods for: (1) reducing dimension in machine learning and (2) non- and semi parametric GARCH volatility model, with fast computing and explicit formulae. Asymptotically simultaneous confidence band are provided for all nonparametric estimation. The proposal aims to develop the underlying theory as a crucial guide to practical implementation. For dimension reduction in machine learning, the focuses are on the generalized additive model (GAM) and the single index model (SIM), with dimensions tending to infinity. For dimensions from low to moderately high (400-D), spline-backfitted kernel smoothing procedure for additive model and direct spline smoothing procedure for SIM are theoretically reliable, intuitively appealing with extremely fast computing. The current project extends these procedures to GAM and SIM with dimension going to infinity, preserving the theoretical, intuitive and computing benefits. The investigator also studies B spline smoothing algorithms for non- and semi- parametric GARCH model, achieving the same asymptotics as kernel smoothing. As typical applications of GARCH model involve sample sizes from thousands to millions and equally large number of lagged values, B spline smoothing can compute in seconds what kernel smoothing would need days. Thus the proposed methods satisfy both theoreticians and financial analysts.<br/><br/>In the age of information overload, researchers in nearly all areas of biological, medical, physical and social sciences are routinely confronted with large data sets. With tens of thousands of characteristics called variables or features, these large data sets are treasure troughs of valuable scientific information. The methods developed by the investigator are powerful new tools for drawing such useful information out of large data sets. Typical examples of such data include but are not limited to, environmental and global change studies, high frequency financial data, state and federal demographic surveys, federal biometric database, etc. Codes written in free software R are made publicly available for wide dissemination. Practitioners from industry and government can analyze their own large data sets with these user-friendly modules, in real time, with confidence and precision. A distinctive feature of the project is the active integration of cutting-edge research with the education and training of graduate students, especially those from underrepresented groups. This is consistent with the education goal of NSF and fulfills NSF's commitment to the principle of fostering diversity in science.<br/><br/>"
"0742690","Higher order accuracy of bootstrap methods for temporal and spatial processes","DMS","STATISTICS","06/01/2007","12/18/2007","Soumendra Lahiri","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","07/31/2008","$73,835.00","","s.lahiri@wustl.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","The emphasis of the project is on investigating higher order properties of common resampling methods for time-series and spatial data and on development of new inference methods that improve the accuracy and stability of existing methods.  Specifically, this project concentrates on <br/>(i)  developing Edgeworth expansion theory for Studentized statistics under dependence, <br/>(ii)  investigating higher order accuracy of bootstrap approximations for time series data, <br/>(iii)  developing resampling methods with an aim towards achieving higher order accuracy, <br/>(iv)  a nonparametric method for the selection of optimal block length empirically,<br/>(v)  a pooling method for the bootstrap that yields more stable estimators of population parameters, <br/>(vi)  investigating accuracy of bootstrap approximation in spatial prediction problems, and<br/>(vii)  investigating accuracy of bootstrap approximation for spatial data for irregularly spaced data-points.<br/><br/>Data exhibiting temporal and spatial dependence appear in many areas of sciences, such as Astronomy, Atmospheric Sciences, Economics, Geology, Hydrology, Physics, etc.  Analyses of such data sets using current statistical methodology face some limitations.  This is primarily due to the fact that the existing statistical methodology mostly relies on strong structural (i.e., parametric model) assumptions that are often inadequate to capture all important features of the data.  This project seeks to<br/>(i) develop new methodology (based on what are known as Resampling Methods) that provides valid assessment of uncertainty without strong structural assumptions and <br/>(ii) develop theoretical tools to investigate optimality properties of various resampling methods for time- and space-dependent data.  <br/>"
"0707031","Statistical methods for mapping imprinted genes underlying complex traits","DMS","STATISTICS","06/01/2007","05/18/2007","Yuehua Cui","MI","Michigan State University","Standard Grant","Gabor Szekely","05/31/2011","$116,817.00","","cuiy@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","0000, OTHR","$0.00","<br/>In nature, most functional regions of the genome for a chromosome pair express equally. A variation from this equivalence results in genomic imprinting, a phenomenon also called parent-of-origin effect. Using statistical approaches to map imprinted genes (or imprinted quantitative trait loci (iQTL)) has shown promising results. However, current mapping approaches based on diploid genome are limited when the study population is polyploidy such as triploid endosperm. Moreover, current mapping approaches by analyzing phenotypic data measured at a single time point are too simple to take into account developmental or dynamic imprinting trajectories. In this project the investigator develops a collection of novel statistical models for mapping imprinted genes that govern phenotypic traits of interests. The objectives of the proposed research are (a) to have a thorough statistical investigation of the variance component model for mapping imprinted genes underlying single or multiple endosperm traits; (b) to propose methodologies for modeling imprinted gene interactions; (c) to develop functional iQTL mapping approaches for mapping of complex dynamic imprinted traits. With the development of human HapMap project and the availability of sequence information for other species, another aim of the project is to develop statistical methods to unravel the genetic secret of genomic imprinting in sequence level. Efficient statistical algorithms, biologically meaningful hypothesis tests, and robust model assessment tools are under investigation.<br/><br/>Imprinting phenomena have been increasingly observed in a wide spectrum, spanning from plants, animals to humans. In cereals, the endosperm of a grain is the main storage organ serving the major source of food for humans. A number of endosperm traits beneficial to humans are controlled by imprinted genes. In humans, many previously puzzling diseases such as Prader-Willis syndrome and Angelman syndrome are known to be affected by imprinted genes. In this project, the investigator will develop biologically meaningful statistical methods to hunt for imprinted genes underlying complex traits, in order to enhance our understanding of genetic architecture of genomic imprinting and the function of imprinted genes. The developed models, algorithms and software will allow researchers to better analyze their data in the hope of better understanding the genetic basis of genomic imprinting. The project will significantly benefit society by advancing the discovery of imprinted genes to help animal and plant breeders to improve trait quality, and to facilitate identification of new drugs to enhance public health. The research will be integrated into education to train new generations in statistical genetics, and will be widely disseminated through publications, presentations, online software and collaborations with geneticists.<br/><br/>"
"0706385","Statistical Methods for Fingerprint Image Analysis","DMS","STATISTICS","09/01/2007","07/27/2007","Sarat Dass","MI","Michigan State University","Standard Grant","Gabor Szekely","08/31/2011","$200,000.00","Anil Jain","sdass@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","0000, OTHR","$0.00","Biometric recognition, or biometrics, refers to the automatic recognition of a person based on his anatomical or behavioral characteristics. Among the various biometric traits (e.g., face, iris, fingerprint, voice), fingerprint-based authentication has the longest history, and it has been successfully adopted in both forensic and civilian applications. However, the performance of current fingerprint recognition systems is inadequate in the presence of noisy and deformed images, especially when the fingerprint database is very large. Three areas of statistical research are impacted, namely, the analysis of functional data, multivariate dependence, and spatial and general point processes. The proposed research will develop and utilize a Bayesian framework and related computational schemes for inference. This framework will be used to address following specific problems: fingerprint feature detection, modeling noisy and deformed images, fingerprint individuality (i.e., uniqueness) assessment, and effective distributional representations for information fusion (multi-biometrics).<br/><br/>Advances in fingerprint capture technology have resulted in new large scale civilian applications such as the US-VISIT program. However, these systems still encounter difficulties due to the effects of biometric variability present in operating environments and the massive number of comparisons that have to be executed in each identification task. The proposed model-based methods are likely to improve the effectiveness of various fingerprint processing tasks which will eventually yield improved identification performance in real operating environments. Further, this research will impact how fingerprint evidence is reported and used for the identification of suspects. The proposed research increases the role of statistics in important computer science and engineering applications, and provides an impetus for inter-disciplinary research and synergistic activities. Both undergraduate and graduate students working on the proposed topics will develop the analytical and computing skills required to perform scientific research. In this way, the proposed research helps in the creation of future scientists to work in the emerging and critical field of biometric recognition."
"0706755","Development of Stochastic Approximation Monte Carlo Methods","DMS","STATISTICS","09/01/2007","05/24/2007","Faming Liang","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","08/31/2010","$140,000.00","","fmliang@purdue.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, OTHR","$0.00","During the past five decades, Markov chain Monte Carlo (MCMC) methods have been developed as a versatile and powerful tool for scientific computing. However, as known by many researchers, conventional MCMC methods are prone to get trapped in local energy minima in simulations from a system with a rugged energy landscape, rendering the simulations inefficient. To overcome this difficulty, the investigaor and his collaborators recently proposed a non-Markov chain Monte Carlo algorithm, the so-called stochastic approximation Monte Carlo (SAMC) algorithm. Extensive numerical  results show that SAMC can outperform its MCMC competitors for many hard computational problems, such as molecular structure prediction, phylogenetic tree reconstruction, and complex model selection problems. This project continues to develop SAMC in both theory and methodology. First, SAMC is generalized by allowing some statistical smoothing techniques to be used in iterations to improve its efficiency. A rigorous theory is established concerning the convergence and asymptotic behavior of the generalized algorithm. Second, SAMC is generalized by changing its current discrete setting to continuous one. The resulting algorithm is particularly suitable for solving marginal density estimation problems. Third, SAMC is further improved by making use of some techniques developed in evolutionary computing. Preliminary results show that the performance of SAMC can be significantly improved by the new developments.<br/><br/><br/>This project provides some advanced computational methods, which can play an important role in solving some hard scientific problems, such as molecular structure prediction, phylogeny analysis, genetic network inference, machine learning, and VLSI design. Successful computation to these problems in turn enhances people's understanding to them. This project has broader impacts in both communities of statistical theory and scientific computing. The research results are disseminated to these communities via PI's direct collaboration with researchers in other disciplines, conference presentations, books, and papers published in academic journals. This project has also significant impacts on education through direct involvement of graduate students and incorporation of results into undergraduate and graduate courses.<br/><br/>"
"0706041","Statistics and Information Theory","DMS","STATISTICS","05/15/2007","03/24/2009","Bhaskar Bhattacharya","IL","Southern Illinois University at Carbondale","Continuing Grant","Gabor Szekely","04/30/2011","$118,213.00","","bhaskar@siu.edu","900 S NORMAL AVE","CARBONDALE","IL","629014302","6184534540","MPS","1269","0000, OTHR","$0.00","This research enhances statistical inference by applying techniques from information theory, makes new developments and gains new insights into many existing methodology. Specifically, this project concentrates on the following topics.  (1) In the context of incorporating historical data when analyzing current data, often optimal posterior data analysis uses power priors; however,  the suggested value of the optimal power has been called 'highly skeptical' (Ibrahim, et al, 2003, JASA) and when multiple historical data sets are present, no optimal set of powers is known. PI solves this by framing the optimization problem differently and using tools from the duality principles of optimization. For the multiple historical data sets, this project introduces different criteria motivated by different objectives, which will be studied and compared. (2) PI investigates the efficiency of the optimal solutions of (1) under constraints involving divergence between densities and with emphasis on `no loss of information' between input information and output information. (3) PI investigates the Neyman-Pearson testing rule in the  context of information theory with emphasis on finding the type I and type II errors. Finding these errors is extremely helpful when only asymptotic tests are available. Results are applied to testing the effect of dose using a logistic regression model with several covariates, which has an asymptotic chi-square distribution with 1 d.f.  (4) New methods are developed for variable selection in regression by considering projection from the full model subject to constraints that limit the search to only those which are within a certain 'divergence' from a given model (e.g., intercept only). One of the merits of this procedure is that it is simpler to use and avoids the complicated Bayesian calculations.  (5) PI investigates the asymptotic behavior of an I-projection when the constraints are any convex sets C. Haberman  (Annals of Statistics, 1984) has considered the asymptotic properties of an I-projection when the constraints are moment equalities; however, those results do not extend directly when other types of constraints are involved (e.g., moment inequality, constraints with divergences, etc.). This research emphasizes on the dual optimization problem and its asymptotic properties, through which a connection is made to the primal problem.  Estimation of a functional, including its confidence interval, will also be considered. (6) PI introduces the concept of pseudo-divergence, where a nuisance parameter is replaced by a consistent  estimator into the criterion to be optimized. The asymptotic properties of the estimator of the parameter of interest is investigated. (7) PI applies information theoretic approach in multinomial response models where logarithmic penalty functions are commonly used to determine optimal prediction under a given model.  This research would enable one to construct models with built-in inequality constraints on parameters, which would be very useful in model selection.<br/><br/>The intellectual merit of the project is that using information theoretic techniques new aspects of statistical inference procedures have been developed. In this project new optimal distributions are presented in posterior data analysis and they are shown to be highly efficient, the error rates in hypothesis testing are found for small sample sizes, new strategies for model selection are demonstrated, the asymptotic distribution of the I-projection for general convex sets is derived and the concept of pseudo-divergence is introduced where the asymptotic distribution of the estimator of the parameter of interest is found.  The broader impact of the project is that the results are directly useful in many areas including actuarial science, product reliability and manufacturing, Bayesian prior selection, econometrics and finance, health and medicine. The proposed activities  involve training of graduate students in statistics as well as providing selected undergraduate students with research experience.<br/>"
"0704130","Model diagnostics under long memory, and for spatial data","DMS","STATISTICS","06/01/2007","03/24/2009","Hira Koul","MI","Michigan State University","Continuing Grant","Gabor Szekely","05/31/2011","$242,741.00","","koul@stt.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","0000, OTHR","$0.00","A discrete time stationary stochastic process is said to have long memory if its auto-correlations tend to zero hyperbolically in the lag parameter, as the lag tends to infinity. Physical and social sciences are full of real<br/>data examples that exhibit this behavior in the presence of conditionalheteroscedasticity and where regression functions are nonlinear and non-smooth. The first part of the proposal focuses on developing useful and optimal lack-of-fit tests for fitting a nonlinear and non-smooth parametric regression function in the presence of heteroscedastic and long memory moving average (LMMA) errors, and when designs are either non-random or  LMMA. It is further proposed to construct useful and optimal tests for testing the equality of two or more regression functions against one or two sided alternatives, when the error and the covariate processes follow some LMMA models. The second part of proposal is concerned with developing robust inference for a first order quadrant autoregressive process, a process that is a unilateral autoregressive process in the plane. P.I. proposes to provide a class of minimum distance tests for fitting a parametric first order quadrant autoregressive process. In addition, assuming such a model is valid, P.I. proposes to develop asymptotically distribution free tests for fitting an error distribution.<br/> <br/>A data set is said to have long memory if an association between distant observations is slowly decaying but persistent, as the distance between observations increases. A data set observed over a period of time is called<br/>a time series. A heteroscedastic time series is one where the conditional variability of an observation at the current time, given the past, depends on the past. Such data often arises in economics, finance, hydrology, and<br/>physical sciences. In particular, an important example of long memory heteroscedastic time series is  the volatility of spot returns. Part of the emphasis of the proposal is on developing optimal inferential procedures in a class of non-smooth non-linear heteroscedastic time series models. Practical modelling of numerous agricultural and environmental phenomenon involve spatial correlations. A useful model for analyzing spatial correlations is a unilateral autoregressive time series, also known as a first-order quadrant autoregressive process.  This type of processes is especially appropriate when there is an evidence of a spatial movement over the plane in one direction, such as with environmental pollutants transported by winds or ocean currents, or with the spread of a disease.  A model where certain fractional differences of a spatial time series are first-order quadrant autoregressive has been found useful in modelling the slow decay of correlations between yields in two dimensional agricultural field trials. Part of the focus of this proposal is to develop useful and robust inference procedures for the underlying parameters in these models with applications to agriculture and environmental science.<br/> <br/>"
"0706917","Collaborative Research: Optimal Design of Experiments for Categorical Data","DMS","STATISTICS","06/01/2007","03/23/2009","John Stufken","GA","University of Georgia Research Foundation Inc","Continuing Grant","Gabor Szekely","05/31/2011","$86,972.00","","jstufken@gmu.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","<br/>The investigators develop methods for identifying optimal and efficient designs for experiments with categorical data. The project consists of three main parts. (i) Identification of optimal designs for binary data under generalized linear regression models. This part includes consideration of models in which slope and intercept parameters can vary for different groups of subjects and models with a random subject effect. (ii) Identification of optimal allocations of treatments to blocks for comparative studies with binary data. A logistic model is a popular choice for such studies. (iii) Identification of optimal designs for count data under loglinear regression models. In this setting, the investigators focus also on optimal designs for models that can account for subject heterogeneity. This project is innovative in that it uses a new technique that has vast advantages over the commonly used geometric approach.<br/><br/> <br/>Categorical responses are very common in designed experiments in many scientific studies, such as drug discovery, clinical trials, social sciences, marketing, etc. Generalized Linear Models (GLMs) are widely used for modeling such data. Using efficient designs for collecting data in such experiments is critically important. It can reduce the sample size needed for achieving a specified precision, thereby reducing the cost, or improve the precision of estimates for a specified sample size. While research on optimal designs for linear models has been systematically developed over more than 30 years, there are very few research publications on optimal designs for GLMs. This project is important both for the introduction of novel theoretical tools and for its impact on applications. For example, the results of the project significantly reduce the time, money, and the number of patients needed in clinical trials, as well as other scientific studies. The results can help the U.S. Food and Drug Administration to improve its guidelines for clinical trials.<br/><br/>"
"0706192","Empirical Likelihood for the Analysis of Longitudinal Data","DMS","STATISTICS","06/15/2007","04/13/2009","Nicole Lazar","GA","University of Georgia Research Foundation Inc","Continuing Grant","Gabor Szekely","05/31/2011","$145,051.00","","nlazar@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, OTHR","$0.00","Empirical likelihood (EL) is a non-parametric analog of standard parametric likelihood, which has gained in popularity since its introduction in the late 1980s.  Versions of EL have been shown to apply to linear models, generalized linear models, survival data and more.  Furthermore, EL inherits many of the asymptotic properties and behaviors of ordinary likelihoods and is also connected to other popular procedures such as the bootstrap.  All of these facts make it an attractive alternative to parametric methods.  It is more robust, since it makes fewer assumptions (typically only assumptions on the first two moments or on the specific functionals that are being estimated), yet the theoretical performance is equal to that of standard techniques which posit distributional assumptions on the observations.  To date, there has been no extensive work exploring the usefulness of EL in the context of longitudinal data.  This, in spite of the fact that longitudinal data are prevalent in medicine, psychology, epidemiology, and social sciences more broadly.  A popular family of models for longitudinal data is that of generalized estimating equations (GEE), which can also be fit into the framework of EL, creating a natural extension that would combine these two constructs.  Comparison with other widespread approaches, for instance the generalized method of moments and the quadratic inference <br/>function, is then possible.  In the current work the investigator builds EL for longitudinal data settings, compares its performance to existing methods, and develops diagnostics for evaluating the sensitivity of the inference to the specific sample on which it is based. <br/> <br/>Longitudinal data - collected on the same individuals over time - are important in many medical and biological applications.  For instance, in studying the effectiveness of a new treatment for cancer, medical <br/>researchers need to follow patients over time and compare their survival rates to the survival rates of patients receiving an existing treatment.  It is therefore critical to have good statistical models for longitudinal data, models that will take best advantage of the information available from these types of studies.  In this project, the researcher develops a class of models based on ""empirical likelihood.""  Empirical likelihood models make minimal assumptions about the data and are therefore very flexible, general, and robust. At the same time, they inherit many of the important mathematical properties of standard statistical techniques, which make much stronger assumptions that are unlikely to be met in many real-life situations. <br/>The investigator also compares the class of empirical likelihood models to existing methods for the analysis of longitudinal data and establishes conditions under which the new models are expected to perform better. <br/> <br/>"
"0706949","Statistical Graphics Research in Association with GGobi","DMS","STATISTICS","06/01/2007","04/09/2009","Dianne Cook","IA","Iowa State University","Continuing Grant","Gabor Szekely","05/31/2012","$319,999.00","Heike Hofmann","dicook@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","0000, OTHR","$0.00","This research develops methods for interactive graphics, and makes them publicly available in the open source project, GGobi, freely available at http://www.ggobi.org. The development focuses on three<br/>areas: building the infrastructure; developing new methods for interactive facetting, tours and logical linking; and disseminating the research. Infrastructure development includes making the the data pipeline underlying GGobi more modular, improving the communication between GGobi and R, and adding more drawing primitives to facilitate map drawing. GGobi's capacity for handling categorical variables and large data is also expanded. The new methods development is driven by challenges provided in the analysis of multivariate longitudinal and spatio-temporal data. The project pays careful attention to disseminating the work, maintaining stability, portability and extensibility of GGobi, developing curriculum material and video demonstrations, and holding a yearly users and developers meeting.<br/><br/>This research extends an open source software project, called GGobi, which provides interactive plots to explore and discover hidden or unexpected structure in high-dimensional data. High-dimensional data arises in many, many real-life situations. For example, in environmental studies where weather balloons, or remote instruments mounted on satellites, measure variables such as temperature, humidity, wind speed, wind direction, and pressure at many locations on the globe, to study global warming.  Each individual variable might not reveal much. Interactive graphics helps to piece together these multiple variables, to investigate different weather patterns, and discover the impact on local climate patterns. Other examples include studying the characteristics of images as might be used in homeland security applications, and exploring individual trends in longitudinal studies of health records or education achievement. The research pursued under this grant will extend the software to incorporate geographic maps, new methods for plotting categorical variables, and dealing with large amounts of data. As an open source software project a lot of energy is put into disseminating the work to the general community. The software is available freely so that it can be used by anyone with their own data, and that instructors can incorporate the software into their college-level classes. A web site with documentation and tutorials with demonstration movies is maintained at http://www.ggobi.org.<br/><br/><br/>"
"0706857","Estimation of Analytic Surfaces with Applications to Nanoparticle Characterization via Surface Waves","DMS","STATISTICS","08/01/2007","08/01/2007","Richard Charnigo","KY","University of Kentucky Research Foundation","Standard Grant","Gabor Szekely","07/31/2011","$249,999.00","Cidambi Srinivasan, M. Pinar Menguc","RJCharn2@aol.com","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","1269","0000, 7237, OTHR","$0.00","This three-year project advances the statistical field of functional data analysis by rigorously developing a new compound estimation paradigm for simultaneously estimating a mean response and several of its derivatives from noisy data.  A hybrid between local modeling and global modeling, compound estimation circumvents the difficulties associated with local averaging (e.g., kernel), local modeling (e.g., local regression), and global modeling (e.g., spline) approaches.  These difficulties include: the empirical disparity between asymptotic theory and finite-sample performance in local averaging; the pointwise character of local modeling, along with the incompatibilities between mean response estimates and derivative estimates in that setting; and, the unrealistic assumptions tacitly imposed in global modeling, such as there being three nonzero derivatives with discontinuities in the third derivative.  Compound estimation is particularly promising for problems in which physical phenomena are described by differential equations, problems in which modeling velocities and accelerations is of scientific importance, and pattern recognition problems in which features of higher-order derivatives can be exploited for classification.  One such pattern recognition problem has remained an outstanding challenge in nanoscale engineering: can the configuration of nanosize metallic particles, agglomerates, and structures on or near a surface be inferred from surface wave scattering data?  If so, then a major hurdle to building real-time diagnostic tools in nanoscale engineering can be overcome, thereby advancing future nanomanufacturing efforts.  Compound estimation offers a solution to this pattern recognition problem, as mean response estimates and derivative estimates from surface wave scattering data can be employed to objectively determine the most plausible configurations of nanosize particles. Although there is a wealth of underlying mathematics, the principal motivation for compound estimation is practical: to provide tractable yet realistic descriptions of the possibly complicated relationships among variables of scientific interest.  Many applications are envisioned, including studies of: human growth and development patterns in biology; temporal trajectories for infectious disease incidence in public health; dose-response relationships for pharmacological treatments in medicine; stock market and gross domestic product trends in economics; and, the behavior of nanosize particles in engineering.  This three-year project gives particular attention to the last application, both to guide the development of compound estimation and to address an important scientific problem in its own right.  There is a crucial need for advanced instrumentation allowing real-time on-line diagnostics of chemical and physical processes in nanoscale engineering; compound estimation, through its solution to a pattern recognition problem, can help to create the ""eyes"" and ""brain"" for such diagnostics. <br/><br/>Theoretical and methodological developments in compound estimation, along with empirical findings, will be published in appropriate venues and presented to local, regional, and national audiences.  User-friendly software will be made freely available online.  This project will positively impact graduate and undergraduate education at the University of Kentucky, through both the direct involvement of five students during its three-year course and its visibility to incoming or prospective students as an example of exciting multidisciplinary research.  Finally, the investigative team's connections with the Appalachian Math and Science Program and the Umbrella Program on Nanoscale Engineering will be utilized to raise undergraduate and high school student awareness of and appreciation for statistics and applications.<br/><br/>"
"0706805","Theory and algorithms for semi-supervised learning","DMS","STATISTICS","09/01/2007","08/10/2007","Tong Zhang","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","08/31/2010","$150,001.00","","tozhang@illinois.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","<br/>The investigator studies semi-supervised learning from a decision theoretical point of view. The research <br/>shows that in the Bayesian framework, unlabeled data should be used to construct a prior for the purpose of improving predictive learning. More generally, the investigator considers the problem of constructing priors and learning predictive structures on hypothesis spaces from unlabeled data. Under this unified framework, the investigator systematically studies theoretical and algorithmic consequences of semi-supervised learning. <br/><br/>Statistical machine learning is concerned with building computer systems that can predict unobserved information (labels) based on observed information (data). For example, to predict whether a patient has cancer (label) based on blood test (data). Traditionally, a statistical machine learning algorithm builds prediction rules from a set of labeled data. One of the most important issues in practical applications of statistical machine learning is whether one can improve the performance of a learning algorithm by using unlabeled data. This is because unlabeled data are generally abundant while their labels are very costly to obtain. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. This research attempts to establish a general statistical theory for semi-supervised learning, and applies the theory to improve state of the art machine learning algorithms."
"0706733","Statistical Modeling with High-dimensional Data: Variable Selection and Regularization","DMS","STATISTICS","06/01/2007","05/18/2007","Hui Zou","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","05/31/2010","$118,491.00","","hzou@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","With high-dimensional data parsimonious models are preferred because they are much more interpretable and at the same time reduce prediction errors. Regularization is also an essential component in most modern <br/>developments for data analysis, in particular when the number of predictors is large. Non-regularized fitting is guaranteed to give badly over-fitted and useless models. The investigators take a regularization approach to the variable selection problem in high-dimensional statistical modeling such that the resulting model enjoys excellent prediction accuracy and at the same time has a sparse representation. In particular, the investigators develop: (1) new fused variable selection methods in proteomics data analysis which has been a<br/>revolutionary cancer diagnostic tool; (2) a novel kernel logistic regression model which automatically adopts a support-vector representation; (3) several new techniques for performing simultaneous variable selection in estimating multiple quantile regression functions. The investigators also study the theory of these new variable selection techniques. Efficient algorithms and software are developed for public use.<br/><br/>Modern scientific innovations allow scientists to collect massive and high-dimensional data. It is critical in scientific investigations to extract useful information from the huge amount of data. For this reason,  variable selection and dimension reduction play a fundamental role in high-dimensional statistical modeling. Variable selection problems arise from a wide range of fields, machine learning, drug discovery, biomarker finding, genetics, proteomics, brain imaging analysis, financial modeling, environmental sciences, to name a few. The research project aims to develop state-of-the-art statistical tools that help researchers in various fields to analyze their data.<br/>"
"0706436","MSPA-MPS: Experimental design for achieving consistent and high yield in the controlled synthesis of nanostructures","DMS","STATISTICS, METAL & METALLIC NANOSTRUCTURE, MATH PRIORITY SOLICITATION, MSPA-INTERDISCIPLINARY","08/15/2007","08/10/2009","C. F. Jeff Wu","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","07/31/2011","$577,025.00","Roshan Joseph, Zhong Wang","jeffwu@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269, 1771, 7446, 7454","0000, 7237, 7303, 9161, AMPP, OTHR","$0.00","Nanostructures, by virtue of their novel physical, chemical and biological properties, are building blocks in nanoscience and nanotechnology. To meet the needs of large scale, controlled and designed synthesis of nanostructures, it is critical to systematically find experimental conditions under which the desired nanostructures are synthesized reproducibly, at large quantity and with controlled morphology. This project aims at an extensive application of statistical design for achieving the above goal. The problems encountered in the synthesis of nanostructures pose challenges that cannot be solved by existing experimental design techniques. Therefore, the primary objective of this research is to develop and apply novel experimental design techniques in order to find optimum and robust processing conditions for growing pure and high-quality nanostructures under time and cost constraints. Based on the collaboration carried out in the last two years between the two groups, the investigators study a novel technique called minimum energy design that is suitable for guiding experiments in controlled nanostructure synthesis. A unique feature of this technique lies in the fact that it originates from a combination of statistical theory and fundamental laws of physics. It has the potential to become important in both statistical and nanomaterial research. The proposed methods will be developed and validated using the synthesis of nanostructures grown in two different materials: Zinc Oxide (ZnO) and Cadmium Selenide (CdSe). Specifically, the focus is on the synthesis of nanowires. Nanowires have the potential to impact numerous areas ranging from electronics, photonics, optoelectronics to life sciences and health care. In particular, ZnO and CdSe nanowires have tremendous potential of being used in the manufacturing of path breaking nanodevices. Thus the improvement that can be achieved in the synthesis of nanowires by using the proposed technique may create a significant impact in nanotechnology. The progress already made in synthesizing ZnO and CdSe nanowires provides a sound platform to launch the new research project.<br/><br/>With this research project, the issues of yield, quality, purity and robustness of nanomaterial production can all be resolved by applying different variants of the minimum-energy design, which is tailor-made to suit the specific phenomena associated with the growth of nanostructures. This is likely to be an early instance of the integration of statistical design with nanomaterial synthesis, thus opening a new path for nanomaterials manufacturing. Scientifically, this research can facilitate fundamental understanding about the growth mechanisms and kinetics of different nanostructures. Industrially, this research can lead to a commercialized supply of nanostructure with custom desired features. By applying the unique and novel ZnO nanostructures as nanodevice building blocks, this project will also advance micro sensor systems and improve their sensitivity, stability, selectivity, power consumption, and response speed. These advancements can have huge impacts on energy, homeland security and environmental monitoring. From the statistical point of view, the minimum energy algorithm is a novel approach to generate designs that are model independent, can quickly ?carve out? regions with no observable nanostructure morphology, allow for the exploration of complex response surfaces, and can be used for sequential experimentation. Owing to its origination from physical laws, it should be appealing and comprehensible to a broad spectrum of scientific researchers. <br/><br/>"
"0653140","Tenth Conference on Probability and Statistics: Promoting Learning and Research in the Mathematical Sciences in Peru","DMS","PROBABILITY, STATISTICS, Catalyzing New Intl Collab","04/01/2007","03/21/2007","Alicia Carriquiry","IA","Iowa State University","Standard Grant","Tomek Bartoszynski","03/31/2008","$20,000.00","","alicia@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1263, 1269, 7299","0000, 5927, 5977, 7556, OTHR","$0.00","This award supports travel and expenses for participants in the tenth Latin American Congress on Probability and Statistics, to be held in Lima, Peru in March 2007. The main objectives of the conference are:<br/>. To contribute to the development of probability and statistics in Latin America.<br/>. To facilitate the interaction between scientists from the U.S. and Europe and their colleagues in Latin America.<br/>. To provide a valuable experience for talented scientists who have few opportunities to attend conferences outside their region.<br/><br/>Eight distinguished U.S. based researchers will participate in the conference as invited speakers. The award will also provide travel support fourteen young researchers; eight from the U.S. and six from Latin America. Peru has offered to host this year's conference as a means of encouraging its young researchers to increase the level of research activity and education in the mathematical sciences. The National Science Foundation has supported this conference series since 1990, when the congress was held in Mexico. <br/>"
"0706985","Quantile Regression With Time-to-Event Data","DMS","STATISTICS","07/01/2007","03/30/2009","Limin Peng","GA","Emory University","Continuing Grant","Gabor Szekely","12/31/2010","$102,491.00","","lpeng@emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1269","0000, OTHR","$0.00","Time-to-event data frequently arise in diverse fields including biomedical research, economics, engineering, and social sciences. As a significant extension of the classic linear models, quantile regression has growing appeal in survival analysis by offering flexibility in accommodating heterogeneous association as well as direct physical interpretation on event times. Although the foundation of quantile regression has been laid out, many important and challenging statistical issues have not yet been resolved for inferences with time-to-event data: (1) most current methods are derived only for randomly censored data; (2) existing methods for censored quantile regression either require stringent assumptions or involve nonignorable algorithmic complications. In the proposed research, the investigator aims to develop a comprehensive statistical methodology for quantile regression with survival data under various censoring and truncation mechanisms, such as random censoring, left truncation, doubly censoring, and competing risks. The proposed methodological framework will facilitate the development of inferential procedures including hypothesis testing, model diagnostics as well as robust explorations of the varying pattern of covariate effects. The new approaches are expected to preserve the ``computational-ease''<br/>feature of the traditional quantile regression methods for complete data.<br/><br/>The proposed research will have significant impacts and many applications across a variety of fields. For example, applying quantile regression to medical or public health follow-up studies can better inform health professionals of the relationship between predictors, such as treatment or risk factors, and survival outcomes, such as time to morbidity or mortality. The enhanced understanding of variable effects may be effectively used in practice to advance disease treatment or prevention strategies. Applications of the proposed methods can also influence many other aspects of the society, for example, engineering quality control, and economic or social policy making. Results stemming from this project will be integrated with education through graduate level courses and special topic study groups, and will be widely disseminated through publications, conference presentations, internet postings, and free software."
"0704621","Collaborative Research: Model-Based and Model-Free Dimension Reduction with Applications to Bioinformatics","DMS","STATISTICS","07/01/2007","06/04/2007","Bing Li","PA","Pennsylvania State Univ University Park","Standard Grant","Gabor Szekely","06/30/2011","$260,000.00","Francesca Chiaromonte","bing@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, OTHR","$0.00","This proposal is focused on  sufficient dimension reduction (SDR), which comprises methods for reducing the dimension of the predictor vector  X in reference to the response Y in regression or classification problems. In the last 10 to15 years a variety of SDR methods have been developed that do not require a regression model<br/>and that exploit the conditional moments of X given Y. These methods have accrued a striking record of successful applications and have led to a variety of techniques. The investigators propose to introduce inverse reductive models that describe the stochastic structure of X given Y, and not Y given X as in traditional<br/>regression. Preliminary results indicate that this will lead to significant advances in theory, methods and applications. Reductive models provides a unified perspective linking traditional methods such as principal components  and various recent model-free inverse methods. In addition, reductive models can provide information bounds, which make it possible to evaluate and improve upon the performance of existing model-free methods in recognizable contexts.<br/><br/>High-throughput technologies produce massive amounts of complex and interconnected data. More than ever before, understanding experimental evidence and exploring scientific hypotheses require methods to meaningfully reduce high-dimensional data. This is particularly the case for contemporary genomic sciences. Sequencing techniques, alignment algorithms, microarrays and other emerging experimental technologies generate information on genomes, myriads of novel functional elements within them, patterns of simultaneous expression for the thousands of genes they contain, and patterns of evolution across related species. The need to handle this growing body of information has spun a whole new discipline, Bioinformatics,<br/>at the very heart of which are indeed data reduction methods. In this proposal the investigators plan to study a class of  inverse reductive models that unify and improve on existing dimension reduction methods, and that are capable of handling situations where the number of variables far exceed the number of subjects. Such<br/>situations are typical for genomic applications, and are difficult or impossible to study using existing methods.<br/>"
"0706745","Proper Scoring Rules, Calibration and Sharpness: Assessing Predictions for an Uncertain World","DMS","STATISTICS","06/01/2007","01/19/2010","Tilmann Gneiting","WA","University of Washington","Continuing Grant","Gabor Szekely","05/31/2010","$329,878.00","","tilmann@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, OTHR","$0.00","One of the major purposes of statistical analysis is to make forecasts for the future, and to provide suitable measures of the uncertainty associated with them.  Consequently, forecasts should be probabilistic in nature, taking the form of probability distributions over future quantities or events.  With the advent of probabilistic forecasting in applications including weather and climate prediction, computational finance and macroeconomic forecasting, the need for principled statistical techniques for the comparison and evaluation of distributional forecasters is becoming critical.  The project addresses this challenge by developing tools for the assessment of calibration and sharpness, including ramifications of the probability integral transform and the verification rank histogram, and furthering theoretical insight into the construction and characterization, the properties, and the computation of proper scoring rules with desirable properties, such as kernel scores, local scores and skill scores.<br/><br/>A major human desire is to make forecasts for the future.  Forecasts characterize and reduce, but generally do not eliminate uncertainty. Consequently, predictions should be probabilistic in nature, pointing decision makers to alternative scenarios and assigning probabilities to potential future events.  With the advent of probabilistic predictions in applications that include severe weather warnings as well as economic and demographic projections, the need for principled tools for the comparison and evaluation of probabilistic forecast techniques is becoming critical.  In essence, probabilistic predictions ought to be calibrated and sharp, in the sense that they are statistically compatible with the scenarios and events that realize, and reduce uncertainty to the extent possible.  The project studies and develops tools for the assessment of calibration and sharpness, which are tailored to prompt and guide improvements in probabilistic prediction methodologies.  In addition to biomedical, environmental, economic and financial applications, the project informs the reorientation of national weather and climate prediction efforts towards probabilistic forecasting that has recently been recommended by the National Academies.<br/><br/>"
"0726219","Current and Future Trends in Nonparametrics Conference 2007","DMS","STATISTICS","09/01/2007","06/29/2007","Donald Edwards","SC","University South Carolina Research Foundation","Standard Grant","Gabor Szekely","08/31/2008","$13,000.00","Edsel Pena","edwards@stat.sc.edu","915 BULL ST","COLUMBIA","SC","292084009","8037777093","MPS","1269","0000, 7556, 9150, OTHR","$0.00","An international conference on `Current and Future Trends in Nonparametric Statistics' will be held October 11-12, 2007. The conference is co-sponsored by the National Institute of Statistical Sciences. It will feature four plenary talks by internationally prominent researchers. There will also be eight to twelve topic-orientic sessions, each session having four invited talks. There will also be contributed session for junior investigators and graduate students.<br/><br/>Nonparametric statistical methods are methods which avoid unrealistic assumptions about underlying structures in scientific studies. The advent of universally-accessible, powerful computing, and also of massive data sets arising from automated data collection and remote sensing, has brought nonparametric methods to the forefront of statistical theory and practice. The goal of the proposed conference is to present some of the most important recent advances in nonparametric statistics and to discuss future research directions.  The conference is expected to accelerate interactions and collaborations among researchers in the important area of nonparametric statistics, and thereby lead to the development of new and more modern methods of nonparametric modeling and inference."
"0710803","North American Regional Environmetrics Meeting","DMS","STATISTICS","03/15/2007","03/07/2007","Peter Guttorp","WA","University of Washington","Standard Grant","Grace Yang","02/29/2008","$8,000.00","","guttorp@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, 7556, OTHR","$0.00","Funding is requested for costs of registration and travel for young investigators to the first North American Regional Meeting of the International Environmetric Society, taking place in Seattle, WA, June 19-21, 2007. The theme of the meeting is ""Climate change and its environmental effects: monitoring, measuring, and predicting."" Invited session include Paleo-climate assessment, Agro-risk assessment, The role of statistics in public policy, and Assessing trends in extreme climate events. Keynote speakers will be Paul Switzer of Stanford University, and David Brillinger of the University of California at Berkeley.<br/><br/>The International Environmetric Society has traditionally held international meetings every year. A few years ago the European region of the society starting holding regional meetings. The request for this type of regional meeting has been particularly strong from government employees, who have found international travel difficult under limited travel budgets. The meeting will focus on quantitative aspects of environmental science, and is expected to draw some 100-150 participants. The focus and size of the meeting will enable strong interactions between environmental scientists and statisticians/mathematicians.<br/><br/>"
"0705532","Collaborative Research: Generalized Variable Selection With Applications To Functional Data Analysis And Other Problems","DMS","STATISTICS","07/01/2007","05/16/2007","Ji Zhu","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","06/30/2010","$84,940.00","","jizhu@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","When variable selection is performed in situations where the number of predictors is significantly larger than the number of observations, one generally assumes sparsity in the regression coefficients, i.e., most of the coefficients are zero. However, there turn out to be many practical applications where, rather than the parameters being sparse, certain predefined functions of the parameters are sparse. This is referred to as Generalized Variable Selection (GVS). Specifically, the investigators study four important applications of GVS in areas as diverse as functional regression, principal component analysis (both standard and functional), multivariate non-parametric regression, and transcription regulation network problems for microarray experiments.<br/><br/>The investigators have direct connections in many fields outside statistics such as Biology, Finance, Manufacturing, Marketing, Medicine and Physics. The investigators believe that statisticians can, and should, make important contributions in all these areas. With the advent of new technologies, such as bar code scanners and microarrays etc., enormous data sets are becoming increasingly common in these and many other fields. Such vast quantities of data have made it important to develop statistical methodologies that can produce sparse and interpretable solutions. The investigators aim to systematically develop software to implement the proposed methods through free software packages, like R, and then make them readily available and publicize them in all these fields. The investigators believe that, because of the interpretive power of their proposed methods, once the software is available, it will be widely utilized.<br/><br/>"
"0808993","Spectrum Estimation for Spatial Processes","DMS","STATISTICS","09/21/2007","03/26/2009","Tailen Hsing","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","12/31/2010","$175,519.00","","thsing@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","A new, unified approach for estimating spectral densities of spatial processes is proposed. The theoretical properties as well as practical implementation issues of this approach will be thoroughly explored. The completion of this project will provide powerful new tools for kriging, or optimal prediction in certain situations in spatial data analysis. Based on the relationship between the generalized covariance and the spectral density, a new approach is formulated for estimating the spectral density in terms of solving a regularized inverse problem. The generalized covariance can then be estimated thorough the estimated spectral density, which paves the way for kriging. The regularized inverse problem is solved in a reproducing kernel Hilbert space essentially as a constrained optimization problem. A number of crucial issues arise from that. Candidate procedures based on the ideas of unbiased-risk and generalized cross-validation will be studied for the determination of the optimal smoothing parameters from data. Theoretical properties, including mean squared error bounds and asymptotic properties, will be investigated to assess the performance of the approach. Efficient computational algorithms will be sought to overcome the difficulties brought by the high-dimensional nature of the data.<br/><br/>The research in this project offers a new perspective on the analysis of spatial data. The kind of data that the investigator has in mind are data observed at multiple spatial locations and possibly also at multiple time points. The general goals are to identify the data generation process and to make predictions beyond the spatial-temporal region where data are available. One of the keys in such problems is to understand the dependence relationship between the various pieces of the data. The approach in this project targets this problem for a broad class of models. Potential applications of the new theory and methodology exist in numerous contexts, including the environment, geography, and sensor networks.<br/><br/><br/>"
"0706700","Theory and Methodology for Semiparametric Linear Models with Censored Data","DMS","STATISTICS","06/01/2007","03/23/2009","Bin Nan","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","05/31/2011","$172,678.00","","nanb@uci.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","Instead of modeling the hazard function for censored survival data as the famous Cox model does, modeling the survival time directly by certain transformation becomes increasingly appealing to practitioners because it postulates a simple relationship between the response variable and covariates with easily interpretable parameters. As a special example, the semiparametric accelerated failure time model that transforms the failure time by logarithm has been studied extensively in the past decade. Existing challenges for those semiparametric linear transformation models include semiparametric efficient estimation, asymptotic theory with more realistic conditions that may lead to good properties for survival time prediction, a measurability issue in the stochastic integral formulation for the outcome-dependent weighted estimating methods, and high-dimensional data analysis. The investigator proposes new methods to tackle those emerging issues in the semiparametric linear transformation models. Asymptotic theories will be proved by using the modern empirical process theory. Numerical implementations of all the proposed methods will be based on either those well developed algorithms for discrete estimating functions or the Newton-Raphson method for smooth objective functions in which the infinite-dimensional parameter is approximated by a smoothing estimator. To enhance the predicting ability, more flexible transformations are considered for problems with high-dimensional data. Penalized method will be investigated in order to obtain simultaneous variable selection and survival time prediction.<br/><br/><br/>Statistical models considered in this project have important applications in a wide spectrum of disciplines such as biology, medicine, health studies, and engineering. The proposed research is particularly motivated by the multi-cohort study for the women's reproductive life staging in which the prediction of age at menopause is of major interest, and by the Michigan ovary cancer and lung cancer studies that look for relevant genes and good models for predicting patients' survival using gene expression data. It will provide methods that use data more efficiently and yield more precise prediction. It will also allow the investigator to add more thorough statistical results to the courses of advanced survival analysis and semiparametric models for graduate students. The proposed research activities will motivate graduate students to become independent researchers who are able to engage in fundamental statistical research.<br/><br/>"
"0705210","Multivariate Statistical Analysis and Image Classification with Applications","DMS","STATISTICS","07/01/2007","05/26/2009","Donald Richards","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","12/31/2010","$290,000.00","Jia Li","richards@stat.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, OTHR","$0.00","This research project will develop methodologies in multivariate statistical analysis for the purposes of applications to statistical image learning, incomplete data, regularized discriminant analysis, and related areas.  The investigators will develop exact inference for multiple population comparisons, principal components, imputation procedures, clustering and mixture modeling techniques for large-scale image classification, and regularization methods for statistical image models.  The results of the project will have impact on image retrieval and annotation, and statistical methods for analyses of data arising from national panel surveys, social and behavioral research, toxicology research, and any area in which multivariate incomplete data commonly appear.  Computer software packages for algorithms and analytical tools developed in the project will be documented for public access.<br/><br/>Owing to cutting-edge trends in wildlife research, environmental sciences, social and behavioral research, image analysis, and related areas, new statistical and probabilistic ideas are needed to analyze statistical data at small sample sizes.  The investigators will develop statistical formulas and computational algorithms for analyzing such data at small sample sizes.  Image searches provided by major search engines, such as Google and Yahoo!, rely on textual descriptions of images found on Web pages containing the images and file names of those images.  This project will advance the technology of computer-assisted annotation of images by words and hence enhance the visibility of images on the Internet.  Other benefits of the project to the society include <br/>outreach activities in which the investigators will be involved in summer school programs for high-school students and the development of publicly-available image analysis and discriminant analysis software.<br/><br/>"
"0706771","Corrected Brownian Approximations and Hybrid Bootstrap Applications","DMS","STATISTICS","07/01/2007","07/29/2009","Robert Keener","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","06/30/2011","$149,991.00","","keener@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","Brownian motion approximations have been developed to estimate probabilities and expectations that arise in boundary crossing problems in discrete time.  The proposed research derives correction terms to improve the accuracy of these Brownian approximations in various settings.  The hybrid bootstrap is a new resampling method used to set confidence intervals. It is particularly relevant and appropriate when an experiment provides limited information about the parameter of interest, but substantial information about nuisance parameters.  Three applications are considered: estimating the signal in a signal plus noise Poisson model of interest in high energy physics; estimating the location of a quantitative trait loci on a strand of DNA with data from a back-cross or inter-cross design; and estimating new parameters after sequential change point detection.<br/><br/>Two different topics are considered for this project. The first concerns Brownian approximations. The specific goal is to derive correction terms to improve these approximations for a class of boundary crossing problems in discrete time. Brownian and diffusion approximations have been a major tool in stochastic modeling, with applications to diverse areas including sequential analysis in statistics, queuing theory for industrial and networking applications, and options pricing in finance. New methods to improve the accuracy these approximations should have broad value. The other topic concerns the hybrid bootstrap approach to interval estimation.  Bootstrap methods in statistics use computer simulation to help a researcher assess the precision of an estimator. Hybrid bootstrapping is a modern variant of this approach which is particularly relevant in situations where the experiment provides limited information about the parameter of interest.  Three specific applications are being studied. The first concerns experiments in physics in which a new particle or phenomenon, if present, will increase the rate of events, counted in the course of the experiment. A second application concerns modern genetics, trying to estimate locations for loci on a strand of DNA associated with a quantitative trait of interest. The final application arises in situations where a process of interest is monitored to detect changes. Industrial processes are often tracked since changes are often associated with production problems that should be noted and fixed as soon as possible to improve quality and output. Networks are often monitored for similar reasons, and financial series may be monitored for a variety of reasons. The hybrid bootstrap should be useful for new parameters describing process evolution after the change point.<br/>"
"0706935","Bayesian Analysis for Studies of Gene-Environment Interaction","DMS","STATISTICS","06/01/2007","04/30/2008","Bhramar Mukherjee","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","05/31/2010","$134,451.00","","bhramar@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","ABSTRACT<br/>                 <br/>In case-control studies of gene-environment association with disease, when genetic and environmental exposures can be assumed to be independent in the underlying population, one may exploit the<br/>independence  assumption in order to derive more efficient estimation techniques than the traditional logistic regression analysis. Many of the classical results for case-control analysis, which assume the covariate distribution to be non-parametric, do not hold under a constrained space of exposure distributions. However, the gain in efficiency of modern retrospective methods comes at the cost of lack of robustness, since large biases are introduced in the retrospective estimates under violation of the gene-environment independence assumption. The main goal of this research proposal is to find natural analytical tools to solve the model specification dilemma of modern retrospective analysis of studies of gene-environment interaction, under some commonly used epidemiological designs. Using the profile-likelihood framework developed by Chatterjee and Carroll (2005, Biometrika), the investigator proposes a Bayesian approach that incorporates uncertainty regarding the assumed constraint of gene-environment independence in a natural data adaptive way. The proposed shrinkage estimator, conceived from a Bayesian standpoint, is designed to maintain attractive efficiency properties, without relying on unverifiable model constraints. Theoretical properties of the proposed estimator are studied under varying scenarios of gene-environment association. The investigator considers both empirical Bayes and hierarchical Bayes methods to relax gene-environment independence assumption. The proposal explores the connection of the Bayesian approaches to an alternative random-effects model. The methods are extended beyond the commonly used unmatched case-control study design to two-phase and family-based studies of gene-environment interaction.<br/><br/>Two scientific streams are currently playing extremely important roles in clinical medicine and public health: the molecular biology approach with an emphasis on genetics, and the quantitative approach with an <br/>emphasis on epidemiology. The developments in these areas jointly are making fundamental contributions to the study of etiology, diagnosis, prognosis and treatment of complex diseases.  Phenomenal advancement of <br/>medical science and genetic technology is giving rise to many complex design and analysis issues which statisticians and epidemiologists have never confronted before.  This proposal lies in that new interface of human genetics, epidemiology and statistics. Case-control studies are being increasingly used for studying the<br/>association between a disease and a candidate gene.  However, except for some rare diseases, such as Huntington or Tay Sachs disease which may be the result of a deficiency of a single gene product,<br/>most common human diseases have a multifactorial etiology involving complex interplay of many genetic and environmental factors. By identifying and characterizing such complicated gene-environment interactions through clinical and epidemiological studies, one has more opportunities to understand the genesis and etiology of complex diseases and to develop targeted intervention strategies for high-risk individuals. The proposal presents robust and efficient statistical techniques to investigate the synergism between gene and environment in studying complex diseases. The high-performance computing tools developed in the proposal makes it feasible to use the methods in large-scale applications such as genome-wide association studies.<br/><br/><br/><br/><br/>"
"0705288","Function estimation under shape constraints and detection of thresholds in nonparametric and semiparametric problems","DMS","STATISTICS","07/01/2007","04/18/2007","Moulinath Banerjee","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","06/30/2010","$185,476.00","","moulib@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","The proposed project deals with methodology and inference strategies for nonparametric and semiparametric problems exhibiting non--standard asymptotics, problems where estimators converge at rates different from the usual rate (square root of the sample size) and/or have non--Gaussian limit distributions. The two core areas of investigation are: (A) Estimation of functions under shape restrictions, and (B) Estimation of an appropriate ``threshold'' in the domain of a function where sharp and potentially substantial changes occur. Part of the emphasis in shape--restricted inference will be on maximum likelihood/least squares based procedures and in particular, the construction of confidence sets for quantities of interest (like a regression function, a hazard function etc.) by inversion of residual sum of squares or likelihood ratio statistics. This is in light of initial exploration for monotone function models where such statistics are seen to exhibit asymptotically pivotal behavior, which facilitates inference, since nuisance parameters need not be estimated from the data. Shape constraints of interest are monotonicity, unimodality and convexity/concavity, and often a combination of such features. Smoothing under shape constraints will also be investigated in some of these problems, since smoothing typically yields faster rates of convergence and provides automatic estimation of derivatives of functions, which are often of interest, for example in economics. On the threshold estimation front, two main areas will be explored. The first is change--point estimation, where there is a jump in the value of the function of interest or in the value of the derivative, with the focus being on design issues: how to design the sampling mechanism, given a fixed budget (of points that can be sampled) so as to entail precise detection of the jump. The second area concerns studying appropriate notions of a threshold for smooth functions that show rapid change over a short domain. One such notion can be formulated in terms of a function with a finite number of discontinuities that is used simply as an approximation, or a working model for the true function. The discontinuities of the best fitting working model provide a natural description of a threshold. Split--point estimation, as this is known, turns out to be radically different from change point estimation, in light of some initial work and will be investigated in more detail.<br/><br/>The proposed research will have diverse applications, ranging from disciplines in public health like biomedical studies and epidemiology to topics in the social sciences (economics) and the physical sciences (astronomy). Shape restrictions, for example, show up naturally in the analysis of productions of firms/companies (economics), the study of the risk of succumbing to illness or infection with age (biomedical research/public health), and problems associated with the detection of dark matter in galaxies, solutions to which can shed light on the future evolution of the universe (astrophysics). The P.I. is actively involved in collaborations with econometricians, epidemologists and astronomers: the research emanating from this grant will therefore have strong interdisciplinary flavor and will address many real scientific questions of interest.  On the threshold estimation front, statistical methods for split point detection are of importance, since split points have been used as a measure of threshold by ecologists in the development of pollution control standards. The problems related to change point detection will be applied to detection of stress thresholds in engineering systems. The statistical methodology to be developed will be circulated to the statistical community in academia and industry through free software packages. On the educational front, some of the research material in this proposal will be incorporated in advanced courses at the graduate level and an interdisciplinary seminar series. Some projects will also serve as dissertation topics for Ph.D. advisees and will therefore play an important role in the training of future statisticians.<br/>"
"0652624","FRG:  Collaborative Research: Overcomplete Representations with Incomplete Data: Theory, Algorithms, and Signal Processing Applications","DMS","STATISTICS","07/01/2007","03/30/2009","Marianna Pensky","FL","The University of Central Florida Board of Trustees","Continuing Grant","Gabor Szekely","06/30/2011","$159,990.00","","Marianna.Pensky@ucf.edu","4000 CENTRAL FLORIDA BLVD","ORLANDO","FL","328168005","4078230387","MPS","1269","0000, 1616, OTHR","$0.00","Driven by accumulated scientific results and recent breakthroughs in sparse representations, recent years have seen an ever-increasing interest in overcomplete expansions with incomplete data---a critical subject requiring close cooperation and exchange of ideas amongst statisticians, mathematicians, and engineers.  A number of indicators suggest the appropriateness and timeliness of a Focused Research Group (FRG) involving these three communities as the best means to approach to this high-potential yet challenging research area.  In particular, this project follows a comprehensive and vertically integrated research plan for (1) deriving new theoretical results for statistical estimation in the context of overcomplete Gabor time-frequency representations and multiresolution wavelet dictionaries; (2) leveraging these results to develop algorithms tailored for canonical problems in signal and image processing, where practitioners are often faced with missing data or more generally incomplete measurements; and (3) addressing ubiquitous and important cross-cutting applications, including curve fitting as well as audio and color image enhancement. To respond to these pressing scientific needs and prepare the ground for significant developments in the mathematical sciences, the FRG team is exploiting recent results from harmonic analysis and the theory of frames to develop a coherent framework for statistical modeling in the case of overcomplete expansions, including an examination of key open questions such as the impact of the choice of prior coefficient distributions in a Bayesian framework and asymptotic risk bounds for regression when the set of potential predictors is overcomplete.  As a definitive first step toward these grand challenges, the team proposes and investigates an innovative common-component model for frame coefficients that recovers currently used methods as special cases but opens up important new avenues for advancement.  The FRG team has significant prior experience in multiresolution analysis, computational Bayesian inference, and self-consistency methods for missing data, and hence is also developing and applying state-of-the-art procedures to implement the resulting new algorithms.<br/><br/>The Focused Research Group (FRG) project team combines scientists from an established institution (Harvard University) and a young, rapidly growing one (University of Central Florida).  The project's research agenda is set to substantially advance the theoretical knowledge and understanding of the applicability of overcomplete representations (a new and important cross-cutting area of mathematics, with many major open questions relating  to the area of ""compressed sensing"" recently featured in the New York Times, The Economist, and elsewhere in the mainstream media) in both statistical and engineering practice. This will ultimately lead to development of more efficient algorithms for signal processing and data analysis in situations where data must be collected at a very low rate (as in the compressed sensing regime described above), or when a portion of available data has been lost or highly contaminated. The latter scenario is particularly salient both for commercial applications (e.g., voice data in the case of cellular<br/>communications) as well as military and homeland security concerns (for instance, to recover unobserved data from related sources). Another benefit of the project it its emphasis on close collaboration amongst mathematicians, statisticians, and engineers through a single team, which will lead not only to solution of the specific problems under study, but also to formulations of new important areas of research and their application to the real world. Using support from NSF, the team trains a number of students who are ready to carry out research on the cutting edge of mathematics, statistics and engineering, and holds regular workshops to increase the involvement of new researchers and disseminate results to the wider scientific community.<br/>"
"0749718","CAREER: Nonparametric likelihood, estimating functions, and causal inference","DMS","STATISTICS","07/01/2007","03/28/2011","Zhiqiang Tan","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","02/28/2013","$400,005.00","","ztan@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, 1045, OTHR","$0.00","Likelihood introduced by Fisher is a central concept in statistics both from frequentist and Bayesian viewpoints. The  research project is to advance a nonparametric likelihood approach that retains both the original meaning and the inferential power of Fisher's likelihood, and at the same time to construct estimating functions geared towards point estimators and Wald-type confidence intervals. The research studies semiparametric models for two-sample and regression problems in the absence and in the presence of missing data. The project also investigates statistical tools for causal inference in longitudinal studies with time-dependent treatments and confounders. The investigator's education plan involves designing a course on nonparametric likelihood and estimating functions with applications to semiparametric models and causal inference; supervising students with various backgrounds; establishing a causal inference working group as a research and educational platform; and organizing causal inference workshops for researchers and students to facilitate communications and collaborations.<br/><br/>The research will improve the validity and accuracy of inferences about environmental exposures, medical treatments, behavioral interventions among others in environmental, biomedical, and socioeconomic studies. The educational activities will help students from various backgrounds and researchers from various disciplines to acquire state-of-the-art statistical ideas and methods for empirical investigation and discovery.<br/>"
"0704098","Collaborative Research: Model-Based and Model-Free Dimension Reduction with Applications to Bioinformatics","DMS","STATISTICS","07/01/2007","06/04/2007","Ralph Cook","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","06/30/2011","$184,564.00","","dennis@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","This proposal is focused on  sufficient dimension reduction (SDR), which comprises methods for reducing the dimension of the predictor vector  X in reference to the response Y in regression or classification problems. In the last 10 to15 years a variety of SDR methods have been developed that do not require a regression model<br/>and that exploit the conditional moments of X given Y. These methods have accrued a striking record of successful applications and have led to a variety of techniques. The investigators propose to introduce inverse reductive models that describe the stochastic structure of X given Y, and not Y given X as in traditional<br/>regression. Preliminary results indicate that this will lead to significant advances in theory, methods and applications. Reductive models provides a unified perspective linking traditional methods such as principal components  and various recent model-free inverse methods. In addition, reductive models can provide information bounds, which make it possible to evaluate and improve upon the performance of existing model-free methods in recognizable contexts.<br/><br/>High-throughput technologies produce massive amounts of complex and interconnected data. More than ever before, understanding experimental evidence and exploring scientific hypotheses require methods to meaningfully reduce high-dimensional data. This is particularly the case for contemporary genomic sciences. Sequencing techniques, alignment algorithms, microarrays and other emerging experimental technologies generate information on genomes, myriads of novel functional elements within them, patterns of simultaneous expression for the thousands of genes they contain, and patterns of evolution across related species. The need to handle this growing body of information has spun a whole new discipline, Bioinformatics,<br/>at the very heart of which are indeed data reduction methods. In this proposal the investigators plan to study a class of  inverse reductive models that unify and improve on existing dimension reduction methods, and that are capable of handling situations where the number of variables far exceed the number of subjects. Such<br/>situations are typical for genomic applications, and are difficult or impossible to study using existing methods.<br/>"
"0706850","Model Selection Diagnostics and Localized Model Selection/Combination","DMS","STATISTICS","06/01/2007","05/07/2007","Yuhong Yang","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","05/31/2011","$197,482.00","","yyang@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","Although research in the last decade has brought in general awareness of the seriousness of statistical uncertainty due to model selection, much more effort is needed to reform the currently still dominating practice of basing all statistical conclusions on a final selected model. From a methodological standpoint, a critical component missing in the toolbox of model selection and model combination is model selection diagnostics (not model diagnostics). The PI seeks model selection diagnosis methods that go beyond simple bootstrap uncertainty measures. They will address the uncertainty in variable selection and in estimation of a quantity of interest via means that take into account the distances between subsets of variables or between estimates from the candidate models. For high-dimensional or complex data, it is very likely that different candidate procedures perform the best in different regions, especially when very distinct learning methods are considered. This calls for localized model selection/combination methodology and theory, which is the second major component of this project. The PI takes new approaches and derives oracle inequalities on performance of the new methods for localized model selection or combination.<br/><br/>Statistical methods have become an essential ingredient in all applied sciences. Proper quantifications of the true uncertainty in mathematical descriptions of the natural and social phenomena are fundamentally important to draw unbiased and accurate conclusions. Since model selection and model combination play a central role in statistical analysis, the proposed work on accurately measuring model selection uncertainty and the resulting better tools for model selection and model combination, together with other researches in the area, are expected to contribute substantially to changing the currently unsound practices of statistical model selection in applied sciences. The improved use of data in information extraction will have broader impacts in scientific research, policy and decision making.<br/>"
"0707053","From Centrality To Extremity in Multivariate Statistics:  Data Depth, Extreme Value Theory and Applications","DMS","STATISTICS","07/01/2007","07/01/2009","Regina Liu","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","06/30/2011","$299,767.00","","rliu@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","<br/>Much of multivariate inference and applications evolve around the centrality and/or extremity of the data sets or their underlying distributions. The goals of this proposal are: (i) to develop new nonparametric statistical methodologies for studying the effects of centrality and extremity of data by using data depth and extreme value theory, and (ii) to demonstrate the usefulness of these methodologies in real-life applications, including: using data depth to construct a tractable measure of self-complexity for studying depression and anxiety, detecting performances with extreme risk in the simultaneous monitoring of multiple risk measures, and classifying different genome groups for more effective medical treatments. The research findings would advance the theory underlying each topic and broaden the applicability of statistics to other fields. The lines of investigation are interwoven and are all motivated to build a comprehensive multivariate statistical analysis scheme.<br/><br/>This research helps to develop a meaningful self-complexity measure to diagnose patients with psychiatric disorders and provide better health care for such patients. This research also aims to devise an effective threshold system for signaling extreme risks, which should be useful for risk management of rare events, such as in aviation safety or catastrophic events due to climate changes."
"0733734","Travel Support for the 3rd International Joint IMS-ISBA Conference","DMS","STATISTICS","09/01/2007","08/17/2007","Bradley Carlin","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","08/31/2008","$11,000.00","","brad@biostat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, 7556, OTHR","$0.00","Statisticians play an indispensable role in the analysis of biomedical, environmental and public health data, from the study design stage all the way through to final analysis and report-writing. Scholarly conferences<br/>where new ideas can be exchanged are important for statistical science to move forward.  International meetings, while of course rarer due to their size and expense, are particularly beneficial since they permit exposure to ideas and colleagues the attendee might not ordinary see or even read about in the familiar<br/>journals of one's own country.  The benefit of and need for attendance at such meetings by junior statistical researchers is particularly great, since they contribute mightily to their professional development and help<br/>level the playing field with more established senior investigators.  This grant will provide support for the Third International Joint Meeting of the IMS (Institute of Mathematical Statistics) and ISBA (the International Society for Bayesian Analysis), to be held in Bormio, Italy on Wednesday, January 9 to Friday, January 11, 2008.  A central theme of the conference will be Markov chain Monte Carlo (MCMC) and<br/>related methods and applications ranging from finance to technology, with specific focus on challenging problems in biostatistics.  All grant funds will be used to support the travel expenses of junior biostatistical<br/>investigators, i.e., persons pursuing a PhD or DrPH in statistics, biostatistics, or a closely related field, or who have received such a degree within the five years preceding the conference.<br/><br/><br/><br/>"
"0706842","Semiparametric and Nonparametric Methods of Model Selection and Model Checking for Correlated Data","DMS","STATISTICS","07/01/2007","05/11/2009","Lan Wang","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","06/30/2010","$124,978.00","","lanwang@mbs.miami.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","Model selection and model checking are fundamental to statistical analysis. The main objective of model selection is to identify parsimonious well-fitting models in order to balance the increase in model fit against the increase in model complexity; and that of model checking is to assess or test the validity of a proposed model. An enormous amount of literature is available for independent data in this research area.  For correlated data, however, there exists only fragmented work and the asymptotic theory is largely undeveloped, mainly due to (1) the lack of a rich class of models such as multivariate Gaussian for the joint distributions of the responses, (2) the complexity of the joint likelihood even when such a multivariate family of distributions is available. These obstacles make it extremely challenging, if not impossible, to apply existing model selection and model checking procedures that were developed for independent data or based on full likelihood. This project addresses this challenge by developing a set of semiparametric and nonparametric tools for model selection and model checking for correlated data, including model checking procedures based on moment conditions via the recently developed quadratic inference function and rank-based estimation equations; data-driven model checking procedures that allow for flexible alternative and increase general power performance. The large sample theory and practical performance will be investigated in depth in this project. Also on the agenda are related research issues, such as the characterization of rank regression under possible model misspecification and the theoretical robustness properties of rank-based model selection algorithms.<br/><br/>Correlated data frequently occur in many fields, such as biomedical and health sciences, economics, social sciences and environmental studies. This work will greatly enhance the available methodologies and theories for model selection and model checking. The investigator will develop computational packages that can be easily implemented by statisticians and scientists. This project will provide scientists with new and flexible tools for analyzing high-dimensional correlated data. Education will be an important component. The research results will be incorporated at different levels of statistical courses. Undergraduate and graduate students, especially those from underrepresented groups, will be encouraged to participate in this research project.<br/>"
"0706870","Hierarchical models for Large Geostatistical Datasets with Applications to Forestry and Ecology","DMS","STATISTICS","06/15/2007","06/06/2007","Sudipto Banerjee","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","08/31/2010","$253,511.00","Andrew Finley","sudipto@ucla.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","This proposal lays down a comprehensive framework for carrying out statistical inference on point-referenced spatial data that are available from a large number of locations. Statistical theory is used to develop mathematically formal but computationally feasible methods that can have a broad range of applications. Hierarchical models implemented through Markov chain Monte Carlo (MCMC) methods have become especially popular for spatial modelling, given their flexibility and power to fit models that would be infeasible with classical methods as well their avoidance of possibly inappropriate asymptotics. However, fitting hierarchical spatial models often involves expensive matrix decompositions whose computational complexity increases in cubic order with the number of spatial locations, rendering such models infeasible for large spatial data sets. This computational burden is aggravated in multivariate settings with several spatially dependent response variables and also when data is collected at frequent time points and spatiotemporal process models are used. The investigators propose a class of models based upon a stochastic process that results from projecting the original process onto a lower-dimensional subspace. The investigators term these models as predictive process models and propose to explore their theoretical properties. The long-term goal of the PI is to develop a full suite of statistical methods that estimate spatial models in a wide variety of experiments in forestry and ecology. A recurrent underlying theme of the proposed methods that distinguishes it from existing methods is that the modeler does not need to sacrifice richness in modeling as a compromise for the large datasets. This resolves the statistical irony that large datasets are precisely where statistical estimates of rich association structures are permissible.  The emphasis is on models that can be executed even with moderately powerful computing tools and so would be accessible to a large number of researchers.<br/><br/>With the increasing popularity and availability of spatial referencing technologies such as Geographical Information Systems (GIS) and Global Positioning Systems (GPS) that can identify geographical coordinates with a simple hand-held device, scientists and researchers in a variety of disciplines today have access to large amounts of geocoded data. The broader impact of the proposed methods is best assessed by connecting the outcome of this research with the widely recognized impact of GIS on human society. From identifying spatial disparities in health standards to more precise weather predictions, GIS technology is used today in almost every sphere of society. By redeeming the investigators from using ad-hoc and qualitative methods that often bring out spurious stories, the proposed methods can have far reaching beneficial effects in environmental research that potentially touch unexpected corners of society. Consider a situation where an ecologist is unable to recognize critical symbiotic relationships between multiples species, due to inadequate models. Mathematical formalism, for all its complexities, minimizes such errors arising from qualitative techniques currently prevalent in forestry and ecological analysis. Such and several other scientific problems require formal spatial analysis, harnessing the full power of the information that large datasets carry. They include, but are not limited to, public and environmental health, meteorology, engineering, geosciences and so on, where the fundamental goal is the same: use new findings that will help improve human society.<br/><br/>"
"0706082","Statistical Analysis of Image Restoration and Its Applications in Magnetic Resonance Imaging","DMS","STATISTICS","07/01/2007","09/10/2008","Peihua Qiu","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","06/30/2011","$140,000.00","","pqiu@ufl.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00","<br/>Image analysis is an interdisciplinary research area with broad applications. One major focus of this project is on image deblurring when an observed image has both pointwise noise and spatial blur and when the blurring mechanism is unknown, which is often referred to as the blind image deblurring problem. In the statistical literature, little existing discussion can be found on image deblurring. In the applied mathematics and computer sciences literatures, existing image deblurring methods assume that the blurring mechanism, described by a point spread function, either is completely known, or follows a parametric model with one or more unknown parameters, or satisfies various conditions some of which are too restrictive to satisfy in applications. This project proposes alternative approaches to the blind image deblurring problem without imposing restrictive assumptions on the point spread function. The major idea is to estimate the point spread function from an observed test image or portions of an observed image with simple image structure, and then to restore true images using the estimated point spread function. If test images or portions of an observed image with simple image structure are not available, then the investigator suggests using the above idea locally, based on the observation that a true image can be well approximated by a simple structured one in a local neighborhood. Such local deblurring procedures allow the blurring mechanism to vary over location. In recent years, magnetic resonance imaging (MRI) is widely used for demonstrating pathological or other physiological alterations of living tissues. Due to movement of the imaged object and many other reasons, observed MRI images usually contain various contaminations, among which spatial blur and pointwise noise are most common. Right now, people usually use existing image deblurring techniques to deblur MRI images of individual 2-dimensional (2-D) slices of the imaged object slice by slice. This project suggests treating 2-D MRI images of different slices as profiles of a 3-D image and deblurring the 3-D image directly, after generalizing the proposed 2-D deblurring methods to 3-D cases, which would greatly improve the quality of the restored MRI images. This project also suggests using observed data in both frequency domain and spatial domain, and making use of their major strengths.<br/><br/>This project would have broader impacts on the society through its impacts on the progress of image deblurring techniques. Proposed deblurring procedures should greatly improve the quality of restored MRI images, which should help people better understand living tissues and neural activities of humans and other animals and diagnose various diseases more accurately. Besides MRI and functional MRI, image deblurring is used widely elsewhere. For instance, it is used in machine recognition of handwriting, including machine reading of postal addresses, bank checks, and so forth. It is also used in preprocessing satellite images and various other images in medical sciences, meteorology, oceanography, military, space communication, etc. Therefore, this project could have a positive impact in all these areas. This project would also contribute to the development of human resources in science and engineering through its educational activities. For instance, the investigator will offer an advanced topics course on image analysis, from which graduate students from various departments will get systematic training in all aspects of scientific research. A web page will be created at the end of this project to include all computer programs and research results so that other researchers can easily download and use them. A Ph.D. student of the investigator is currently writing his thesis on image processing. All these educational activities should have a great impact on the popularity and further development of the related research areas.<br/>"
"0645676","CAREER: Asymptotic Statistical Decision Theory and Its Applications","DMS","STATISTICS","05/01/2007","02/05/2013","Huibin Zhou","CT","Yale University","Continuing Grant","Gabor Szekely","10/31/2013","$400,000.00","","huibin.zhou@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","0000, 1045, 1187, OTHR","$0.00","Asymptotic equivalence, one of the most important statistical contributions of Lucien Le Cam, is a theory to build the connections among various statistical models. If two models are asymptotically equivalent, all asymptotically optimal statistical estimators can be carried over from one model to the other.  A basic principle of establishing asymptotic equivalence is to approximate a complicated statistical model by  a more tractable one. The Gaussian location model is a tractable model that captures the essence of a number of statistical settings.  The investigator studies explicit and practical procedures to convert a general nonparametric estimation to a Gaussian regression, using improved quantile coupling inequalities and new variance stabilization transformations. Other statistical problems are better understood by relating them to Poisson process models. The investigator studies infinitely divisible approximation to density estimation and its connection to nonparametric edge estimation and classification. The investigator is also proposed to study a long-standing issue in this area -- asymptotic equivalence theory for unbounded loss, and to study the asymptotic equivalence theory for multiple comparisons, functional data analysis and long memory models. <br/><br/>The project would help statisticians in many areas such as robust nonparametric estimation, machine learning, multiple comparison, functional data analysis, long memory models and generalized linear models, to understand and appreciate the simplification of Le Cam's theory and use it as a guidance to produce new theory and methodologies. The models the investigator is studying can be used in signal and image processing, calling data analysis, detection of bioweapons use, Genomic research, disease prevention, etc. The project will integrate research and education by teaching courses on decision theory, by organizing seminars and workshops to disseminate and preserve Le Cam's theory, and by advising graduate students working on this topic. The investigator will serve as the Diversity Coordinator for graduate student admissions in the Yale Statistics Department, and will seek to attract women and minorities to do research on the grant. <br/><br/>"
"0714817","Collaborative Research: A General Framework for High Throughput Biological Learning: Theory Development and Applications","DMS","STATISTICS","09/15/2007","08/24/2007","Hongyu Zhao","CT","Yale University","Standard Grant","Mary Ann Horn","08/31/2011","$119,999.00","Sherman Weissman","hongyu.zhao@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","0000, 4075, 7303, OTHR","$0.00","This application presents a comprehensive research plan for the investigation of a general framework and various new methods to handle complex large-scale data sets generated from biological (medical) as well as other scientific studies. Two goals are articulated in this proposal: theory development and application in biology and medicine. The former is focused on the study of a general yet core, model-free framework to effectively address major issues arising from high dimensional data. In the latter, the investigators seek to apply methods developed from the theory part to resolve machine learning type problems that arise in biology and medicine. In particular, this team intends to study the problems related to biological and medical prediction in response to treatments, clinical diagnosis of diseases (such as cancers), discovery of protein-protein interactions and biological network constructions related to disease etiology and motif identification. To achieve these two goals, the investigators will study theoretical and practical properties under a general setting and evaluate a series of novel statistical/computation procedures/software which will then be tested by a broad range of real and simulated data, some from current on-going studies.<br/><br/>The emergence of high dimensional data in most scientific fields poses new challenges for statisticians. Methods successful in dealing with low dimensional data are no longer effective for high dimensional data. One of the greatest difficulties in analyzing these data is to identify the informative variables/features and their associated clusters, and decipher the characteristics of the interaction between these variables and clusters. To meet current and future needs for digging hidden knowledge out of high dimensional data comprehensively and systematically, the scientific fields must develop new methods. The current project is a direct response to this need. Based on theoretical evidence (as preliminary results) already obtained in extracting low dimensional information, this team plans to apply and to develop various effective procedures to address practically important problems in the domains of biology and medicine. The investigators will study a novel screening process applicable across fields to demonstrate how high quality classifiers of low dimensionality can be identified while joint information among the influential variables are fully utilized. For further interpretation for biological validation/confirmation this team will study how to construct biological networks based on low dimensional classifiers and how to identify significant association patterns among them. A feedback mechanism will be established between the methodology development and biological validation teams, where statistical/computational results will be regularly discussed and biologically validated. It is anticipated that the key ideas and methods developed here will find numerous applications in disciplines other than biology/medicine. The proposed research is likely to advance substantial knowledge and significantly benefit current and future efforts in molecular biology/statistics/computational biology/disease prediction/drug discovery. The project would also provide valuable research experiences and training to undergraduates.<br/><br/>"
"0750451","Saddlepoint and Bootstrap Methods in Stochastic Systems and Related Fields","DMS","STATISTICS","05/31/2007","04/16/2009","Ronald Butler","TX","Southern Methodist University","Continuing Grant","Gabor Szekely","06/30/2011","$147,516.00","","rbutler@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1269","0000, OTHR","$0.00","Abstract <br/><br/>    The author proposes to develop a complete framework for implementing nonparametric statistical inference in stochastic systems. These stochastic systems are semi-Markov processes and include most of the commonly used stochastic models in reliability, multi-state survival analysis, epidemic modeling, and communication and manufacturing systems. Three tools are required to complete the framework: cofactor rules for transforms, saddlepoint approximations to invert the transforms, and the bootstrap to provide statistical inference in conjunction with the two previous tools. With any one of the three tools missing, statistical inference is no longer generally possible. Such a complete theory was the goal of the cybernetics movement during the late 40s to mid 70s which devoted a great deal of effort into developing a Laplace transform approach to such modelling. Ultimately this approach failed due to the difficulty of inverting the <br/>transforms involved, a task very successfully performed by using saddlepoint approximations. It also <br/>lacked a statistical theory of inference, a need that is filled admirably by the bootstrap. The work of this <br/>proposal takes a step towards achieving the ultimate aims of the cybernetics movement: to facilitate probability computations and nonparametric (bootstrap) inference for stochastic systems that cannot be easily achieved by other means. Bootstrap simulation for inference without saddlepoint assistance is beyond <br/>computational feasibility for systems of even modest size and complexity.<br/> <br/>This project proposes to develop a complete framework for implementing nonparametric statistical inference<br/>in complex stochastic systems. These stochastic systems include most of the commonly used stochastic models<br/>used in reliability, multi-state survival analysis, epidemic modelling, and communication and manufacturing<br/>systems. The proposal also addresses significant questions in other disciplines where answers are lacking due to certain computational difficulties. In population genetics, solutions are provided for statistical inference problems dealing with natural selection, mutation and genetic drift; in ocean and electrical engineering accurate approximations are given for distributions of wave crest heights in models used for sea surfaces and in signal processing; in biological models for the transmission of pain through the nervous system, methods are given to allow inferences about the underlying mechanisms that drive the fluctuating polarities of these ion channel models with the ultimate aim of helping to reveal the mechanisms that control pain sensation."
"0707082","RUI:     Nonstationary and High Dimensioanl Nonparametric Transfer Function Models Using Polynomial Splines","DMS","STATISTICS","06/01/2007","05/18/2007","Jun Liu","GA","Georgia Southern University Research and Service Foundation, Inc","Standard Grant","Gabor Szekely","09/30/2010","$105,000.00","","jliu@georgiasouthern.edu","261 FOREST DR","STATESBORO","GA","304586724","9124785465","MPS","1269","0000, 9229, OTHR","$0.00","This project focuses on modeling and forecasting nonlinear time series using nonparametric methods. Because of their data-driven nature, nonparametric methods are flexible therefore ideal for approximating nonlinear features whose functional forms are usually unknown a priori. In his earlier study, the investigator used local polynomial regression to model the nonlinear relationship between time series and assumed that the noise follows an Autoregressive-Moving Average (ARMA) process (Liu, 2005). This model is named nonparametric transfer function model. The nonparametric transfer function model assumes the noise to be strictly stationary, and the transfer function to be low-dimensional. In this project, the investigator intends to generalize the nonparametric transfer function models by relaxing these assumptions. However, generalizing the local polynomial-based model directly is difficult partly because of the added estimation complexity related to local polynomial estimators when the assumptions are dropped. Polynomial spline provides an alternative because it has an explicit functional form, which greatly simplifies the estimation. At the same time it retains the flexibility of the model. To relax the assumption of stationary noise, in this project the investigator studies a new estimator in which the transfer function is approximated with polynomial splines. This estimator allows the noise to follow an ARIMA process. To extend the nonparametric transfer function models to higher dimensions, the main difficulty is the ``curse of dimensionality''. The investigator plans to overcome this problem by using an additive model to approximate the multivariate transfer function. The additive components are modeled with polynomial splines. The polynomial spline-based estimator can also be used to model time-varying conditional variance in nonparametric transfer function model. The investigator plans to approximate the transfer function and the conditional variance function by polynomial splines. The proposed study includes the asymptotic behavior of the proposed estimators and related issues, such as tests for additivity/unit root, model selection, estimation and forecasting.<br/><br/>Forecasting and process control have been a constant interest in human society. They cannot be achieved without proper understanding of the underlying process. The proposed research extends the family of nonparametric transfer function models and provides new tools to explore complex relations in real-world, therefore enhances our abilities of forecasting and control. The proposed research adds new capabilities to the nonparametric transfer function models so they can be used to model high-dimensional transfer function, nonstationary time series, and time-varying conditional variance. As a result, the applicability of the nonparametric transfer function models is greatly enhanced. The proposed procedures are flexible enough for many nonlinear features encountered in practice, they are computationally efficient. The proposed methods can be applied in many areas, for example, it is used successfully in forecasting short-term electricity usage (Liu, Chen, Liu and Harris, 2006), which helps us use energy more efficiently and protect the environment. The proposed research contributes to nonlinear/nonparametric time series analysis. It further expands the application areas of the nonparametric transfer function models and provides useful tools to statisticians and researchers in many areas including ecology, environmental sciences, economics, finance, engineering and biology. For example, the proposed approach can be used to forecast volatility, which is an important issue in financial time series analysis.<br/>"
"0653736","Travel Support for the Directions in Statistical Computing (DSC) 2007 Meeting","DMS","STATISTICS","03/01/2007","02/27/2007","Luke-jon Tierney","IA","University of Iowa","Standard Grant","Gabor Szekely","02/28/2010","$10,000.00","","luke@stat.uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1269","0000, 7556, OTHR","$0.00","DSC 2007, an international workshop on Directions in Statistical Computing, will be held February 15-16, 2007 at the University of Auckland, New Zealand.  This is the fifth in a series of meetings; previous meeting were held in Vienna, Austria (1999, 2001, and 2003), and Seattle, WA (2005). The workshop will focus on, but is not limited to, open source statistical computing and aims to provide a platform for exchanging ideas about developments in statistical computing. This project is for funding exclusively to provide support to assist with travel expenses for junior investigators from US institutions to attend this meeting. Work of young researchers is often among the most novel, yet often these researchers lack the travel funds necessary to attend such a conference because they have not yet established themselves sufficiently to attract external travel and other funding for their work.<br/><br/>Statistics is playing an increasingly important role in interdisciplinary research, and statistical computing is essential as the means for making new methodology available to researchers and allowing the best statistical practices to be applied effectively. Scholarly conferences on statistical computing are an essential framework for the exchange of ideas and for encouraging the exploration of new directions in the design of statistical software. International conferences are particularly valuable as they allow the exchange of ideas among researchers from around the world who would not ordinarily come into direct contact.  The benefit to young researchers of a first exposure to the international research community is particularly important.  International meetings also provide an opportunity to meet with outstanding international graduate students and encourage them to consider opportunities for further study in the United States.<br/><br/>"
"0706818","Collaborative Research: Nonparametric Methods for Emerging Technologies in Bioinformatics","DMS","STATISTICS","09/01/2007","06/05/2009","Jianhua Hu","TX","University of Texas, M.D. Anderson Cancer Center","Continuing Grant","Gabor Szekely","08/31/2011","$169,997.00","","jhu@mdanderson.org","1515 HOLCOMBE BLVD","HOUSTON","TX","770304000","7137923220","MPS","1269","0000, OTHR","$0.00","The proposed research targets two important statistical problems in protein and gene expression microarray experiments: (I) quantification of the protein lysate arrays, an emerging technology for directly measuring protein contents of different lysed tissue samples simultaneously; (II) modeling probe level gene expression<br/>data, in particular, the exon tiling arrays to detect alternative splicing, which is an essential process resulting in much of the human diversity.  The investigators provide a statistical framework that allows for unknown regressor values in a nonparametric regression model, with applications to the quantification of protein lysate array data. The investigators also develop a quantile regression approach for mixed-effect models that are appropriate for detecting treatment and/or interaction effects without parametric distributional assumptions on the model. The investigators propose to make use of information across genes to enhance performance of the inferential methods in small sample problems. The new principles developed in the proposal are statistically interesting beyond their direct applications to gene and protein expression data.<br/><br/>Findings from the Human Genome Project highlight the intricacy of interactions between cell regulation,<br/>proteins and genes. It is generally understood that biological functions and biological activities are controlled by subsets of genes interacting with proteins in a highly controlled manner.  High throughput technologies such as microarrays are valuable for studying a large number of biological components simultaneously.  In particular, the protein lysate and exon tiling arrays have begun to show their important roles in cancer study and other biomedical research.  However, sound conclusions from these technologies depend on appropriate statistical analysis of the proteomic and genomic data. The statistical methods developed in the proposal are timely and important for proper quantification of the protein lysate arrays and for detecting alternative splicing through the exon tiling arrays.  The nonparametric approach proposed  is especially appealing due to its flexibility and adaptivity in modeling probe level gene expression data as well as protein lysate array data. <br/><br/>"
"0706048","Controlling Positive False Discovery Rate with Power","DMS","STATISTICS","06/01/2007","06/04/2007","Zhiyi Chi","CT","University of Connecticut","Standard Grant","Gabor Szekely","05/31/2011","$119,957.00","","zhiyi.chi@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1269","0000, OTHR","$0.00","This research proposal aims to develop theoretical and methodological tools to improve error rate control and power for multiple hypothesis testing. The centerpiece of the investigation is the False Discovery Rate (FDR) and its important relative, the so-called positive FDR (pFDR). First, in order to evaluate rejected nulls or discoveries more objectively, the PI will develop methods to estimate the pFDR for multiple testing as well as methods to set criteria for test statistics in order to make sure they have enough information to attain desired pFDR control. Second, the PI will develop FDR control based on multivariate statistics. Although using multivariate statistics for multiple testing is widely seen in many areas, there has been little work on this topic in the current literature on FDR control.  The PI will develop different FDR controlling procedures that incorporate multivariate statistics and investigate how to combine the information in the statistics effectively in order to achieve pFDR control with optimal power.<br/><br/>Multiple hypothesis testing provides a statistical foundation for massive data analysis and knowledge finding in a wide range of areas of modern science and technology, including neuroscience, brain imaging, genomics and imagery processing.  A fundamental challenge in these areas is to obtain true discoveries while avoiding false discoveries. The project will generate various tools to reach this goal.  It will enable researchers to evaluate discoveries more carefully and to avoid potential pitfalls in their data collection and analysis.  Moreover, it will provide researchers with a large collection of statistical methods to find true discoveries more efficiently.<br/><br/><br/>"
"0706732","Computer-intensive methods for  nonparametric time series analysis","DMS","STATISTICS","08/01/2007","07/24/2007","Dimitris Politis","CA","University of California-San Diego","Standard Grant","Gabor Szekely","07/31/2010","$140,001.00","","dpolitis@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","0000, OTHR","$0.00","The statistical analysis of time series and random fields is vital in many diverse scientific disciplines. This project continues the development of  computer-intensive statistical methods of inference for the analysis of dependent data without relying on unrealistic or unverifiable model assumptions. In particular: (a) General  estimators are constructed based on nested subsample values of converging/diverging statistics with general applications including tail index and rate estimation. (b) Limit theorems are proven for the distribution of self-normalized statistics from marked point processes with (possibly) heavy tails; it is shown how subsampling can be used for inference purposes without explicit knowledge and/or estimation of the heavy tail index. (c) It is demonstrated that the use of special `flat-top' kernels is advised both in the context of residual bootstrap, as well as in the problems of functional estimation in  nonparametric  autoregression, estimation of conditional moments, and spectral density and large-sample covariance matrix estimation. (d) Two different block  bootstrap schemes, one based on a local blocking technique and the other on residuals, are devised to address data from locally (but not globally) stationary series. (e) The way to conduct a most powerful bootstrap hypothesis test in linear/nonlinear (auto)regression set-ups is identified and powerful bootstrap unit root tests are devised as a result.<br/><br/>This project falls in the realm of nonparametric statistics where inferences (estimation, confidence intervals, hypothesis tests, etc.) are carried out without relying on ad hoc model assumptions. In some sense, the nonparametric viewpoint allows the data to ``speak for itself'', and is particularly appropriate in a `large-sample' situation where data are abundant; in our information-explosion age, this is progressively a typical situation. For example, in a daily series of exchange rates or stock returns spanning a decade, or a series of (average) annual temperatures over the last 100 years, there may be evidence that the stochastic structure of the series has not remained invariant over such a long stretch of time. Part of this project deals with devising appropriate computer-intensive methods for inference in such nonstationary environments (e.g., trend detection and estimation) that would be most helpful in economic applications as well as the problem of climate change.<br/>As another example, consider meteorological data gathered from weather stations scattered all around the country; since the spatial locations of the measurements are highly irregular, this type of data constitutes a so-called  `marked point process'. The work under this project provides powerful  methodology for the analysis of data under such practically important and difficult settings.<br/>"
"0706348","Collaborative Research: Penalized Methods for Variable Selection and Estimation in High-Dimensional Models","DMS","STATISTICS","08/15/2007","08/03/2007","Joel Horowitz","IL","Northwestern University","Standard Grant","Gabor Szekely","07/31/2008","$45,000.00","","joel-horowitz@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1269","0000, OTHR","$0.00","This project studies penalized methods for variable selection and estimation in high-dimensional models. A general approach for fitting high-dimensional models is to use regularization penalties. Several important penalized methods for variable selection and estimation have been proposed, but the properties of these methods have not been systematically studied. To apply the methods in scientific investigations, it is important to understand their properties. In particular, it is important to know under what conditions, the methods correctly select the important variables and estimate their effects in an efficient way. Standard methods for evaluating a statistical procedure assume that the number of variables in a model is fixed and much smaller than the sample size. This formulation is not applicable to high-dimensional models. The problem of analyzing high-dimensional models presents novel and challenging theoretical questions in mathematical statistics. Current variable selection methods using penalties assume a known form of the statistical model, which can be a misrepresentation of the reality. It is important to investigate what happens if a parametric model is misspecified or if no parametric assumptions are made about the model. In particular, it is important to know whether there are conditions under which penalized methods select variables correctly despite misspecification and under what conditions misspecification causes them to yield misleading results. It is also important to extend the penalized methods to nonparametric and semiparametric models.<br/><br/>High-dimensional data arise in many important applications, notably biological and biomedical investigations. With rapid advances in biotechnology, more and more large data sets are being generated. The identification of statistically and biologically significant patterns from high-dimensional and noisy data sets is a major challenge. The investigators apply the proposed research to genome-wide association (GWA) analysis, detection of copy number variation (CNV), and analysis of censored survival data with gene expression profiles. GWA analysis and detection of CNV enable the identification of genes and pathways responsible for the development and progression of a disease, such as many forms of cancer. Correlating a gene expression profiles with survival is useful, because survival is perhaps the most important clinical endpoint in many cancer studies. The development of statistical methods that can deal with high-dimensional problems in estimating the relationship between clinical outcomes and genetic and genomic data contribute to better understanding of the genetic basis of diseases, better diagnoses, and better survival prediction.<br/><br/>"
"0706108","Collaborative Research: Penalized Methods for Variable Selection and Estimation in High-Dimensional Models","DMS","STATISTICS","08/15/2007","08/03/2007","Jian Huang","IA","University of Iowa","Standard Grant","Gabor Szekely","07/31/2008","$45,000.00","","jian-huang@uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1269","0000, OTHR","$0.00","This project studies penalized methods for variable selection and estimation in high-dimensional models. A general approach for fitting high-dimensional models is to use regularization penalties. Several important penalized methods for variable selection and estimation have been proposed, but the properties of these methods have not been systematically studied. To apply the methods in scientific investigations, it is important to understand their properties. In particular, it is important to know under what conditions, the methods correctly select the important variables and estimate their effects in an efficient way. Standard methods for evaluating a statistical procedure assume that the number of variables in a model is fixed and much smaller than the sample size. This formulation is not applicable to high-dimensional models. The problem of analyzing high-dimensional models presents novel and challenging theoretical questions in mathematical statistics. Current variable selection methods using penalties assume a known form of the statistical model, which can be a misrepresentation of the reality. It is important to investigate what happens if a parametric model is misspecified or if no parametric assumptions are made about the model. In particular, it is important to know whether there are conditions under which penalized methods select variables correctly despite misspecification and under what conditions misspecification causes them to yield misleading results. It is also important to extend the penalized methods to nonparametric and semiparametric models.<br/><br/>High-dimensional data arise in many important applications, notably biological and biomedical investigations. With rapid advances in biotechnology, more and more large data sets are being generated. The identification of statistically and biologically significant patterns from high-dimensional and noisy data sets is a major challenge. The investigators apply the proposed research to genome-wide association (GWA) analysis, detection of copy number variation (CNV), and analysis of censored survival data with gene expression profiles. GWA analysis and detection of CNV enable the identification of genes and pathways responsible for the development and progression of a disease, such as many forms of cancer. Correlating a gene expression profiles with survival is useful, because survival is perhaps the most important clinical endpoint in many cancer studies. The development of statistical methods that can deal with high-dimensional problems in estimating the relationship between clinical outcomes and genetic and genomic data contribute to better understanding of the genetic basis of diseases, better diagnoses, and better survival prediction.<br/><br/>"
"0706885","A Theoretical Foundation for Applications of Bayesian Variable Selection","DMS","STATISTICS","09/01/2007","08/28/2007","Wenxin Jiang","IL","Northwestern University","Standard Grant","Gabor Szekely","08/31/2008","$38,087.00","Martin Tanner","wjiang@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1269","0000, OTHR","$0.00","In the popular approach of `Bayesian Variable Selection' (BVS), one uses prior and posterior distributions to select a subset of candidate variables to enter the model. In some examples, the number of candidate variables `K' can be much larger than the number of study units `n'. This idea has been applied to various statistical models, e.g., regression, graphical models, survival analysis, and cluster analysis. Despite its popularity, theoretical properties, especially frequentist  convergence properties, have not been well established. Recently, the investigators have successfully studied the frequentist convergence properties (consistency, convergence rates, and predictive performances) of BVS for generalized linear models. A completely new direction is considered in this project to study BVS with a Gibbs posterior originating in statistical mechanics. In contrast to the usual Bayesian prior which is constructed from a likelihood function, the Gibbs posterior is constructed from a risk function of practical interest (such as the classification error) and aims at minimizing a risk function without modeling the data probablistically. This can improve the performance over the usual Bayesian approach, since the usual Bayesian approach depends on a probability model which may be misspecified. The investigators studies the statistical performance of BVS with a Gibbs posterior constructed for the purpose of classification. Conditions are provided so that BVS will achieve good classification performance, even in the presence of high dimensionality (K>>n).<br/> <br/>BVS has multi-disciplinary applications that include various practices of data mining, where a few important variables are to be selected from many candidates and used for prediction and decision making, e.g., pattern recognition, fraud detection, homeland security, customer-oriented marketing decisions, machine learning, microarray analysis, and bioinformatics. The applications typicially involve many candidate variables (sometimes much more than the sample size). BVS, through selecting a few important variables, can be very helpful for interpretation, prediction, and decision making in each of these applications, despite the potentially high dimensionality. The current project will provide a theoretical framework and conditions under which BVS with a Gibbs posterior will be nearly optimal in some sense, despite high dimensionality, therefore providing theoretical justification for this important technique. A solid theoretical foundation for BVS will also lead to better interpretations of the results obtained from BVS, and provide useful information for practitioners on specification of the prior distribution. Such a good theoretical understanding will likely lead to improvement of empirical performance under many circumstances."
"0713945","Carnegie Mellon Summer School in Logic and Formal Epistemology","DMS","ALGEBRA,NUMBER THEORY,AND COM, STATISTICS","06/01/2007","05/18/2007","Jeremy Avigad","PA","Carnegie-Mellon University","Standard Grant","Tomek Bartoszynski","05/31/2008","$24,024.00","Teddy Seidenfeld","avigad@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1264, 1269","0000, 7556, OTHR","$0.00","There is a long tradition of fruitful interaction between philosophy and the sciences. Logic and statistics emerged, historically, from combined philosophical and scientific inquiry into the nature of mathematical and scientific inference; and the modern conceptions of psychology, linguistics, and computer science are the results of sustained reflection on the nature of mind, language, and computation. In today's climate of disciplinary specialization, however, foundational reflection is becoming increasingly rare. As a result, developments in the sciences are often conceptually ill-founded, and philosophical debates lack scientific substance.<br/><br/>In 2007, the Department of Philosophy at Carnegie Mellon University will hold a three-week summer school in logic and formal epistemology for promising undergraduates in philosophy, mathematics, computer science, linguistics, and other sciences. The goals are to introduce promising students to cross-disciplinary fields of research at an early stage in their career, and forge lasting interdisciplinary links between the various disciplines. This year's lectures will address causal statistical inference, formal verification, decision theory, and game theory.<br/>"
"0707069","Multi-resolution lattice models and theory for spatial process estimators","DMS","STATISTICS","08/01/2007","04/26/2013","Stephan Sain","CO","University Corporation For Atmospheric Res","Continuing Grant","Gabor Szekely","07/31/2014","$227,844.00","Douglas Nychka","ssain@ucar.edu","3090 CENTER GREEN DR","BOULDER","CO","803012252","3034971000","MPS","1269","0000, OTHR","$0.00","Estimating a smooth function from noisy observations is a core problem in mathematical statistics and supports the areas of nonparametric regression and spatial data analysis. However, there are still gaps in our knowledge of the statistical properties of methods such as smoothing splines and geostatistical estimators (Kriging), and there is also limited understanding of estimators that adapt to heterogeneous structure in the function. In this proposal, the investigators address some of the issues in nonparametric function estimation through a spatial statistics framework.  The approach is based on a new model for nonstationary covariance functions that combines a multiresolution (or wavelet) basis with a multivariate lattice model. This multiresolution lattice (MRL) model builds off previous work on lattice models for spatial fields and the use of multiresolution bases for representing nonstationary covariance functions.  The key innovation is that the lattice model describes dependence on the coefficients of the basis, not the spatial field. The multivariate extension allows for connections of basis coefficients between different scales, and the localization of the basis functions in space facilitates modeling nonstationary covariance. An important component is the extension of large sample statistical theory to analyze spatial estimators applied to irregular locations and with nonstationary covariance. Ultimately, in addition to methodological and practical advances, this research seeks to break new ground in the theoretical understanding of how nonparametric and spatial smoothers behave, and, in effect, unifying a broad area of mathematical statistics.<br/><br/>The interpretation of spatial observations or fields is a fundamental data analysis problem that is ubiquitous in the geosciences.  A specific example is the study of regional climate change where complex numerical models are coupled to simulate climate at local scales.  These numerical models are a primary tool to quantify specific impacts of climate change at a scale that can be understood by the general public. The fields produced by these simulations have a great deal of large-scale structure, are noisy, and often exhibit heteroscedastic and nonstationary behavior. Drawing inferences about these fields to provide, for example, a probabilistic assessment of the projected climate change, requires a deliberate statistical approach. The research outlined in this proposal seeks to expand the tools available to analyze geophysical data, in particular the complex outputs of regional climate models.<br/><br/>"
"0705007","Flexible and Adaptive Statistical Modeling","DMS","STATISTICS","08/01/2007","05/06/2009","Robert Tibshirani","CA","Stanford University","Continuing Grant","Gabor Szekely","07/31/2012","$345,043.00","","tibs@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","The investigator studies and develops computer-intensive methods for applied statistics, with applicatiion in biology. His current proposal has three projects a) complementary clustering, a kind of orthogonal decomposition analogous to principal compoennts, b) the fused lasso for spatial smoothing and hot-spot detection, c) pre-validation- a method for inference for the p>N setting and d) The new edition of the text ""The Elements of Statistical Learning""<br/><br/>There have been significant developments in the areas of applied regression and classification over the past 10-15 years. Much of the impetus originally came from outside of the field of statistics, from areas such as computer science, machine learning and neural networks. As a result, we now have at our disposal a very powerful collection of techniques for adaptive regression and classification. These are now being applied to medical diagnosis, bioinformatics and genetic modeling, chemical process control, shape, handwriting, speech and face recognition, financial modeling, and a wide range of other important practical problems. In this work the investigator plans to develop and study new tools for the important practical problems.<br/><br/>"
"0707085","New Methodology for Multiple Testing and Simultaneous Inference","DMS","STATISTICS","07/01/2007","05/08/2009","Joseph Romano","CA","Stanford University","Continuing Grant","Gabor Szekely","12/31/2011","$262,402.00","","romano@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","The investigator develops new methods and   theory for problems in multiple testing and simultaneous inference.  A classical approach to dealing with multiplicity is to require that decision rules control the familywise error rate.  But, when the number of tests is large, this measure is so stringent that alternative hypotheses have little chance of being detected.  Thus, alternative measures of error control are studied both in finite sample and asymptotically. Such measures include: the false discovery rate; the probability of k or more false rejections; tail probabilities of the false discovery proportion.  In order to develop methods which do not rely on unrealistic or unverifiable model assumptions, the investigator makes extensive use of computer-intensive methods.  The power of resampling is that the joint dependence structure of the individual tests can be captured so that methods need not be overly conservative.  The pursuit of such methodology is investigated from theoretical, computational and theoretical points of view, with special emphasis on a large number of tests.<br/><br/>The goal of this research is to develop new theory and methods for problems of multiple inference.  Virtually any scientific experiment sets out to answer questions about the process under investigation, which often can be translated formally into a set of hypotheses to be tested.  It is the exception that only a single hypothesis or question is under study.  In the ""information age"", the statistician is faced with the challenge of accounting for all possible errors resulting from a complex data analysis, so that any interesting conclusions can reliably be viewed as real structure rather than the result of ""data snooping"", i.e. finding artifacts of random data.  For example, current methods in biotechnology and genomics generate DNA microarray experiments, where gene expression level  in cells for thousands of genes are analyzed simultaneously on a gene by gene basis.  The goal then is to devise new techniques that are not based on strong assumptions that effectively deal with problems of multiplicity in the face of vast amounts of data.  The resulting inferential tools can be applied to such diverse fields as genetics, econometrics, finance, brain imaging, clinical trials, education and astronomy.<br/>"
"0702256","Travel Support for the 56th Session of the International Statistical Institute","DMS","STATISTICS","04/01/2007","09/17/2007","William Smith","VA","American Statistical Association","Standard Grant","Gabor Szekely","03/31/2008","$13,000.00","","bill@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","0000, 7556, OTHR","$0.00","<br/>The American Statistical Association (ASA) will use this NSF award as a travel grant for United States participants to attend the 56th Session of the International Statistical Institute (ISI) in Lisboa, Portugal from August 22-29, 2007.  The ISI meeting includes the meetings of the Bernoulli Society, International Association for Official Statistics (IAOS), International Association for Statistical Computing (IASC), International Association of Survey Statisticians (IASS), and the International Association for Statistical Education (IASE).  Thus it is an umbrella meeting with a wide range sessions with applications of interest for thousands of statisticians.  The travel grants will provide partial support to defray transportation costs for individuals selected from institutions and non-profit associations. An emphasis of the award is to encourage and provide the opportunity for younger statisticians, as well as statisticians from disadvantaged groups, to participate in the meeting.<br/><br/>Participants will be notified of the availability of the travel grant through Amstat News, a membership publication of the ASA, and on the ASA Home Page on the Internet. Notices will be provided to university and college departments to encourage younger statisticians to apply for the travel grant. A review and selection committee will be established to review the applications and select grantees.  The committee will be comprised of four or more ASA members and will convene at the ASA office in Alexandria, Virginia.  Special consideration will be given to statisticians who have recently received their Ph.D.'s and to women and minorities.<br/><br/><br/><br/>"
"0706723","Collaborative Research: The Analysis of Time Series Collected in Experimental Designs","DMS","STATISTICS","07/01/2007","05/24/2007","David Stoffer","PA","University of Pittsburgh","Standard Grant","Gabor Szekely","06/30/2008","$46,062.00","","stoffer@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","0000, OTHR","$0.00","The research project has three basic goals. The first goal is to estimate the second order properties of a nonstationary process using data from multiple units. A wavelet-based functional model with stochastic subject-specific representation is proposed. A computationally efficient algorithm for estimation will be developed along with theoretical optimal properties of the estimators. While this project is focused on the frequency or spectral domain, the second project will develop methods in the time domain. The goal of this project is to accommodate structural breaks within the time series using mixtures of state-space models. These mixtures are characterized by covariates, treatment and subject-specific effects in both the mixture components and mixing weights. The third project focuses on both temporal and spatial components. In particular, this project integrates spatial information by developing models and corresponding estimators to accommodate data structures in both time and space. The goal is to assess the differences across groups and the subject-specific and covariate effects on the dynamic structure.<br/><br/>The research focuses on an area that has been, for the most part, neglected by the statistics community.  In particular, the question to be addressed is how to best analyze data obtained from experiments when the data are taken sequentially in time and/or space.  Typical examples include experiments where EEGs are taken on various subjects with different diagnoses, or functional magnetic resonance imaging (fMRI) experiments, where subjects with different conditions are requested to perform tasks while in a magnet. The difficulty in analyzing such data is that they are often irregular (nonhomogeneous or nonstationary) in nature.  The techniques for analyzing regular time series are well established, but new techniques are needed to account for the irregularities over time and/or space.  The fact that these data are collected in complex experiments adds another layer of difficulty onto the analysis.<br/>"
"0706880","Collaborative Research:  Integral Transform Methods for Sufficient Dimension Reduction in Regression","DMS","STATISTICS","08/15/2007","08/10/2007","Peng Zeng","AL","Auburn University","Standard Grant","Gabor Szekely","07/31/2010","$49,997.00","","zengpen@auburn.edu","321-A INGRAM HALL","AUBURN","AL","368490001","3348444438","MPS","1269","0000, OTHR","$0.00","This project is aimed to develop theory and methods for sufficient dimension reduction in regression analysis involving a large number of predictor variables. The investigators propose a general approach called the integral transform approach to facilitating dimension reduction. The key idea of this approach is to use integral transform and response transformation to change the domain where dimension reduction is <br/>performed. Due to the availability of a wide range of transformations and integral transforms, this approach leads to a flexible and effective framework for addressing and resolving challenges raised by high dimensionality. Through a series of well-defined research problems, the investigators study this framework and develop specific dimension reduction methods for many important regression applications. The success of this project not only provides effective practical tools for high-dimensional data analysis but also represents an advance in the theory and methodology of semiparametric inference.<br/><br/>High-dimensional data that involve a large amount of variables are nowadays routinely generated and collected in areas such as scientific research, government, business, etc. It is well-known that high dimensionality causes difficulties in processing and analyzing these data. This is commonly referred to as the curse of dimensionality. There is an urgent demand of statistical tools that are able to mitigate the curse of dimensionality through dimension reduction. This project represents an answer to this demand and is particularly aimed at achieving dimension reduction in regression. The results from this project can be widely applied in areas where regression involving a large number of variables is required. Gene expression and protein sequence data analysis is one such example. Therefore, this project can help enhance scientific research and discovery and benefit a variety of social and economical activities."
"0707013","Collaborative Research: Optimal Design of Experiments for Categorical Data","DMS","STATISTICS","06/01/2007","03/24/2009","Min Yang","MO","University of Missouri-Columbia","Continuing Grant","Gabor Szekely","05/31/2011","$144,345.00","","myang2@uic.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269","0000, OTHR","$0.00","<br/>The investigators develop methods for identifying optimal and efficient designs for experiments with categorical data. The project consists of three main parts. (i) Identification of optimal designs for binary data under generalized linear regression models. This part includes consideration of models in which slope and intercept parameters can vary for different groups of subjects and models with a random subject effect. (ii) Identification of optimal allocations of treatments to blocks for comparative studies with binary data. A logistic model is a popular choice for such studies. (iii) Identification of optimal designs for count data under loglinear regression models. In this setting, the investigators focus also on optimal designs for models that can account for subject heterogeneity. This project is innovative in that it uses a new technique that has vast advantages over the commonly used geometric approach.<br/><br/> <br/>Categorical responses are very common in designed experiments in many scientific studies, such as drug discovery, clinical trials, social sciences, marketing, etc. Generalized Linear Models (GLMs) are widely used for modeling such data. Using efficient designs for collecting data in such experiments is critically important. It can reduce the sample size needed for achieving a specified precision, thereby reducing the cost, or improve the precision of estimates for a specified sample size. While research on optimal designs for linear models has been systematically developed over more than 30 years, there are very few research publications on optimal designs for GLMs. This project is important both for the introduction of novel theoretical tools and for its impact on applications. For example, the results of the project significantly reduce the time, money, and the number of patients needed in clinical trials, as well as other scientific studies. The results can help the U.S. Food and Drug Administration to improve its guidelines for clinical trials.<br/><br/>"
"0706829","Sparsity oracle inequalities via l_1 regularization in nonparametric models","DMS","STATISTICS","06/01/2007","03/24/2009","Marten Wegkamp","FL","Florida State University","Continuing Grant","Gabor Szekely","05/31/2010","$243,144.00","Florentina Bunea","marten.wegkamp@cornell.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","0000, OTHR","$0.00","This research effort characterizes sparse model recovery for general model classes, extending existing results for generalized linear models and in classification.  The central goals of this proposal are: (a) to define sparsity and target recovery in high dimensional - low sample size settings; (b) to show that empirical risk minimization with a lasso-type penalty allows for target recovery, under minimal assumptions. The investigators advance the use of a novel type of oracle inequalities to show that the penalized empirical risk minimizers adapt to the unknown sparsity of the underlying statistical model. Special attention is given to random design regression and classification with a reject option.<br/><br/>High dimensional data are increasingly common in many scientific disciplines such as biological and medical sciences.  Accurate estimation and  implementation of  complex statistical models used for the analysis of such data are challenging.  The aim of this project is to develop a unified theory for the analysis of computationally efficient procedures in high dimensional data settings. The usefulness of these techniques will be demonstrated by applications to gene expression data and  concurrent EEG / fMRI data. The newly introduced classification procedures have a built-in reject option that allows for withholding decision in cases that are hard to classify. This greatly improves the performance of tumor classification where the consequences of misdiagnosis are severe. The software for these procedures will be made freely available on the world wide web.<br/>"
"0705037","Spike and Slab Models: Theory and Applications","DMS","STATISTICS","08/01/2007","12/11/2007","Hemant Ishwaran","OH","Cleveland Clinic Foundation","Standard Grant","Gabor Szekely","07/31/2011","$159,995.00","","HIshwaran@med.miami.edu","9500 EUCLID AVE","CLEVELAND","OH","441950001","2164456440","MPS","1269","0000, OTHR","$0.00","The investigator seeks to expand the theory and application for rescaled spike and slab models, a class of Bayesian models, to address the general problem of variable selection and prediction.  This will be accomplished in three distinct aims: (1) By developing theory as well as fast computational algorithms for non-orthogonal designs making using of  spike and slab orthogonalization.  The resulting predictor, a bagged ensemble derived using generalized ridge regression, will be shown to possess state of the art predictiveness, when one factors in interpretation over black-box prediction.  Theory, in the form of finite sample arguments, will show this is due to selective shrinkage, a property whereby only truly zero coefficients are shrunk towards zero; (2) By developing general methodology for hard thresholding estimated regression coefficients; (3) By extending the rescaled spike and slab framework to include non-linear models such as generalized linear models and non-proportional survival regression models with time dependent predictors.<br/> <br/><br/>Intellectually, this research will enhance our understanding of model building and outcome prediction, especially in ill-determined settings when the sample size is on the order of, or dominated by, the number of predictors (variables).  This type of setting is becoming all too common in scientific settings.  Among applications considered will be colon cancer genomics, an important public health problem.  Currently, colorectal cancer is the second leading cause of cancer mortality in the adult American population, accounting for 140,000 new cases annually and 60,000 deaths.  Although widely used, it is known that the current classification scheme is highly imperfect in reflecting the actual underlying molecular determinants of colon cancer behavior. For instance, upwards of 20% of patients whose cancers metastasize to the liver are not given life saving adjuvant chemotherapy based on the current clinical staging system.  Thus, there is an important need for the identification of a molecular signature that will identify tumors that metastasize.  Another area of application will be long-term prediction models for predicting outcomes following coronary artery bypass surgery, a widely used surgical modality for patients with obstructive coronary artery disease.  Current long-term prediction models have serious limitations which have hindered our understanding.  Yet another application will be in understanding survival behavior of heart and lung transplant recipients and the role viruses play in potential dysfunction of the transplanted organs. Methodology will be complemented by development of software for fast computational solutions in high dimensional settings.<br/>"
"0704337","High-dimensional statistical learning and inference","DMS","STATISTICS","06/15/2007","05/09/2011","Jianqing Fan","NJ","Princeton University","Continuing Grant","Gabor Szekely","05/31/2014","$920,001.00","","jqfan@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","0000, OTHR","$0.00","The challenge of high-dimensionality characterizes many contemporary statistical problems arising from<br/>many frontiers of scientific research and technological development. In high-dimensional statistical research, low-dimensional structures, which entail sparsity under suitable parametrization, are needed to be explored in order to circumvent the issue of noise accumulation with dimensionality. This proposal intends to confront a number of important high-dimensional statistical problems from genomics, machine learning, health studies, economics, and finance. These include various emerging issues from the analysis of microarray data such as normalization, significance analysis, and disease classification; variable selection and feature extraction from high-dimensional statistical learning; sparse classification and clustering from high-dimensional feature spaces; high-dimensional covariance matrix estimation for asset allocation and portfolio management; sparse covariance estimation for spatial and temporal studies and genetic networks. All of these problems have their distinguished characters from the context of their applications, but nevertheless share similar challenges with high dimensionality and admit features of sparsity. These emerging problems of high societal impacts will be confronted via developing new statistical methods to address the features and challenges associated with high-dimensionality, from statistical computation, feature selection, to noise reduction. At the same time, the PI also intends to provide fundamental understanding, via asymptotic analysis and simulation studies, to these problems and their associated methodologies that push theory, methods, and computation forward.<br/><br/>Thanks to technological innovation, the availability of large-scale and complex data are widely available nowadays in many contemporary scientific problems. High-dimensional statistical models are required to address these scientific endeavors. The challenges of high-dimensionality arise from diverse fields of sciences and the humanities, ranging from genomics and health sciences to economics and finance. In these fields, variable selection, feature extraction, sparsity explorations are crucial for knowledge discovery. In this proposal, we propose to develop cutting-edge statistical theory and methods to address these problems from genomic studies, machine learning, health science, economics, and finance.  The proposed techniques and results will not only help researchers to solve emerging problems in their disciplines, but also have strong impact on statistical thinking, methodological development, and theoretical studies.<br/><br/>"
"0751568","Workshop on: Discovery in Complex or Massive Datasets:  Common Statistical Themes","DMS","STATISTICS","09/01/2007","08/24/2007","Jianqing Fan","NJ","Princeton University","Standard Grant","Gabor Szekely","08/31/2008","$45,530.00","","jqfan@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269","0000, 7556, OTHR","$0.00","The amount and complexity of data generated to support contemporary science continues to grow rapidly. In domains from genomics to climate science, statisticians are actively engaged in interdisciplinary research teams. The history of statistics shows that, while frequently arising in response to challenges in specific scientific domains such as these, statistical methods and theory often achieve their full range and power from later application to subjects far remote from those of origin. The study of the ``intersections'' -- statistical problems, theories and methods that are relevant to multiple domains of scientific enquiry -- thus offers great opportunity in today's Age of Information.<br/><br/>The proposed workshop responds to this opportunity: it aims to enumerate today's most intellectually compelling statistical challenges arising out of these intersections, in the hope of stimulating future research advances that will extend and enhance our data analytic toolkit for scientific discovery. The proceedings and discussions of the workshop will be distilled into a 10-15 page report, which will be disseminated widely by NSF through its website and other usual channels. This workshop and report respond to a number of compelling national needs: a) enhancing the nation's ``methodologic infrastructure'' for research, by advancing the capability of statistical theories and methods to contribute to discovery from massive and/or complex datasets, b) through promotion of research on statistical intersections, to advance the pre- and post- doctoral level training of research statisticians who will be critically needed core faculty members in expanding graduate and undergraduate degree programs in statistics in the U.S., c) fostering the flow of ideas between disciplines, as analysis methods developed in one domain are transferred in other areas.<br/><br/><br/>"
"0725407","A NISS/ASA Writing Workshop for New Researchers","DMS","STATISTICS","08/15/2007","08/03/2007","Nell Sedransk","VA","American Statistical Association","Standard Grant","Gabor Szekely","07/31/2008","$19,965.00","Martha Aliaga","sedransk@niss.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","0000, 7556, OTHR","$0.00","This proposal will support a workshop on effective technical writing for new researchers in the statistical sciences who seek to publish their research or to present their research plans in the form of grant proposals for federal funding.  Researchers, especially new researchers, often have difficulty disseminating their research results not because of the quality of the research but rather because of inappropriate choices of publication venues for the particular research and/or because of poor presentation of technical material to the chosen audience.<br/><br/>This workshop will open with tutorial sessions on the organization of material for a technical article or grant application, on technical writing techniques and on the specific missions and audiences of key journals in the statistical sciences.  Then each participating new researcher will work individually with an experienced journal editor as mentor to address these issues on an individualized basis for draft of the new researcher's work in progress.  Revisions following this guidance will be critiqued by the mentor to assure that the new researcher's implementation of writing techniques has been successful before the article or the grant proposal is submitted for review.  <br/>"
"0711142","Symposium on Case Studies in Bayesian Statistics","DMS","STATISTICS","07/01/2007","03/16/2007","Joseph Kadane","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","06/30/2008","$14,960.00","","kadane@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, 7556, OTHR","$0.00","Case Studies in Bayesian Statistics IX is the ninth workshop in the series that was begun in 1991. The workshops are held in odd years at Carnegie Mellon University in early fall. The ninth workshop is planned for October 19-20, 2007. The highest level goal of the workshop series is to advance statistical practice by examining Bayesian methods in specific applied contexts. The objectives of the workshop are to explore the interplay of statistical theory and practice in the context of substantive scientific research; promote the continued development of Bayesian statistics by highlighting problems in the sciences that require non-standard approaches; provide an opportunity for scientists and statisticians to present their work in depth, highlighting both the scientific background and the analytical approaches; and encourage dissemination of the findings presented at the workshop via well documented and peer reviewed case studies. As it has evolved, this workshop series has become an important meeting for younger researchers in Bayesian statistics. The workshop aims to encourage young researchers, including graduate students, to present their applied work; provide a small meeting atmosphere to facilitate the interaction of young researchers with senior colleagues; expose young researchers to important challenges and opportunities in collaborative research; and include as participants women, under-represented minorities and persons with disabilities who might benefit from the small workshop environment. In addition to our traditional poster session, our workshops include a session devoted to presentations by younger researchers. The conference will also run a short course at the beginning of the workshop. The topic chosen for this year is sequential Markov Chain Monte Carlo methods.<br/><br/>Case Studies in Bayesian Statistics IX is the ninth workshop in the series that was begun in 1991. The workshops are held in odd years at Carnegie Mellon University in early fall. The ninth workshop is planned for October 19-20, 2007. The highest level goal of the workshop series is to advance statistical practice by examining Bayesian methods in specific applied contexts. The objectives of the workshop are to explore the interplay of statistical theory and practice in the context of substantive scientific research; promote the continued development of Bayesian statistics by highlighting problems in the sciences that require non-standard approaches; provide an opportunity for scientists and statisticians to present their work in depth, highlighting both the scientific background and the analytical approaches; and encourage dissemination of the findings presented at the workshop via well documented and peer reviewed case studies. Most statistics meetings allow a very limited time (like 20 minutes) for the presentation of a paper. While this is OK for studies that depart only slightly from what is already known and understood, it is a major expositional problem for more ambitious studies. Bayesian statistics, a recent movement in statistics, requires exploration of the scientific background of a study, the statistical modeling and computation, and the conclusions, with all the necessary caveats. Accordingly, the Case Studies in Bayesian Statistics workshops are organized to give researchers and discussants ample time (3 hours) to present and discuss each of two serious Bayesian empirical papers. There is also a special session for young researchers to present their work, a poster session, and a short course (which this year is on sequential Markov Chain Monte Carlo methods, an important new Bayesian computing technique). The conference supports younger researchers, women and under-served minorities to attend the meeting.<br/>"
"0707060","Statistical Methods for Prediction of Individual Sequences","DMS","STATISTICS","07/01/2007","05/08/2008","Peter Bartlett","CA","University of California-Berkeley","Continuing Grant","Gabor Szekely","06/30/2011","$237,237.00","","bartlett@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","In many prediction problems that arise, for example, in computer security and computational finance, the process generating the data is best modeled as an adversary with whom the predictor competes. The broad goal of this research project is the analysis and design of statistical methods for complex prediction problems in an adversarial setting: a prediction strategy must predict any individual sequence almost as accurately as the best strategy in some comparison class.  The research focuses on statistical methods, which are based on probabilistic models of the data, since (a) these methods are commonly used in practice; (b) there is evidence that these methods perform well in adversarial settings; (c) there are good prospects of exploiting the computationally efficient approaches that have been developed for probabilistic settings, which is especially important for high-dimensional problems; and (d) positive results in adversarial settings should provide better understanding of the robustness of statistical methods in probabilistic settings. The research aims are: to develop techniques for analyzing the performance of statistical methods, such as Bayesian methods, for the prediction of individual sequences; to improve the understanding of appropriate ways to measure the complexity of a probability model used by these methods; to elucidate the impact of computational simplifications (such as empirical Bayes approaches and MAP estimates) on the performance of these prediction strategies; and hence to develop design methodologies for computationally efficient statistical methods for complex adversarial prediction problems.<br/><br/>There are many estimation and prediction problems for which it is appropriate to model the process generating the data as an adversary with whom the predictor competes. Such problems are common in information technology. For instance, in the problem of spam filtering, the aim is to label incoming email as either legitimate or spam, but at the same time, spammers try to design email messages that slip past these spam filters. Thus, the prediction problem is a two-player repeated game. Similar decision problems arise in computer network security (for instance, the problem of deciding whether network traffic is normal or the result of a denial-of-service attack) and in internet search (for instance, deciding if a highly linked web page is genuinely authoritative and should have a high page rank).  In these problems, some fraction of the data seen by the prediction strategy is chosen by an adversary who aims to fool the prediction strategy.  These adversarial problems are also common in financial applications. Suppose that the predictions that emerge from a financial time series analysis are used to optimize the allocation of capital across a portfolio.  Then, in the short term, it is in the interests of other market players to act so as to diminish the returns of the portfolio, and thus render the predictions inaccurate.  It is common for statistical methods, such as Bayesian methods, to be applied to these adversarial prediction problems, despite the fact that they are designed for probabilistic, rather than adversarial, models of the data.  This research project aims to develop techniques to understand the inherent limitations on the performance of these prediction methods, and hence inform the design of more powerful prediction methods.  It focuses on the predictive accuracy and computational efficiency of methods that are suitable for the complex and high-dimensional prediction problems that arise in practise.<br/>"
"0706761","New Statistical Modeling Procedures for Object Oriented Data Analysis (OODA)","DMS","STATISTICS","06/01/2007","05/05/2009","Haonan Wang","CO","Colorado State University","Continuing Grant","Gabor Szekely","05/31/2011","$149,930.00","","wanghn@stat.colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1269","0000, OTHR","$0.00","The objectives of this research are to develop novel statistical models, theory, algorithms and applications geared towards the analysis of complex object oriented data, including tree-structured objects and random graphs. The research not only introduces a number of innovative techniques, but also provides various new and deep insights into statistical foundations, e.g., modeling procedure for complex data objects. This research significantly enhances the toolkit available for the analysis of object oriented data. In particular, three inter-related topics are proposed for investigation. First, the investigator develops a careful axiomatic structure for understanding tree-structured objects, which circumvents the need to define linear operations. Moreover, the investigator studies how to carry out statistical inference, based on the metric induced probability measures, in tree space. Nonparametric and semiparametric modeling procedures are also proposed in the space of trees and graphs. Second, a model selection procedure is studied using Hellinger distance. The asymptotic behavior of the estimated Hellinger discrepancy, and testing the adequacy of the approximation are considered. The performance of the proposed model selection procedure is examined through its application to the microarray gene expression data. Third, the investigator develops new techniques for analyzing data collected on manifolds. Manifold data, such as data collected along a river in an ecological study, and data gathered over a surface, have become popular in many scientific fields. New statistical methodology to extract useful features from manifold data is needed. Here, a geodesic low-rank thin plate splines method is under investigation.<br/><br/>The research project lays out a well-grounded and comprehensive framework for analysis of object oriented data. It greatly enhances the research on object oriented data analysis by developing interdisciplinary research including bioinformatics, computer science, neuroscience, mathematics and statistics. The research on tree-structured objects can significantly benefit society by developing new techniques in image analysis and improving medical diagnoses. The investigator integrates research and education by working closely with both undergraduate and graduate students, especially underrepresented groups, from various fields. In addition, the results are to be disseminated through presentations, tutorials and conferences and via internet."
"0705264","North American Meeting of Researchers in Statistics and Probability","DMS","PROBABILITY, STATISTICS","03/15/2007","03/07/2007","Mayetri Gupta","NC","University of North Carolina at Chapel Hill","Standard Grant","Gabor Szekely","02/29/2008","$20,920.00","","gupta@bu.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1263, 1269","0000, 7556, OTHR","$0.00","The investigator and colleagues will organize the meeting of New Researchers in Statistics and Probability.  This involves the organizing of an application process, selecting conference attendees from the applicant pool, arranging senior speakers and panelists, coordinating talks and poster sessions, creating and hosting the conference website, and a number of other administrative tasks (e.g. advertising the conference through professional bulletins and statistical journals, arranging the conference center, accommodation and food at the time of the conference, etc.).<br/><br/><br/>The primary purpose of the New Researchers' Conference (NRC) is to provide a much needed venue for interaction among new researchers in Statistics and Probability (i.e. recipients of doctoral degrees within the last five years) at an important developmental stage in their careers.  At present, researchers develop contacts with others in their field either through their own institutions or at meetings of professional societies. Junior researchers often have limited contact with senior colleagues, even at their own institutions.  Meetings of professional societies are rather large and junior researchers often find them overwhelming.  In contrast with large meetings, this conference will be restricted in size, with a target of 70-80 participants.  Sessions will be followed by discussions and breaks to facilitate interactions. The NRC provides a unique opportunity for junior researchers to exchange ideas and initiate contacts amongst themselves as well as provide them an enhanced opportunity to interact with the invited senior researchers, through invited lectures and panel discussions on funding opportunities, journal publications, teaching and research. The relationships established in this informal collegiate setting among junior researchers often are ones that may last for the rest of their professional careers.<br/>"
"0707090","Function estimation for biased sampling and fMRI data","DMS","STATISTICS","07/01/2007","06/02/2009","Kinh Truong","NC","University of North Carolina at Chapel Hill","Continuing Grant","Gabor Szekely","06/30/2011","$200,000.00","Aiyou Chen","truong@bios.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","0000, OTHR","$0.00","The proposal has three projects that involve the use of polynomial splines to model (1) point process and spatial-temporal time series, (2) selection biased (length-biased), and (3) randomly truncated data. The first part of Project (1) considers a linear regression model in which the response is a stationary time series and the explanatory series is a convolution of a point process and an unknown smooth function. The aim is to estimate the unknown smooth function. The second part of (1) deals with a non-parametric likelihood based approach to independent component analysis (ICA) by estimating the marginal distributions of the blind sources and the mixing matrix using the log-spline methodology. One of the advantages of the proposed procedure is the flexibility for incorporating the temporal or spatial correlation of the sources. The aim of Project (2) is to estimate the underlying density or conditional density function using selection-biased samples, by paying special attention to the situation in which the biased sampling density tends to zero at certain rates. To what extent this will affect the performance of the spline-based estimator is an important question and the investigator proposes to address it by examining the rates of convergence of the estimator. The last project deals with the estimation of density and conditional density functions involving randomly truncated data. A new methodology based on polynomial splines is proposed and software for the methodology will be developed. This methodology is important for examining a wide range of selection-bias or under-detection limits problems. Specifically, one can study the truncation pattern closely, especially when the information about how certain type of truncation might have occurred is available. Sampling properties of the spline-based estimators proposed in this grant application will be studied by extending the asymptotic results established previously by the investigator and his colleagues. Specifically, optimal rates of convergence and local asymptotics of the proposed estimates will be investigated and issues related to high-dimensional explanatory variables will also be addressed.<br/><br/>This proposal has several broader impacts on applications to research in life sciences. First, the time series regression models and the approach to spatial-temporal data have a significant impact on brain research involving fMRI data, the proposed procedures are (a) less-biased, (b) capable to display the spatial-temporal information effectively, and (c) mathematically tractable in terms of statistical sampling properties. Second, the proposed unified approach to selection-biased or randomly truncated data provides major insights to the scientific community into the issues related to data acquisition. Robust statistical techniques are essential in the quest for important information from brain and genomic data. Third, methods proposed here will be useful for developing a course in statistical fMRI analysis to graduate students. Finally, a much broader health significance of this project will be its contribution to the better understanding of diseases that affect human lives.<br/>"
"0706816","Synscenelab: A statistical analysis of feasibility and computability of scene interpretation in synthetic stochastic images","DMS","STATISTICS, Robust Intelligence","09/15/2007","08/10/2007","Yali Amit","IL","University of Chicago","Standard Grant","Gabor Szekely","08/31/2011","$225,170.00","","amit@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269, 7495","0000, OTHR","$0.00","The introduction of statistical techniques in computer vision has yielded a number of interesting algorithms able to partially solve certain constrained recognition problems. However limitations on computing power and available training data impose certain difficult tradeoffs which are rarely quantified so that choices of parameters and models are typically done in an ad-hoc manner. These tradeoffs can only be quantified in a context where the statistical properties of the objects and their appearance in the images are well defined, yet this is far from the case in real images. The alternative, which is the goal of this project, is to perform an analysis of the same issues in a synthetic stochastic setting, using a generative model for images. Object classes are stochastically generated and instantiated in the images, together with clutter, occlusion and noise. The generative model should be rich enough to qualitatively pose the same problems as real images, yet sufficiently simple to enable quantitative analysis; hence this is not an attempt to synthesize real images. Questions regarding the limits of feasibility of various tasks such as detection and classification as a function of key parameters defining the generative model is analyzed quantitatively, in particular the analysis of the tradeoff between accuracy and computation time. The emphasis on integrating computation time in the analysis gives rise to new types of statistical questions, and new forms of asymptotic regimes as a function of the image resolution, the number of distinct classes and their variability. The hope is that the proposed framework will offer a setting in which systematic algorithmic choices can be made and contribute to the development of concrete computer vision algorithms.<br/><br/>Computer vision algorithms have produced some partial solutions to some constrained problems such as face detection, hand written digit recognition, or face recognition in severely restricted settings. Since a proper theoretical foundation for the field is lacking, a wide variety of algorithms have been proposed based on ad-hoc choices and it is difficult to assess what components of the different approaches are the most useful, which elements should be extended further and which elements should be dropped. The first step in laying a theoretical foundation for computer vision algorithms is a statistical description of the population of images. Since this is very hard to define the investigators propose to study a synthetically generated world of images, which is much simpler but which gives rise to qualitatively similar tradeoffs and challenges. In this synthetic setting the investigators will rigorously quantify the tradeoffs and hopefully be able to draw important conclusions with respect the algorithmic applications.<br/>"
"0705261","Collaborative Research: GOALI  Statistical Methods for Modern IT Systems","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","07/01/2007","06/08/2007","C. F. Jeff Wu","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","12/31/2011","$259,942.00","","jeffwu@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1253, 1269","0000, 1269, 1504, OTHR","$0.00","Motivated by real problems in the IT industry like data center/supercomputer thermal management and simulations of business operations, the investigators aim to develop a set of novel statistical methods for the experimental planning, modeling, and optimization of modern IT systems.  Specifically, they propose new modeling techniques for IT systems with high-dimensional responses, with spatial and temporal responses, or with a large number of configuration variables, and for the experimental planning and optimization of business operation simulations. The investigators bring together expertise in computer modeling and experiments, latent variable and structural equation models, modern optimization, and design of experiments. Their research will bridge the gap between statistical practice in academe and industry. They will use challenging real-world problems at IBM to motivate and develop new methodologies, and in turn test them on real data for improvement.  Its intellectual merit lies in the development of a general methodology for designing, modeling, and optimizing IT or other systems which exhibit similar traits to those described above. It can lead to major advances in multivariate statistical modeling, stochastic process modeling, statistics-aided stochastic optimization, optimization-aided design search, and space-filling designs. It is conceivable that the methodology can be incorporated into publicly released software, thus directly benefiting practitioners and researchers. <br/><br/>Many new IT systems like supercomputers, data centers, and storage systems have been invented to accommodate different business and personal needs. A recent trend in the IT industry is the integration of the traditional business in hardware with enterprise IT services like web hosting, data storage, and high-performance computing. The proposed work is expected to have broad-based impacts on the designing, monitoring and management of complex systems in general. It can be applied to a variety of problems like power systems and aircraft engine combustors. It can also benefit large-scale computer modeling for climate change, reaction to natural crisis, and the spread of pandemic diseases like bird flu and SARS.  Specifically, the developed methods can improve the thermal management and cooling efficiency of electronic systems, which is a pressing issue faced by many US industries today. The team is committed to education and dissemination of the new methodology to make a long-term impact. They plan to infuse research outcomes into coursework. The proposed education plan will result in rigorous training of a diverse group of students with solid background in statistical methods, decision-making under uncertainty, and computational modeling, who will meet a critical need of IT and other high-tech industries. They are committed to creating a diverse environment in their research groups in terms of race, gender and national origin.<br/>"
"0706709","Collaborative Research:  The Analysis of Time Series Collected in Experimental Designs","DMS","STATISTICS","07/01/2007","05/24/2007","Hernando Ombao","IL","University of Illinois at Urbana-Champaign","Standard Grant","Grace Yang","11/30/2007","$42,982.00","","hombao@uci.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","0000, OTHR","$0.00","The research project has three basic goals. The first goal is to estimate the second order properties of a nonstationary process using data from multiple units. A wavelet-based functional model with stochastic subject-specific representation is proposed. A computationally efficient algorithm for estimation will be developed along with theoretical optimal properties of the estimators. While this project is focused on the frequency or spectral domain, the second project will develop methods in the time domain. The goal of this project is to accommodate structural breaks within the time series using mixtures of state-space models. These mixtures are characterized by covariates, treatment and subject-specific effects in both the mixture components and mixing weights. The third project focuses on both temporal and spatial components. In particular, this project integrates spatial information by developing models and corresponding estimators to accommodate data structures in both time and space. The goal is to assess the differences across groups and the subject-specific and covariate effects on the dynamic structure.<br/><br/>The research focuses on an area that has been, for the most part, neglected by the statistics community.  In particular, the question to be addressed is how to best analyze data obtained from experiments when the data are taken sequentially in time and/or space.  Typical examples include experiments where EEGs are taken on various subjects with different diagnoses, or functional magnetic resonance imaging (fMRI) experiments, where subjects with different conditions are requested to perform tasks while in a magnet. The difficulty in analyzing such data is that they are often irregular (nonhomogeneous or nonstationary) in nature.  The techniques for analyzing regular time series are well established, but new techniques are needed to account for the irregularities over time and/or space.  The fact that these data are collected in complex experiments adds another layer of difficulty onto the analysis.<br/>"
"0706724","Statistical Modeling with High-dimensional Data: Variable Selection and Regularization","DMS","STATISTICS","06/01/2007","05/18/2007","Ming Yuan","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","10/31/2010","$101,953.00","","ming.yuan@columbia.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","0000, OTHR","$0.00","With high-dimensional data parsimonious models are preferred because they are much more interpretable and at the same time reduce prediction errors. Regularization is also an essential component in most modern <br/>developments for data analysis, in particular when the number of predictors is large. Non-regularized fitting is guaranteed to give badly over-fitted and useless models. The investigators take a regularization approach to the variable selection problem in high-dimensional statistical modeling such that the resulting model enjoys excellent prediction accuracy and at the same time has a sparse representation. In particular, the investigators develop: (1) new fused variable selection methods in proteomics data analysis which has been a<br/>revolutionary cancer diagnostic tool; (2) a novel kernel logistic regression model which automatically adopts a support-vector representation; (3) several new techniques for performing simultaneous variable selection in estimating multiple quantile regression functions. The investigators also study the theory of these new variable selection techniques. Efficient algorithms and software are developed for public use.<br/><br/>Modern scientific innovations allow scientists to collect massive and high-dimensional data. It is critical in scientific investigations to extract useful information from the huge amount of data. For this reason,  variable selection and dimension reduction play a fundamental role in high-dimensional statistical modeling. Variable selection problems arise from a wide range of fields, machine learning, drug discovery, biomarker finding,  genetics, proteomics, brain imaging analysis, financial modeling, environmental sciences, to name a few. The research project aims to develop state-of-the-art statistical tools that help researchers in various fields to analyze their data.<br/>"
"0706752","Collaborative Research: The Analysis of Time Series Collected in Experimental Design","DMS","STATISTICS","07/01/2007","05/24/2007","Ori Rosen","TX","University of Texas at El Paso","Standard Grant","Gabor Szekely","06/30/2008","$19,371.00","","orosen@utep.edu","500 W UNIVERSITY AVE","EL PASO","TX","799680001","9157475680","MPS","1269","0000, OTHR","$0.00","The research project has three basic goals. The first goal is to estimate the second order properties of a nonstationary process using data from multiple units. A wavelet-based functional model with stochastic subject-specific representation is proposed. A computationally efficient algorithm for estimation will be developed along with theoretical optimal properties of the estimators. While this project is focused on the frequency or spectral domain, the second project will develop methods in the time domain. The goal of this project is to accommodate structural breaks within the time series using mixtures of state-space models. These mixtures are characterized by covariates, treatment and subject-specific effects in both the mixture components and mixing weights. The third project focuses on both temporal and spatial components. In particular, this project integrates spatial information by developing models and corresponding estimators to accommodate data structures in both time and space. The goal is to assess the differences across groups and the subject-specific and covariate effects on the dynamic structure.<br/><br/>The research focuses on an area that has been, for the most part, neglected by the statistics community.  In particular, the question to be addressed is how to best analyze data obtained from experiments when the data are taken sequentially in time and/or space.  Typical examples include experiments where EEGs are taken on various subjects with different diagnoses, or functional magnetic resonance imaging (fMRI) experiments, where subjects with different conditions are requested to perform tasks while in a magnet. The difficulty in analyzing such data is that they are often irregular (nonhomogeneous or nonstationary) in nature.  The techniques for analyzing regular time series are well established, but new techniques are needed to account for the irregularities over time and/or space.  The fact that these data are collected in complex experiments adds another layer of difficulty onto the analysis.<br/>"
"0726015","International Conference on Advances in Interdisciplinary Statistics and Combinatorics","DMS","ALGEBRA,NUMBER THEORY,AND COM, STATISTICS","09/01/2007","09/09/2008","Sat Gupta","NC","University of North Carolina Greensboro","Standard Grant","Gabor Szekely","08/31/2009","$15,000.00","Scott Richter, Kirsten Doehler","sngupta@uncg.edu","1000 SPRING GARDEN STREET","GREENSBORO","NC","274125068","3363345878","MPS","1264, 1269","0000, 7556, OTHR","$0.00","The investigators seek to promote interdisciplinary statistical and combinatorial research by bringing together in this conference both junior and senior researchers from different disciplines. The investigators have enlisted many senior researchers of international repute to give plenary talks and/or to organize academic sessions on various topics representing such diverse fields as biology, combinatorial theory, computer science, econometrics, educational research methodology, environmental science, epidemiology, geography, information systems, psychology, and public health education. The project seeks funding to support young researchers who are at an early stage in their careers, particularly from groups that are traditionally underrepresented in mathematical sciences, such as women and minorities. The opportunity for personal interaction among beginning and established researchers that this conference provides is, we believe, essential for the grooming of young researchers.<br/><br/><br/><br/>The investigators seek funding to promote interdisciplinary research involving statistical techniques, and to facilitate participation in this conference of approximately 30 young researchers (advanced graduate students and recent PhD recipients), particularly from groups that are traditionally underrepresented in mathematical sciences, such as women and minorities, so that they can become part of existing or newly created research networks. Another vital aspect of this conference at the local level is that it aims to create a network of researchers in North Carolina involving those from major research centers such as UNC Chapel Hill, Duke University, N. C. State University and Wake Forest University, and those from somewhat lesser known campuses such as The University of North Carolina at Greensboro and The North Carolina A&T University. <br/>"
"0707004","Collaborative Research:  Integral Transform Methods for Sufficient Dimension Reduction in Regression","DMS","STATISTICS","08/15/2007","08/10/2007","Yu Michael Zhu","IN","Purdue University","Standard Grant","Gabor Szekely","07/31/2010","$79,928.00","","yuzhu@stat.purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","0000, OTHR","$0.00","This project is aimed to develop theory and methods for sufficient dimension reduction in regression analysis involving a large number of predictor variables. The investigators propose a general approach called the integral transform approach to facilitating dimension reduction. The key idea of this approach is to use integral transform and response transformation to change the domain where dimension reduction is <br/>performed. Due to the availability of a wide range of transformations and integral transforms, this approach leads to a flexible and effective framework for addressing and resolving challenges raised by high dimensionality. Through a series of well-defined research problems, the investigators study this framework and develop specific dimension reduction methods for many important regression applications. The success of this project not only provides effective practical tools for high-dimensional data analysis but also represents an advance in the theory and methodology of semiparametric inference.<br/><br/>High-dimensional data that involve a large amount of variables are nowadays routinely generated and collected in areas such as scientific research, government, business, etc. It is well-known that high dimensionality causes difficulties in processing and analyzing these data. This is commonly referred to as the curse of dimensionality. There is an urgent demand of statistical tools that are able to mitigate the curse of dimensionality through dimension reduction. This project represents an answer to this demand and is particularly aimed at achieving dimension reduction in regression. The results from this project can be widely applied in areas where regression involving a large number of variables is required. Gene expression and protein sequence data analysis is one such example. Therefore, this project can help enhance scientific research and discovery and benefit a variety of social and economical activities.<br/><br/>"
"0714669","Collaborative Research: A General Framework for High Throughput Biological Learning: Theory Development and Applications","DMS","STATISTICS","09/15/2007","08/24/2007","Shaw-Hwa Lo","NY","Columbia University","Standard Grant","Mary Ann Horn","08/31/2011","$270,028.00","Tian Zheng","slo@stat.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","0000, 4075, 7303, OTHR","$0.00","This application presents a comprehensive research plan for the investigation of a general framework and various new methods to handle complex large-scale data sets generated from biological (medical) as well as other scientific studies. Two goals are articulated in this proposal: theory development and application in biology and medicine. The former is focused on the study of a general yet core, model-free framework to effectively address major issues arising from high dimensional data. In the latter, the investigators seek to apply methods developed from the theory part to resolve machine learning type problems that arise in biology and medicine. In particular, this team intends to study the problems related to biological and medical prediction in response to treatments, clinical diagnosis of diseases (such as cancers), discovery of protein-protein interactions and biological network constructions related to disease etiology and motif identification. To achieve these two goals, the investigators will study theoretical and practical properties under a general setting and evaluate a series of novel statistical/computation procedures/software which will then be tested by a broad range of real and simulated data, some from current on-going studies.<br/><br/>The emergence of high dimensional data in most scientific fields poses new challenges for statisticians. Methods successful in dealing with low dimensional data are no longer effective for high dimensional data. One of the greatest difficulties in analyzing these data is to identify the informative variables/features and their associated clusters, and decipher the characteristics of the interaction between these variables and clusters. To meet current and future needs for digging hidden knowledge out of high dimensional data comprehensively and systematically, the scientific fields must develop new methods. The current project is a direct response to this need. Based on theoretical evidence (as preliminary results) already obtained in extracting low dimensional information, this team plans to apply and to develop various effective procedures to address practically important problems in the domains of biology and medicine. The investigators will study a novel screening process applicable across fields to demonstrate how high quality classifiers of low dimensionality can be identified while joint information among the influential variables are fully utilized. For further interpretation for biological validation/confirmation this team will study how to construct biological networks based on low dimensional classifiers and how to identify significant association patterns among them. A feedback mechanism will be established between the methodology development and biological validation teams, where statistical/computational results will be regularly discussed and biologically validated. It is anticipated that the key ideas and methods developed here will find numerous applications in disciplines other than biology/medicine. The proposed research is likely to advance substantial knowledge and significantly benefit current and future efforts in molecular biology/statistics/computational biology/disease prediction/drug discovery. The project would also provide valuable research experiences and training to undergraduates.<br/><br/>"
"0706835","Spatial and Spatio-temporal Processes: Asymptotics, Misspecification and Multivariate Extension","DMS","STATISTICS","06/01/2007","03/15/2007","Hao Zhang","WA","Washington State University","Standard Grant","Gabor Szekely","07/31/2008","$179,719.00","","zhanghao@purdue.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","MPS","1269","0000, OTHR","$0.00","The investigator develops appropriate infill or fixed domain asymptotic results for the evaluation of approximation methods for spatial data.  The infill asymptotic framework is generally preferred for spatial data. However, infill asymptotic results for estimation are in general difficult to derive, and there exist only a few explicit infill asymptotic results pertaining to specific models. In this proposal, a general approach to establishing the infill asymptotic properties is outlined and followed to establish asymptotic distributions of estimators that maximize some approximated likelihood functions such as Vecchia's approximation and covariance tapering. These infill asymptotic results assure that the approximation methods may yield asymptotically efficient estimators. In addition, for spatio-temporal data, the investigator considers a new asymptotic framework in which the temporal domain is increasing while the spatial sampling domain is fixed. Under this asymptotic framework, the investigator shows that an incorrect covariance function (such as covariance tapering) generally results in biased estimators. The investigator establishes asymptotic results that allow for the correction of biases. The adjusted estimators are expected to be asymptotically normal and unbiased. These asymptotic results allow one to employ incorrect but simpler spatio-temporal covariance functions and then adjust for the bias. Finally, the investigator extends the results from the univariate case to the multivariate case when multiple spatial variables are observed across space and/or over time.<br/><br/>Data across space and time are routinely observed in many scientific studies that are very important to the society such as those on global warming, environmental monitoring, precision agriculture, epidemiology, hot spot detection in homeland security, etc. The immense amount of data and the correlation across space and time have raised new challenges to the modeling and analysis of such space-time data. The primary goal of this project is tackle these challenges by studying computationally feasible and efficient approaches to the analysis of vast space-time data, which also bear no or little loss of statistical efficiency.  It is expected that this project will make more accessible and feasible the analysis of huge spatial and spatial-temporal data to scientists in broader disciplines, and thus enables scientists to retrieve significant and reliable information from the vast amount of data.<br/>"
"0705968","Advances in Variable Selection with Grouped Predictors","DMS","STATISTICS","08/15/2007","06/08/2007","Howard Bondell","NC","North Carolina State University","Standard Grant","Gabor Szekely","07/31/2011","$139,999.00","","bondell@stat.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","0000, OTHR","$0.00","This project develops variable selection procedures in the context of grouped predictors, where the grouping structure can be either known or unknown. Known grouping structures include the case of Analysis of Variance (ANOVA) models where multiple `dummy' variables represent a single factor, or in nonparametric regression using basis functions. In these situations, it is desired to either include or exclude the entire set of variables as a group. Unknown grouping structure occurs when there are underlying clusters of predictors that have a combined effect on the response, such as a set of genes sharing a common pathway. This research addresses the issue of variable selection under both known and unknown grouping structures, while simultaneously addressing additional goals specific to the problem at hand. The first component of this project is combining `supervised clustering' and variable selection into a single step to facilitate the identification of important predictive clusters when the underlying grouping structure is unknown. Secondly, for the known grouping structure, this project develops penalization techniques to perform the grouped selection while additionally allowing for the enforcement of hierarchical constraints. The third component of the project is the development of a technique to perform the typical pairwise comparison post-hoc analysis in ANOVA within the factor selection process. All three components are developed in a penalization framework by appropriate choices of the penalty.<br/><br/>With the abundance of information now available in all scientific fields, it can be an overwhelming task to decide on which of the massive number of possible characteristics, or variables, are important. Therefore, it is essential to develop techniques to perform variable selection. It is often the case that there is an underlying group structure that the scientist would like to discover as well. One common example occurs in gene expression studies, in which classification of patients into disease subtypes based on their gene expression profile is a major focus. Among the thousands of genes in a gene expression study, there is only a small fraction of them that are actually useful indicators of disease status, and many of the genes can be combined into functional groups. The investigator's research is particularly geared toward enabling the accomplishment of these types of multi-faceted analyses, such as finding the relevant genes while also identifying the group structure. A general theme of the research is that appropriately designed statistical procedures can achieve multiple objectives simultaneously and in an integrated fashion. The importance of the variable selection problem across all disciplines, and the investigator's collaborations with medical researchers and other scientists allows the results to be readily disseminated into the applied research community where it can be used to improve the quality of life.<br/><br/>"
"0706948","High-Dimensional Regression Modeling via Distributed Computing","DMS","STATISTICS","07/01/2007","06/04/2007","Christopher Hans","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","Gabor Szekely","06/30/2011","$111,181.00","","hans.11@osu.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","The research concerns the development of methods for analyzing complex, high-dimensional Bayesian regression models, with emphasis placed on the use of distributed computing to advance existing statistical methodology and inform new ways of thinking about high-dimensional model uncertainty problems. In the context of regression modeling, the availability of many predictor variables (the ""large p"" scenario) necessarily leads to questions of variable selection and model uncertainty: which predictors are most important with respect to an outcome of interest, and how does one address the reality that there may be many combinations of different predictor variables that all provide similar fit to the data? Both questions are difficult from a computational perspective, as large p problems generate model spaces that contain enormous numbers of models. The recent development of computationally-intensive, parallel-computing-based stochastic search methods designed to quickly explore promising regions of model space has given rise to new questions regarding both the search methods themselves and also the ways in which results from such searches can inform us about the scientific problems at hand. Key components of the methodological focus of the research include (i) characterization and analysis of recently developed stochastic search methods designed to explore high-dimensional model spaces, (ii) combination of output from such search methods with theoretical results to aid in sparse regression modeling, and (iii) the development of methods that use relationships between the many possible predictor variables to aid in model search and prediction.<br/><br/>The impact of the research is wide-reaching: large, complex datasets have become common in many areas of science where focus is often placed on determining how potentially thousands of predictor variables combine to predict an outcome of interest. A current example is in the field of clinico-genomics in cancer studies. Tumor samples from cancer patients can be used to generate data about the ""activity"" of upwards of tens of thousands of genes in a tumor. The research described above will allow for information to be extracted from this data to aid in answering important questions such as ""which genes work together to create particularly aggressive types of cancer?"" The ability to extract such information from large, noisy datasets may lead to simple prognostic tests that inform doctors about the potential severity of cancer in a patient, which in turn can lead to better decisions about treatment options. As a second example, the research described above has the potential to increase the efficiency of marketing by businesses: companies that collect enormous amounts of data on the purchasing habits of their customers will be able to extract information that better informs them about to whom they should market particular products. This will lead to greater corporate efficiency and in <br/>turn contribute to economic growth.<br/>"
"0705206","Collaborative Research: GOALI  Statistical Methods for Modern IT Systems","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","07/01/2007","06/08/2007","Peter Chien","WI","University of Wisconsin-Madison","Standard Grant","Gabor Szekely","06/30/2010","$128,847.00","Yasuo Amemiya","peterq@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1253, 1269","0000, 1269, 1504, OTHR","$0.00","Motivated by real problems in the IT industry like data center/supercomputer thermal management and simulations of business operations, the investigators aim to develop a set of novel statistical methods for the experimental planning, modeling, and optimization of modern IT systems.  Specifically, they propose new modeling techniques for IT systems with high-dimensional responses, with spatial and temporal responses, or with a large number of configuration variables, and for the experimental planning and optimization of business operation simulations. The investigators bring together expertise in computer modeling and experiments, latent variable and structural equation models, modern optimization, and design of experiments. Their research will bridge the gap between statistical practice in academe and industry. They will use challenging real-world problems at IBM to motivate and develop new methodologies, and in turn test them on real data for improvement.  Its intellectual merit lies in the development of a general methodology for designing, modeling, and optimizing IT or other systems which exhibit similar traits to those described above. It can lead to major advances in multivariate statistical modeling, stochastic process modeling, statistics-aided stochastic optimization, optimization-aided design search, and space-filling designs. It is conceivable that the methodology can be incorporated into publicly released software, thus directly benefiting practitioners and researchers. <br/><br/>Many new IT systems like supercomputers, data centers, and storage systems have been invented to accommodate different business and personal needs. A recent trend in the IT industry is the integration of the traditional business in hardware with enterprise IT services like web hosting, data storage, and high-performance computing. The proposed work is expected to have broad-based impacts on the designing, monitoring and management of complex systems in general. It can be applied to a variety of problems like power systems and aircraft engine combustors. It can also benefit large-scale computer modeling for climate change, reaction to natural crisis, and the spread of pandemic diseases like bird flu and SARS.  Specifically, the developed methods can improve the thermal management and cooling efficiency of electronic systems, which is a pressing issue faced by many US industries today. The team is committed to education and dissemination of the new methodology to make a long-term impact. They plan to infuse research outcomes into coursework. The proposed education plan will result in rigorous training of a diverse group of students with solid background in statistical methods, decision-making under uncertainty, and computational modeling, who will meet a critical need of IT and other high-tech industries. They are committed to creating a diverse environment in their research groups in terms of race, gender and national origin.<br/>"
"0742079","2008 Workshop on Bayesian Model Selection and Objective Methods","DMS","STATISTICS","12/01/2007","11/27/2007","Hani Doss","FL","University of Florida","Standard Grant","Gabor Szekely","11/30/2008","$10,000.00","George Casella, Malay Ghosh","doss@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","0000, 7556, OTHR","$0.00","Model selection in the frequentist setting is a well developed field. In the Bayesian framework, in principle, model selection is very simple.  Prior probability distributions are used to describe the uncertainty surrounding all unknowns, including models being considered and the parameters for these models.  After observing the data, the posterior distribution provides a coherent post data summary of the remaining uncertainty which is relevant for model selection. However, the practical implementation of this approach is not straightforward, and involves issues such as choice of priors, interpretability, and computational feasibility.  In this workshop, twelve distinguished individuals who work in Bayesian model selection present their work in a number of different areas, including determination of good objective priors, assessment of various information criteria (AIC, BIC, RIC), methods of calculation of Bayes factors, and Markov chain Monte Carlo.  A number of young researchers participate in the workshop and present their work in poster sessions.<br/><br/>Variable selection is an important and pervasive problem in scientific and medical research.  A few important variables are to be selected from many candidates and used for understanding, prediction and decision making.  Historically, variable selection has been carried out in a frequentist setting.  However, Bayesian approaches offer important advantages.  In broad terms, they give a coherent way of dealing with the distribution of the future response of an individual for whom the predictor variables are now known.  Recent advances in both computing power and statistical methodology have greatly enhanced the feasibility of Bayesian approaches to regression and variable selection.  The workshop provides an excellent opportunity to discuss the many recent significant developments in Bayesian model selection and objective methods; to discuss what has been found to work and what does not; and to identify important problems and new research directions.<br/>"
"0709985","Conference Proposal: The Third Erich L. Lehmann Symposium -- Statistical Optimality","DMS","STATISTICS","03/15/2007","03/07/2007","Javier Rojo","TX","William Marsh Rice University","Standard Grant","Gabor Szekely","02/29/2008","$18,000.00","","jrojo@iu.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","0000, 7556, OTHR","$0.00","The Third Erich L. Lehmann Symposium<br/><br/>The four-day conference is the third of a series of symposia. The goal of the symposia is to examine the role that optimality can play, or should play, in modern statistics. Due to the advent of high throughput data collection technology and the parallel development of computing power to analyze such data, it often happens that statistical theory gives way to raw computing power. Although most of the new exciting statistical methodologies have provided tools to make headway in many important scientific problems, a need to generalize and systematize this knowledge is now quite evident. The Third Symposium will take place at Rice University during May 16th - 19th and will bring together a group of experts to discuss cutting-edge research optimality ideas in the context of modern statistical methodologies. It is believed that, although much progress has taken place in areas such as data visualization and data mining and knowledge discovery among others, the subjects are ripe for the development of an optimality paradigm that allows for objective comparisons of methodologies. This new paradigm, although still to be defined, is necessary to push the research frontiers in these important disciplines. The conference will showcase new developments by leading researchers in an environment conducive to the development of new human resources.<br/><br/>This proposal will fund airfare and per diem expenses support for twelve young investigators and eight Ph.D. students to attend the conference. Women and minority young investigators and Ph.D. students will be actively recruited and encouraged to apply. Pfizer, Rice University, The University of Texas Health Science Center at Houston, and MD Anderson Cancer Center have made contributions to fund other aspects of the conference. Refereed papers will be published and invited and plenary sessions will be digitally recorded and made available to the scientific community through Rice University's online archives.  An evolving webpage for the Symposium may be found at http://www.stat.rice.edu/~jrojo/3rd-Lehmann.<br/><br/>"
"0706989","A New MCMC Framework with  Applications to Protein Bioinformatics","DMS","STATISTICS","07/01/2007","07/14/2011","Jun Liu","MA","Harvard University","Continuing Grant","Gabor Szekely","06/30/2013","$629,206.00","","jliu@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","0000, OTHR","$0.00","In the past two decades, statisticians and other quantitative researchers have begun to appreciate the power of Monte Carlo integration and optimization methods.  This proposal focuses on the development of a novel Markov chain Monte Carlo (MCMC) framework, which promises to greatly enhance our capability of and flexibility in designing effective Monte Carlo algorithms. More precisely, the investigator proposes a unified<br/>framework to generalize the standard Metropolis-Hastings approach to design Markov chains and shows its deep relationship with a few existing MCMC methods, such as multigrid Monte Carlo, configurational-bias Monte Carlo, and orientational-bias Monte Carlo. The investigator will also  focus on one of the fastest growing application areas, protein bioinformatics (encompassing multiple sequence alignments, protein function annotation, and protein-protein interactions, and protein structural modeling, etc.), which serves both as an important application and as a great source of significant challenges to existing MCMC methods. On one hand, the investigator seeks to apply the new MCMC framework to design novel protein structure and <br/>sequence analysis tools; on the other hand, the challenging problems encountered during such endeavors will motivate and steer the investigator to develop new MCMC strategies. <br/><br/>With the ever growing need of quantitative (statistical) analysis of very large datasets with complex structures (such as  genomics data, consumer goods data, internet data, etc.), the need for designing more efficient computational methods to analyze these data and to make useful predictions is also strong. This proposal has three inter-related themes: to develop a  novel Monte Carlo framework, which can be generally understood as a new way of utilizing computer-generated random numbers to approximately solve an optimization or integration problem, to develop novel statistical models for biological sequence and proteinstructure analysis,  and to apply these new computational methods and statistical models to infer molecular mechanisms of protein functions and to predict protein structures. The proposed research will not only significantly advance the Monte Carlo methodology and computational statistics theory, which are applicable to a wide range of optimization and simulation problems in different application areas, but will also bring the power of these new methods and theory to bear on one of the most important application areas, computational biology. It will particularly advance the modeling, analysis, and computational techniques <br/>in  protein bioinformatics. It will help educators revise and generate new courses on computational biology and Monte Carlo methodologies for both undergraduate and graduate students. It will also provide interdisciplinary research opportunities for such students, and will result in software and methodologies <br/>that may be of interest to the pharmaceutical industry.<br/>"
"0705209","Regularization and Optimization for High Dimensional Regression and Classification with Biological Applications","DMS","STATISTICS","06/01/2007","05/18/2007","Chunming Zhang","WI","University of Wisconsin-Madison","Standard Grant","Gabor Szekely","05/31/2011","$180,001.00","","cmzhang@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","0000, OTHR","$0.00","The investigator develops new regularization and optimization techniques for high dimensional data that arise from frontiers of biological, medical and scientific research. Four interrelated research topics are proposed for investigation. First, the functional sparse inference for functional magnetic resonance imaging data in neuroscience is proposed for more accurate localization of brain areas in response to the time-varying stimuli. Second, the investigator develops curvature-based shape analysis for high-dimensional space curves with applications to comparing biological shapes, detecting key anatomical regions which exhibit shape difference, and classifying functional data objects. Third, the investigator studies unified theory and methodology for regularized parametric and nonparametric estimators under a general class of loss functions. Fourth, a high-dimensional pseudo logistic regression and classification approach is proposed which simultaneously combines the strengths of both support vector machine and traditional logistic regression.<br/><br/>High-dimensional data sets and streams arising from bioinformatics, environment, financial markets, and signal and image processing pose numerous challenges to conventional statistical methods. A major<br/>goal of the proposal is to make methodological and theoretical contributions to the important and challenging regularization approach in the analysis of high-dimensional data, like spatio-temporal fMRI brain images, functional data objects and gene expression profiles. These new developments allow scientists to analyze high-dimensional data with efficient dimension reduction and increased interpretability. In addition, the investigator will integrate new computational tools and mathematical theories with those in sciences and engineering. Dissemination of these developments will enhance new knowledge discoveries and prudent<br/>policy making, and strengthen interdisciplinary collaborations. The research will also serve an educational purpose through multi-disciplinary courses on the contemporary state-of-the-art data mining and machine learning, and benefit the training and learning of undergraduate, graduate students and underrepresented minorities. <br/>"
"0706919","Sufficient Dimension Reduction for Missing, Censored, and Correlated Data","DMS","STATISTICS","09/15/2007","08/24/2007","Lexin Li","NC","North Carolina State University","Standard Grant","Gabor Szekely","08/31/2011","$119,900.00","","lexinli@berkeley.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","0000, OTHR","$0.00","This proposal aims to develop new theory and methodology for sufficient dimension reduction (SDR). In particular, the research focuses on biostatistical problems which commonly include missing data, survival data, and longitudinal data analysis. The proposed methodology can effectively transform a high dimensional regression problem to a low dimensional projection, retain full regression information, and impose few or no probabilistic models. There are three components to this research. First, the investigator proposes a family of augmented inverse probability weighted SDR estimators when predictors have missing observations. This new approach allows a more general missing data mechanism than the existing solution and permits more flexible regression forms beyond the homoscedastic linear model. The second component of the research targets SDR for survival data, where the response of time to death or disease recurrence is subject to censoring. Viewing the censored response as a specific type of missing data, the investigator integrates an inverse probability weighted estimation strategy with a variety of SDR methods. Thirdly, the investigator studies a type of longitudinal data where measurements for all the study subjects are collected at the same scheduled time points. Both a population foundation and the associated estimation procedure are developed. All three components center around commonly encountered biostatistical problems and the development of the three components are interrelated.<br/><br/>Modern technologies have pushed the frontier of science with the capability of generating and collecting data in large quantity and high dimensionality. Examples of large high dimensional data sets arise in a great number of research areas, such as environmental studies, human health and medical research, and homeland security. Sufficient dimension reduction (SDR) methodology effectively transforms a high dimensional data problem to a low dimensional one. Consequently, SDR allows many existing analytical methods, which used to be hindered by the curse of dimensionality, to now work for the high dimensional problems. In addition, informative visualization of the data often becomes possible after dimension reduction, facilitating both the understanding and the analysis of the data. By developing new theory and methodology for missing, censored, and correlated data, the investigator's research extends the boundary of SDR to biostatistics as well as other disciplines such as econometrics, finance and bioinformatics. The impact of this research is anticipated to be widespread, due to the prevalence of the high dimensional data and the urgent demand for effective analytical tools to tackle those problems.<br/><br/>"
"0705961","Some Problems in Nonparametric Function Estimation","DMS","STATISTICS","07/01/2007","06/13/2007","Chong Gu","IN","Purdue University","Standard Grant","Gabor Szekely","06/30/2011","$180,070.00","","chong@stat.purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","0000, OTHR","$0.00","The investigator continues the methodological/theoretical/algorithmic developments and the software implementation of smoothing spline ANOVA models.  Through the inclusion/exclusion of selected terms in functional ANOVA structures, rich modeling options are made available in the settings of regression, density estimation, and hazard rate estimation; the popular additive models are special cases.  Among specific tasks in this phase of the research are (i) the separate model configurations of variables of different natures such as geographic locations and time for seasonal effect, (ii) conditional density estimation on generic domains which induce regression models with all sorts of responses including multivariate ones with mixtures of discrete, continuous, and possibly other types, (iii) Fast computation for multivariate density estimation which makes nonparametric graphical models practically feasible, and (iv) hazard estimation with censored lifetime data and frailty terms.  Also explored are the spectral analysis of non-stationary time series and spike trains.<br/><br/>As nonparametric extensions of the widely used (generalized) linear models, the methods being developed provide empowering modeling tools for researchers in a broad spectrum of application areas to extract fine features from the ever larger data sets they collect; documented past applications were found in biomedical studies, epidemiology, natural resource management, etc.  The end product of the proposed research is open-source software with a friendly user-interface in a popular platform with a diverse user base, which facilitates the routine use of the tools being developed by practitioners and thus enhances the infrastructure for information-processing.<br/>"
"0652743","FRG:  Collaborative Research: Overcomplete Representations with Incomplete Data: Theory, Algorithms, and Signal Processing Applications","DMS","STATISTICS","07/01/2007","07/29/2009","Xiao-Li Meng","MA","Harvard University","Continuing Grant","Gabor Szekely","06/30/2011","$589,831.00","Patrick Wolfe","meng@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","0000, 1616, OTHR","$0.00","Driven by accumulated scientific results and recent breakthroughs in sparse representations, recent years have seen an ever-increasing interest in overcomplete expansions with incomplete data---a critical subject requiring close cooperation and exchange of ideas amongst statisticians, mathematicians, and engineers.  A number of indicators suggest the appropriateness and timeliness of a Focused Research Group (FRG) involving these three communities as the best means to approach to this high-potential yet challenging research area.  In particular, this project follows a comprehensive and vertically integrated research plan for (1) deriving new theoretical results for statistical estimation in the context of overcomplete Gabor time-frequency representations and multiresolution wavelet dictionaries; (2) leveraging these results to develop algorithms tailored for canonical problems in signal and image processing, where practitioners are often faced with missing data or more generally incomplete measurements; and (3) addressing ubiquitous and important cross-cutting applications, including curve fitting as well as audio and color image enhancement. To respond to these pressing scientific needs and prepare the ground for significant developments in the mathematical sciences, the FRG team is exploiting recent results from harmonic analysis and the theory of frames to develop a coherent framework for statistical modeling in the case of overcomplete expansions, including an examination of key open questions such as the impact of the choice of prior coefficient distributions in a Bayesian framework and asymptotic risk bounds for regression when the set of potential predictors is overcomplete.  As a definitive first step toward these grand challenges, the team proposes and investigates an innovative common-component model for frame coefficients that recovers currently used methods as special cases but opens up important new avenues for advancement.  The FRG team has significant prior experience in multiresolution analysis, computational Bayesian inference, and self-consistency methods for missing data, and hence is also developing and applying state-of-the-art procedures to implement the resulting new algorithms.<br/><br/>The Focused Research Group (FRG) project team combines scientists from an established institution (Harvard University) and a young, rapidly growing one (University of Central Florida).  The project's research agenda is set to substantially advance the theoretical knowledge and understanding of the applicability of overcomplete representations (a new and important cross-cutting area of mathematics, with many major open questions relating  to the area of ""compressed sensing"" recently featured in the New York Times, The Economist, and elsewhere in the mainstream media) in both statistical and engineering practice. This will ultimately lead to development of more efficient algorithms for signal processing and data analysis in situations where data must be collected at a very low rate (as in the compressed sensing regime described above), or when a portion of available data has been lost or highly contaminated. The latter scenario is particularly salient both for commercial applications (e.g., voice data in the case of cellular communications) as well as military and homeland security concerns (for instance, to recover unobserved data from related sources). Another benefit of the project it its emphasis on close collaboration amongst mathematicians, statisticians, and engineers through a single team, which will lead not only to solution of the specific problems under study, but also to formulations of new important areas of research and their application to the real world. Using support from NSF, the team trains a number of students who are ready to carry out research on the cutting edge of mathematics, statistics and engineering, and holds regular workshops to increase the involvement of new researchers and disseminate results to the wider scientific community.<br/>"
"0645293","CAREER: Nonparametric Models Building, Estimation, and Selection with Applications to High Dimensional Data Mining","DMS","STATISTICS","07/01/2007","05/06/2011","Hao Zhang","NC","North Carolina State University","Continuing Grant","Gabor Szekely","09/30/2013","$400,000.00","","hzhang@math.arizona.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","0000, 1045, 1187, OTHR","$0.00","Nonparametric methods are increasingly applied to regression, classification and density estimation, both in statistics and other related areas such as data mining and machine learning. However, a key difficulty with nonparametric models is model fitting for high dimensional data due to the curse of dimensionality. Another difficulty is model inference and interpretation, i.e., how to evaluate or test individual variable effects on the complex surface fit. For heterogeneous data with complicated covariance structure, nonparametric model estimation is even more challenging. The objectives of this proposal are to develop novel and widely applicable procedures to simultaneous model selection and estimation for nonparametric models and their related paradigms in data mining. In the framework of reproducing kernel Hilbert space (RKHS), the PI proposes a host of new regularization techniques for several families of models: smoothing spline ANOVA models for correlated data, semiparametric regression models, support vector machines for supervised and semi-supervised learning. The proposed methodologies constitute key advances over standard methods through their unified framework for achieving model sparsity and function smoothing altogether, their tractable theoretical properties, and their easy adaptation to high dimensional problems. The PI will study asymptotic behaviors of the proposed estimators, explore data-driven procedures for tuning regularization parameters, and develop computation algorithms and softwares to implement the proposed procedures. The PI will also examine finite sample performance of new methods via extensive simulation studies and real data analysis.<br/><br/>In the current information era, the volume and complexity of scientific and industrial databases have been exponentially expanding. As a consequence, the data form keeps gaining higher and higher dimensionality. Analysis of such data poses new challenges to statisticians and is becoming one of the most important research topics in modern statistics. The purpose of this project is to significantly increase the available tools for analyzing complex high dimensional data. In this project, the PI aims to accomplish the following three goals: (1) meet the challenges of nonparametric model estimation and selection within a unified mathematical framework; (2) develop flexible methods with desired statistical properties and high-performance statistical softwares for mining massive data; (3) integrate research opportunities and findings from the above two activities into disciplinary and interdisciplinary statistical education at graduate, undergraduate and high school levels. This research will broaden traditional understanding of nonparametric inferences <br/>and model selection, provide a broad range of researchers and practitioners in various fields including sociology, economics, environmental, biological and medical sciences with state-of-the-art data analysis tools, and help to prepare the next-generation students with the necessary modern statistical perspectives. <br/> <br/>"
"0639980","CAREER: Inferences on Large-Scale Multiple Comparisons:  The Temptation of the Fourier Kingdom","DMS","STATISTICS","07/01/2007","12/11/2006","Jiashun Jin","IN","Purdue University","Continuing Grant","Gabor Szekely","01/31/2009","$182,761.00","","jiashun@stat.cmu.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","0000, 1045, OTHR","$0.00","This research project is to create new tools for large-scale multiple comparisons.   In particular, the investigator develops new tools in the frequency domain to tackle problems in this field. The project includes (a). Introduce Fourier analysis as a tool for multiple comparisons. The investigator devotes to push the boundary of the field by harnessing the power of Fourier analysis. The Fourier analysis has been repeatedly proven to be a powerful tool in many scientific areas, but has seldom been used in the field of large-scale multiple comparisons. (b). Develop practically feasible tools, and lay out theoretic frameworks for studying the optimality of the tools. (c). Extend and apply  the developed methodology and theory to the analysis of massive data  generated in  various scientific fields,  including comparative genomic hybridization (CGH),   cosmology and astronomy, and gene microarray.  <br/><br/>Modern data acquisition routinely produces massive data sets in many scientific areas, e.g. genomics, astronomy, functional Magnetic Resonance Imaging (fMRI), and image processing.  The vision is that advances in massive data analysis will enable scientist from various fields to quickly extract the information they need, and at the same time,   benefit the statistical discipline both with a broader scope of theory and methodology but also with a deeper understanding of nature and science. The project pushes the boundary of the field by introducing new ideas for problem solving, developing new tools and novel theory, and applying the tools to other scientific fields including but not limited to comparative genomic hybridization (CGH), cosmology and astronomy, and gene microarray. <br/>  <br/>"
"0706731","Multivariate space-time models and methods to combine large disparate spatial data and numerical models","DMS","STATISTICS","05/15/2007","03/24/2009","Montserrat Fuentes","NC","North Carolina State University","Continuing Grant","Gabor Szekely","04/30/2012","$259,976.00","Lian Xie, Brian Reich","mfuentes@vcu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","0000, OTHR","$0.00","Multivariate space-time models and methods to combine large disparate spatial data and numerical models<br/><br/>Multivariate spatial-temporal statistical problems are prevalent in the environmental sciences, particularly in atmospheric and oceanic data applications. In many cases the processes of interest are inherently nonlinear and dynamic. Different sources of information for these systems include observational data as well as physics-based numerical models. Over the past decade there has been an increase in the availability of real-time observations as well as advances in the sophistication and resolution of deterministic atmospheric and oceanic models. A  modeling framework  to combine numerical models and observations is proposed, this framework allows for estimation of a multivariate statistical model for the data as well as parameters of physically-based deterministic models, while accounting for potential additive and multiplicative bias in the observed data. A broad class of multivariate spatial-temporal models is developed to explain the variability in the multivariate space-time data, as well as the cross-dependency between different variables. This general class of models goes beyond the standard assumptions of symmetry, separability and stationarity of the covariance function, and an extension to non-Gaussian processes is presented. <br/><br/>Storm surge is the onshore rush of seawater associated with hurricane winds and can lead to loss of property, billion of dollars in damage, and large number of fatalities. Numerical ocean models are used to determine when and where to send evacuation warnings and recovery units to affected areas. One of the main inputs to the ocean models is the surface wind field, which is calculated based on a physical model. Currently, physical wind measurements from buoys and satellites  are not used to forecast storm surge. The proposed statistical framework and models are used to better model hurricane surface wind fields by supplementing the physics-based model output with wind information from buoys and satellites.  Statistical multivariate space-time modeling is used to combine these data  to make predictions. Statistical models have proven to be an essential tool in the environmental sciences to describe complex spatial and temporal behavior of physical processes. Statistical models also allow for prediction of the underlying spatial-temporal processes at new locations and times. Through  collaborations between scientists and statisticians, it is anticipated that the new statistical models  and methods presented in this proposal  for multivariate space-time processes will enhance science by improving ocean coastal prediction, and by introducing new methodology to analyze massive datasets.  The investigators will use part of the funds to travel and disseminate broadly the methods  proposed here to enhance mathematical and scientific understanding. The principal investigator will give some talks and short courses in Hispanic countries to broaden the participation of underrepresented geographic and ethnic  groups. The investigators will continue their efforts to broaden the participation of minorities and women.<br/>"
"0743459","Collaborative Research: Applied Probability and Time Series Modeling","DMS","STATISTICS, COFFES","08/15/2007","08/10/2007","Richard Davis","NY","Columbia University","Standard Grant","Gabor Szekely","07/31/2011","$188,534.00","","rdavis@stat.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269, 7552","0000, OTHR","$0.00","An investigation of the properties of Levy-driven CARMA (continuous-time ARMA) processes will be undertaken and efficient methods of inference developed. The results will be applied to the study of stochastic volatility models with Levy-driven CARMA volatility and to the further study of COGARCH models. Time series in which the parameters are constant over time-intervals between ``change-points'' constitute an important class of non-stationary time series which has been found particularly useful in hydrology, seismology and finance. Properties and applications of a new estimation technique based on the minimization of the minimum description length of a model that includes the number of change-points and their locations as parameters will be developed and extended to cover a general class of processes with structural breaks of various types. Estimation techniques for all-pass models driven by non-Gaussian noise will also be developed.  These techniques, including maximum likelihood and minimum dispersion estimation, will be applied to the problem of identification and estimation for non-causal or non-invertible ARMA models.Adaptive techniques for efficient estimation of such models will be explored.<br/><br/>In the last fifteen years, there has been a widely-recognized need for the development of new models and techniques for the analysis of time series data from scientific, engineering, biomedical, and financial applications. Some of the features required of these new models are nonlinearity, complex dependence structures, strong deviations from normality and non-stationarity. The current proposal addresses these needs. It seeks to enhance understanding of the physical and economic processes represented by the models. The development of efficient estimation and simulation techniques will be an essential component of the research. <br/>"
"0700078","Algebraic Statistical Models","DMS","ALGEBRA,NUMBER THEORY,AND COM, STATISTICS","07/01/2007","06/13/2007","Seth Sullivant","MA","Harvard University","Continuing Grant","Tie Luo","09/30/2008","$70,826.00","","smsulli2@ncsu.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1264, 1269","0000, OTHR","$0.00","Algebraic statistics is concerned with the use of commutative algebra, algebraic geometry and combinatorics in statistical inference.  The connection between algebra and statistics arises from the fact that many statistical models for discrete random variables have the structure of algebraic varieties.  This underlying algebraic structure can be exploited to develop new tools for analyzing statistical data and also suggests new research directions in algebra and combinatorics.  <br/>The research undertaken by the PI concerns the study of the algebraic structure of statistical models.  In particular, the research focuses on the study of the ideals defining the statistical models as algebraic varieties, ways that the ideal generators and Grobner bases can be used as tools in statistical inference, and the development of algebraic techniques for studying and computing these defining ideals.  The particular models studied by the PI are log-linear models, phylogenetic models, and mixture models.  Among the varieties that arise in this study are toric varieties, determinantal varieties, secant varieties, and many new varieties which deserve further study.<br/><br/><br/>Statistical models for discrete data are increasingly used throughout the social and biological sciences.  Of particular note is the emergence of statistical techniques for the analysis of biological sequence data (i.e. DNA, RNA, and protein sequences).  These statistical models are families of probability distributions inside a finite dimensional space, and these families often have an algebraic structure.  The PI proposes to further his work exploring the algebraic structure of these statistical models, increasing the interaction between two important areas (algebra and statistics) in the mathematical sciences.   <br/>"
"0808626","Bayesian Methods for Large-Scale Applications","DMS","STATISTICS","07/01/2007","02/01/2008","David Madigan","NY","Columbia University","Continuing Grant","Gabor Szekely","06/30/2009","$80,445.00","","madigan@stat.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","MPS","1269","0000, OTHR","$0.00","                                                   ABSTRACT <br/><br/>PROPOSAL NUMBER.: DMS-0505599  <br/>INSTITUTION: Rutgers University New Brunswick  <br/>NSF PROGRAM: STATISTICS<br/>PRINCIPAL INVESTIGATOR: Madigan, David<br/>PROPOSAL TITLE: Bayesian Methods for Large-Scale Applications   <br/><br/>The investigators work on Bayesian statistical methods for large-scale<br/>applications. Three applications provide the backdrop for the work.<br/>""Text categorization"" concerns the automatic assignment of documents<br/>to predefined categories and requires ultra-high dimensional supervised<br/>learning models. ""Authorship attribution"" uses similar methodology<br/>but attempts to identify authors of anonymous documents.<br/>The ""Localization"" problem uses signal characteristics to locate users <br/>in  wireless networks.  The investigators focus on technical challenges <br/>that span these applications including sequential Bayesian analysis,<br/>non-linear optimization, and novelty detection algorithms.<br/><br/>In both the business and scientific realms, computing advances have<br/>drastically altered the role of data analysis.  Historically, analysts<br/>produced data locally to address specific research questions. Now,<br/>ubiquitous computing and cheap storage have decoupled the production<br/>of data from the research questions. Data of all kinds are produced<br/>and deposited in remotely accessible databases with myriad questions<br/>in mind, both foreseen and unforeseen. Statistics has historically<br/>focused on squeezing the maximum amount of information out of limited<br/>data. The investigator's work focuses instead on so-called<br/>Bayesian statistical methods for these  emerging larger-scale applications<br/><br/>"
"0707037","Generalized Fiducial Inference for Modern Statistical Problems","DMS","STATISTICS","08/01/2007","05/08/2009","Jan Hannig","CO","Colorado State University","Continuing Grant","Gabor Szekely","12/31/2009","$243,760.00","Hariharan Iyer, Thomas Chun Man Lee","jan.hannig@unc.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1269","0000, OTHR","$0.00","In this proposal the investigators revisit Fisher's controversial fiducial argument with a modern set of questions in mind.  This is motivated by the success of generalized inference as introduced by Tsui & Weerahandi (1989), which in fact leads to the same results as Fisher's fiducial inference (Hannig, Iyer & Patterson, 2006).  The investigators do not attempt to derive a new ``paradox free theory of fiducial inference''.  Instead, with minimal assumptions, the investigators present a new simple fiducial recipe that can be applied to conduct statistical inference via the construction of generalized fiducial distributions.  This recipe is inspired by the concept of generalized pivotal quantity and is designed to be fairly easily applicable in many practical applications. It can be applied regardless of the dimension of the parameter space (i.e., including nonparametric problems), and it often leads to statistical procedures that are asymptotically exact and, more importantly, possess very good approximate small sample properties. The investigators propose to investigate theoretical properties of generalized fiducial distributions for statistical problems and apply their findings to various problems of broader interest.<br/> <br/> <br/>Systematic study of properties of generalized fiducial inference will increase our understanding of foundations of statistics and will give statisticians an additional tool to use when dealing with problems they encounter in practice.  More directly, successful solution of the proposed applied problems will immediately bear fruit in the application areas, e.g., pharmaceutical statistics and metrology. For instance, the U.S. Food and Drug Administration (FDA) guidance document spells out analysis procedures for demonstration of equivalence of two or more drug formulations. The investigators aim to show that the fiducial approach will lead to more efficient procedures, which will result in cost and time savings, an important issue for the drug industry. In metrology, the International Bureau of Weights and Measures (BIPM) in conjunction with the International Organization for Standardization (ISO), has published a ``Guide to Expression of Uncertainty in Measurements"" (GUM) which spells out the procedures to be followed by national metrological institutes such as NIST in the US, NPL in UK, and PTB in Germany. A problem that is unique to metrology is that every measurement is subject to unknown and unknowable systematic errors that are often larger than random errors. The only way to quantify these unknowable systematic errors is via specification of subjective distributions for them. The GUM specifies how to combine data-based estimates of standard deviations for some error components in the calculations and subjective estimates of uncertainty for other error components. The investigators aim to demonstrate that the fiducial method provides a natural approach for accomplishing this. Such results are likely to influence the metrology community in modifying and improving their current procedures."
"0744058","Collaborative Research: Applied Probability and Time Series Modeling","DMS","STATISTICS, COFFES","08/15/2007","08/10/2007","Peter Brockwell","CO","Colorado State University","Standard Grant","Gabor Szekely","07/31/2011","$141,481.00","","pjb2141@columbia.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1269, 7552","0000, OTHR","$0.00","An investigation of the properties of Levy-driven CARMA (continuous-time ARMA) processes will be undertaken and efficient methods of inference developed. The results will be applied to the study of stochastic volatility models with Levy-driven CARMA volatility and to the further study of COGARCH models. Time series in which the parameters are constant over time-intervals between ``change-points'' constitute an important class of non-stationary time series which has been found particularly useful in hydrology, seismology and finance. Properties and applications of a new estimation technique based on the minimization of the minimum description length of a model that includes the number of change-points and their locations as parameters will be developed and extended to cover a general class of processes with structural breaks of various types. Estimation techniques for all-pass models driven by non-Gaussian noise will also be developed.  These techniques, including maximum likelihood and minimum dispersion estimation, will be applied to the problem of identification and estimation for non-causal or non-invertible ARMA models.Adaptive techniques for efficient estimation of such models will be explored.<br/><br/>In the last fifteen years, there has been a widely-recognized need for the development of new models and techniques for the analysis of time series data from scientific, engineering, biomedical, and financial applications. Some of the features required of these new models are nonlinearity, complex dependence structures, strong deviations from normality and non-stationarity. The current proposal addresses these needs. It seeks to enhance understanding of the physical and economic processes represented by the models. The development of efficient estimation and simulation techniques will be an essential component of the research."
"0706965","Theory and Applications of U-statistics for Multistate Models under Censoring","DMS","STATISTICS","07/01/2007","04/07/2009","Somnath Datta","KY","University of Louisville Research Foundation Inc","Continuing Grant","Gabor Szekely","06/30/2011","$100,782.00","","somnath.datta@ufl.edu","Atria Support Center","Louisville","KY","402021959","5028523788","MPS","1269","0000, 9150, OTHR","$0.00","<br/>U-statistics are fundamental objects in the theory and methods of statistics. They are generalizations of sample means and cover directly or indirectly a whole range of estimators, estimating functions (for each value of the parameter) and test statistics. The purpose of this proposal is to investigate systematically a class of these statistics that may arise in various multistate censored data problems under a unified framework. Multistate models are progressive systems where individuals move from one state to the next and the resulting data are generalizations of event time data. The variables of interest include various transition times, entry and exist times to and from a given state, waiting times in a given transient state and so on or a mixture of all of the above and additional covariates, if any. Both right and interval censored multistate data are considered. Applications of the U-statistics theory and methods to the construction of nonparametric tests and confidence intervals in multistate problems are also studied. <br/> <br/>Multistate data arise in diverse fields and applications. For example, the states may correspond to the health status of patients or the strength of the housing market or the climatic conditions in various parts of the world and so on. As a result, it is expected that the resulting methods will provide valuable statistical tools to researchers in multiple disciplines including medicine, marketing, political science and engineering. The proposed research will also contribute substantially in training future statisticians since parts of it will be used for doctoral dissertations under PI's supervision and some of the findings will be incorporated in graduate level courses.<br/>"
"0813827","Localized Cross Spectral Analysis and Pattern Recognition Methods for Non-Stationary Signals","DMS","STATISTICS","10/15/2007","06/19/2008","Hernando Ombao","RI","Brown University","Standard Grant","Gabor Szekely","06/30/2008","$48,352.00","","hombao@uci.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1269","0000, OTHR","$0.00","Abstract<br/>PI: Hernando Ombao<br/>proposal: 0405243<br/><br/>The PI develops a systematic body of methods and models for analyzing massive non-stationary signals.  The basic tool is the SLEX library, a collection of bases, each basis consisting of orthogonal localized Fourier waveforms.  The SLEX methods give results that are easy to interpret because they are time-dependent analogues of the Fourier spectral analysis of stationary signals.  Moreover, the SLEX methods use computationally efficient algorithms, thus, they will be capable of handling massive data sets.  The PI develops a family of multivariate models for non-stationary signals recorded from several subjects.  The model explicitly takes into account the time-evolving inter-connection between the components of the multivariate signals.  In addition, the PI develops an automatic procedure for decomposing the high dimensional multivariate signals into SLEX components using the eigenvalue-eigenvector decomposition of the time-varying SLEX spectral matrix.  The SLEX components are non-stationary and have zero-coherency.  Thus, they contain non-redundant information on the time-varying cross spectra, which will be used as the primary feature for model selection as well as for discrimination and classification.  Finally, the PI develops an automatic and systematic method for extracting time-varying higher order spectral features of non-stationary signals.  The PI develops the SLEX higher order spectra, which can account for the time-evolutionary interaction between different frequency components in the signal.  In this proposal, the SLEX are the foundation on which the body of coherent and systematic methods for non-stationary signals is built.<br/><br/>This proposal is motivated by the statistical problems that confront the neuroscience community.  Major advances in technology now enable neuroscientists to collect complex data sets for investigating the more intricate functioning of the human brain.  There is currently a major interest to study how different brain areas interact with each other in response to a mental stimulus.  There is also a widespread interest in exploring the association between impairment in brain connectivity and various mental disorders.  To study brain connectivity, various types of signals (EEGs, MEGs, fMRI) are recorded.  Analyzing brain signals is quite challenging because the brain is a complex organ.  Moreover, the signals collected are both non-stationary and massive.  The SLEX methods that the PI develops in this proposal address these issues.  The SLEX methods are able to capture the local temporal features of the signals.  Moreover, the methods are able to handle massive data sets, because they use computationally efficient algorithms.  As part of the educational component of this proposal, the PI works closely with graduate and undergraduate students in this research undertaking.<br/>"
"0753043","Conference Proposal: Theory and Applications of Benford's Law","DMS","PROBABILITY, ALGEBRA,NUMBER THEORY,AND COM, STATISTICS","12/01/2007","10/17/2007","Steven Miller","RI","Brown University","Standard Grant","Tomek Bartoszynski","11/30/2008","$13,368.00","","sjm1@williams.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1263, 1264, 1269","0000, 7556, 9150, OTHR","$0.00","Benford's Law describes the distribution of the leading significant digit of many data sets; rather than being uniformly distributed, the probability the first digit equals d is often log_10(1 + 1/d). The number of systems satisfying Benford's Law is astounding, and extremely diverse, including budget and income tax data, stock market tables, census figures, DCT-domain coefficients in images, hydrology data, standard mathematical functions (such as n!), iterates of dynamical maps, values of L-functions and characteristic polynomials of random matrix ensembles, solutions of recurrence relations, and the 3x+1 problem, to name just a few. We believe that our proposed workshop is the first attempt to assemble a large, representative sample of people in the myriad of disciplines working on the theory and applications of Benford's law. There are numerous examples of major advances in science and mathematics arising from people applying the tools and techniques of one field in another. We hope to have similar success here. Specifically, by gathering experts from all the different fields studying Benford's law, we plan on disseminating the problems and techniques across the different disciplines. It is likely that participants will be able to provide helpful suggestions to each other (or be intrigued by proposed questions and participate in joint research).<br/><br/>The list of topics include mathematical foundations, interpretation, information forensics and fraud detection, signal and data processing, dynamical systems, complex networks and system design. As one of our primary goals is to disseminate knowledge of tools, techniques and problems related to Benford's Law throughout the community, we plan on publishing proceedings of the workshop. We are investigating the publication through SIAM or AMS of a book based on the workshop talks, as well as possibly some expository articles. The immediate deliverables of the workshop will be a report, as well as electronic proceedings.  The longer term deliverables are published proceedings as well as a team proposal for a large grant.<br/><br/>"
"0753787","Collaborative Research:  The Analysis of Time Series Collected in Experimental Designs","DMS","STATISTICS","08/01/2007","10/29/2007","Hernando Ombao","RI","Brown University","Standard Grant","Gabor Szekely","06/30/2008","$42,982.00","","hombao@uci.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1269","0000, OTHR","$0.00","The research project has three basic goals. The first goal is to estimate the second order properties of a nonstationary process using data from multiple units. A wavelet-based functional model with stochastic subject-specific representation is proposed. A computationally efficient algorithm for estimation will be developed along with theoretical optimal properties of the estimators. While this project is focused on the frequency or spectral domain, the second project will develop methods in the time domain. The goal of this project is to accommodate structural breaks within the time series using mixtures of state-space models. These mixtures are characterized by covariates, treatment and subject-specific effects in both the mixture components and mixing weights. The third project focuses on both temporal and spatial components. In particular, this project integrates spatial information by developing models and corresponding estimators to accommodate data structures in both time and space. The goal is to assess the differences across groups and the subject-specific and covariate effects on the dynamic structure.<br/><br/>The research focuses on an area that has been, for the most part, neglected by the statistics community.  In particular, the question to be addressed is how to best analyze data obtained from experiments when the data are taken sequentially in time and/or space.  Typical examples include experiments where EEGs are taken on various subjects with different diagnoses, or functional magnetic resonance imaging (fMRI) experiments, where subjects with different conditions are requested to perform tasks while in a magnet. The difficulty in analyzing such data is that they are often irregular (nonhomogeneous or nonstationary) in nature.  The techniques for analyzing regular time series are well established, but new techniques are needed to account for the irregularities over time and/or space.  The fact that these data are collected in complex experiments adds another layer of difficulty onto the analysis.<br/>"
"0707074","Theil-Sen Estimators in Semiparametric Mixed Models","DMS","STATISTICS","09/01/2007","03/18/2008","Hanxiang Peng","MS","University of Mississippi","Standard Grant","Gabor Szekely","07/31/2009","$111,160.00","Xin Dang","hpeng@math.iupui.edu","100 BARR HALL","UNIVERSITY","MS","386771848","6629157482","MPS","1269","0000, 9150, 9178, 9251, OTHR","$0.00","<br/>The investigators propose to construct the robust Theil-Sen estimators (TSE's) in general regression models based on multivariate medians, to study their theoretical behaviors, and to explore their practical applications.  The proposed TSE's are given for different regression models based on different multivariate medians. These models include multivariate linear regression, nonparametric regression, semiparametric regression, mixed and additive regression, penalized spline regression, local polynomial regression, wavelet-based smoothers, kriging, etc., while the multivariate medians include in particular those based on different depth functions such as the half space depth, the projection depth, the simplicial depth, the spatial depth, etc. The theory of depth functions can be viewed in part as a multivariate generalization of the univariate rank theory. Depths induce an ordering of all points from a center outward in a high dimensional space because of the lack of the linear ordering in the high dimension. The investigators specifically propose to: 1) Generalize depth functions of vector to matrix argument; 2) Investigate the uniqueness, robustness,  consistency,  and asymptotic normality; 3) Compare asymptotic relative efficiency of the proposed  TSE's with other common estimators(e.g. least squares estimators), compare different multivariate-median-based TSE's,  and compare with other robust estimators;  4) Calculate the complexities, implement algorithms and provide codes that can be accessed by other potential users; 5) Conduct statistical inference, perform simulations, and apply to real applications.  <br/><br/>The Theil-Sen estimator is an estimator of the slope parameter in a simple linear regression model. It is robust to outliers, easy to compute, competitive to the least squares estimator, and has an intuitive geometric interpretation. Despite its many good properties, the TSE is vastly under-utilized because it is developed for a simple linear regression. In recent years semiparametric and nonparametric models have become a popular choice in statistical modeling. They now play an increasing important role in many areas of statistics since they are more realistic and flexible than parametric models. The proposed research will extend the robust TSE's to various useful semiparametric and non-parametric regression models, and will accordingly advance the theory of robust estimation in semiparametric models and provide more robust ways of analyzing data from many applications.  Just like regression analysis which is so popularly used in almost every area of science, these proposed Theil-Sen estimators have wide applications, for example, in astronomy; in remote sensing; in geosciences(e.g. detecting trends of extreme rainfall series); in environmental sciences (e.g. trend analysis for ambient water quality such as detecting seasonal patterns, changes in rainfall, etc so as to assess the relationships among different factors;  to help set water quality guidelines for impacted streams; etc.); in pattern recognitions (e.g. detection of road segments in noisy aerial images), in social sciences;  and so forth.  The proposed research will also involve training of graduate students for future researchers in statistics as well as providing selected undergraduate students with research experience.  <br/>"
"0706971","Inference for Contour Sets","DMS","STATISTICS","07/01/2007","09/10/2008","Wolfgang Polonik","CA","University of California-Davis","Continuing grant","Gabor Szekely","12/31/2011","$253,419.00","","wpolonik@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","0000, OTHR","$0.00","The central objects of this project are contour sets or level sets. These are sets on which a function, such as a regression function or a probability density, exceeds a given threshold. The development of methodology allowing to draw statistical inference about contour sets is the main objective of this project. In one of the <br/>subprojects the investigator is developing confidence regions for contour sets using plug-in estimates based on kernel estimation. These methodological developments are supported by large sample theory showing that the proposed confidence regions are (asymptotically) valid, meaning that they hold the pre-specified confidence level. Two approaches for the construction of confidence regions will be considered. <br/>One is based on the bootstrap methodology, and the other is based on large sample distribution theory for plug-in level set estimates. As for the latter it is shown that the L1-distance between the plug-in estimate and their theoretical counterpart is asymptotically normal when standardized appropriately. In another subproject the investigator is analyzing related novel algorithms for the computation of contour set estimates in high dimensions that have been developed in the literature recently. The focus here is practical applicability.<br/><br/>In the sciences, contour sets are well-known via contour plots that come with almost every scientific software package. Such contour sets are crucial for drawing scientific conclusions in many fields of application. These fields include astronomical sky surveys, flow cytometrie, detection of minefields, analysis of seismic data, image segmentation, as well as anomaly or novelty detection including intrusion detection, detection of anomalous jet engine vibration, medical imaging and EEG-based seizure analysis. The contour sets used in these applications usually depend on observed data. In other words, these sets are random objects, and consequently a statistical analysis of these sets is desirable or even necessary, in order to quantify scientific conclusions. The development of methodology for such a statistical analysis is one of the main topics of this project. No such methodology exists so far, although its availability shows the clear potential to have an immediate impact in many of the fields of application mentioned above. The importance of a statistical understanding of contour set estimates is underlined by a  recent sharp increase in activity in this field. However, so far all the existing work, while important from various points of view, does not allow for quantifying the statistical uncertainty that goes along with estimation of the contour sets. The statistical methodology developed in this project as well as the challenging theory underlying these developments is novel and adds significant insight to a modern field of statistics. The project also impacts the field of statistics via the support of graduate students and their education in a modern field of statistics.<br/>"
"0707157","Stochastic gradient systems: inference and applications","DMS","STATISTICS, COFFES","07/15/2007","07/10/2007","David Brillinger","CA","University of California-Berkeley","Standard Grant","Gabor Szekely","06/30/2011","$390,000.00","","brill@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269, 7552","0000, OTHR","$0.00","<br/>   Data on the motion of objects has become common in many fields of science, engineering, and general human experience. Since the time of Newton mechanical motion has been described analytically by differential equations. Deterministic differential equations theory and application have developed into corresponding study of stochastic differential equations (SDEs). In a variety of practical situations it has been unclear how to select the drift function of an SDE. This research will investigate the use of a potential function to provide a formula for drift. In particular it will  be assumed that the motion of interest is governed by a potential function. The drift is then the potential function's gradient. This structure is referred to as a stochastic gradient system. Being real-valued a potential function is easier to model than a vector-valued drift function. An estimated potential function may be used for simple description, summary, comparison, simulation, prediction, model appraisal, bootstrapping, and employed for estimating quantities of interest. The work will include study of models based on functional stochastic differential equations. This will allow the inclusion of time history in the description. Specific analytic problems to be studied include: unequally spaced times of observation, development of simulation methods to display variability, allowing model appraisal and making predictions.<br/><br/>   The theoretical structure investigated in the particle motion research will be applied to a variety of biological, ecological and other motion situations. In particular Hawaiian monk seal GPS data will be modeled and questions asked by the concerned marine biologists addressed. These data are important because the monk seal is America's most endangered marine mammal. Only about 1300 remain. The study of migration routes of northern elephant seal will also be modeled. This animal is a protected species under the Marine Mammal Act of 1972. Continuing, paths of animals in the Starkey Experimental Reserve in Oregon will be included in the work. These data have been collected to study the management of habitats shared by wild animals, cows and people. Lastly, work will continue on risk models concerning wildfires at the urban-wildfire interface will be modeled employing data from the San Diego County fires of 2003.<br/>"
"0707033","Shrinkage Estimation in Modern Statistics","DMS","STATISTICS","07/01/2007","09/10/2008","Lawrence Brown","PA","University of Pennsylvania","Continuing grant","Gabor Szekely","06/30/2011","$389,440.00","Linda Zhao","lbrown@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, OTHR","$0.00","Technical Description: Contemporary statistics deals with increasingly large and complex collections of data. Complementary styles are emerging for appropriately interpreting such data. One general style involves constructing multi-faceted models for the data. These should be built from relatively simpler components to treat separate aspects of the situation, but with additional modeling parameters at higher levels of the structure in order to provide connections among the components and also to provide flexibility and a measure of robustness to cushion against the possibility that the model may be overly restrictive or partially inappropriate. These additional higher order quantities may be described numerically, qualitatively, or as unknown functions depending on the context. They have various names, such as hyperparameters or latent variables, but irrespective of the terminology and of their subjective interpretation they serve comparable roles relative to the analysis of the data. Adding such higher order quantities to component models has the effect in their analysis of shrinking estimates and other forms of inference toward global averages or patterns. The major thrust of the proposal is to understand the shrinkage phenomenon from a more fundamental, componentwise perspective. Charles Stein's discovery of the advantages of shrinkage in the estimation of independent normal means is among the most surprising and important statistical developments of the preceding century. As the proposal emphasizes this discovery can be interpreted in exactly the framework of independent pieces tied together by higher level quantities. A great deal of theory has been developed over the past 50 years to rigorously understand the consequences of dealing with such a connection, and of using certain structurally appealing techniques such as those labeled as random-effects models"" or hierarchical or empirical objective Bayes analyses. However this theory has failed to adequately address some issues that need to be understood before it can be adequately and properly applied in modern complex settings. For example, classical theory has tended to break groups of parameters into separate blocks and, at best, to shrink separately within each block. But it will be shown how additional shrinkage across blocks can be beneficial, and further research is proposed in this regard. Another classical deficiency that this proposal focuses on trying to correct is that current theory of shrinkage is relatively incomplete and somewhat unsatisfactory with regard to unbalanced data situations involving unequal sampling variances (heteroscedasticity), but these are typical in all highly complex modern data.<br/><br/>General Description: Modern statistical applications often involve massive amounts of data. Conceptual organization and interpretation of such large data sets is a fundamental challenge. It often involves modeling the data as being probabilistically dependent on parameters that control the process being investigated. Classical statistical formulations typically view individual parameters as the primitive structural quantities, rather than taking as primitives ensembles of parameters whose joint modeling characteristics are well understood and controlled. This proposal introduces the ensemble-risk to better address this issue and suggests judging estimators according to their performance relative to this ensemble-risk. Applications of the theory and methodology to be developed in this proposal include nearly all areas of science and technology, but principle applications can be identified in areas of physical and biological sciences such as genomics, climatology and astronomy where large data sets and ensembles of related parameters appear in a natural fashion. As a further instance of the range of potential applications, the orientation and conceptualization in this proposal derives in part from previous data modeling of telephone call-center traffic and internet traffic and intrusions, and a portion of the current proposal involves modeling in a different, complex context involving forecasting housing prices. <br/><br/> <br/><br/> <br/><br/> <br/><br/> <br/><br/> <br/><br/> <br/><br/>"
"0729565","Design and Analysis of Experiments (DAE 2007) Conference","DMS","STATISTICS","09/01/2007","07/10/2007","Manohar Aggarwal","TN","University of Memphis","Standard Grant","Gabor Szekely","08/31/2008","$25,000.00","","maggarwl@memphis.edu","Administration 315","Memphis","TN","381523370","9016783251","MPS","1269","0000, 7556, 9150, OTHR","$0.00","The Design and Analysis of Experiments 2007 Conference (DAE 2007) will be hosted by the University of Memphis in October 31 to November 3, 2007.  This will be the fifth in a series of conferences whose principal aims are to provide support and encouragement to junior researchers in the field of design and analysis of experiments and to stimulate interest in topics of practical relevance to science and industry.  The focus of DAE2007 is on emerging areas of research in experimental design, as well as novel innovations in traditional areas.  The conference will cover topics of importance to the academic and industrial community, such as screening experiments, computer experiments, sequential designs, split plot and split lot designs, designs for biomedical experiments and genetic studies.<br/><br/>The design and analysis of experiments is at the foundation of the scientific method. It is used in applications across all scientific disciplines and engineering. It is used throughout business and industry to improve the reliability of processes and equipment and to identify and understand how process factors affect output. It is used in medicine, especially in the design of clinical trials and biomedical experiments and is also used in many other scientific disciplines. The advancement of statistical theory and methodology over the past years has resulted in the development of sophisticated methods capable of analyzing complex experiments. This has given rise to a demand for efficient designs for these experiments. At the same time, a new generation of enthusiastic and talented researchers in design of experiments is needed in the years to come to carry out original theoretical and applied research in the emerging areas. The junior researchers of today need to be nurtured and encouraged to assume this role. For this purpose, DAE2007 will bring together senior and junior researchers and practicing statisticians from universities and industry to discuss research and the future directions of the field.  Additionally, DAE2007 will strengthen collaborative research between academia and industry and will broaden the participation of underrepresented groups, mainly minorities and women. <br/>"
"0707021","Spectrum Estimation for Spatial Processes","DMS","STATISTICS","06/01/2007","03/15/2007","Tailen Hsing","OH","Ohio State University Research Foundation -DO NOT USE","Continuing grant","Gabor Szekely","04/30/2008","$67,287.00","Chunfeng Huang","thsing@umich.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","A new, unified approach for estimating spectral densities of spatial processes is proposed. The theoretical properties as well as practical implementation issues of this approach will be thoroughly explored. The completion of this project will provide powerful new tools for kriging, or optimal prediction in certain situations in spatial data analysis. Based on the relationship between the generalized covariance and the spectral density, a new approach is formulated for estimating the spectral density in terms of solving a regularized inverse problem. The generalized covariance can then be estimated thorough the estimated spectral density, which paves the way for kriging. The regularized inverse problem is solved in a reproducing kernel Hilbert space essentially as a constrained optimization problem. A number of crucial issues arise from that. Candidate procedures based on the ideas of unbiased-risk and generalized cross-validation will be studied for the determination of the optimal smoothing parameters from data. Theoretical properties, including mean squared error bounds and asymptotic properties, will be investigated to assess the performance of the approach. Efficient computational algorithms will be sought to overcome the difficulties brought by the high-dimensional nature of the data.<br/><br/>The research in this project offers a new perspective on the analysis of spatial data. The kind of data that the investigator has in mind are data observed at multiple spatial locations and possibly also at multiple time points. The general goals are to identify the data generation process and to make predictions beyond the spatial-temporal region where data are available. One of the keys in such problems is to understand the dependence relationship between the various pieces of the data. The approach in this project targets this problem for a broad class of models. Potential applications of the new theory and methodology exist in numerous contexts, including the environment, geography, and sensor networks.<br/><br/><br/>"
"0707055","From Information Scaling to Regimes of Statistical Models of Natural Image Patterns","DMS","STATISTICS","07/01/2007","08/29/2008","Yingnian Wu","CA","University of California-Los Angeles","Continuing grant","Gabor J. Szekely","12/31/2010","$389,999.00","Song-Chun Zhu","ywu@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","0000, OTHR","$0.00","The focus of the proposed research is to develop statistical models for image patterns of natural scenes, guided by the study of information scaling, i.e., the change of statistical properties of the image data over the scaling process. The proposed research will integrate two streams of statistical and mathematical theories in image modeling andrepresentation: (1) spatial statistical models such as Markov random fields and Gibbs distributions originated from statistical physics; and (2) representation and coding theories including wavelets and sparse coding originated from harmonic analysis. At present the two areas are studied almost in mutual isolation, with random field models working (better) in stochastic (high-entropy) regime while the coding theories working (better) in structured (low-entropy) regime. The PIs identify a fundamental concept, the image entropy rate, whose scaling behavior connects the two regimes, namely the high-entropy regime of texture patterns and low-entropy regime of geometric patterns. More important, the connection reveals a most crucial regime in between, that is, the mid-entropy regime of object patterns. The PIs propose to integrate the three regimes within a unified theoretical framework, and this integration will lead to powerful models and algorithms for learning and recognition of natural image patterns. The issue of scale and modeling is important in many scientific areas. As a biological application, the proposed research also includes modeling and analysis of 1D ChIP-chip data, based on the work done by the PI and collaborators. ChIP-chip is a technology for isolation and identification of genomic sites occupied by specific DNA binding proteins in living cells. This technology is playing an important role in studying gene regulation. The proposed work will strengthen existing methods for analyzing such data.<br/><br/>Images of daily environments contain a bewildering variety of patterns and objects, such as trees, foliage, grass, rivers, houses, cars, human figures, faces, dogs, etc. The images are large arrays of numbers. In order to teach computers to automatically learn these patterns and recognize them from such image data, it is crucial to understand the mathematical and statistical properties of natural images and to develop simple but general statistical models as well as efficient computational algorithms for representing and recognizing these patterns. The goal of the proposed research is to study the information contents of natural images and to develop such models and algorithms within a unified framework. The proposed research will make useful contributions to both statistics and computer vision. The goal of the latter is to teach computers to see as accurately and effortlessly as human beings do.<br/><br/>"
"0644838","CAREER: Nonparametric likelihood, estimating functions, and causal inference","DMS","STATISTICS","03/15/2007","03/05/2007","Zhiqiang Tan","MD","Johns Hopkins University","Continuing grant","Gabor J. Szekely","09/30/2007","$152,946.00","","ztan@stat.rutgers.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","MPS","1269","0000, 1045, OTHR","$0.00","Likelihood introduced by Fisher is a central concept in statistics both from frequentist and Bayesian viewpoints. The  research project is to advance a nonparametric likelihood approach that retains both the original meaning and the inferential power of Fisher's likelihood, and at the same time to construct estimating functions geared towards point estimators and Wald-type confidence intervals. The research studies semiparametric models for two-sample and regression problems in the absence and in the presence of missing data. The project also investigates statistical tools for causal inference in longitudinal studies with time-dependent treatments and confounders. The investigator's education plan involves designing a course on nonparametric likelihood and estimating functions with applications to semiparametric models and causal inference; supervising students with various backgrounds; establishing a causal inference working group as a research and educational platform; and organizing causal inference workshops for researchers and students to facilitate communications and collaborations.<br/><br/>The research will improve the validity and accuracy of inferences about environmental exposures, medical treatments, behavioral interventions among others in environmental, biomedical, and socioeconomic studies. The educational activities will help students from various backgrounds and researchers from various disciplines to acquire state-of-the-art statistical ideas and methods for empirical investigation and discovery.<br/>"
"0707160","Study of dimension reduction methods driven by large scale biological data","DMS","STATISTICS","09/01/2007","08/17/2007","Ker-Chau Li","CA","University of California-Los Angeles","Standard Grant","Gabor J. Szekely","08/31/2011","$139,999.00","","kcli@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","0000, OTHR","$0.00","The growing awareness of the importance of dimension reduction in bioinformatics has led to the development of new techniques. Various extensions of sliced inverse regression(SIR) have appeared. More recently a new way of studying the relationship between variables in a complex data system, called liquid association (LA), was proposed. LA describes how variation in the pattern of association between a pair of variables, including its sign and strength, is mediated by a third variable from the background. In this proposal, the investigators  will (1) develop hybrid dimension reduction methods that blend in ideas from PCA (principal component analysis), K-mean, SIR, PHD (Principale Hessian Direction), LA and a variety of clustering methods; (2) provide effective dimension reduction tools for incorporating clinical or other phenotype data involving life time which are usually subject to censoring; (3) investigate related statistical inference issues concerning false positives; (4)  investigate the pattern of cellular coordination at the functional module level; (5)  develop LA based methods for variable selection; (6)  provide better insight to intrinsic nonlinearity in marginal Gaussian models.<br/><br/>The recent rapid growth in the public repertoire of biological data and knowledge resources has been astonishing. This includes the completion of genome sequencing for human and many species, the stride in the SNP detection and international HapMap project, the accumulation of full genome microarray gene expression data under a number of conditions for numerous organisms and tissues, the identification of the high density genetic markers, protein-protein interaction and complexes, as well as the availability of various gene annotation websites featuring both functional and structural information of the gene products, their biological roles and relevance to disease studies. Such open data resources hold the promise of benefiting numerous projects aiming at solving detail genetic profiles predisposing to complex diseases and their trait components. However, researchers are also facing the insurmountable difficulties due to the enormous complexity of data structure and exceedingly high dimensionality. To succeed, it is critical to continue the research on developing new computational arsenals. Modeling through probabilistic and statistical reasoning has found numerous compelling applications. <br/><br/><br/>"
"0707106","Semiparametric regression for correlated data","DMS","STATISTICS","07/01/2007","06/13/2007","Tatiyana Apanasovich","NY","Cornell University","Standard Grant","Gabor J. Szekely","04/30/2009","$91,403.00","","apanasovich@gwu.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269","0000, OTHR","$0.00","There has been growing interest in developing innovative statistical models for dependent data. The project aims to address the following issues which have arisen in modeling correlated data: extra zeros in count data, non-linear functional relationships and measurement errors.  Frequently it is of interest to make marginal inference about trends and effects of explanatory variables on correlated counts. Failure to account for the extra zeros may result in biased parameter estimates and misleading inferences. The investigator proposes a generalization of the standard zero-inflated regression model to the correlated data case, allowing for a Heckman-type selection process. Measurement Error Models offer coverage of estimation for situations where the model variables are observed subject to measurement error. The second part of the project focuses on developing Semiparametric Measurement Error Model for dependent data using two approaches: the first is based upon the idea of Monte Carlo corrected scores and the second is a generalization of the SIMEX (simulation-extrapolation) method to general semiparametric models.<br/><br/>Semiparametric regression is concerned with the flexible incorporation of non-linear functional relationships in regression analyses. The investigator is particularly interested in correlated data which arise in a variety of settings in many areas of applications: Biology, Genetics, Bioinformatics, Biostatistics, Medicine, Econometrics, Engineering and Sociology. In many practical situations the attribute or event of interest is rare and/or other variables preclude observation of an event, consequently a higher proportion of the counts than implied by standard models may equal zero. Part of the focus of this proposal is to develop semiparametric models for the correlated data with extra zeros. When there is an uncertainty in measuring covariates, the usual regression estimators are biased and when the measurement error is substantial, alternative procedures are necessary. Part of the emphasis of the proposal is on developing semiparametric measurement error regression models for dependent data. The statistical methodology to be developed will be circulated to the statistical community in academia and industry through series of papers and publicly available programming code. On the educational front, some of the research material in this proposal will be incorporated in courses at the undergraduate and graduate level. Some projects will also serve as dissertation topics for Ph.D. advisees and will therefore play an important role in the training of future statisticians.<br/><br/>"
"0706997","Spatial Prediction of Surfaces in the Presence of Uncertainty","DMS","STATISTICS","09/01/2007","08/24/2007","Noel Cressie","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","Gabor J. Szekely","08/31/2009","$110,000.00","","ncressie@uow.edu.au","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, OTHR","$0.00","Surfaces, or spatial fields, may be known imperfectly or incompletely.  In the presence of such uncertainty, a hierarchical spatial statistical approach to making inference on these surfaces, or important summaries of them, offers a coherent approach. It separates the uncertainties into components, namely those represented in a data model, those arising from a scientifically motivated process model, and those due to a lack of perfect knowledge of the parameters of the models. Inference is Bayesian (either fully Bayesian or empirical Bayesian), and hence it is based on the posterior distribution of the unknowns given the data. A core methodology in this project is the use of spatial random effects (SRE) models to represent the spatial dependence in the process model.  These may simply provide a flexible class of random surfaces or they may include effects from physical laws of the phenomenon under study. The methodological research lies at the interface of two important properties: statistical optimality (obtained from summaries of the posterior distribution)and computational speed.<br/><br/>The problem addressed in this project, namely spatial prediction (or mapping) of surfaces, is fundamental to large areas of science and engineering. It is recognized that Geographic Information Systems (GISs) are powerful tools that have superior database and visualization tools for mapping. An important function of a GIS is to convert georeferenced data into spatially coherent images that convey a lot of information visually. The proposed research adds uncertainty (in data, models, parameters) into spatial modeling and shows how maps can be made optimally. Variability measures associated with the optimal maps allow two maps to be compared and statistically meaningful changes to be detected.<br/>"
"0706786","Long and Short Memory Stationary Processes: Prediction and Estimation","DMS","STATISTICS, COFFES","09/01/2007","07/10/2007","Murad Taqqu","MA","Trustees of Boston University","Standard Grant","Gabor J. Szekely","08/31/2011","$199,812.00","Mamikon Ginovyan","murad@math.bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","MPS","1269, 7552","0000, OTHR","$0.00","The project focuses on prediction and estimation problems for second order discrete - or continuous-parameter stationary random processes. These processes may display short, intermediate or long memory.<br/>The project investigates two problems: (1) The prediction problem. Having observed part of the past, one wishes to predict the future. The goal is to describe the rate of decrease of the prediction error as the number of past observations increases. This rate will depend on the dependence structure of the underlying random process and the smoothness properties of its spectral density function. (2) The estimation problem: Under suitable assumptions on the underlying random process, the goal is to investigate the statistical properties of unknown estimators parameters characterizing the process.<br/><br/>Long memory processes are observed in many areas of applications, such as long term global temperature records, financial asset prices and many others. The study of the probabilistical and statistical properties of such process is a chanllenging problem. However, such a study will provide better understanding of the underlying phenominon and provide better prediction of the future -- all very important tasks."
"0706886","Adaptive Regression via Basis Selection from Multiple Libraries","DMS","STATISTICS","07/01/2007","03/24/2009","Yuedong Wang","CA","University of California-Santa Barbara","Continuing grant","Gabor J. Szekely","06/30/2011","$150,448.00","","yuedong@pstat.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","MPS","1269","0000, OTHR","$0.00","The objective of this research is to develop more adaptive non-parametric and semi-parametric methods. The approach is to use multiple libraries and allow fusion among them in the selection process. Data-driven estimates of model complexities will be used to correct bias incurred by adaptive model selection. New model selection criteria will be developed to allow basis functions in different libraries to compete on an equal footing. The general covariance penalty will be developed for extended linear models. Since model complexities are estimated and incorporated at each step of the selection procedure, the proposed methods are fully adaptive in the sense that they dynamically adjust their strategy to take into account the behavior of the function to be estimated. The proposed procedures are general in the sense that they can be applied to combinations of any generic libraries which may include Fourier, truncated polynomial, spline and wavelet bases. The methods also combine variable selection with basis selection in a semi-parametric model.<br/><br/><br/>Increasingly complex data sets are being collected in many fields. Powerful statistical methods are essential for the extraction of as much information as possible from the data. Advances in computational power have afforded modelers unprecedented opportunities to exploit possible hidden structure using non-parametric and semi-parametric modeling techniques. The novel methodologies developed in this proposal constitute advances in adaptive non-parametric and semi-parametric modeling procedures. The methods and software are quite general which can be applied to a number of different fields including biological sciences, economics, engineering, geological and environmental sciences, information technology, health and medicine, physical sciences, and social sciences. The proposed activities involve training of graduate students for future researchers in statistics. The P.I. is engaged in several collaborations with investigators in the environmental, medical and social sciences. Some proposed methods will be applied to analyze data from ongoing and future experiments. The procedures will be implemented in R and will be contributed to the Comprehensive R Archive Network."
"0705312","Collaborative Research: Generalized Variable Selection With Applications To Functional Data Analysis And Other Problems","DMS","STATISTICS","07/01/2007","05/16/2007","Gareth James","CA","University of Southern California","Standard Grant","Gabor J. Szekely","06/30/2010","$99,768.00","","gareth@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","1269","0000, OTHR","$0.00","When variable selection is performed in situations where the number of predictors is significantly larger than the number of observations, one generally assumes sparsity in the regression coefficients, i.e., most of the coefficients are zero. However, there turn out to be many practical applications where, rather than the parameters being sparse, certain predefined functions of the parameters are sparse. This is referred to as <br/>""Generalized Variable Selection"" (GVS). Specifically, the investigators study four important applications of GVS in areas as diverse as functional regression, principal component analysis (both standard and functional), multivariate non-parametric regression, and transcription regulation network problems for microarray experiments.<br/><br/>The investigators have direct connections in many fields outside statistics such as Biology, Finance, Manufacturing, Marketing, Medicine and Physics. The investigators believe that statisticians can, and should, make important contributions in all these areas. With the advent of new technologies, such as bar code scanners and microarrays etc., enormous data sets are becoming increasingly common in these and many other fields. Such vast quantities of data have made it important to develop statistical methodologies that can produce sparse and interpretable solutions. The investigators aim to systematically develop software to implement the proposed methods through free software packages, like R, and then make them readily available and publicize them in all these fields. The investigators believe that, because of the interpretive power of their proposed methods, once the software is available, it will be widely utilized."

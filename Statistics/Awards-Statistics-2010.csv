"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1016239","Conference on resampling methods and high dimensional data","DMS","STATISTICS","03/15/2010","04/07/2010","Soumendra Lahiri","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","02/28/2011","$10,000.00","Jianhua Huang","s.lahiri@wustl.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, 7556, OTHR","$0.00","This project focuses on organizing a 2-day international conference entitled Conference on resampling methods and high dimensional data at the Texas A & M university, College Station, TX, March 25-26, 2010. The conference provides a unique platform for bringing together researchers working in two cutting edge areas of current research in statistics, namely, Resampling methods for complex data structures and Inference for high dimensional data, in order to facilitate the exchange of research ideas on the two topics and to further the development of these booming fields. The meeting is co-sponsored by the National Institute of Statistical Sciences and by the Section on Nonparametric Statistics, American Statistical Association. The conference has an international program committee chaired by Professors S.N. Lahiri (Texas A & M), and an Advisory Committee, consisting of Professors R.J. Carroll (Texas A & M), Bradley Efron (Stanford), and Hans R. K¨unsch (ETH, Zurich, Switzerland).<br/><br/>The two-day conference Conference on resampling methods and high dimensional data brings together top and junior researchers to define and expand the research frontiers of two highly active research areas of statistics and probability that deal with complex data structures. There are a number of areas of science and engineering, such as bio-informatics, brain imaging, econometrics,electrical engineering, finance, meteorology, etc., where such data appear frequently and which directly benefit from advances in these topics. The funding provides support for selected groups of participants including graduate students, minorities, and young researchers who can not attend the conference otherwise, thereby promoting human resource development."
"1007457","Sampling from Distributions with Intractable Integrals","DMS","STATISTICS","08/01/2010","05/07/2012","Faming Liang","TX","Texas A&M Research Foundation","Continuing Grant","Gabor Szekely","07/31/2013","$100,000.00","","fmliang@purdue.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","","$0.00","During the past five decades, Markov chain Monte Carlo (MCMC) methods have been developed as a versatile and powerful tool for scientific computing.  However, as known by many researchers, conventional MCMC methods suffer from the inability to sample from distributions with intractable integrals. The goal of this project is to develop some innovative Monte Carlo algorithms which are capable of sampling from distributions with intractable integrals. To achieve this goal, the PI proposes a new population Monte Carlo algorithm---Monte Carlo dynamically weighted importance sampling (MCDWIS). In simulations, MCDWIS replaces the ratio of intractable integrals by its Monte Carlo estimate, and the bias introduced thereby is counterbalanced by giving different weights to new samples produced. MCDWIS allows for the use of Monte Carlo estimates in MCMC simulations, while leaving the target distribution invariant with respect to important weights. Unlike auxiliary variable MCMC methods, MCDWIS avoids the requirement for perfect samples, and thus can be applied to many statistical models for which perfect sampling is unavailable or very expensive. As discussed in the proposal, MCDWIS can also be used to sample from incomplete posterior distributions for missing data and random effects-related models (e.g., generalized linear mixed models), which are traditionally treated with the expectation-maximization (EM) or Monte Carlo EM algorithms. In addition to providing a fully Bayesian analysis for these models, the MCDWIS can potentially overcome, due to its self-adjusting mechanism, the local-trap problem suffered by the EM and Monte Carlo EM algorithms. In this proposal, the PI also proposes an importance sampling-targeted stochastic approximation Monte Carlo algorithm, the so-called importance stochastic approximation Monte Carlo algorithm, which can be used for Bayesian inference for the models with intractable normalizing constants.<br/><br/>The intellectual merit of this project is to provide some innovative computational methods, which are expected to play a major role in statistical inference for an important class of scientific models, including random graph models used in social network analysis, autonormal models used in spatial data analysis, autologistic models used in disease mapping, and generalized linear mixed models used in biomedical data analysis, among others. Successful inferences of the models will enhance people's underderstanding to the underlying natural, social, or biological systems. This project will have broader impacts in both communities of statistical methodology and scientific computing. The research results will be disseminated to these communities via direct collaboration with researchers in other disciplines, conference presentations, books, and papers to be published in academic journals.  The project will have also significant impacts on education through direct involvement of graduate students in the project and incorporation of results into undergraduate and graduate courses."
"1007594","Simultaneous Confidence Regions for Functional Data Analysis: Theory and Methods","DMS","STATISTICS","09/01/2010","08/24/2010","Lijian Yang","MI","Michigan State University","Standard Grant","Gabor Szekely","08/31/2013","$159,986.00","","yang@stt.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","","$0.00","This research project provides simultaneous confidence regions for various functional features in functional data analysis (FDA), with asymptotic theory and guide to practical implementation. Specifically, asymptotically correct confidence regions will be constructed for (1) the mean function of functional data and the coefficient function in varying coefficient longitudinal regression model; and (2) the covariance function of functional data and the regression function in functional linear model. For the simpler functions in (1), the investigator will employ both regression spline and local polynomial methods in order to establish rigorous asymptotic theory for both sparse and dense function data. Results on partial sum strong approximation by Brownian motions and advanced extreme value theory for sequences of non-stationary Gaussian processes will be applied to obtain distributional properties of the maximal deviation processes. For the more complicated functions in (2), the investigator will propose two-step estimators and show that it is asymptotically as efficient as some ?infeasible? analogs. Asymptotic distributions for maximal deviations are established for the ?infeasible estimators? which are then inherited by the two-step estimators.<br/><br/>Functional data, also known as curve data, consist of collections of digitally recorded curves or surfaces, often with random errors. Such data abound in virtually all scientific disciplines, including but not limited to, climatology, clinical studies, epidemiology, evolutionary biology and food engineering/science. The need to draw information out of a sample of curves, coupled with the unleashing of modern computing power, has made functional data analysis (FDA) one of the most active areas of contemporary statistics research. While multivariate statistics is about unknown vectors and matrices, FDA concerns unknown curves and surfaces, which is most naturally done with confidence regions. The methods developed by the investigator fill a major gap in the current FDA methodology, which lacks procedures to make conclusions on an entire curve with quantifiable uncertainty.  Codes written in common software packages such as Matlab or R will be freely distributed so practitioners from academia and industry for analyzing functional data in real time, with own chosen significance levels. Completing this project depends crucially on several capable Ph. D. students working under the investigator?s supervision, so state-of-the-art research is integrated with the training of graduate students as future researchers, consistent with NSF's education goal."
"1148545","Collaborative Research: Fence Methods for Complex Model Selection Problems","DMS","STATISTICS","02/28/2010","09/21/2011","Jonnagadda Rao","FL","University of Miami School of Medicine","Standard Grant","Gabor Szekely","07/31/2012","$23,066.00","","js-rao@umn.edu","1400 NW 10TH AVE","MIAMI","FL","331361000","3052843924","MPS","1269","0000, OTHR","$0.00","Many model search strategies involve trading off model fit with model <br/>complexity in a penalized goodness of fit measure. Asymptotic properties <br/>for these types of procedures in some conventional situations, such as <br/>regression and ARMA time series have been studied. Yet, such strategies <br/>do not always translate into good finite sample performance. Furthermore, <br/>such standard model selection procedures encounter difficulties for <br/>nonconventional model selection problems as well. This project aims at <br/>developments of a new model selection strategy, called fence methods, in <br/>following four major areas of methodology research and applications: (i) <br/>development of adaptive fence methods for high dimensional and complex <br/>model selection problems using the idea of restricted maximum likelihood; <br/>(ii) development of data adaptive fence methods for nonparametric model <br/>selection problems such as penalized smoothing spline estimation; (iii) <br/>development of fence methods for quantitative trait loci (QTL) mapping; <br/>and (iv) development of user-friendly standalone software for implementing <br/>the fence methods. <br/><br/>The fence idea is generally based on building a statistical fence, or <br/>barrier, to carefully eliminate incorrect models. This is done by <br/>determining which models are within variation of a goodness-of-fit <br/>measure of an anchor model. Once the fence is constructed, the optimal <br/>model is selected from amongst those within the fence according to <br/>a criterion which can be made flexible. For example, the criterion can <br/>incorporate scientific or economic concerns. The adaptive fence method <br/>may be viewed as comparing signals with noises to come out with an optimal <br/>decision supported by the data. Given such a wide spectrum of models that <br/>can be handled, the range of applications seems enormous. Of particular <br/>interests are applications in human genetics, medical research and surveys. <br/>To facilitate such translational research, the investigators plan to freely <br/>disseminate available computer software to implement the fence methods. <br/> <br/>"
"1007527","High Dimensional Statistical Theory for Sparse Regularization","DMS","STATISTICS","07/01/2010","05/03/2012","Tong Zhang","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","06/30/2014","$250,000.00","","tozhang@illinois.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","High Dimensional Statistical Theory for Sparse Regularization<br/><br/><br/>The investigator studies statistical machine learning with sparse regularization in the setting of high dimensional statistical estimation. A number of research directions will be explored, including improved performance bounds for sparse regularization, new sparse learning formulations, and the statistical theory for several important computational algorithms.<br/><br/><br/>In the information age, more and more data become available electronically, and these data need to be automatically analyzed by computers in order to filter out the most important information. Statistical machine learning is the main technical tool for analyzing electronic data. Many modern applications involve data in very high dimension that cannot be handled by traditional algorithms. Sparse regularization is an important new statistical machine learning technique that can deal with this issue by effectively identifying the most significant patterns from a vast amount of available information. This research develops new sparse regularization algorithms that will significantly enhance the capability for modern computer systems to find critical information from available electronic data."
"1007652","REML in time series models: Applications to unified inference in moderate and near integrated autoregressions, dynamic panels, cointegrated systems and non-linear IV regressions","DMS","STATISTICS","05/15/2010","05/12/2010","Willa Chen","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","04/30/2014","$143,047.00","","wchen@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","","$0.00","The proposed research demonstrates how the restricted likelihood approach can resolve several well known estimation and inference problems. For the inference problem of a near integrated process with deterministic components, this research provides a framework for unified inference based on the chi-squared distribution for autoregressive processes with deterministic components regardless of whether they are stationary, moderate integrated, near integrated or integrated processes. A weighted least  squares approximate restricted likelihood is provided for multivariate time series models so that a computationally simple method with attractive theoretical properties is available. The proposal also includes using the restricted likelihood for the incidental parameter problem in dynamic panel data model. Research is also planned to explore the restricted likelihood for non-linear models for which there do not seem to be any results available. <br/><br/>This research will help to build a bridge between statistics and economics.  Restricted likelihood has existed for almost four decades and been routinely used in linear mixed models. While the restricted likelihood has historically been used for bias reduction, recent research has also shown that the restricted likelihood based likelihood ratio test statistic has nice properties in nonparametric models. However, this large body of work on the restricted likelihood has largely ignored its potential use in time series models with only very few exceptions including the PI's research under the previous grant. This proposal is a continuing dialogue with researchers and practitioners in statistics as well as econometricians on the applications of restricted likelihood. A number of projects are presented with the aim of facilitating the application of restricted likelihood in the most widely used econometric models."
"1007618","A new approach of statistical modeling and analysis of massive spatial data sets","DMS","STATISTICS","07/15/2010","04/09/2012","Huiyan Sang","TX","Texas A&M Research Foundation","Continuing Grant","Gabor Szekely","06/30/2014","$179,660.00","Jianhua Huang","huiyan@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","","$0.00","This proposal focuses on the development of a new approach to tackle the challenges in statistical modeling and analysis of massive spatial data sets. Due to the complexity and enormousness of the data, conventional statistical methods for analyzing, modeling and making inference of large data become indispensable in current research and application of environmental, earth and biological sciences. Two approaches have been recently proposed but have their own drawbacks. One approach, based on low rank approximation of covariance functions, works well to model large scale spatial variability but may fail to adequately capture small scale behavior. The other approach, based on sparse matrix approximation, appears to work better when the spatial data have only relatively small scale dependence. The investigators propose a new approach that combines these two approaches to provide a high quality approximation to the covariance function at both the large and small spatial scales. Specific research projects will include parameter estimation of various geostatistics models, data imputations for missing satellite measurements, spatial-temporal modeling for detection and prediction of global climate change, multivariate spatial models for multivariate satellite measurements, and non Gaussian spatial models for characterization of extreme environmental events. <br/><br/>With rapid advances of science and technology, large amounts of spatial data are generated from various sources including remote ground sensors, satellite images, scientific climate computer models, Geographic Information Systems, and public health and spatial genetics. The proposed methods will make it possible to analyze, model and make inferences about massive spatial data sets and benefit researchers and practitioners in environmental, earth and biological sciences. Research results will be disseminated through collaborative work, academic presentations, and journal publications. Web pages will be created to enable quick access to user-friendly and accessible software implementations of new methods as well as technical reports and relevant references."
"1007420","Analysis of incomplete data in quantile regression and semiparametric models","DMS","STATISTICS","09/01/2010","05/10/2010","Huixia Wang","NC","North Carolina State University","Standard Grant","Gabor Szekely","08/31/2013","$140,000.00","","judywang@gwu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","","$0.00","The principal investigator (PI) aims to develop new statistical methodology for analyzing incomplete data using regression of quantiles where causes of incomplete data are due to either censoring or measurement error. The research is  challenging mainly because quantile regression aims to avoid parametric error distributional assumptions so the standard likelihood-based methods cannot be used. The PI will focus on three different but related problems. First, new approaches to estimation based on corrected scores will be developed to account for a class of measurement errors in the covariates. Second, an index-based estimation method will be proposed for censored quantile regression to accommodate high dimensional covariates. Penalization methods will be developed for variable selection. The third problem focuses on data with covariates subject to fixed censoring. To improve the efficiency over estimators from complete samples, a new multiple imputation approach based on censored regression of quantiles will be developed. The new imputation method can be used to improve statistical inference for not only quantile regression but also more general regression problems.<br/><br/>The proposed research will have broad and valuable applicability in various fields, for instance, in microarray studies where the gene expression data are often measured with errors, in survival studies where random censoring is common, and in environmental and geological studies where measurements are often subject to fixed censoring. For example, in contrast to conventional statistical methods, quantile regression models can help discover heterogeneous effects of drug treatments on survival times of both high and low risk patients. The project will integrate research and education by developing advanced topics courses, mentoring students especially those from under-represented groups.<br/>"
"1007634","Statistical learning with high-dimensional structured data: a regularized boosting approach","DMS","STATISTICS","08/01/2010","07/20/2010","Lifeng Wang","MI","Michigan State University","Standard Grant","Gabor Szekely","07/31/2013","$99,792.00","","wang@stt.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","","$0.00","The proposed project aims to develop new statistical learning theories and methodologies for the analysis of high-dimensional data with complex structures. The central problem is how to effectively incorporate the a priori information on data structures to reduce statistical uncertainty in high-dimensional learning.  In particular, the PI will investigate: a) a novel general framework based on regularized boosting for flexible high-dimensional modeling adaptive to data structures, and the associated learning theory; b) a new regularized boosting method that performs bi-level variable selection in presence of grouping structures in the predictors; c) a new boosting method for function estimation and subnetwork selection in presence of  graphical structures in the predictors.<br/><br/>With advances of technology, high-dimensional data analysis becomes increasingly important in various scientific disciplines, including genomics, medicine, engineering, environmental studies, and economics. Conventional statistical methods suffer from the high-dimension, low sample size, as well as the high correlation among these data. For such ill-posed problems, it is crucial to incorporate the complementary a priori structural knowledge in data analysis in order to achieve more robust models and more consistent discoveries. For example, in many genomic researches, the information on data structures, such as grouping or graphical structures of the genes, are widely available in forms of gene pathways and regulatory networks. The investigator's work will contribute new statistical methods and computational tools, in forms of free software, to efficiently integrate these structural information in high-dimensional modeling. It will facilitate the analysis of high-dimensional data to achieve a substantial improvement on predictive accuracy, as well as to build more stable and interpretable models. It will also promote collaborations between statisticians and scientists from other fields. Moreover, the proposed project includes an educational program that involves development of new courses, mentoring undergraduate and graduate students and exposing them to the state-of-the-art research in this project."
"1007507","Optimal Design for Non-Linear Models, With an Emphasis on Categorical Data","DMS","STATISTICS","06/01/2010","05/04/2012","John Stufken","GA","University of Georgia Research Foundation Inc","Continuing Grant","Gabor Szekely","05/31/2014","$219,388.00","","jstufken@gmu.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","","$0.00","The investigator identifies optimal and efficient designs for non-linear models. The focus is on (1) generalized linear models (GLMs) for binary data or count data; and (2) non-linear models for Event Related functional Magnetic Resonance Imaging (ER-fMRI) experiments. For the first of these, recent results are mostly restricted to models with a single covariate. The investigator studies common GLMs, such as logistic, probit and loglinear models, with multiple covariates and higher order terms. He develops novel theory and computational tools for identifying locally optimal designs under various optimality criteria as well as for identifying robust designs. For the second problem, the investigator identifies optimal and efficient designs under more realistic non-linear models for the combined objectives of estimation of the hemodynamic response function (HRF) and detection of brain activity. <br/>Traditionally, two separate linear models have been used for these disparate objectives. The use of a single non-linear model for modeling the hemodynamic response facilitates the simultaneous pursuit of both objectives. This approach provides not only a more natural formulation of design optimality criteria, but also results in better designs for ER-fMRI experiments.<br/><br/>Binary data and count data are very common in many scientific fields, such as drug discovery, clinical trials, social sciences, marketing, etc. While models and methods of analysis for such data are well established, the study of optimal design for the efficient use of available resources lags considerably. For example, when planning a dose-response study, it is important to know which dose levels of a drug should be used in the study, and how many subjects should be assigned to these levels in order to get the most information for questions that are of scientific interest. Recent advances and new tools developed by the investigator and his collaborators make it possible to derive optimal designs for a variety of commonly used models. For a second part of the project, the investigator finds efficient designs for ER-fMRI experiments. These experiments are part of a cutting edge approach for studying brain activity caused by certain simple tasks. A subject in an MRI scanner is presented with a series of tasks, each of them repeated multiple times, and the hemodynamic response is measured. The investigator identifies optimal and efficient orders for presenting the tasks to a subject in order to gain as much information as possible for the scientific goals of the experiment."
"1007612","North American Meeting of New Researchers","DMS","STATISTICS","03/15/2010","04/07/2010","Samiran Sinha","TX","Texas A&M Research Foundation","Standard Grant","Gabor Szekely","02/28/2011","$26,858.00","","sinha@stat.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","0000, 7556, OTHR","$0.00"," This proposal describes plans for the Thirteenth Meeting of New<br/>Researchers in Statistics and Probability to be held in the campus of<br/>British Columbia, Vancouver, during July 27--July 30, 2010. Housing, meals,<br/>and conference facilities will be provided on campus. This conference<br/>series is organized by and held for junior researchers. The primary objective<br/>is to provide a much needed venue for interaction among new researchers.<br/> In contrast with large meetings, this conference will be restricted<br/> in size, with a target of 85-90 participants.  Sessions will be followed<br/> by panel discussions  and breaks to facilitate interactions.<br/> Participants will be statisticians and probabilist who have<br/>received their Ph.D. within the past five years or are expecting to receive<br/>their degree within the next year.  Each participant will present a<br/>talk or poster.  Topics will range across a variety of areas in statistical<br/>research from theory and methods to applications. Senior speakers will<br/> discuss topics of particular interest to new researchers.<br/>Panel discussions during the conference will cover the topics of journal<br/>publications, opportunities in statistics laboratories and industry,<br/>funding, mentoring, and teaching.<br/><br/><br/>The intellectual merit of this proposal is based on its human resource<br/>development. The professional development of new researchers is<br/>stimulated by promoting their interaction, creating networks of<br/>colleagues (which is usually difficult at a large meeting, such as<br/>the Joint Statistical Meeting). Participants present their work<br/>in a smaller, more controlled conference environment, maximizing<br/>their intellectual interaction and growth. The primary goals of the<br/>conference are professional education of new researchers and to help<br/>create a network among these junior researchers, both are vital parts<br/>of professional growth.  Women, minorities, and the disabled are<br/>explicitly encouraged to attend, and we have an established track<br/>record of attracting participants from these categories.<br/>"
"1007703","Long range dependence and resampling methodology for spatial data","DMS","STATISTICS","05/15/2010","03/09/2012","Soumendra Lahiri","TX","Texas A&M Research Foundation","Continuing Grant","Gabor Szekely","03/31/2013","$250,000.00","","s.lahiri@wustl.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","","$0.00","This project concentrates on (i) developing limit theory for a class of long range dependent spa­tial processes under various spatial sampling designs, including the case of irreguraly spaced data-sites, which is encountered frequently in spatial applications; (ii) developing new resampling methodology for spatial data under both short-and long-range dependence that are immune to the e.ects of the curse of dimensionality, (iii) developing Edgeworth expansion theory for spatial data for both regularly and irregularly spaced cases, and (iv) investigating higher order properties of resampling methods for spatial data and study their higher order properties. <br/><br/><br/>The proposed project aims to make important theoretical and methodological contributions to several critical areas of spatial statistics that have a wide of potential applications but the state of the current literature on these areas is very sparse. In addition to advancing the state of statistical methodology for spatially referenced data, the proposed research would also bene.t many other areas of sciences, such as Astronomy, Hydrology, Geology, Economics, Atmospheric Sciences, etc. where spatial data exhibiting di.erent forms of dependence are known to occur naturally, and model-free statistical methods such as those proposed in the project play an important role in their analysis. Further, the project would lead to the development of human resources through advising of Ph.D. students and mentoring of junior researchers."
"1007504","Space-Time Statistics for Wind Power Forecasting","DMS","STATISTICS","07/15/2010","10/19/2012","YANYUAN MA","TX","Texas A&M Research Foundation","Continuing Grant","Gabor Szekely","06/30/2014","$180,000.00","","yzm63@psu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","","$0.00","The technology to harvest electricity from wind energy<br/>is now sufficiently advanced to make entire cities powered<br/>by wind a reality. High-quality, short-term forecasts of<br/>wind speed are vital for making this a more reliable energy<br/>source. The investigator proposes to study two topics in<br/>wind power forecasting, develop relevant methodologies and<br/>analyze their corresponding properties and performances both<br/>theoretically and through Monte Carlo simulations studies.<br/>The new methods will be applied to a first-of-a-kind testbed<br/>wind dataset from northeastern US. The first topic concerns<br/>shrinkage and selection of space-time variables in various<br/>models for wind power forecasting. The investigator proposes<br/>to combine techniques such as partial least squares and the<br/>adaptive lasso with space-time correlation information that<br/>naturally arises among the predictor variables in the wind<br/>forecasting models. The second topic concerns realistic<br/>forecast evaluations in the context of wind power forecasting.<br/>The investigator proposes to develop new loss functions that<br/>are economically relevant for wind power and study their<br/>properties and their use in building and evaluating forecasting<br/>models in the wind energy arena.<br/><br/>The primary impact of this project is that the new methods<br/>for wind power forecasting will provide valuable tools to<br/>applied practitioners in wind farming. There is an obvious<br/>need for statisticians to be involved in such important<br/>problems related to renewable and clean energies for the<br/>well-being of our society. In particular, the research <br/>topics address crucial and major challenges for advancing <br/>the use of energy from wind."
"1007528","Constrained Statistical Inference and Information Theory","DMS","STATISTICS","08/01/2010","05/09/2012","Bhaskar Bhattacharya","IL","Southern Illinois University at Carbondale","Continuing Grant","Gabor Szekely","10/31/2015","$99,999.00","","bhaskar@siu.edu","900 S NORMAL AVE","CARBONDALE","IL","629014302","6184534540","MPS","1269","","$0.00","By applying techniques from constrained optimization and information theory, this research enhances statistical inference, makes new developments and gains new insights into existing methodology. PI concentrates on:  (1) Considering the  covariance selection problem of multivariate normal distributions, PI develops  its Fenchel dual formulation and shows that the dual solutions are themselves elements of the solution covariance matrix. This insightful observation yields  mechanics to calculate direct estimates under decomposable models. This understanding coupled with tools from convex duality help PI to  generalize the covariance selection  to multivariate dependence,   which includes  MTP2 and trends popularly used in longitudinal studies using covariance pattern matrices. The iterative proportional scaling algorithm, used for estimation in covariance selection problems,  may not lead to the correct solution under such dependence. Addressing this situation, PI presents a new algorithm  for dependence models and shows that it  converges correctly using tools from Fenchel duality. Results concerning the speed of convergence of the new algorithm are addressed.  The methodology will be applied on a real data set involving decreasing CD4+ cell numbers from an AIDS study. (2) PI develops constrained conditional log-linear (or multinomial response) models for panel data. Information theoretic tools are used to show these models are I-projections on certain moment equalities. This observations leads to construction of prediction functions which can incorporate restrictions on parameters by considering moment inequalities. Constrained versions of conventional Markov models, independence models, distance models are developed by using previous responses and present and past covariates when predicting the current response. This approach has advantages over the marginal modelling approach used in longitudinal studies. Statistical properties of these prediction functions are investigated from a population and as well as a sampling perspective. Problems such as existence and uniqueness of optimal prediction functions are addressed. Basic properties of measures of prediction quality are examined using information theory tools. Estimation, consistency and asymptotic distributions of the estimators are studied. The results will be applied on data from National Longitudinal Study of Youth. (3) A parametric model is presented for the analysis of square contingency tables where there is a one-to-one correspondence betweeen the categories of the row, column variables. Parametric scores are assigned to the rows/columns to reflect the ordinality of the categories. Efficient estimation under order restrictions of these scores is a difficult problem as they are functions of each other. Instead of using unrestricted estimates, PI proposes functions which can be used when pooling the adjacent violators (PAV). For these functions, PI proposes results that  derive their asymptotic normality, consistency, and show that they have same order relations as the score parameters. For computational purposes, several algorithms are proposed, including PAV, complete search and an active-set method. Goodness-of-fit testing of the model has a chi-bar-square distribution. Parametric bootstrap procedure is proposed for yielding reliable p-value. <br/><br/>New constrained statistical inferential methods are developed using information theoretic techniques by exploring connections between covariance selection  and multivariate dependence; constrained conditional log-linear models for panel data are  presented and their statistical properties are investigated; and in square tables with ordered parameters, statistical inference under order restrictions are proposed. The results are directly useful in analyzing longitudinal data  arising from  medicine, reliability, sociology, econometrics, etc. The proposed activities will involve training  graduate students for future researchers in statistics and providing selected undergraduate students with research experience."
"1007697","Collaborative Research: Inference for Statistical Graphics","DMS","STATISTICS","09/15/2010","09/02/2010","Heike Hofmann","IA","Iowa State University","Standard Grant","Gabor Szekely","08/31/2014","$189,974.00","Dianne Cook","hofmann@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","9150","$0.00","Since the publication of the NSF landmark report ""Visualization in<br/>Scientific Computing"" in 1987, computer-aided visualization has been<br/>recognized as one of the most potent tool sets for scientific<br/>discovery. However, discoveries based on data displays are often<br/>criticized because they are not secured by statistical inference. The<br/>team of researchers from Iowa State University, Rice University and<br/>University of Pennsylvania is addressing exactly this issue by<br/>bringing the rigors of statistical inference to visual data<br/>exploration. Statistical inference for plots are cast as comparison of<br/>a plot of the actual data with plots of null data simulated under a<br/>null hypothesis. If the actual plot stands out from a background of<br/>""null plots"", it amounts to the rejection of the null hypothesis.<br/>Executing this idea leads to rigorous protocols that can confer proper<br/>statistical significance to visual discoveries. Tools of mathematical<br/>statistics are employed to reduce composite null hypotheses to single<br/>reference distributions: conditioning on a minimal sufficient<br/>statistic, bootstrap plug-ins, and posterior predictive sampling. The<br/>protocols also have the potential to shift the perception of<br/>exploration-based findings in the scientific communities and<br/>dramatically increase the impact that these findings are allowed to<br/>have. The testing protocols will be made accessible with<br/>implementation in the open-source R language.<br/><br/>Data graphics are an essential part of communicating information. But<br/>how reliable is the information that we gather from them? The<br/>investigators will develop a rigorous framework for visual inference<br/>modeled after formal statistical testing. This framework allows the <br/>reader of a graphic to determine whether structure is real or spurious <br/>(is that a man in the moon, or just some rocks?). These protocols have <br/>the potential to shift the perception of exploration-based findings in <br/>the scientific community and dramatically increase the impact of <br/>exploratory work. Some aspects of the protocols are so intuitive that <br/>they can be used for general audiences and integrated in the teaching <br/>of introductory statistics at from grade school to college."
"0954737","CAREER: Statistical and Computational Complexities of Modern Learning Problems","DMS","STATISTICS","03/01/2010","02/21/2014","Alexander Rakhlin","PA","University of Pennsylvania","Continuing Grant","Gabor Szekely","02/29/2016","$400,000.00","","rakhlin@mit.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","1269","0000, 1045, 1187, OTHR","$0.00","The research objective of this proposal is to develop a mathematical theory relating statistical and computational complexities of learning from data. Through an integrated study of these complexities, the PI aims to fill the gap in the understanding of fundamental connections between Statistics and Computation. The problems considered in this proposal are aligned with the following overlapping directions: (1) effects of regularization on statistical and computational guarantees; (2) information-theoretic limitations of estimation and optimization; (3) trade-offs between statistical performance and computation time, as well as the effect of budget constraints; (4) sequential prediction methods as a link between optimization and statistical learning; and (5) limited-feedback models and the value of feedback in sequential prediction and optimization. Progress along these directions is of great significance from both theoretical and practical points of view. <br/><br/>Statistical Learning Theory has been successful in designing and analyzing algorithms that extract patterns from data and make intelligent decisions. Applications of learning methods are ubiquitous: they include systems for face detection and face recognition, prediction of stock markets and weather patterns, learning medical treatment strategies, speech recognition, learning user's search preferences, placement of relevant ads, and much more. As statistical learning methods become an essential part of many computerized systems, new challenges appear. These challenges include large amounts of data, high dimensionality, limited feedback, and a possibility of malicious behavior. All these challenges have a profound impact on (a) the statistical performance and (b) the computation time required to perform the task at hand. Little work exists on studying these two aspects simultaneously, and the goal of this project is to fill this gap. Better understanding of the interaction between Statistics and Computation is likely to lead to faster and more precise methods, thus positively impacting technology and society. The project's broader impact includes components for integration of interdisciplinary research and education through the development of new courses, seminars, workshops, and a summer school program. <br/>"
"1007666","Collaborative Research: Extension of Quantile Regression and Empirical Likelihood Analysis for Censored Data","DMS","STATISTICS","10/01/2010","09/20/2010","Mai Zhou","KY","University of Kentucky Research Foundation","Standard Grant","Gabor Szekely","09/30/2013","$72,873.00","","mai@ms.uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","1269","9150","$0.00","Linear models analysis is one of the most appealing statistical methods for its directly interpretable results. The accelerated failure time (AFT) and censored quantile regression (QR) model serve counterparts of the classical linear and uncensored QR model for censored data, and complement the Cox-proportional hazards model. Censored QR, in particular, enriches linear models analysis for censored data by allowing non-constant covariate effects across the distribution of event times. Other regression methods unduly constrain the covariate effects to be constant and fail to provide consistent results. In contrast censored QR allows the treatment effect to be negative for more severe cases (with shorter event-free survival times) but positive in other cases. The AFT and censored QR model are, however, under-utilized as flexible and general methods for estimation, variable selection and inference do not exist. This investigation includes developing (A) flexible estimation methods that work under less stringent conditions than those for existing methods, (B) methods for variable selection, including high dimensional data, and (C) general empirical likelihood(EL) methods parallel to uncensored case. In addition, the general ideas of the proposed research and method developed are applicable to truncation or other censoring types, although they are developed under random right censoring mechanism.<br/><br/>Improving statistical models for predicting medical outcomes is always an important part of statistical research. Thanks to recent advancement in high throughput technologies, a vast amount of potentially useful information, including patient's gene profile, is available and anticipated to lead to much improved prediction. The proposed study investigates novel methods to incorporate those data in building a better statistical model to more accurately predict a patient survival. The type of models to be investigated are also more sophisticated: instead of predicting only an ""average"" person's survival, they allow prediction for ""top 10%, or ""bottom 10%"", while allowing the survivals can be very differently impacted by the gene profile."
"1007574","Computer Experiments: Multi-Layer Designs, Kriging, and Beyond","DMS","STATISTICS","07/01/2010","05/18/2010","C. F. Jeff Wu","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","06/30/2016","$400,000.00","Roshan Joseph","jeffwu@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","","$0.00","This project aims at developing new ideas and methods on the design and analysis of computer experiments. On the design side, the investigators propose a new class of designs, called multi-layer designs. They are constructed from the well-known two-level fractional factorial designs by moving the points into different layers. This increases the number of levels for each factor and helps in filling the experimental region more evenly. The multi-layer designs have comparable space-filling properties to those of the optimal Latin hypercube designs, but are much easier to construct. Because of its unique geometric features, the proposed multi-layer designs may lead to a rethinking about the construction and properties of space-filling designs. The proposed approach gives a new tool for the researchers to take advantage of the vast amount of knowledge on fractional factorial designs in the construction of computationally efficient experimental designs for computer experiments.  On the analysis side, two topics are proposed to address the potentially serious issue of stability with the kriging method, which is the most common tool for analyzing such data. The first is to investigate the possible causes for the numerical stability in the inversion of the correlation matrix in kriging via its condition number. The second is to propose a new method, called hybrid kriging, by combining the prediction accuracy of kriging with the cheap/fast computation of the regression-based inverse distance weighting method. Since kriging has been commonly used in spatial statistics as well as in computer experiments, the proposed work on kriging can also influence the research on statistical modeling of spatial variation.<br/><br/>Because of the rapid advances in physical modeling and numerical methods, complex mathematical models can now be reliably used to mimic physical realities. Their practical implementations benefit from the advent in fast algorithms and software development. Therefore, complex system simulations are now routinely used in lieu of physical experimentations. For example, airbags in a car can be designed through sophisticated computer simulation that mimics a car-crash in a computer instead of building and crashing real cars. The proposed work should lead to the methodological development of a generic nature for designing and analyzing such computer experiments, which in turn can lead to reduced development cycle time, better product, and cost reduction. In view of the wide ranges of applications of complex system simulations, the proposed experimental design and analysis methodology should have broad-based impacts on a variety of problems like geological and atmospheric studies, computational material design, thermal management of supercomputers, and other green energy applications."
"1007660","Method Development for Censored Quantile Regression","DMS","STATISTICS","08/01/2010","04/27/2012","Limin Peng","GA","Emory University","Continuing Grant","Gabor Szekely","07/31/2014","$99,989.00","","lpeng@emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1269","","$0.00","Quantile regression has shown great promise in censored data analysis. The investigator proposes to broaden the scope of censored quantile regression by developing methods that can accommodate practical situations where censoring mechanism is more complicated than univariate random censoring. Part of research goals for the 3-year plan are: (1) to develop quantile regression methods in the presence of dependent censoring which can provide semiparametric sensitivity analysis of regression quantiles or joint inference on conditional quantiles of the response and response?censoring dependency; (2) to derive a new formulation of doubly censored regression quantiles based on a stochastic integral equation and provide practical remedies for addressing the associated identifiability issues; (3) to develop publicly available software which implements the proposed cutting-edge statistical methodology. The investigator plans to provide rigorous asymptotic studies for the proposed methods utilizing theory in empirical processes, stochastic integral equation, and functional analysis, and other statistical and probabilistic techniques.<br/><br/> <br/><br/>The proposed research will have significant impact and many applications in diverse fields including biomedical research, economics, and public health studies. For example, the methods to be developed can appropriately address the problem of nonrandom patient dropout in clinical studies, or account for the occurrence of event before study entry as well as no observation of event by the end of follow-up in many registry studies of chronic disease, thereby contributing to improving disease treatment or prevention. The investigator plans to integrate the results from the proposed research with education through student mentoring and course teaching, which may involve undergraduate students, and to widely disseminate the proposed research via publications, conference presentations, seminars, Internet postings, and free software."
"1007683","Data Depth: Multivariate Spacings and DD-Classifiers for Nonparametric Multivariate Classification","DMS","STATISTICS","09/01/2010","08/07/2012","Regina Liu","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","08/31/2014","$169,999.00","","rliu@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","Data depth has provided a systematic nonparametric multivariate framework and given rise to a powerful multivariate analysis tool set. However, its full potential in spacings and classification is yet to be fully explored. Motivated by several real applications, the investigator plans to: 1) develop nonparametric classification procedures based on DD (Depth-vs-Depth) plots. These procedures are referred to as DD-classifiers, and they are to be compared with the so-called support vector machine procedures; 2)  use the multivariate spacings derived from data depth to: (2a) construct tolerance envelopes for functional or time series data and (2b) develop a class of multivariate goodness-of-fit tests.<br/><br/><br/>Classification is is an important task in all scientific domains, such as identifying new species in archaeological investigations or distinguishing disease types in medical studies. Applying the notion of data depth, the investigator proposes to develop effective classification procedures, which can automatically yield the best separating power for classification purposes and compete well with the highly calibrated existing classification procedures. The classification outcomes can be easily visualized in a two-dimensional plot regardless of the dimension of the data. The investigator also introduces multivariate spacings for the analysis of multi-dimensional data.  These multivariate spacings should have a wide range of utilities. In particular, the investigator applies these spacings to develop both tolerance envelopes for tracking multivariate data and a class of multivariate goodness-of-fit tests. She plans to apply the proposed tolerance envelope to the monitoring of aircraft landing patterns and to ensure landing safety. She also plans to apply the proposed classifications to disease identification. These applications are motivated by the investigator's ongoing collaborative research projects with the Federal Aviation Administration and the Department of Psychiatry of the Robert Wood Johnson Medical School. The proposed projects  involve real databases and are ideally suited for engaging students and postdocs."
"1008884","Travel Support for the 4th International Joint IMS-ISBA Conference","DMS","STATISTICS","03/15/2010","04/07/2010","Bradley Carlin","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","02/28/2011","$8,000.00","","brad@biostat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, 7556, OTHR","$0.00","This grant provides support for the Fourth International Joint Meeting of the IMS (Institute of Mathematical Statistics) and ISBA (the International Society for Bayesian Analysis), to be held on Wednesday, January 5 to Friday, January 7, 2011. A central theme of the conference will be Markov chain Monte Carlo (MCMC) and related methods and applications. The lion's share of the funding will be used to support the travel expenses of junior biostatistical investigators, i.e., persons pursuing a PhD or DrPH in statistics, biostatistics, or a closely related field, or who have received such a degree within the five years preceding the conference. Such investigators are often doing research that is among the most novel, interesting, and important for international dissemination, yet lack the travel funds necessary to attend such a conference, since they have not yet established a publication track record sufficient to attract external travel and other funding for their work. We have a commitment from ISBA to provide partial travel support for 10 young recent PhDs from economically disadvantaged countries, and are also seeking support from NSA for support for 10-15 additional young investigators. Specific goals and anticipated benefits of the conference include promoting the continued development of statistical theory and applications in finance, technology, environmental science, biomedicine and other areas, and a fuller exploration of the interplay of classical and Bayesian statistical methods in the context of specific areas of research.<br/><br/>Statisticians play an indispensable role in the analysis of biomedical, environmental and public health data, from the study design stage all the way through to final analysis and report-writing. Statisticians also serve on a myriad of scientific review and advisory panels, as well as provide statistical training and consulting to substantive area researchers. <br/>Finally, the development of new statistical methodology for interpretation of data from clinical, observational, and laboratory studies is a key area of statistical endeavor. As a result, scholarly conferences where new ideas can be exchanged are important for statistical science to move forward. The benefit of and need for attendance at such meetings by junior statistical researchers is particularly great, since they contribute mightily to their professional development and help ""level the playing field"" with more established senior investigators. This grant provides support for the Fourth International Joint Meeting of the IMS (Institute of Mathematical Statistics) and ISBA (the International Society for Bayesian Analysis), to be held Wednesday, January 5 to Friday, January 7, 2011. The lion's share of the support we seek is to support the travel expenses of junior investigators.<br/>"
"1007647","An Approach to Semiparametric Regression with Random Covariates","DMS","STATISTICS","09/01/2010","08/20/2010","Benjamin Kedem","MD","University of Maryland, College Park","Standard Grant","Gabor Szekely","08/31/2013","$100,000.00","","bnk@math.umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1269","","$0.00","Given multiple multivariate data sources, each represented by an unknown multivariate distribution, the investigator proposes an approach to regression analysis based on relationships between semiparametric estimates of these multivariate distributions. Resulting from this are regression estimates expressed as estimates of the conditional expectation of a response given its covariates, for each source. The investigator plans to study the statistical properties of the regression estimates, associated diagnostic tools, the applicability and computability of the method using real data, and compare the method to multiple and kernel regression methods.<br/><br/>A basic statistical problem, referred to as regression, is to estimate a relationship between a dependent variable, called the response, and corresponding independent variables, called the predictors or covariates. Most often this is done using a single data source. In many cases, however,  multiple data sources are available such as different groups of patients and a control group, data sets from many surveillance sensors, multiple surveillance sources from land, sea, and space, or multiple time records. The existence of multiple data sources motivates a generalization. The investigator will study a novel approach to regression based on multiple data sources, by combining or fusing the data from all sources as to increase the precision of all estimates, and by studying the joint statistical behavior of all sources. Potentially the approach may impact regression analysis in general, as well as forecasting of such time series as unemployment, commodity prices, and the course of a moving object given covariate information, whenever multiple data sources are available. The investigator plans to compare the approach with existing regression methods both theoretically and by  data analysis using real data."
"1007650","Conference on Modeling High Frequency Data in Finance II","DMS","PROBABILITY, APPLIED MATHEMATICS, STATISTICS","04/15/2010","04/27/2010","Ionut Florescu","NJ","Stevens Institute of Technology","Standard Grant","Tomek Bartoszynski","03/31/2011","$25,000.00","Maria Mariani, Frederi Viens","ifloresc@stevens.edu","1 CASTLEPOINT ON HUDSON","HOBOKEN","NJ","07030","2012168762","MPS","1263, 1266, 1269","7556","$0.00","This award provides funding for the second edition of the Conference on Modeling High Frequency Data. This year the conference is a joint effort between Stevens Institute of Technology, University of Texas at El Paso and Purdue University. The purpose of the conference is to improve the models used to analyze data sampled with high frequency. Tools available from a variety of areas such as statistics, stochastic processes, statistical mechanics, clustering, and systems will be exposed. Academics, industry professionals, and government regulators will meet to collaborate, with the goal of advancing the quality of research currently under development in the field.<br/><br/>The scientific motivation for the conference arises from the fact that organizing a conference in the trade capital of the world has the potential of bringing together the best mathematicians, practitioners and regulators to help develop and better the modeling aspect of the marketplace.<br/><br/>The main training objective of this meeting is to expose today's economic and modeling problems to current graduate students in the hope that this will improve the quality of their research problems."
"1007396","Efficient Modeling in Quantile Regression","DMS","STATISTICS","09/01/2010","08/18/2011","Xuming He","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Gabor Szekely","04/30/2012","$271,823.00","","xmhe@umich.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","","$0.00","Quantile regression has in recent years emerged successfully as a powerful supplement to the more conventional least squares regression. By modeling the conditional quantile functions, the researchers are often able to gain a much more comprehensive picture of how a response variable is associated with its covariates. The prevailing approach in quantile regression is to perform analysis of the conditional quantile functions one percentile level at a time. This approach offers great modeling flexibility at the cost of statistical efficiency. The Principle Investigator proposes to develop and study new approaches to efficient modeling of conditional quantile functions. By ""borrowing strength"" across neighboring quantiles and utilizing a Bayesian empirical likelihood approach, the investigator aims to advance the theory, methodology, and applications of efficient quantile regression. Efficiency gain is an important consideration of any statistical research, and the proposed modeling techniques are especially helpful in the analysis of quantiles in the data-sparse areas. The Bayesian empirical likelihood approach for quantile regression can be used in conjunction with optimal weighting for semiparametric efficiency, and with Markov chain Monte Carlo sampling for effective computation in a high dimensional parameter space.The proposed models, to be called semi-local quantile models, strike to balance bias and variance; when the models do not hold exactly, the proposed estimators follow the spirit of regularization.<br/><br/>Inference in data-sparse areas, including but not restricted to the analysis of high tails, is highly valuable in a wide range of scientific and social studies. The proposed research is motivated by the investigator's interdisciplinary research in climate studies and public health, and will provide researchers in statistics and other fields novel tools for better understanding and quantifying relationships between measurements. The proposed activities include new opportunities for graduate students to participate in transformative research, and will enable the investigator to continue integration of research with teaching and mentoring. The investigator pursues active academic exchanges through lecturers and collaborations, and free distribution of software, for broad dissemination of the research results."
"1007494","Collaborative Research: Case-Control Studies, New Directions and Applications","DMS","STATISTICS","09/01/2010","08/26/2010","Bhramar Mukherjee","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","08/31/2013","$118,926.00","","bhramar@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Case-control studies mark possibly the single most important and far-reaching contribution that statisticians have made in the domain of Public Health and Epidemiology. The prime objective of this research is to make some new contributions to case-control studies, both in methodology and in application. In particular, the PIs propose a semiparametric Bayesian method to incorporate longitudinal data in a case-control analysis using penalized splines. New Bayesian functional data-analytic tools are needed here. The methods will be applied to biomarker based screening procedures, and will provide a critical appraisal of the association between prostate cancer and the past trajectory of prostate-specific antigen measurements. A second component of the proposed research is the analysis of case-control data generated from two-phase sampling with non-monotone missingness in covariates. Such designs have many potential applications in case-control studies that explore interplay of genetic and environmental risk factors. Exploiting assumptions like gene-gene and gene-environment independence adds to the complexity of the inference. The methods are motivated by an immediate application to a large population based case-control study of colorectal cancer. The final aspect of this proposal deals with a united methodology which provides equivalent inference for odds ratio parameters based on prospective and retrospective models. Both frequentist and Bayesian methods will be considered. The PIs have a track record of successful collaboration in this domain, and want to advance/ extend their work further in these new directions. <br/><br/>Two scientific streams are currently dominating clinical medicine and public health: the molecular biology approach with an emphasis on genetics and discovery of novel biomarkers, and the quantitative approach with an emphasis on epidemiology. The developments in these areas jointly are making fundamental contributions to the study of etiology, diagnosis, prognosis and treatment of complex diseases. Though the standard unmatched case-control study design still remains one of the most popular epidemiologic tools, phenomenal advancement of medical science and genetic technology is giving rise to many complex design and analysis issues which statisticians and epidemiologists have never confronted before. This proposal lies in that new interface of epidemiology and statistics. To understand the mechanism of complex diseases and to design targeted intervention strategies for high-risk individuals is one of the major areas of scientific research in this century. The current proposal is not a mere academic pursuit but an effort to contribute to this scientific process."
"1007466","Statistical Methods for Spatially Correlated Hierarchical Functional Data","DMS","STATISTICS","05/15/2010","03/05/2012","Ana-Maria Staicu","NC","North Carolina State University","Continuing Grant","Gabor Szekely","04/30/2014","$125,000.00","","staicu@stat.ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","","$0.00","This research project is to create new statistical models and methods for the analysis of hierarchical functional data. In particular the investigator proposes a novel methodological framework for fast and robust inferential tools when the true data generating process accounts for complex correlation mechanisms that mimic and represent true biological structures. The project has the following aims. (a). To develop a new methodological framework for the analysis of hierarchical univariate functional data when the functions at the lowest level of hierarchy are correlated. Understanding and quantifying the dependence structure between these functions is of scientific importance. (b). To extend the developed methodology to the analysis of multivariate functional data. (c). To propose new inferential methods for group means and differences between group means when functional data have a natural hierarchical and spatial structure.<br/><br/>Modern research data have become increasingly complex, raising non-traditional modeling and inferential challenges. In particular, advancements in technology and computation have made recording and processing of functional data possible. An increasing number of scientific experiments record hierarchical functional data with complex dependence structures. Although the proposed research was motivated by data from a colon cancer experimental study, correlated functional data arise in many areas of research. The statistical methods developed in this proposal are timely and important and will be relevant to many new data sets, where the object of inference are functions or images that remain dependent even after conditioning on the subject on which they are measured. Such data are collected in engineering and climate modeling among others. In contrast with other published methods, the methodology proposed here is computationally efficient and it scales well to moderate and large data sets."
"1007590","Estimation Theory for Semiparametric Models with Bundled Parameters","DMS","STATISTICS","08/01/2010","06/25/2012","Bin Nan","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","07/31/2013","$200,000.00","","nanb@uci.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","The investigator proposes extensions of existing asymptotic distributional theories for M- and Z-estimations in the semiparametric models with separated parameters to accommodate situations where the estimation criteria for the semiparametric models are parameterized with bundled parameters, i.e. the infinite dimensional parameter is an unknown function of the parameter of interest. The proposal is motivated by several statistical problems including the efficient estimation in the linear regression model with censored data under several different censoring mechanisms, the efficient estimation in the single index model, the partial likelihood estimation in the Cox regression model with an unknown link function, and the weighted estimation for missing data problems in survival analysis. The investigator also proposes to apply the general theory for bundled parameters to all these problems, particularly for the case that the infinite dimensional nuisance parameter is approximated by regression splines.<br/><br/>The proposed research is primarily motivated by PI's collaboration in biomedical studies, where more robust statistical modeling techniques are desirable to reduce the uncertainty of model misspecification, particularly when data are incomplete due to limited study follow-up. The proposed research will also allow the investigator to add more thorough statistical results to the course of advanced survival analysis and be helpful in developing the special topic course on semiparametric models into a regular Ph.D. level course. The proposed research activities will motivate graduate students to become independent researchers who are able to engage in fundamental statistical research."
"1007751","A Study of Boundary Phenomena in a Class of Parametric and Nonparametric Problems","DMS","STATISTICS","07/01/2010","05/18/2010","Moulinath Banerjee","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","06/30/2014","$250,000.00","","moulib@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","The project deals with boundary effects in a class of parametric and nonparametric models. Three main directions are pursued. The first involves the<br/>development of methodological and inferential techniques for estimating boundaries in the covariate space (in regression models) using both likelihood based methods and least squares type criteria and are motivated primarily by (a) the need to build stochastic models in response--covariate studies and survival analysis that cater to the development of individualized treatment therapies for cancer-afflicted individuals, and, (b) the development of adaptive multistage strategies for estimating discontinuities in regression surfaces. The second direction involves the determination of a region enclosed by a closed surface in Euclidean space where a function assumes an extremal value (a maximum or a minimum). A novel procedure based on P--values obtained from statistical tests conducted at different points in the domain of the function, for the hypothesis that the function assumes its extreme value at those points, is developed to this end.  The final direction deals with quantifying the effect of the sampling distribution of covariates on nonparametric MLEs of the regression function, in nonparametric regression under shape--constraints like monotonicity or convexity.  The resolution of the grid on which the covariate is supported is seen to be critically related to the asymptotic distributions of these shape-constrained estimators. Here, one encounters boundary effects in the sense that there exist specific resolutions at which the asymptotics transition dramatically from Gaussian to non-Gaussian.<br/><br/>The proposed research program is motivated by compelling problems in several different areas: from clinical trials for cancer patients, epidemiological studies and data from complex<br/>`omics' experiments to problems in systems engineering, signal processing and FMRI studies. On the scientific front, the research based on this grant will lead to new design-based adaptive procedures for studying the behavior of engineering systems under different input intensities as well as new designs for clinical trials to identify core factors involved in cancer progression, to the development of indvidualized treatment therapies in cancer research, to a better understanding of brain activation mechanisms via accurate identification of signals obtained from FMRI and to methods for signal processing via sensor networks. The novel methodological procedures ensuing from our research will be disseminated to the relevant scientific communities, both via inter--disciplinary interaction and collaboration, and the development of software in a readily accessible language environment. Finally, on the educational front, the material from the proposed research will provide dissertation topics for graduate students who will also be supported on this grant; the project will therefore play an important role in the training of future statisticians."
"1007682","Knowledge-Driven Bayesian Regression","DMS","STATISTICS","07/15/2010","07/10/2010","Christopher Hans","OH","Ohio State University","Standard Grant","Gabor Szekely","06/30/2014","$179,878.00","Steven MacEachern","hans.11@osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269","","$0.00","The research concerns the development of Bayesian regression models that are carefully designed to include specific forms of knowledge common to many data analytic settings.  Researchers who work in specialized fields accumulate quantifiable, problem-specific expert knowledge about model features based on their repeated experience designing experiments and analyzing the resulting data.  The ability to build models that respect such knowledge is a critical component to good data analysis.  In the Bayesian modeling paradigm, such knowledge is incorporated in the analysis through prior distributions on model parameters.  Unfortunately, many standard prior models compete against this knowledge, assigning significant prior probability to events that researchers know are implausible, if not impossible.  A major focus of the research is the identification of the types of information that are available to researchers for specific classes of models and the development of new prior distributions whose structures are driven by this knowledge.  The project considers data-analytic settings ranging from linear regression models to models arising from conjoint choice experiments.  Particular forms of prior knowledge range from information about the coefficient of determination in a linear regression model to more complex information about collinearity and grouped predictor variables. The research places emphasis on understanding how such prior information affects the comparison of competing models, and how properties of the new prior distributions affect the questions of Bayesian model comparison and variable selection.  The research in new classes of prior distributions for regression problems includes investigations of new and emerging ideas about parameter regularization.  Generalizations of new Bayesian regularization priors will be developed that allow empirical knowledge about correlation structures in the predictor variables to be incorporated in the regression models.  Such priors will be useful in high-dimensional problems, where extensive information is not always readily available and where regularization is known to help stabilize inference.<br/><br/>The motivation for the research comes from the social sciences, in particular Psychology and Marketing.  A key question in Psychology is how the brain perceives, reacts to, and processes stimuli.  Extensive experimentation in this field has led to the development of a large base of knowledge about how manipulation of particular stimuli affects the brain.  Analysis of these experiments provides insight into how the brain works.  A common experiment in Marketing involves assessing the attractiveness of product offerings to different groups of consumers.  These assessments are then used to design products that will be successful in the marketplace.  In order to provide new insights into these processes in Psychology and Marketing, researchers require methods of data analysis that are tailored to incorporate knowledge about these processes that has been accumulated by researchers.  The statistical methodology developed in this research will provide investigators with sophisticated data-analytic tools that will allow them to perform focused data analysis, leading to a more accurate understanding of individual and consumer behavior."
"1007808","Modeling Higher-Order Dependence With Cumulant Tensors","DMS","STATISTICS","09/01/2010","08/16/2010","Jason Morton","PA","Pennsylvania State Univ University Park","Standard Grant","Gabor Szekely","08/31/2014","$125,000.00","","morton@math.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","The second cumulant tensor  of a multivariate distribution is its<br/>covariance matrix, which provides a partial description of its<br/>dependence structure (complete in the Gaussian case). Innumerable<br/>successful statistical methods are based on analyzing the covariance<br/>matrix, e.g. imposing rank restrictions as in principal component<br/>analysis or zeros in its inverse as in Gaussian graphical models.<br/>Moreover, the covariance matrix plays a critical role in optimization<br/>in finance and other areas involving optimization of risky payoffs,<br/>since it is the bilinear form yielding  the variance of a linear<br/>combination of variables.  For multivariate, non-Gaussian data, the<br/>covariance matrix is an incomplete description of the dependence<br/>structure.  Cumulant tensors are the multivariate generalization of<br/>univariate skewness and kurtosis and the higher-order generalization<br/>of covariance matrices, and allow a more complete description of<br/>dependence.  The research investigates a number of problems in theory,<br/>estimation, algorithms, and applications around modeling higher-order<br/>non-Gaussian dependence with cumulant tensors.<br/><br/>Data arising from modern applications like computer vision, finance,<br/>and computational biology are rarely well described by a normal<br/>distribution, though  analysis often proceeds as if they were.  For<br/>example, one cause of the financial crisis and the damage it did to<br/>many investors was an over-reliance on the variance-based risk<br/>measures appropriate primarily for normal distributions. This can<br/>allow risk to be in a sense hidden in the higher-order structure,<br/>where it can be ignored or even made worse by application of<br/>traditional risk metrics.  Cumulant tensors provide a promising avenue<br/>for modeling higher-order dependence.  Success in developing these<br/>models will have broad impacts in the analysis of real-world data with<br/>complex dependence, particularly in modeling and managing financial<br/>risk and in dimension reduction, and could help improve the robustness<br/>of parts of the financial system."
"1007801","Statistical Methods for High Dimensional Discrete Data","DMS","STATISTICS","06/01/2010","06/18/2012","Naomi Altman","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","05/31/2015","$200,000.00","","nsa1@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","Very high dimensional count and binary data are now common in many fields including machine learning, imaging and marketing.  In high-throughput biology, ultra-high thoughput sequencing technologies which produce count and categorical data are displacing microarrays and other ""omics"" measurement devices. The output  of these measurement devices are counts per gene or other biological subunit for tens of thousands of responses per sample, or presence/absence for features such as single nucleotide polymorphisms (SNPs), for possibly millions of responses per sample.  Similar data can be derived on for features on satellite images, medical scans, monitoring devices and other very high dimensional measurement devices.  The investigator will extend highly multivariate and multiple testing methods developed for continuous (primarily normally distributed) data to discrete data.   New methods will be developed in four areas: A)  analyses for differences in distribution for discrete data that can accommodate complex experimental designs using generalized linear mixed models with overdispersion and Bayesian or empirical Bayes shrinkage.  B)  methods for supervised clustering of samples and variables in the discrete data setting taking into account the error structure of the discrete predictors.  C) classical and sufficient dimensions reduction methods such as canonical correlation and sliced inverse regression for discrete data.  D) extension of concepts and methods in multiple testing, such as false discovery rate estimation to the discrete setting in which the p-values from independent or weakly dependent tests may have different null distributions using conditional mixture modeling.  The methods will be tested on genomics and imaging data.<br/><br/>Very highly multivariate data are now the norm in fields as diverse as cell biology, marketing, medical and satellite imaging, meteorology, epidemiology,<br/> fraud detection and cancer research.  These data may include thousands or millions of measurements on each item in the sample.  For example, genotyping <br/>services provide individuals with information on hundreds of thousands of genetic variants in their cells and retailer databases may have information on <br/>the sales of tens of thousands of items for each store in the chain. Many of these data come in the form of counts (such as number of items of each type <br/>in inventory, number of mRNA molecules encoding a particular protein) or in the form of categories (such as on/off, present/absent, or genotype AA, aa <br/>or Aa).  Methodology for highly multivariate continuous measurements such as blood pressure and temperature are well-developed but do not apply directly <br/>to count and categorical data.  The investigator will develop statistical methodology and software to improve analysis and summary of count and categorical data.  Four main areas of research are proposed:  A) statistical models and tests to determine if the variables are associated with differences among groups; B) statistical methods for prediction or classification of group membership; C) methods to summarize the data with a much smaller set of derived variables which preserve the predictive power of the full data and D) multiple comparisons methods to estimate the error rates.   For example, in a study of the genes associated with metastatic versus non-metastatic cancer, the methods could be used to determine which genes express differently in tumors which did or did not advance to metastasis, select a smaller set of genes which could be used as a diagnostic tool and then provide convenient  summaries which can readily be interpreted by clinicians.  In a study of stresses on a machine part, the pixels of scans of the part before and during  the application of the stresses could be used to determine precise locations at which the part might fail and differences among features of the scan  between parts which fail at low versus high stress.  In studies in which a large number of models are fitted or tests conducted, it is necessary to tolerate a small percentage of errors.  Concepts and methods in multiple testing which have been developed for continuous data will be extended to assist in estimating and controlling the number of false conclusions with count and categorical data."
"1007547","Envelope Models and Methods for Efficient Multivariate Analysis with Applications to Tissue Engineering","DMS","STATISTICS","08/15/2010","05/04/2012","Ralph Cook","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","07/31/2014","$309,922.00","","dennis@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","The investigator and his colleagues propose to develop a new class of statistical tools -- called envelopes -- for studying multivariate data.  Enveloping is based on novel parameterizations that use reducing subspaces to link a location matrix L with a dispersion matrix D. For instance, the outer envelope is the smallest reducing subspace of D that contains the span of L, while the inner envelope is the largest reducing subspace of D that is contained within the span of L.  In multivariate linear regression, the maximum likelihood estimator of the coefficient matrix L based on an envelope model can be substantially less variable than the maximum likelihood estimator under the classical normal model, particularly when the mean function varies in directions that are orthogonal to the directions of maximum variation for the dispersion matrix.  It is expected that similar results will hold in other multivariate areas, like discriminant analysis and functional data analysis.  Enveloping is a new paradigm for addressing multivariate statistical problems that has the potential to facilitate interpretation, to improve analyses that might otherwise be tenuous and to produce truly massive gains in efficiency relative to standard methods.<br/><br/><br/>Technological advances in many scientific fields have been followed by configurations of multivariate data that strain or are beyond the capabilities of standard statistical theory and methods. More than ever before, understanding experimental evidence and exploring scientific hypotheses require methods to meaningfully study contemporary data. This is particularly true in the life sciences, where the ability to extract the relevant information from a complex body of data is paramount.  The investigator and his colleagues plan to study a new class of multivariate statistical methods that are capable of efficiently extracting relevant information for a given purpose from complex data.  For instance, the overarching goal in tissue engineering is to gain the ability to replace damaged human connective tissue with viable tissue patches fabricated in vitro. Current technology has failed to reach this goal because tissues grown in vitro lack adequate mechanical integrity for in vivo applications. The mechanical integrity of tissues is controlled by a network of several  hundred intercellular signaling proteins that shape long-term tissue growth and can be measured by mass spectrometry.  The statistical objective here is to identify the most important stimuli and to extract the relevant information by reducing the signaling proteins to a few key protein indices that can be monitored during in vitro growth and directed by the external stimuli."
"1007506","Intensity-Based Image Registration and 3-D Image Denoising","DMS","STATISTICS","09/01/2010","08/31/2010","Peihua Qiu","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","08/31/2013","$150,000.00","","pqiu@ufl.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","This project focuses on two important image analysis problems. One is on image registration, which is to match up images or image volumes for structure localization, difference detection, and other purposes. It is widely used in medical imaging, remote sensing, finger print or face recognition, and so forth.  The second major focus is on 3-D image denoising with edges and major edge features preserved. Because of fast progress in image acquisition techniques, 3-D images become increasingly popular in magnetic resonance imaging (MRI), functional MRI (fMRI), and other applications. However, observed 3-D images often contain noise, due to hardware imperfection and other reasons, which  should be removed beforehand so that subsequent image analyses would be more reliable. In the literature, existing image registration (IR) methods can be roughly classified into two categories: feature-based IR methods and intensity-based IR methods. Because feature selection is often a time-consuming and challenging process, intensity-based IR methods have become popular in various applications. However, most existing intensity-based IR methods require a parametric model  for describing the image matching transformation, which is often difficult to  verify in practice. In this project, the investigator and his colleagues propose  an intensity-based IR procedure without imposing any parametric form on the  matching transformation. Therefore, the proposed method has the potential to greatly improve the intensity-based IR techniques and greatly broaden their applications. In the literature, most existing image denoising methods are for analyzing 2-D images. They often have certain ability to preserve planar parts of the edges, but cannot preserve angular parts of the edges well. Their direct extensions to 3-D cases generally cannot handle 3-D images efficiently, because the structure of 3-D images is often substantially more complicated than that of 2-D images. This project proposes a novel 3-D image denoising method which can preserve edges and major edge features well. Therefore, it would provide a reliable tool for 3-D image denoising.<br/><br/>Images are used everywhere in our society, ranging from medical diagnostics by CT, MRI, and other medical imaging techniques to satellite monitoring of global environmental changes. This project aims to improve image registration and 3-D image denoising techniques, which are used broadly in various imaging applications. Thus, it will have broader impacts on our society through its direct impact on improvement of medical diagnostics, security systems involving fingerprint and face recognition, remote sensing techniques, and so forth. This project also aims to contribute to the development of human resources in science and engineering through its educational activities. For instance, the investigator offers an advanced topics course on image analysis, from which graduate students from various departments can receive systematic training in scientific research. Several graduate students are doing their thesis research with the investigator on image processing. Some computer software packages developed by the investigator and his graduate students would be posted on a project web page for other researchers to download and use. The major research results obtained from this project would be presented in national and international conferences, and be submitted for publication in academic journals."
"1007603","Semiparametric Inference for High-dimensional Correlated or  Heterogeneous Cross-sectional Data with Discrete Response","DMS","STATISTICS","07/01/2010","05/21/2010","Lan Wang","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","06/30/2013","$176,595.00","","lanwang@mbs.miami.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","Substantial advancement has been achieved over the past decade in high-dimensional data analysis with diverging number of covariates. <br/>However, when the research interest is focused on modeling the relationship between the response variable and a high-dimensional vector of covariates, most existing work only applies when the response variable is continuous and often requires stringent conditions such as independence or homogeneity. Many fundamental problems remain unsolved for high-dimensional data with discrete responses, especially when the standard modeling assumptions are not satisfied. This project aims to develop new statistical theory, methodology and algorithms for analyzing high-dimensional correlated or heterogeneous cross-sectional data with binary or count responses.  More specifically, the investigator will (1) rigorously study the asymptotic theory, including consistency and asymptotic normality, of the semiparametric procedure of generalized estimating equations in the new diverging p asymptotic framework;  (2) investigate generalized estimating equations based variable selection procedures for high-dimensional longitudinal and spatially correlated data; and (3) investigate the theory and methodology of sparse quantile regression, where the number of parameters may greatly exceed sample size, for analyzing heterogeneous data with possibly discrete responses.<br/><br/><br/>The prevalence of high-dimensional binary and count data in various scientific fields, such as biomedical and health sciences, economics, social sciences and environmental studies, demands new statistical theory, methodology and software. Many important issues in analyzing high-dimensional binary or count data, especially in the presence of correlation or heterogeneity, have not been systematically studied. Moreover, existing work based on the full likelihood or the independence assumption in the high-dimensional setting cannot be readily applied. This project will make significant and timely contribution to the general theory and methodology of high-dimensional data analysis in the diverging p framework. Such theories are critical for guiding practical data analysis. Undergraduate and graduate students, especially those from underrepresented groups, will be encouraged to participate in this research  project."
"1005529","Travel Support for the 10th ISBA World Meeting on Bayesian Statistics","DMS","STATISTICS","03/01/2010","03/10/2010","Peter Mueller","TX","University of Texas, M.D. Anderson Cancer Center","Standard Grant","Gabor Szekely","02/28/2011","$10,000.00","","pmueller@math.utexas.edu","1515 HOLCOMBE BLVD","HOUSTON","TX","770304000","7137923220","MPS","1269","0000, 7556, OTHR","$0.00","The funding is for travel support for US participants in the ""10th ISBA World Meeting on Bayesian Statistics"", to be held June 3 through 8, 2010, in Benidorm, Spain. The grant will pay partial travel expenses for 10 young investigators, including at least 5 women and other under-represented groups.  The purpose of the conference is to bring together the diverse international community of investigators in statistics and in other areas who develop and use Bayesian methods to share recent findings and to present new and challenging problems to be addressed by the community.<br/><br/>This grant enables 10 young investigators from US institutions to participate in the main international meeting for Bayesian statistics which is held every two years.  The use of Bayesian methods in academia, government and industry has been steadily increasing over the past 20 years, with many US researchers playing a leading role. This grant will help to maintain and strengthen this leading role of US scientists. The project will do so in particular by providing young investigators with important opportunities for networking and dissemination of their research results. The meeting is organized by the International Society for Bayesian Analysis (ISBA, http://www.bayesian.org).  The mission of ISBA is to promote the development and application of Bayesian statistical theory and methods useful in the solution of theoretical and applied problems in science, industry and government. <br/>"
"1007686","Spatially Correlated Data with Errors-in-variables: Inteference and Prediction with Application to Paleoclimate Reconstruction","DMS","STATISTICS","06/01/2010","05/21/2010","Bo Li","IN","Purdue University","Standard Grant","Gabor Szekely","05/31/2014","$145,000.00","","libo@illinois.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1269","","$0.00","The project focuses on methods to retrieve signals in spatial data that are possibly masked due to measurement errors. Specifically, the investigator proposes to establish unbiased parameter estimates and optimal predictions for spatially correlated data with errors present in both predictors and responses, and then apply the solutions to paleoclimate reconstruction to achieve a faithful representation of the past climate. The results from the proposed work will take both the spatial correlation and errors-in-variables into account to uncover the true relationship between the response and explanatory variables. The bias and the asymptotic behavior of the parameter estimates and the optimality of predictions in different senses under various measurement error structures are investigated. Besides, a new practical method for estimating the variance-covariance matrix of measurement errors for data with no replicates is proposed.<br/><br/>The primary impact of this project is to provide practical statistical tools to correct the effects of errors-in-variables in spatial data analysis. Once the results are applied to paleoclimate reconstructions, they will solve a long standing problem concerning the amplitudes of past climate that plays a central role in understanding the dynamics of the climate system. In addition to climatology, the proposed methods can be generally applied to a variety of other disciplines such as seismology, environmetrics, atmospheric sciences and public health studies, where data are usually spatially correlated and contain substantial noise. However, the broader impacts of the proposed activities are multiple. A key aspect of this proposal is the integration of research and teaching, which will be achieved by proposing specific projects for students during the teaching of classes on measurement errors and on spatial statistics."
"1007535","Collaborative Research: Extension of Quantile Regression and Empirical Likelihood Analysis for Censored Data","DMS","STATISTICS","10/01/2010","09/20/2010","Mi-Ok Kim","OH","Children's Hospital Medical Center","Standard Grant","Gabor Szekely","09/30/2013","$120,838.00","","miok.kim@cchmc.org","3333 BURNET AVE","CINCINNATI","OH","452293039","5136361363","MPS","1269","","$0.00","Linear models analysis is one of the most appealing statistical methods for its directly interpretable results. The accelerated failure time (AFT) and censored quantile regression (QR) model serve counterparts of the classical linear and uncensored QR model for censored data, and complement the Cox-proportional hazards model. Censored QR, in particular, enriches linear models analysis for censored data by allowing non-constant covariate effects across the distribution of event times. Other regression methods unduly constrain the covariate effects to be constant and fail to provide consistent results. In contrast censored QR allows the treatment effect to be negative for more severe cases (with shorter event-free survival times) but positive in other cases. The AFT and censored QR model are, however, under-utilized as flexible and general methods for estimation, variable selection and inference do not exist. This investigation includes developing (A) flexible estimation methods that work under less stringent conditions than those for existing methods, (B) methods for variable selection, including high dimensional data, and (C) general empirical likelihood(EL) methods parallel to uncensored case. In addition, the general ideas of the proposed research and method developed are applicable to truncation or other censoring types, although they are developed under random right censoring mechanism.<br/><br/>Improving statistical models for predicting medical outcomes is always an important part of statistical research. Thanks to recent advancement in high throughput technologies, a vast amount of potentially useful information, including patient's gene profile, is available and anticipated to lead to much improved prediction. The proposed study investigates novel methods to incorporate those data in building a better statistical model to more accurately predict a patient survival. The type of models to be investigated are also more sophisticated: instead of predicting only an ""average"" person's survival, they allow prediction for ""top 10%, or ""bottom 10%"", while allowing the survivals can be very differently impacted by the gene profile."
"1007726","Generalized semiparametric odds ratio models","DMS","STATISTICS","09/01/2010","08/26/2010","Hua Yun Chen","IL","University of Illinois at Chicago","Standard Grant","Gabor Szekely","08/31/2014","$175,006.00","","hychen@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","","$0.00","The investigator has two objectives to accomplish in this research<br/>project. The first objective is to develop theory of statistical <br/>inference for a new class of semi-parametric odds ratio models that<br/>include both the generalized linear model and the Cox regression model <br/>as special cases. The second objective is to apply this class of <br/>models to solve a number of theoretical problems that are of <br/>importance in applications. These applications include <br/>(1) addressing issues in parameter identifiability, estimation, <br/>and inference in biased sampling designs in studying the association <br/>of a disease with gene and environment factors, (2) introducing <br/>a new approach for testing goodness of fit of generalized linear <br/>models, and (3) developing a new flexible semi-parametric<br/>procedure for multivariate density estimation and <br/>survival analysis with complex dependence structures.<br/><br/>Flexible and easily interpretable stochastic models are <br/>very useful in extracting information from data with complex <br/>structures. Such data are often collected in studies exploring <br/>the causes of diseases and other socio-economic problems. <br/>The investigator proposes a new class of models for the <br/>statistical analysis of such data. Results from this <br/>research will have broad applications in a variety of scientific fields. <br/>When applied to epidemiological studies, results from this research <br/>will be able to facilitate the design of more powerful statistical <br/>tests for detecting genetic and environmental causes of disease. <br/>When applied to sociological studies, results from this research <br/>will be able to facilitate the understanding of underlying causes <br/>of complex socio-economic problems so that better solutions to the <br/>problems can be found."
"0968309","FRG: Collaborative Research: Mathematics of large scale urban crime","DMS","APPLIED MATHEMATICS, STATISTICS","09/01/2010","07/19/2010","Andrea Bertozzi","CA","University of California-Los Angeles","Standard Grant","Henry Warchall","08/31/2014","$1,008,105.00","Lincoln Chayes, P. Jeffrey Brantingham, George Mohler, Martin Short","bertozzi@math.ucla.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244201","3107940102","MPS","1266, 1269","1616","$0.00","This multidisciplinary project aims to develop new mathematical methods, at the interface of the theory of nonlinear partial differential equations, statistical mechanics, graph theory, and statistics, for predictability and control of urban crime.  The project focuses on spatio-temporal crime patterns and includes (1) new mathematical analysis and comparisons to crime data for discrete and continuum models of crime hotspots; (2) models with spatially embedded social networks, especially with regard to gang activity; and (3) exploration of new methods of Geographic Profiling, incorporating detailed features of urban terrain and more accurate modes of criminal movement into existing models. Mathematical work on this project includes analysis of nonlinear PDE models, analysis of statistical physics models, and further development of these models to include spatial heterogeneity, different offender movement patterns, and urban street gang networks.  At the same time it provides both a deeper understanding of the mechanisms behind pattern formation in urban crime and some useful algorithms and software for local law enforcement agencies.<br/><br/>Mathematics of criminality is an emerging topic in applied mathematics with interest on a global scale and direct relevance to U.S. homeland security. This focused research group involves interactions between researchers whose primary expertise lies within very different fields -- mathematics, physics, anthropology, and criminology -- so that pattern formation of criminal activity is dissected and understood from very different viewpoints and perspectives. The project addresses algorithm development for analyzing real field data and agent-based simulation tools for urban crime. The research will also develop new models for urban crime and carry out mathematical analysis of these models.  The project involves training of students and postdoctoral scholars at all levels, including a significant undergraduate component.  Ph.D. students and postdoctoral scholars will also obtain valuable mentoring experience necessary for development of their research careers.  The work includes direct interaction with local law enforcement agencies and the Institute for Pure and Applied Mathematics."
"1007167","Collaborative Research: Nonparametric Smoothing for Data with Multiple Components","DMS","STATISTICS","06/01/2010","05/18/2010","Hua Liang","NY","University of Rochester","Standard Grant","Gabor Szekely","05/31/2013","$99,994.00","","hliang@gwu.edu","910 GENESEE ST","ROCHESTER","NY","146113847","5852754031","MPS","1269","","$0.00","Over the decades, nonparametric smoothing has become a standard tool for many classical statistical problems owing partly to the boom of computing power. Relatively little work has addressed nonparametric smoothing in more complex settings where data have multiple components and the analysis requires nontrivial integration of techniques from different statistical domains. This project concerns three types of such complex data that are common in practice, and propose a suite of nonparametric statistical models in the framework of smoothing spline ANOVA models. Existing methods for these three types of data are mainly parametric and semi-parametric, whose practical uses are limited by their strong assumptions on the dependence structure of response on predictors or covariates. The nonparametric methods proposed in this project, combining nonparametric Gaussian regression, nonparametric logistic regression and nonparametric hazard rate estimation, offer much more flexibility and are extremely useful at the exploratory stage when researchers are not certain of the pattern of dependence. Accompanied with the proposed models are useful inference tools such as model selection and confidence intervals.  Asymptotic properties of the estimates are investigated through a combination of asymptotic analysis techniques for nonparametric smoothing splines, semiparametric estimation, and measurement error models.<br/><br/> <br/><br/>Major challenges to today's federal government, such as health care reform, education reform and financial system improvement, provide data with complex structures that call for accurate, informative and flexible data analysis methods. The proposed nonparametric smoothing methods provide an innovative direction for developing analysis tools appropriate for tackling these challenges. These types of data can also be found in a broad spectrum of scientific fields such as biological sciences, economics, social sciences, psychological sciences, and biomedical studies. This research will advance education and training of graduate and undergraduate students in the relevant statistical areas."
"1007513","Computer-intensive methods for nonparametric time series analysis'","DMS","STATISTICS","05/15/2010","02/23/2012","Dimitris Politis","CA","University of California-San Diego","Continuing Grant","Gabor Szekely","04/30/2014","$274,986.00","","dpolitis@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","","$0.00","The project focuses on the development of  methods of inference for the analysis of time series and random fields that do not rely on unrealistic or unverifiable model assumptions. In particular, the investigator and his colleagues are working on: (a) extending the range of  applicability of the AR-sieve bootstrap  beyond the setting of linear time series; (b) devising  a new Time-Frequency bootstrap procedure in which bootstrap pseudo-series are generated in the time domain although the resampling happens in the frequency domain; (c) devising a residual bootstrap scheme with larger resample size to be used for improved density estimation from time series data; (d) constructing an automatic method of efficient aggregation of spectral density estimators; (e)  testing for the support of a density, as well as testing for overdifferencing and estimating the   spectral density at a vanishing point; (f) devising an improved block bootstrap procedure to handle time series that are periodically or almost periodically correlated; (g)  resampling and inference for locally  stationary time series and inhomogeneous (but locally homogeneous) marked point processes; and (h) investigating  different aspects of resampling   with functional data,  including the difficult problem of   appropriately studentizing a functional statistic.<br/><br/><br/>Ever since the fundamental recognition of the potential role of the computer in modern statistics, the bootstrap and other computer-intensive statistical methods have been developed extensively for inference with independent data.  Such methods are even more important in the context of dependent data   where  the distribution theory for estimators and tests statistics may be  difficult or impractical to obtain.   Furthermore, the recent information explosion   has resulted in data sets of unprecedented size that call for flexible, nonparametric, computer-intensive  methods of data analysis. Time series analysis in particular  is vital in many diverse scientific disciplines, e.g., in economics, engineering, acoustics,  geostatistics, biostatistics,  medicine, ecology, forestry, seismology, and meteorology.  As a consequence of the proposal's development of efficient and robust methods for the statistical analysis of dependent data,   more accurate and reliable  inferences may be  drawn from  data sets of practical import resulting into appreciable benefits to society.  Examples include data from  meteorology/atmospheric science, such as climate data, economics, such as stock market returns, medicine, such as EEG data, and bioinformatics, such as genomic data."
"1007444","Matrix estimation under rank constraints for complete and incomplete noisy data","DMS","STATISTICS","06/01/2010","05/04/2011","Florentina Bunea","FL","Florida State University","Continuing Grant","Gabor Szekely","01/31/2012","$222,429.00","Marten Wegkamp","fb238@cornell.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1269","","$0.00","The central goals of this  proposal are:(a) to provide methods for the estimation of matrices of unknown rank from both completely and incompletely observed noisy matrices, using rank regularized risk minimization and (b) to establish novel oracle type risk bounds for the matrix estimates and the rank estimates, under minimal assumptions. The difficulty of the problem of recovering the underlying target matrix from an observed noisy matrix is that the number of independent parameters is large relative to the number of observations.  Special attention is given to multivariate response regression models. There is an interesting resemblance between matrix estimation under low rank assumptions and estimation in general regression models under sparsity assumptions, but matrix models pose different mathematical and computational challenges.<br/><br/><br/>High dimensional data arranged in matrix format are increasingly common in many scientific disciplines such as genetics, medical imaging, engineering, psychology and neuroscience. The matrices containing observed data in these areas tend to have high rank due to the presence of noise, but the signal matrix underlying the data may have significantly lower rank. Ignoring this in any inferential procedure may lead to poor recovery of the target, with severe repercussions on the interpretation of the results. Instances of targets that must be recovered with the highest possible precision include: faces against background, ensembles of genes that are associated with a disease, brain structures associated with cognitive processes, to name just a few example. Some of the challenges associated with the analysis of such data can be met via the methodological and  theoretical study of  the problem of matrix estimation under rank constraints. A second problem, which is substantially more difficult, is to perform the same task when only partially observed  noisy matrices are  available. Systematic investigation of these two problems is the focus of this proposal. The usefulness of these techniques will be immediately disseminated to the scientific community by applying them to data obtained from a study of the effects of HIV on brain structure and functions. Free software that implements the developed methodology will be made available on the web in a readily implementable form."
"1007719","Flexible Statistical Modeling","DMS","STATISTICS","08/01/2010","05/09/2013","Trevor Hastie","CA","Stanford University","Continuing Grant","Gabor Szekely","07/31/2015","$399,999.00","","hastie@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","This project studies methods for analyzing large datasets using L1 and<br/>related regularization. Coordinate descent algorithms are developed to<br/>provide entire families of solutions for L1 and more aggressive<br/>concave penalized regression problems. Applications include<br/>generalized linear regression models for very wide datasets, and<br/>structure-finding algorithms for undirected graphical models. L1<br/>regularization is used as well to develop efficient convex algorithms<br/>for finding low-rank approximations (SVDs) to extremely large,<br/>sparsely populated matrices.<br/><br/>This project develops tools with a wide variety of applications,<br/>illustrated here in medicine and merchandising. Modern technologies in<br/>genomics produce measurements of half a million or more<br/>genotypes at particular locations (SNPs) along an individual's genome<br/>in a few hours. Armed with such measurements on a few thousand<br/>individuals, some sick and some healthy, this project develops<br/>powerful statistical tools for identifying groups of SNPs associated<br/>with diseases such as Alzheimer's or breast cancer. Online movie<br/>renters or book buyers are often asked to rate their<br/>purchases. Although each individual sees a minuscule fraction of the<br/>selections available, the investigators are able to develop<br/>recommender systems that exploit the overlap to learn genres of<br/>movies, and assign viewers to like-minded cliques, and which allow<br/>them to make recommendations for products not yet seen."
"1007722","Detection with scan statistics and average likelihood ratio: Methodology","DMS","STATISTICS","07/01/2010","06/07/2012","Guenther Walther","CA","Stanford University","Continuing Grant","Gabor Szekely","06/30/2014","$256,601.00","","Walther@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","The project will study and compare various methods to detect clusters or `hot spots'. It will establish rigorous results about the average likelihood ratio statistic, which has recently been claimed on empirical grounds to be superior to the scan statistic. The investigator will derive guidelines for deciding when one is preferable to the other. The project will also examine how various approximation schemes affect the performance of the average likelihood ratio statistic in terms of power and computational complexity.<br/><br/>The problem of detecting spatial clusters or `hot spots' has received considerable attention in recent years, due to emerging important problems in various areas such as biosurveillance, the detection of radioactive materials, or the detection of illicit container shipments. Recent empirical findings suggest that the statistic that is commonly used for these purposes is suboptimal and can be improved upon by a different criterion. This project will perform a rigorous mathematical investigation of this empirical finding and will derive guidelines for deciding in which cases one methodology is preferrable to the other."
"1007732","Multiple Problems in Multiple Testing and Simultaneous Inference","DMS","STATISTICS","07/01/2010","06/04/2012","Joseph Romano","CA","Stanford University","Continuing Grant","Gabor Szekely","06/30/2015","$369,997.00","","romano@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","The investigator develops new methods and theory for problems in multiple testing and simultaneous inference.  The classical approach to dealing with multiplicity is to require decision rules that control the familywise error rate, the probability of rejecting at least one true null hypothesis. But when the number of tests is large, control of the familywise error rate is so stringent that alternative hypotheses have little chance of being detected. In response, the false discovery rate and other measures of error control have gained wide use.  For each measure of error control, it is desired to construct procedures that exhibit error control under the weakest possible assumptions. Resampling methods  offer viable approaches to obtaining valid distributional approximations while assuming very little about the stochastic mechanism generating the data. While many new methods have been developed, many more questions remain and are studied. Some of the technical challenges include:  asymptotics that grow with both sample size and number of tests; orders of error in approximation;  uniformity in approximation; optimality theory;  direction errors. Related problems are also studied, such as the statistical evaluation of bioequivalence across multiple measures, testing for stochastic dominance, and inference for partially identified econometric models.<br/><br/>Virtually any scientific experiment sets out to answer  questions about the process under investigation, which  often can  be translated formally into a set of hypotheses. It is the exception that a single hypothesis is considered. For example, in clinical trials, even a  single treatment  may be evaluated using multiple outcome measures,  multiple time points, multiple doses, and multiple subgroups.  Moreover, due to effects of ``data snooping'' (or ``data mining''), additional hypotheses  arise as well. The statistician is then faced with the challenge of accounting for all possible errors resulting from a complex data analysis, so that any resulting inferences or interesting conclusions can reliably be viewed as real structure rather than artifacts of random data.  In general, the philosophical approach is the development of practical methods that may be applied in increasingly complex situations as the scope of modern data analysis continues to grow. The broader impact of this work is potentially quite large because the resulting inferential tools can be applied to such diverse fields as genetics, bioengineering, image processing and neuroimaging, clinical trials, education,  astronomy, finance and econometrics.  For example, current methods in biotechnology and genomics generate DNA microarray experiments, where expression levels in cells for thousands of genes must be analyzed simultaneously. The many burgeoning fields of applications demand new statistical methods, creating challenging and exciting opportunities for young scholars under the direction of the PI."
"1006281","Dynamic Functional Regression Models","DMS","STATISTICS","06/01/2010","03/12/2012","Daniel Gervini","WI","University of Wisconsin-Milwaukee","Continuing Grant","Gabor Szekely","05/31/2014","$149,932.00","","gervini@uwm.edu","3203 N DOWNER AVE","MILWAUKEE","WI","532113153","4142294853","MPS","1269","","$0.00","The analysis of samples of curves is a field of growing relevance in statistics. Samples of curves arise when a time-dependent process is observed on a group of individuals. Such curves present both time and amplitude variability (""horizontal"" and ""vertical"" variability), and both types of variation have to be statistically modeled in order to draw valid inference from the data. The investigator is developing dynamic regression models, that is, models for prediction of response curves based on explanatory curves, that explicitly take into account time variability. The properties of these methods are being studied theoretically, by simulation, and by the analysis of real-life data sets. Computer software implementing these methods is also being developed.<br/><br/>Data consisting of samples of curves include, among many others, human growth curves, time-dependent gene expression profiles, daily air pollutant concentrations, and stock prices. The investigator's work will help scientists work on a broad range of applications and these statistical techniques will help provide new insights into scientific areas as diverse as medicine, genetics, environmental studies, and economics."
"1025396","A NISS/ASA Writing Workshop for New Researchers","DMS","STATISTICS","03/15/2010","04/07/2010","Keith Crank","VA","American Statistical Association","Standard Grant","Gabor Szekely","02/28/2011","$20,000.00","Nell Sedransk","kcrank@gmu.edu","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","0000, 7556, OTHR","$0.00","This award will support a course on effective technical writing for new researchers in the statistical sciences who seek to publish their research or to present their research plans in the form of grant proposals for federal funding.  New researchers often have difficulty disseminating their research results, not because of the quality of the research, but rather because of inappropriate choices of publication venues for the particular research and/or because of poor presentation of technical material to the chosen audience. The National Institute of Statistical Sciences and the American Statistical Association will manage the workshop.   This workshop will open with tutorial sessions on the organization of material for a technical article or grant application, on technical writing techniques and on the specific missions and audiences of key journals in the statistical sciences. Then each participating new researcher will work individually with an experienced journal editor as mentor to address these issues on an individualized basis for draft of the new researcher's work in progress. Revisions following this guidance will be critiqued by the mentor to assure that the new researcher's implementation of writing techniques has been successful before the article or the grant proposal is submitted for review. The Statistics Program Officers agree that this course is essential to the health of the profession and recommend its support at the requested level of funding.<br/><br/><br/>This award will support a course on effective technical writing for new researchers in the statistical sciences who seek to publish their research or to present their research plans in the form of grant proposals for federal funding.  New researchers often have difficulty disseminating their research results, not because of the quality of the research, but rather because of inappropriate choices of publication venues for the particular research and/or because of poor presentation of technical material to the chosen audience. The National Institute of Statistical Sciences and the American Statistical Association will manage the workshop.   This workshop will open with tutorial sessions on the organization of material for a technical article or grant application, on technical writing techniques and on the specific missions and audiences of key journals in the statistical sciences. Then each participating new researcher will work individually with an experienced journal editor as mentor to address these issues on an individualized basis for draft of the new researcher's work in progress. Revisions following this guidance will be critiqued by the mentor to assure that the new researcher's implementation of writing techniques has been successful before the article or the grant proposal is submitted for review. The Statistics Program Officers agree that this course is essential to the health of the profession and recommend its support at the requested level of funding."
"1007512","Workshop on Statistical Analysis of Neural Data (SAND)","DMS","STATISTICS","03/01/2010","03/10/2010","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","02/28/2011","$15,000.00","","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","7556","$0.00","The workshop Statistical Analysis of Neuronal Data (SAND5) is planned for May 20-22, 2010. SAND5 will bring together neurophysiologists, statisticians, physicists, and computer scientists who are interested in quantitative analysis of neuronal data. There will be 5 scientific sessions, at which 8 keynote investigators and 16 junior investigators will speak. There will also be a poster session. Selected papers will be published in the Journal of Computational Neuroscience.<br/><br/>Statistical Analysis of Neuronal Data is the fifth workshop in a series that began in 2002. The workshops are held in even years at Carnegie Mellon University during the Spring. The primary objectives of the workshop are to define important problems in neuronal data analysis and useful strategies for attacking them; foster communication between experimental neuroscientists and those trained in statistical and computational methods; and provide further dissemination of the findings presented at the workshop via a set of peer-reviewed articles. Secondary objectives are to encourage young researchers, including graduate students, to present their work; expose young researchers to important challenges and opportunities in this interdisciplinary domain, while providing a small meeting atmosphere to facilitate the interaction of young researchers with senior colleagues; and include as participants women, under-represented minorities and persons with disabilities who might benefit from the small workshop environment.<br/>"
"0943577","EMSW21-RTG: Statistics and Machine Learning for Scientific Inference","DMS","STATISTICS","06/01/2010","05/12/2010","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","05/31/2012","$349,996.00","William Eddy, Kathryn Roeder, Larry Wasserman, Christopher Genovese","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, 7301","$0.00","Statistics curricula have required excessive up-front investment in statistical theory, which many quantitatively-capable students in ``big science'' fields initially perceive to be unnecessary.  A training program at Carnegie Mellon will expose students to cross-disciplinary research early, showing them the scientific importance of ideas from statistics and machine learning, and the intellectual depth of the subject. Graduate students will receive instruction and mentored feedback on cross-disciplinary interaction, communication skills, and teaching. Postdoctoral fellows will become productive researchers who understand the diverse roles and responsibilities they will face as faculty or members of a research laboratory.<br/><br/>The statistical needs of the scientific establishment are huge, and growing rapidly, making the current rate of workforce production dangerously inadequate.  The Department of Statistics at Carnegie Mellon University will train undergraduates, graduate students, and postdoctoral fellows in an integrated program that emphasizes the application of statistical and machine learning methods in scientific research. The program will build on existing connections with computational neuroscience, computational biology, and astrophysics.Carnegie Mellon will recruit students from a broad spectrum of quantitative disciplines, with emphasis on computer science.  Carnegie Mellon already has an unusually large undergraduate statistics program. New efforts will strengthen the training of these students, and attract additional highly capable students to be part of the pipeline entering the mathematical sciences."
"1007773","Collaborative Research: Bayesian Analysis and Applications","DMS","STATISTICS","06/01/2010","05/29/2013","James Berger","NC","Duke University","Continuing Grant","Gabor Szekely","05/31/2015","$333,000.00","","berger@stat.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","","$0.00","Five research areas in Bayesian analysis, involving theory, methodology and application, will be pursued: objective Bayesian analysis, multiplicity adjustment, search and approximations in model selection, analysis of complex computer models, and differences between Bayes and empirical Bayes analysis. Research in objective Bayesian analysis will focus on the development of objective priors, together with their computational implementation, in semi-invariant contexts, which include spatial problems and problems arising in psychiatry. The Bayesian approach to multiplicity correction has the attraction that it does not depend on the error structure of the data; multiplicity correction is done only through the prior probabilities assigned to models or other multiplicity features. Understanding which probability assignments do, and do not, adjust for multiplicity will be an important feature of this research. A focus of the research on model selection will be the development of a generalization of BIC which is much more widely applicable than the standard version, especially overcoming the major hurdle of defining effective sample size for a parameter. Advances in these areas will have application to research involving the analysis and use of complex computer models of processes. Also, surprising differences between Bayes and empirical Bayes analysis arise in several of the above settings, and better understanding of these differences will also be a focus of the research.<br/><br/>Objective Bayesian analysis has existed for over 250 years, but interest in the field has increased markedly in recent years. A major reason is that many of the significant scientific problems today (such as much of climate change research) involve some type of assimilation of data and physical modeling, typically done by Bayesian methods. Many of today?s most challenging problems ? including microarray and other bioinformatic analyses, syndromic surveillance, high-throughput screening, and many others ? involve consideration of multiple-testing with a huge number of possible tests, and require major multiplicity adjustments. For instance, the work on multiplicity will be done in the context of subgroup analysis in clinical trials, providing major new insights into HIV vaccine trials, and in refining detection methodology in high-energy physics."
"1007775","Sequential testing of multiple hypotheses, simultaneous confidence estimation, and multichannel change-point detection","DMS","STATISTICS","06/01/2010","03/05/2012","Michael Baron","TX","University of Texas at Dallas","Continuing Grant","Gabor Szekely","05/31/2014","$200,000.00","","baron@american.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1269","","$0.00","The project focuses on the development of new theory and methodology of sequential multiple comparisons. It aims to develop cost-minimizing methods and supporting theory for conducting multiple statistical inferences sequentially. This includes testing multiple hypotheses, constructing sequences of simultaneous confidence sets, detecting changes in multiple channels, and making other sequential statistical decisions involving multiple parameters or multiple measurements. This study extends the recently obtained step-up and step-down procedures for multiple comparisons to sequential designs. It searches for optimal stopping rules that minimize the expected cost of the experiment while controlling for the false positive and false negative rates. The new methodology combines flexibility and cost-optimization of sequential procedures with the ability of modern statistical methods for multiple comparisons to control the familywise error rate and power. Proposed sequences of simultaneous confidence sets generalize the idea of repeated confidence intervals to the case of multiple parameters and achieve the desired overall confidence level. The new multiple hypothesis testing methodology is used for the derivation of sequential change-point detection algorithms sensitive to a change in any one or several parameters.<br/><br/>Deliverables of the project include a sound statistical methodology for designing multiple comparison experiments at the minimum expected cost. One of the main applications is in sequential clinical trials that are conducted to answer multiple questions, for example, about the efficacy and safety of the tested treatment. Cost-optimization of such medical studies ultimately results in the reduced cost of health care. The new change-point detection procedures allow simultaneous tracking of changes in multiple parameters, which is used for the timely discovery of epidemic and pre-epidemic patterns and bioterrorist attacks. Controlling for the rate of false alarms, proposed change-point detection schemes are aimed to minimize the expected detection delay ensuring prompt reaction to unexpected changes. Their application sheds light to a number of global questions. Is the economy (welfare, climate, environment) changing? In what way and what direction is it changing? When did the change begin? Does the change continue, or has the process stabilized? Proposed sequential statistical tools address these and other important questions that involve multiple statistical comparisons."
"1007874","Collaborative Research: Bayesian Analysis and Applications","DMS","STATISTICS","06/01/2010","08/13/2013","Dongchu Sun","MO","University of Missouri-Columbia","Continuing Grant","Gabor Szekely","05/31/2015","$150,000.00","","sund@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269","","$0.00","Five research areas in Bayesian analysis, involving theory, methodology and application, will be pursued: objective Bayesian analysis, multiplicity adjustment, search and approximations in model selection, analysis of complex computer models, and differences between Bayes and empirical Bayes analysis. Research in objective Bayesian analysis will focus on the development of objective priors, together with their computational implementation, in semi-invariant contexts, which include spatial problems and problems arising in psychiatry. The Bayesian approach to multiplicity correction has the attraction that it does not depend on the error structure of the data; multiplicity correction is done only through the prior probabilities assigned to models or other multiplicity features. Understanding which probability assignments do, and do not, adjust for multiplicity will be an important feature of this research. A focus of the research on model selection will be the development of a generalization of BIC which is much more widely applicable than the standard version, especially overcoming the major hurdle of defining effective sample size for a parameter. Advances in these areas will have application to research involving the analysis and use of complex computer models of processes. Also, surprising differences between Bayes and empirical Bayes analysis arise in several of the above settings, and better understanding of these differences will also be a focus of the research.<br/><br/>Objective Bayesian analysis has existed for over 250 years, but interest in the field has increased markedly in recent years. A major reason is that many of the significant scientific problems today (such as much of climate change research) involve some type of assimilation of data and physical modeling, typically done by Bayesian methods. Many of today?s most challenging problems ? including microarray and other bioinformatic analyses, syndromic surveillance, high-throughput screening, and many others ? involve consideration of multiple-testing with a huge number of possible tests, and require major multiplicity adjustments. For instance, the work on multiplicity will be done in the context of subgroup analysis in clinical trials, providing major new insights into HIV vaccine trials, and in refining detection methodology in high-energy physics."
"1007478","Hierarchical Bayesian Random Sets with Applications to Growth Models","DMS","STATISTICS","05/15/2010","05/10/2010","Athanasios Micheas","MO","University of Missouri-Columbia","Standard Grant","Gabor Szekely","04/30/2014","$115,000.00","","amicheas@stat.missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269","","$0.00","Knowing if a medical treatment is effective or anticipating the path of a severe storm is of great importance to the public in terms of potential loss of life and property. Such experiments or phenomena can be described by defining appropriate random sets, which can be done is several ways. A ""hitting function"" approach involves calculation of the probabilities that the set intersects a given class of test sets, e.g., discs. The major difficulty is that a likelihood cannot be specified in a simple way in order to provide inference on model parameters, while the choice of test set can potentially alter the parameter estimates. In addition, random sets can often be viewed as marked point processes (MPP), where the marks define the characteristics of a well known geometric object, e.g., the radius of a disc centered at an event from the point process. However elegant this view might be, it does not eliminate the aforementioned problems in obtaining a likelihood for the random set and conducting a statistical analysis. In this proposal, the investigator considers an approach that models a point in the random set and not the entire set directly. In this way, random sets are viewed as if they were created by an underlying process that helps realize the observed data, and thus the observed data are modeled hierarchically, given the underlying random process. The hierarchy can be described as [Data|Process]x[Process|Parameters]x[Parameters]. Thus, an alternative approach, such as a MMP, serves as the second stage in this hierarchical formulation. This approach is more general with its major advantage being that it easily facilitates inference on model parameters. The investigator studies several models for random sets based on the general setting of a Boolean model, as well as, models to capture the evolution of the random object over time. The inclusion of relevant covariate information is also considered. The investigator efficiently models a random set via a multistage hierarchical Bayesian framework. Several novel growth models are proposed, as well as the corresponding Bayesian formulation that provides inference and prediction. The models are applied to modeling forestry (random tree objects) as well as storm cell development as obtained from weather radar over time (storm cell evolution).<br/><br/><br/>    Every experiment or phenomenon can be described by defining appropriate objects that describe its main characteristics. Thus, understanding the characteristics of an object and its evolution is needed in almost every scientific discipline. From observing tumor growth to assess the effectiveness of a medical treatment, studying the movement of storm or tornado systems based on radar images, to the investigation of an epidemic as it spreads through a populated area, defining and studying growth models has become an increasingly critical research topic. Such research is of great importance to the public in terms of potential loss of life and property, since knowing, for example, the path of a tornado or a hurricane can then be used to provide ample warning to the population of the area, in an effort to minimize the effects of these disasters. The proposed methodology not only helps to better understand the evolution of objects in the four dimensional world we live in, but also provides a measure of uncertainty in the forecasts, whereas, traditional deterministic models require perfect knowledge of the phenomenon of interest, which is typically an unrealistic assumption, and thus are often inadequate to describe the phenomenon. The investigator's major concern is the development and study of statistical models that capture the evolution of such random objects."
"1045137","Travel Support for the 58th Session of the International Statistical Institute, August 2011, Dublin, Ireland","DMS","STATISTICS","09/15/2010","09/08/2010","Martha Aliaga","VA","American Statistical Association","Standard Grant","Gabor Szekely","08/31/2011","$24,145.00","","martha@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","7556","$0.00","The American Statistical Association (ASA) will use this grant for 20 United States participants to attend the 58th Session of the International Statistical Institute (ISI) in Dublin, Ireland from August 21-26, 2011.  The ISI meeting includes the meetings of the Bernoulli Society, International Association for Official Statistics (IAOS), International Association for Statistical Computing (IASC), International Association of Survey Statisticians (IASS), and the International Association for Statistical Education (IASE).  Thus it is an umbrella meeting with sessions of interest for thousands of statisticians.  The travel grant will provide support to defray transportation costs for individuals selected from institutions and non-profit associations. An emphasis of the award is to encourage and provide the opportunity for younger statisticians to participate in the meeting.  Participants will be notified of the availability of the travel grant through Amstat News, a membership publication of the ASA, and on the ASA Home Page on the Internet. Notices will be provided to university and college statistics and mathematics departments to encourage younger statisticians to apply for the travel grant. A review and selection committee will be established to review the applications and select grantees.  The committee will be comprised of three or more ASA members and will convene at the ASA office in Alexandria, Virginia.  Special consideration will be given to statisticians who have recently received their Ph.D.?s and to women and minorities.<br/><br/><br/>The American Statistical Association (ASA) will use this grant for 20 United States participants to attend the 58th Session of the International Statistical Institute (ISI) in Dublin, Ireland from August 21-26, 2011.  The ISI meeting includes the meetings of the Bernoulli Society, International Association for Official Statistics (IAOS), International Association for Statistical Computing (IASC), International Association of Survey Statisticians (IASS), and the International Association for Statistical Education (IASE).  Thus it is an umbrella meeting with sessions of interest for thousands of statisticians.  The travel grant will provide support to defray transportation costs for individuals selected from institutions and non-profit associations. An emphasis of the award is to encourage and provide the opportunity for younger statisticians to participate in the meeting.  Participants will be notified of the availability of the travel grant through Amstat News, a membership publication of the ASA, and on the ASA Home Page on the Internet. Notices will be provided to university and college statistics and mathematics departments to encourage younger statisticians to apply for the travel grant. A review and selection committee will be established to review the applications and select grantees.  The committee will be comprised of three or more ASA members and will convene at the ASA office in Alexandria, Virginia.  Special consideration will be given to statisticians who have recently received their Ph.D.?s and to women and minorities."
"1007698","Variable Selection Methods in High Dimensional Feature Space","DMS","STATISTICS","09/01/2010","08/20/2010","Rui Song","CO","Colorado State University","Standard Grant","Gabor Szekely","11/30/2012","$100,000.00","","rsong@ncsu.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1269","","$0.00","With rapid advances of computing power and other modern technology, high-throughput data of unprecedented size and complexity are becoming a commonplace in diverse fields. Examples include data from genetic, microarrays, proteomics, fMRI, cancer clinical trials and high frequency financial data. These high dimensional data characterize many important contemporary problems in statistics and feature selection play pivotal roles in these problems. This research project aims to develop cutting-edge statistical theory and methods for high dimensional variable selections. In particular, the PI proposes the following interrelated research topics for investigation: (1) grouped-variables screening with sparse linear models; (2) nonparametric components screening with sparse additive models; (3) parametric components screening with sparse semiparametric models and(4) their further extensions. The proposed methods will be studied theoretically for their sure screening behavior and compared with some of the existing methods empirically in terms of computational expediency, statistical accuracy and algorithmic stability.<br/><br/>The outlined research project on variable selection in high dimensions tries to tackle fundamental problems in statistical learning and will stimulate interests from a large group of scientists and researchers in diverse fields of sciences, engineering and humanities ranging from genomics and health sciences to economics and finance. Another key aspect of this project is the integration of research and education, which will be achieved by developing two new courses on statistical learning and non-, semi-parametric inference and proposing specific projects for students during the teaching of classes. It will enable the participation of all citizens from various disciplines, including underrepresented groups of students."
"1007543","Collaborative Research: Generalized Fiducial Inference - An Emerging View","DMS","STATISTICS","09/01/2010","06/15/2012","Jan Hannig","NC","University of North Carolina at Chapel Hill","Continuing Grant","Gabor Szekely","08/31/2014","$125,000.00","","jan.hannig@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","This proposal is motivated by the success of Generalized Fiducial Inference as introduced by the PIs as a generalization of Fisher's fiducial argument. As a result of the many studies conducted by the PIs on the theory and applications of generalized fiducial methods the following important conclusions can be made: (a) A unified and systematic procedure is available for developing fiducial solutions for large classes of problems; (b) The fiducial approach generally leads to very efficient inference procedures and thus they are competitive with procedures developed using other approaches;  (c) Fiducial procedures are asymptotically correct in large classes of problems;  (d) Many fiducial distributions can also be realized as a Bayesian posterior by an appropriate choice of a prior. However, this is not always possible, which establishes that the two approaches are not equivalent in general; (e) Both the Bayesian approach and the fiducial approach lead to useful interval inference procedures as have been established in various publications in both areas. It is clear that neither approach can claim to dominate the other; (f) Both approaches typically require MCMC simulations in regards to actual numerical computation of the required posterior or fiducial distributions. After giving due consideration to areas of statistical inference where a fiducial approach is expected to lead to new and useful results, both theoretical and practical, the PIs propose to conduct research into the following topics: (a) Extensions of Interval Data fiducial framework for Generalized Linear Mixed Models together with associated computational approaches; (b) Extension of the work of the PIs to address the model selection problem within the Generalized Fiducial Inference framework; (c) Definition and investigation of the concept of a Robustified Fiducial Distribution for a parameter and development of computational methods for calculating it from data; (d) Application of robust fiducial approaches to arrive at new robust inference methods in some standard parametric examples; (e) Development of some general computational strategies for implementation of fiducial methods for complex practical problems.<br/><br/>This proposal studies a new approach to statistical inference based on Fisher's fiducial argument. The implications of this work will have an immediate effect on public policy. For instance, the U.S. Food and Drug Administration (FDA) guidance document spells out analysis procedures for demonstration of equivalence of two or more drug formulations.  The PIs aim to show that the fiducial approach will lead to more efficient procedures, which will result in cost and time savings, an important issue for the drug industry. In metrology, the International Bureau of Weights and Measures (BIPM) in conjunction with the International Organization for Standardization (ISO), has published a ``Guide to Expression of Uncertainty in Measurements'' (GUM) which gives the procedures to be followed by national metrological institutes such as NIST in the US, NPL in UK, and PTB in Germany. A problem that is unique to metrology is that every measurement is subject to unknown and unknowable systematic errors that are often larger than random errors. The only currently known way to quantify these unknowable systematic errors is via specification of subjective distributions for them. The GUM specifies some ad hoc methods for combining data-based estimates of standard deviations for some error components and subjective estimates of uncertainty for other error components.  The PIs aim to demonstrate that the fiducial method provides a new natural approach for accomplishing this. Such results are likely to influence the metrology community in modifying and improving their current procedures."
"1007126","Collaborative Research: Nonparametric smoothing for data with multiple components","DMS","STATISTICS","06/01/2010","05/18/2010","Pang Du","VA","Virginia Polytechnic Institute and State University","Standard Grant","Gabor Szekely","05/31/2013","$100,006.00","","pangdu@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1269","","$0.00","Over the decades, nonparametric smoothing has become a standard tool for many classical statistical problems owing partly to the boom of computing power. Relatively little work has addressed nonparametric smoothing in more complex settings where data have multiple components and the analysis requires nontrivial integration of techniques from different statistical domains. This project concerns three types of such complex data that are common in practice, and propose a suite of nonparametric statistical models in the framework of smoothing spline ANOVA models. Existing methods for these three types of data are mainly parametric and semi-parametric, whose practical uses are limited by their strong assumptions on the dependence structure of response on predictors or covariates. The nonparametric methods proposed in this project, combining nonparametric Gaussian regression, nonparametric logistic regression and nonparametric hazard rate estimation, offer much more flexibility and are extremely useful at the exploratory stage when researchers are not certain of the pattern of dependence. Accompanied with the proposed models are useful inference tools such as model selection and confidence intervals.  Asymptotic properties of the estimates are investigated through a combination of asymptotic analysis techniques for nonparametric smoothing splines, semiparametric estimation, and measurement error models.<br/><br/> <br/><br/>Major challenges to today's federal government, such as health care reform, education reform and financial system improvement, provide data with complex structures that call for accurate, informative and flexible data analysis methods. The proposed nonparametric smoothing methods provide an innovative direction for developing analysis tools appropriate for tackling these challenges. These types of data can also be found in a broad spectrum of scientific fields such as biological sciences, economics, social sciences, psychological sciences, and biomedical studies. This research will advance education and training of graduate and undergraduate students in the relevant statistical areas."
"1007583","Model Functional Data Through a Local FPCA Framework","DMS","STATISTICS","07/15/2010","07/10/2010","Jie Peng","CA","University of California-Davis","Standard Grant","Gabor Szekely","06/30/2014","$149,685.00","","jiepeng@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","This research is motivated by numerous real life problems whose modeling and analysis involve functional data, i.e., data where the measurements per subject/replicate correspond to values of a function (referred to as sample trajectory).  In particular, this research is motivated by functional clustering problems and functional data which are dynamical in nature. Functional principal components analysis (FPCA) has been widely used in analyzing functional data. In spite of its success, FPCA tends to be inefficient if the geometry of the trajectory space is non-Euclidean, especially when sample trajectories are only observed at sparse sets of time points, as is the case for many scientific studies. Sources for such nonlinearity include but not limited to the existence of underlying clusters of the sample trajectories, or the sample trajectories being governed by a nonlinear dynamical system. The investigator proposes a new strategy (referred to as the local FPCA framework) for analyzing sparsely and noisily observed functional data.  It aims to derive more efficient localized representations for sample trajectories which take into account geometric structures of the trajectory space. This framework combines the principles underlying functional principal components analysis with the notions of functional clustering and nonlinear dimensionality reduction. Specific aims of this research include: (a) Develop a local FPCA framework which clusters the sample trajectories into homogeneous subgroups and applies FPCA within each cluster to derive more efficient representations of the sample trajectories. (b) Fit ordinary differential equation models with random parameters by a model-based local FPCA approach.  (c) Study theoretical aspects of the proposed methods and apply them to various scientific problems.<br/><br/>This research will produce a new set of statistical tools for scientists working in various fields such as plant biology, ecology and epidemiology who must analyze longitudinal/functional data. In particular, this research is a stepping stone toward understanding complex dynamical systems. The PI is collaborating with scientists on studying HIV disease dynamics at a population level, and this research helps achieving a better understanding of these systems which hold important implications in the pathologies of AIDS. The computational and analytical tools resulted from this research are also likely to stimulate further studies in related fields. Moreover, this research develops open source software that is freely available to the whole scientific community. Facing complex data and challenging questions, a new generation of researchers needs to be trained in an inter-disciplinary manner. The broader training component includes exposing statistics/biostatistics students to real scientific problems involving functional data.  On the other hand, through collaborations, scientists working in related fields are able to enhance their quantitative analysis skills."
"0954704","CAREER: Streaming Data Analysis in Sensor Networks","DMS","STATISTICS","06/01/2010","03/11/2014","Yajun Mei","GA","Georgia Tech Research Corporation","Continuing Grant","Gabor Szekely","05/31/2016","$400,000.00","","yajun.mei@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","0000, 1045, 1187, OTHR","$0.00","This research aims to offer statistical foundation and a host of efficient scalable methodologies for streaming data analysis in sensor networks. In many applications, sensor networks are deployed to online monitoring of changing environments over time and space, with a goal of early detection of some particular trigger events that can cause significant damage. However, the nature of streaming data from distributed, diverse sources and the constrained network resources (on communication, computing, costs, privacy of raw data, etc.) pose significant challenges, which require the development of new statistical tools, methods and theories. In this project, the investigator proposes a novel general framework for monitoring sensor networks in which a trigger event may affect different sensors or data streams differently. Some specific research topics include pure (consensus or parallel) detection and inference after detection, under different scenarios, depending on the models for sensor observations and the design requirements of sensor protocols. In addition, the research will integrate research and education by infusing the research findings into the curriculum, by organizing seminars and workshops, and by advising graduate and undergraduate students.<br/><br/>Senor networks have broad real-world applications, including but not limited to health and environmental monitoring, biomedical signal processing, wireless communication, intrusion detection in computer networks, and biosurveillance. On the one hand, this research project will offer crucial statistical tools to effectively and efficiently monitor and analyze dynamic data streams in these sensor network applications. On the other hand, it also has a frustrating yet profound implication in these applications: Faced with the limitations implied by the (asymptotic) optimality theories of the proposed research, practitioners and researchers may need to constantly look for better data sources to achieve desired system performance in their specific applications rather than relying on an improved methodology for existing data sources."
"1005336","Collaborative Research: Reducing Computation in Empirical Likelihood Methods","DMS","STATISTICS","09/01/2010","08/27/2010","Liang Peng","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","08/31/2013","$160,001.00","","lpeng@gsu.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","","$0.00","The study of empirical likelihood methods has attracted much attention in many areas of statistics. This proposal develops new procedures to overcome the computational burden in applying these methods to inference problems in different settings such as: estimating equations, stationary sequences, high-dimensional data, Pickands dependence functions in multivariate extreme-value distributions and  copulas in risk management. A practical obstacle in using empirical likelihood methods is the computational issue due to the high dimensional data, the large size of nuisance parameters as well as dependence in the data. The investigators intend to develop new methodologies to overcome the computational problems and  extend the scope of the potential applications.<br/><br/>The new research results can be applied to economics, insurance, finance, risk management and other social sciences that require effective tools for exploring nonlinear dependence among multivariate series and to increase the complexity of the models."
"1007877","Collaborative Research: Inference for Statistical Graphics","DMS","STATISTICS","09/15/2010","09/02/2010","Hadley Wickham","TX","William Marsh Rice University","Standard Grant","Gabor Szekely","08/31/2013","$59,808.00","","hadley@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","","$0.00","Since the publication of the NSF landmark report ""Visualization in<br/>Scientific Computing"" in 1987, computer-aided visualization has been<br/>recognized as one of the most potent tool sets for scientific<br/>discovery. However, discoveries based on data displays are often<br/>criticized because they are not secured by statistical inference. The<br/>team of researchers from Iowa State University, Rice University and<br/>University of Pennsylvania is addressing exactly this issue by<br/>bringing the rigors of statistical inference to visual data<br/>exploration. Statistical inference for plots are cast as comparison of<br/>a plot of the actual data with plots of null data simulated under a<br/>null hypothesis. If the actual plot stands out from a background of<br/>""null plots"", it amounts to the rejection of the null hypothesis.<br/>Executing this idea leads to rigorous protocols that can confer proper<br/>statistical significance to visual discoveries. Tools of mathematical<br/>statistics are employed to reduce composite null hypotheses to single<br/>reference distributions: conditioning on a minimal sufficient<br/>statistic, bootstrap plug-ins, and posterior predictive sampling. The<br/>protocols also have the potential to shift the perception of<br/>exploration-based findings in the scientific communities and<br/>dramatically increase the impact that these findings are allowed to<br/>have. The testing protocols will be made accessible with<br/>implementation in the open-source R language.<br/><br/>Data graphics are an essential part of communicating information. But<br/>how reliable is the information that we gather from them? The<br/>investigators will develop a rigorous framework for visual inference<br/>modeled after formal statistical testing. This framework allows the <br/>reader of a graphic to determine whether structure is real or spurious <br/>(is that a man in the moon, or just some rocks?). These protocols have <br/>the potential to shift the perception of exploration-based findings in <br/>the scientific community and dramatically increase the impact of <br/>exploratory work. Some aspects of the protocols are so intuitive that <br/>they can be used for general audiences and integrated in the teaching <br/>of introductory statistics at from grade school to college."
"1007678","Large-Scale Multinomial Inference and Its Applications in Genome-Wide Association Studies","DMS","STATISTICS","08/01/2010","05/07/2012","Chuanhai Liu","IN","Purdue University","Continuing Grant","Gabor Szekely","07/31/2014","$230,000.00","Jun Xie","chuanhai@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1269","","$0.00","The investigators study statistical analysis of multinomial counts with a<br/>large number K of categories and a small number n of sample size. This<br/>""large K and small n"" problem is a very challenging problem that requires thinking about statistical inference at a fundamental level. Employing an auxiliary data-generating approach to reason directly toward probabilistic inference , the PIs develop methods that are probabilistic and have desirable frequency properties. A sequence of topics to be investigated include 1) one-sample and two-sample ""large K and small n"" multinomial inference; 2) large-scale simultaneous hypothesis testing; 3) application in genome-wide association study; and 4) associated efficient computational methods.<br/> <br/>The ""large K and small n"" multinomial inference is motivated by genome-wide<br/>association studies with a large number of genotypes from single<br/>nucleotide polymorphisms (SNPs) data. SNPs are major genetic variants that may<br/>associate with common diseases such as cancer and heart disease. With new <br/>statistical methods and computing software to be developed in the project, the<br/>research is expected to generate useful tools for applied statisticians and<br/>scientists who are challenged by very-high-dimensional count data."
"1019634","The Fourth Erich L. Lehmann Symposium -- Optimality; May 9-12, 2011; Rice University","DMS","STATISTICS","08/01/2010","07/16/2010","Javier Rojo","TX","William Marsh Rice University","Standard Grant","Gabor Szekely","07/31/2012","$25,000.00","","jrojo@iu.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","7556","$0.00","The conference is the fourth of a series of symposia. The goal of the Symposia is to examine the role that Optimality can play, or should play, in modern statistics. Due to the advent of high throughput data collection technology and the parallel development of computing power to analyze such data, it often happens that statistical theory gives way to raw computing power. Although most of the new exciting computational/statistical methodologies have provided tools to make headway in many important scientific problems, a need to generalize and systematize this knowledge is now quite evident. The Symposia will bring together a group of experts to discuss cutting-edge research optimality ideas in the context of modern statistical methodologies. It is believed that, although much progress has taken place in areas such as data visualization and data mining and knowledge discovery among others, the subjects are ripe for the development of an optimality paradigm that allows for objective comparisons of methodologies. This new paradigm, although still to be defined, is necessary to push the research frontiers in these important areas. The conference will showcase new developments by leading researchers in an environment conducive to the development of new human resources and an opening session will showcase the work of young investigators.<br/><br/>With the substantial contributions that statistics continues to make to the analyses of massive high-dimensional data arising in the biomedical sciences, national security, reliability of urban infrastructures, atmospheric sciences, etc, the need to synthesize this knowledge to more efficiently and effectively analyze such data has come to the forefront of the discipline. Current statistical efforts, for example, leading to a better understanding of the stochastic behavior of the power grid, should help in the creation of an intelligent grid that can better respond to changes in the grid's status and thus avert cascading failures that currently cost in the order of $104 billion dollars in the United States alone. The symposium will provide a forum to showcase the exciting and impacting theoretical work that needs to be developed to better understand the behavior of these complex systems. In addition, the symposium will provide the environment for the maturing of young researchers and the development of more human resources in these important areas."
"1007689","Collaborative Research: Inference for Statistical Graphics","DMS","STATISTICS","09/15/2010","09/02/2010","Andreas Buja","PA","University of Pennsylvania","Standard Grant","Gabor Szekely","08/31/2013","$129,999.00","","buja.at.wharton@gmail.com","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","","$0.00","Since the publication of the NSF landmark report ""Visualization in<br/>Scientific Computing"" in 1987, computer-aided visualization has been<br/>recognized as one of the most potent tool sets for scientific<br/>discovery. However, discoveries based on data displays are often<br/>criticized because they are not secured by statistical inference. The<br/>team of researchers from Iowa State University, Rice University and<br/>University of Pennsylvania is addressing exactly this issue by<br/>bringing the rigors of statistical inference to visual data<br/>exploration. Statistical inference for plots are cast as comparison of<br/>a plot of the actual data with plots of null data simulated under a<br/>null hypothesis. If the actual plot stands out from a background of<br/>""null plots"", it amounts to the rejection of the null hypothesis.<br/>Executing this idea leads to rigorous protocols that can confer proper<br/>statistical significance to visual discoveries. Tools of mathematical<br/>statistics are employed to reduce composite null hypotheses to single<br/>reference distributions: conditioning on a minimal sufficient<br/>statistic, bootstrap plug-ins, and posterior predictive sampling. The<br/>protocols also have the potential to shift the perception of<br/>exploration-based findings in the scientific communities and<br/>dramatically increase the impact that these findings are allowed to<br/>have. The testing protocols will be made accessible with<br/>implementation in the open-source R language.<br/><br/>Data graphics are an essential part of communicating information. But<br/>how reliable is the information that we gather from them? The<br/>investigators will develop a rigorous framework for visual inference<br/>modeled after formal statistical testing. This framework allows the <br/>reader of a graphic to determine whether structure is real or spurious <br/>(is that a man in the moon, or just some rocks?). These protocols have <br/>the potential to shift the perception of exploration-based findings in <br/>the scientific community and dramatically increase the impact of <br/>exploratory work. Some aspects of the protocols are so intuitive that <br/>they can be used for general audiences and integrated in the teaching <br/>of introductory statistics at from grade school to college."
"1007675","New Theory and Methodology for Large-Scale Multiple Testing","DMS","STATISTICS","08/01/2010","05/06/2011","Wenguang Sun","NC","North Carolina State University","Continuing Grant","Gabor Szekely","08/31/2012","$107,176.00","","wenguans@marshall.usc.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","","$0.00","Large-scale multiple testing is an important and rapidly growing area in modern Statistics. The proposed research focuses on new theories, methodologies and computational algorithms to address the fundamental questions and new challenges in this field. The investigator develops new concepts, data-driven schemes and solid theories that promise to improve the statistical efficiency and lay the foundation for simultaneous inferences in large-scale studies, especially when heterogeneity, dependence and other complex structures are present. The major components of the proposed research include: (i) the concept of simultaneously incorporating statistical significance and effect size in multiple testing and a new approach to identifying large non-null effects in heteroscedastic models; (ii) the strategy of exploiting spatial dependency and a new approach to testing correlated hypotheses in a hidden Markov random field; (iii) the strategy of grouping hypotheses in sets and a new approach to testing the significance of multiple groups of important variables; and (iv) the concepts of discovery boundary and effective screening, and a data-driven approach to reducing dimensionality by constructing subsets that are optimal in size and adaptive to unknown sparsity. <br/><br/>The proposed research has significant impact on many scientific applications such as genome-wide association studies, time-course microarray experiments, disease mapping in environmental studies, climate modeling, and medical imaging studies. The multiple testing and screening methods outlined in the proposal will improve the quality of simultaneous decision-making in complicated situations, yield more interpretable and reproducible scientific results, lead to great savings in costs in large-scale investigations, and hence help achieve the ultimate goal of understanding the underlying mechanisms in complex systems or human diseases in a precise, fast and cost-effective way. User-friendly software will be developed and made freely available for public use. Research results will be disseminated through publications, seminars and workshops. The investigator is committed to encouraging the participation of under-represented groups in science, and to integrating the proposed research into educational activities through developing new courses, and through mentoring and training students to work on the frontiers in Statistics with important health science applications."
"1005612","Shrinkage Methods for Variable Selection and Structure Discovery, with Applications to High Dimensional Data","DMS","STATISTICS","08/15/2010","08/03/2010","Howard Bondell","NC","North Carolina State University","Standard Grant","Gabor Szekely","07/31/2014","$130,000.00","","bondell@stat.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","","$0.00","The proposed research addresses variable selection within the context of today's complex data structures. The first component of this project is to combine ""supervised clustering"" and variable selection into a single step. The goal is to facilitate the identification of important predictive clusters leading to the discovery of an underlying grouping structure. Secondly, this project proposes to introduce a new technique to tune variable selection methods which not only outperforms existing methods, but has an intuitive interpretation to non-statisticians. The third component of this project is to perform simultaneous variable selection and constrained quantile regression. In complex data, simply modeling the mean as a function of the predictors may not capture the full relationships. Quantile regression models the effect of the predictors on various percentiles of the response. The approach proposed in this project alleviates the well-known issue of crossing curves in quantile regression. Finally, in complex and high-dimensional data, outliers are almost certain to exist, so methods that are robust to these outliers are essential. The final component of this project is an approach to combine robust methods with variable selection via a direct weighting of observations. All four components of this research will be developed via a penalization, or equivalently, a constrained optimization, framework via appropriate choices of penalty.<br/> <br/>      With the abundance of information now available in all scientific fields, it can be an overwhelming task to decide on which of the massive number of possible predictor variables to include in a model. Therefore, it is essential to develop techniques to perform variable selection. Penalization techniques to perform variable selection have gained increasing popularity and are routinely applied in diverse branches of subject-matter research including drug discovery, consumer marketing, environmental systems, financial markets, image processing, homeland security, genomics, proteomics, and metabolomics. It is often the case that the investigator has a number of goals in mind when performing a statistical analysis. A common example occurs in gene expression studies, where one may wish to perform subject classification, gene selection, and gene clustering, simultaneously. The proposed research is particularly geared toward enabling the accomplishment of these types of multi-faceted analyses. A general theme of the proposed research is that appropriately chosen penalty functions can achieve multiple statistical objectives simultaneously and in an integrated fashion. The importance of the variable selection problem across all disciplines, and the investigator?s collaborations with medical researchers and other scientists will allow the results to be readily disseminated into the applied research community where it can be used to improve the quality of life."
"1007219","Kolmogorov's Algorithm Statistics for Dynamics in High Frequency Data","DMS","STATISTICS","07/15/2010","07/14/2010","Fushing Hsieh","CA","University of California-Davis","Standard Grant","Gabor Szekely","12/31/2014","$348,765.00","","fhsieh@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","7721, 7752","$0.00","When applying likelihood theory for analyzing high-frequency time series data, scientists and analysts simultaneously face both computational complexity due to the accommodation of several million observations,  and informational complexity, due to the involvement of manifold and diverse dynamic mechanisms.  The investigator proposes to establish Kolmogorov's algorithmic statistics as the unified foundation to bridge the gaps caused by these computational and informational complexities, and to make it possible for systematic and effective discovery of characteristic dynamics.  The proposal focuses on its development by resolving critical issues including: What are the models of data's individuality and typicality, why are they crucial, and how can they be applied by scientists and analysts for discovery and detection?  A new vehicle for this development is the Hierarchical Factor Segmentation (HFS) algorithm.  This completely distinct approach is undertaken to transform an observed time series into various counting processes corresponding to different events of interest, and then to apply the coding schemes to achieve lossy data compression as a way to find the governing state-space trajectory.  This is accomplished without estimating the point processes' time-varying intensity functions, nor relying on any unrealistic prior knowledge about the number of changes, nor assumptions about the regime-generating mechanisms.  Using the computed state-space trajectory, the investigator is able to modify or replace currently prevailing statistical thinking ? such as likelihood theory ? and existing popular methodologies ? such as those based on statistical correlation and association ? by using the connectivity and concurrence of the decoded states.  These real-world applications in finance, biology and national security will realistically illuminate the great merit and potential of this new statistical thinking and computing for discovering real dynamics that are of great interest in the sciences and in society. <br/><br/>Currently, there are many situations in which data are being sampled and recorded on a time scale of milliseconds, or even nanoseconds.  These high-frequency data are found not only in the sciences, but also in economics, finance and national security.  However, due to its enormous length and complexity, these data types cannot be handled well using existing statistical methodologies.  In fact, prevailing statistical thinking is inadequate for resolving issues underlying these kinds of data.  Brand-new statistical thinking is urgently needed to bridge the gap between computing and conception in order to produce coherent and real mechanisms for data analysis.  The investigator proposes the algorithmic statistics as the new foundation for scientists and analysts to focus on extracting key characteristics, such as individuality and typicality, within high-frequency data.  An algorithmic statistic is a computer algorithm that takes a multidimensional time series consisting of millions of time points as the input, and efficiently computes realistic and sufficient analytic results as the output. Accordingly, the classic concepts, such as correlation and association, would be modified or replaced based on the connectivity and concurrence of decoded significant regimes.  In particular, resultant models of individuality and typicality are tremendously useful and important for regulating and detecting purposes.  This proposal also targets the detection of any abnormality or extremism, for example, in trading or in physiological and behavioral processes, embedded within long and noisy high-frequency data."
"1007542","Collaborative Research: Generalized Fiducial Inference -- An Emerging View","DMS","STATISTICS","09/01/2010","08/03/2012","Hariharan Iyer","CO","Colorado State University","Continuing Grant","Gabor Szekely","08/31/2013","$49,992.00","","hari@stat.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1269","","$0.00","This proposal is motivated by the success of Generalized Fiducial Inference as introduced by the PIs as a generalization of Fisher's fiducial argument. As a result of the many studies conducted by the PIs on the theory and applications of generalized fiducial methods the following important conclusions can be made: (a) A unified and systematic procedure is available for developing fiducial solutions for large classes of problems; (b) The fiducial approach generally leads to very efficient inference procedures and thus they are competitive with procedures developed using other approaches;  (c) Fiducial procedures are asymptotically correct in large classes of problems;  (d) Many fiducial distributions can also be realized as a Bayesian posterior by an appropriate choice of a prior. However, this is not always possible, which establishes that the two approaches are not equivalent in general; (e) Both the Bayesian approach and the fiducial approach lead to useful interval inference procedures as have been established in various publications in both areas. It is clear that neither approach can claim to dominate the other; (f) Both approaches typically require MCMC simulations in regards to actual numerical computation of the required posterior or fiducial distributions. After giving due consideration to areas of statistical inference where a fiducial approach is expected to lead to new and useful results, both theoretical and practical, the PIs propose to conduct research into the following topics: (a) Extensions of Interval Data fiducial framework for Generalized Linear Mixed Models together with associated computational approaches; (b) Extension of the work of the PIs to address the model selection problem within the Generalized Fiducial Inference framework; (c) Definition and investigation of the concept of a Robustified Fiducial Distribution for a parameter and development of computational methods for calculating it from data; (d) Application of robust fiducial approaches to arrive at new robust inference methods in some standard parametric examples; (e) Development of some general computational strategies for implementation of fiducial methods for complex practical problems.<br/><br/>This proposal studies a new approach to statistical inference based on Fisher's fiducial argument. The implications of this work will have an immediate effect on public policy. For instance, the U.S. Food and Drug Administration (FDA) guidance document spells out analysis procedures for demonstration of equivalence of two or more drug formulations.  The PIs aim to show that the fiducial approach will lead to more efficient procedures, which will result in cost and time savings, an important issue for the drug industry. In metrology, the International Bureau of Weights and Measures (BIPM) in conjunction with the International Organization for Standardization (ISO), has published a ``Guide to Expression of Uncertainty in Measurements'' (GUM) which gives the procedures to be followed by national metrological institutes such as NIST in the US, NPL in UK, and PTB in Germany. A problem that is unique to metrology is that every measurement is subject to unknown and unknowable systematic errors that are often larger than random errors. The only currently known way to quantify these unknowable systematic errors is via specification of subjective distributions for them. The GUM specifies some ad hoc methods for combining data-based estimates of standard deviations for some error components and subjective estimates of uncertainty for other error components.  The PIs aim to demonstrate that the fiducial method provides a new natural approach for accomplishing this. Such results are likely to influence the metrology community in modifying and improving their current procedures."
"1006344","Collaborative Research: Constructing New Multiple Testing Methods","DMS","STATISTICS","06/01/2010","05/24/2010","Sanat Sarkar","PA","Temple University","Standard Grant","Gabor Szekely","05/31/2013","$167,659.00","","sanat@temple.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","MPS","1269","","$0.00","Multiple hypothesis testing has become an increasingly active area of research because of its usefulness as a statistical tool to analyze data from modern  scientific investigations, such as DNA microarray and functional magnetic resonance imaging (fMRI) studies. While several statistical methods have been put forward to address different multiple testing problems arising in these studies, often they were developed without fully utilizing the fact that the underlying test statistics are dependent, although such dependence is a natural phenomenon for data from these studies and might lead to misleading conclusions if not properly taken into consideration. The proposed research project seeks to develop new and innovative multiple testing methods by properly addressing the so called `dependence issues?. It focuses on three broad areas of research: (i) developing new multiple testing methods controlling generalized versions of some standard error rates that allow a few false rejections (ii) developing new multiple testing methods controlling multiple false directional errors, and (iii) developing new data-adaptive methods controlling the familywise error and false discovery rates.<br/> <br/>This project will be expected to have a broad impact on the theory and practice of statistics. The results from this project will be of importance to virtually any statistical investigation where questions are posed in terms of testing several hypotheses. For instance, in microarray or fMRI studies where detection of differentially expressed genes or active voxels is often framed as a multiple testing problem, in pharmaceutical investigations where multiple testing techniques are routinely used in dose-response study or in evaluating a drug's efficacy over standard drug or placebo, our project can potentially offer new and improved methodologies. The project would also benefit education through training of graduate students, incorporation of the developed methodologies in statistics courses. The results will be disseminated through presentations and discussions at national and international conferences, and visits to other institutions. The software to be developed under this project will be made available, free of charge, to the scientific community."
"1007794","Precise Conditions for Permutation Tests to Control Multiple Testing Error Rates","DMS","STATISTICS","08/01/2010","07/20/2010","Jason Hsu","OH","Ohio State University","Standard Grant","Gabor Szekely","07/31/2013","$86,399.00","Eloise Kaizar","jch@stat.osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","MPS","1269","0000, OTHR","$0.00","The purpose of this project is to improve the control of multiple<br/>testing error rates.  A popular approach to multiple testing is based on<br/>permutation. One motivation for the use of permutation-based<br/>testing is to reduce the conservativeness of the tests while still<br/>controlling error rates.  Permutation-based tests are thought to achieve<br/>this by incorporating the multivariate distribution of the multiple test<br/>statistics. However, the hypotheses for which permutation testing is known<br/>to be appropriate are different from those usually considered in multiple<br/>testing. In this project, the investigators precisely describe model<br/>assumptions and conditions for permutation testing to control multiple<br/>testing error rates.  In particular, the investigators study precise model<br/>assumptions connecting the marginal distributions of interest in multiple<br/>testing to the joint distributions known to be appropriate for permutation<br/>testing.<br/><br/>While the methods studied in this project can be used in many fields, they<br/>will be particularly applicable to genetic association studies.<br/>Genome-Wide Association Studies (GWAS) are conducted to discover<br/>biological pathways and prognostic biomarkers, and to guide treatment.<br/>Some GWAS discoveries have failed to replicate, possibly due to<br/>statistical testing procedures lacking error rate control.  In this<br/>project, the investigators study the conditions under which currently used<br/>methods are guaranteed to appropriately control statistical error rates.<br/>Such control will lead to better return on investments in developing drugs<br/>and treatments."
"1007657","Post Model Selection Inference and Empirical Bayes Methods","DMS","STATISTICS","07/01/2010","05/24/2010","Lawrence Brown","PA","University of Pennsylvania","Standard Grant","Gabor Szekely","06/30/2014","$400,000.00","Linda Zhao, Andreas Buja","lbrown@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","","$0.00","Consider a standard Gaussian multiple regression model involving p independent covariates. In many applications a first step of the analysis is to reduce the data via model selection to one containing only a subset of these possible predictors. If the covariates are correlated, conventional inference based on the selected model may be invalid; for example, probabilities that confidence intervals cover the true parameter values for the selected model may be grossly overstated. The investigators propose a version of classical inference criteria and a corresponding method for guaranteeing that post selection inferences will be valid within these criteria. The inference is conservative in that it is valid independent of the model selection method that was used, and correct (though possibly conservative) marginal coverage is guaranteed for all parameter configurations. The procedure is algorithmically easy to describe. However in its optimal implementation requires numerical estimation of certain probabilities related to high dimensional Gaussian distributions, and feasible computation of these probabilities for larger values of p is an issue still under investigation. Notwithstanding certain useful asymptotic bounds can be derived, and some important special cases can be analyzed with greater precision.<br/><br/> <br/><br/>            Conventional statistical inference requires that a model of how the data were generated be known before the data are analyzed. Yet in applications involving such common procedures as the Analysis of Variance and multiple regression it is often the case that one or more model selection procedures are first undertaken in order to help determine a model for the analysis. This model selection is then followed by statistical tests and confidence intervals computed as if the final model had been chosen in advance of examining the data. Examples abound in the social sciences, in the econometric literature, in epidemiology and in genomics. This proposal begins by examining consequences of such a practice in order to categorize the degree to which it may be misleading and misguided. Without additional care the parameters being estimated are no longer well defined, and post-model-selection sampling distributions have properties that are very different from what would be the case without model selection. Statistical inference such as confidence intervals and statistical tests does not perform as is customarily assumed. Many authors have noted some or all of these problems, but have not proposed valid general statistical inference procedures to cope with the situation. The investigators propose and study a method that produces valid statistical inference within the models selected based on the observed data. The proposed approach is universally valid, independent of the procedure that was used to select the variables to be retained in the model. Thus, from this perspective it is not necessary to investigate the details of the various model selection proposals in current use. Nevertheless, certain models and model selection procedures do yield improved performance of our confidence interval proposal, and some aspects of this will naturally be included in our research. In particular some new model selection methods based on nonparametric Bayesian ideas will be investigated both for their ability to flexibly produce satisfactory models and from the perspective of post model selection inference. Extension of these post model selection ideas will also be explored in a variety of statistical settings beyond the most common Gaussian linear models that are the initial target of this proposal."
"1007480","Local Likelihood Estimation for Nonstationary Random Fields","DMS","STATISTICS","07/15/2010","04/26/2012","Ethan Anderes","CA","University of California-Davis","Continuing Grant","Gabor Szekely","06/30/2014","$148,752.00","","anderes@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","","$0.00","Stationary random fields play a  fundamental role in both theoretical and applied spatial statistics. Unfortunately, stationarity is often violated when working with real data.  This presents a challenge for the spatial statistician who is interested in estimating and modeling dependence structure in random fields. The investigator and his collaborator will study, and subsequently apply, a recently developed version of local likelihood estimation for estimating parameters that govern the local dependency in nonstationary random fields.  The application  is to use local likelihoods for the estimation of the local distortion of the cosmic microwave background (CMB) due to gravitational lensing. Detecting and estimating this lensing is important for two reasons. First, measuring the distortion gives an indirect measurement of matter in the Universe which can provide detailed maps of the distribution of Dark Matter. Secondly, one can use the estimates of gravitational lensing to obtain a more accurate measurement of the  un-lensed CMB. This will provide scientists with a deeper probe of fundamental cosmological questions.  Specific aims of this proposal include: 1) develop computational techniques which will make local likelihood estimation applicable to large data sets such as the CMB; 2)  develop the theory of estimating equations for local weight construction with a particular focus on mitigating bias and automatically adjusting for boundaries and uneven observation locations; 3) construct estimates of uncertainty in the local likelihood estimates of the parameter  function; 4) use the  principle irregular term to develop flexible and universal local models for general nonstationary random fields. <br/><br/>Nonstationary random fields are becoming a  ubiquitous feature in the recent data deluge and high resolution sensing.  Unfortunately, the techniques for modeling, estimating and predicting nonstationary random fields have yet to be fully developed and analyzed. This proposal will make steps towards the goal of developing a complete set of methodological  techniques for analyzing nonstationary random fields. Moreover, the tools thus constructed will be useful, not only for Astronomy and Cosmology, but other branches of science and technology as well.  This makes the broader impact of the scientific consequences of this proposal two fold. On the one hand, accurately estimating gravitational lensing in the CMB has far reaching consequences for the understanding of cosmic structure and the beginnings of the Universe. On the other hand, the tools from this project are expected to have broad use in other areas of real world application. Recently developed technologies such as fMRI and diffusion tensor imaging are two examples of other scientific areas that will benefit from the methodologies developed for nonstationary random fields."
"1007556","AMC-SS: Asymptotic Analysis of Extreme Risks with High-Dimensional Tail Dependence Modeling","DMS","STATISTICS","08/01/2010","08/03/2012","Haijun Li","WA","Washington State University","Continuing Grant","Gabor Szekely","07/31/2014","$99,999.00","","lih@math.wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","MPS","1269","","$0.00","This grant provides funding for the extremal dependence analysis of high-dimensional distributions with dependence structures described by vine copulas that are built from basic building blocks of bivariate distributions. The extremal dependence among multivariate extremes can be characterized in terms of the spectral or intensity measure using Multivariate Extreme Value Theory. The parametric feature, enjoyed by marginal univariate extreme values, vanishes in the dependence structure of multivariate extremes, and thus rich dependence properties remain largely unexplored, especially for large stochastic systems modeled by vine copulas. Focusing on the interplay between the tail dependence method and multivariate regular variation, the investigator in this research develops an extremal value theory for high-dimensional graphical models, such as vine copulas, by exploring recursive schemes for tail dependence according to underlying graph structures. This graphical extreme value theory is then used to quantitatively analyze tail dependence emergence and contagion in large stochastic systems, and to develop tractable asymptotic estimates for extremal system risks fueled by tail dependence of high-dimensional multivariate extremes.<br/><br/><br/>Extremal dependence has been observed in diverse fields, such as data networks, financial risk management, and global climate change, to name just a few. Extreme risk fueled by tail dependence and its contagious adverse effects have been best illustrated from the global financial crisis and climate change. This project targets a fundamental research for these pressing issues that are important to safety, security and sustainability of complex societies. Successful completion of the project will lead to efficient and accurate estimations for extreme risks and will enhance research capabilities to understand, detect, and mitigate extreme risks, which will facilitate effective catastrophe risk management that benefits society."
"1007762","Bayesian Partition Models for Detecting Influential and Interactive Variables","DMS","STATISTICS","05/15/2010","05/10/2013","Jun Liu","MA","Harvard University","Continuing Grant","Gabor Szekely","04/30/2015","$349,850.00","","jliu@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","","$0.00","Variable selection in regression modeling is a long-standing problem in statistics.  Recently, there has been a significant surge of interest in analytically elegant, numerically robust, and algorithmically efficient variable selection methods, largely due to the tremendous advance in data collection techniques such as those in biology, internet, and marketing.  This proposal considers the model selection problem in both univariate and multivariate high-dimensional regression problems with discrete covariates.  The main goal is to develop Bayesian methodologies enabling the discovery of interactions among certain independent variables (e.g., genetic markers) affecting the response(s). By taking a Naive Bayes modeling perspective and introducing flexible latent structures to model dependence among variables, the investigator will design methods that can detect interactions of a handful of covariates among tens of thousands of candidate predictors.  By introducing ?individual type? variables, the investigator outlines a strategy to decouple the modeling of the covariates from that of the responses.  This strategy allows one to link a subset of covariates to a subset of the responses, which is an important goal in many biomedical studies.  Strategies for extending the ideas to cases with continuous covariates will be explored and tested.  This research will also bring the power of these new methods and theory to bear on several important application areas such as genetics, bioinformatics, and economic data analysis.<br/><br/>Selecting a subset of predictors among a large number of candidates for accurate prediction of certain outcomes (e.g., weather, stock price movement, heart disease risk etc) is a long-standing and challenging problem in statistics.  Recently, there has been a significant surge of interest in analytically elegant, numerically robust, and algorithmically efficient variable selection methods, largely due to the tremendous advance in data collection techniques such as those in biology, internet, and marketing. It has now been widely recognized by both general scientists and quantitative modelers the importance of discovering among many candidate factors that are truly influential on the outcomes/responses. The proposed research is motivated by important genetics and genomics problems.  Its goal is to develop statistical and computational strategies for not only selecting informative predictors but also discovering interactions among the predictors that may significantly influence the outcome.  In genetics, such interactions among genetic mutations are called ?epistasis,? and their detection is one of the main challenges in the post-genome era.  In HIV  drug-resistance mutation studies, such interactions often reveal new insights on the molecular basis of virus's drug resistance and can lead to innovative and effective treatments.  It is expected both to enrich statistical modeling theory and to provide novel computational and statistical strategies applicable to a wide range of problems in diverse fields.  The preliminary tools the investigator has developed have already been successfully applied to a number of genetics and biomedical studies.  These methods can also be readily applicable to many ?data mining'? tasks, such as text mining, network studies, and e-commerce.  It will provide both educational and interdisciplinary research opportunities for graduate students, and will result in software that may be of interest to biomedical researchers, economists, and other practitioners."
"1005635","Large Matrix Estimation for Super-High Dimensional Data","DMS","STATISTICS","07/01/2010","08/19/2013","Yazhen Wang","WI","University of Wisconsin-Madison","Continuing Grant","Gabor Szekely","06/30/2016","$529,978.00","","yzwang@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","The amount and complexity of data generated to support modern scientific studies continues to grow rapidly. Large data sets, characterized by many variables or features and/or many samples, are now commonly studied in fields ranging from finance and biomedical sciences to geoscience and engineering. Such large complex data pose a number of statistical and computational challenges that are absent in more traditional statistical tools, where sample size is required to be much larger than the number of features or variables. At the same time, they present unprecedented opportunities to statistics discipline. For these super-high dimensional data, practical statistical methods with rigorously-established properties, while remain difficult, become more important than ever to many frontier scientific studies like climate modeling, portfolio allocation and risk management, quantum computation and quantum communication, gene expression study, and image understanding. This project studies the estimation of (i) large covariance matrices; (ii) large volatility matrices in high-frequency finance; (iii) large density matrices in quantum information science. The investigator intends to develop novel statistical methodologies and theories via sparsity for the large matrix inference problems based on complex super-high dimensional data. The research project has great potential to make a significant impact on the broad scientific community.<br/><br/>Digital revolution has a profound impact on data collections in scientific research and knowledge discovery, and technological advances make it possible to collect data with relatively low costs.  As a result, the amount and complexity of data generated to support modern scientific studies continues to grow rapidly. Large data sets are now commonly used in fields ranging from finance and biomedical sciences to geoscience and engineering. Such large scale, complex data pose a number of statistical and computational challenges that are absent in more traditional statistical tools. At the same time, they present unprecedented opportunities to statistics. For these data sets, valid statistical methods become more important than ever to many frontier scientific studies like climate modeling, portfolio allocation and risk management, quantum computation and quantum communication, gene expression study, and image understanding. The research project creates advanced effective statistical tools for the analysis of such vast complex data. The investigator actively engages in activities to integrate research with student training and address applications in the fields of biomedical sciences, geoscience, finance, and quantum information science."
"1007553","Modeling and analyzing phenomena, particularly interactions amongst moving particles","DMS","STATISTICS","06/15/2010","05/13/2015","David Brillinger","CA","University of California-Berkeley","Continuing Grant","Gabor Szekely","12/31/2015","$320,000.00","","brill@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","","$0.00","Consider a moving object whose locations are known at known times. This is the data/process to be studied in the project. It will be called trajectory data. Inherent variability will be assumed making the phenomenon random. The basic process will be assumed to satisfy a Stochastic Differential Equation (SDE) or a Functional Stochastic Differential Equation (FSDE). In the former case it will be convenient to assume that the object moves in a potential field whose gradient gives the drift term of the SDE. When data are available an approximate likelihood function will be set down for the data. This allows inference procedures such as estimation, simulation and testing to be invoked. The models may be parametric or nonparametric. One or several objects, or particles, may be involved. In the latter case there may be interactions. In the case that explanatory variables are present both fit and predictability may be improved by their inclusion. Practical and theoretical properties of the approach will be developed.<br/><br/>   Trajectory data appear in many places these days, particularly since the Global Positioning System (GPS) appeared. Locations of individuals of a collection or of just one object are estimated at a succession of times. The times may be equally spaced or not. They may be different for different objects. One object of this work is to develop mathematical and statistical models for the tracks of the objects. These models may be used to examine scientific hypotheses concerning movements as well as to discover novelties. There may be explanatory variables, such as bathymetry, to include in the modeling. The future movement may depend only on the most recent position or it may depend on more past values. The models need to reflect this and allow examination of the assumption. Data from fields including animal biology, marine biology, astronomy, biophysics will be studied as will the implications of unequally spaced data."
"1007593","Conditional Modeling and Conditional Inference","DMS","STATISTICS","09/15/2010","09/09/2010","Stuart Geman","RI","Brown University","Standard Grant","Gabor Szekely","08/31/2013","$245,999.00","Matthew Harrison","Stuart_Geman@Brown.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1269","9150","$0.00","In many applications, the complexity and dimensionality of the data preclude nonparametric inference, despite the availability of massive data sets. At the same time, it is usually true that too little is known about the detailed mechanisms generating the data to meaningfully specify parametric models. Conditional modeling and conditional inference are semi-parametric approaches to complex high-dimensional data in which attention is focused on manageable low-dimensional statistical modeling and estimation. Applications include efficient feature estimation and data classification (e.g. in computer vision), exact tests for broad and scientifically relevant hypotheses (e.g. in the statistical analysis of multi-electrode neuronal recordings), exploration of time scale in non-stationary processes (e.g. in the study of market dynamics), and construction of complex distributions through successive low-dimensional perturbations (e.g. in the study of probabilistic context-sensitive grammars). <br/> <br/>High-dimensional data are ubiquitous. Sources include molecular biology, finance, neurophysiological recordings, and the imagery and text of the Internet. Despite the availability of almost unlimited amounts of these data, their complexity and high dimensionality challenge existing statistical models and represent a bottleneck to successful applications. Oftentimes the complexity and dimensionality can be finessed through mathematical methods that select and focus on a collection of low-dimensional characteristics of the data. The approach avoids untenable or un-testable model assumptions without necessarily compromising the information content and power of the data. The research is at the interface between statistical theory and scientific application, with potential impact in technology (e.g. through computer vision) and, more broadly, society (e.g. through neuroscience and better financial modeling)."
"1007520","Collaborative Research: Generalized Fiducial Inference - An Emerging View","DMS","STATISTICS","09/01/2010","06/18/2012","Thomas Chun Man Lee","CA","University of California-Davis","Continuing Grant","Gabor Szekely","08/31/2014","$124,953.00","","tcmlee@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","","$0.00","This proposal is motivated by the success of Generalized Fiducial Inference as introduced by the PIs as a generalization of Fisher's fiducial argument. As a result of the many studies conducted by the PIs on the theory and applications of generalized fiducial methods the following important conclusions can be made: (a) A unified and systematic procedure is available for developing fiducial solutions for large classes of problems; (b) The fiducial approach generally leads to very efficient inference procedures and thus they are competitive with procedures developed using other approaches;  (c) Fiducial procedures are asymptotically correct in large classes of problems;  (d) Many fiducial distributions can also be realized as a Bayesian posterior by an appropriate choice of a prior. However, this is not always possible, which establishes that the two approaches are not equivalent in general; (e) Both the Bayesian approach and the fiducial approach lead to useful interval inference procedures as have been established in various publications in both areas. It is clear that neither approach can claim to dominate the other; (f) Both approaches typically require MCMC simulations in regards to actual numerical computation of the required posterior or fiducial distributions. After giving due consideration to areas of statistical inference where a fiducial approach is expected to lead to new and useful results, both theoretical and practical, the PIs propose to conduct research into the following topics: (a) Extensions of Interval Data fiducial framework for Generalized Linear Mixed Models together with associated computational approaches; (b) Extension of the work of the PIs to address the model selection problem within the Generalized Fiducial Inference framework; (c) Definition and investigation of the concept of a Robustified Fiducial Distribution for a parameter and development of computational methods for calculating it from data; (d) Application of robust fiducial approaches to arrive at new robust inference methods in some standard parametric examples; (e) Development of some general computational strategies for implementation of fiducial methods for complex practical problems.<br/><br/>This proposal studies a new approach to statistical inference based on Fisher's fiducial argument. The implications of this work will have an immediate effect on public policy. For instance, the U.S. Food and Drug Administration (FDA) guidance document spells out analysis procedures for demonstration of equivalence of two or more drug formulations.  The PIs aim to show that the fiducial approach will lead to more efficient procedures, which will result in cost and time savings, an important issue for the drug industry. In metrology, the International Bureau of Weights and Measures (BIPM) in conjunction with the International Organization for Standardization (ISO), has published a ``Guide to Expression of Uncertainty in Measurements'' (GUM) which gives the procedures to be followed by national metrological institutes such as NIST in the US, NPL in UK, and PTB in Germany. A problem that is unique to metrology is that every measurement is subject to unknown and unknowable systematic errors that are often larger than random errors. The only currently known way to quantify these unknowable systematic errors is via specification of subjective distributions for them. The GUM specifies some ad hoc methods for combining data-based estimates of standard deviations for some error components and subjective estimates of uncertainty for other error components.  The PIs aim to demonstrate that the fiducial method provides a new natural approach for accomplishing this. Such results are likely to influence the metrology community in modifying and improving their current procedures."
"1006021","Collaborative Research: Constructing New Multiple Testing Methods","DMS","STATISTICS","06/01/2010","05/24/2010","Wenge Guo","NJ","New Jersey Institute of Technology","Standard Grant","Gabor Szekely","05/31/2014","$91,387.00","","wenge.guo@njit.edu","University Heights","Newark","NJ","071021982","9735965275","MPS","1269","","$0.00","Multiple hypothesis testing has become an increasingly active area of research because of its usefulness as a statistical tool to analyze data from modern  scientific investigations, such as DNA microarray and functional magnetic resonance imaging (fMRI) studies. While several statistical methods have been put forward to address different multiple testing problems arising in these studies, often they were developed without fully utilizing the fact that the underlying test statistics are dependent, although such dependence is a natural phenomenon for data from these studies and might lead to misleading conclusions if not properly taken into consideration. The proposed research project seeks to develop new and innovative multiple testing methods by properly addressing the so called `dependence issues?. It focuses on three broad areas of research: (i) developing new multiple testing methods controlling generalized versions of some standard error rates that allow a few false rejections (ii) developing new multiple testing methods controlling multiple false directional errors, and (iii) developing new data-adaptive methods controlling the familywise error and false discovery rates.<br/> <br/>This project will be expected to have a broad impact on the theory and practice of statistics. The results from this project will be of importance to virtually any statistical investigation where questions are posed in terms of testing several hypotheses. For instance, in microarray or fMRI studies where detection of differentially expressed genes or active voxels is often framed as a multiple testing problem, in pharmaceutical investigations where multiple testing techniques are routinely used in dose-response study or in evaluating a drug's efficacy over standard drug or placebo, our project can potentially offer new and improved methodologies. The project would also benefit education through training of graduate students, incorporation of the developed methodologies in statistics courses. The results will be disseminated through presentations and discussions at national and international conferences, and visits to other institutions. The software to be developed under this project will be made available, free of charge, to the scientific community."
"1007616","Estimation for non-linear processes with long memory","DMS","STATISTICS","05/15/2010","02/02/2012","Murad Taqqu","MA","Trustees of Boston University","Continuing Grant","Gabor Szekely","08/31/2014","$324,999.00","","murad@math.bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","MPS","1269","","$0.00","Long-memory is characterized by a covariance function which decreases<br/> slowly to zero as the lag increases.  The decrease is so slow that the<br/> corresponding spectral density blows up at very low frequencies, a<br/> phenomenon also known as ``long-range dependence'' or ``1/f noise''.<br/> Because wavelets are associated with scaling, it is natural to<br/> attempt to use wavelets in order to estimate the intensity of long<br/> memory in time series. The advantage of wavelets on Fourier methods<br/> is that there is no need to difference the time series if these are<br/> not stationary.  Fourier and wavelet techniques have been applied to<br/> processes with long memory that are Gaussian or linear.  This study<br/> focuses instead on non-linear processes with long memory, for<br/> example, outputs of non-linear filters with Gaussian or with linear<br/> inputs.  The goal is to derive effective techniques to estimate the<br/> exponents which characterize the intensity of long-memory, in these<br/> more realistic contexts.<br/><br/>Time series are a collection of data points collected through time,<br/> for instance income or temperature. The dependence between these data<br/> points may be weak or strong. When this dependence is strong the time<br/> series is said to have long memory. Time series with long memory<br/> appear in a number of applications, for example, in economics<br/> and in the analysis of traffic in computer  networks.<br/> The goal of this study is to understand their properties<br/> and how to estimate them."
"1007417","Collaborative Proposal: Case-Control Studies, New Directions and Applications","DMS","STATISTICS","09/01/2010","08/26/2010","Malay Ghosh","FL","University of Florida","Standard Grant","Gabor Szekely","08/31/2014","$159,828.00","","ghoshm@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","","$0.00","Case-control studies mark possibly the single most important and far-reaching contribution that statisticians have made in the domain of Public Health and Epidemiology. The prime objective of this research is to make some new contributions to case-control studies, both in methodology and in application. In particular, the PIs propose a semiparametric Bayesian method to incorporate longitudinal data in a case-control analysis using penalized splines. New Bayesian functional data-analytic tools are needed here. The methods will be applied to biomarker based screening procedures, and will provide a critical appraisal of the association between prostate cancer and the past trajectory of prostate-specific antigen measurements. A second component of the proposed research is the analysis of case-control data generated from two-phase sampling with non-monotone missingness in covariates. Such designs have many potential applications in case-control studies that explore interplay of genetic and environmental risk factors. Exploiting assumptions like gene-gene and gene-environment independence adds to the complexity of the inference. The methods are motivated by an immediate application to a large population based case-control study of colorectal cancer. The final aspect of this proposal deals with a united methodology which provides equivalent inference for odds ratio parameters based on prospective and retrospective models. Both frequentist and Bayesian methods will be considered. The PIs have a track record of successful collaboration in this domain, and want to advance/ extend their work further in these new directions. <br/><br/>Two scientific streams are currently dominating clinical medicine and public health: the molecular biology approach with an emphasis on genetics and discovery of novel biomarkers, and the quantitative approach with an emphasis on epidemiology. The developments in these areas jointly are making fundamental contributions to the study of etiology, diagnosis, prognosis and treatment of complex diseases. Though the standard unmatched case-control study design still remains one of the most popular epidemiologic tools, phenomenal advancement of medical science and genetic technology is giving rise to many complex design and analysis issues which statisticians and epidemiologists have never confronted before. This proposal lies in that new interface of epidemiology and statistics. To understand the mechanism of complex diseases and to design targeted intervention strategies for high-risk individuals is one of the major areas of scientific research in this century. The current proposal is not a mere academic pursuit but an effort to contribute to this scientific process."
"1004769","Estimation of Discrete Probablility Distribution with a Pareto Tail","DMS","STATISTICS","08/01/2010","07/19/2010","Zhiyi Zhang","NC","University of North Carolina at Charlotte","Standard Grant","Gabor Szekely","07/31/2011","$34,255.00","","zzhang@uncc.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","MPS","1269","","$0.00","The central theme of statistical sciences may be described as making inferences about the probability distribution of a random variable based on a sample of the random variable driven by the underlying distribution. Over the central region of the data range, the higher data frequency often enables more reliable estimation of the distribution. However, regardless how large a sample may be, there are always regions in the range of the random variable where few or no sample observations are available, for example, the tail region. Ironically it is on the tail regions where statistical inferences are often most important. The investigator studies the problem of estimating a parametric tail with power decay in, and only in, the extremely distant tail of a discrete probability distribution. The investigator proposes to develop a consistent estimator of the tail probability distribution via a perspective offered by Turing?s formula. Such an estimator, if established, would represent a previously unknown methodology which could shed light on an array of statistical problems involving tails of discrete probability distributions across a range of research disciplines.<br/><br/>The proposed project is motivated by, in addition to its theoretical merits, many practically important problems. The central focus of the project is to provide a methodology to quantify the likelihood of an extremely rare event, so rare that it may not have been previously observed. For example, in finance, the assessment of value at risk may involve quantification of an extremely unlikely event that would cause a huge loss of value during a short time period in a portfolio; the scenarios of stress test for financial industry may also be beneficially considered under this methodology. In insurance, it may be of interest to assess the likelihood of a natural or personal disaster of extreme magnitude. In environmental biology, it may be of interest to assess bio-diversity in a population accounting for those super small minority species that are not represented in a sample. In homeland security, it may be of interest to assess the likelihood of a terrorist attack whose type is previously unobserved or unaware of.  The proposed project provides an opportunity to enhance the ability to find solutions to all the above mentioned problems and beyond."
"1005539","High Dimensional Inference and Signal Recovery","DMS","STATISTICS","07/01/2010","06/10/2010","Lie Wang","MA","Massachusetts Institute of Technology","Standard Grant","Gabor Szekely","06/30/2013","$150,000.00","","liewang@math.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1269","","$0.00","This research project is to reconstruct high-dimensional sparse signals based on a small number of measurements, possibly corrupted by noise. More precisely, four of the main objectives of the program are: 1) further weaken the conditions and strengthen the results of current methods. The investigator aims to push the boundary of the field by developing new theoretical tools to analyze the current algorithms; 2) analyze the connections among different methods to get a deeper understanding of the nature of sparse signal recovery problem; 3) extend the research to multichannel setup which involves simultaneously recovery of a set of signals; 4) develop courses for both graduate student and undergraduate student. Build research projects for graduate students. Make the undergraduate students at least be aware of the possible limitation of classical methods and suitable alternatives.<br/><br/>Due to advances in science and technology, scientists and engineers are now able to collect and process enormously large data sets of all kinds. Such data sets pose many statistical challenges not encountered in smaller scale studies. One of the key problems in this area is the reconstructing of high-dimensional sparse signals, which is a fundamental problem in signal processing. This and other related problems have attracted much interest in a number of fields including applied mathematics, electrical engineering, statistics, finance, and bioinformatics. The proposed research will benefit applications in these scientific areas, for instance the compression of audio, images, and video signals and the analysis of microarray data. It is also of critical importance in linear regression, signal modeling, and machine learning."
"1007454","Inference with Survey Data Having Nonignorable Nonresponse","DMS","STATISTICS","09/01/2010","08/16/2010","Jun Shao","WI","University of Wisconsin-Madison","Standard Grant","Gabor Szekely","08/31/2014","$221,586.00","","shao@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","Item nonresponse exists in most surveys and nonresponse rates are often appreciable. Nonresponse is ignorable if it is related to the observed data only; otherwise, it is nonignorable. Research for nonignorable nonresponse is far from complete. This investigator focuses on statistical estimation and inference in cross-sectional or longitudinal surveys with nonignorable nonresponse. Methods for deriving approximately unbiased and consistent survey estimators for parameters such as population totals and quantiles will be developed. In addition, the efficiency and robustness of estimators will also be studied. Effort will be made to study multivariate or longitudinal survey data, and problems with nonresponse in not only the main survey variables but also the covariates.  This investigator will also study variance estimation for valid estimators, using methods such as linearization, substitution, replication or resampling such as the jackknife, the balanced half samples, the random groups, and the bootstrap.<br/><br/>Many statistical and government agencies collect data through surveys. In most of these surveys, there are typically people who do not respond to the survey, or give partial answers; such cases are called nonresponses.  Often a nonresponse is related to the nonrespondent; for example, males under the age of twenty-five years may be more likely not to respond than older males.  In this case, the statistical methodology is not well developed. The investigator plans to study methods for estimation and inference in the presence of nonresponse for various types of surveys. To increase the precision of estimators, effort will be made to use all observed data, to utilize auxilliary information, and to statistically model the nonresponse rate and/or distribution of the survey data. Methods of assessing the varibility of the derived estimators will also be studied. Since most of the proposed research topics are motivated by problems in survey agencies such as the Census Bureau, the Bureau of Labor Statistics, Westat, and Statistics Canada, results obtained from the proposed research will have significant impacts on the methodology for handling noresponse in these survey agencies."
"1007871","Bayesian Methods for Variable Selection in Generalized/Nonlinear Models","DMS","STATISTICS","07/01/2010","04/09/2012","Marina Vannucci","TX","William Marsh Rice University","Continuing Grant","Gabor Szekely","06/30/2014","$200,000.00","","marina@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","","$0.00","The current proposal builds upon the P.I.'s experitize in variable selection and summarizes her current and future directions in the development of Bayesian methodologies. In particular, the P.I. plans to consider extensions to generalized models and to models that allow arbitrary nonlinear associations of a set of variables to a response. <br/>Inferential strategies for the proposed models are more challenging than the typical linear settings addressed by the P.I. in previous work. In addition, nonparametric priors will be investigated with the purpose of sharpening the selection and relaxing distributional assumptions of the models, such as those on random effects and on the error terms, often questionable in real-data applications.<br/><br/>Methodologies developed through this project carry potential for significant impact in statistics and in applied fields in which high-dimension/low-sample-size data arise. Applications to data arising from interdisciplinary collaborations will demonstrate the practical usefulness of the proposed methods. In particular, the P.I. plans to build upon her recent interest in brain imaging data by investigating applications and extensions of Bayesian methods for variable selection to generalized linear and mixed models currently used for the analysis of such data. The broader impacts of this proposal are in its educational and training objectives, in its efforts to disseminate results and in the collaborative nature of the proposed research. The P.I. maintains an updated webpage on her research activities where papers and accompanying software are posted in a timely manner."
"1007889","Statistical Modeling and Learning in Vision","DMS","STATISTICS","07/01/2010","06/18/2012","Yingnian Wu","CA","University of California-Los Angeles","Continuing Grant","Gabor Szekely","06/30/2013","$300,000.00","Song-Chun Zhu","ywu@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","","$0.00","Finding statistical models that capture the regularities and variabilities of the bewildering varieties of visual patterns in natural scenes is at the heart of understanding the mystery of vision. Continuing the pattern-theoretical approach pioneered by Grenander and advocated by Mumford, and building on the active basis model that the PIs have recently developed, the PIs propose research projects to further develop statistical models as well as associated learning and inference algorithms for vision. The active basis model is a mathematical representation of deformable templates of object patterns. Each template is a sparse composition of selected Gabor wavelet elements that are allowed to perturb their locations and orientations. The template can be learned from training images by a shared sketch algorithm. The learned template can then be used to recognize objects from testing images using a cortex-like architecture of sum-max maps. The proposed research develops hierarchical compositional models with active basis models as building blocks or part-templates. The proposed research studies unsupervised learning of dictionaries of active basis templates from natural images or images of objects from multiple categories and viewpoints. The proposed research also studies a shape script model where the part-templates are designed elementary geometric shapes that are represented by the active basis models. Moreover, the proposed research compares generative and discriminative approaches to learning, using active basis model as an example of generative model. In addition, the proposed research extends the active basis model by coupling wavelet sparse coding for shape patterns and Markov random fields for texture patterns.<br/><br/>Biological visual cortex can learn and recognize huge number of visual patterns in its environment effortlessly. One may consider the visual cortex as an extremely sophisticated statistical model equipped with extremely efficient and robust learning and inference algorithms. What this model looks like and how it learns from its visual environment is still a deep mystery. The proposed research has the potential to contribute to advancing our understanding of this issue. It also leads to concrete models and algorithms that can be used for learning and recognizing a wide variety of object patterns."
"1005345","Collaborative Research: Reducing Computation in Empirical Likelihood Methods","DMS","STATISTICS","09/01/2010","08/27/2010","Yongcheng Qi","MN","University of Minnesota Duluth","Standard Grant","Gabor Szekely","08/31/2014","$150,000.00","","yqi@d.umn.edu","1049 University Drive","Duluth","MN","558123011","2187267582","MPS","1269","","$0.00","The study of empirical likelihood methods has attracted much attention in many areas of statistics. This proposal develops new procedures to overcome the computational burden in applying these methods to inference problems in different settings such as: estimating equations, stationary sequences, high-dimensional data, Pickands dependence functions in multivariate extreme-value distributions and copulas in risk management. A practical obstacle in using empirical likelihood methods is the computational issue due to the high dimensional data, the large size of nuisance parameters as well as dependence in the data. The investigators intend to develop new methodologies to overcome the computational problems and  extend the scope of the potential applications.<br/><br/>The new research results can be applied to economics, insurance, finance, risk management and other social sciences that require effective tools for exploring nonlinear dependence among multivariate series and to increase the complexity of the models."
"1007060","Inference Based on Pairwise Distance/Dissimilarity Measures","DMS","STATISTICS","07/01/2010","06/25/2014","Tao Shi","OH","Ohio State University","Standard Grant","Gabor Szekely","06/30/2014","$144,984.00","","taoshi@stat.osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","MPS","1269","","$0.00","With developments in modern information technology, massive datasets with complicated structures have been collected in many scientific fields such as astronomy, biology, climatology, etc.  In this project, the investigator plans to explore the connections between the data sampling distribution and spectrum of the distribution dependent operators and to develop a theoretical foundation for analyzing commonly used spectral techniques based on pairwise distance/dissimilarity measures. Based on the theoretical analysis,  a new class of statistical inference tools will be proposed for robust estimation, dimension reduction, clustering  and data summarization. Computationally effective algorithms will be designed and their software implementations will be disseminated. Besides theoretical development in statistical methodology, the proposed inference tools will be applied to climate change studies using satellite data and climate model outputs. <br/><br/>The proposed research is motivated by real world scientific problems that require statistical inference from massive datasets. The proposed method is designed to extract useful information and knowledge from those massive datasets with complicated structures. The novel algorithms to be developed in this project have the potential to not only help geoscientists and climate modelers  in analyzing climate records and calibrating climate models, but also provide statistical tools for scientific investigations for researchers in a wide spectrum of disciplines."
"0955316","CAREER: High Dimensional Variable Selection and Risk Properties","DMS","STATISTICS","09/01/2010","08/08/2014","Jinchi Lv","CA","University of Southern California","Continuing Grant","Gabor Szekely","08/31/2015","$400,000.00","","jinchilv@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","MPS","1269","0000, 1045, 1187, OTHR","$0.00","High dimensional variable selection plays a pivotal role in contemporary statistical modeling, learning and scientific discoveries. Long-standing theoretical questions in the literature include how high dimensionality regularization methods with general penalties can handle, what the role of penalty functions is, and how to characterize the optimality of variable selection procedures. The investigator proposes to study four interrelated research topics. First, the investigator studies penalized likelihood methods with general penalties, which are widely applied for simultaneously selecting important variables and estimating their effects in high dimensional statistical inference, where the dimensionality can be much larger than sample size. Second, various contexts of high dimensional variable selection beyond penalized likelihood methods including penalized empirical risk and hunting for interactions are investigated. Third, the investigator proposes new principles for model selection when models are possibly misspecified and studies the robustness of various regularization methods for high dimensional variable selection under model misspecification. Fourth, the risk properties and optimality of various high dimensional regularization methods in the contexts of penalized least squares and penalized likelihood are further investigated.<br/><br/>The analysis of vast data sets now commonly arises in diverse fields of sciences, engineering and humanities ranging from genomics and health sciences to economics, finance and machine learning. High dimensional data analysis poses numerous challenges to statistical theory, methods and implementations that are not present in smaller scale studies. A major goal of this proposal is to make theoretical and methodological contributions to the important and challenging topic of high dimensional variable selection and statistical inference. These new developments provide unified and systematic understandings of various regularization methods in high dimensions, and allow scientists to analyze high dimensional data with increased efficiency, expediency and interpretability. The proposed work is incorporated into new courses on the state-of-the-art high dimensional statistical learning, and will benefit the training and learning of undergraduates, graduate students, and underrepresented minorities. The proposed work on variable selection in high dimensions will not only help better identify factors that are important to, for example, public health and market risk, but also benefit a broad range of scientists and researchers in various fields.<br/>"
"0954865","CAREER: Algebraic Problems in Statistics and Biology","DMS","ALGEBRA,NUMBER THEORY,AND COM, STATISTICS","07/01/2010","05/28/2014","Seth Sullivant","NC","North Carolina State University","Continuing Grant","Andrew Pollington","06/30/2016","$400,000.00","","smsulli2@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1264, 1269","0000, 1045, 1187, OTHR","$0.00","The long-term goal of the proposed research is to develop theoretical and computational methods to solve problems in algebraic statistics and its applications. The specific problem areas described in this proposal concern Gaussian graphical models and statistical models in evolutionary biology. The problems in the proposal have typically been open for many years and are fundamentally of an algebraic character; however, advanced algebraic techniques have rarely been applied to them.  The motivation for studying these problems comes from the fact that the statistical models under investigation in the proposal arise frequently in applications, including economics, computational biology, and sociology.  The specific problems that Sullivant will address for Gaussian graphical models are the identification problem, the computation and existence of maximum likelihood estimates, and the structure of covariance constraints.  The specific problems that Sullivant will address in evolutionary biology are the identifiability of mixture models, the computational and combinatorial complexity of gene trees and species trees, and the comparison of speciation models to tree building algorithms.  The techniques Sullivant will employ to address these problems come from computational algebra, algebraic geometry, and combinatorics.<br/><br/>Algebra is the mathematical study of sets with operations, for example the set of all polynomials with the operations of ""adding polynomials"" and ""multiplying polynomials"".  Statistics is the science of data analysis.  Parametric statistics uses statistical models  (families of probability distributions) as tools for analyzing data.  Algebra and statistics come together in the field of algebraic statistics, which is based on the observation that many parametric statistical models are described parametrically via polynomials.  Sullivant proposes to exploit the algebra/statistics connection to address a number of problems in algebraic statistics and its applications to evolutionary biology.  In evolutionary biology, Sullivant and his students will study phylogenetic models, which are statistical models for determining the ancestral relationships between a collection of extant species.  These models are used in applications in increasingly complicated ways, but even basic questions about the models remain open.  The projects in the proposal will involve research with both graduate and undergraduate students."
"0957049","Borrowing Strength: Theory Powering Applications","DMS","STATISTICS","01/01/2010","12/28/2009","T. Tony Cai","PA","University of Pennsylvania","Standard Grant","Gabor Szekely","12/31/2010","$25,000.00","","tcai@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, 7556, OTHR","$0.00","A three-day  workshop will be held at the Wharton School, University of Pennsylvania, on December 15-17, 2010. This workshop will bring together some of the most prominent researchers in the field of statistics from around the world. The main objectives of the meeting are to facilitate the exchange of recent research developments, to provide opportunities for new researchers and underrepresented groups, and to establish new collaborations that will channel efforts into pending problems and open new directions for investigation.<br/><br/>The workshop will emphasize areas of statistical research offering innovative approaches to problems arising in various branches of the sciences including bioinformatics, image analysis, signal processing and genomics. At the same time, the workshop will cover topics of fundamental statistical theory having broad applicability. In addition to the core invited talks, the workshop will feature poster sessions and discussion sessions."

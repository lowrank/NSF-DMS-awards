"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1515194","RAPID: Fitting Ebola multi-type branching process models to data","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, BIO Innovation Activities","01/01/2015","12/19/2014","Andrew Park","GA","University of Georgia Research Foundation Inc","Standard Grant","Nandini Kannan","12/31/2015","$58,021.00","","awpark@uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1253, 1269, 8015","001Z, 7914","$0.00","The main goal of the project is to provide the scientific community with a set of quantitative modeling tools that will assist in the real-time identification of factors contributing most to the ongoing spread of Ebola in Africa, and that will allow straightforward testing of plausible intervention strategies.  This will be done by developing branching process models of the spread of Ebola that connect to available data sources, and that allow for changes in human behavior as well as incorporate realistic scenarios aimed at mitigating transmission.  Estimates of the key transmission rates and associated measures of uncertainty will allow for improved precision in forecasting under a range of scenarios.<br/><br/>While multi-type branching process models are an ideal way to capture key heterogeneities in transmission, and can readily include the dynamic landscape associated with serious disease threats unfolding in real time, their potential is maximized only when we develop techniques to interface them with data. This is a two-way communication in which existing data allows estimation of critical process rates and probabilities, and in addition, the accurately parameterized models allow forecasting. For the ongoing Ebola outbreak in West Africa, the utility of both wings of this workflow have important implications. More broadly, multi-type branching process models have wide applicability to emerging infectious disease threats. Improving fitting techniques for such models fills a knowledge gap regarding the connection between a flexible modeling framework and available data sources. By identifying simple, key heterogeneities in transmission, the modeling approach transcends purpose-built models that are not easily extended to other systems. Models and associated fitting procedures for the analysis of data related to the Ebola disease will be relevant also for future emerging disease threats."
"1462156","FRG: Collaborative Research: Extreme Value Theory for Spatially Indexed Functional Data","DMS","STATISTICS","08/01/2015","07/18/2017","Mark Meerschaert","MI","Michigan State University","Continuing Grant","Gabor Szekely","07/31/2019","$286,798.00","","mcubed@stt.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","1303, 1616","$0.00","This project focuses on the development of statistical tools to model the spatial and temporal structure of environmental and climate extreme events. Most climate and environmental data sets can be viewed as collections of curves, one curve per year, available at several locations within a region. For example, temperature at a specific location has an annual pattern. The shapes of such annual curves change from year to year, and from location to location. Extreme departures from a typical pattern over a sizeable region can impact agricultural production and public health.  The economic impacts are considerable, particularly, if they occur at unexpected times and locations.  An unusual timing of a heat wave over a large area may cause significant economic damage due to crop failure and forest fires, and also affect the level of preparedness of public health services.  Similarly, long spells of cold, storm-free winter time weather often lead to an increase in particulate pollution levels in densely populated mountain valleys.  It is important that public officials are well-informed about the possible range and impact of such extreme events.  This project will contribute toward a rigorous and objective understanding of the risks involved, and provide quantitative tools for researchers and decision makers in the fields of agriculture, public health, actuarial science, climatology and ecology.<br/><br/>The project seeks to develop a statistical framework for a quantitative assessment of possible extremal departures from the usual annual pattern over a region, i.e. departures of the form that have not been observed in historical records, but can occur with a positive probability. The primary focus of the project is the creation of a mathematical framework, and implementation through the development of statistical software. Building on recent advances in functional data analysis, extreme value theory and spatio-temporal statistics, methodology for modeling the extremal distributions of curves observed at spatial locations will be developed.  Extreme curves will be determined by functionals defined on a function space in which the curves live.  The work will be guided and validated by the analysis of several historical, derived, and computer data sets.  Exploratory analysis will reveal the most prominent properties of extremal shapes. This will be followed by model building and the development of asymptotic theory needed to evaluate probabilities of events not previously observed. The models will reveal extremal features of the spatially indexed functional data that are not apparent from the exploratory analysis.  Procedures for the construction of confidence regions, where extremal departures may occur with prescribed probability, will be obtained. Exploratory and inferential tools for the assessment of trends in the extremal shapes and regions over which they occur will also be derived."
"1513481","Collaborative Research:  Hierarchical Sparsity-Inducing Gaussian Process Models for Bayesian Inference on Large Spatiotemporal Datasets","DMS","STATISTICS","09/15/2015","09/11/2015","Andrew Finley","MI","Michigan State University","Standard Grant","Gabor Szekely","08/31/2019","$80,000.00","","finleya@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","","$0.00","With the increasing capabilities of geographical referencing and remote-sensing technologies such as Geographical Information Systems (GIS) and Global Positioning Systems (GPS) that can identify geographical coordinates with a simple hand-held device, scientists and researchers in a variety of disciplines today have unprecedented access to spatially-referenced data.  From identifying spatial disparities in health standards to more precise weather predictions, GIS technology is used today in almost every sphere of human life with beneficial effects that can be far-reaching.  Statistical modeling and analysis for spatial data constitute a key element in harnessing the scientific potential of GIS and related technologies.  As the scientific community moves into a data-rich era, there is unprecedented opportunity to build an understanding about how environmental ecosystems function and how they will respond to changing environmental conditions.  This research project will advance data modeling in disciplines as diverse as forestry, ecology, public and environmental health, meteorology, engineering, and the geosciences.  It will help discover complex scientific relationships, which, in turn, will lead to better analysis and understanding of our environment and how our ecosystem is evolving.<br/><br/>Analysts and researchers using GIS technology are increasingly faced with analyzing massive amounts of spatial data.  With spatial and spatial-temporal data becoming increasingly high-dimensional -- both in terms of number of observed locations and the number of observations per location -- scientists are seeking to hypothesize extremely complex relationships.  Not surprisingly, statistical models accounting for spatial associations have become an enormously active area of research over the last decade and, in particular, hierarchical models capturing variation at multiple scales have become extremely popular for spatial modeling.  These, in turn, lead to rather complex models that are computationally expensive and unfeasible even for moderately sized data sets.  This project recognizes the increased computational demands in statistical modeling of large high-dimensional spatial and spatial-temporal data and offers a model-based setup to tackle a wide variety of data analytic problems.  The emphasis of this project is on rigorous and principled statistical methodology that can be implemented on standard computing platforms, thereby ensuring accessibility for a very wide group of researchers.  The project outlines a suite of spatial models that easily scale to massive databases and have a broad range of applications.  Theoretical and methodological innovations that enhance current methods will be presented, and their practical implications will be illustrated using freely distributed open-source statistical software products developed as a part of this project."
"1513072","Generalized Semiparametric Varying-Coefficient Models for Longitudinal Data","DMS","STATISTICS","09/01/2015","08/25/2015","Yanqing Sun","NC","University of North Carolina at Charlotte","Standard Grant","Nandini Kannan","08/31/2018","$119,999.00","","yasun@uncc.edu","9201 UNIVERSITY CITY BLVD","CHARLOTTE","NC","282230001","7046871888","MPS","1269","","$0.00","This research focuses on the development of new theory, methods, and computational algorithms for analysis of longitudinal data, motivated by problems in AIDS clinical trials and HIV vaccine efficacy trials.  The methods will enhance our understanding of the benefits of treatment switching in an AIDS clinical trial, and how HIV vaccination modifies the disease progression in HIV infected individuals over time.  The broader impacts of the research include the development of statistical theory for longitudinal data, applications to medicine, public health and the social sciences, advancing statistical learning and training for undergraduate and graduate students, and promoting the participation of women in scientific research.  The statistical models and methods will provide a broad platform for investigating complex covariate effects including linear and nonlinear effects, time-varying effects, and nonlinear interactions among the covariates.<br/><br/>The project investigates a class of generalized semiparametric varying-coefficient models for longitudinal data.  These models can be used to analyze both categorical and continuous longitudinal responses through a link function, and flexibly model three types of covariate effects: constant effects, time-varying effects, and covariate-varying effects.  Part I studies the generalized semiparametric varying-coefficients model (GSVCM), where the covariate-varying effects are parametric functions of an exposure variable specified up to a finite number of unknown parameters, and Part II  investigates the GSVCM model, where both the covariate-varying effects  and  the time-varying effects are unspecified functions.  In Part III, the generalized semiparametric single-index varying-coefficients model (single-index GSVCM) is investigated.  The model is more powerful for assessing interactions among multiple covariates.  Estimation procedures for the model based on multivariate local linear smoothing and generalized weighted least squares are proposed.  Large sample theory will be developed for the proposed estimation and hypothesis testing procedures.  Other important problems that will be investigated include variance estimation, hypothesis testing of covariate effects, weight function and bandwidth selection, goodness-of-fit diagnostics, and estimation and hypothesis testing of link functions.  Computational algorithms will be developed to facilitate the applications of the proposed methods to real data from AIDS clinical trials and vaccine efficacy trials.  By pursuing the directions outlined in the project, significant progress may be made in building biologically interpretable models, and in developing statistically efficient methods to handle the complexity of longitudinal data."
"1513644","Next-generation random graph models","DMS","STATISTICS","08/01/2015","08/04/2017","Michael Schweinberger","TX","William Marsh Rice University","Continuing Grant","Gabor Szekely","01/31/2019","$200,001.00","","mus47@psu.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1269","","$0.00","Networks are ubiquitous in the modern world, with social networks and the World Wide Web as well-known examples. Understanding the structure of networks is critical to understanding real-world phenomena, making predictions, and helping inform decisions on, for example, strategies to disrupt and dismantle terrorist networks, curb the spread of infectious diseases, and reduce systematic risk in financial markets. To help understand and predict such phenomena in the face of uncertainty, professionals need statistical models which are both complex and scalable (i.e., models which are capable of modeling a wide range of network characteristics and which can be applied to large networks). Existing models are either scalable but simplistic or complex but not scalable. This research project will develop the first generation of models which are both complex and scalable. The developed models and methods will have applications in a wide range of areas, including national security (e.g., insurgencies, terrorism), public health (e.g., the spread of infectious diseases), and finance (e.g., systematic risk in financial markets).<br/><br/>This research project will develop the next generation of random graph models which are both complex and scalable by merging the two most important streams of statistical network analysis, stochastic block models and exponential-family random graph models, with a view to reducing the disadvantages of each while retaining the advantages of both. Stochastic block models are scalable but simplistic, whereas exponential-family random graph models are complex but not scalable. The next-generation models studied here bridge the gap between complexity and scalability and are both complex and scalable. In addition to elaborating next-generation models, this research will address the unique computational and theoretical challenges raised by next-generation models. The computational challenge of estimating next-generation models will be addressed by taking advantage of model structure, including local dependence and local convexity properties, and by exploiting massive-scale minorization-maximization methods which break down the high-dimensional optimization problem into low-dimensional ones that can solved in parallel. The theoretical challenge of studying the properties of estimators will be addressed by exploiting novel concentration of measure inequalities. The concentration of measure inequalities will take into account the dependence inherent in networks as well as the lack of smoothness of estimators."
"1513266","Collaborative Research:  Prediction and Modeling Selection for New Challenging Problems with Complex Data+","DMS","STATISTICS","08/15/2015","08/07/2015","Jonnagadda Rao","FL","University of Miami School of Medicine","Standard Grant","Gabor Szekely","07/31/2018","$112,000.00","","js-rao@umn.edu","1400 NW 10TH AVE","MIAMI","FL","331361000","3052843924","MPS","1269","","$0.00","Mixed model prediction, that is, prediction based on a class of statistical models known as mixed effects models, has a fairly long history. The traditional fields of applications have included genetics, agriculture, education, and surveys. Nowadays, new and challenging problems have emerged from such fields as business and health sciences, in addition to the traditional fields, to which methods of mixed model prediction are potentially applicable, but not without further methodology and computational developments. Some of these problems occur when interest is at subject level, such as personalized medicine, or (small) sub-population level, such as small communities, rather than at large population level. In such cases, it is possible to make substantial gains in prediction accuracy by identifying a class that a new subject belongs to. Other challenging problems occur when applying existing model search strategies in situations of incomplete or missing data, in model search or selection when prediction is of primary interest, and in making statistical inference based on the result of model search or selection. This collaborative research project aims at solving these challenging problems in prediction and model selection in situations of complex data, such as incomplete or missing data, and data that are correlated due to presence of random effects.<br/><br/>In this collaborative research project the PIs develop a novel statistical method, called classified mixed model prediction, to identify the subject class. This way, the new subject is associated with a random effect corresponding to the same class in the training data, so that the mixed model prediction method can be used to make the best prediction. Furthermore, the PIs develop a recently proposed method, called E-MS algorithm, for model selection in the presence of incomplete or missing data. The PIs also develop an idea called predictive model selection by deriving a predictive measure of lack-of-fit, and combining this measure with a recently developed class of strategies of model selection, called the fence methods. Finally, the PIs develop a unified Jackknife method to accurately assess uncertainty in mixed model analysis after model selection. Theories will be established for these new methods, and their performance and potential gains through extensive Monte-Carlo simulations will be studied. The new methods will be implemented in the R language/environment for statistical computing and graphics. All of the developed methodologies will be applied and tested in a number of applications via a series of close collaborations with experts who will provide access to the data and also guidance in interpretation and dissemination of findings. The fields of applications include genetics, health and medicine, agriculture, education, business and economy. The research project will also promote teaching, training and learning that involve under-represented groups, and build research networks between our institutions."
"1502640","RTG: Data-Oriented Mathematical and Statistical Sciences","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS, GOALI-Grnt Opp Acad Lia wIndus, WORKFORCE IN THE MATHEMAT SCI","08/01/2015","05/19/2021","Anne Gelb","AZ","Arizona State University","Continuing Grant","Junping Wang","07/31/2021","$1,143,368.00","Ming-Hung Kao, Douglas Cochran, Rodrigo Platte, Anne Gelb, John Stufken","annegelb@math.dartmouth.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1269, 1271, 1504, 7335","019Z, 1504, 7301, 9102, 9263","$0.00","The need and desire to analyze copious volumes of disparate data result in significant challenges. This RTG (Research Training Group) program will create a research environment and associated curricular elements that will engage U.S. citizen and permanent resident trainees in activities that will foster an understanding of the roles that statistics, computational mathematics, and applied harmonic analysis play in addressing data-oriented problems and appreciation of the synergies that can manifest when ideas from these areas, which are often studied by separate groups of students with little crossover, are brought to bear simultaneously. Durable impact is sought at ASU through cultivation of crosscutting faculty collaborations and curricular innovations intended to stimulate long-term strength in rigorous, integrated data-oriented mathematical and statistical research. Aggressive dissemination of innovative elements of the program will seek to provide a model for modern integrated data-oriented mathematics training for other institutions, and we will launch the careers of young researchers who will carry this vision.<br/><br/>The research will focus on the following three problem areas. (i) Closed-loop design of experiment for efficient data acquisition: Traditional approaches to collection and analysis of data are essentially ""feed-forward"" in nature, for example, data are collected, numerical algorithms are used to process it (e.g., for compression or to transform it in some other way), and statistical methods are ultimately employed for inference. Statisticians have long recognized the appeal of sequential design of experiments in which the nature of a sample is not fixed in advance, rather depends on previously observed samples. Recent and ongoing technological advances have led to measurement devices possessing many degrees of freedom that enable manipulating the nature of the measurement, often electronically and in real time. In this context, sequential design of experiments takes on a fundamental importance in throttling back otherwise unmanageable data torrents. Instead of collecting all the data all the time, a feedback strategy can be used to acquire only the most important new data for the task at hand based on what has already been observed; (ii) Data driven non-classical numerical approximation tools: A central problem in sensing and model simulation is recovery of characteristic features of a function, signal, image, or operator from a set (frequently these days a very large set) of collected data. In almost all such situations, the data set constitutes an incomplete and noisy description of the system. Classical numerical methods mostly use a limited system model as well data interpolation or approximation to extract pertinent model information and features. Deducing crucial features in large volumes of data calls out for new methods that are readily adaptable to model improvements and inclusion of appropriate prior or statistical information on provided data; (iii) Approximation for statistical inference: Traditional methods in approximation theory and their numerical realizations seek to reconstruct functions from measurements with fidelity described by a metric on a function space. Our RTG has particular expertise with problems where the data are not direct samples of the underlying function (e.g., they may be measurements of some transformed version of the function) and also where the objective is to reconstruct specific features of the function rather than the function itself. Earlier collaborations among the RTG PIs has led to the use of both overcomplete representations (e.g., frames) and statistical ideas in this vein of research."
"1513653","Change-Point Analysis for Multivariate and Object Data","DMS","STATISTICS","07/01/2015","06/29/2015","Hao Chen","CA","University of California-Davis","Standard Grant","Gabor Szekely","06/30/2019","$341,765.00","","hxchen@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","Technological advances allow for the collection of massive data in the study of complex phenomena over time and/or space in various fields. Many of these data involve sequences of high dimensional or non-Euclidean measurements, where change-point analysis is a crucial early step in understanding the data: Segmentation or offline change-point analysis divides data into homogeneous temporal or spatial segments, making subsequent analysis easier; its online counterpart detects changes in sequentially observed data, allowing for real-time anomaly detection. Traditional change-point analyses primarily focus on univariate measurements. There is some literature on multivariate data, but very little on object data.  This project considers both offline and online change-point analysis for multivariate and object data, for instance, for temporal analysis of multiple sensor systems, images, and social networks. <br/> <br/>The proposed methods and corresponding theory build on previous work of the PI, which adapts nonparametric graph-based two-sample tests to the segmentation problem. The PI has shown that the graph-based approach scales flexibly to high dimensional and object data, and allows for a universal analytic permutation p-value approximation that is decoupled from application-specific modeling. Despite this recent development, many challenges remain. This project identifies these challenges, formulates them into approachable frameworks, and develops appropriate methods and theoretical treatments.  In particular, this project will (1) study more sensitive distance-based tests for testing equality of distributions in high dimensional or in non-Euclidean spaces, which will be adapted to the change-point testing and estimation problem, resulting in a more sensitive and accurate detection of general changes; (2) address methodological and theoretical issues in extending the nonparametric graph-based framework on the offline case to the online scenario; and (3) extend graph-based segmentation and online detection to a circular block permutation framework, enabling them to work for multivariate and object data with weak local dependence."
"1505111","Collaborative Research: New Statistical Methods and Theory for High-Dimensional Data","DMS","STATISTICS","08/15/2015","06/09/2017","Hui Zou","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","07/31/2018","$173,898.00","","hzou@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","High-dimensional data have become ubiquitous in this big-data era. In recent years, many statistical methods and theory have been developed for analyzing high-dimensional data with successful applications in practice. There are still many challenges and open problems to be addressed. Their solutions call for innovative ideas. The proposed research projects are motivated by real applications where the current state-of-the-art high-dimensional data analytic methods fail to deliver good solutions. The research results will be directly applicable in various fields such as genomics, medical imaging, public health, social networks, E-commerce, and among others. For example, methods developed in this proposal will enable us to better understand how a social network evolves and how brain functions change with age. The research results will be disseminated through journal publications, conference presentations and seminar talks. This proposal has an education program that contributes to the education and training of the next-generation statisticians.<br/><br/>In this project novel statistical methods and theory are proposed to study three important topics of large-scale statistical inference: (a) dynamic graphical models and latent graphical models, (b) high-dimensional regression with noisy and corrupted data, and (c) profile matrix inference in structural pursuit. The investigators will develop innovative techniques to handle the methodological, computational and theoretical challenges. The research results will not only provide new powerful data analytic tools for solving open problems in (a), (b) and (c), but also shed light on general principles for statistical learning from complex high-dimensional data. In order to make the research outcomes readily available to other researchers and practitioners, the investigators will implement the methodology developed in this proposal into software packages that will be publicly distributed."
"1513579","Optimal Decision Strategies for Large Spatio-Temporal Decision Problems","DMS","STATISTICS","09/15/2015","07/30/2019","Eric Laber","NC","North Carolina State University","Continuing Grant","Gabor Szekely","08/31/2020","$149,998.00","Brian Reich, Jamian Pacifici","eric.laber@duke.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","9251","$0.00","A number of current public health, ecological, and environmental crises can be conceptualized as spatio-temporal decision problems wherein a harmful replicating agent is spreading across space and, simultaneously, a decision maker must select when, where, and how to allocate limited resources targeted at controlling the spread of the agent.  Examples include the spread of an infectious disease across people in a social network, the spread of a computer virus across machines in a network, and the spread of an invasive species across an ecological landscape.  The costs of these epidemics are enormous.  For example, 32% of global deaths are attributed to infectious diseases, the cost of computer viruses to U.S. businesses is estimated to exceed 60 billion dollars per year, and the cost of invasive species is estimated to exceed 100 billion dollars per year and to affect 100 million acres of land.  Thus, improvements to spatio-temporal decision making could have tremendous benefits to all sectors of society. Technological advances have made it increasingly easy to collect, store, and manipulate large amounts of data.  This research project takes first steps toward methods that use accumulating data on the spread of a replicating agent to inform resource allocation over time.  The methodology adjusts for non-stationary agent dynamics, changing availability of resources, and uncertain or incomplete measurements.<br/><br/>A key component of controlling the spread of a replicating agent over space and time is deciding where, when, and how to apply interventions.  This control process is formalized as an allocation strategy which comprises a sequence of functions, one per time point, that map up-to-date information on the spread of the agent to a distribution over subsets of locations to receive an intervention.  The project formally defines an optimal allocation strategy using potential outcomes, and demonstrates that spatial proximity induces causal interference among locations, thereby preventing direct application of existing methods for sequential treatment assignment.  A parametric estimator of the optimal strategy within a pre-specified class of allocation strategies using a systems dynamics model and simulation-optimization is developed.  This estimator has low variance and can be applied in a data-impoverished setting, however it may suffer from high bias if the systems dynamics model is misspecified.  A semi-parametric estimator of the optimal allocation strategy which does not require correct specification of a systems dynamics model is also developed.  Because the semi-parametric estimator relies on fewer assumptions about the underlying systems dynamics, it is potentially robust to model misspecification but may have high variance.  To balance bias and variance, and optimize finite sample performance, shrinkage of the semi-parametric estimator toward the parametric estimator will be investigated.  The methodologies will be illustrated with an application to the spread of white-nose syndrome in bats."
"1505136","Theory and Methods for Dependent Tensor Data","DMS","STATISTICS","09/15/2015","09/11/2015","Peter Hoff","WA","University of Washington","Standard Grant","Gabor Szekely","08/31/2018","$200,000.00","","peter.hoff@duke.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","","$0.00","Modern scientific studies often gather data under combinations of multiple factors. For example, neuro imaging experiments record brain activity at multiple spatial locations, at multiple time points, and under a variety of experimental stimuli. Studies of social networks record social links of a variety of types from multiple initiators of social activity to multiple receivers of the activity. Data such as these are naturally represented not as lists or tables of numbers, but as multi-indexed arrays, or tensors. However, few tools are available for the statistical analysis of such data, and as a result, scientists frequently analyze tensor data using methods that ignore the tensor structure of the data. This can lead to inefficient use of data, and important patterns in the data being overlooked. This project will remedy this situation by developing usable, practical statistical tools for the analysis of tensor data. <br/><br/>Currently available tools for statistical inference and parameter estimation are generally based on least-squares criteria and the assumption of residual independence. Such limitations can lead to highly sub-optimal inference: In general, great improvements in estimator performance can be obtained by appropriately accounting for residual dependence. Additionally, estimation of high-dimensional datasets can often be greatly improved by employing shrinkage techniques, such as empirical Bayes methods, that are based on variance models for the parameters. In this project, we will first develop theory and methods for covariance modeling of tensor-valued data. These methods will lead to the development of Bayes and empirical Bayes methods, from which we will develop a general data analysis framework for tensor data of a variety of types."
"1506255","17th IMS New Researchers Conference (IMS-NRC)","DMS","STATISTICS","06/15/2015","06/12/2015","Ali Shojaie","WA","University of Washington","Standard Grant","Henry Warchall","05/31/2016","$23,000.00","Noah Simon","ashojaie@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","7556","$0.00","The 17th Meeting of New Researchers in Statistics and Probability, a conference series sponsored by the Institute of Mathematical Statistics (IMS), is organized by and held for junior researchers.  The primary objective is to provide a much needed venue for interaction among new researchers.  The meeting will take place over three days, August 6-8, 2015, at the University of Washington, in Seattle, WA.  The meeting will be held immediately preceding the Joint Statistical Meetings (JSM) in downtown Seattle. Participants will be statisticians and probabilists who have received their Ph.D. within the past five years or are expecting to receive their degree within the same year.  Each participant will present a talk or poster. Topics will cover a variety of areas in statistics and probability, from theory and methods to applications.  Senior speakers will give plenary talks for inspiration and other senior researchers will take part in discussion panels covering topics of importance for young people embarking on an academic/research career, such as teaching, mentoring, publishing and funding.<br/><br/>This conference series is explicitly aimed at training the future leaders and workers in statistics and probability. In helping to create networks of new researchers, it lays the groundwork for future collaboration and the informal exchange of ideas and knowledge. It is also critical for professional cohesion, which is particularly important as research fields become more and more specialized.  Meeting people outside of one's research specialty area, and learning about their research, favors a more comprehensive view of research, which is important when taking editor positions and other professional service activities.  The conference also attracts participants with different backgrounds and nationalities, and underrepresented groups --- women, minorities and people with disabilities --- are explicitly encouraged to attend."
"1454515","CAREER: Statistical Analysis of Massive Data Sets under Low-Complexity Constraints","DMS","STATISTICS, Division Co-Funding: CAREER","06/01/2015","08/24/2018","Karim Lounici","GA","Georgia Tech Research Corporation","Continuing Grant","Yong Zeng","08/31/2018","$203,293.00","","klounici6@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269, 8048","1045","$0.00","The development of methods of statistical inference for massive data sets has become a focal point of research in statistics and machine learning in recent years with the dramatic increase of collected data in a wide range of applications such as meteorology, genomics, or finance. A common denominator of these new data sets is that they satisfy certain low-complexity structure conditions. In specific settings, this could mean, for instance, sparsity of a vector in high-dimensional regression or low-rank properties of a high-dimensional matrix. This research aims at providing new efficient solutions exploiting these particular structures in order to perform accurate inference. The future findings should be of interest to a broad audience in the data science community. The investigator also intends to integrate his contributions into educational activities such as course development and mentoring of undergraduate and graduate students.<br/><br/>The research objectives of this project are to identify new informative low-complexity structures in large scale data sets and propose new methods that are adaptive to these structures, computationally efficient, and statistically optimal in a variety of models, including in particular vector regression, trace regression, principal component analysis for standard and functional data. The new procedures will be based on penalized empirical risk minimization and exponential weights mixing that favor low-complexity solutions. The investigator plains to (a) determine fundamental characteristics of the problems that govern the performance of these procedures; (b) establish new oracle inequalities results in high dimension to assess the statistical performances of these procedures; (c) investigate the computational performance of these procedures under various conditions on the models."
"1509023","Collaborative Research: Studies on Signals and Images via the Fourier Transform","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, Cross-BIO Activities, MSPA-INTERDISCIPLINARY, Activation","08/15/2015","05/30/2017","John Muth","CA","University of California-Irvine","Standard Grant","Yong Zeng","03/31/2019","$199,001.00","","muth@unity.ncsu.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1253, 1269, 7275, 7454, 7713","1253, 8007, 8091","$0.00","The goal of this project is to develop novel statistical methods that address some of the current challenges in analyzing spatio-temporal data frequently encountered in neuroimaging. One major application of this project is to identify features in brain signals that could differentiate healthy individuals from patients with neurological or mental diseases. The second application is to identify changes that take place in a brain signal during cognitive processing (e.g., while a human learns a new motor skill or while a rat learns risks and rewards in a controlled experiment).  The third application is to identify biomarkers in brain signals that could predict a stroke patient's ability to recover loss of motor functionality.   The approach used to solve these problems requires a study of the oscillatory patterns in these brain signals. <br/> <br/>Motivated by these practical problems, statistical methods based on the discrete Fourier transform (DFT) are developed. The DFT gives an indication of the decomposition of variance in the time series. Under stationarity, the covariance of the DFT is sparse and thus a departure from sparsity is an indication of non-stationarity.   Moreover, the covariance of the DFT can be utilized as a discriminator between classes of signals.  Using the properties of the DFT, novel methods for (1) change-point detection in time series based on sparsity of the DFT, and (2) discrimination and classification of classes of time series based on the properties of the covariance of the DFT will be developed.  The DFT will also be used to estimate the variance of functionals of the spectrum and test for serial correlation and stationarity in nonlinear time series.  Validation for stationary spatial processes and non-stationary spatial processes using the two-dimensional DFT will also be developed."
"1506202","Topology Conferences at the Pacific Institute for the Mathematical Sciences","DMS","TOPOLOGY, STATISTICS","07/15/2015","07/15/2015","Alejandro Adem","","University of British Columbia","Standard Grant","Joanna Kania-Bartoszynska","06/30/2016","$55,000.00","","adem@pims.math.ca","224-6328 MEMORIAL RD","VANCOUVER","","V6T 1Z2","6048228595","MPS","1267, 1269","7556","$0.00","This project proposes to support two conferences in the subject known as  topology. The Pacific Institute for the Mathematical Sciences (PIMS) Symposium on the Geometry and Topology of Manifolds is to be held June 29-July 7, 2015, at University of British Columbia and the Applied Topology and High-Dimensional Data Analysis conference is to be held August 17-28, 2015, at the University of Victoria. Topology studies the shape of objects and how they can be deformed. It plays a key role in a variety of disciplines in science and engineering.  Top level international experts will come together to discuss recent developments  and applications. This grant will enable young researchers and students from the United States to attend these meetings.<br/><br/>The PIMS Symposium on Geometry and Topology of Manifolds has as its objective to bring together mathematicians working on a broad range of topics in this area, and provide an opportunity for researchers and graduate students to learn about new connections. The program will include a combination of expository and research talks, as well as time for informal contacts and exchanges of ideas. Some particular themes that we hope will be represented during this activity include geometric structures and analysis on manifolds (e.g., Ricci flow, Einstein metrics, special holonomy); geometric group theory and 3-manifolds; geometric topology (e.g., surgery theory, cobordism and stability of diffeomorphisms); smooth 4-manifolds and gauge theory; symplectic geometry and topology. The conference ""Applied Topology and High-Dimensional Data Analysis"" aims to bring together applied topologists and spatial statisticians to explain their research and develop interactions between them, as well as to expose students to this emerging interdisciplinary field. <br/><br/>Workshops' websites:<br/><br/>http://www.pims.math.ca/scientific-event/150629-psgtm<br/><br/>http://www.pims.math.ca/scientific-event/150817-athdda"
"1454942","CAREER: Next Generation Functional Methods for the Analysis of Emerging Repeated Measurements","DMS","STATISTICS, Division Co-Funding: CAREER","06/01/2015","05/31/2019","Ana-Maria Staicu","NC","North Carolina State University","Continuing Grant","Gabor Szekely","08/31/2021","$400,000.00","","staicu@stat.ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269, 8048","1045","$0.00","This project will develop new statistical methods for the analysis of data structures that are correlated. A motivating example is a longitudinal neuroimaging clinical study of Multiple Sclerosis, where the focus is to study the natural evolution/dynamics of the disease over time. Patients are observed at multiple hospital visits, and the disease status is measured through a brain measurement, such as a one-dimensional brain summary or a three-dimensional brain scan. This work will be used: (i) to predict specific brain measurement at a future time; (ii) to assess the dependence between the brain measurement and age, and (iii) to quantify the association between a cognitive assessment and the specific brain measurement. The new statistical methods will be relevant to many other applications, including medicine, economics, environmetrics, and agriculture; they will allow scientists to analyze such data structures using methods that are theoretically sound, interpretable, and easily accessible. The proposed methods make major contributions to the area of functional data analysis and will impact other areas of statistical applications, such as brain imaging and dynamic treatment regimes. The integration of the research with education will impact society at various levels. The investigator will implement an educational initiative to increase exposure of middle-school and high-school students to exciting statistical methods, through hands-on project-related activities, and will increase exposure of undergraduate students to cutting-edge research in statistics.  The investigator's outreach initiative to developing countries through teaching of functional data techniques is valuable for the advancement of all societies through the sharing and dissemination of knowledge.<br/><br/>The development of the next generation statistical methods for the analysis of correlated data structures is necessary because of a longitudinal-based design: each subject is observed at repeated time visits and for each visit we record a functional variable, in addition to other scalar or vector variables. The project meets the growing demand for pragmatic and data efficient statistical methods for such complex data. Two situations are studied: a) the functional variables are the response of interest and b) the functional variables are predictors and another scalar variable is the response. In both cases, accounting for the dependence within the subject as well as for the longitudinal design is crucial for modeling and inference. However, current methods either ignore the dependence or are too complicated and computationally intensive. The specific research goals of this project are: 1) to introduce novel parsimonious modeling framework for the repeatedly observed functional variables, which allows to extract low dimensional features and use them to study the process dynamics; 2) to develop significance tests to formally assess the effect of covariates; and 3) to develop association models and inferential procedures when the functional variables are predictors and another scalar variable is the response observed in a longitudinal design."
"1541233","An Undergraduate Workshop on ""Big Data, Human Health and Statistics""","DMS","STATISTICS","06/15/2015","06/06/2015","Bhramar Mukherjee","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","05/31/2016","$15,000.00","","bhramar@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","7556","$0.00","This workshop is a concluding event to an Undergraduate Summer Institute on ""Big Data, Human Health and Statistics"" held at the Department of Biostatistics, University of Michigan, Ann Arbor from June 1-26 and is intended to be an event focusing on the undergraduates. The training of the next generation of quantitative scientists needs to change to meet the demands of the data. We define ""Big Data"" as datasets of enormous size and complexity (either in number of observations, and/or in the number/nature of predictors/outcomes). Classical theory, computation and intuition often fail for such irregular, sparse data sets of vast size. More training in data management, data storage, visualization, high dimensional statistics, optimization, causal methods, modeling sparse data, machine learning are needed to equip students to tackle these big data challenges. It is expected that the knowledge obtained from these massive heterogeneous data sources will inform prevention, screening, prognosis, and treatment of human diseases and play a major role in biology, medicine, and public health in the coming decade. This workshop lies in the intersection of Big Data,  Human Health and Statistics. <br/><br/>In this two-day symposium, the first day will feature talks by distinguished researchers in areas of relevance to Big Data, including mobile health, precision medicine, genomics, data visualization. There will be a poster session by undergraduate attendees and an oral presentation session by the undergraduate participants of the summer institute. The second day will be a professional development workshop for undergraduate attendees.  Registration will be open to anyone interested (with a maximum limit of 120 participants). The grant will support participation cost of selected undergraduate attendees, faculty mentors and outside speakers."
"1505256","Collaborative Research:  New Statistical Methods and Theory for High-Dimensional Data","DMS","STATISTICS","08/15/2015","08/04/2017","Lingzhou Xue","PA","Pennsylvania State Univ University Park","Continuing Grant","Gabor Szekely","07/31/2018","$126,102.00","","lzxue@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","High-dimensional data have become ubiquitous in this big-data era. In recent years, many statistical methods and theory have been developed for analyzing high-dimensional data with successful applications in practice. There are still many challenges and open problems to be addressed. Their solutions call for innovative ideas. The proposed research projects are motivated by real applications where the current state-of-the-art high-dimensional data analytic methods fail to deliver good solutions. The research results will be directly applicable in various fields such as genomics, medical imaging, public health, social networks, E-commerce, and among others. For example, methods developed in this proposal will enable us to better understand how a social network evolves and how brain functions change with age. The research results will be disseminated through journal publications, conference presentations and seminar talks. This proposal has an education program that contributes to the education and training of the next-generation statisticians.<br/><br/>In this project novel statistical methods and theory are proposed to study three important topics of large-scale statistical inference: (a) dynamic graphical models and latent graphical models, (b) high-dimensional regression with noisy and corrupted data, and (c) profile matrix inference in structural pursuit. The investigators will develop innovative techniques to handle the methodological, computational and theoretical challenges. The research results will not only provide new powerful data analytic tools for solving open problems in (a), (b) and (c), but also shed light on general principles for statistical learning from complex high-dimensional data. In order to make the research outcomes readily available to other researchers and practitioners, the investigators will implement the methodology developed in this proposal into software packages that will be publicly distributed."
"1513595","Regression Analysis of Networked Data: Estimating Function Theory and Applications","DMS","STATISTICS","08/01/2015","06/09/2017","Peter Song","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","07/31/2018","$200,000.00","","pxsong@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Network data are pervasive in practice such as genetic network data and social network data. Regression analysis of network data is needed to understand the relationship between a set of network-correlated outcomes and another set of covariates. Accomplishing this task requires fast and efficient estimation and inference methods for the model parameters. This motivates researchers to develop a new regression modeling framework for data arising from either networks with undirected edges or networks with directed edges. Through PI's long-term collaborations in Epidemiology, Environmental Health Sciences, and Nephrology, the studied methods can be used by local scientists who will provide valuable feedback. This new research project can also lead to substantial educational initiatives that will involve undergraduate and graduate students and expose them to the state-of-the-art research in various interdisciplinary topics related to the corresponding research. These include new courses, short courses at major conferences, summer workshops, mentoring, and software development. These and other dissemination activities will increase awareness of modern powerful methods for data analysis among scientists from other fields.<br/><br/>Although the network analysis has been extensively studied in the literature for dependent structures, little has been studied yet in the regression analysis of response-covariate relationships. This project attempts to fill in such a gap through a few steps. Specifically, the PI proposes to study three different but related problems, including network dependence models, estimating functions methodology via the generalized method of moments, and large-sample theory. This research project includes several innovative and efficient statistical procedures based on estimating functions for estimation and inference. It is anticipated that the studied framework will allow researchers to handle a large variety of network-correlated discrete and continuous data."
"1512982","EVA 2015: The 9th International Conference on Extreme Value Analysis","DMS","STATISTICS","05/01/2015","04/16/2015","Stilian Stoev","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","04/30/2016","$15,000.00","","sstoev@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","7556","$0.00","This award supports participation by students and junior researchers in the 9th International Conference on Extreme Value Analysis (EVA) held at the University of Michigan, Ann Arbor, June 15-19, 2015.  The EVA conferences are the flagship events for the international statistics community doing research on the theory and applications of extreme values.  These conferences take place every two years around the world.  The 9th EVA conference will gather the majority of the internationally recognized experts in the field as well as a large group of students and junior researchers.  The 9th conference will feature recent developments in extreme value theory and its applications.  This is the area of statistics that provides tools for modeling, estimation, and prediction of the probabilities and magnitudes of events involving very large  (extreme) values.  Extreme value theory is widely used to model and quantify risk of extreme losses in insurance and finance, in many engineering and environmental applications, as well as in quantifying and predicting weather and climate extremes. <br/><br/>The program of the forthcoming EVA conference features four themes from the general area of Extreme Value Theory: (i) statistical theory; (ii) probability theory; (iii) applications; (iv) emerging theory and applications. Theme (i) involves advances on the inference for multivariate extremes, regression, robustness, quantile regression, and spatial and spatio-temporal extremes.  Theme (ii) covers max-stable processes, geometry and extremes, large deviations, extremes for Gaussian processes, time series, and random fields.  Theme (iii) features applications to precipitation and environmental extremes, finance, insurance, and detection and attribution of climate extremes.  Theme (iv) explores new topics such as extremes in random networks, high-dimensional data, and algebraic topology.<br/><br/>Conference web site: http://sites.lsa.umich.edu/eva2015/"
"1462368","FRG:  Collaborative Research: Extreme value theory for spatially indexed functional data","DMS","STATISTICS","08/01/2015","07/08/2017","Stilian Stoev","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Nandini Kannan","07/31/2018","$249,687.00","","sstoev@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","1303, 1616","$0.00","This project focuses on the development of statistical tools to model the spatial and temporal structure of environmental and climate extreme events. Most climate and environmental data sets can be viewed as collections of curves, one curve per year, available at several locations within a region. For example, temperature at a specific location has an annual pattern. The shapes of such annual curves change from year to year, and from location to location. Extreme departures from a typical pattern over a sizeable region can impact agricultural production and public health.  The economic impacts are considerable, particularly, if they occur at unexpected times and locations.  An unusual timing of a heat wave over a large area may cause significant economic damage due to crop failure and forest fires, and also affect the level of preparedness of public health services.  Similarly, long spells of cold, storm-free winter time weather often lead to an increase in particulate pollution levels in densely populated mountain valleys.  It is important that public officials are well-informed about the possible range and impact of such extreme events.  This project will contribute toward a rigorous and objective understanding of the risks involved, and provide quantitative tools for researchers and decision makers in the fields of agriculture, public health, actuarial science, climatology and ecology.<br/><br/>The project seeks to develop a statistical framework for a quantitative assessment of possible extremal departures from the usual annual pattern over a region, i.e. departures of the form that have not been observed in historical records, but can occur with a positive probability. The primary focus of the project is the creation of a mathematical framework, and implementation through the development of statistical software. Building on recent advances in functional data analysis, extreme value theory and spatio-temporal statistics, methodology for modeling the extremal distributions of curves observed at spatial locations will be developed.  Extreme curves will be determined by functionals defined on a function space in which the curves live.  The work will be guided and validated by the analysis of several historical, derived, and computer data sets.  Exploratory analysis will reveal the most prominent properties of extremal shapes. This will be followed by model building and the development of asymptotic theory needed to evaluate probabilities of events not previously observed. The models will reveal extremal features of the spatially indexed functional data that are not apparent from the exploratory analysis.  Procedures for the construction of confidence regions, where extremal departures may occur with prescribed probability, will be obtained. Exploratory and inferential tools for the assessment of trends in the extremal shapes and regions over which they occur will also be derived."
"1513040","High-Dimensional Bayesian Computations: The Moreau-Yosida Posterior Approximation","DMS","STATISTICS","07/15/2015","08/04/2017","Yves Atchade","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Gabor Szekely","10/31/2018","$358,449.00","","atchade@bu.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Bayesian inference is a powerful statistical inference method that allows statisticians and other scientists to combine existing knowledge with new data samples for better inferences and decisions. The difficulty of sampling from posterior distributions is one of the biggest impediments to a wider adoption of Bayesian procedures in high-dimensional/big data analysis. There is a need for fast and accurate posterior approximation methods to assist with the practical implementation of Bayesian statistics in high-dimensional problems. This research project will use ideas from the related field of optimization to develop a Bayesian posterior approximation method that satisfies these requirements. The methodology will find applications in a wide-range of areas such as finance, marketing science, epidemiology, biology, medical sciences, and others.<br/><br/>More specifically, there is a need in statistics for posterior approximation methods in high-dimensional problems that: (a) produce approximations that are easier to explore by Markov Chain Monte Carlo (MCMC), and (b) are well-understood from a theoretical viewpoint. This project will use the Moreau-Yosida approximation and related tools from optimization and variational analysis to develop a Bayesian posterior approximation method that satisfies the above two conditions. The research from this project will help clarify similarities and differences between optimization and simulation problems. This research will also contributes to the theoretical analysis of Markov Chain Monte Carlo algorithms, with the special focus on understanding the mixing time of MCMC algorithms in high-dimensional settings. The project will also address open problems in high-dimensional Bayesian variable selection and will develop some novel modeling and computational solutions. There are many applied research areas, including biomedical research, epidemiology, marketing science, and social science research, where variable selection plays an important role. Hence, results from this research will allow researchers in those areas to better handle available data and gain new insights into relevant scientific questions. On the educational side, the material from this research will form a key component of the doctoral dissertation of the Ph.D. students supported by this grant. The project will also enable the PI to use the related scientific problems and datasets to enrich the learning experience of students in his classes and possibly other classes taught by his colleagues. Furthermore, novel methodologies from this research will be widely disseminated to the scientific community through presentation of academic seminars as well as presentations at high-visibility conferences in statistical computing."
"1542123","Conference proposal: From Industrial Statistics to Data Science","DMS","STATISTICS","07/01/2015","06/23/2015","Elizaveta Levina","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Gabor Szekely","06/30/2016","$15,000.00","","elevina@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","7556","$0.00","The conference ""From Industrial Statistics to Data Science"" will be held October 1-3, 2015, in Ann Arbor, Michigan, on the University of Michigan campus.    The theme of the conference is a path toward addressing the next generation of critical challenges in science and industry to which statisticians can contribute, building upon the strong legacy of industrial statistics that has resulted from the contributions of many researchers in academia and in industry.   The conference will feature approximately 20 invited speakers, from both academia and industry, many of whom have made fundamental contributions to industrial statistics, data science, and allied areas of statistical methodology, and a student poster session.   This award will support registration and travel to the conference for students, postdoctoral fellows, and junior researchers.  <br/><br/>Recent crystallization of interest around the term ""Data Science"" provides a framework for thinking about ways to effectively build on the contributions of industrial statisticians and leverage the well developed classical frameworks in modern applications.  Industrial statistics with its focus on reliability and design of experiments has traditionally been an area of statistics with strong connections to industry.   In the era of data science, many of the traditional areas such as design of experiments are as relevant as ever, but new connections need to be made to bring these ideas into the context of big data.   To address this need and help build relevant connections, the conference program will include sessions on industrial statistics, data science, and new methodologies for addressing challenging problems in natural science, social science, and engineering, with a view toward how research in these areas contributes to critical national and global priorities. More information about the conference can be found at https://sites.lsa.umich.edu/vn65."
"1512422","Collaborative Research: High-Dimensional Projection Tests and Related Topics","DMS","STATISTICS","07/01/2015","08/10/2015","Runze Li","PA","Pennsylvania State Univ University Park","Standard Grant","Gabor Szekely","05/31/2019","$123,288.00","","rzli@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","","$0.00","Although high-dimensional data analysis has become the most active research area in statistics, there are still many challenging unsolved problems which call for the development of new methods and theory. This project aims to develop new statistical tools and software to statistical modeling and inference on high-dimensional data. The proposed research is expected to significantly enhance the availability of statistical tools and software for analysis of high-dimensional data, which have frequently been collected in many research areas including genomics, biomedical imaging, functional magnetic resonance imaging, tomography, tumor classifications and finance. Hence, the proposed work is expected to benefit a broad range of scientists and researchers in various fields. <br/><br/>Considerable attention has been devoted to high-dimensional estimation and sparsity recovery over the last 10 years, but much less is known about hypothesis testing. In this project, the PIs first plan to develop new projection Hotelling's test and chi-squares tests for high-dimensional one-sample and two-sample mean problems. The tests are distinguished from the existing ones in that they are based on optimal projection directions that are derived to achieve optimal power performance. The PIs further propose an effective data-driven method to estimate the optimal projection direction by a sample-splitting strategy. The proposed procedure can be easily carried out. They plan to investigate the estimation of the sparsity optimal projection direction via regularization methods. Linear discriminant analysis has been hugely successful in classification, but most of the existing procedures cannot handle diverging number of classes. In this project, they also plan to study ultrahigh dimensional linear discriminant analysis with a diverging number of classes and develop new procedures enable researchers to apply low-dimensional linear discriminant analysis techniques for ultrahigh-dimensional linear discriminant analysis, and make ultrahigh-dimensional linear discriminant analysis with a diverging number of classes computationally feasible in practice. This model and associated new methodology have high potential for big data analysis. The PIs plan to continue collaborating with engineers, meteorologists, public health science researchers and prevention researchers and introduce the proposed methodology to scientists beyond statistics and biostatistics. The PIs plan to disseminate the research results through publications, conference presentations and software distribution."
"1513483","Confidence Distribution (CD) and Efficient Approaches for Combining Inferences from Massive Complex Data","DMS","STATISTICS","07/01/2015","06/18/2015","Minge Xie","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","06/30/2019","$442,227.00","Regina Liu","mxie@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","Modern powerful data acquisition technology has greatly facilitated the collection of massive data, often with heterogeneous and complex structures, in many domains. Those data are used for drawing inferences for scientific discoveries or marketing values. In practice, the data often involve multiple studies or subpopulations, each with its own data source, targeting the same hypotheses or parameters. In such cases, coherent and efficient overall inference methods for combining findings from individual studies would be needed. The need for efficient combining inferences also arises in the implementation and development of different algorithmic or high-performance computing methods in dealing with big data. The goal of this project is to apply the statistical concept of confidence distribution to developing novel efficient approaches for combining multiple inferences from different sources and in massive complex data settings. Such approaches should be timely and useful for many domains, including health and medicine, market analysis, information retrieval, aviation safety, homeland security, just to name a few. <br/><br/>This project builds on the recent exciting developments from the so-called ""confidence distribution"" to develop fusion learning for massive data. It focuses on three specific developments: 1) Efficient nonparametric fusion learning: an efficient nonparametric approach for combining individual inferences from multiple studies that has implementable algorithms and full theoretical support. The development is nonparametric and data driven, which is broadly applicable with little model assumptions. 2) Efficient fusion learning for the split-conquer-combine approach for handling massive and possibly heterogeneous data: This research utilizes the idea of parallel computing to develop several split-conquer-combine schemes for analysis of massive data. The approach can reduce substantially the computational expenses and yet still achieve the oracle inference outcome associated with the entire data. 3) Fusion learning in prediction and testing: The research develops and generalizes the theoretical framework of inference for prediction and testing based on confidence distributions. This development helps mitigate several well known difficulties surrounding multiple testing, model selection problems, especially in the setting of big data. Overall, the project involves in-depth theoretical development and real problem solving in complex data. It is ideally suited for collaborative research and active participation from students."
"1513409","Nonlinear dynamic factor models and dynamic factor driven functional time series models","DMS","STATISTICS","07/01/2015","07/15/2017","Rong Chen","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","05/31/2019","$230,000.00","","rongchen@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","Time series analysis comprises methods that allow for the discovery of dependent and dynamic structures in observations taken over time and that provide accurate predictions of the future.   Time series data occur in many important application fields including economics, finance, environmental studies, neuroscience, ecology, and meteorology.  In this age of Big Data, with advanced data collection capability, researchers routinely encounter large panels of time series data.   How to effectively analyze the common dynamic feature of these time series, how to discover their interconnection, how to make accurate predictions, and how to assess the overall risk are important questions.  The project aims to answer these questions by investigating statistical methods that extract common features from a large number of time series.  The project will also describe methods to analyze data in the form of curves or images observed over time.  This project provides advanced data analysis tools for solving many real world problems, and paves the way for developing a new research area in statistics.  The project includes activities related to education and research training of graduate and undergraduate students, and plans for recruiting women and underrepresented minority students into the field of statistics.   Results will be disseminated through conference presentations, publications, and distribution of software. <br/><br/>The project focuses on two closely related topics: (i) developing a class of nonlinear dynamic factor models, along with associated statistical inference procedures and derivation of the theoretical properties of the proposed estimators; and (ii) developing an efficient nonparametric inference procedure for functional time series based on dimension reduction using dynamic factor models. Modeling and analyzing high-dimensional time series requires efficient dimensional reduction tools, with  factor models being one of the most commonly used techniques. The project extends standard linear dynamic factor models to nonlinear models in order to capture the nonlinearity often encountered in practice. When functional or distributional observations are observed over time and exhibit dynamic behaviors, time series models in the functional space become a necessary and useful tool for analyzing such data, as well as making predictions of the future.  New nonparametric approaches to modeling functional time series utilizing factor models as a dimension reduction tool will be developed.  The two research topics are rapidly gaining importance as more and more applications involve such types of data.  The combination of these two closely related projects builds a comprehensive framework for modern time series analysis. For each project, statistical properties of the underlying models, statistical inference and predictions for these models, and theoretical properties of the inferential and prediction methods will be studied."
"1540863","The fifth international workshop on Finance, Insurance, Probability and Statistics","DMS","STATISTICS","06/15/2015","06/15/2015","Rong Chen","NJ","Rutgers University New Brunswick","Standard Grant","Gabor Szekely","05/31/2016","$10,000.00","","rongchen@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","7556","$0.00","Rutgers University is hosting the 5-th workshop on Finance, Insurance, Probability, and Statistics between June 25 to 27, 2015. With the current global economic environment, the proliferation of complex insurance and risk management products, and modern information technologies and data collection and processing capability, the importance of using probability and statistics theory and methods in discovering efficient financial instruments that accelerate instead of damage economic development, in understanding and measuring risk, and in develop effective insurance and risk management tools have never been greater. The needed interdisciplinary research not only require close collaboration and cross-fertilization between diverse academic fields in probability, statistics, finance and actuary science, but also require close collaboration and deep understanding between academic communities, industry experts, and government regulators in a truly interdisciplinary approach. The IMS sponsored workshop series on  Finance, Probability and Statistics was started in 2011, under the leadership of Professors Tze L. Lai (Stanford University), Philip Protter (Columbia University) and Xin Guo (University of California-Berkeley). The goal of the workshop was to bring together leading academic experts, practitioners and junior researchers, which will highlight important contributions to mathematical and computational finance made through the use of statistics and probability. Rutgers University and Columbia University are co-hosting the 5-th workshop in this series on June 25 to 27, 2015. The 2015 workshop has added 'insurance' to its scheme and has renamed the series IMS sponsored workshop on Finance, Insurance, Probability and Statistics (IMS-FIPS 2015). It is also co-sponsored by International Chinese Statistical Association, Korean International Statistical Association, Global Association of Risk Professionals, Financial and Risk Modeling Institute, Stanford University, and International Association for Quantitative Finance. The conference has planned six plenary sessions, 13 invited sessions and 4 contributed sessions. The NSF award provides financial support exclusively to junior faculty members and graduate students who may not have funding to attend the conference. The funding supports travel, lodging and registration fee to approximately 15 to 20 junior participants.<br/><br/>The NSF supported conference allows exchange of ideas, research results, potential problems among academic researchers and industry practitioners on the current development of mathematics, probability and statistics in the applications of finance and insurance. Such an exchange promotes further research in the area and advance the theory, method and application of probability and statistics related to finance and insurance. The NSF supported conference has broad impacts in advancing the advance the theory, method and application of probability and statistics related to finance and insurance, hence enhancing the society's ability to maintain an efficient financial market, to effectively manage risk and to promote economic development. The fund is exclusively used to support junior and minority researchers to attend the conference, who may not have funding otherwise to participate the conference."
"1513378","SEMIPARAMETRIC INFERENCE WITH HIGH-DIMENSIONAL DATA","DMS","STATISTICS","08/01/2015","08/16/2017","Cun-Hui Zhang","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","07/31/2019","$300,000.00","","czhang@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","Big Data is an area of intense current interest in statistical research and practice due to the rapid development of information technologies and their applications to modern scientific experiments. High-dimensional statistical methods typically provide crucial elements and ideas in engineering solutions for complex Big Data problems. Important fields with an abundance of such problems include bioinformatics, signal processing, neural imaging, communications and social networks, text mining and more. In many such applications, the nominal complexity of the problem, typically measured by the dimension of the data such as genetic components in bioinformatics, brain regions or voxels in neural imaging, or computers and routers in the Internet, is much greater than number of sample points or the information content of the data. The research project will identify and characterize high-dimensional statistical models and problems in which efficient statistical inference are feasible, and will develop new methodologies and algorithms to carry out such efficient statistical inference with high-dimensional data. The proposed research is motivated by and will be directly applicable to real life problems in the aforementioned areas where modern information technologies prosper. Furthermore, the proposed research will have significant educational impact. <br/><br/>A longstanding challenge in high-dimensional data is to identify problems where regular statistical inference is feasible without relying on model selection consistency theory. Consistent model selection allows reduction of the nominal complexity of the problem to a manageable level by identifying all relevant features. However, model selection consistency typically requires uniformly strong signal to separate relevant features from irrelevant ones. Unfortunately, such uniform signal strength assumption is seldom supported by either the data or the underlying science, especially in biological, medical and sociological applications. The PI has proposed a semi-low-dimensional approach of statistical inference and successfully applied it to construct regular p-values and confidence intervals in high-dimensional regression and graphical models. This approach corrects the bias of model selectors just as semiparametric approach corrects the bias of nonparametric estimators. The proposed research will further develop this approach in high-dimensional data analysis and tackle new problems in ways not visible just a few years ago. It will focus on efficient statistical inference with semisupervised data and problems involving many high-dimensional or complex components, including confidence regions and significant tests for composite and multivariate features with high-dimensional data. The project will develop practical methods, efficient algorithms, statistical software, and solid theory directly relevant to common applications involving many high-dimensional or complex components."
"1454817","CAREER: Maximum likelihood and nonparametric empirical Bayes methods in high dimensions","DMS","STATISTICS, Division Co-Funding: CAREER","08/01/2015","08/02/2019","Lee Dicker","NJ","Rutgers University New Brunswick","Continuing Grant","Gabor Szekely","07/31/2020","$400,000.00","","ldicker@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269, 8048","1045","$0.00","The investigator is combining classical and elegant ideas from statistics (empirical Bayes, mixture models, and nonparametric maximum likelihood), with important recent breakthroughs in computing to help develop a rigorous, practical framework for many problems in modern data analysis.  Applications in genomics and other areas of biology where high-throughput data are generated form an important part of the project.  Beyond biology, the methods developed during the course of the project are expected to have applications in finance (e.g. fraud detection), machine learning (e.g. speech, text, and pattern recognition), and other fields where vast high-dimensional datasets are being rapidly generated and require accurate, incisive analysis. Another important aspect of the project addresses questions about reproducibility, which have come to the forefront in many applications involving high-dimensional data analysis.  To address these questions, the investigator is studying fundamental properties of statistical risk and risk estimation in high dimensions.  Algorithms and methods developed during the course of the project are being implemented in easy-to-use and freely available software packages.  Project research is closely integrated with education, via graduate student training and newly developed courses for graduate and undergraduate students.<br/><br/>The main objective of the project is to develop new methodologies, computational strategies, and theoretical results for the use of nonparametric maximum likelihood (NPML) techniques and empirical Bayes methods in high-dimensional data analysis.  This work is fundamentally related to the analysis of nonparametric mixture models.  Empirical Bayes methods have a long and rich history in statistics, and are particularly well-suited to high-dimensional problems.  Moreover, recent computational results and convex approximations have greatly simplified the implementation of NPML-based methods.  Leveraging these computational breakthroughs, the investigator is developing novel and scalable NPML-based methods for high-dimensional classification, high-dimensional regression, and other statistical problems.  New still-faster algorithms for computing NPML estimators, which take advantage of certain types of sparsity in the estimated mixing-measure, are also being developed.  The investigator is studying theoretical properties of the proposed methods in high-dimensional settings.  Areas of emphasis for theoretical analysis include convergence rates and frequentist risk properties of the proposed empirical Bayes methods."
"1512267","Collaborative Research:  High-Dimensional Projection Tests and Related Topics","DMS","STATISTICS","07/01/2015","08/10/2015","Lan Wang","MN","University of Minnesota-Twin Cities","Standard Grant","Gabor Szekely","06/30/2018","$76,625.00","","lanwang@mbs.miami.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","Although high-dimensional data analysis has become the most active research area in statistics, there are still many challenging unsolved problems which call for the development of new methods and theory. This project aims to develop new statistical tools and software to statistical modeling and inference on high-dimensional data. The proposed research is expected to significantly enhance the availability of statistical tools and software for analysis of high-dimensional data, which have frequently been collected in many research areas including genomics, biomedical imaging, functional magnetic resonance imaging, tomography, tumor classifications and finance. Hence, the proposed work is expected to benefit a broad range of scientists and researchers in various fields. <br/><br/>Considerable attention has been devoted to high-dimensional estimation and sparsity recovery over the last 10 years, but much less is known about hypothesis testing. In this project, the Pis first plan to develop new projection Hotelling's test and chi-squares tests for high-dimensional one-sample and two-sample mean problems. The tests are distinguished from the existing ones in that they are based on optimal projection directions that are derived to achieve optimal power performance. The PIs further propose an effective data-driven method to estimate the optimal projection direction by a sample-splitting strategy. The proposed procedure can be easily carried out. They plan to investigate the estimation of the sparsity optimal projection direction via regularization methods. Linear discriminant analysis has been hugely successful in classification, but most of the existing procedures cannot handle diverging number of classes. In this project, they also plan to study ultrahigh dimensional linear discriminant analysis with a diverging number of classes and develop new procedures enable researchers to apply low-dimensional linear discriminant analysis techniques for ultrahigh-dimensional linear discriminant analysis, and make ultrahigh-dimensional linear discriminant analysis with a diverging number of classes computationally feasible in practice. This model and associated new methodology have high potential for big data analysis. The PIs plan to continue collaborating with engineers, meteorologists, public health science researchers and prevention researchers and introduce the proposed methodology to scientists beyond statistics and biostatistics. The PIs plan to disseminate the research results through publications, conference presentations and software distribution."
"1452068","CAREER: New methods for multivariate analysis in high dimensions","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2015","06/13/2019","Adam Rothman","MN","University of Minnesota-Twin Cities","Continuing Grant","Gabor Szekely","06/30/2021","$400,000.00","","arothman@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269, 8048","1045","$0.00","Datasets from imaging, gene microarray experiments, and many other fields often have more measured characteristics than subjects.  Analyzing these data with standard statistical methods is either impossible or inadequate.  The investigator addresses this problem by developing new statistical methods that are appropriate for such datasets.  The investigator develops theoretical justifications for these new methods and fast computational algorithms for their application.  Software that implements these algorithms will be made available to the public.  These new products will help practitioners in industry create better predictive models and will also help advance research in many other fields.  The investigator will also develop new curricula, including the creation of an undergraduate statistical computing course, an undergraduate statistical machine learning course, a Ph.D.-level topics course, and a new track within the undergraduate statistics major.<br/><br/>Building statistical models when the number of explanatory variables exceeds the sample size is an exciting area at the forefront of multivariate analysis.  Fitting these models with classical techniques is typically impossible and some constraints or penalties must be imposed.  Penalties that encourage zeros in parameter estimates have received substantial attention.  These penalties are useful because they lead to interpretable parameter estimates, but assuming that these zeros exist may be inappropriate in some applications.  The investigator develops and analyzes new methods to fit models in high dimensions that do not require that zeros are present in the parameters of interest, but still allow the practitioner to make simple interpretations of the fit in terms of the measured variables.  This includes the development of new methods to fit multiple response regression models and multinomial logistic regression models, as well as the development of new methods to shrink characteristics of inverse covariance estimates that are needed to fit predictive models.  The investigator will develop new curricula and involve Ph.D. students in the research."
"1534545","Empirical Process and Modern Statistical Decision Theory","DMS","STATISTICS","05/01/2015","04/28/2015","Huibin Zhou","CT","Yale University","Standard Grant","Gabor Szekely","04/30/2016","$21,000.00","","huibin.zhou@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","7556","$0.00","The workshop titled ""Empirical Process and Modern Statistical Decision Theory"" will be held at Yale University on May 7-9, 2015. The era of big data is fundamentally changing every aspect of our life through discoveries in science, medicine, and engineering. Many innovative and intuitively appealing methodologies have been proposed to make novel and significant discoveries by analysis of complex and big data. It is timely and critically important for statisticians to develop deep, broad, and formal statistical theory to understand and justify why and when certain methodologies would work or not work, to guide statistical practice to make valid and influential contributions to our society. This workshop will bring together some of authorities in statistical decision theory and empirical Process to review the most important and influential advances in the past, to report their most recent exciting research, and to discuss their view of future developments. The workshop will provide a venue for promising young researchers to interact with these leaders of the field and each other, leading to future collaborations and discoveries. A potential outcome of this workshop will provide a guidance to researchers and professors on how and what to teach in empirical process to train our students in understanding and developing modern statistical decision theory.<br/><br/>Empirical process has been playing a key role in developing modern statistical decision theory for a wide range of important models and significant methodologies, such as providing a unified framework for many high dimensional linear models by Gaussian width, establishing asymptotic equivalence theory of Le Cam for various statistical models by the KMT construction, and justifying the effectiveness of important algorithms in machine learning including SVM by VC dimension. In the last ten to fifteen years, the statistics research has witnessed  a tremendous successes of empirical process theory in Bayesian nonparametrics, shape constrained estimation, robust estimation, minimax regret, lasso, and sparse principal component analysis. This workshop will bring together some of authorities in statistical decision theory including Lawrence Brown and Iain Johnstone, and in empirical processes including Richard Dudley and Evarist Gine, as well as some of the the most prominent researchers in high dimensional estimation, Bayesian nonparametrics, robust estimation, machine learning, and shape constrained estimation, to celebrate the most significant advances in the past and to discuss exciting future developments."
"1507511","Optimal Estimation of Statistical Networks","DMS","STATISTICS","08/01/2015","06/26/2015","Huibin Zhou","CT","Yale University","Standard Grant","Gabor Szekely","07/31/2018","$320,000.00","","huibin.zhou@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","","$0.00","Network analysis is becoming one of the most active research areas in many fields. It offers a natural way to organize data by incorporating pairwise relations. This subject is highly interdisciplinary. Researchers from physics, computer science, social science, biology and statistics have made significant contributions to network analysis in theories, methodologies and applications. Despite those recent methodological and theoretical progresses in network analysis, there have been little fundamental studies on optimal estimation. The wide range of important applications of networks ensure that the progress towards the proposed fundamental research objectives will have a great impact in a broad scientific community, which may include co-authorship networks, web networks, friendship networks, educational networks, networks with information flow, gene expression networks, political networks, and healthcare networks. Research results from this project will be disseminated through research articles and seminar series to researchers in other disciplines. The project will integrate research and education by teaching monograph courses and organizing seminars to help graduate students and postdocs, particularly minority, women, and domestic students and young researchers, who work on this topic. We will work closely with the Yale Institute for Network Science and the Yale Center for Outcomes Research and Evaluation to explore appropriate and helpful network models for social sciences and medicine, and to make valid statistical inference.<br/><br/>Various algorithms have been proposed and analyzed to understand the underlying generating mechanism of networks, called graphon, and to do community detection. Many consistency results are obtained. Despite these recent methodological and theoretical progresses on graphon estimation and community detection, especially on stochastic block model, there have been little fundamental studies on optimal estimation. For example, it is not clear whether the error rates for graphon estimation and community detection in those popular algorithms can be further improved. The goal of this project is to develop a coherent theory on optimal statistical network analysis. Specifically, we propose to study: 1) rate-optimal graphon estimation, 2) optimal community detection error rate, 3) computational barriers in graphon estimation and community section, 4) rate-optimal Bayesian posterior contraction, 5) generalizations to exponential family, to sparse networks, to networks of power law, to mixed membership networks, and to exchangeable high dimensional arrays or tensors, and 6) applications to social sciences and healthcare. The research in this project will significantly advance the theoretical understanding of statistical network analysis. The optimality theory will unveil the precision to what graphon estimation and community detection can be attained with or without computational constraints, and will integrate both frequentist and Bayesian perspectives for network analysis."
"1532741","International Travel Grant to Support U.S. Researchers to Attend the International Statistical Institute Satellite Meeting on Small Area Estimation","DMS","STATISTICS","06/01/2015","06/02/2015","Parthasarathi Lahiri","MD","University of Maryland, College Park","Standard Grant","Gabor Szekely","05/31/2016","$14,999.00","","plahiri@umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1269","7556","$0.00","A three-day international meeting titled First Latin American International Statistical Institute Satellite Meeting on Small Area Estimation will take place in Santiago, Chile, during August 3-5, 2015.  The meeting will serve as an official meeting of the International Statistical Institute (ISI) and International Association of Survey Statisticians (IASS) and will provide its members and a large international group of statisticians and social scientists a unique opportunity to meet and exchange ideas on small area estimation, a growing interdisciplinary field with a wide range of real life applications. The meeting will include a strong mentoring component for students and young researchers working on small area estimation and related areas. The meeting aims to provide a forum for leading experts and young researchers to discuss recent progress in small area estimation, thereby providing new directions for small area estimation in various fields. This award provides partial travel support and registration for graduate students from U.S. institutions to attend and participate in this meeting in order to disseminate their research and develop collaborations. <br/><br/>The main objective of this meeting is to bring together both well-established and emerging young researchers from around the world who are actively pursuing theoretical and methodological research in small area estimation and their applications.  The meeting will have an excellent slate of invited, special topic, contributed, and poster sessions covering a wide variety of topics in small area estimation. These include resampling methods, hierarchical modeling, time series, higher-order asymptotics, and different real life applications. In addition, a poster session will be held, which will provide an opportunity for junior faculty and students to showcase their research work.  This should lead to acceleration and enhancement of research for participants. The meeting will also be a fitting promotion of the role of small area estimation methodology in science. Conference web page:  http://www.encuestas.uc.cl/sae2015/"
"1513004","Inference of Network Structure from Grouped Data","DMS","STATISTICS","09/01/2015","08/31/2015","Yunpeng Zhao","VA","George Mason University","Standard Grant","Gabor Szekely","07/31/2018","$119,974.00","","Yunpeng.Zhao@asu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1269","","$0.00","Networks, which can be viewed as data structures consisting of nodes (vertices) connected by links (edges), have drawn wide attention in a variety of scientific and engineering areas. The applications include friendship and collaboration networks in social sciences, food webs and gene regulatory networks in biology, network games in economics, the Internet and World Wide Web in computer science, as well as many others. Traditionally, statistical network analysis focuses on modeling explicit network structure. For physical networks, like power grids, links between nodes are well defined and can usually be directly observed. By contrast, explicit network structure may not be observable in other fields, especially in social sciences and biology. In these areas, the raw data available is usually behavior of nodes, which is generally presumed to be the result of latent network structure. This project will study the problem of reconstructing implicit networks from a special data structure--grouped data. Each observation of such data is a group of individuals which are observed to appear together. <br/><br/>The project is composed of three parts, all concerning rigorous statistical methods for network inference from grouped data. The first part focuses on networks with continuous edge weights. The PI considers two intriguing properties -- self-sparsity and identifiability of Star Model (recently introduced by the PI and his PhD student), and proposes L1 regularization and low-rank matrix factorization in order to reduce the complexity of this model. In the second part, networks with binary links are considered. The PI proposes to study two different methods to estimate the network structures, including a global model based on Erdos-Renyi process and a non-parametric criterion based on subgraph densities. In the third part, the PI considers dependency structure among groups. The Markov property is assumed here, that is, a group generated at any time point only depends on the group structure at the previous time point and the latent network. The PI proposes an intuitive In-and-Out Model under the Markov assumption. The contribution of this project is twofold. Firstly, it is expected that the concept of implicit networks and the study of network inference from grouped data will change some fundamental viewpoints of statistical network analysis. Secondly, the rigorous statistical methods proposed in this project bring new challenging theoretical and computational questions, which will significantly advance the theoretical understanding and computational techniques in this area."
"1513566","Nonlinear Dimension Reduction Methods","DMS","STATISTICS","08/01/2015","09/30/2019","Yoonkyung Lee","OH","Ohio State University","Standard Grant","Gabor Szekely","06/30/2020","$150,000.00","","yklee@stat.osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269","","$0.00","In the era of Big Data, technological advances have brought significant changes in the amount and the complexity of data generated in almost every discipline from astronomy to genomics to medicine. It has become an essential component of the intellectual endeavor to find meaningful patterns and extract relevant information from large scale, high-dimensional data in a reliable and efficient fashion. Understanding and capturing the regular structures underlying the data is crucial for subsequent modeling and prediction. Low-dimensional projections of data are often primary tools for uncovering the structure and coping with high-dimensionality along with other techniques for sparsity or structural simplicity. Methods for dimension reduction will help the process of gathering information from data significantly. This project concerns nonlinear dimension reduction methods which can be viewed as an extension of standard principal component analysis (PCA) - a widely used tool for low-rank approximation of data. The research aims to expand the scope of PCA to various types of data from binary to ordinal responses to counts, and unravel the data embeddings given by nonlinear extensions of PCA. Enhanced understanding of the existing tools and the development of new tools in this research will improve statistical practice in many ways.<br/><br/>This project is primarily focused on investigation of two nonlinear extensions of PCA: kernel PCA and generalized PCA, for various data types including the exponential family data. This research has two specific aims: (i) to understand the geometry of the nonlinear data embeddings given by the kernel PCA through the spectral analysis of the kernel operator, and the effect of a kernel and centering kernels on those nonlinear principal components for clustering  in relation to the data distribution, and (ii) to develop statistically principled extensions of the PCA methodology for analysis and modeling of data matrices from the exponential family distributions using generalized linear model framework. On the methodological aspect, the research parallels the coherent extension of linear model to generalized linear model framework for the best low-rank approximation of data. Computational tools will be developed for a wide range of applications of the studied methods."
"1507073","Collaborative Research: Optimal Bayesian Concentration Rates from Double Empirical Priors","DMS","STATISTICS","08/01/2015","06/19/2015","Ryan Martin","IL","University of Illinois at Chicago","Standard Grant","Gabor Szekely","03/31/2017","$124,400.00","","rgmarti3@ncsu.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","","$0.00","Statisticians frequently encounter problems that involve complicated models with high-dimensional parameters, particularly in ""big data"" settings. From a Bayesian perspective, it is imperative in these problems that the prior distribution be chosen to sit in a good position. Information about where is a good starting position can come from the data. There is a potential danger with this basic strategy, namely, that a double use of data might cause the model to track the data too closely, resulting on over fitting. To avoid this, the PIs introduce a regularization technique that suitably re-weights the likelihood, preventing the model from learning too quickly. This general ""double empirical Bayes"" strategy, where the prior is centered on the data and the likelihood is re-weighted, will be applied to several important and challenging high-dimensional problems, including estimation of sparse high-dimensional precision matrices, which is relevant to estimation of large complex networks. <br/><br/>In this project, the PIs will develop this new double empirical Bayes framework for inference on high-dimensional parameters with a relatively low ""complexity"" or ""effective dimension"". For example, in function estimation problems, posited smoothness on the function is a constraint on its complexity. The first step of the double empirical Bayes strategy is to use a prior, indexed by the complexity of the parameter, centered at a complexity-specific estimate of the parameter based on data. To prevent the posterior from tracking the data too closely, the second step is to re-weight the likelihood to be combined with the data-dependent prior. The result is a sort of posterior distribution on the parameter space, and the PIs will provide general conditions for this posterior to concentrate around the truth at optimal rates. An additional advantage of this new approach is that the complexity-specific priors, for suitable centering, can be taken of relatively simple form, which facilitates computation. The PIs will investigate the double empirical Bayes analysis of several important high-dimensional inference problems, including density and function estimation, variable selection problems in non-linear models, and estimation of sparse precision matrices. Software will be developed for each application."
"1512930","Network Detection and Analysis Through Semi-Parametric Odds Ratio Model","DMS","STATISTICS","08/01/2015","08/18/2017","Hua Yun Chen","IL","University of Illinois at Chicago","Continuing Grant","Pena Edsel","07/31/2019","$200,000.00","","hychen@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1269","","$0.00","The objective of this research is to develop innovative statistical methods for tackling the challenging problems in the analysis of massive data generated in the studies of complex systems, such as biological systems, financial systems, and social networks. This project addresses the lack of flexible statistical models and methods for the analysis of high-dimensional network data with mixed categorical and continuous measurements. The methodology and software under development increase the accuracy of the network detection, estimation, and interpretation, which have widespread applications. One such application is to genomic research that holds great promise for better disease prevention, diagnosis, and treatment. The project's approach to network analysis can reveal more accurate structures of causal pathway and system functionalities, which in turn can lead to better understand of the underlying biological mechanisms. The project also provides training opportunities for the next generation statisticians doing research on high-dimensional data.<br/><br/>Specifically, this project studies a flexible alternative to the traditional Gaussian model in the analysis of multivariate data. Such data may be biasedly sampled and may also include many discrete variables, which makes the data inherently non-Gaussianly distributed. The semiparametric odds ratio model under study in this project unifies and extends the Gaussian models for continuous data and the log-linear model for categorical data. This approach not only models complex associations with highly interpretable model parameters, it also naturally address the problem of biased sampling designs frequently encountered in biomedical data collection. The penalized likelihood approach can consistently detect sparse associations among mixed continuous and discrete variables (i.e., sparse association networks with mixed nodes). Novel theory for consistent sparse network detection and algorithms for implementing the methods will be developed. In comparison to the traditional Gaussian model for association network detection, the methods under study have the advantages of detecting sparse association between groups of variables without modeling the association within the groups of variables in high-dimensional settings. The research results can substantially improve the information extraction from high-dimensional data and the interpretation of such information."
"1561141","CAREER: Bridging High-Frequency Data Analysis and Continuous-time Features of Levy Models","DMS","PROBABILITY, APPLIED MATHEMATICS, STATISTICS","07/01/2015","05/06/2022","Jose Figueroa-Lopez","MO","Washington University","Continuing Grant","Pena Edsel","09/30/2022","$217,289.00","","figueroa@math.wustl.edu","ONE BROOKINGS DR","SAINT LOUIS","MO","631304862","3147474134","MPS","1263, 1266, 1269","1045","$0.00","Motivated by recent theoretical findings in the field, the investigator identifies some key open problems of the asymptotic behavior of Levy processes in short time and connects them to two important statistical problems commonly appearing in applications: parametric estimation and change-point detection for Levy models.  For finite random samples, methods such as maximum likelihood estimation and cumulative sum (CUSUM) sequential rules are known to be optimal for dealing with the two previously mentioned problems. Although one expects that optimality would be preserved when the time span between consecutive observations of a Levy process shrinks to zero, there exist important examples showing this not always to be the case. The mystery behind these counterintuitive results is closely connected to the ""fine"" distributional properties of Levy processes in short time. Rather than directly attacking the two proposed problems in continuous time, the investigator builds on the well-studied analogous problems in discrete time and fill in the infinite time continuum by analyzing their evolution when the time span between consecutive observations is made increasingly small. This bottom-up approach is not only appealing but also useful since in practice one would like to determine the performance of statistical methods for high-frequency observations rather than for continuous-time observations, which are arguably never available. The focus on Levy processes is motivated by the fact that the latter are the simplest stochastic models displaying abrupt changes while still preserving the parsimonious statistical properties of their increments. Extensions to other multi-factor stochastic models driven by Levy processes are also contemplated. <br/><br/>Automatic high-frequency monitoring systems of natural and social phenomena are increasingly used in engineering applications, financial markets, and environmental studies. Therefore, there is an increasing need for efficient and accurate statistical and computational methods for the high-frequency data generated by these systems. Two important issues arise with this need: understanding the meaning of statistical efficiency in a high-frequency sampling setting and analyzing the optimality of some of the commonly used statistical methods when applied to high-frequency data. The undertaken research responds to these two pressing problems. The project's outcomes have important applications in pricing of financial derivatives, calibration of financial models, monitoring of navigation system, intrusion detection in computer networks, and more. Educational impacts include providing summer research experiences for undergraduates and developing teaching/computational resources for interdisciplinary topics in statistics, probability, and mathematical finance. These activities involve graduate students and target the participation of underrepresented groups in sciences."
"1506879","Collaborative Research: Optimal Bayesian Concentration Rates from Double Empirical Priors","DMS","STATISTICS","08/01/2015","06/19/2015","Stephen Walker","TX","University of Texas at Austin","Standard Grant","Gabor Szekely","07/31/2018","$125,576.00","","s.g.walker@math.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1269","","$0.00","Statisticians frequently encounter problems that involve complicated models with high-dimensional parameters, particularly in ""big data"" settings. From a Bayesian perspective, it is imperative in these problems that the prior distribution be chosen to sit in a good position. Information about where is a good starting position can come from the data. There is a potential danger with this basic strategy, namely, that a double use of data might cause the model to track the data too closely, resulting on over fitting. To avoid this, the PIs introduce a regularization technique that suitably re-weights the likelihood, preventing the model from learning too quickly. This general ""double empirical Bayes"" strategy, where the prior is centered on the data and the likelihood is re-weighted, will be applied to several important and challenging high-dimensional problems, including estimation of sparse high-dimensional precision matrices, which is relevant to estimation of large complex networks. <br/><br/>In this project, the PIs will develop this new double empirical Bayes framework for inference on high-dimensional parameters with a relatively low ""complexity"" or ""effective dimension"". For example, in function estimation problems, posited smoothness on the function is a constraint on its complexity. The first step of the double empirical Bayes strategy is to use a prior, indexed by the complexity of the parameter, centered at a complexity-specific estimate of the parameter based on data. To prevent the posterior from tracking the data too closely, the second step is to re-weight the likelihood to be combined with the data-dependent prior. The result is a sort of posterior distribution on the parameter space, and the PIs will provide general conditions for this posterior to concentrate around the truth at optimal rates. An additional advantage of this new approach is that the complexity-specific priors, for suitable centering, can be taken of relatively simple form, which facilitates computation. The PIs will investigate the double empirical Bayes analysis of several important high-dimensional inference problems, including density and function estimation, variable selection problems in non-linear models, and estimation of sparse precision matrices. Software will be developed for each application."
"1513465","Some problems in geometric data analysis","DMS","STATISTICS","09/15/2015","09/14/2015","Ery Arias-Castro","CA","University of California-San Diego","Standard Grant","Gabor Szekely","08/31/2019","$200,000.00","","eariasca@math.ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","","$0.00","The analysis of datasets is increasingly geometrical.  In some applications, such as in the analysis of the cosmic web in astrophysics, this is arguably the most natural approach.  In others, modeling datasets via geometric structures allows to bypass the use of functions, which are in general difficult to deal with in high-dimensions because of the so-called ""curse of dimensionality"".  The project aims at making contributions in this general area of geometrical data analysis, and in particular in fields like clustering, dimensionality reduction, and surface estimation, via the development of new methodology and new theory.<br/><br/>Geometrical approaches to data analysis are well-established.  Clustering, dimensionality reduction, and manifold/surface estimation, are well-developed, with ongoing work in the form of robust PCA, subspace clustering, manifold or surface clustering, manifold learning, geometric statistics, computational geometry, etc.  The vast majority of this research is methodological or applied to a particular problem in a specific field, and theory is by and large lagging behind.  This is, for example, the case in important areas such as subspace clustering, manifold embedding and sensor localization.  This project has the ambition to contribute theoretical insights in those areas.  While methodology tends to be well ahead of theory, good and timely theoretical analyses can shed some light on applied problems, and can sometimes inform the design of more effective methodology.  And such methodology is missing in some areas of geometric statistics.  Thus the project also includes the development of practical methodology that provably matches the minimax performance bounds available for the problem at hand, particularly in the areas of manifold estimation and the estimation of geometric characteristics of a distribution."
"1513621","Statistical Learning for High-Dimensional Relational Data","DMS","STATISTICS","08/01/2015","06/29/2015","Vincent Vu","OH","Ohio State University","Standard Grant","Pena Edsel","07/31/2019","$279,965.00","","vqv@stat.osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269","","$0.00","Technological advances in data acquisition, digital observation, and storage have produced an explosion in high-dimensional datasets that are often too large and noisy for traditional data analysis tools. In many cases these high-dimensional datasets are transformed into relational data that summarize relations between pairs of entities in the original dataset. There are two major classes of statistical methods for analyzing such relational data: techniques from classical multivariate analysis and probabilistic graphical models. These two classes are often viewed as disparate yet complementary approaches to relational data analysis. However, both can fail from being overwhelmed by noise and high-dimensionality. This research project proposes a new framework that unifies these two classes of methods under the umbrella of regularized statistical learning, and it will address three important issues: (1) it will reveal important conceptual links between these disparate methods, thus providing a common ground for developing new and hybrid methods; (2) it will advance theoretical understanding of these methods especially in the case of high-dimensional and noisy data; (3) it will advance computational techniques that enable these methods to be applied to big data that is high-dimensional and large-scale.<br/><br/>Statistical theory has shown that traditional multivariate methods can fail on high-dimensional data and that some form of regularization is necessary for estimation. Despite the recent surge in development of regularized methods, there still exists a gap between the methodological developments and theoretical understanding. This research project fills this gap by combining methodological development with theoretical insight to produce a unified family of regularized methods for relational data that are both computationally efficient and theoretically sound. The theoretical insights will reveal the statistical properties of the methods under very weak assumptions, and they will enable computational strategies that exploit hidden structure in the optimization problems underlying the methods."
"1509557","Collaborative Research: Prediction and Model Selection for New Challenging Problems with Complex Data","DMS","STATISTICS","08/15/2015","08/07/2015","Thuan Nguyen","OR","Oregon Health & Science University","Standard Grant","Gabor Szekely","07/31/2018","$102,185.00","","nguythua@ohsu.edu","3181 SW SAM JACKSON PARK RD","PORTLAND","OR","972393011","5034947784","MPS","1269","","$0.00","Mixed model prediction, that is, prediction based on a class of statistical models known as mixed effects models, has a fairly long history. The traditional fields of applications have included genetics, agriculture, education, and surveys. Nowadays, new and challenging problems have emerged from such fields as business and health sciences, in addition to the traditional fields, to which methods of mixed model prediction are potentially applicable, but not without further methodology and computational developments. Some of these problems occur when interest is at subject level, such as personalized medicine, or (small) sub-population level, such as small communities, rather than at large population level. In such cases, it is possible to make substantial gains in prediction accuracy by identifying a class that a new subject belongs to. Other challenging problems occur when applying existing model search strategies in situations of incomplete or missing data, in model search or selection when prediction is of primary interest, and in making statistical inference based on the result of model search or selection. This collaborative research project aims at solving these challenging problems in prediction and model selection in situations of complex data, such as incomplete or missing data, and data that are correlated due to presence of random effects.<br/><br/>In this collaborative research project the PIs develop a novel statistical method, called classified mixed model prediction, to identify the subject class. This way, the new subject is associated with a random effect corresponding to the same class in the training data, so that the mixed model prediction method can be used to make the best prediction. Furthermore, the PIs develop a recently proposed method, called E-MS algorithm, for model selection in the presence of incomplete or missing data. The PIs also develop an idea called predictive model selection by deriving a predictive measure of lack-of-fit, and combining this measure with a recently developed class of strategies of model selection, called the fence methods. Finally, the PIs develop a unified Jackknife method to accurately assess uncertainty in mixed model analysis after model selection. Theories will be established for these new methods, and their performance and potential gains through extensive Monte-Carlo simulations will be studied. The new methods will be implemented in the R language/environment for statistical computing and graphics. All of the developed methodologies will be applied and tested in a number of applications via a series of close collaborations with experts who will provide access to the data and also guidance in interpretation and dissemination of findings. The fields of applications include genetics, health and medicine, agriculture, education, business and economy. The research project will also promote teaching, training and learning that involve under-represented groups, and build research networks between our institutions."
"1506882","Nonlinear and Nonstationary Time Series","DMS","STATISTICS","08/01/2015","06/08/2018","Zhao Ren","PA","University of Pittsburgh","Continuing Grant","Gabor Szekely","07/31/2019","$337,449.00","","zren@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","","$0.00","This project focuses on two problems in analyzing complex data collected over time such as daily stock market returns.  Such irregular time series data occur in many diverse fields such as biology and medicine, ecology, genetics, geoscience, speech recognition, econometrics and finance, and computer vision to mention a few.  Because the data are irregular, problems such as predicting highly volatile periods are difficult.  In general, rather than using explicit mathematical formulas, one must rely on numerical or computer-based optimization.  However, for such data, existing methods have poor properties. The goal of this project is to vastly improve on the existing computational methods. The second project focuses on the fast detection of genes in long DNA sequences.  While many methods have been developed for a thorough micro-analysis of short sequences, there is a shortage of powerful procedures for the macro-analysis of long DNA sequences.<br/><br/>The project focuses on two problems in nonlinear and nonstationary time series analysis. First, there has been an intense focus on the analysis of nonlinear and non-Gaussian time series models via numerical methods.  Particle samplers are a promising approach for classical and Bayesian estimation, but they are plagued by particle degeneration and by poor mixing.  However, there is no need to abandon particle methods; they can be improved, and this is the goal of this project.  For example, particle Gibbs methods can be fashioned to be fast and efficient while improving the mixing property of the sampler.  The basic idea is to build a particle-filter-like procedure that avoids path degeneracy by conditioning on particles. This conditioning implies an invariance property, which is key to its applicability as a particle sampler.  The invariance property is also key to providing the asymptotic accuracy of the sampler.  It is not enough to be asymptotically accurate because of the curse of dimensionality, which we try to avoid.  Moreover, while the technique is not perfect, the methodology can be used as a basis from which to explore faster methods while avoiding poor mixing. The method can also be used in classical inference to perform derivative free maximum likelihood estimation (e.g., EM algorithm) when the likelihood can only be evaluated numerically. The main interest of the second project is on the detection of coding (genes) and other interesting features in very long DNA sequences.  In particular, the focus is on fast detection of change points in long DNA sequences based on the concept of spectral envelope using a wavelet basis.  Rapid accumulation of genomic sequences has increased demand for methods to decipher the genetic information gathered in data banks. Combining statistical analysis with modern computer power makes it feasible to search, at high speeds, for diagnostic patterns within long sequences.  This combination provides an automated approach to evaluating similarities and differences among patterns in very long sequences and aids in the discovery of the biochemical information hidden in these organic molecules."
"1501767","RTG: Geometry and Statistics","DMS","STATISTICS, WORKFORCE IN THE MATHEMAT SCI","09/01/2015","08/21/2019","Susan Holmes","CA","Stanford University","Continuing Grant","Yulia Gel","08/31/2022","$1,912,390.00","Lester Mackey, Guenther Walther","susan@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269, 7335","7301","$0.00","Statistical tools for the analysis of heterogeneous data have become increasingly important, leading both to the prominence of the new field of data science and to a need for more researchers at all levels. This project will train the next generation of leaders in the field. The Statistics and Geometry Research Training Group (RTG) will provide gateways for students at the various academic levels: The program will fund undergraduate research programs to get students excited by research so that they see themselves pursuing a graduate career in statistics. Graduate students will have the opportunity to develop their own classes in the summer, providing them with a gateway to an academic career as they see themselves becoming teachers and mentors. Three newly created postdoctoral positions will train doctoral recipients with a strong quantitative background in the emerging field of data science. The common theme linking all the participants will be the use of geometric methods in statistics and their applications to high dimensional complex data in modern biology. The program is structured to maximize mentoring and interaction opportunities, providing each participant with a broad perspective of the state of the art in statistics through highly relevant courses and working groups in statistics and geometry and their applications to biology. A salient feature of the program is that it will develop both the intellectual breadth as well as the mentoring and communication skills of the participants. The program will allow more U.S. students to supplement their interests in mathematics, applied mathematics, or computer science with involvement in data science, which is a crucial current need in many areas of information technology. The progress made in the analysis of heterogeneous data will enhance medical discovery through improved visualization and geometrical representations as well as the possibility to assess levels of uncertainty in making decisions based on images or network data. Domains of application will include computational anatomy, neuroscience, immunology, and genomics. <br/><br/>The methods developed will extend multivariate statistics where specific metrics (such as Fisher's Information metric, Mahalanobis distance, or L1) have already provided successful projections and geometric representations. Differential geometry will provide a rigorous framework that enables the incorporation of local information. The questions raised by using statistics on non-Euclidean varieties will be addressed in collaboration with computational geometers, statisticians, and mathematicians. Creating formal initiatives in geometry, statistics, and information science will bring new energy and focus into the curriculum, with mathematically grounded students encouraged to attack applied data analytic challenges. A key component of this initiative is outreach to women and minority students. Material and tools developed by the program will include open source visualization and approximations packages written in the R programming language."
"1505780","Semiparametric Statistical Methods for Replicated Point Processes","DMS","STATISTICS","08/01/2015","07/25/2015","Daniel Gervini","WI","University of Wisconsin-Milwaukee","Standard Grant","Gabor Szekely","07/31/2019","$199,907.00","","gervini@uwm.edu","3203 N DOWNER AVE","MILWAUKEE","WI","532113153","4142294853","MPS","1269","","$0.00","The main goal of this project is the development of statistical tools for the analysis of points that occur at random in time or space, such as the locations of street robberies in a given city or the timings of spikes of neural activity for an individual performing a certain task. These data arise in many different fields including neuroscience, ecology, finance, astronomy, seismology, and criminology.  The statistical methods to be developed under this project will then provide new data-analysis and inference tools for researchers and practitioners in diverse scientific fields. They will also be applicable to the analysis of data arising in strategic areas such as defense, public health and economy, thus contributing to the improvement of the well-being of individuals in society, the economic competitiveness of the United Stated, and national security.  The educational activities related to this project, including the supervision of Ph.D. and Master's theses and the development of graduate and undergraduate Statistics courses, will also contribute to the improvement of graduate and undergraduate STEM education.<br/><br/>In this project, semiparametric methods for estimation of the intensity functions of replicated point processes will be developed. In recent years, replicated point processes have become more common, and the possibility of pooling data across replications allows for the development of more efficient statistical methods.  However, replicated point processes have received little attention in the Statistics literature and the methods to be developed under this project will help bridge that gap. The methods will be equally applicable to temporal or spatial processes, and will be developed for independent and identically distributed replications as well as for more complex dependency structures, such as ANOVA models, point processes with covariates, and multivariate processes. The research activities to be undertaken include the development of statistical models and parameter estimation methods, the development of algorithms and computer programs to implement these methods, the study of theoretical large sample properties, the study of small sample properties by simulation, and the analysis of real datasets."
"1513412","Estimating Low Dimensional, High-Density Structure","DMS","STATISTICS","08/01/2015","06/09/2017","Isabella Verdinelli","PA","Carnegie-Mellon University","Continuing Grant","Gabor Szekely","07/31/2018","$400,000.00","Larry Wasserman, Christopher Genovese","isabella@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","Data in high dimensional spaces are now very common. This project will develop methods for analyzing these high dimensional data. Such data may contain hidden structures. For example, clusters (which are small regions with a large number of points) can be stretched out like a string forming a structure called a filament. Scientists in a variety of fields need to locate these objects. It is challenging since the data are often very noisy. This project will develop rigorously justified and computationally efficient methods for extracting such structures. The methods will be applied to a diverse set of problems in astrophysics, seismology, biology, and neuroscience. The project will advance knowledge in several fields including computational geometry, astronomy, machine learning, and statistics.<br/><br/>Finding hidden structure is useful for scientific discovery and dimension reduction. Much of the current theory on nonlinear dimension reduction assumes that the hidden structure is a smooth manifold and is very restrictive. The data might be concentrated near a low dimensional but very complicated set, such as a union of intersecting manifolds. Existing algorithms, such as the Subspace Constrained Mean Shift exhibit erratic behavior near intersections. This project will develop improved algorithms for these cases. At the same time, contemporary theory breaks down in these cases and this project will develop new theory to address the aforementioned problem. A complete method (which will be called singular clusters) will be developed for decomposing point clouds of varying dimensions into subsets."
"1513420","Statistical Analysis of Trajectories on Riemannian Manifolds","DMS","STATISTICS","08/01/2015","07/30/2015","Jingyong Su","TX","Texas Tech University","Standard Grant","Gabor Szekely","07/31/2019","$104,974.00","","jingyong.su@ttu.edu","2500 BROADWAY","LUBBOCK","TX","79409","8067423884","MPS","1269","","$0.00","This project addresses the development of statistical tools for the analysis of complex data structures that arise with the use of modern technology.  In many applications, the observations naturally lie on a nonlinear space, such as a sphere, and it is important to study the temporal and spatial behavior of these processes.  The research will impact several data-rich applications including visual speech recognition, action/emotion recognition, and the analysis of medical images.  This project will use techniques from geometry, statistics, and computer science to develop algorithms and tools that can help practitioners derive much needed insight from these data.<br/><br/>In this research, a comprehensive framework for estimation, registration and analysis of manifold-valued processes is proposed. Functional data analysis in Euclidean spaces has been explored extensively in the literature.  In this project, the functions that are studied take values on nonlinear manifolds, rather than in vector spaces.  The observed data are noisy and discrete, and observed at random/arbitrary times.  Two main problems are investigated: the first involves estimation of smoothing trajectories using time-indexed points.  The second problem focuses on the development of a comprehensive framework for joint registration and analysis of multiple manifold-valued processes.   The approach takes into account the temporal variability, derives a rate-invariant metric, and generates statistical summaries (sample mean, covariance), which can be subsequently used for registering and modeling multiple trajectories."
"1513461","Topics in Nonparametric Statistics: Faster Minimax Rates, Large-p-Small-n Cross-Correlation Matrices, Survival Analysis","DMS","STATISTICS","09/15/2015","09/14/2015","Sam Efromovich","TX","University of Texas at Dallas","Standard Grant","Gabor Szekely","08/31/2020","$200,000.00","","efrom@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1269","","$0.00","The project focuses on three main statistical activities motivated by medical, engineering and insurance applications. The first one is the optimal denoising  and decomposition of signals and images.  Analysis of functional magnet resonance images (fMRI) in the human brain is a particular example of application. Loosely speaking, fMRI is a noisy time series of images which reflects the level of oxygen in blood during measurements.  Because this is about dealing with oxygen in blood,  the time series contains respiratory, cardiac and neural response to functional stimuli. The proposed statistical procedure will allow doctors and bioengineers to separate and denoise these components, with potential applications in developing new methods for diagnosis and treatment of Alzheimer's and Parkinson's Diseases. The second topic is  the estimation of large cross-covariance/correlation  matrices for noisy signals, with the number of elements in millions and  sample sizes  of just several hundreds. This is a familiar problem  in statistical analysis of Chip-on-chip microarrays used to study bacteria. Another application is the study of neural plasticity, which is the ability of the brain to recognize neural pathways based on new experience and change in learning. This will allow physicians to create new methods for early diagnosis and treatment of stroke which is  the  4th leading cause of death in the US. The third topic is adaptive and  efficient estimation of hazard rate and survival function from indirect observations, including new methodology of sequentially controlled experiments and protocols. This research is motivated by new methods of radiation and drug therapy for lung and breast cancers as well as by innovative technologies of  waste-water treatment and the actuarial problem of developing adaptive life tables.<br/><br/>The project focuses on three objectives. (1) Advance knowledge and understanding of nonparametric curve estimation to develop a general theory of shrinking local minimax estimation that allows statisticians to get  a new benchmark for the quality of estimation and generate a family of more accurate estimators. Preliminary results indicate that new efficient estimators can be proposed either via mimicking oracles or via aggregation of different estimators in frequency domain. Most challenging and rewarding results are expected for multivariate curves where new rates can remedy the familiar curse of multidimensionality. Applications in statistical analysis of microarrays, fMRI, radiation and drug therapy of cancer cells, and  innovative technologies of  waste-water treatment are expected.(2) Develop new methods of inference for large cross-covariance/correlation matrices for noisy signals, based on wavelet methods and exponential inequalities that can be applied to dependent and non-Gaussian observations. The main application is the study of neural plasticity of the human brain. This allows doctors and bioengineers to  observe changes in neural pathways based on new experience and change in learning with applications to treatment of stroke and other brain diseases.(3) Improve nonparametric theory and create efficient methods of hazard rate and survival function estimation for indirect observations, including a new methodology of sequentially controlled experiments and protocols."
"1454377","CAREER: An Integrated Inferential Framework for Big Data Research and Education","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2015","07/13/2017","Han Liu","NJ","Princeton University","Continuing Grant","Nandini Kannan","09/30/2018","$192,630.00","","hanliu@northwestern.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1269, 8048","1045","$0.00","This project addresses several fundamental challenges in modern data analysis and aims to create a new research area named Big Data Inference. Currently available literature regarding Big Data research mainly focuses on developing new estimators for complex data. However, most of these estimators are still in lack of systematic inferential methods for uncertainty assessment. This project hopes to bridge this gap by developing new inferential theory for modern estimators unique to Big Data analysis.  The deliverables of this project include easy-to-use software packages, which directly help scientists to explore and analyze complex datasets. The principal investigator is also actively collaborating with many scientists to ensure the more direct impact of this project to the targeted scientific communities.<br/><br/>This project aims to develop novel inferential methods for assessing uncertainty (e.g., constructing confidence intervals or testing hypotheses) of modern statistical procedures unique to Big Data analysis. In particular, it develops innovative statistical inferential tools for a variety of machine learning methods which have not yet been equipped with inferential power. It also provides necessary inferential tools for the next generation of scientists to be competitive in modern data analysis."
"1539258","NISS/ASA Writing Workshop for New Statistics Researchers","DMS","STATISTICS","07/01/2015","04/28/2015","Donna LaLonde","VA","American Statistical Association","Standard Grant","Gabor Szekely","06/30/2016","$9,000.00","","donnal@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","7556","$0.00","This award supports the participation of junior researchers in a workshop held at the Joint Statistical Meetings in Seattle, Washington, August 8 - 13, 2015. The workshop focuses on effective technical writing for new researchers in the statistical sciences, who seek to publish their research or to present their research plans in the form of grant proposals for federal funding. Researchers, especially new researchers, often have difficulty disseminating their research results not because of the quality of the research but rather because of inappropriate choices of publication venues for the particular research and/or because of poor presentation of technical material to the chosen audience. The National Institute of Statistical Sciences and the American Statistical Association will manage the Workshop. <br/><br/>This workshop will open with tutorial sessions on the organization of material for a technical article or grant application, on technical writing techniques, and on the specific missions and audiences of key journals in the statistical sciences. Following the introductory tutorial, each participating new researcher will work individually with an experienced journal editor as mentor to address these issues on an individualized basis in a draft of the new researcher's work. Revisions following this guidance will be critiqued by the mentor to assure that the new researcher's implementation of writing techniques has been successful before the article or the grant proposal is submitted for review."
"1513408","A Novel Statistical Framework for Big Data Prediction","DMS","STATISTICS, Cross-BIO Activities, MSPA-INTERDISCIPLINARY, Physiol Mechs & Biomechanics, Systems and Synthetic Biology","09/01/2015","08/31/2015","Shaw-Hwa Lo","NY","Columbia University","Standard Grant","Gabor Szekely","08/31/2019","$300,000.00","Tian Zheng","slo@stat.columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1269, 7275, 7454, 7658, 8011","7465, 8007","$0.00","Recent advances in genome-wide association studies (GWAS) have led to both an increase in the size of genetic data available and identification of important genetic variants responsible for a variety of diseases.  Prediction for these genetic diseases has also become of paramount importance.  However, prediction for big data such as GWAS is not trivial.  A key obstacle in big data prediction is identifying (perhaps a small number of) variable sets that lead to good prediction when variable dimensionality can be extremely large.   The project explores why a common approach towards prediction can often fail to deliver strong prediction rates.   A novel, interaction-based and prediction-oriented approach to extracting hidden information contained in big data will be investigated.  To improve prediction, a new criterion to guide the selection of variable sets will be developed.<br/><br/>Prioritizing predictivity, not significance, requires using the correct estimates of prediction rates and developing predictivity-based criteria to evaluate variable sets.   The project offers a novel theoretical framework by characterizing what makes for highly predictive variable sets, and providing fundamental work towards a new criterion to identify these sets.   In the framework of this research project, variable sets have theoretical (true) levels of predictivity, which can be estimated with appropriately designed sample-based measures.  This framework is the first that seeks to develop estimators specific to a criterion of predictivity.   Additionally, methods that encompass both marginal and joint effects will be investigated, and a candidate measure of predictivity will be studied.  Four real data examples are analyzed to illustrate how final predictors found via the new approach compare to other approaches in the current literature."
"1513414","New Tools for Large-Scale Sparse Inference","DMS","STATISTICS","09/01/2015","07/05/2017","Jiashun Jin","PA","Carnegie-Mellon University","Continuing Grant","Gabor Szekely","08/31/2020","$149,998.00","","jiashun@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","In many application areas, high-throughput devices enable one to gather thousands and sometimes millions of measurements in very short periods of time. This research project addresses statistical problems that arise from Big Data areas such as brain imaging, genomics and genetics, and social networks. The research contains several components: (a) collect and clean large-scale data sets for social networks; (b) develop new models, methods, and theory for analyzing large social networks; and (c) develop new methods and theory for analyzing genomic and genetic data and brain imaging data. The research will have impact in cancer research, neuroscience, and social sciences. <br/><br/>The flood of high-throughput measurements is driving a new branch of statistical practice called Large-Scale Inference. This research project aims to exploit various types of sparsity (for example, signal sparsity, graphical sparsity, sparsity in eigenvalues and leading eigenvectors) to address (a) network data collection and preprocessing, (b) statistical modeling for different types of networks, (c) computationally feasible approaches to precision matrix estimation, (d) cancer classification and clustering, and (e) sparse principal component analysis and post-selection random matrix theory. The research will lead to new data sets and new methods and theory in analyzing data in application areas such as social networks, genomics and genetics, and brain imaging."
"1512893","Collaborative Research:  Generalized Fiducial Inference for Massive Data and High Dimensional Problems","DMS","STATISTICS","09/01/2015","08/16/2017","Jan Hannig","NC","University of North Carolina at Chapel Hill","Continuing Grant","Gabor Szekely","08/31/2019","$150,000.00","","jan.hannig@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","R. A. Fisher, the father of modern statistics, proposed the idea of Fiducial Inference in the 1930s.  While his proposal led to some interesting methods for quantifying uncertainty, other prominent statisticians of the time did not accept Fisher's approach because it went against the ideas of statistical inference of the time.  Beginning around the year 2000, the PIs and collaborators started to re-investigate the ideas of fiducial inference and discovered that Fisher's approach, when properly generalized, would open doors to solve many important and difficult problems of uncertainty quantification.  The PIs termed their generalization of Fisher's ideas as generalized fiducial inference.  After many years of preliminary investigations, the PIs developed a coherent, well thought out plan for a systematic research program in this area.  A large part of this project develops practical solutions for different modeling problems that have natural applications in diverse fields.  Finance (volatility estimation) and measurement science (calibration of measurements from different government labs, for example, US NIST) are two primary examples, while others include gene expression data, climate problems, recommender systems, and computer vision.<br/><br/>This project is motivated by the success of generalized fiducial inference (GFI) as introduced by the PIs as a generalization of Fisher's fiducial argument.  The PIs are now working towards scaling up their GFI methodology to handle big data problems and other difficult problems that have emerged due to our ability to collect massive amounts of data rapidly.  In particular the PIs plan to conduct research into the following topics: (i) a thorough investigation of fundamental issues of GFI including connection with Approximate Bayesian Calculations and higher order asymptotics; (ii) sparse covariance estimation using GFI in the ""large p small n"" context; (iii) development of the idea of Fiducial Selector so that a sparsity of the fiducial distribution is induced as a natural outcome of a minimization problem; (iv) uncertainty quantification for the matrix completion problem using GFI, and (v) applications of GFI to a wide variety of practical problems, such as volatility estimation in finance and international key comparison experiments in measurement science."
"1513594","Constrained Statistical Estimation and Inference: Theory, Algorithms and Applications","DMS","STATISTICS","08/01/2015","06/29/2015","John Lafferty","IL","University of Chicago","Standard Grant","Gabor Szekely","08/31/2017","$320,000.00","","john.lafferty@yale.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","","$0.00","This project lies at the boundary of statistics and machine learning. The underlying theme is to exploit constraints that are present in complex scientific data analysis problems, but that have not been thoroughly studied in traditional approaches. The project will explore theory, algorithms, and applications of statistical procedures, with constraints imposed on the storage, runtime, shape, energy or physics of the estimators and applications. The overall goal of the research is to develop theory and tools that can help scientists to conduct more effective data analysis.<br/><br/>Many statistical methods are purely ""data driven"" and only place smoothness or regularity restrictions on the underlying model. In particular, classical statistical theory studies estimators without regard to their computational requirements. In modern data analysis settings, including astronomy, cloud computing, and embedded devices, computational demands are often central. The project will develop minimax theory and algorithms for nonparametric estimation and detection problems under constraints on storage, computation, and energy. Other constraints to be studied include shape restrictions such as convexity and monotonicity for high dimensional data. The project will also investigate the incorporation of physical constraints through the use of PDEs and models of physical dynamics and mechanics, focusing on both algorithms and theoretical bounds."
"1513076","Bayesian Inference for Peaks Over Threshold Models for Multivariate and Spatial Extremes","DMS","STATISTICS","07/15/2015","07/18/2017","Bruno Sanso","CA","University of California-Santa Cruz","Continuing Grant","Gabor Szekely","06/30/2019","$309,336.00","","bruno@soe.ucsc.edu","1156 HIGH ST","SANTA CRUZ","CA","950641077","8314595278","MPS","1269","","$0.00","Extreme value theory is a branch of probability and statistics that focuses on the study of rare events. There are many areas of science and technology where such methods find applications.  Examples include the quantification of actuarial risk, estimation of large fluctuations in financial markets, and the estimation of maximum water flow.  Of particular relevance for our society is the study of extreme climate events. Historical records of climate related variables provide evidence that there is an intensification of extreme weather.  Climate projections indicate that the frequency and intensity of events with catastrophic potential will increase even further.  This research focuses on the development of statistical methods that will enable careful assessment of the uncertainties related to extreme events. The proposed methods will focus on models that look jointly at several variables and apply to observations collected in large spatial domains. Probabilistic assessment of the uncertainties in the occurrence of rare events will be made possible by a Bayesian approach.  This will provide a powerful tool for rational decision and policy making.<br/><br/>In this project, novel methodology for the statistical analysis of the distributions of extreme values is proposed. The methods are based on using the amounts in excess of a fixed threshold for the variables of interest, or peaks over thresholds (POT).   POT methods to perform Bayesian inference for (a) multivariate observations, (b) spatially indexed fields, and (c) fields of multivariate observations in space will be developed and implemented.  In extreme value theory, the focus is on extrapolation as scarce extreme observations are used to describe the behavior of the tails of the distribution. The theory and the methods for inference on univariate extreme values are firmly established and fully developed. For multivariate problems, it is key to model the joint tail dependence of the different variables. In this sense, the theory is well understood, but inferential methods are not as straightforward as in the univariate case. This is especially true for POT methods.  A further level of complication is introduced when dealing with georeferenced data. In fact, in the spatial setting, it is impossible to write the full likelihood of realistic POT models for observations collected at an arbitrary number of locations. This research focuses on the development of methods that (a) are conceptually clear to specify using a simple factorization that is at the core of Bayesian hierarchical models, (b) allow for fully integrated Bayesian inference that accounts for all estimation uncertainty and quantifies it probabilistically, (c) have theoretically sound asymptotic properties, (d) provide flexible characterizations of a wide range of tail dependence, and (e) are computationally feasible for large spatial domains."
"1510219","Collaborative Research: Prediction and Model Selection for New Challenging Problems with Complex Data+","DMS","STATISTICS","08/15/2015","08/07/2015","Jiming Jiang","CA","University of California-Davis","Standard Grant","Gabor Szekely","07/31/2018","$112,340.00","","jiang@wald.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","Mixed model prediction, that is, prediction based on a class of statistical models known as mixed effects models, has a fairly long history. The traditional fields of applications have included genetics, agriculture, education, and surveys. Nowadays, new and challenging problems have emerged from such fields as business and health sciences, in addition to the traditional fields, to which methods of mixed model prediction are potentially applicable, but not without further methodology and computational developments. Some of these problems occur when interest is at subject level, such as personalized medicine, or (small) sub-population level, such as small communities, rather than at large population level. In such cases, it is possible to make substantial gains in prediction accuracy by identifying a class that a new subject belongs to. Other challenging problems occur when applying existing model search strategies in situations of incomplete or missing data, in model search or selection when prediction is of primary interest, and in making statistical inference based on the result of model search or selection. This collaborative research project aims at solving these challenging problems in prediction and model selection in situations of complex data, such as incomplete or missing data, and data that are correlated due to presence of random effects.<br/><br/>In this collaborative research project the PIs develop a novel statistical method, called classified mixed model prediction, to identify the subject class. This way, the new subject is associated with a random effect corresponding to the same class in the training data, so that the mixed model prediction method can be used to make the best prediction. Furthermore, the PIs develop a recently proposed method, called E-MS algorithm, for model selection in the presence of incomplete or missing data. The PIs also develop an idea called predictive model selection by deriving a predictive measure of lack-of-fit, and combining this measure with a recently developed class of strategies of model selection, called the fence methods. Finally, the PIs develop a unified Jackknife method to accurately assess uncertainty in mixed model analysis after model selection. Theories will be established for these new methods, and their performance and potential gains through extensive Monte-Carlo simulations will be studied. The new methods will be implemented in the R language/environment for statistical computing and graphics. All of the developed methodologies will be applied and tested in a number of applications via a series of close collaborations with experts who will provide access to the data and also guidance in interpretation and dissemination of findings. The fields of applications include genetics, health and medicine, agriculture, education, business and economy. The research project will also promote teaching, training and learning that involve under-represented groups, and build research networks between our institutions."
"1463642","FRG: Collaborative Proposal: Extreme Theory Value Theory for Spatially Indexed Functional Data","DMS","STATISTICS","08/01/2015","07/08/2017","Joshua French","CO","University of Colorado at Denver","Continuing Grant","Gabor Szekely","07/31/2019","$129,326.00","","joshua.french@ucdenver.edu","13001 E 17TH PLACE F428","AURORA","CO","800452571","3037240090","MPS","1269","1303, 1616","$0.00","This project focuses on the development of statistical tools to model the spatial and temporal structure of environmental and climate extreme events. Most climate and environmental data sets can be viewed as collections of curves, one curve per year, available at several locations within a region. For example, temperature at a specific location has an annual pattern. The shapes of such annual curves change from year to year, and from location to location. Extreme departures from a typical pattern over a sizeable region can impact agricultural production and public health.  The economic impacts are considerable, particularly, if they occur at unexpected times and locations.  An unusual timing of a heat wave over a large area may cause significant economic damage due to crop failure and forest fires, and also affect the level of preparedness of public health services.  Similarly, long spells of cold, storm-free winter time weather often lead to an increase in particulate pollution levels in densely populated mountain valleys.  It is important that public officials are well-informed about the possible range and impact of such extreme events.  This project will contribute toward a rigorous and objective understanding of the risks involved, and provide quantitative tools for researchers and decision makers in the fields of agriculture, public health, actuarial science, climatology and ecology.<br/><br/>The project seeks to develop a statistical framework for a quantitative assessment of possible extremal departures from the usual annual pattern over a region, i.e. departures of the form that have not been observed in historical records, but can occur with a positive probability. The primary focus of the project is the creation of a mathematical framework, and implementation through the development of statistical software. Building on recent advances in functional data analysis, extreme value theory and spatio-temporal statistics, methodology for modeling the extremal distributions of curves observed at spatial locations will be developed.  Extreme curves will be determined by functionals defined on a function space in which the curves live.  The work will be guided and validated by the analysis of several historical, derived, and computer data sets.  Exploratory analysis will reveal the most prominent properties of extremal shapes. This will be followed by model building and the development of asymptotic theory needed to evaluate probabilities of events not previously observed. The models will reveal extremal features of the spatially indexed functional data that are not apparent from the exploratory analysis.  Procedures for the construction of confidence regions, where extremal departures may occur with prescribed probability, will be obtained. Exploratory and inferential tools for the assessment of trends in the extremal shapes and regions over which they occur will also be derived."
"1509739","Asymptotics and concentration in spectral estimation for large matrices","DMS","STATISTICS","07/01/2015","06/19/2015","Vladimir Koltchinskii","GA","Georgia Tech Research Corporation","Standard Grant","Gabor Szekely","06/30/2019","$289,425.00","","vlad@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","","$0.00","Estimation of large matrices and their spectral characteristics is crucial in a variety of problems of science and engineering that deal with large high-dimensional data sets. Spectral methods are of the utmost importance in kernel machine learning, manifold learning, functional data analysis, community detection in large networks, quantum statistics and quantum information, and many other applications. The purpose of the project is to develop new mathematical tools needed in analysis of high-dimensional and infinite-dimensional matrix estimation problems that could potentially lead to more powerful methods of statistical inference for complex, high-dimensional data. The project also includes a number of activities with an impact on graduate education and on research collaborations between statistics, computer science and other areas.<br/><br/>The focus of the project is on concentration properties and asymptotics of spectral characteristics (eigenvalues, eigenvectors, spectral projectors) of several important classes of random matrices and operators playing crucial role in high dimensional and infinite dimensional statistical inference and in machine learning. They include sample covariance operators, kernel matrices in machine learning, empirical heat kernels and Laplacians for manifold data, matrices involved in spectral clustering problems on graphs, matrix estimators in trace regression problems (such as matrix completion and quantum state tomography). The main goal is to study the problems where there exists an operator norm consistent estimator of the target matrix (operator), but it converges at a slow rate and to develop a broad range of concentration bounds and asymptotic results for specific functionals of the underlying random matrix (operator) such as its eigenvalues, bilinear forms of its spectral projection operators, norms of deviations of empirical spectral projection operators from their true counterparts. The solution of these problems relies on further development of the methods of high-dimensional probability, such as concentration inequalities, generic chaining bounds for Gaussian, empirical and related classes of stochastic processes, non-asymptotic bounds for random matrices."
"1512975","Functional Data Analysis: From Univariate to High-Dimensional Functional Data","DMS","STATISTICS","09/15/2015","07/09/2017","Jane-Ling Wang","CA","University of California-Davis","Continuing Grant","Gabor Szekely","08/31/2019","$179,999.00","","janelwang@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","This project deals with data that are in the form of functions, images, and shapes.  A common feature of such data is that they are infinite dimensional, which distinguishes them from traditional data. Methodology and theory for these data is termed ""functional data analysis,"" an area that is rapidly evolving.  The infinite dimensional nature of functional data calls for sampling and dimension reduction approaches, which are challenging due to the large sizes of the data sets.  This research tackles these challenges by developing novel dimension reduction methodology to model univariate functional data (Projects 1 and 2), as well as next generation functional data (Projects 3 and 4).  The guiding principle is to accomplish dimension reduction and flexible modeling simultaneously.  The proposed research, motivated by potential applications to research on aging, biomedical research, and neuroimaging, includes theory, methodology, data analysis, and applications.  The project is also driven by the pressing need to involve more statisticians in brain research and to train the next generation of statisticians to tackle the challenges of big data.  Software development is a key component of this project, and the accompanying computer code will be integrated into an existing open-source package, PACE, available for public access. <br/><br/>Functional data analysis is a fascinating area in statistics that deals with a sample of random functions. However, measurements of these random functions, realistically, can only be taken at discrete time points or grids, and the measurements often are contaminated by noise.  Current approaches in functional data analysis are typically tailored toward specific sampling plans for the measurements.  In Project 1, a unifying approach, both in theory and in implementation, for functional regression is proposed.  The focus is on dimension reduction models for functional responses and functional/vector covariates, where spline basis functions will be used to estimate the unknown nonparametric components functions.  Project 2 deals with a new class of functional survival models that accommodate censored univariate response data with functional/vector covariates.  These models differ fundamentally from existing survival models, and can handle censored response data in contrast to current functional (generalized) linear models.  Projects 3 and 4 deal with multivariate functional data, with Project 3 focusing on measures of disparity or synchronization for pairs of functional data, and Project 4 focusing on reconfiguration of high-dimensional multivariate functional data.  New functional measures of distance or synchronization are proposed in Project 3 using a novel concept that aims at concordance of the derivatives of functional data.  These new measures facilitate the reconfiguration of high-dimensional functional data in Project 4, so that functions in nearby regions of the reconfigured space are smoothly connected, thereby overcoming the curse of high dimensionality.  The proposed reconfiguration methods not only extend the multidimensional scaling method from multivariate to functional data, but also turn the curse of high dimensionality into a blessing under mild assumptions."
"1513647","Studies on Signals and Images via the Fourier Transform","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, Cross-BIO Activities, MSPA-INTERDISCIPLINARY, Activation","08/15/2015","08/13/2015","Suhasini Subba Rao","TX","Texas A&M University","Standard Grant","Gabor Szekely","07/31/2019","$184,328.00","","suhasini@stat.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1253, 1269, 7275, 7454, 7713","1253, 8007, 8091","$0.00","The goal of this project is to develop novel statistical methods that address some of the current challenges in analyzing spatio-temporal data frequently encountered in neuroimaging. One major application of this project is to identify features in brain signals that could differentiate healthy individuals from patients with neurological or mental diseases. The second application is to identify changes that take place in a brain signal during cognitive processing (e.g., while a human learns a new motor skill or while a rat learns risks and rewards in a controlled experiment).  The third application is to identify biomarkers in brain signals that could predict a stroke patient's ability to recover loss of motor functionality.   The approach used to solve these problems requires a study of the oscillatory patterns in these brain signals. <br/> <br/>Motivated by these practical problems, statistical methods based on the discrete Fourier transform (DFT) are developed. The DFT gives an indication of the decomposition of variance in the time series. Under stationarity, the covariance of the DFT is sparse and thus a departure from sparsity is an indication of non-stationarity.   Moreover, the covariance of the DFT can be utilized as a discriminator between classes of signals.  Using the properties of the DFT, novel methods for (1) change-point detection in time series based on sparsity of the DFT, and (2) discrimination and classification of classes of time series based on the properties of the covariance of the DFT will be developed.  The DFT will also be used to estimate the variance of functionals of the spectrum and test for serial correlation and stationarity in nonlinear time series.  Validation for stationary spatial processes and non-stationary spatial processes using the two-dimensional DFT will also be developed."
"1512188","Bayesian Time Series Methods in the Frequency Domain","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS","08/15/2015","08/11/2015","Ori Rosen","TX","University of Texas at El Paso","Standard Grant","Gabor Szekely","07/31/2019","$250,653.00","","orosen@utep.edu","500 W UNIVERSITY AVE","EL PASO","TX","799680001","9157475680","MPS","1253, 1269","1253, 8091","$0.00","This project focuses on developing novel statistical methods for the analysis of dependent data arising in biomedical applications. Often, frequency patterns in such data contain interpretable scientific information. The main motivating applications in this project include epilepsy-related EEG (electroencephalogram), sleep, and DNA sequence data. In epilepsy research, advance warning of an epileptic seizure based on the analysis of intracranial EEG could minimize injury, and give patients a sense of control in their management of the disease. Simultaneous analysis of multiple channels may lead to more accurate estimates, compared to separate analyses of signals from individual channels.  The proposed methodology will also be used to analyze data from the AgeWise study to help understand the connections between self-reported measures of sleep and electrophysiological signals.<br/><br/>This project is focused on adaptive spectral estimation for nonstationary multivariate time series using Bayesian modeling that relies on Markov chain Monte Carlo methods for the estimation. Methods are developed for estimating local spectra of qualitative or quantitative bivariate and trivariate nonstationary time series. The multivariate Whittle approximation is used for approximating local likelihoods corresponding to small segments of the time series. Each approximate local likelihood is a function of the discrete Fourier transform of the segment and the corresponding local spectral matrix. Spectral matrices are expressed as modified Cholesky decompositions, thus allowing estimation that guarantees Hermitian and positive definite matrices. Smoothing splines are used for estimating the elements of the spectral matrices as a function of frequency. Additionally, the proposal develops methodology for the analysis of time series collected along with covariates on multiple subjects, where the goal is to model spectral matrices as a function of both frequency and covariates. The proposed methods are applied to the analysis of intracranial EEG signals from two channels in the brain of an epileptic patient, sleep data on older adults from the AgeWise study, and DNA sequences."
"1455172","CAREER: New Frontiers in Time Series Analysis","DMS","STATISTICS, Division Co-Funding: CAREER","07/01/2015","07/26/2019","David Matteson","NY","Cornell University","Continuing Grant","Gabor Szekely","06/30/2020","$400,000.00","","dm484@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1269, 8048","1045","$0.00","Big data permeates business, engineering, and science -- the number of connected smart devices, even excluding phones, tablets, and PCs, is projected to grow from billions to tens of billions within five years. Vast data is generated from sensors, GPS, RFID, medical devices, and emergency and energy systems, to provide rich information about untold aspects of the modern world. Despite the ubiquity and significant interest in mining such data, there are few existing analytical tools that are suitable. The investigator focuses on the development of new statistical methodology, extending application of these methods to new fields, and on increasing understanding of the theoretical challenges in data-driven model building and inference. The methods under development have the potential to strengthen research in numerous fields, including astronomy, economics, emergency medical services, neuroscience, and statistics itself.<br/><br/>Time series analysis is a rich and historic field, but it remains centered on univariate and low dimensional multivariate analysis. In recent years, big data has begun to permeate business, engineering, and science, but despite the ubiquity and significant interest in mining such data, there are few existing analytical tools suitable for data with a time structured format. The investigator studies the development of new high dimensional and functional time series (HDTS) tools to help researchers and practitioners meet increasingly ambitious inferential and modeling aims. Specifically, the investigator studies: (i) new methods for simultaneous modeling, inference, and forecasting of dynamic functional data; (ii) new structured regularization methods for modeling high dimensional time ordered data; (iii) new adaptive, yet methods of stability analysis for big data monitoring systems; and (iv) linking these new methods with emergent lines of inquiry and providing an infrastructure for answering critical research questions in a wide range of fields."
"1513654","Collaborative Research:  Hierarchical Sparsity-Inducing Gaussian Process Models for Bayesian Inference on Large Spatiotemporal Datasets","DMS","STATISTICS","09/15/2015","09/11/2015","Sudipto Banerjee","CA","University of California-Los Angeles","Standard Grant","Nandini Kannan","08/31/2018","$240,000.00","","sudipto@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","","$0.00","With the increasing capabilities of geographical referencing and remote-sensing technologies such as Geographical Information Systems (GIS) and Global Positioning Systems (GPS) that can identify geographical coordinates with a simple hand-held device, scientists and researchers in a variety of disciplines today have unprecedented access to spatially-referenced data.  From identifying spatial disparities in health standards to more precise weather predictions, GIS technology is used today in almost every sphere of human life with beneficial effects that can be far-reaching.  Statistical modeling and analysis for spatial data constitute a key element in harnessing the scientific potential of GIS and related technologies.  As the scientific community moves into a data-rich era, there is unprecedented opportunity to build an understanding about how environmental ecosystems function and how they will respond to changing environmental conditions.  This research project will advance data modeling in disciplines as diverse as forestry, ecology, public and environmental health, meteorology, engineering, and the geosciences.  It will help discover complex scientific relationships, which, in turn, will lead to better analysis and understanding of our environment and how our ecosystem is evolving.<br/><br/>Analysts and researchers using GIS technology are increasingly faced with analyzing massive amounts of spatial data.  With spatial and spatial-temporal data becoming increasingly high-dimensional -- both in terms of number of observed locations and the number of observations per location -- scientists are seeking to hypothesize extremely complex relationships.  Not surprisingly, statistical models accounting for spatial associations have become an enormously active area of research over the last decade and, in particular, hierarchical models capturing variation at multiple scales have become extremely popular for spatial modeling.  These, in turn, lead to rather complex models that are computationally expensive and unfeasible even for moderately sized data sets.  This project recognizes the increased computational demands in statistical modeling of large high-dimensional spatial and spatial-temporal data and offers a model-based setup to tackle a wide variety of data analytic problems.  The emphasis of this project is on rigorous and principled statistical methodology that can be implemented on standard computing platforms, thereby ensuring accessibility for a very wide group of researchers.  The project outlines a suite of spatial models that easily scale to massive databases and have a broad range of applications.  Theoretical and methodological innovations that enhance current methods will be presented, and their practical implications will be illustrated using freely distributed open-source statistical software products developed as a part of this project."
"1510238","Bayesian estimation and uncertainty quantification for high dimensional data","DMS","STATISTICS","07/01/2015","06/12/2017","Subhashis Ghoshal","NC","North Carolina State University","Continuing Grant","Gabor Szekely","08/31/2019","$240,000.00","","subhashis_ghoshal@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","","$0.00","Statistical data in modern context appear in increasing size, form and complexity such as images, videos, functions, trees  from diverse sources including barcodes, internet searches, social networks, mobile devices, satellites, genomics, medical scans etc. Such data are typically huge in size and dimension. Nevertheless, some lower dimensional structures is commonly hidden within such data. The Bayesian approach to decision making is particularly useful in the context since structural property in the data can be easily incorporated in its framework and can automatically quantify the uncertainty in the decision making process.  Computation however remains a challenge in the big data regime since common computing methods do not scale well, especially when a large number of models are involved in the analysis. Some of the newest cutting-edge techniques for computing and evaluating methods will be used in the proposed research. It will have significant impact on studying relations between variables in human brain development, gene-pathway analysis and other applications. Computational packages will be developed and users will be given free access. Results will be disseminated through articles, seminars and talks given at various places. The proposed research will connect various concepts together and synthesize into a powerful approach for analyzing high dimensional data appropriate for subject specific and interdisciplinary research in STEM disciplines. The educational component of the proposal will impact human resource development in the form of graduate student advising and offering of special topics courses. The PI is committed to involving female students and students from under-represented groups to promote diversity.<br/><br/>The proposed research will have all round involvement in theory, computation and application concerning Bayesian analysis of high dimensional data of various types. Both parametric and nonparametric models will be considered and important issues of estimation, prediction, clustering and assessing model uncertainty will be addressed for a variety of data types including graphs, networks, pathways and trees. Techniques of prior construction, scalable computation and uncertainty quantification will be developed and study of frequentist convergence properties of the resulting procedures will be initiated. Some of the most recent ideas on continuous shrinkage priors which have computational advantage in the high dimensional setting will be employed in the proposed research. Study of posterior convergence properties is extremely delicate in nonparametric and high dimensional models. Using the theoretical tools developed by the PI and other researchers will be employed to study convergence properties of the posterior distributions, and thus will help identify the most efficient methods. The methods developed from the proposed research will be applied in studying brain images, cancer studies and various other contexts."
"1510446","Optimal Shrinkage Estimation for Heteroscedastic Data","DMS","STATISTICS","07/01/2015","06/09/2017","Samuel Kou","MA","Harvard University","Continuing Grant","Gabor Szekely","06/30/2019","$304,146.00","","kou@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","","$0.00","Shrinkage estimators are powerful statistical methods that have profound impact in scientific and engineering applications. They provide efficient tools to pool information from related populations for simultaneous inference -- the data on each population alone often do not lead to the most effective estimation, but by pooling information from the related populations together one can often obtain more accurate estimates for each individual population. Examples of the applications of shrinkage estimators include the analysis of healthcare systems (quality of hospital services, etc.), the analysis of education programs (the effectiveness of teaching programs), the assessment of medical treatments (pooling information from multiple studies or clinical trials), the ranking of multiple genes on their association with diseases, and the comparison of multiple manufacturing processes. This research project will investigate shrinkage estimations in the context of heterogeneous data, aiming to identify optimal ways to conduct shrinkage estimation in various settings.<br/><br/>This research project studies and identifies risk-optimal shrinkage estimators under parametric as well as semi-parametric settings for heteroscedastic data. In addition to thorough theoretical investigation, the project includes comprehensive numerical studies and real applications. The research topics include the investigation of optimal shrinkage estimators for heteroscedastic data from non-Gaussian exponential families; the investigation of optimal shrinkage estimators for heteroscedastic data from location-scale families; and the investigation of optimal shrinkage estimators for heteroscedastic data from linear regression models. The research aims to advance the theoretical understanding of shrinkage estimators and also provide powerful techniques for the analysis of data in medical, natural and social sciences, and engineering."
"1545738","Monte Carlo Methods for Analysis of Large Spatial Data","DMS","STATISTICS","03/02/2015","05/26/2015","Faming Liang","FL","University of Florida","Standard Grant","Gabor Szekely","07/31/2015","$38,829.00","","fmliang@purdue.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","","$0.00","Spatial data sets are analyzed in many scientific disciplines, such as ecology, geology, and environmental sciences.  However, the classical approaches, such as Kriging and Bayesian hierarchical Gaussian modeling, often break down for large data sets due to expensive matrix inverse operations, whose computational complexity increases in cubic order with the number of spatial locations. To alleviate this difficulty, various approximation approaches, such as covariance tapering,  lower-dimensional space spatial process approximation, likelihood approximation and Markov random field approximations, have been proposed under the general idea of approximating the original spatial model with a computationally convenient model. A general concern on these approaches is the adequacy of approximation. In this proposal, the investigators propose three new approaches, Bayesian auxiliary lattice approach, Bayesian site selection approach and marginal inference approach. The Bayesian auxiliary lattice approach introduces an auxiliary lattice to the space of observations and defines a hidden Gaussian Markov random field on the auxiliary lattice.  By using some analytical results of Gaussian Markov random fields,  the Bayesian auxiliary lattice approach completely avoids the problem of matrix inversion in likelihood evaluation. The Bayesian site selection approach reformulates the problem of spatial model estimation as a problem of Bayesian variable selection. It works with only a small proportion of the data at each iteration and thus significantly reduces the dimension of the data. The marginal inference approach is proposed based on the idea of bootstrap resampling. Like the Bayesian site selection approach, it works with only a small proportion of the data at each iteration and thus significantly reduces the dimension of the data. It is worth noting that the Bayesian site selection and marginal inference approaches are conceptually very different from the approximation approaches existing in the literature. The existing approximation approaches are to approximate the original model using a computationally convenient model. Instead, the Bayesian site selection and marginal inference approaches seek to reduce the dimension of the data,  while not sacrificing the complexity of the original model. In this proposal, the investigators also extend the proposed approaches to spatio-temporal models with applications to satellite climate data. How to deal with missing data for spatio-temporal models are addressed.<br/><br/>The intellectual merit of this project is to provide some computationally efficient or data dimension reduction approaches for statistical analysis of large spatial data. The new approaches address some core problems in spatial data analysis, such as large matrix inversion and missing data imputation. The new approaches are expected to play a major role in statistical analysis of geostatistical data, satellite climate data and other large spatial data. This project will have broader impacts in both communities of spatial statistics and computational atmospheric sciences. The research results will be disseminated to the communities via direct collaboration with researchers in other disciplines, conference presentations, books, and papers to be published in academic journals.  The project will have also significant impacts on education through direct <br/>involvement of graduate students in the project and incorporation of results into undergraduate and graduate courses."
"1541099","CAREER: Large Scale Stochastic Optimization and Statistics","DMS","STATISTICS","02/01/2015","07/14/2015","Philippe Rigollet","MA","Massachusetts Institute of Technology","Continuing Grant","Gabor Szekely","06/30/2017","$208,669.00","","rigollet@math.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1269","1045","$0.00","Stochastic optimization offers a general framework to study many fundamental statistical problems related to prediction such as regression, classification and density estimation. Furthermore, it is a natural framework to import powerful algorithms from numerical optimization, especially for large scale problems. The broad goal of this project is to understand the fundamental interactions between statistics and stochastic optimization. To accomplish this task the investigator (a) identifies new problems from statistics, especially with complex structure, that can be recast as stochastic optimization problems; (b) develops new algorithms that optimally and efficiently solve large scale problems; (c) determines essential characteristics of the problems that govern the performance of algorithms and their fundamental limitations; and (d) explores peripheral problems of stochastic optimization including stochastic optimization with stochastic constraints and stochastic optimization with limited feedback. <br/><br/>The information era has witnessed an explosion in the collection of data and large scale data sets are ubiquitous in a wide range of applications including biology, networks, environmental science, sociology and marketing. This results in an acute need of new statistical methods to analyze these data sets of unprecedented size. While techniques from numerical optimization can be used in several scenarios, their analysis remains largely dissociated from that of the statistical task at hand. This research aims at providing a unified treatment of a number of large scale problems emerging from statistical learning and from optimization under uncertainty in general. Therefore, the project will not only result in new and effective algorithms, but also in a novel theoretical framework that supports the analysis of stochastic optimization problems and enables further improvements of said algorithms."
"1645093","Tensor Regressions and Applications in Neuroimaging Data Analysis","DMS","STATISTICS","10/01/2015","04/25/2017","Hua Zhou","CA","University of California-Los Angeles","Continuing Grant","Gabor Szekely","06/30/2018","$85,599.00","","huazhou@ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","","$0.00","Rapidly advancing medical imaging technologies are producing massive amounts of complex imaging data, and are imposing unprecedented demands for new statistical methodology. The investigators aim to integrate advanced statistical modeling with modern computational techniques to address some most challenging questions arising from medical imaging analysis. The investigators propose a novel statistical framework and develop accompanying theory and algorithms for tensor regression, i.e., regression with image covariates that are in the form of multidimensional arrays / tensors. They study a variety of regularization schemes in the context of tensor regression to stabilize estimation, improve risk property, and reconstruct sparse signals. They also develop methodology within the tensor regression framework for scientific applications including brain region and connectivity pattern identification, imaging based disease diagnosis, and multiple imaging modalities analysis. The project offers a systematic solution to a family of imaging data problems, and also provides a new class of statistical regression methods.<br/><br/>One of the most intriguing questions in modern science is to understand human brains, both those of general population and those with neuropsychiatric and neurodegenerative disorders. Advanced medical imaging technologies provide powerful tools to help address the question, producing imaging data of unprecedented size and complexity. The investigators aim to develop a host of novel statistical methods, theories, and highly scalable algorithms for the analysis of massive medical imaging data. The proposed research is expected to make significant contributions on two fronts: timely response to the growing needs and challenges of neuroimaging data analysis, and development of an utterly new and broad statistical framework and the associated methodology that contributes to the advance of the statistical discipline."
"1513622","A Novel Approach to Study Nonlinearity and Interaction in Regression","DMS","STATISTICS","09/01/2015","08/07/2017","Ker-Chau Li","CA","University of California-Los Angeles","Continuing Grant","Gabor Szekely","03/31/2020","$300,000.00","","kcli@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","","$0.00","The increasing access to large databases across many global economic sectors, as well as various scientific disciplines, poses significant challenges that demand novel statistical methodologies for conducting data analytics.  A common feature of these databases is the large number of subjects under study, and the large number of variables describing a wide variety of attributes attached to each subject.  Guided by the dual principles of parsimony and accuracy, statistical models can be developed to serve as informative approximations to the unknown physical reality underlying the intricate mechanism of the data generation process.  However, a lack of understanding of the nature and the pattern of complex nonlinear interactions between variables involved in the system has largely hindered the progress in the field of high-dimensional data analysis.  In this project, novel statistical theory, methods, and software will be developed. These results will address key foundational issues and open up new avenues for research.<br/><br/>Regression is one of the most fundamental concepts in statistics, with applications in almost all disciplines.  The growing interest in high-dimensional regression either from the forward or from the inverse perspective has brought out many facets that increase its flexibility in modeling complex data with proven statistical efficiency.  Working in the data-intensive area of genomics, a novel statistical notion, liquid association (LA), has been developed that allows researchers to study dynamic patterns of co-regulation between genes.  This project will investigate methods of employing LA for variable selection in high-dimensional regression.  In combination with inverse modeling techniques including sliced inverse regression (SIR), hidden patterns of interaction between clusters of nonlinearly-correlated variables can be revealed.  In many applications, it is often necessary to incorporate a certain set of background variables that are known to be important in the model.  This adds another layer of complexity in revealing patterns in the interlocked variable-to-variable interactions.  Methods of marginalizing the interference caused by background variables will also be developed."
"1517152","Conferences on Frontiers in Applied and Computational Mathematics: 2015-2017","DMS","APPLIED MATHEMATICS, STATISTICS, COMPUTATIONAL MATHEMATICS, FD-Fluid Dynamics, MATHEMATICAL BIOLOGY","06/01/2015","05/16/2015","Michael Siegel","NJ","New Jersey Institute of Technology","Standard Grant","Lora Billings","05/31/2016","$20,000.00","Amitabha Bose, Cyrill Muratov","misieg@njit.edu","University Heights","Newark","NJ","071021982","9735965275","MPS","1266, 1269, 1271, 1443, 7334","058E, 059E, 7556, 9263","$0.00","This grant supports the participation of undergraduate and graduate students, postdoctoral fellows, junior faculty, and other researchers in a conference on ""Frontiers in Applied and Computational Mathematics"" (FACM) at the New Jersey Institute of Technology (NJIT) on June 5-6, 2015.  FACM 2015 is focused on mathematical fluid dynamics and applications in biomedicine and climate science.  There will be dedicated minisymposia on biofluid dynamics, the connection between mathematical modeling and experiment, geophysical fluid dynamics, wave propagation in fluids, and computational methods, including high-performance computing and multiscale methods. In addition, there will be five minisymposia on the application of statistics and data analysis to biomedicine and to atmosphere/ocean science, which are areas of great current interest.  This conference series brings together mathematicians, statisticians, scientists and engineers in an environment where significant interaction and cross-fertilization takes place. More information can be found on the conference website http://m.njit.edu/Events/FACM15/.<br/> <br/>The FACM conference series has been organized over the past eleven years by the Department of Mathematical Sciences and Center for Applied Mathematics and Statistics at NJIT. The annual meeting is a forum for the dissemination of research in applied and computational mathematics and statistics.  FACM conferences are more intimate and student centered than large society meetings, and a goal of the organizers is to introduce future leaders of applied mathematics to established investigators and emerging research areas. Participation among graduate students and postdocs is greatly enhanced with contributed talks through which they give presentations in minisymposia alongside leading scientists. For students and postdocs, this will be a learning and networking experience that will help them with their research and career paths. Special efforts will be made to continue the participation underrepresented groups in the conference."
"1462067","FRG:  Collaborative Research:Extreme Value Theory for Spatially Indexed Functional Data","DMS","STATISTICS","08/01/2015","07/08/2017","Piotr Kokoszka","CO","Colorado State University","Continuing Grant","Nandini Kannan","07/31/2018","$209,075.00","","Piotr.Kokoszka@colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1269","1303, 1616","$0.00","This project focuses on the development of statistical tools to model the spatial and temporal structure of environmental and climate extreme events. Most climate and environmental data sets can be viewed as collections of curves, one curve per year, available at several locations within a region. For example, temperature at a specific location has an annual pattern. The shapes of such annual curves change from year to year, and from location to location. Extreme departures from a typical pattern over a sizeable region can impact agricultural production and public health.  The economic impacts are considerable, particularly, if they occur at unexpected times and locations.  An unusual timing of a heat wave over a large area may cause significant economic damage due to crop failure and forest fires, and also affect the level of preparedness of public health services.  Similarly, long spells of cold, storm-free winter time weather often lead to an increase in particulate pollution levels in densely populated mountain valleys.  It is important that public officials are well-informed about the possible range and impact of such extreme events.  This project will contribute toward a rigorous and objective understanding of the risks involved, and provide quantitative tools for researchers and decision makers in the fields of agriculture, public health, actuarial science, climatology and ecology.<br/><br/>The project seeks to develop a statistical framework for a quantitative assessment of possible extremal departures from the usual annual pattern over a region, i.e. departures of the form that have not been observed in historical records, but can occur with a positive probability. The primary focus of the project is the creation of a mathematical framework, and implementation through the development of statistical software. Building on recent advances in functional data analysis, extreme value theory and spatio-temporal statistics, methodology for modeling the extremal distributions of curves observed at spatial locations will be developed.  Extreme curves will be determined by functionals defined on a function space in which the curves live.  The work will be guided and validated by the analysis of several historical, derived, and computer data sets.  Exploratory analysis will reveal the most prominent properties of extremal shapes. This will be followed by model building and the development of asymptotic theory needed to evaluate probabilities of events not previously observed. The models will reveal extremal features of the spatially indexed functional data that are not apparent from the exploratory analysis.  Procedures for the construction of confidence regions, where extremal departures may occur with prescribed probability, will be obtained. Exploratory and inferential tools for the assessment of trends in the extremal shapes and regions over which they occur will also be derived."
"1523379","Shape constraint inference: Open problems and new directions","DMS","STATISTICS","09/01/2015","06/04/2015","Wolfgang Polonik","CA","University of California-Davis","Standard Grant","Gabor Szekely","08/31/2016","$7,000.00","","wpolonik@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","7556","$0.00","The international conference on ""Shape constraint inference: Open problems and new directions"" will take place October 5 - 9, 2015, at the Lorentz Center, Leiden, The Netherlands. The topic of this conference is the study of the performance of a certain category of statistical methodologies. These methodologies distinguish themselves from related methods by their goal to explicitly incorporate a certain type of structural prior knowledge about the target object. Many examples of practical importance exist, for instance, in finance, economics, genomics and medicine. A simple example is given by the reasonable statement that older cars tend to have a higher risk of failure. Thus when predicting the risk of failure of a car  at different ages, the statistical methodology should explicitly use this prior knowledge. Ideally this should be achieved without imposing any further constraints that are difficult to justify in practice. The structural assumption just described, i.e. being increasing over time, is classical. Modern statistical challenges are much more complex. For instance, the object of interest might not just be influenced by one factor (""age"" in the above example), but by a large number of factors, and the influence of these factors might differ. While some of them might increase the risk, some others might decrease it, for instance. There are many other important types of structural constraints for which statistical methodology needs to be developed. It is of crucial importance to gain a thorough understanding for the behavior of these methods. Answering questions such as ""When do these methods work, and when do they not work?"" is crucial. There are also computational challenges associated with the development of these methodologies that need to be tackled. The conference will provide a forum to advance this field of statistics. Participants will consist of both senior and more junior researchers, as well as postdocs and PhD students. Special emphasis will be given to increase female participation.<br/><br/>Shape and order constraints can be considered as a way to regularize an underlying problem in a more explicit and verifiable way, which in particular is useful in multi and high-dimensional situations. Despite the intuitive appeal often inherent in shape constraint methods, their analysis and the computations involved tend to be challenging. Thus tackling such problems in complex situations (e.g. high-dimensional) often requires novel ideas. Recent promising advances exist and they will guide the organization of this conference. Moreover, the connection of shape constrained inference to related geometrically motivated statistical approaches, including the investigation of modality and of related ideas from topological data analysis (persistent homology) deserve being explored in more depth. The conference will serve as a catalyzer for further developments in these fields, including novel developments for shape constraint estimation in high-dimensional situations, computational and methodological ideas for log-concave estimation in multivariate settings, distributional results for multivariate order restricted non-parametric maximum likelihood estimators, and more."
"1513492","Collaborative Research:   Principled Science-Driven Methods for Massive, Intricate, and Multifaceted Data in Astronomy and Astrophysics","DMS","STATISTICS","07/01/2015","07/03/2017","Xiao-Li Meng","MA","Harvard University","Continuing Grant","Gabor Szekely","06/30/2018","$87,499.00","","meng@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","","$0.00","Massive new data resources are coming online in every area of human exploration, changing the way researchers approach data analysis. All-purpose algorithms are expected to identify patterns in data with little tailoring to the problem at hand. While this all-purpose approach is understandable given the massive computational challenges of ""big data,"" it often sacrifices our ability to understand underlying scientific processes. On the other hand, explicitly embedding scientific models into massive statistical analyses may pose significant computational challenges. This project navigates the tension between such all-purpose or ""data-driven"" methods and specially-designed or ""science-driven"" methods using a suite of data analytic projects in astro- and solar physics. This work is motivated by recent advances in space-based instrumentation that are increasing both the quality and the quantity of data available to astronomers. Several projects focus on developing new methods for detecting and characterizing astronomical sources, combining information in observations made across the electromagnetic spectrum, including high resolution spectrography, imaging, and time series. Other projects investigate methods for extracting useful features from ultra-high-resolution images of the Sun with the ultimate aim of predicting explosive dynamic processes in the solar atmosphere. The CHASC International Center for Astrostatistics has a track record of designing methods that leverage efficient data-driven techniques but still incorporate scientific understanding of the astronomical sources and maintain the ability to answer specific scientific questions about the underlying astronomical and physical processes. The CHASC Center not only aims to develop new methods for astronomy but also plans to use these problems as springboards in the development of new general statistical methods, especially in signal processing, image analysis, multilevel modeling, and computational statistics.<br/><br/>The CHASC International Center for Astrostatistics plans to tackle these challenges using principled statistical methods that incorporate both data-driven and science-driven approaches.  For example, the investigators will use coarse data-driven models in an initial analysis that aims to identify simple structures that can be used in a more scientifically meaningful secondary analysis. To formally test for unexpected features in astrophysical images, the team will use flexible data-driven models for deviations from science-driven models for known features. As these examples illustrate, modern astrostatistical analyses involve subtle tradeoffs between complexity and practicality and pose significant computational challenges. A primary aim of this project is to produce tailored Monte Carlo methods that are efficient in such complex settings. The team's statisticians (Meng, van Dyk, Lee, and Stein) have substantial research experience in developing the methods that the Center will extend, employ, and publicize to tackle these challenges: inferential and efficient computational methods under highly-structured models that involve multi-scale structure and/or multiple levels of latent variables and incomplete data. Such models are ideally suited to account for the many physical and instrumental filters of the data generation mechanisms in astrophysics. The astronomers (Kashyap and Siemiginowska) have expertise in the instrumentation and science of high-energy and optical astronomy, and have collaborated with statisticians in developing methods to address scientific questions. It is expected that a fundamental impact of this research will be more general acceptance and use of appropriate methods among astronomers. Second, the development of methods for efficient modeling of scientific phenomena, the comparison of complex models, and science-driven classification and clustering will help solve complex data analytic challenges throughout the natural, social, medical, and engineering sciences."
"1507428","10th Conference on Bayesian Nonparametrics","DMS","STATISTICS","05/01/2015","04/22/2015","Subhashis Ghoshal","NC","North Carolina State University","Standard Grant","Gabor Szekely","04/30/2016","$15,000.00","Brian Reich","subhashis_ghoshal@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1269","7556","$0.00","The 10th Conference on Bayesian Nonparametrics is going to be held during June 22-26, in Raleigh, NC. Providing travel support to junior researchers who do not have access to other sources of funding to attend the conference is key to maintaining the current leadership of American institutions in this field. The conference will also provide American researchers opportunity to exchange ideas with leading researchers from elsewhere in the world such as Europe, Asia and Latin America. Many of the invited speakers including a plenary speaker are women. More women and minorities will be highly encouraged to take part in the conference by providing them local support obtained through funding from the National Science Foundation.<br/><br/>Bayesian nonparametrics has evolved as one of the fastest growing areas of research in statistics. Its application areas include genetics, finance, survival analysis, sociology, networks and machine learning. The primary objective of the conference is to bring together experts and young researchers, theoreticians and practitioners, who use Bayesian nonparametric techniques. The conference has a well-structured balanced program covering various areas of the subject. The scientific committee of the conference consists of renowned international experts on Bayesian nonparametrics and related topics. The meeting will include three overview plenary talks, twenty four invited talks, several contributed talks and two large contributed poster sessions. The poster session and some slots for contributed talks are especially reserved for young researchers. In addition, the conference will provide opportunities for young researchers to disseminate widely the results of their work by facilitating the publication of peer-reviewed papers and a proposed special issue of a leading statistics journal.  Funding from the National Science Foundation will support participation of students, post-doctoral scholars and junior faculty members working in U.S. institutions to participate in the conference."
"1511945","Development of New Approaches for Analysis of Markov Chain Monte Carlo Algorithms to Facilitate Principled Use of MCMC in Practice","DMS","STATISTICS","07/01/2015","06/24/2015","James Hobert","FL","University of Florida","Standard Grant","Gabor Szekely","06/30/2019","$199,977.00","Kshitij Khare","jhobert@stat.ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","MPS","1269","","$0.00","Markov Chain Monte Carlo (MCMC) is a probability-based simulation technique that is used to approximate high-dimensional intractable integrals. MCMC has revolutionized scientific computing in the last two decades by enabling the use of intricate statistical models in a vast array of disciplines as diverse as genetics, agricultural science, computer science, physics, and economics. Any new methodology that leads to more effective ways to employ MCMC algorithms has countless potential applications in myriad scientific fields. It is vital for users of MCMC to have principled methods for constructing error bounds for the resulting estimates, and to have theoretical guarantees of convergence for the underlying Markov chains. Unfortunately, such methods and guarantees are currently lacking. This research project aims to address this problem by developing new methodology that will allow for more principled application of MCMC. Because the use of MCMC has become so widespread, there is great potential for the methods developed in this project to contribute to the improvement of society from many different corners of science.<br/><br/>It is typically straightforward to construct an MCMC algorithm for sampling from a given intractable posterior probability distribution. However, a long-standing difficulty with MCMC is in determining how long an algorithm should be run to produce useful results. The principled approaches to making such a determination are all predicated on the asymptotic normality of the MCMC estimators. Unfortunately, establishing the existence of the requisite central limit theorems (CLTs) requires a detailed analysis of the underlying Markov chain. Worse yet, the methods that are currently available for analyzing Monte Carlo Markov chains are extremely difficult to apply in practice. In fact, for the vast majority of MCMC algorithms that are used in practice, it is unknown whether these CLTs exist. This project concerns the development of new techniques for analyzing complex Markov chains, like those that underlie MCMC algorithms, with an eye towards making it easier to establish the existence of CLTs. The are two main ideas that will be pursued: (1) The standard techniques for analyzing Monte Carlo Markov chains were developed using the total variation (TV) metric (between probability distributions), but it is now becoming clear that the Wasserstein metric is actually much more natural than TV for analyzing the types of Markov chains that arise in statistical applications of MCMC. This suggests that going ""back to the drawing board'' with Wasserstein distance in place of TV distance may lead to new methods that are far more useful in practice than those based on TV. (2) Markov chains with countable state spaces are routinely analyzed with great success via spectral techniques, but this approach is not often used for the Markov chains that underlie statistical applications of MCMC (which usually have uncountable state spaces). This is (at least) partly due to a general perception in the MCMC community that very few of the Markov operators associated with practically relevant Monte Carlo Markov chains are compact. However, recent work suggests that many Gibbs samplers and data augmentation (DA) algorithms do, in fact, have compact Markov operators. Furthermore, application of the spectral techniques to these operators is often much simpler than the standard analysis. This calls for the development of new, general, spectral techniques for the analysis of Markov operators associated with Gibbs samplers and DA algorithms."
"1513546","Collaborative Research:   Principled Science-Driven Methods for Massive, Intricate, and Multifaceted Data in Astronomy and Astrophysics","DMS","STATISTICS","07/01/2015","07/26/2018","Nathan Stein","PA","University of Pennsylvania","Continuing Grant","Gabor Szekely","07/31/2016","$24,293.00","","natstein@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","","$0.00","Massive new data resources are coming online in every area of human exploration, changing the way researchers approach data analysis. All-purpose algorithms are expected to identify patterns in data with little tailoring to the problem at hand. While this all-purpose approach is understandable given the massive computational challenges of ""big data,"" it often sacrifices our ability to understand underlying scientific processes. On the other hand, explicitly embedding scientific models into massive statistical analyses may pose significant computational challenges. This project navigates the tension between such all-purpose or ""data-driven"" methods and specially-designed or ""science-driven"" methods using a suite of data analytic projects in astro- and solar physics. This work is motivated by recent advances in space-based instrumentation that are increasing both the quality and the quantity of data available to astronomers. Several projects focus on developing new methods for detecting and characterizing astronomical sources, combining information in observations made across the electromagnetic spectrum, including high resolution spectrography, imaging, and time series. Other projects investigate methods for extracting useful features from ultra-high-resolution images of the Sun with the ultimate aim of predicting explosive dynamic processes in the solar atmosphere. The CHASC International Center for Astrostatistics has a track record of designing methods that leverage efficient data-driven techniques but still incorporate scientific understanding of the astronomical sources and maintain the ability to answer specific scientific questions about the underlying astronomical and physical processes. The CHASC Center not only aims to develop new methods for astronomy but also plans to use these problems as springboards in the development of new general statistical methods, especially in signal processing, image analysis, multilevel modeling, and computational statistics.<br/><br/>The CHASC International Center for Astrostatistics plans to tackle these challenges using principled statistical methods that incorporate both data-driven and science-driven approaches.  For example, the investigators will use coarse data-driven models in an initial analysis that aims to identify simple structures that can be used in a more scientifically meaningful secondary analysis. To formally test for unexpected features in astrophysical images, the team will use flexible data-driven models for deviations from science-driven models for known features. As these examples illustrate, modern astrostatistical analyses involve subtle tradeoffs between complexity and practicality and pose significant computational challenges. A primary aim of this project is to produce tailored Monte Carlo methods that are efficient in such complex settings. The team's statisticians (Meng, van Dyk, Lee, and Stein) have substantial research experience in developing the methods that the Center will extend, employ, and publicize to tackle these challenges: inferential and efficient computational methods under highly-structured models that involve multi-scale structure and/or multiple levels of latent variables and incomplete data. Such models are ideally suited to account for the many physical and instrumental filters of the data generation mechanisms in astrophysics. The astronomers (Kashyap and Siemiginowska) have expertise in the instrumentation and science of high-energy and optical astronomy, and have collaborated with statisticians in developing methods to address scientific questions. It is expected that a fundamental impact of this research will be more general acceptance and use of appropriate methods among astronomers. Second, the development of methods for efficient modeling of scientific phenomena, the comparison of complex models, and science-driven classification and clustering will help solve complex data analytic challenges throughout the natural, social, medical, and engineering sciences."
"1507078","Collaborative Research: Smoothing Spline Semiparametric Density Models","DMS","STATISTICS","08/01/2015","07/30/2015","Anna Liu","MA","University of Massachusetts Amherst","Standard Grant","Gabor Szekely","07/31/2019","$129,265.00","","anna@math.umass.edu","Research Administration Building","Hadley","MA","010359450","4135450698","MPS","1269","","$0.00","A probability density function of multiple variables describes the likelihood of different values the variables can jointly take, therefore, contains full information regarding the distribution of individual variables and their interactions. Given observed data of the random variables, density estimation is at the heart of Statistics and machine learning, where the classical problems such as regression, variable selection, clustering, and dimension reduction, can all be cast into a density estimation problem.  Advanced density estimation methods are therefore essential for the extraction of as much information as possible from the data. There has been lack of systematic research in flexible density estimation with high dimensional data or complex data such as clustered data. The overall goal of this project is to develop a smoothing spline based systematic framework that allows for flexible density model building for complex and high dimensional data. As such data arise from a wide range of applications, the results of this proposed research are useful for researchers from a wide range of fields. In particular, the proposed methods will be applied to analyze data in health and medicine, speech, environmental change, food and computer sciences, in collaboration with researchers in these areas. High-performance computing tools will be developed as a result of this research and made publicly available.<br/><br/>This project adopts a semi-parametric approach that combines advantages of parametric and nonparametric methods. Flexible and general semi-parametric density and conditional density models for independent and clustered data will be developed and studied. Regularization methods for adaptive density estimation, variable selection in high dimensional conditional density estimation and interaction selection in semi-parametric graphical models will be developed. Nonparametric components will be modeled using reproducing kernel Hilbert spaces which can deal with different density models on different domains with different penalties in a unified fashion. The semiparametric density models considered in this project contain most existing semiparametric density models as special cases as well as many new interesting models. Many methods in this project for adaptive estimation, model/variable selection, model diagnostics and inference are new. These novel methodologies constitute advances in density estimation."
"1513657","Spatial-Temporal Modeling and Estimation of Epidemic Diseases and Invasive Plants Using Hawkes Point Processes","DMS","STATISTICS","09/15/2015","07/18/2017","Frederic Schoenberg","CA","University of California-Los Angeles","Continuing Grant","Nandini Kannan","08/31/2018","$180,000.00","Ryan Harrigan","frederic@stat.ucla.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1269","","$0.00","This research will provide new insights into the spread of epidemics and invasive species.  In particular, this project will introduce new, more precise methods of estimating the spatial-temporal spread of epidemics.  The results will also be more robust and less dependent on potentially faulty modeling assumptions compared with those based on currently used epidemiological methods.  As a result, the project will lead to improved, more accurate estimations and forecasts, and a better understanding of the impact of policy decisions related to the spread of epidemics and invasive species.  Such implications are important for preparedness as well as for urban planning, insurance, and public health policy.  The results will be disseminated scientifically, rigorously, and responsibly, reflecting as accurately as possible the true threat presented by infectious or invasive species.  This project is especially timely given the public health threat of recent disease epidemics such as Ebola.<br/><br/>This research project makes use of spatial-temporal Hawkes point process models to characterize the dynamics of both human disease epidemics and invasive species spread.  Hawkes models are currently widely used in seismology to describe earthquake catalogs.  Though these models have outperformed their competitors in earthquake forecasting experiments, and are often called Epidemic-Type Aftershock Sequence (ETAS) models based on the notion that earthquakes spread like epidemics, their use in application to the spread of diseases or invasive species has been sparse.  Instead, epidemiologists have primarily used compartmental SIR models and their variants, which can have serious limitations when used to describe the detailed local behavior of an epidemic and can significantly overpredict counts of infections such as SARS.  Indeed, existing estimates of epidemic spread rates are fundamentally model-dependent, and the addition of seemingly small changes to the models, or their parameters, can result in dramatic differences in local hazard estimates.  This project will use and extend state of the art residual analysis techniques, such as deviance residuals, super-thinned residuals, and Voronoi residuals, in order to assess the goodness-of-fit of existing and proposed models and suggest ways to refine and improve them.  Recently, a modified Hawkes model was fit to sightings of one invasive species of red banana trees spreading in a Costa Rican rainforest, and the results proved useful for estimating immigration and spatial-temporal spread rates, forecasting, and the detailed description of properties of the invasive species.  This type of point process analysis will be extended to the study of human disease epidemics as well as other invasive species.  This project will use recently developed statistical methods for estimating Hawkes point process models and their parameters, including non-parametric techniques for estimating the triggering function without relying on potentially flawed parametric models for the triggering rate, as well as modern integral approximation techniques that substantially add stability and computational efficiency to parameter estimates."
"1505367","Max-Linear Competing Factor Models and Applications","DMS","STATISTICS","09/15/2015","07/09/2017","Zhengjun Zhang","WI","University of Wisconsin-Madison","Continuing Grant","Nandini Kannan","08/31/2018","$150,000.00","","zjz@stat.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","Throughout applications in diverse fields, analysis of extreme risks plays an important scientific and societal role. This importance is exemplified by the 2007-2008 financial crisis, during which the concurrent decline of almost every asset category gave investors few options. These kinds of events can be described by an index termed asymptotic dependence, a probability concept that has been used to depict two risk variables concurrently going wrong. On the other hand, although the extremes of high-frequency financial transaction data have a huge economic impact, the basic structure of the data has been ignored up to now. The primary goal of this research project is to find a statistical method for characterizing such extreme risks. The nonlinear competing factor model and nonlinear time series model under investigation in this project will bridge the gap between theoretical research and practice. The dissemination of new methodologies and statistical tools will lead to a better understanding of concurrent decline and extreme co-movement. <br/><br/>In the multivariate context, it is well-known that nonlinear dependence, asymmetric dependence, and asymptotic dependence co-exist in financial time series, social network studies, climate studies, image processing, and many other application areas. A major goal of this project is to make significant methodological and theoretical contributions to modeling observations simultaneously, while embracing the different variable dependence features. The project consists of two main sub-projects. The first sub-project proposes a max-linear competing factor model that can incorporate the different variable dependence features but still possesses a simple form of factor structure. The second sub-project builds a new family of multivariate time series models (Copula Structured M4 Processes) suitable for multivariate maxima of high frequency intra-day returns. Using these models, the probability of the concurrent decline of assets in a typical stock market will be evaluated. The integration of the two sub-projects provides a comprehensive framework for understanding how variables depend on each other in high dimensional and temporal observational data."
"1513484","Collaborative Research:  Principled Science-Driven Methods for Massive, Intricate, and Multifaceted Data in Astronomy and Astrophysics","DMS","STATISTICS","07/01/2015","07/03/2017","Thomas Chun Man Lee","CA","University of California-Davis","Continuing Grant","Gabor Szekely","06/30/2019","$87,500.00","","tcmlee@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","","$0.00","Massive new data resources are coming online in every area of human exploration, changing the way researchers approach data analysis. All-purpose algorithms are expected to identify patterns in data with little tailoring to the problem at hand. While this all-purpose approach is understandable given the massive computational challenges of ""big data,"" it often sacrifices our ability to understand underlying scientific processes. On the other hand, explicitly embedding scientific models into massive statistical analyses may pose significant computational challenges. This project navigates the tension between such all-purpose or ""data-driven"" methods and specially-designed or ""science-driven"" methods using a suite of data analytic projects in astro- and solar physics. This work is motivated by recent advances in space-based instrumentation that are increasing both the quality and the quantity of data available to astronomers. Several projects focus on developing new methods for detecting and characterizing astronomical sources, combining information in observations made across the electromagnetic spectrum, including high resolution spectrography, imaging, and time series. Other projects investigate methods for extracting useful features from ultra-high-resolution images of the Sun with the ultimate aim of predicting explosive dynamic processes in the solar atmosphere. The CHASC International Center for Astrostatistics has a track record of designing methods that leverage efficient data-driven techniques but still incorporate scientific understanding of the astronomical sources and maintain the ability to answer specific scientific questions about the underlying astronomical and physical processes. The CHASC Center not only aims to develop new methods for astronomy but also plans to use these problems as springboards in the development of new general statistical methods, especially in signal processing, image analysis, multilevel modeling, and computational statistics.<br/><br/>The CHASC International Center for Astrostatistics plans to tackle these challenges using principled statistical methods that incorporate both data-driven and science-driven approaches.  For example, the investigators will use coarse data-driven models in an initial analysis that aims to identify simple structures that can be used in a more scientifically meaningful secondary analysis. To formally test for unexpected features in astrophysical images, the team will use flexible data-driven models for deviations from science-driven models for known features. As these examples illustrate, modern astrostatistical analyses involve subtle tradeoffs between complexity and practicality and pose significant computational challenges. A primary aim of this project is to produce tailored Monte Carlo methods that are efficient in such complex settings. The team's statisticians (Meng, van Dyk, Lee, and Stein) have substantial research experience in developing the methods that the Center will extend, employ, and publicize to tackle these challenges: inferential and efficient computational methods under highly-structured models that involve multi-scale structure and/or multiple levels of latent variables and incomplete data. Such models are ideally suited to account for the many physical and instrumental filters of the data generation mechanisms in astrophysics. The astronomers (Kashyap and Siemiginowska) have expertise in the instrumentation and science of high-energy and optical astronomy, and have collaborated with statisticians in developing methods to address scientific questions. It is expected that a fundamental impact of this research will be more general acceptance and use of appropriate methods among astronomers. Second, the development of methods for efficient modeling of scientific phenomena, the comparison of complex models, and science-driven classification and clustering will help solve complex data analytic challenges throughout the natural, social, medical, and engineering sciences."
"1512084","Valid Inference when Analytical Models are Approximations","DMS","STATISTICS","07/15/2015","08/20/2018","Linda Zhao","PA","University of Pennsylvania","Standard Grant","Gabor Szekely","06/30/2019","$531,966.00","Linda Zhao","lzhao@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","","$0.00","Statistical inferential methods are used to answer questions throughout modern life.  For example, what affects the crime rate in a town; which factors are important influences on the housing market; which genes are associated to a certain disease; what are the most important elements to control in order to mitigate climate change?  Statistical methods are used to address questions such as these. However, often the statistical data structure and the mathematical model developed for the analysis do not agree. This project arises from a broadly based statistical concern about the mismatch between standard inferential analyses and the statistics of the world they are trying to describe. This research draws a distinction between the statistical models that conventionally describe the correlational relations in experimental and observational data and the inferential models that are used in their analysis. To this end, the project investigates a paradigm in which sampling models are meant to be faithful representations of the real-world structure of the data they are describing.  At the same time, the analytical models to be applied to the data are viewed only as approximate descriptions of that reality.  The statistical-sampling representations need not match the analytical models, though the two should harmonize in certain important respects. There is a significant disparity between accurate characterization and what is claimed by classical procedures that ignore this distinction. The distinction has been noted by many previous statistical researchers, and various partially adequate approaches have been suggested. Nevertheless, clarifying this distinction in the directions under study and then pursuing the consequences leads to a theory of inference somewhat different from that in common use for relational and observational data. Acknowledging and properly accommodating this duality then leads to new methodology for some important statistical problems. One such new methodology is within the setting of randomized clinical trials in which one wishes to estimate the effect of certain treatment(s) relative to others or to placebo controls. Another is within the setting of semi-supervised learning that occurs in various big-data contexts. <br/><br/>The core of the current research is designed for linear analytical models. These involve observations on a vector of explanatory covariates (X-variables) and a numerical dependent variable (Y). The analytical model constructs the best linear approximant of Y as a linear function of the X variables. Virtually no assumptions are made about the (X,Y) pairs in the sample, other than that they form a statistical sample drawn from some unknown joint distribution of (X,Y) pairs and possess desired low-order moments. The notion of ""best"" is defined in a statistically natural fashion related to minimizing squared prediction error. It follows that the ordinary least squares estimators of parameters still have desirable asymptotic properties. Inference about their (asymptotic) performance can be derived via the standard sandwich estimator. However, a newly derived iterated pairs-bootstrap is shown to give substantially more accurate inferential information for realistic sample sizes. If more information is available about the distribution of X (such as knowledge of its mean and variance) then the usual least-squares solutions can be improved. This observation leads via an indirect path to suggestions that improve the standard methodology for estimating average treatment effect in randomized clinical trials and for producing linear predictions of numerical outcomes in settings of semi-supervised learning. Various additional issues are exposed in the course of the above developments. We also plan to investigate generalizations of the above setting -- for example to models having categorical Y-variables (classification) and to other generalized-linear analytical models. Our earlier research involved post-selection inference in the classical setting in which models for the data and its analysis coincide, and we now intend to pursue analogous issues in the current context in which they do not."
"1512945","Collaborative Research: Generalized Fiducial Inference for Massive Data and High Dimensional Problems","DMS","STATISTICS","09/01/2015","08/16/2017","Thomas Chun Man Lee","CA","University of California-Davis","Continuing Grant","Gabor Szekely","08/31/2019","$150,000.00","","tcmlee@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1269","","$0.00","R. A. Fisher, the father of modern statistics, proposed the idea of Fiducial Inference in the 1930s. While his proposal led to some interesting methods for quantifying uncertainty, other prominent statisticians of the time did not accept Fisher's approach because it went against the ideas of statistical inference of the time. Beginning around the year 2000, the PIs and collaborators started to re-investigate the ideas of fiducial inference and discovered that Fisher's approach, when properly generalized, would open doors to solve many important and difficult problems of uncertainty quantification. The PIs termed their generalization of Fisher's ideas as generalized fiducial inference. After many years of preliminary investigations, the PIs developed a coherent, well thought out plan for a systematic research program in this area. A large part of this project develops practical solutions for different modeling problems that have natural applications in diverse fields. Finance (volatility estimation) and measurement science (calibration of measurements from different government labs, for example, US NIST) are two primary examples, while others include gene expression data, climate problems, recommender systems, and computer vision. <br/><br/>This project is motivated by the success of generalized fiducial inference (GFI) as introduced by the PIs as a generalization of Fisher's fiducial argument. The PIs are now working towards scaling up their GFI methodology to handle big data problems and other difficult problems that have emerged due to our ability to collect massive amounts of data rapidly. In particular the PIs plan to conduct research into the following topics: (i) a thorough investigation of fundamental issues of GFI including connection with Approximate Bayesian Calculations and higher order asymptotics; (ii) sparse covariance estimation using GFI in the ""large p small n"" context; (iii) development of the idea of Fiducial Selector so that a sparsity of the fiducial distribution is induced as a natural outcome of a minimization problem; (iv) uncertainty quantification for the matrix completion problem using GFI, and (v) applications of GFI to a wide variety of practical problems, such as volatility estimation in finance and international key comparison experiments in measurement science."
"1510172","High-dimensional M-estimation: Understanding risk, improving performance and assessing resampling","DMS","STATISTICS","08/01/2015","09/30/2019","Noureddine El Karoui","CA","University of California-Berkeley","Continuing Grant","Gabor Szekely","10/31/2019","$394,178.00","","nkaroui@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","","$0.00","The nature of datasets that scientists in academia or industry are currently working with is changing at a very rapid pace. The data is more complex, larger and higher-dimensional than it has ever been before. A lot of the methods used in practice are based on the idea that they are somehow optimal, in terms of measurement accuracy or prediction of future outcomes, at least for certain models of data generating mechanism. A most widely applied and time-honored principle of data analysis is the use of so-called ""maximum likelihood methods"". It has recently been discovered by the PI and collaborators that in a setting often encountered with modern and large datasets, these maximum likelihood methods are suboptimal and can be improved upon. This is true even for an extremely basic and widely used technique (e.g.,linear regression). One of the aims of the project is to understand if the same phenomena occur for other methods that are widely used in machine/statistical learning practice and in turn develop better tools for data scientists and data analysts. Currently, accuracy assessment for these estimators are often performed through data driven procedures (such as the bootstrap). Another aim of the project is to understand if the corresponding accuracy assessment are misleading for datasets with many predictors. If that is the case, the PI is planning to work on methods to correct the existing procedures so they yield trustworthy accuracy assessments. <br/><br/>High-dimensional statistics offers a profound challenge to classical statistics, both on the applied and the theoretical end. A broad class of methods used in practice is based on solving nontrivial optimization problems to estimate parameters of interest. This yields a so-called M-estimator. When the dimension of this estimator is small compared to the number of observations the practitioner has, standard empirical process techniques can be applied to understand the statistical properties of those estimators. In the setting the PI considers, these techniques fail and new techniques need to be developed. The PI plans on using a mix of tools inspired from random matrix theory, convex analysis and concentration of measure results to study those estimators. The development of new optimal methods is expected - based on using tools from convex analysis. Another exciting research line is that the techniques developed by the PI should allow us to study resampling methods in high-dimension (such as the bootstrap). Those are widely used to assess statistical significance from the observed dataset, without having to appeal to theoretical arguments. While the low-dimensional theory is well-established and relatively easy, and suggests that these numerical methods should work well, the high-dimensional case has yet to be understood. The PI plans on studying these problems thoroughly and propose practically relevant solutions if these widely used-in-practice methods are shown to provide statistically misleading accuracy assessments."
"1501724","The SRCOS Summer Research Conference in Statistics and Biostatistics","DMS","STATISTICS","06/01/2015","03/25/2015","Jane Harvill","TX","Baylor University","Standard Grant","Gabor Szekely","05/31/2016","$15,000.00","","Jane_Harvill@baylor.edu","One Bear Place #97360","Waco","TX","767987360","2547103817","MPS","1269","7556","$0.00","This award supports the participation of graduate students in the Summer Research Conference in Statistics and Biostatistics, held in Carolina Beach, North Carolina on June 7-10, 2015.  The meeting is the 51st in a series of annual conferences sponsored by the Southern Regional Council on Statistics (SRCOS). Its purpose is to encourage the exchange of current research ideas in statistics and biostatistics and to give motivation and direction to further research progress. The conference series focuses on young researchers, providing excellent opportunities for interaction among junior researchers and leaders in the field.  The meeting features talks on current research topics together with dedicated time for clarification, amplification, and further informal discussions in small groups. Under the travel support provided by this award, students will attend and present their research in posters to be reviewed by more experienced researchers.<br/><br/>The Southern Regional Council on Statistics is a consortium of statistics and biostatistics programs in the South, stretching as far west as Texas and as far north as Maryland. It currently has member programs at 45 universities in 15 states in the region. The 2015 Summer Research Conference in Statistics and Biostatistics sponsored by SRCOS is particularly valuable for students and faculty from smaller regional schools within driving distance, affording them the opportunity to participate and interact closely with internationally-known leaders in the field without the cost of travel to distant national or international venues. It will strengthen the research of the statistics and biostatistics community as a whole, and particularly in the relatively underdeveloped southern region. More details are available at the conference web site http://www.uncw.edu/src2015/."
"1540663","Participant Support for Attendants to the 11th International Conference on Objective Bayes Methodology","DMS","STATISTICS","06/15/2015","06/15/2015","Edward George","PA","University of Pennsylvania","Standard Grant","Gabor Szekely","05/31/2016","$15,000.00","","edgeorge@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","7556","$0.00","The International Workshop on Objective Bayes Methodology, O-Bayes15, will be held in Valencia, Spain, June 1-4, 2015.  This will be the 11th meeting of one of the longest running and preeminent meetings in Bayesian statistics, following earlier meetings held in West Lafayette, IN, USA, 1996; Valencia, Spain, 1998; Ixtapa, Mexico, 2000; Granada, Spain, 2002; Aussois, France, 2003; Branson, MO, USA, 2005; Roma, Italy, 2007; Philadelphia, PA, USA, 2009; Shanghai, China, 2011; and Durham, NC, USA 2013.  The principal objectives of O-Bayes15 will be to facilitate the exchange of recent research developments in objective Bayes theory and methodology, and related topics, to provide opportunities for new researchers, and to establish new collaborations that will channel efforts into pending problems and open new directions for investigation.   O-Bayes15 will feature 21 invited talks and discussions, 3 tutorials and a poster session, all centering on major recent developments in objective Bayesian theory, methodology and application.   The organization of the meeting is designed to facilitate interactions, with at most 7 talks per day and extensive discussion built in.  The tutorials and evening poster sessions enhance the conference experience for graduate students and new researchers.  Funding primarily supports graduate students and new researchers.<br/><br/>O-Bayes15 will bring together leading researchers in Bayesian analysis from around the world. The last decade has seen an explosion of interest in Bayesian statistical methodology across engineering, medicine and science, and this meeting provides a timely forum for the exchange of recent research developments, provides opportunities for new researchers and underrepresented groups, and establishes new collaborations that will channel efforts into pending problems and open new directions for investigation.  In particular, the past two years have seen remarkable advances in the subjects of the conference; an event to disseminate these advances and spark new developments and collaborations is thus very timely.   The scientific topics that will be considered include foundations of objective Bayesian analysis; objective priors in high dimensions; model uncertainty with huge model spaces; nonparametric analysis; massive multiple testing and subgroup analysis; and objective Bayesian methods for the interface of statistics and computational modeling of processes."
"1507620","Collaborative Research: Smoothing Spline Semiparametric Density Models","DMS","STATISTICS","08/01/2015","07/30/2015","Yuedong Wang","CA","University of California-Santa Barbara","Standard Grant","Gabor Szekely","01/31/2019","$228,108.00","","yuedong@pstat.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","MPS","1269","","$0.00","A probability density function of multiple variables describes the likelihood of different values the variables can jointly take, therefore, contains full information regarding the distribution of individual variables and their interactions. Given observed data of the random variables, density estimation is at the heart of Statistics and machine learning, where the classical problems such as regression, variable selection, clustering, and dimension reduction, can all be cast into a density estimation problem.  Advanced density estimation methods are therefore essential for the extraction of as much information as possible from the data. There has been lack of systematic research in flexible density estimation with high dimensional data or complex data such as clustered data. The overall goal of this project is to develop a smoothing spline based systematic framework that allows for flexible density model building for complex and high dimensional data. As such data arise from a wide range of applications, the results of this proposed research are useful for researchers from a wide range of fields. In particular, the proposed methods will be applied to analyze data in health and medicine, speech, environmental change, food and computer sciences, in collaboration with researchers in these areas. High-performance computing tools will be developed as a result of this research and made publicly available.<br/><br/>This project adopts a semi-parametric approach that combines advantages of parametric and nonparametric methods. Flexible and general semi-parametric density and conditional density models for independent and clustered data will be developed and studied. Regularization methods for adaptive density estimation, variable selection in high dimensional conditional density estimation and interaction selection in semi-parametric graphical models will be developed. Nonparametric components will be modeled using reproducing kernel Hilbert spaces which can deal with different density models on different domains with different penalties in a unified fashion. The semiparametric density models considered in this project contain most existing semiparametric density models as special cases as well as many new interesting models. Many methods in this project for adaptive estimation, model/variable selection, model diagnostics and inference are new. These novel methodologies constitute advances in density estimation."

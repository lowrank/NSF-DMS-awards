"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"9402904","Mathematical Sciences:  Optimal Inference in Non-Linear     Regression Models with Long Range Dependent Errors and in   Non-Linear Time Series","DMS","STATISTICS","06/15/1994","06/02/1994","Hira Koul","MI","Michigan State University","Standard Grant","James E. Gentle","05/31/1998","$105,000.00","","koul@stt.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","0000, OTHR","$0.00","A discrete time stationary stochastic process is said to be long  range dependent if its correlations decrease to zero like a power  of the lag, as the lag tends to infinity, but their sum diverges.  Part I of the proposal proposes to investigate the large sample  behavior of three classes of robust estimators and to develop  asymptotically efficient and adaptive estimators in non-linear  regression models when the errors are long range dependent with  possibly unknown joint distributions. The Part II of the proposal  is concerned with studying the asymptotic behavior of several  classes of robust estimators and devloping asymptoticlaly optimal  inference procedures in non-linear time series in the presence of  regression component, in a semi-parametric setup. In particular,  the P.I. plans to develop asymptotically efficient and adaptive  estimators in random coefficient and threshold autoregression  models when there may be a regression variable present in these  models, in a llinear or non-linear fashion, and when the  distributions of the random coefficient and the error variable  are unknown. The optimal estimators would be developed a l  a Hajek - Le Cam theory.   A data set where an association between distant observations is  slowly decaying but persistent, as distance between observations  increases, is called long range dependent. Such data arise often  in astronomy, economics, geophysics, hydrology, meteorology, and  many other disciplines. An example worth mentioning is the data  of 289 high-precision measurements on the 1-kg check standard  weight made between 1963 to 1975 by the U.S. National Bureau of  Standards. In spite of ideal conditions for preserving  independence, the observations turned out to be long range  dependent. The first part of the proposal is concerned with  developing some optimal statistical procedures for analyzing the  long range dependent data in the presence of a covariate. The  second part of the proposal is concerned with developing  efficient inferential  procedures in some complicated time series  models that often arise in econometrics. It is anticipated that  these procedures will be broadly applicable and not very  sensitive to model departures."
"9319948","Mathematical Sciences:  Seventeenth International Biometric Conference - August 8-12, 1994","DMS","STATISTICS","06/01/1994","06/15/1994","Lynne Billard","GA","University of Georgia Research Foundation Inc","Standard Grant","Sallie Keller-McNulty","04/30/1995","$35,000.00","","lynne@stat.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","0000, 9183, BIOT, OTHR","$0.00","This proposal requests funds to support travel expenses of approximately 25 U.S. scientists, approximately ten participants who have an official capacity such as organizers and invited speakers and approximately fifteen young investigators (including women and minorities), to attend the 17th International Biometric Conference. The meeting will be held August 8-12, 1994 in Hamilton, Ontario. The strength of the meetings lies in the broad international participation of statisticians, biologists and mathematicians interested in quantitative and statistical aspects of biology, genetics, epidemiology, pharmacology, medicine, public health and other fields. Topics to be discussed on statistical topics directly related to these issues include quality management, mixture distributions, neural networks, genetics, repeated measures and longitudinal data analyses, spatial models and climatic change in fisheries, bioassay calibration, epidemics, DNA and protein sequences, Gibbs sampling and resampling computer intensive methods, spatial and temporal environmental modelling, frailty models, cancer models and ecological impact analysis; all of which have direct bearing on solutions to the major concerns of today including ecology and the environment, AIDS, cancer and heart disease, plant science epidemiology, agriculture, forestry, etc. In addition, discussions on fundamental statistical issues of repeated measures, structural inference, image processing and causal inference all have direct application to current health related and agricultural sciences. It is anticipated that the Conference, which is the major international meeting of biostatisticians and biometricians throughout the world will be attended by 500 or more scientists from the international community. This proposal requests funds to support travel expenses of approximately 25 U.S. scientists consisting of approximately ten participants who have an official capacity such as organizers and invited speakers and approximately fifteen young investig ators (including women and minorities), to attend the 17th International Biometric Conference to be held August 8-12, 1994 in Hamilton, Ontario. The strength of the meetings lies in the broad international participation of statisticians, biologists and mathematicians interested in cutting edge research and the application of quantitative and statistical methods to of biology, genetics, epidemiology, pharmacology, medicine, public health and other fields. It is anticipated that the Conference, which is the major international meeting of biostatisticians and biometricians throughout the world will be attended by 500 or more scientists from the international community."
"9404585","Statistics of Estimated Rotations","DMS","STATISTICS","07/15/1994","05/06/1996","Theodore Chang","VA","University of Virginia Main Campus","Continuing Grant","James E. Gentle","06/30/1998","$60,000.00","","tcc8v@virginia.edu","1001 EMMET ST N","CHARLOTTESVILLE","VA","229034833","4349244270","MPS","1269","9218, HPCC","$0.00","This project studies the statistics of estimated rotations with  especial concentration on problems which arise in the  reconstructions of tectonic plates.  We propose to adapt our  previous work on M-estimators for spherical regressions to the  types of data which arise in such reconstructions.  We also  propose to develop methodology for a statistical test of the  fixed hot spot hypothesis.  One component of this problem will be  to develop methodology for use with contoured data.  We propose  to continue our work on splining together fitted rotations and  developing a confidence band for the fitted path.  Finally, we  propose to continue our work on Behrens Fisher type problems  which arise when data of different types are used to calculate a  reconstruction.  This project is to study the statistical  properties of estimated rotations.  Such problems arise in the  statistical estimation of the motion of rigid bodies on the  sphere and in Euclidean space or in the statistical estimation of  an unknown coordinate system.  Of these potential applications,  the most scientifically compelling application of this line of  research is the statistical determination of the errors in  tectonic plate reconstructions.  The proposal will concentrate on  studying the types of problems which arise in the tectonic  context."
"9410138","Mathematical Sciences:  Foundations and Methods of Inference","DMS","STATISTICS","08/01/1994","02/23/1996","M. Halloran","GA","Emory University","Standard Grant","James E. Gentle","07/31/1996","$30,000.00","","","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1269","0000, 9222, OTHR","$0.00","9410138  Halloran       This career advancement award provides support for research  in the areas of mathematical statistics and bio-statistics,  concentrating on the understanding and development of statistical  inference with particular applications to the analysis of  diseases.   Particular emphasis is on decision or inference, on testing or  estimation, the use of prior information, location and nature of  inference and sensitivity of sampling procedure.  Development of  skill involving statistical computing will play a major role in  this activity.  Methods such as bootstrap, exploration of  posterior distributions and likelihood functions, Markov Chain  Monte Carlo algorithms will be explored.  Some of these studies  will take place at the University of Minnesota where methods and  applications of Bayes and empirical Bayesian methods of inference  are being developed.       Statistical methods in the analysis of causal inference,  especially in studies of spread of infectious diseases and  effectiveness of vaccines is an important and highly developed  component of the mathematical sciences.  This project will  continue this effort in the study of disease transmission and  designs for vaccine evaluation.  ***"
"9402734","Mathematical Sciences:  3D Scanning:  From Physical Objects to Electronic Models","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS, INTEGRATION ENGINEERING, COMPUTER SYSTEMS ARCHITECTURE","08/01/1994","06/17/1996","Werner Stuetzle","WA","University of Washington","Continuing Grant","James E. Gentle","07/31/1998","$290,024.00","John McDonald, Thomas Duchamp, Anthony DeRose","wxs@stat.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269, 1271, 1463, 4715","0000, 9139, 9218, 9263, HPCC, OTHR","$0.00","Stuetzle   9402734    The goal of this project is to develop methods that will enable rapid, automatic, and inexpensive ""3D scanning""; that is, the use of optical scanning technology to create electronic models from physical objects such as a clay model of a car, or a turbine blade. 3D scanning is at present an important technology, used in a number of critical US industries, such as automobile and aerospace production. However, the expense of current scanner hardware and the need for extensive human intervention has prevented 3D scanning from achieving its full potential. The methods developed by our interdisciplinary research team will make 3D scanning far less expensive, faster, and more automatic. The goal of 3D scanning is the inverse of computer aided manufacturing: given a physical object, such as a clay model of a car, a turbine blade, a chair, or a house, create an electronic model that captures its shape and color. 3D scanning is at present an important technology in a number of critical US industries, such as aerospace and automotive production. However, the expense of current scanner hardware and the need for extensive human intervention in both data collection and modeling has prevented 3D scanning from achieving its full potential. This grant will result in new algorithms and software that will enable new scanner systems that are faster, more automatic, smaller, less expensive, model other visual properties in addition to shape, and produce higher quality and more usable output."
"9403371","Mathematical Sciences:  Multivariate Polynomial Splines     for Function Estimation","DMS","STATISTICS","06/15/1994","06/07/1994","Charles Kooperberg","WA","University of Washington","Standard Grant","James E. Gentle","05/31/1998","$60,000.00","","clk@fhcrc.org","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, OTHR","$0.00"," There are numerous situations in which observed data is generated by some (unknown) mechanism, where interest lies in estimating a function that is related to a model for the data. Using polynomial splines, an unknown function is modeled to be in a linear space. An algorithm employing stepwise addition and stepwise deletion of basis functions makes it possible to determine this space adaptively. The coefficients of the basis functions are estimated by the maximum likelihood method. The problems that are discussed in this proposal are hazard regression, including situations in which there are time-dependent covariates or interval censored data; bivariate survival estimation; bivariate survival regression; modeling of aftershocks of a major earthquake; polychotomous regression and classification; rotation invariant regression and density estimation. The primary aim of this project is to develop methodologies for the estimation of unknown functions using polynomial splines in a variety of problems. Polynomial splines are building stones that can be used to model functions without making assumptions of their form. The problems that are discussed in this proposal are: survival analysis, including situations in which there are time-dependent explanatory variables, situations in which the survival times are only known to be in an interval and situations in which more than one survival time is measured for each unit; modeling of aftershocks of a major earthquake; classification of observations based upon explanatory variables; rotation invariant regression and density estimation. For each of the problems that we propose, there are vast amounts of data whose improved analysis would be of much scientific interest."
"9402398","Algebraic Methods in Multivariate Statistical Analysis","DMS","STATISTICS","06/15/1994","06/27/1994","Michael Perlman","WA","University of Washington","Standard Grant","James E. Gentle","05/31/1997","$60,000.00","","michael@ms.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","0000, OTHR","$0.00","  It is proposed to continue the study of the statistical  properties of multivariate normal models determined by lattice  conditional independence (LCI) assumptions and/or group symmetry  assumptions on the covariance matrix, augmented by compatible  linear restrictions on the mean. LCI models have been shown to be  applicable to the analysis of multivariate normal data sets with  nonnested missing data patterns. A new application of LCI models  will be emphasized here: their application to the analysis of a  collection of nonnested dependent linear regression models, known  in econometrics as a seemingly unrelated regression (SUR) model.  A SUR model may be thought of as a finite nonnested collection of  linear regression subspaces with correlated errors across  regressions. For a given SUR model, the least restrictive LCI  covariance model compatible with the mean structure can be  determined, leading to explicit maximum likelihood estimates for  the SUR model. To date, LCI models have been studied only for  multivariate normal distributions. Another new aspect of this  proposal is the application of LCI models to categorical data in  multiway contingency tables. As in the case of normal data, such  LCI models should allow explicit maximum likelihood estimates for  contingency tables with nonnested missing categories.   Many familiar statistical models occurring in classical  multivariate analysis (the study of correlated data) can be  viewed as special cases of models defined in terms of natural  algebraic conditions on the means and/or covariances. This  viewpoint will (a) lead to a unified and explicit (non-iterative)  analysis of these models, and (b) expand the scope of  multivariate analysis by allowing the application of classical  methods to many new models, as well as allowing the possibilities  of missing data occurring in nonnested patterns and of nonnested  regression subspaces."
"9408837","Missing Data Methods for Non-Random Attrition in            Longitudinal Studies","DMS","STATISTICS","07/01/1994","03/27/1996","Roderick J.A. Little","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","James E. Gentle","06/30/1997","$142,156.00","","rlittle@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","0000, OTHR","$0.00","This project will develop new methods for multivariate  longitudinal data with nonrandomly missing data such as arise  with attrition from the sample. Maximum likelihood and Bayesian  inference will be derived for a variety of pattern- mixture  models, which stratify the data by the pattern of missing data  and identify parameters by exploiting assumptions about the  missing-data mechanism. Methods will be developed for general  patterns of missing data, normal repeated measures models,  multivariate t models that accommodate outlying values, and data  involving mixtures of continuous and categorical variables. The  methods will be compared with existing methods, including those  based on stochastic censoring models. Tests for the type of  missing-data mechanism will also be developed.   Many important scientific studies involve repeated measures of  subjects over time. A common problem in analyzing such studies is  that some subjects are missing some of their measurements,  because they miss visits or do not stay in the study to the end.  Discarding such incomplete cases is wasteful, and can lead to  erroneous conclusions when the cases that are completely observed  differ systematically from those that are incomplete. This grant  studies methods of analyses that include all the data and  incorporate a variety of reasons for why values are missing."
"9404300","Design & Analysis of Experiments, and Inference in Reliability Studies","DMS","STATISTICS","07/01/1994","05/06/1996","C. F. Jeff Wu","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","James E. Gentle","06/30/1998","$279,000.00","Vijayan Nair","jeffwu@isye.gatech.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","9146, MANU","$0.00","The first part of the proposal (sections 1-6) is concerned with  several important aspects of the design and analysis of  experiments. In section 1 we study a fundamental problem on the  maximum number of factors an orthogonal array can accommodate. In  section 2 we examine three problems of design selection: (i)  determination and construction of mixed 2- and 4-level designs  with minimum aberration, (ii) study of a new design criterion,  (iii) a theory for choosing ``optimum'' blocking schemes for full  and fractional factorial designs. In section 3 we propose some  optimal design algorithms for constructing efficient  supersaturated designs. In section 4 we study analysis strategies  that allow us to entertain and estimate interactions from designs  with complex aliasing. In section 5, we study the properties of  M-estimators in the context of factorial experiments. Section 6  deals with joint modeling and efficient estimation of location  and dispersion effects in robust parameter design. The second  part of the proposal is concerned with estimation problems that  arise in some reliability studies. Section 7.1 considers  situations where there are size-biases inherent in the analysis  of ``discovery'' or observational data. Section 7.2 deals with  inference in regression models with both time and covariate  censoring. Finally, in Section 7.3 we consider reliability  inference under an acceleration transform model, allowing the  study of non-linear transformations.   An important method of scientific inquiry is to run experiments  by purposely changing the conditions of the variables of  interest. It has been successfully employed in many  industrial,and agricultural investigation.   Two major issues in experimental investigation are how to  properly choose the experimental conditions(i.e., design), and to  analyze the experimental data(i.e., analysis).   Our proposed research strives to find novel designs that achieve  higher efficiencies and economy, and wider applications, and to  develop  novel analysis strategies that can extract more  information from the data for better decision making.   The second part of the proposal is on product reliability. We  propose to develop methods of statistical inference for a wider  class of models that can better describe the actual reliability  of industrial and commercial products. These statistical methods  can also be used to study other interesting problems like medical  follow-up, software reliability and depletion of petroleum  reserves."
"9403847","Mathematical Sciences:  Parametric and Nonparametric        Likelihood Studies","DMS","STATISTICS","07/01/1994","05/17/1996","Bruce Lindsay","PA","Pennsylvania State Univ University Park","Continuing Grant","James E. Gentle","06/30/1998","$156,000.00","","bgl@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","0000, OTHR","$0.00"," The proposed research is directed towards a number of topics in the area of likelihood based statistical methods. One set of topics will involve the development of several new asymptotic methods in the area of parametric models, focussing on improved distributional properties in the presence of nuisance parameters and on testing methods for the number of components in a mixture. The second set of topics revolves around semiparametric and nonparametric inference in the mixture model, where the class of models needs to be expanded and further inferential methods need to be developed.  The most widely used method for drawing statistical conclusions, called the method of maximum likelihood, is based on a mathematical version of the intuitive notion that the true value of a parameter will generally be found among those that seem ""most likely"" given the observed data. This powerful tool enables one to write down models for complex phenomenon and, by maximizing the likelihood, draw conclusions about the state of nature that generated the data. However, the likelihood method is not perfect, and the objective of this research proposal is to further its development, both in computation and in theory, in several areas of wide application."
"9401191","Mathematical Sciences:  Some Bayesian Problems in Sample    Survey","DMS","STATISTICS","07/01/1994","03/22/1996","Glen Meeden","MN","University of Minnesota-Twin Cities","Continuing Grant","James E. Gentle","09/30/1997","$60,000.00","","glen@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","0000, OTHR","$0.00"," This proposal will consider the application of the Bayesian methodology to some problems in finite population sampling. Over the past several years the principle investigator has helped to develop a noninformative Bayesian approach to some problems in finite population sampling. It is based on the `Polya posterior' and is appropriate when one's prior beliefs about the units are roughly exchangeable. The first major goal of this project is to extend the techniques underlying the `Polya posterior' to the area of cluster sampling. This promises to yield a flexible and unified theory for an important class of problems. Recent developments in Markov chain Monte Carlo technology mean that Bayesian models which until recently have been computationally intractable can now be studied. The second major goal of this project is to use these methods to develop Bayesian models for survey sampling when one's prior beliefs about the units are no longer exchangeable. In one class of models to be considered the units will be assumed to be partially exchangeable, while in another class the populations will be assumed to be generated by a generalized urn processes. This should yield a collection of models that extends the type of prior information that can be used in an analysis in sample survey. A standard setting in survey sampling or finite population sampling assumes each member of some population possesses an unknown amount of a characteristic of interest. The statistician is interested in estimating a quantity related to this characteristic of interest. For example the characteristic of interest could be household income, the population all households in a given city and the quantity of interest the average or median household income for the city. The statistician will then use the values of the characteristic in a sample drawn from the population to estimate this quantity. The major goal of this project is to study how prior information about the population can be used to construct sensible estimates about  the quantity of interest. It will use recent developments in computer simulation to investigate various models for prior information which up until now have been impossible to study."
"9400476","Mathematical Sciences:  Investigations in Order Restricted  Inference and Improved Inference Procedures","DMS","STATISTICS","07/01/1994","03/28/1996","Arthur Cohen","NJ","Rutgers University New Brunswick","Continuing Grant","James E. Gentle","12/31/1997","$216,000.00","William Strawderman, Harold Sackrowitz","artcohen@rci.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","0000, OTHR","$0.00","9400476   Cohen        Inference in order restricted models and improved inferences  procedures will be pursued. A summary is as follows: We will  pursue the study of ""one sided"" confidence regions and  simultaneous confidence intervals for parameters that lie in a  subset of k dimensional space. This problem is meaningful to the  statistical practitioner. We will introduce and study the notion  of cone order association. Ordinary association is linked to the  cone which is the first quadrant of k dimensional space.  Important applications exist where ordinary association is not  good enough to obtain meaningful results but cone order  association would help. We study 2 and 3 dimensional contingency  tables and seek optimal tests for a wide variety of hypothesis  testing problems. A list of problems concerning a wide variety of  extensions of results in order restricted testing problems is  also discussed. Professor Strawderman will study problems related  to hierarchical Bayes models, adaptive minimax estimators,  estimators which improve on truncated estimators such as the  positive-part James-Stein estimator or the MLE of a positive  normal mean, and minimax estimation for spherically symmetric  distributions. He is also writing a monograph on multiparameter  estimation with James Berger.         This proposal is concerned with improved statistical  inference methodology. Statistical inference typically is  concerned with estimating unknown characteristics of populations  or testing hypotheses about these unknown characteristics.  Statistical methodology has progressed greatly over the past 60  years. Yet there is considerable room for improving procedures  that will be more efficient and provide substantial savings to  users of the improved procedures. This proposal is primarily  devoted to developing such new and better procedures."
"9401296","Mathematical Sciences:  Statistical Theory for              Classification","DMS","STATISTICS","07/01/1994","04/11/1996","John Hartigan","CT","Yale University","Continuing Grant","James E. Gentle","06/30/1997","$204,000.00","","hartigan@stat.yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","0000, OTHR","$0.00"," The proposal develops classification based approaches to prediction and statistical inference. Examples of statistical problems considered are change point problems, histograms, mode identification, reticulate evolution, and quantization of probability ditributions. A particular classification problem studied is the problem of types, the family of classes that most succinctly describes a primitive given family of classes.  One way of predicting the future is by recognising circumstances that are similar to circumstances already experienced, that is, by placing the present circumstances in some established class of circumstances, from which we can infer other aspects of th future. Our method of classifying experience will determine how we predict the future. Thus we need to study classification to study prediction. We can't simply seize on some scheme of classification and use it permanently; we must develop new classifications adapting to the success or failure of predictions. Thus classification is needed for practical probability judgments, and practical probability judgments are needed for classification."
"9404180","Mathematical Sciences:  Asymptotic Problems in Statistics   and Econometrics","DMS","STATISTICS, Economics","07/15/1994","06/05/1996","David Pollard","CT","Yale University","Continuing Grant","James E. Gentle","06/30/1997","$141,000.00","","david.pollard@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269, 1320","0000, OTHR","$0.00"," Under traditional assumptions, estimators converge at root n rates and have asymptotic normal distributions when suitably standardized. The project will study problems where such asymptotics fail, or where the traditional smoothness-based methods of analysis are unable to handle unusual features of particular problems. Work has already begun on examples where the failure is due to a near lack of identifiability, or to the effects of boundaries of parameter spaces, or to the effects of infinite dimensional nuisance parameters. Mixture problems, in which all these difficulties arise naturally, will be the motivating examples against which the new methods will be tested.  The project will concentrate on asymptotic problems in statistics and econometrics where the standard methods fail. The PI has long been involved in the development of probability tools in the area known as empirical process theory--an area that cuts across the fields of statistics, econometrics, and computer science. The main aim of the project is to adapt those tools, and invent new methods, to apply to currently intractable asymptotic problems."
"9404408","Variable Selection and Related Problems","DMS","STATISTICS","09/01/1994","06/05/1996","Edward George","TX","University of Texas at Austin","Continuing Grant","James E. Gentle","08/31/1997","$75,000.00","","edgeorge@wharton.upenn.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1269","0000, OTHR","$0.00","The proposed research will develop and evaluate variable  selection procedures for multiple regression and related  problems.  Both the Bayesian and the frequentist points of view  will be considered.  From the Bayesian perspective, the focus  will be on stochastic search variable selection methods which use  a hierarchical mixture model to guide variable selection.  These  methods will include a procedure for fast Bayes variable  selection to handle hundreds of variables at once, a procedure  for variable selection in generalized linear models, a procedure  for variable selection across exchangeable regressions, and a  procedure for performing simultaneous variable selection and  outlier removal.  From the frequentist perspective, the focus  will be on a new criterion for evaluating selection criteria  called risk inflation.  This research will entail the application  and generalization of risk inflation to a broad set of model  selection problems which includes generalized linear models,  order selection procedures for time series models, and  change-point selection for estimation in change-point problems.  Risk inflation will also be used to gauge the bias of using  variable selection procedures in conjunction with heuristic  search methods.  A new class of variable selection procedures  which use adaptive dimensionality penalties will also be  developed.     One of the main goals of modern statistical methods is to provide  statistical models which relate input variables to outputs of  interest.  For example, such models are used to predict and  explain annual crop production in agriculture, interest rates in  business and economics, cancer incidence in medicine, and crime  rates in sociology.  A key component in building such models is  the selection of input variables which contain strong predictive  and explanatory information.  This component is especially  important because of the recent proliferation of large databases  containing vast numbers of potential input variables.  The  propos ed research will provide powerful new methods for selecting  such input variables for a wide variety of model building  situations."
"9404329","Mathematical Sciences:  Computer Intensive Methods for      the Statistical Analysis of Time Series and Random Fields","DMS","STATISTICS","06/01/1994","04/25/1994","Dimitris Politis","IN","Purdue Research Foundation","Standard Grant","Stephen M. Samuels","05/31/1998","$65,000.00","","dpolitis@ucsd.edu","1281 WIN HENTSCHEL BLVD","WEST LAFAYETTE","IN","479064182","3174946200","MPS","1269","0000, OTHR","$0.00","Politis       The general goal of this project is to develop methods of  inference for the statistical analysis of time series and random  fields that do not rely on unrealistic or unverifiable model  assumptions. Bootstrap resampling or computer-intensive methods  offer viable approaches to obtaining valid distributional  approximations while assuming very little about the stochastic  mechanism generating the data. However, many important questions  need to be addressed in order for these modern approaches to be  applied. The main issues we wish to tackle include the following:  finding general conditions for asymptotic validity of  computer-intensive methods, especially subsampling, in the  presence of nonstationarity; higher-order comparison of  block-resampling and subsampling; developing computationally  efficient and accurate estimates of standard error, and the  corresponding improvement on confidence regions for parameters of  interest; optimal choice of design  parameters, as well as  practical guidelines for implementation; assessing goodness of  fit in time series settings; and finally interval estimation with  random field data.       The statistical analysis of time series and random fields,  i.e., observations that exhibit serial (temporal) or spatial  dependence,  is vital in many diverse scientific disciplines,  with applications in the fields of physics, engineering,  acoustics, geostatistics, geophysics, medicine, econometrics,  ecology, forestry, seismology, and others. The general goal of  this project is to develop methods of statistical inference in  the context of time series and random fields that do not rely on  unrealistic or unverifiable model assumptions. The main approach  that will be pursued is the utilization and refinement of the  bootstrap and other computer-intensive techniques, which so far  have been developed extensively and have been proven very useful  in the setting of independent observations."
"9403826","Mathematical Sciences:  Computer-Intensive Methods for the  Statistical Analysis of Time Series and Random Fields","DMS","STATISTICS","09/01/1994","02/21/1996","Joseph Romano","CA","Stanford University","Continuing Grant","James E. Gentle","10/31/1997","$75,000.00","","romano@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","The statistical analysis of time series and random fields is  vital in many diverse scientific disciplines.  The general goal  of this project is to develop methods of inference for the  analysis of time series and random fields that do not rely on  unrealistic or unverifiable model assumptions.  Typical  inferential methods in the data-dependent setting rest upon  strong assumptions.  In contrast, bootstrap resampling or  computer-intensive methods offer viable approaches to obtaining  valid distributional approximations while assuming very little  about the stochastic mechanism generating the data.  Many  important questions need to be addressed in order for these  modern approaches to be applied safely in practice.  The main  issues we wish to tackle include the following: finding general  conditions for asymptotic validity of computer-intensive methods,  especially subsampling, in the presence of nonstationarity;  higher-order comparison of block-resampling and subsampling;  developing computationally efficient and accurate estimates of  standard error, and the corresponding improvement on confidence  regions for parameters of interest; optimal choice of design  parameters, such as block size, as well as practical guidelines  for implementation; assessing goodness of fit in time series  settings; and finally interval estimation with lattice or  non-lattice random field data.    Addressing these general problems fruitfully will have many  practical applications.  Applications of statistical methods for  time series and spatial data are well-known and numerous,  especially in the fields of physics, engineering, acoustics,  geostatistics, medicine, econometrics, ecology, forestry,  seismology and others.  The general purpose of this research  proposal is to develop inferential methods for dependent data  that does not rely on strong model assumptions. These methods are  computer-intensive, very generally applicable, flexible, and  offer solutions to problems when there are no alternatives;  how ever, further mathematical study of these procedures is needed  in order to fully under their potential and limitations."
"9403804","Mathematical Sciences:  Adaptive Spatial Regression and     Classification","DMS","STATISTICS","07/01/1994","05/16/1994","Jerome Friedman","CA","Stanford University","Continuing Grant","James E. Gentle","06/30/1998","$110,000.00","","jhf@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","9403804   Friedman        Prediction is one of the most widely applied statistical  procedures. The purpose is to predict (estimate) the value(s) of  one or more attributes {y(1),...,y(q)} (""response"" variables)  associated with an object (observation), given the simultaneous  values of another set of attributes {x(1),...,x(n)} (""predictor""  variables) associated with the same object. The prediction rule  is derived from a set of ""training"" observations for which the  values of all attributes (predictor and response) have been  measured. When the response variables assume real (orderable)  values the prediction problem is referred to as ""regression"".  When a (single) response takes on K unorderable categorical  values the problem is called ""classification"". In this case the  response values can be viewed as a label that assigns the  observation to one of K groups or classes, each class associated  with one of the label values. The research under this proposal  addresses an important subclass of prediction problems in which  the predictor variables {x(t)} are labeled by an index t that  takes on real values in a d - dimensional Eucludean space, and it  is not unnatural to associate a distance between such variables  with the distance between their corresponding index values. In  these applications all of the predictor variables are generally  measurements of the same quanity for different values of the  index t. When t is one - dimensional, {x(t)} for varying values  of t is call a ""signal"" or ""spectrum"". Similarly, a  two-dimensional index gives rise to a ""pattern"", whereas  dimensionalities of three or higher give rise to more general  ""spatial patterns"". The goal of this research is to derive  methods that exploit the spatial nature of the predictor variable  index in more general (and more powerful) ways than have been  done (with linear methods) in the past. These new methods will be  based on adaptive (regression) splines, which have been among the  most promising extensions of linear smoot hing to nonlinear  flexible modeling, especially in higher dimensions. If  successful, the result will be a more general class of spatial  prediction procedures that will achieve higher accuracy in many  situations than present day linear methods.         The purpose of the research proposed under this grant is to  develop new, more powerful, methods for computer automated  pattern recognition. Patterns such as signals, spectra, or images  are received, and the purpose is to predict the identity of the  particular (unknown) system that produced the pattern, or some  property of that system. Examples are disease diagnosis form EKG  or EEG measurements, recognition of specific words (or speakers)  from digitized electronic patterns of spoken speech, recognition  of printed or handwritten characters from their digitized images,  or identification of objects on the ground from satellite  images.   The methods to be developed are based on learning  through experience from past successfully solved cases. The  method is presented with a series of patterns, along with the  corresponding correct answer for each one, obtained from past  experience. Using this data, the method attempts to automatically  learn rules for predicting future patterns for which the correct  answer is unknown. This research focuses on developing pattern  recognition learning methods that have greater flexibility and  adaptibility than those presently in use. If successful, this  research should produce new procedures that provide higher  prediction accuracy for many applications than has been  achieveable in the past."
"9404594","Mathematical Sciences:  Investigations Into Computationally Intensive Statistical Methods","DMS","STATISTICS, CONTROL, NETWORKS, & COMP INTE","07/01/1994","07/07/1994","Art Owen","CA","Stanford University","Standard Grant","James E. Gentle","06/30/1997","$60,000.00","","owen@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269, 1518","0000, OTHR","$0.00"," 9404594   Owen   This project considers modern computationally intensive statistical methods, focussing on problems of numerical quadrature in high dimensions and neural networks in noisy settings. The work on quadrature will develop hybrids of equidistribution methods and Monte Carlo methods, in order to combine the best features of each. Equidistribution methods commonly provide more accurate estimates of integrals, as borne out by asymptotic calculations and some examples in the computational physics literature. Monte Carlo methods, make it easier to assess the accuracy of an estimated integral. The hybrid is formed by randomizing within a class of equidistribution methods developed by Faure and Niederreiter. It is expected that the resulting methods will produce accurate answers whose accuracy can be reliably gauged from the same data used to generate them. Artificial neural networks are widely used to predict and classify responses based on a set of predictors. They are better able to estimate complicated structures than many traditional statistical tools. They are also more prone to finding structures when given purely random data to train on. The problems considered here are guaging how much structure a neural network will learn in a noisy setting, and constructing networks that find less structure in the noise while remaining sensitive to true structure.  The integrals considered here may be thought of as averages of one ""output"" quantity as perhaps ten or twenty ""input"" quantities vary over their possible values. These averages are of interest in problems from chemistry, physics, finance and statistics. One approach to calculating these averages is based on picking a list of representative input settings, evenly spread through the possible input values, and then averaging the corresponding output values. For many problems this method is quite accurate, but on any given problem it can be hard to tell exactly how accurate the answer is. A second approach uses a randomly chosen list of in put settings. This approach is usually less accurate but there are ways of using the randomness to make probabilistic accuracy statements about the answer. The proposed research combines these ideas by taking a representative list of input settings and randomly scrambling it in a way that preserves the representativeness but should still allow probabilistic statements of accuracy to be made. Artificial neural networks are often used in statistical problems such as predicting what group an object belongs to, given some measured features of it, or predicting an output number given some input numbers. They are called neural networks based on an analogy between their structure and that of a brain. They are usually trained on a set of data containing the true inputs and outputs and in many problems are effective at learning to predict future outputs from future inputs, even when the input-output relationship is very complicated. The proposed work is to study the extent to which artificial neural networks mistakenly learn random patterns from data in which the inputs are irrelevant to the outputs, and to identify which sorts of neural networks are less prone to this problem."
"9403794","Mathematical Sciences:  Detection and Estimation Problems inQuality Control, Dynamical Systems and Genetic Analysis","DMS","STATISTICS","07/01/1994","03/18/1996","Tze Lai","CA","Stanford University","Continuing Grant","James E. Gentle","09/30/1997","$300,000.00","David Siegmund","lait@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","0000, OTHR","$0.00","A major direction of the proposed research is in sequential  change-point detection schemes. Powerful techniques to tackle  this problem are currently available from recent advances in  sequential testing theory and boundary crossing problems in  random fields. It is expected that relatively simple algorithms  which are not too demanding in computational and memory  requirements for on-line implementation and yet are nearly  optimal from a statistical viewpoint will be developed for a wide  variety of practical applications. Not only will the methodology  to be developed address the widely recognized discrepancies  between the assumptions underlying conventional control charts  and today's industrial processes, but it will also integrate both  the detection aspect and the quality measurement aspect of  industrial quality control. Another closely related direction of  research is in fixed sample change-point problems and its  applications to econometrics, signal reconstruction and genetic  linkage analysis. Related fundamental problems in boundary  crossing probabilities of random processes and random fields will  be investigated, and are expected to lead to definitive solutions  of some long-standing problems concerning the distribution of  generalized likelihood ratio statistics in change-point models. A  third related area of research is estimation and control of time  series models and stochastic dynamical systems whose parameters  may change with time. Although in practice abrupt parameter  changes typically occur very infrequently, the unknown times of  their occurrence have led to prohibitive complexity of the Bayes  estimators and controllers in the literature. By using parallel  recursive algorithms and combining some new ideas in sequential  change-point detection with empirical Bayes methodology, it is  anticipated that asymptotically efficient estimation and control  schemes which have manageable complexity and which can be  implemented on-line will be developed.   An important objeat ive of the proposed research is the  development of a powerful statistical methodology for quality  control of modern industrial processes and for automated fault  detection in complex manufacturing systems. Another important  goal is to develop statistical methods appropriate for analysis  of genetic linkage data related to disease susceptibility and  other traits in humans, animals and plants and for extraction of  relevant information from these data that may lead to better  diagnostic tests and treatments of the disease."
"9402561","Mathematical Sciences:  The Generalized MLE and             Redistribution-to-the-Center Estimator of a Survival        Function","DMS","STATISTICS","09/01/1994","07/13/1994","Qiqing Yu","NY","SUNY at Stony Brook","Standard Grant","Sallie Keller-McNulty","08/31/1996","$30,000.00","","qyu@math.binghamton.edu","W5510 FRANKS MELVILLE MEMORIAL L","STONY BROOK","NY","117940001","6316329949","MPS","1269","0000, OTHR","$0.00","The investigator proposes to study nonparametric estimation of an  unknown survival function S (=1-F) with interval-censored data.   Suppose the survival time X has a distribution function F.  Let  (Y,Z) be the random censoring interval.  The observations are  (L1,R1),...,(Ln,Rn), which are i.i.d. random vectors from a  population (L,R), where L=R=X if X is not inside the interval  (Y,Z) and (L,R)=(Y,Z) otherwise.  Interval-censored data arise  quite naturally in medical follow-up studies or in industrial  life-testing.  To date, there is no closed form expression for  the generalized maximum likelihood estimator (GMLE).  The  investigator, jointly with Wong, is proposing an estimator  obtained by a redistribution-to-the-center method, called the  RTCE.  The RTCE has an explicit expression and is a GMLE in many  cases.  The funding of this proposal would lead to an explicit  expression of a GMLE in a more general interval censorship model  rather than these special cases, and a better understanding of  the properties of the GMLE and the RTCE.  These results then lead  to a more convenient and useful estimator than the current  procedure derived from a self-consistent algorithm.                Interval-censored data arise naturally in medical follow-up  studies or in industrial life-testing, for example, in a breast  cancer chemoprevention study in which the effect of a  chemopreventive agent is investigated.  An important question in  such a study is how long a woman can go without taking the agent  before the protective effect of the agent wears off.  Let X  denote the time interval from cessation of use of the agent to  the loss of its protective effect qualified as a return to  baseline level of an intermediate biomarker.  When the biomarker  levels are monitored in scheduled intervals, the exact value of X  is usually not known except that it lies in an interval.  The  overall technical objective in this proposal is to carry out  nonparametric estimation of the survival function, i.e., the   probability of X>t for any given time t."
"9313013","Mathematical Sciences: Measurement, Modeling and Prediction for Infrastructural Systems","DMS","OFFICE OF MULTIDISCIPLINARY AC, INFRASTRUCTURE PROGRAM, STATISTICS, SSA-Special Studies & Analysis, CONSTRUCTION AND INFRASTRUCTUR, DYNAMICS & CONTROL, GEOTECHNICAL I, NAT & MAN-MADE HAZARD MITIGATI, ARCHITECTURE & MECHAN SYSTEMS, STRUCTURES II, OPERATIONS RESEARCH","10/01/1994","06/14/2000","Alan Karr","NC","National Institute of Statistical Sciences","Continuing Grant","Lloyd E. Douglas","09/30/2001","$5,900,000.00","Jerome Sacks, Eric Pas","karr@rti.org","19 TW ALEXANDER DR","RESEARCH TRIANGLE PARK","NC","277090152","9196859300","MPS","1253, 1260, 1269, 1385, 1442, 1445, 1448, 1474, 1497, 1498, 5514","1038, 1039, 1057, CVIS","$0.00","9313013  Karr       The nation's physical infrastructure, in particular, surface  transportation, is in urgent need of restoration, improvement and  expansion.  Economic resources are insufficient to address all  aspects of the problem, so that rational evaluation of proposed  responses is crucial.  To this end, methodology must be deployed  for measuring characteristic of infrastructural systems, modeling  their properties and predicting their performance under various  scenarios.       The PIs select a set of key problems affecting  transportation supply and demand:  network modeling, an intrinsic  issue for Intelligent Vehicle-Highway Systems, with specific  attention to real-time route planning in current experimental  programs in IVHS; materials properties, with a focus on the  design and behavior of concrete pavement as well as fundamental  links between performance characteristics and microstructure of  concrete; and travel demand modeling that incorporates  travel/activity behavior of individuals to enable detailed  predictions of demand under various policy scenarios and,  eventually, how the transportation network is affected.       For the systems, we study statistics is the enabling  technology for characterization and prediction.  Through cross-  disciplinary collaboration among materials, regional, statistical  and transportation scientists, strategies for short-term  solutions and longer-term improvements will be developed.  The  methods will be transferable to a wide range of transportation  problems that rely on a combination of simulation models,  laboratory tests and field performance data."
"9404151","""Nonlinear Bayesian Function Estimation in Complex Models""","DMS","STATISTICS","07/01/1994","06/20/1994","Peter Mueller","NC","Duke University","Standard Grant","Stephen M. Samuels","12/31/1996","$65,000.00","","pmueller@math.utexas.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, OTHR","$0.00","The underlying theme of this research is the synthesis of recent  advances in nonparametric Bayesian methods, such as mixture  models and Dirichlet process models, and some important  statistical models and concepts, including nonlinear  autoregression, neural networks and wavelet representations. Key  elements in the proposed research are computational issues.  Specific problems include Markov chain Monte Carlo simulations to  estimate mixture models which do not allow straightforward Gibbs  sampling implementation, estimation of variable architecture  neural networks, and estimation of the posterior distribution of  the coefficients of a wavelet representation of an appropriate  probability model. The neural network research will be based on  formulating the neural network approach as a hierarchical  nonlinear regression model with a mixture involving a random  number of terms and hierarchies. Work on the wavelet  representations will provide thresholding rules as maximum  posterior estimates in a hierarchical Bayes model.  This project brings together methodology from Bayesian function  estimation with problems arising in applied fields. In  particular, the research deals with neural network models,  wavelet representations, and nonlinear autoregression.  Applications of neural networks are found in biology, psychology,  physics, engineering, and computer science. Wavelet  representations have been found useful in cleaning noisy data.  Nonlinear autoregression is an extension of widely used methods  to study time series data."
"9311071","Mathematical Sciences: Statistical Inference in             Paleoclimatology","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS, GLOBAL CHANGE","03/01/1994","03/02/1994","Mike West","NC","Duke University","Standard Grant","Michael Steuerwalt","08/31/1996","$100,000.00","Tom Johnson","Mike.West@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269, 1271, 1577","0000, 1577, GLCH, OTHR","$0.00","9311071  West       This project concerns statistical issues arising in data  processing, analysis and inference based on geological records of  environmental change.  The current proposal results from  discussion between the two investigators about the statistical  issues and problems for models and methods of analysis,  especially time series analysis, geared to the realistic  assessment of patterns of time variation in sedimentary records  relevant to climate change.  This project will initiate a  collaborative research and reciprocal training program to address  these issues.  The focus will be on the processes of uncertainty  propagation and formal statistical inference about climatic  changes based on raw records from deep lake sediments.  Similar  issues exist and deserve attention in the assessment of data from  deep sea sediments, ice cores, and tree rings.  Technically, the  project concerns statistical calibration of raw data records, and  time series analysis to assess patterns of variation in the  geological records over time.         The initial objective will be to develop an appreciation of  the statistical issues as they permeate the various stages of the  experimental processes in analyzing isotope and other proxy  climatic variables from deep-lake sediment cores.  Cross-  disciplinary training for each investigator will  e a central and  continuing feature of the collaboration.  This will be directed  at bridging the gulf between, on one hand, current standards of  practice in statistical  analysis of paleoclimate data (e.g.  Pisias and Moore 1981; Hagelberg, Pisias and Elgar 1991)  and, on  the other hand accessible and appropriate modern statistical   methods and models.  A reciprocal training and  development  schedule will provide the opportunity for each of the two  investigators to become sufficiently proficient in the  fundamentals of  the other's field of expertise so as to initiate  and enable a research program focussed on rigorous statistical  analysis of paleoclima te data generated from deep lake sediment  cores.  Statistician Mike West will become familiar with the  methods used to select sediments core sites for paleoclimatic  analysis, the various methods of geochemical analysis of sediment  cores (and notably the limitations of age assignment to sediments  as a result of various natural processes such as erosion and  redeposition, bioturbation, diagenesis and laboratory  contamination), calibration of carbon dating assessments, and  current methods of time series analysis used in paleoclimatology.   Geologist Tom Johnson will become familiar with the capabilities  and pitfalls of statistical methods, especially methods of time  series analysis that are currently used in this area, and gain  appreciation of the consequences for  uncertainty assessment in  geochronology.  The investigators anticipate eventual development  of Bayesian methods in statistical calibration at the various  stages of the experimental process, and resulting time series  analysis in assessing cyclical variability in calibrated records.   Both investigators will also become familiar with the statistical  issues and methods of calibration of 14C dates (together with  associated uncertainty assessments) to calendar dates, and with  the recent development of advanced statistical methods in the   field.       This project will eventually lead to focussed research  efforts in developing Bayesian statistical models and methods  appropriate for all stages of the experimental process in  sediment based paleoclimatology, and that will ultimately apply  in other geological and archaeological arenas.  Most  optimistically, it is hoped that this collaboration will seed the  development of an internationally recognised centre for the  statistical analysis of paleoclimate proxy records at Duke  University.  ***"
"9403818","Bayesian Methods and Decisions for Observation Times","DMS","STATISTICS","07/01/1994","07/07/1994","Giovanni Parmigiani","NC","Duke University","Standard Grant","James E. Gentle","06/30/1998","$63,000.00","","gp@jimmy.harvard.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","9218, HPCC","$0.00","This proposal considers problems in the area of optimal choice of  observation times for a random process. Bayesian inference and  decision theory provide a general and successful theoretical  framework. Within this framework, the goal of the proposed  research is to seek advances in important application areas. In  particular, goals are: To lay the groundwork for Bayesian dynamic  adjustment of sampling frequency in monitoring a random process  for control and other purposes; to study models for detecting the  presence of conditions that do not manifest naturally; and to  study design problems in which the goal of taking observations is  learning about process parameters.    Acquiring information about random phenomena over time is often  costly and hard. Efficiency can be substantially enhanced by  careful choice of observation times. The proposed research  examines specific applications in the hope of a) providing  solutions to pressing practical problems; b) indicating general  strategies. Examples of applications include timing of screening  exams for the early detection of cancer; designing tests for  improving software reliability; efficient monitoring in  meteorology, seismology and environmental sciences; efficient  implementation of process control and quality control in  manufacturing; regulation of airplane maintenance and others."
"9404200","Bayesian Approaches for Handling Multiplicities and         Designing Experiments","DMS","STATISTICS","08/01/1994","04/08/1996","Donald Berry","NC","Duke University","Continuing Grant","James E. Gentle","07/31/1997","$88,000.00","","","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1269","0000, OTHR","$0.00",""
"9404343","The Spectral Envelope","DMS","STATISTICS","07/01/1994","07/07/1994","David Stoffer","PA","University of Pittsburgh","Standard Grant","Stephen M. Samuels","06/30/1997","$59,000.00","","stoffer@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","0000, OTHR","$0.00","The spectral envelope was recently introduced as a method for  analyzing categorical time series in the spectral domain and has  been successfully applied in analyzing DNA sequences. In  developing the theory and methodology of the spectral envelope  for categorical time series, it was discovered that there are  many interesting extensions to the concept. Although there are  numerous directions in which to pursue this idea, this research  will focus, at least, on extending the concept of the spectral  envelope to the analysis of real-valued time series. Here, the  main goals will be to obtain optimal transformations of  real-valued (univariate and multivariate) time series that may be  nonlinear, non-Gaussian, or have some unsuspected dependence  structure, and to envelop (or blanket) the spectrum of any such  transformation of the time series.  The spectral envelope was recently introduced as a method for  analyzing qualitative data observed sequentially in time or space  (time series) and has been successfully applied in analyzing DNA  sequences. In developing the theory and methodology of the  spectral envelope for such data, it was discovered that there are  many interesting extensions to the concept. Although there are  numerous directions in which to pursue this idea, this research  will focus, at least, on extending the concept of the spectral  envelope to the analysis of quantitative time series data.  Here,  the main goals will be to obtain optimal transformations of time  series that may be nonlinear, non-Normal, or have some  unsuspected dependence structure, and to discover any interesting  cyclic (repetitive) properties of these data."
"9596092","Mathematical Sciences:  Methods for Ranked Data","DMS","STATISTICS","07/01/1994","03/06/1995","Georgia Thompson","TX","University of Texas at Dallas","Standard Grant","Sallie Keller-McNulty","12/31/1995","$26,163.00","","","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1269","","$0.00",""
"9404142","Mathematical Sciences:  Jump and Sharp Cusp Detection       by Wavelets","DMS","CLASSICAL ANALYSIS, STATISTICS","07/01/1994","07/14/1994","Yazhen Wang","MO","University of Missouri-Columbia","Standard Grant","Stephen M. Samuels","12/31/1997","$63,000.00","","yzwang@stat.wisc.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1261, 1269","0000, 9146, 9218, HPCC, MANU, OTHR","$0.00","9404142  Wang   This project is aimed at jump and sharp cusp detection of a function in one dimension as well as several dimensions, where the function is observed with noisy data. Jumps and sharp cusps describe sudden and localized changes in functions. They have been used in modeling many practical problems such as edge detection and signal segmentation in image and signal processing, system monitoring, drug studies in medicine, and sudden structural changes in economics. The theory of wavelets with compact support --- a recent breakthrough in applied mathematics --- offers a degree of localization in space as well as frequency. By looking at the wavelet transformation of a function at fine scales and at spatial positions near a point, one can easily check if there is a significant local change such as a jump or a sharp cusp in the function near the point. Wavelets provide an ideal tool for jump and sharp cusp detection. The proposed method is to compute the wavelet transformation of the data first and then use the spatial positions at which the wavelet transformation at certain fine scales changes rapidly to estimate the locations of jumps and sharp cusps. Wavelets with compact support enable one to develop a theory for the estimates and fast algorithms to compute the estimates. The method will be very useful in a variety of applications including computer image coding and digital compression.   This project is aimed at jump and sharp cusp detection of signals in the presence of noise. Jumps and sharp cusps describe sudden and localized changes. For example, in an electrocardiogram, sharp cusps exhibit the accelerations and decelerations in the beating of hearts. For a digitized TV or movie picture, jumps and sharp cusps correspond to contours and outlines in the picture. Detection allows one to sort out these contours and outlines from the picture, which is very crucial in computer image coding and image compression. The theory of wavelets, a recent breakthrough in mathematics, engineering a nd physics, offers a degree of localization in space as well as frequency. This project uses wavelets to detect jumps and sharp cusps and develops a theory and fast computer algorithms for such detection. The method will be very useful in a variety of applications including computer image coding and digital compression, edge detection and signal segmentation in image and signal processing, system monitoring, detection of abrupt adverse reaction to drugs, and identification of sudden structural changes in economical phenomenon."
"9404130","Mathematical Sciences:  Sampling Plans, Asymptotic Results, Resampling Algorithms, & Applications to Random Processes   on the 3-dimensional Sphere","DMS","STATISTICS","07/15/1994","07/12/1994","Jason Brown","MO","University of Missouri-Columbia","Standard Grant","James E. Gentle","06/30/1997","$60,000.00","","","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269","0000, OTHR","$0.00","To: ""Alan J. Izenman"" <aizenman@nsf.gov> From: aizenman@nsf.gov (Alan J. Izenman) Subject: Brown Abstract Cc: Bcc: X-Attachments:  9404130 Brown Research on an isotropic random field on the sphere has mainly focused on the representation of the random field, although, there has been some research on proving a central limit theorem (CLT) for a continuously indexed random field on the sphere. From a practical point of view, however, it is impossible to sample continuously throughout the sphere and a finite global sampling plan needs to be found in order to investigate statistical relations associated with global data, in particular a CLT and resampling algorithms. Work by Brown  1993  lays the groundwork for future research in this area and he has applied his research to global land-area and coastline data. His work addresses the above statistical issues in a nonparametric setting using weak general conditions as the radius of the sphere grows without bound with the sample size. More recently, in the parametric setting there has been work done on modeling the entire random process on the sphere. This research will consider both nonparametric and parametric cases and asymptotic results for situations where the radius of the sphere remains fixed while the sampling plan gets more dense on the sphere and where the radius of the sphere grows without bound along with the sample size. Specifically, this research will study the following areas: finite global sampling plans, asymptotic results for different sampling plans and general statistics, resampling mechanisms of the data for various sampling plans, and applications to this new research.  Typically, we only have an opportunity to sample data at a finite number of points on the globe, comprising a global sampling plan. Once a sampling plan is established, data is gathered at the points and quantities of interest are calculated. In order to make decisions about the quantity of interest, some of its characteristics need to be known or estimated. Since  gathering data via the sampling process is expensive, some methods for reusing (resampling) the original data need to be developed as well. This research will study the following areas: global sampling plans, characteristics for various quantities of interest, resampling mechanisms for global data, and applications to this new research."
"9403718","Mathematical Sciences:  Nonparametric Density & Regression  Estimation for Dependent Random Variables","DMS","STATISTICS","06/01/1994","05/18/1994","Lanh Tat Tran","IN","Indiana University","Standard Grant","Stephen M. Samuels","05/31/1997","$60,000.00","","","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","MPS","1269","0000, OTHR","$0.00","Tran            Density and regression under dependence have recently  received increasing attention.  This research covers two broad  but related areas, nonparametric density estimation and  nonparametric regression.  The dependence dealt with is of serial  or spatial type.  Optimality results on nonparametric functional  estimation for serially dependent random variables have been  mostly of the asymptotic type.  The practical question as to what  to do in the small sample case is of great concern and still  poses basic challenges.  The purpose of this research is to  extend existing theory toward the needs of practitioners.  The  objective is the development of a finite sample theory for  nonparametric density and regression estimators.  Another major  feature of the current proposal is the extension of certain  results on nonparametric density and regression known for time  series to random fields.  Different density estimators and both  fixed and random regression designs are to be investigated.            In contrast to traditional statistical theory,  nonparametric estimation gives a flexible approach to data  analysis without requiring detailed knowledge or assumptions  about the process under investigation.  The goal of the research  is to develop nonparametric statistical theory to better serve  practitioners.  The applicability of the investigation will be  demonstrated by concrete and real-life examples.  The long-range  goal is to develop nonparametric techniques for digital image  processing, and even for the analysis of data which are both  space and time varying, such as data obtained from a series of  digitized photographs taken at different times.  The proposed  research also has potential application to the analysis of data  collected irregularly at different space and time points. The  resulting methodology may prove important for processing  environmental data and data from geology, soil science and  meteorology. ***"
"9402714","Mathematical Sciences:  Algebraic Methods in Multivariate   Statistical Analysis","DMS","STATISTICS","06/15/1994","06/13/1994","Steen Andersson","IN","Indiana University","Standard Grant","James E. Gentle","05/31/1997","$60,000.00","","standers@indiana.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","MPS","1269","0000, OTHR","$0.00"," It is proposed to continue the study of the statistical properties of multivariate normal models determined by lattice conditional independence (LCI) assumptions and/or group symmetry assumptions on the covariance matrix, augmented by compatible linear restrictions on the mean. LCI models have been shown to be applicable to the analysis of multivariate normal data sets with nonnested missing data patterns. A new application of LCI models will be emphasized here: their application to the analysis of a collection of nonnested dependent linear regression models, known in econometrics as a seemingly unrelated regression (SUR) model. A SUR model may be thought of as a finite nonnested collection of linear regression subspaces with correlated errors across regressions. For a given SUR model, the least restrictive LCI covariance model compatible with the mean structure can be determined, leading to explicit maximum likelihood estimates for the SUR model. To date, LCI models have been studied only for multivariate normal distributions. Another new aspect of this proposal is the application of LCI models to categorical data in multiway contingency tables. As in the case of normal data, such LCI models should allow explicit maximum likelihood estimates for contingency tables with nonnested missing categories. Many familiar statistical models occurring in classical multivariate analysis (the study of correlated data) can be viewed as special cases of models defined in terms of natural algebraic conditions on the means and/or covariances. This viewpoint will (a) lead to a unified and explicit (non-iterative) analysis of these models, and (b) expand the scope of multivariate analysis by allowing the application of classical methods to many new models, as well as allowing the possibilities of missing data occurring in nonnested patterns and of nonnested regression subspaces."
"9496219","Mathematical Sciences: NSF Young Investigator","DMS","STATISTICS","06/15/1994","06/12/1996","Kathryn Roeder","PA","Carnegie-Mellon University","Continuing Grant","James E. Gentle","01/31/1998","$98,602.00","","roeder@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, 9297, OTHR","$0.00",""
"9404438","Theory and Applications of Latent Variable and Mixture      Models for Repeated Measurements","DMS","STATISTICS, Methodology, Measuremt & Stats","07/01/1994","07/07/1994","Brian Junker","PA","Carnegie-Mellon University","Standard Grant","James E. Gentle","06/30/1997","$75,000.00","","brian@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269, 1333","0000, OTHR","$0.00","The focus of this work is on strictly unidimensional latent  variable models, which may be thought of as mixture models which  induce conditional association (Rosenbaum, 1984; Holland and  Rosenbaum, 1986) in the marginal distribution of the data, or as  a special case of Stout's (1987, 1990) essentially unidimensional  models. I propose to extend my past efforts to use these ideas  together with notions from the literature on positive and  negative dependence (e.g. Joag-Dev, 1983; Joag-Dev and Proschan,  1982; Newman and Wright, 1981; or more recently the collection  edited by Block, Sampson and Savits, 1990) to characterize  strictly unidimensional models. My recent explorations of this  problem suggest a reasonably straightforward approach that, as a  side benefit, generalizes de Finetti's characterization of  exchangeability, without the need to specify sufficient  statistics as in, for example, Diaconis and Freedman (1984). A  second line of work in this proposal is the exploration, using  asymptotic methods along the lines of Kass, Tierney and Kadane  (1990), and Clarke and Barron (1990), of inferences about the  latent trait under a strictly unidimensional model, which asserts  conditional independence given the latent trait, when in fact  some mild form of conditional dependence holds. In addition,  biases in the asymptotic standard error of an MLE-like estimator  can also be calculated and, in some cases, corrected using  nonparametric regression ideas due to Ramsay (Ramsay, 1991;  Ramsay and Winsburg, 1991). Finally, some problems in  applications and computing will be examined, including unifying  and extending nonparametric techniques for latent variables data  analysis (e.g. Molenaar, 1991; and Grayson, 1988); and developing  parametric statistical models and computational methods (e.g.  efficient estimation of a polytomous version of the model  specified by Lindsay, Clogg and Grego, 1991) that arise in the  analysis of data from small scale experiments in cognitive  science.   This  proposal concerns statistical and probabilistic features of  latent variable models for repeated measures data, which is of  interest to quantitative psychologists, psychometricians, and  cognitive scientists, as well as other social scientists. A  typical application for latent variable models is psychological  measurement, in which the latent variable is an unobservable  variable that indicates the level of a psychological feature of a  person---such as depression, mathematical aptitude, job  satisfaction, or working memory capacity---that we observe only  indirectly through the person's responses to a series of tasks,  questionnaire items, etc. Data of this type might be obtained  from psychiatric rating forms, standardized academic achievement  or aptitude tests like the SAT and GRE, standardized  questionnaires in sociology, or coded responses to a set of tasks  in experiments in cognitive psychology. A primary outcome of this  research will be a deeper understanding of latent variable models  for measurement problems, at both the level of fundamental  statistical theory and the level of practical applications.  Practical tools arising from this research would include:  enhanced methods for deciding how well or poorly this class of  models matches particular situations or data sets; rules for  adjusting scientific inferences based on these models for the  inevitable mismatch, however small, between the model being used  and the mechanism that generated the data; and computational and  model-building methods that are adapted to small-scale  experimental data, such as might be found in cognitive  psychology, where these models are conceptually natural but  current methods tend to break down. Much of the work proposed  here is built around interdisciplinary collaboration, especially  with quantitative psychologists and educational measurement  specialists, with the goal of developing statistical theory that  will be of use in applications."
"9403874","Mathematical Sciences:  Asymptotics for the Spectra of      Long-Memory Processes","DMS","STATISTICS","07/01/1994","06/20/1994","Norma Terrin","PA","Carnegie-Mellon University","Standard Grant","K Crank","06/30/1996","$43,000.00","","","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","0000, OTHR","$0.00","A sequence of observations made over time is said to be stationary if the covariance between two data points depends only on how far apart in time the two observations were made, and not on the times themselves. A stationary sequence has long memory if, as the distance between observations gets larger, the covariances decay so slowly that the classical limit theory for stationary sequences is not applicable. Data with long memory have been observed in many fields, from economics to geophysics. Most recently, climatologists studying global warming have modeled temperature data using long memory. The research outlined in this proposal is aimed at an understanding of the asymptotic properties of some functions of long-memory processes which are of interest to statistical researchers. One of the primary tools to be employed in the proposed research is the spectral density, which is the Fourier transform of the covariance function. It both captures the essential nature of long memory, and makes many difficult problems mathematically tractable.   Many types of data are observed over time. Examples are monthly birth rates, daily stock prices, and hourly temperature readings. Frequently sequences of data (referred to as time series) contain observations that are interrelated. For example, the value of a stock tomorrow is related to its current value. Classical statistical theory for analyzing time series relies on the assumption that observations taken far apart in time are not closely related. However, in many fields from economics to geophysics, data which does not have this property has been observed. That is, observations far apart in time may nevertheless be closely related. Most recently, climatologists have observed this phenomenon (called long memory') while studying global warming. Consequently, to determine whether global warming has indeed occurred, new methods for studying long-memory sequences are needed. The research outlined in this proposal is aimed at developing techniques for the analy sis of long-memory data."
"9403800","Mathematical Sciences:  Polynomial Spline Modeling in       Survival Analysis and Stationary Stochastic Processes","DMS","STATISTICS","07/01/1994","05/27/1994","Kinh Truong","NC","University of North Carolina at Chapel Hill","Standard Grant","James E. Gentle","06/30/1997","$54,000.00","","truong@bios.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","0000, OTHR","$0.00"," The proposal has four projects that involve the application of logspline methodology to survival analysis and stationary stochastic processes. The primary objective of the first project is to systematically extend survival analysis in all of its various aspects to handle multivariate data involving censored responses and covariates and to do so in a manner that will efficiently balance model bias and variance in the estimates. The second project considers the problem of estimating survival and hazard functions for bivariate censored failure times. Since this is a new and important research area in survival analysis, the project is further divided into two subprojects. The first considers the estimation of the bivariate survival function without covariates, and the second considers regression analysis involving covariates. The goal is to provide a consistent and unified nonparametric framework for bivariate survival analysis. In particular, asymptotic properties of the proposed procedures will be established. The third project considers the estimation of a possibly mixed spectral distribution of a stationary process. The objective is to establish asymptotic properties for the joint estimator of continuous and discrete spectra. Extensions to other stationary processes such as spatial and point processes will also be emphasized. The fourth project considers nonparametric time series regression. Its objective is to study the asymptotic properties of regression function estimates subject to having the form of a specified sum of functions of at most d variables. The first project in this proposal involves the development of several statistical procedures for identifying and evaluating important risk factors that are related to the occurrence and recurrence of various diseases such as coronary heart disease, breast, lung, prostate and colon cancers. The second project studies the effects of these risk factors on the relationship between causes of several diseases. Specifically, the method will be appl ied to study (1) the laser photocoagulation eye treatment for diabetic patients, (2) survival probabilities of nonfatal myocardial infarction and heart failures, (3) the relationship of recurrence and survival times of colon or breast cancer patients. The third project develops statistical procedures for studying elements of the nervous system and to understand how they function and work together. Through this developments, we hope to explain things such as memory, emotion, learning, sleep and expectation. The fourth project develops efficient prediction procedures for applications in environmental studies such as temperature effect on riverflow, ozone concentration."
"9322817","Mathematical Sciences:  ""Markov Dependence in Statistics andInformation Theory and Statistical Problems in Physical     Mapping""","DMS","STATISTICS","08/01/1994","07/21/1994","Bin Yu","CA","University of California-Berkeley","Standard Grant","James E. Gentle","07/31/1998","$33,000.00","","binyu@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","One of the main goals of the Human Genome Project is to produce  physical maps of the human chromosomes.  Such maps are important  steps on the path to the complete sequencing of chromosomes, and  have other important applications, such as assisting in the  localizing of disease genes.  A common method for constructing  physical maps is based on fingerprinting random clones. The  proposed research uses information theory notions to quantify  information needed for the completion of a physical map, and  information obtained from random clone data.  Maximizing the  mutual information between a configuration variable and data is  also proposed as a design criterion to select the hybridization  probability parameter.  Numerical and analytic approximations to  the criterion will be considered in the simple pairwise  comparison case, on which most current physical mapping  strategies are based.  Optimal values of the parameter can then  be obtained using these approximations.  Finally, it will be  addressed how to utilize mapping information to localize  positions of genes of interest on the chromosomes.    The proposed research is on applying information theory to  physical mapping problems arising from the Human Genome Project.   An information-theoretic approach is proposed to address certain  problems arising from physical mapping in the Human Genome  Project.  The goals of a physical map will be clarified and we  will answer questions regarding the amount of information we need  for and the amount we learn about the map.  A 3-clone approach is  expected to extract more information out of mapping data than  from 2-clone data, and the possibility of using mapping data to  locate genes of biological interest will be better understood."
"9409774","Mathematical Sciences: Conference on Forecasting, Predictionand Modeling in Statistics and Econometrics: Bayesian and   Non-Bayesian Approaches","DMS","STATISTICS","07/01/1994","06/15/1994","Arnold Zellner","IL","University of Chicago","Standard Grant","Sallie Keller-McNulty","06/30/1995","$18,775.00","","arnold.zellner@gsb.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00"," Good procedures for formulating quantitative models that are useful in explanation, prediction and decision-making are sorely needed in all sciences and applied areas. Currently, model formulation is more of an art than a science with much disagreement about how to approach the problem. Some emphasize simplicity, parsimony, and Ockham's razor while others emphasize the need for complexity, detail and realism. A number of approaches have been put forward that have been used in practice, namely, method of moments, time-series ""identification"" procedures using autocovariance matrices, etc., encompassing methods, structural econometric modeling-time series analysis procedures, maximum entropy, quantum statistical inference and so on. Further, the roles of measurement, description, unusual and ugly facts in model formulation have to be considered. Papers presented at the Geisser Conference will consider these difficult issues in detail, present and compare Bayesian and non-Bayesian approaches, and incorporate analyses of data to illustrate the results of applying various methods. A summary of the Conference's research findings will be prepared. Forecasting, prediction and modeling are central theoretical and applied topics in Statistics and Econometrics. Time series forecasting methods that are widely employed in industry and government yield useful forecasts of future developments but little in the way of explanation. On the other hand, causal models can provide predictions and explanations of future developments and how they may be influenced by various policies. Improving procedures for developing, implementing and using forecasting and causal models is a major objective of this Conference. In this connection, a comparative evaluation of Bayesian and non-Bayesian methods for achieving the above objective will be provided using many applications to illustrate general points and evaluate alternative approaches. Thus the Conference will provide evaluations and applications of old and new procedures  for developing, implementing and using forecasting and causal models."
"9403560","Mathematical Sciences:  General Linear Models","DMS","STATISTICS","07/01/1994","04/10/1996","Peter McCullagh","IL","University of Chicago","Continuing Grant","James E. Gentle","06/30/1998","$190,000.00","","pmcc@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","0000, OTHR","$0.00","Peter McCullagh will investigate a number of issues, all bearing  directly or indirectly on generalized linear models. Apart from  specialized models such as those arising in the analysis of  ranked data, the most pressing need has been for satisfactory  methods for dealing with non-linear models having several  components of variation.  Examples of experiments where such  models are appropriate are not hard to find.  The salamander data  (McCullagh and Nelder, 1989, p. 440-444) has been widely studied  as an archetypal example involving purely binary data.  Linear  models having several variance components were developed early by  Yates and Fisher, but in the pre-computer era only balanced  designs could be tackled.  For the estimation of variance  components in unbalanced designs the residual likelihood, i.e.  the likelihood based on the residuals, has come to play a key  role in recent work.  A major aim of the present proposal is to  extend this work to models of the generalized linear type. A  second and related aim is to develop new methodologies for  longitudinal data, particularly where this occurs in an  epidemiological context.  An important difference between linear  and non-linear models is the distinction between subject-specific  and population-averaged parameters, the latter being more  relevant in epidemiological contexts.  The final part of the  proposal, a sort of counterpoint to the exponential-family, is  the study of transformation models with particular emphasis on  Mobius transformation and Cauchy models.  The results obtained  thus far are helpful for the light they shed on conditional  inference and asymptotics.  This proposal aims to develop  statistical methods that enable a scientist to draw reliable  conclusions from data where the model is non-linear and the  design is such that there is more than one source of variation in  the experiment.  Such experimental designs are rather common in  biological and agricultural research, where methods of analysis  for linear mo dels are fairly well developed.  For non-linear  models, likelihood calculations are generally difficult and  time-consuming.  I propose to develop useful analytical  approximations as an alternative to numerical Monte-Carlo  simulation techniques."
"9312170","Mathematical Sciences: Some Problems for Incomplete Survival Data","DMS","STATISTICS","10/01/1994","06/01/1994","Jane-Ling Wang","CA","University of California-Davis","Standard Grant","James E. Gentle","09/30/1997","$60,000.00","","janelwang@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","0000, OTHR","$0.00","This project deals with three general statistical problems for  survival data which are either incomplete or subjected to  selection bias. For demonstration simplicity we focus on right  censored/left truncated data. Such incomplete data may arise in  follow-up studies with delayed entries. The first problem deals  with some basic properties like the strong law of large numbers  (SLLN) and the central limit theorem (CLT) for functionals of  product-limit estimates based on incomplete data or data with  selection bias. Except for censored data such fundamental results  are not available so far and will be investigated. Applications  of SLLN and CLT are plentiful in statistical inference and will  also be studied. The second problem deals wth M-estimators for  incomplete data. General approach to handle the limit theory of  M-estimators is not yet available for incomplete data even for  right censored data. We intent to develop general analytical  sufficient conditions for the strong consistency and asymptotic  normality of M-estimators based on incomplete data. Robustness  issues of M-estimators for incomplete data will also be explored.  The third problem deals with dimension reduction methods for  incomplete data. Such methods have not caught on in the  literature for incomplete data where the curse of dimensionality  is much more serious than the widely explored noncensored case.  We focus in particular on a recent promising method, sliced  inverse regression (SIR). Some issues on robustness of the  procedure and estimating statistical quantities of the response  variable for a given covariate, such as the regression function,  will be included. The project deals with general statistical  problems for lifetime data, for example, the life time of a  certain mechanical or electronical product or the incubation time  of a disease such as AIDS. One common feature of lifetime data is  the difficulties of observing some of the actual lifetimes and  those observations are thus termed ""incomplete"" sta tistically.  Incomplete data arise in various forms among which ""censoring""  and ""truncation"" are the most common ones. Our study focus on,  but not limited to, those types of incomplete data. In the  idealistic situation where all data can be observed fully most  statistical quantity of interest, such as the survival  probability or risk of certain disease, can be estimated  empirically. Compared to such a situation the incompleteness of  the data poses very challenging statistical problems and many of  the basic properties or structures remain unsolved or unknown. In  this project, three specific open problems for incomplete data  will be investigated. The first one deals with the two most  fundamental probability properties, the strong law of large  numbers and central limit theorem for incomplete data. Such  properties play central role in probability theory and are  essential for statistical inferences. However, it is only until  very recently that researachers are able to put their hands on  some special type of incomplete data. Our goal is to establish  such fundamental results for other general type of incomplete  data. The findings in this part of the project will facilitate  the study of robust statistical procedures, such as M-estimators,  for incomplete data. The third problem to be explored in this  project deals with high dimensional data analytical methods when  the response variable are possibility incomplete. Even for  complete data, the handling of high dimensional data, via  dimensional reduction methods, requires special skills and is on  the cutting edge of statistical research. The proposed procedure  for handling incomplete data extends the scope and usefullness of  existing dimension reduction procedures."
"9404276","Mathematical Sciences:  New Methods for Inference from      COBE Data","DMS","EXTRAGALACTIC ASTRON & COSMOLO, STATISTICS","07/01/1994","04/10/1996","Philip Stark","CA","University of California-Berkeley","Continuing Grant","James E. Gentle","06/30/1997","$64,000.00","George Smoot","stark@stat.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1217, 1269","9218, HPCC","$0.00"," Gravitational instability theories usually assume that the spectrum of primordial density fluctuations follows a power law with amplitudes drawn from a Gaussian distribution, and the existence of some form of dark matter. Alternative theories depend upon topological defects, such as monopoles, cosmic strings, textures, or other dynamical effects to generate present day structures. Various theories predict different angular spectra for CMB fluctuations. Topological defect models generally predict non-Gaussian statistics for the fluctuation amplitude distribution. We will develop methodology and software to find confidence intervals for moments of the CMB temperature distribution, multipole coefficients, and spectral parameters of ``power laws'' predicted by some theories. These estimates also test the gaussianity of the initial fluctuations. Our new methodology will include minimax estimation and ``strict bounds,'' and will rely heavily on large-scale nonlinear optimization. We will employ a network of 35 Sun SPARCStations to perform distributed simulations and solve large optimization problems in parallel. Both the minimax and strict bounds approaches require a priori constraints on the CMB, which we will derive from physical theory and by incorporating the results from other experiments on different spatial scales.  The Cosmic Background Explorer (COBE) team recently announced the detection of fluctuations in the temperature of the Cosmic Microwave Background (CMB). CMB fluctuations trace the primordial variations in the density of matter and energy as they were about 300,000 years after the Big Bang. These density variations are required by cosmological theories to account for observed present-day large-scale structure, e.g., galaxies, clusters of galaxies, superclusters and voids. Different theories require different structure in the primordial density, and a principal goal is to use COBE data to discriminate among these theories. The statistical tools currently used in the astrophysics comm unity do not permit rigorous discrimination among these theories: the tools are subject to known biases whose magnitudes are largely unknown. We will develop and apply new tools that use constraints derived from the cosmological theories and from other experiments to limit the possible bias and reduce the uncertainty."
"9404327","Mathematical Sciences:  Modeling & Statistical Analysis of  Multivariate, Rank, & Mental Test Data, with Social Science Applications","DMS","STATISTICS, Methodology, Measuremt & Stats","09/01/1994","08/24/1994","William Stout","IL","University of Illinois at Urbana-Champaign","Standard Grant","James E. Gentle","02/28/1998","$120,000.00","","stout@stat.uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269, 1333","0000, OTHR","$0.00"," Various improvements in Stout's statistical dimensionality assessment procedure DIMTEST are proposed. In particular, adapting the procedure for partial-credit scoring of items is proposed. Improvements in the DIMTEST large sample theory are proposed. The incorporation of modern nonparametric regression techniques is proposed. Use of more sophisticated versions of cluster analysis is proposed in order to improve the capability of DIMTEST to confirm simple structure. The development of two estimators of the amount of lack of unidimensionality is also proposed. Various improvements in Shealy and Stout's statistical test bias detection procedure SIBTEST are proposed. In particular, adapting the procedure for partial-credit item scoring is proposed. Work on the development of a version of SIBTEST to handle crossing bias is to be continued. The development of a large sample theory for SIBTEST and crossing SIBTEST is proposed. It is proposed that psychological construct validity theory be applied to enable educational practioners to use SIBTEST to distinguish between mere differential item functioning and actual test unfairness. It is proposed that computational likelihood-based algorithms be developed for parameter estimation in the unified cognitive diagnostic model of Dibello and Stout. It is proposed that the truth of Holland's Dutch Identity two parameters per item conjecture be investigated.   Work is proposed on the psychometric modeling and statistical analysis of mental test data. This research promises to be of great value in improving the manufacture and scoring of standardized tests and in carrying out cognitive diagnoses for educational remediation purposes. It also should contribute to the advancement of the theory of mental test modeling. Special emphasis is placed upon the statistical assessment of latent ability dimensionality (that is, the number of abilities influencing test performance), the statistical assessment of simple structure (clumps of test items such that items within eac h clump measure a similar configuration of basic latent abilities), the modeling and statistical assessment of mental test bias (i.e. test unfairness), and the modeling and statistical diagnosis of examinee cognitive attribute knowledge states."
"9403582","Mathematical Sciences:  RUI:  Semiparametric Regression     Analysis","DMS","STATISTICS","06/01/1994","06/10/1994","Joan Staniswalis","TX","University of Texas at El Paso","Standard Grant","James E. Gentle","12/31/1997","$90,000.00","","","500 W UNIVERSITY AVE","EL PASO","TX","799680001","9157475680","MPS","1269","0000, 9178, 9229, 9251, OTHR, SMET","$0.00","The overall thrust of the research objectives is to make progress  towards providing, for a wide variety of applications and  experimental designs, a semiparametric alternative to the popular  parametric regression methods used in the analysis of data.  Methodology is proposed for discrete and continuous data as well  as for independent and correlated responses. The objectives are  to study: i) nonparametric estimation of a regression function  when the explanatory variable is discrete, ii) estimation of the  dose effect given observations over time in a dose-response  experiment, and nonparametric detection of interactions between  dose and time, iii) estimation in conditionally specified  distributions. Decisions and recommendations based on data  analysis using the wrong parametric model are not reliable.  Therein lies the need for the development of methodology for  fitting semiparametric models to data. The basic research efforts  outlined in this proposal include: i) theoretical work, ii)  computer simulations to validate the procedures, and iii)  application of the semiparametric methods to biomedical data  obtained from researchers at Virginia Commonwealth University -  Medical College of Virginia, The University of Texas - M.D.  Anderson Cancer Center, and The Eye Institute of The University  of Wisconsin, Madison. This project also includes an  undergraduate student mentorship and community outreach  component. Undergraduate students in the Department of  Mathematical Sciences will assist with the research projects and  with mathematics outreach activities designed for the El Paso  area Middle School students."
"9404344","Mathematical Sciences:  Sequential Imputations and Gibbs    Sampling:  Combinations, Comparisons, and Applications","DMS","STATISTICS","08/01/1994","07/28/1994","Jun Liu","MA","Harvard University","Standard Grant","Sallie Keller-McNulty","07/31/1997","$55,900.00","","jliu@stat.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","0000, OTHR","$0.00"," Liu  9404344     The method of sequential imputation (henceforth, SI) introduced in Kong, Liu and Wong (1994) is a special way of conducting multiple imputations in treating missing data problems and has shown potential in providing efficient computational methods for some difficult problems in genetic linkage analysis (e.g., Kong et. al. 1993; Irwin, Cox and Kong 1993). The method is generally applicable when the corresponding complete data (Bayesian) predictive distributions are easy and is especially useful when data are collected sequentially. The Gibbs sampler is a recently popular tool for sampling from the Bayesian posterior distributions to facilitate inference. The proposed research targets at comparing the relative merits of the two methods and combining the two methods in some applications. In the first project, the SI method is applied to attack the blind deconvolution problem in digital communication, where it is assumed that the observed signals are an unknown (thus blind) linear combination of discrete input signals. A new computationally efficient algorithm for blind signal restorations results from combining the Gibbs sampler and the SI method. The second project is concerned with nonparametric hierarchical Bayesian analysis. Both the Gibbs sampler and the SI are applied to conduct sensitivity analysis and hierarchical analysis. In doing these, some theoretical understanding of the mathematical properties of a Dirichlet process is necessary. As an application, the Bayesian nonparametric method and the SI procedure are also proposed to address the problem of estimating human fecundability, or the per-cycle probability of a recognizable conception. The third project contains some ideas on how to use multiply imputed data sets more efficiently. A ""split sampling"" method is proposed to hybrid the jointly imputed complete data and then to adjust by using importance weights. This will result in a more efficient estimator, called ""cross-match"" estimate, of the quantity of interest.  T he proposed research targets at comparing two novel Monte Carlo simulation methods, the sequential imputation and the Gibbs sampler, for Bayesian statistical analysis, and applying the two methods to several important problems. The first problem to attack is one in digital comunication, called ""blind deconvolution."" It has many applications in, for example, seismology, underwater acoustics, and multipoint network. We plan to use a complete probabilistic model to describe the system and apply the aforementioned Monte Carlo methods to overcome computational difficulties. Another application is the estimation of human fecundability, or the per-cycle probability of a recognizable conception. As there is considerable variability among couples in their waiting times to pregnancy, previous work has focused on parametric models to account for this heterogeneity and to estimate the population distribution of fecundability. We propose an alternative approach that does not require that the distribution follow a particular parametric form. One of the major difficulty in our approach is computational. The new Monte Carlo methods can be suitably applied. This type of ""Bayesian nonparametric problem"" has long been a topic for theoretical statisticians. Recent development of novel Monte Carlo methods in computation boomed its applicability. Our final project contains some ideas on how to use Monte Carlo samples more efficiently. It has long been recognized that our computational ability is directly linked to our theoretical thinking. The new computational methods presents us many interesting theoretical questions. We plan to study some of these questions, one of which is concerned with the relative efficiencies and mathematical properties of the two methods, another is concerned with how we can make better use of the samples obtained from Monte Carlo simulations."
"9408158","Mathematical Sciences: A Projection Pursuit Estimator for   Ordinal Contingency Table Cell Probabilities","DMS","STATISTICS","07/15/1994","07/15/1994","Jianping Dong","MI","Michigan Technological University","Standard Grant","Sallie Keller-McNulty","06/30/1995","$13,639.00","","jdong@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","MPS","1269","0000, 9221, OTHR","$0.00","  9408158  Dong     The purpose of the proposed research is to obtain a projection  pursuit estimator for cell probabilities of high-dimensional  ordered contingency tables.  Discrete data often appear in the  form of contingency tables.  For instance, consider the  association among smoking statuses, age and breathing test  results.  A table consisting of the number of individuals in each  category is a three-dimensional ordinal contingency table.  The difficulty of estimating cell probability of high-dimensional  tables is the lack of data.  A sample of 100 observations in a  one-dimensional table with 10 categories has an average cell  count of 10, but it only has an average cell count of 0.001 in a  five-dimensional table with the same number of categories in  each dimension. All the existing estimators share a common  problem known as the curse of dimensionality, which is caused by  the lack of data in high-dimensional tables. The proposed  research studies the features of high-dimensional data by looking  at its one-dimensional projections, which are no longer sparse.   A one-dimensional table is defined as a projection of a  d-dimensional table along an arbitrary direction.  An index is  introduced to evaluate the projections.  Interesting features can  be uncovered by using smaller smoothing parameters without  sacrificing the variance of the estimator -hus overcoming the  problem of the curse of dimensionality.  ***"
"9404396","Mathematical Sciences:  Principles and Practices of Applied Statistics","DMS","STATISTICS","07/01/1994","06/17/1998","Arthur Dempster","MA","Harvard University","Continuing Grant","Rong Chen","06/30/1999","$281,103.00","","dempster@fas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","0000, OTHR","$0.00","The project is designed to follow up projects carried out under  NSF Grant DMS-90-03216 (Data Analysis, Modelling, and Inference)  and to intitiate and carry out research of the same types as  under the previous grant. These projects are of two basic types.  The first type seeks to develop new statistical techniques in the  context of several case studies, with the goal of integrating  tools of various sorts, including study design, data analysis,  modelling, and inference (mainly Bayesian inference). The  projects includeJ a study of estimating state level employment  and unemployment rates, modelling and inference concerning  dynamic patterns of hormone concentrations in blood plasma, a  study of hospital ICUs, and other social , environmental or  geophysical processes. The second basic type of study is  theoretical analysis of methodological issues, including  understanding and improving statistical methods in the face of  low frequency time series behavior, addressing the logic of model  selection, studying design and analysis issues under weighted  sampling, studying the justification and computation of belief  function inferences, and tracing the history of contentious  issues in 20th century statistics, mostly centering on  probabilistic inference.    Statistical studies are essential contributors to scientific  knowledge in virtually every branch of physical, biological, and  social sciences. The nature of these studies is changing rapidly,  in large part because computers have made possible the collection  and analysis of large quantities of information, and also have  revolutionized the study of the mathematical theories and laws  that govern statistical phenomena. Consequently, there is a  continuing and growing need to develop and assess the methods  that statistical scientists rely upon to make sense of data and  to package their findings for practical purposes, such as  assessing long term trends in global warming, aiding doctors to  make good decisions about medical treatments, or  providing  accurate and timely information about the economy for government  poicymakers. A basic strategy adopted in the project is that  technical tools should be developed in the framework of specific  scientific problems, and through collaborative efforts with  scientists actively working on such problems, in order to  guarantee the relevance of the procedures and models developed.  Also, since statistical methodology has features that are shared  in common across very different fields of science, it is  important to pursue a second strategy of addressing and improving  problems that arise in the same form in these different fields.  In statistics, these problems center around coping with  uncertainties about measurements or about the consequences of  decisions that might be taken on the basis of uncertain  knowledge. The project addresses such problems through techniques  that rely on the mathematical theory of probability. It is  important to refine and clarify the principles and techniques  that govern the selection and use of such mathematics."
"9400024","Mathematical Sciences: Probability Measures on Vector       Spaces; Basic Results and Application","DMS","PROBABILITY, STATISTICS","07/15/1994","01/30/1996","James Kuelbs","WI","University of Wisconsin-Madison","Continuing Grant","K Crank","06/30/1997","$80,000.00","","kuelbs@math.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1263, 1269","0000, OTHR","$0.00","This continues research involving limit theorems for real and  vector valued random variables.  Problems related to the central  limit theorem and the law of the iterated logarithm motivated by  trimming and self-normalizations will be studied.  Limit sets for  random samples of processes, as well as coverage problems will be  examined.  A primary focus will be to further examine the link  between the small ball problem for Gaussian measures and metric  entropy problems.  Empirical processes and precise large  deviation probabilities will also be considered.    A central theme in probability, and throughout its diverse  applications to problems in engineering, physics, and statistics,  is the study of limit theorems for partial sums of independent  random variables in both finite and infinite dimensional  settings.  This research place emphasis on the infinite  dimensional situation and involves a blend of functional analysis  and probability theory.  Current work has fallen into several  different areas, and includes small ball and large deviation  probabilities for Gaussian measures, limit sets for random  samples and related coverage problems, and Gaussian chaos  applications to multiple Wiener-Ito stochastic integrals."
"9404479","Data Analysis Using Finite Mixture Models","DMS","STATISTICS","07/01/1994","01/17/1997","Donald Rubin","MA","Harvard University","Standard Grant","James E. Gentle","06/30/1998","$116,000.00","Hal Stern","donald.rubin@temple.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385369","6174955501","MPS","1269","0000, OTHR","$0.00","Finite mixture models are increasingly popular in the social and  medical sciences for analyzing data thought to arise from a  population consisting of categorical types. This research project  considers Bayesian methods for analyzing data using finite  mixture models. The application of classical methods to such  models is difficult because the likelihoods for such models are  nonstandard:  they are inherently multimodal due to symmetries in  the labeling of the mixture components, may be multimodal even  for a single fixed labeling of the components, and fail to  satisfy the regularity conditions required for classical  likelihood ratio tests. The principal scientific questions of  interest concern drawing inferences about model parameters,  determining the number of mixture components, and stochastically  classifying sampling units into the mixture components. Modern  advances in statistical computing, such as the EM and ECM  algorithms, data augmentation, and Gibbs sampling, are used to  obtain inferences about the posterior distribution of model  parameters; however special care is needed in applying these  methods because of the difficulties mentioned above. This  research describes methods for distinguishing between the two  types of modes described above, and for carrying out the data  analysis given the existence of multiple modes. Draws from the  posterior distribution of the model parameters can be used to  obtain draws from the posterior predictive distribution of  replicate experiments similar to the current experiment. The  posterior predictive distribution of test statistics, or of other  discrepancy measures, can be used to evaluate the fit of a model,  e.g., comparing two component and three component mixture models  even though the problem is irregular. Additionally, averaging  over a prior distribution on plausible alternatives to the  existing model can be used to estimate the sample size required  to assess the appropriateness of the existing model against such  alternatives  and therefore can be used to inform design  decisions.    It is not uncommon to consider a particular class of statistical  models, called mixture models, that assume the population of  interest consists of a number of relatively homogeneous  subpopulations. Mixture models are useful when a relatively  complex model would be required to describe the pattern of data  that is observed within the entire population, whereas a  relatively simple model applies within each subpopulation.  Classical approaches are of limited use in such cases, e.g.,  classical methods do not apply to the crucial question of  determining whether and how many subpopulations are in evidence.  This research proposal aims to develop new methods for analyzing  data using mixture models and for assessing the adequacy of such  models. These new methods take advantage of recent theoretical  and computational advances to draw accurate inferences about the  important features of the mixture models. The basic approach is  to average over all descriptions of the population that are  supported by the data and thereby provide an accurate assessment  of the variation and patterns to be expected in the population."
"9457824","NSF Young Investigator","DMS","STATISTICS","08/15/1994","05/08/1996","Andrew Gelman","CA","University of California-Berkeley","Continuing Grant","James E. Gentle","07/31/1997","$97,344.00","","gelman@stat.columbia.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, 9297, OTHR","$0.00","To: From: aizenman@nsf.gov (Alan J. Izenman) Subject: Gelman.NYI Abstract Cc: Bcc: X-Attachments:  This research, supported by a National Science Foundation Young Investigator Award, will focus on the theory, computation, and testing of Bayesian probability models, and on the study of electoral systems, the effects of redistributing, and voting behavior.   The National Science Foundation Young Investigator Award recognizes outstanding young faculty. This award recognizes the recipient's strong potential for significant development as a teacher and academic leader."
"9404267","Mathematical Sciences:  Statistical Problems in Genetics    and Molecular Biology","DMS","STATISTICS","07/15/1994","04/18/1996","Terence Speed","CA","University of California-Berkeley","Continuing grant","James E. Gentle","06/30/1998","$162,000.00","","terry@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","The project's general aim is to bring the tools and techniques of  modern statistics to bear on a variety of problems from genetics  and molecular biology. The specific objectives of this project  are (i) to develop a suitable statistical model and method of  analysis for determining which of a given set of DNA markers come  from the same chromosome, based on data obtained from scoring  hybrid cell lines; (ii) to extend and refine statistical methods  designed to ascertain overlap of cloned DNA fragments; (iii) to  develop accurate statistical methods for recognizing the  resemblance of a new protein to a known structure or structural  motif, on the basis of its amino acid sequence; (iv) to extend  the theory of linear invariants for use in general phylogenetic  inference; (v) to derive statistical models for the evolution of  viral genomes; and (vi) to develop a statistical model for the  evolution of certain repetitive DNA elements in human  populations.    The aim of this project is to develop and use statistical methods  for analysing a variety of forms of molecular genetic data.  Included in the scope will be data collected on individuals, on  single chromosomes, on small cloned DNA fragments, on amino acid  sequences of proteins, and on DNA sequences of genes. Such data  invariably exhibits some form of randomness, arising from its  manner of collection, the location of the DNA in the genome, and  the evolutionary history of the molecules. Statistical methods  are needed to cope with this randomness, and to permit inferences  to be made concerning questions such as the relative or absolute  location of genetic or other markers on the chromosomes, and the  nature of the evolution of the genes or genomes."
"9404114","Mathematical Sciences:  Theory of Statistical Estimation    and Stochastic Models for Ion Channels","DMS","STATISTICS","08/01/1994","07/21/1994","Lucien Le Cam","CA","University of California-Berkeley","Standard Grant","James E. Gentle","07/31/1998","$96,000.00","","","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","   Sodium channels are large proteins that traverse the membranes of nerves. They are responsible for controlling the motion of sodium ions into the nerve cell and thereby play a major role in generation of nerve signals. With adequate, sophisticated, apparatus, one can take recordings of the successive passages of a ionic channel into open or closed position. Stochastic models have been suggested to explain the pattern of open and closed periods. They contain a number of adjustable parameters, which can be varied to obtain a good fit to experimental data. The methods of fit used up to now require extensive computer use. Also, they are unstable. We intend to taylor to this problem a simpler method described in 1990 by L. Le Cam and G.L. Yang The problem of assessing the uncertainty in the fitted equations is part of a general problem on which we are making some progress. The project has two parts. One is a detailed study of certain integrals that occur naturally in the evaluation of the performance of statistical estimates when one uses quadratic los functions. The integrals involve a denominator that is a convex combination of the probability measures attached to the experiment. One will study how the integrals vary when these convex combinations change. The second part is about statistical procedures to be used for the study of sodium channel in nerves. Various stochastic models for the functioning of these channels have been proposed. Standard statistical analyses proceed through the method of maximum likelihood, which, in this case is computationally cumbersome and unstable. Another procedure, due to Le Cam, is just as efficient and computationally much simpler. It is proposed to taylor it to the sodium channels situation and study what it can achieve there."
"9404305","Mathematical Sciences:  Using Inference from Simulation     to Improve Efficiency of Simulations","DMS","STATISTICS","07/01/1994","07/14/1994","Andrew Gelman","CA","University of California-Berkeley","Standard Grant","James E. Gentle","06/30/1997","$45,000.00","","gelman@stat.columbia.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","9218, HPCC","$0.00","   9404305   Gelman     Iterative simulation methods such as the Gibbs sampler and Metropolis' algorithm have recently become popular tools in statistical analysis, especially in the calculation of posterior distributions arising in Bayesian inference. We consider methods for using the output from parallel runs of iterative simulation to perform inference about the target distribution and the transition probabilities. These inferences can then be used to adaptively alter the simulation algorithm to speed convergence. Developing and evaluating these methods involves four research objectives. First, conjectures in the mathematical theory of Markov chain simulation--notably on the generality of some results so far established for the normal distribution--need to be tested. Second, specific adaptive algorithms must be developed, with care taken that adaptive altering of the transition probabilities does not violate the conditions for convergence. Third, the methods of inference from iterative simulations work most effectively when based on parallel simulations; we plan to set up an asynchronous parallel processing setup, in which the independently running parallel processes would send results to a single central processor that would be continually performing inference about the target distribution and periodically send commands outward to alter the transition rules. Fourth, the methods are intended to be applied to speed the computation of posterior distributions in applied Bayesian hierarchical models. Computer simulations of random walks have become increasingly useful in many areas of science and statistics. The methods we are studying to speed the computations, based on basic mathematical and statistical ideas, were inspired by statistical computation for a variety of applications, including forecasting Presidential elections, analyzing nonresponse in sample surveys, searching for the factors that influence high radon levels in homes, and modeling the flow of toxic chemicals in the body. All of  these problems have been attacked with random walk computer simulations; improvements in computation speed will allow us and others to fit more complicated and accurate mathematical models."
"9404477","Studies in Efficient Design of Experiments","DMS","STATISTICS","07/15/1994","04/08/1996","Ching-Shui Cheng","CA","University of California-Berkeley","Continuing grant","James E. Gentle","06/30/1998","$51,000.00","","cheng@stat.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1269","0000, OTHR","$0.00","The Principal Investigator plans to study several problems in  design of experiments.  The projection properties of orthogonal  arrays and some other designs will be investigated.   Recent research has shown that interesting projection properties  of certain orthogonal arrays have important statistical  implications.  The newly introduced concept of estimation  capacity, a measure of the capability of a design to handle and  estimate different potential models involving interactions, will  also be investigated.  In addition to factorial designs, the  Principal Investigator will study optimal and efficient  regression designs under random block-effects models.  Other  proposed research includes the Principal Investigator's  collaboration with Professor Ker-Chau Li on the study of  dimension reduction techniques for the analysis of data from  designed experiments.  Such techniques, in conjunction with  dynamic graphics, can help extract additional useful information  that may not be provided by traditional analyses.  The resulting  design issues will also be studied.    Experimental design is used extensively in a wide range of  scientific and industrial investigations.  In industrial  experiments, often a large number of factors have to be studied,  but the experiments are expensive to conduct.  Fractional  factorial designs are particularly useful to handle this problem.   In recent years, factorial designs have received considerable  attention, mainly due to the Japanese success in applying them to  improve quality in industrial manufacturing.  This re-surgence of  interest provides an opportunity to re-examine some conventional  wisdom.  This will be given the highest priority in the Principal  Investigator's research activities during the coming years.  In  addition to theoretical studies, good designs will also be  constructed for experimenters' use.  Other proposed research  includes a problem arising from a recent optometry experiment,  and the study of the performance of a new data-analyt ic tool on  experimental data."
"9406348","Mathematical Sciences: Program in Mathematics and Molecular Biology","DMS","COMPUTATIONAL BIOLOGY ACTIVITI, Genetic Mechanisms, GENE EXPRESSION, INFRASTRUCTURE PROGRAM, GEOMETRIC ANALYSIS, APPLIED MATHEMATICS, TOPOLOGY, STATISTICS, COMPUTATIONAL MATHEMATICS, THEORY OF COMPUTING","09/01/1994","08/18/1998","Nicholas Cozzarelli","CA","University of California-Berkeley","Continuing grant","Michael Steuerwalt","08/31/2000","$2,400,000.00","","","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1107, 1112, 1154, 1260, 1265, 1266, 1267, 1269, 1271, 2860","9125, 9179, 9183, 9216, 9263, BIOT, HPCC, SMET","$0.00","9406348  Cozarelli         The Program in Mathematics and Molecular Biology (PMMB) promotes  the discovery and expansion of the applications of mathematics,  including statistics and computer science, to molecular biology Because  of its precise measurement techniques, molecular biology appears now in  the position that physics was a century ago, ready for an explosion of  productivity and new insights arising from interactions with  mathematics.       Although administered through the University of California at  Berkeley, this is a Program without walls, a group unified by a common  goal. PMMB enables the collaboration of experimental biologists with  mathematicians, professions often viewed as disparate. PMMB members  apply state-of the-art mathematics and computational techniques to the  design and analysis of biology experiments. PMMB provides a forum for  mathematicians and biologists with interesting problems and a  commitment to the application of mathematics to molecular biology.       The Program uses three major mechanisms to achieve this  interdisciplinary collaboration. (1 ) An active fellowship program and  well-attended annual scientific meetings draw particularly the younger  scientists into the area of study and provide support for  interdisciplinary training. (2) The Program actively promotes the  education of the general mathematical and biological audiences on the  potentialities of the field through presentations and tutorials at  national meetingst focused small meetings, individual seminars,  courses, and didactic articles. (3) The Program provides modest direct  support for interdisciplinary studies by the members, their students  and postdoctoral associates, fostering a network of interactions among  the members for productive collaborations."
"9596094","Mathematical Sciences: Investigations in Mathematical       Statistics","DMS","STATISTICS","11/01/1994","04/14/1995","Lawrence Brown","PA","University of Pennsylvania","Continuing grant","James E. Gentle","12/31/1996","$66,900.00","","lbrown@wharton.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, OTHR","$0.00",""
"9404150","Mathematical Sciences:  Projective Score Methods and        Approximate Conditional Inference","DMS","STATISTICS","07/01/1994","05/27/1994","Richard Waterman","PA","University of Pennsylvania","Standard Grant","James E. Gentle","06/30/1997","$75,000.00","","","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1269","0000, OTHR","$0.00"," A common theme to the areas of research described in this proposal is the presence of nuisance parameters and the necessity of  removing an estimation bias in the parameter of interest, induced by the estimation of the nuisance parameters. The reason why the bias must be removed is that it may lead to inconsistency in the maximum likelihood estimator of the parameter of interest. The projects all use a projective approach to modify the score function for the parameter of interest, so that the bias attributable to the estimation of the nuisance parameter is reduced. The benefits of reducing bias will transfer to more accurate parameter estimates, confidence intervals with better coverage rates and test statistics that for small sample sizes have distributions ""closer"" to their asymptotic limiting distributions. We live in an age where information, that is data, is abundant. Our statistical understanding of this data arises from making accurate models. The large data sets that we have at hand show that simple models rarely fit the data well, and so we are led to consider more complex models with many unknowns. The proposed research will address the practical implementation and interpretative issues involved with these rich statistical models involving many variables."
"9316624","Mathematical Sciences: Global Ocean Modeling and Estimation for Climate Forecasting","DMS","APPLIED MATHEMATICS, STATISTICS, COMPUTATIONAL MATHEMATICS, PHYSICAL OCEANOGRAPHY","03/01/1994","03/28/1994","Alan Willsky","MA","Massachusetts Institute of Technology","Standard Grant","Michael H. Steuerwalt","08/31/1996","$80,000.00","","willsky@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1266, 1269, 1271, 1610","0000, 1326, 9263, GLCH, OTHR","$0.00","9316624  Willsky       A new prognostic model of the global ocean circulation is  being developed based on the Navier-Stokes equations, using  state-of-the-art parallel computers and languages with an  ultimate goal of creating a global ocean modeling/estimation tool  that can be used for climate research.  A key element of this  endeavor is the development of statistical estimation algorithms  for the assimilation of data into the forward model.  The  combined forward/estimation model could be used to estimate the  state of the ocean form observations constrained by dynamics.   The data of concern is sparse, heterogeneous and multiresolution  and the associated global ocean circulation models are of  gargantuan size, having upwards of 10 ~ 10 degrees of freedom.   The resulting estimation problem is of such a large size that  standard computational procedures become prohibitive. Novel  approaches to statistical modeling and the circulation models are  needed.  We proposed the first steps in this direction through  the establishment of a collaboration involving physical  oceanographers and probabilistic modeling and statistical  inference specialists.  The short term focal point of this  collaboration is the definition and solution of a series of  subproblems designed to foster understanding insight, and mutual  communication and in the process advance the state of the art in  both data assimilation for global ocean modeling and statistical  modeling and estimation of random fields.  ***"
"9410984","Mathematical Sciences:  Bayesian Inference for Nonparametric Regression in Generalized Linear Models","DMS","STATISTICS","09/15/1994","06/09/1994","Nandini Raghavan","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","James E. Gentle","02/28/1997","$16,381.00","","nandini@ornarose.com","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1269","0000, 9221, OTHR","$0.00","9410984  Raghavan       The focus of this proposal is the modeling of the  functional dependence of non-Gaussian response variables on a  given set of predictors. Examples of this include the logistic  regression model, where the response is binary and the log-linear  Poisson model where the response represents counts of incidents  in a given time interval. The function of interest is estimated  nonparametrically, using generalized smoothing splines.   Inferential procedures based on a Bayesian formulation, wherein a  stochastic process prior is imposed on this function, will be  investigated.  A more complete answer requires finite sample  calculations which is a computationally imposing task. Monte  Carlo methods for analysis of the posterior will be explored.  ***"
"9409273","Research Planning Grant:  Data-driven Modeling and          Forecasting of Nonlinear Time Series Systems","DMS","STATISTICS","07/15/1994","07/15/1994","Bonnie Ray","NJ","New Jersey Institute of Technology","Standard Grant","Sallie Keller-McNulty","12/31/1995","$17,960.00","","borayx@m.njit.edu","University Heights","Newark","NJ","071021982","9735965275","MPS","1269","0000, 9221, OTHR","$0.00","9409273  Ray       The author proposes to investigate several open problems in  nonlinear time series analysis, with the goal of combining  techniques from nonparametric regression modeling and linear time  series analysis and extending them to the nonlinear time series  framework. In particular, the author plans to investigate the  appropriateness of nonparametric regression techniques for  modeling nonlinear processes having very slowly decaying serial  correlation, known as long-range dependence.  Modifications of  existing procedures that take into account the amount of  correlation in a series will be investigated.  Additionally,  the author will investigate appropriate methods of testing  model assumptions underlying the use of nonlinear regression  algorithms when the algorithms are applied to time series.       The author also plans to extend existing methodology  concerning tests of nonlinearity in univariate series to the  multivariate case. Data-driven methods for obtaining full  multivariate nonlinear models will be investigated.  Finally, the  author will consider the advantages, in terms of forecasting,  of using a data-driven nonlinear model for data which suggests  a nonlinear generating process.  ***"
"9414193","Proposal to Fund 1994 IEEE Information Theory Workshop on Information Theory and Statistics","DMS","STATISTICS","07/15/1994","07/11/1994","Geoffrey Orsak","VA","George Mason University","Standard Grant","Sallie Keller-McNulty","01/31/1995","$5,000.00","","gorsak@engr.smu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1269","9218, HPCC","$0.00","The 1994 IEEE/IMS Information Theory Workshop on ""Information Theory and Statistics"" will present papers in the related fields of (1) Stochastic complexity and universal data compression, (2) randomization complexity and information theory, (3) Markov random fields, (4) Vector quantization, classification, and regression trees, (5) theory and applications of wavelets, and (6) nonparametric function estimation. In addition to these specific topics, there will be two open poster sessions supporting research in all fields related to the theme of the workshop. The workshop will be held in the Old Town Holiday Inn, Alexandria, Virginia over the dates of October 27 through October 29, 1994. The IEEE Information Theory Workshops play an important role in exploring and supporting critical research trends in the broad areas of communication system utilization and digital data storage. This area of research is commonly referred to as the field of Information Theory and over the last forty years has played a central role in the development of the nation's communications infrastructure. Recently, there has been a significant rise in common research conducted by Information Theorists and Statisticians. It is the aim of the 1994 IEEE/IMS Information Theory Workshop on ""Information Theory and Statistics"" to enhance this work and improve the cross-fertilization of ideas between the respective research communities. To support this collaboration, this workshop is being co-sponsored by the IEEE Information Theory Society and the Institute of Mathematical Statistics."
"9400023","Mathematical Sciences: Computing Science and Statistics; Symposium on the Interface Theme; Research Triangle Park, N.C.; June 15-18, 1994","DMS","STATISTICS","06/01/1994","05/18/1994","John Sall","VA","Interface Foundation of North America Inc","Standard Grant","Sallie Keller-McNulty","05/31/1995","$15,000.00","","","P.O. Box 7460","Fairfax Station","VA","220397460","7039931691","MPS","1269","0000, OTHR","$0.00","9400023 Sall The Interface Conference was designed to bring together professionals from different fields that share a need for better statistical computing methods. Originally focused on the interface with numerical analysis and computer science, in more recent years it has been involved with a broad range of scientific and technical areas, particularly hot areas such as biotechnology and quality engineering. Computing is currently a very active area in the statistics profession as desktop computing power has made a huge advance in the last 5 years. For example, Bayesian statistics used to be mostly theory and little practice, because it was saddled with difficult numerical integration issues. Today, the emergence of Monte Carlo methods coupled with fast machines has made the techniques suddenly practical. Researchers are asking for the distribution of estimates, rather than just approximate standard errors, and a number of resampling-based methods have emerged to provide those answers. Exact significance levels are replacing approximate ones. Smooth curves are replacing straight line fits. Statistical graphics is helping researchers make discoveries. The Interface Conference will have the specialized sessions on the state-of-the-art, and also a number of tutorial sessions and short courses to help bring the new technology to a general technical audience. The Interface conference is the one annual conference dedicated to statistical computing issues. Statistical computing has emerged as a particularly important field in the last few years because the arrival of cheap and fast desktop computers has opened up a lot of new opportunities that were only dreamed of a few years ago. A generation ago, the best statisticians could do with limited computing resources was to get a statistic if the data was organized a certain way, and look that statistic up in a table. Then computing and numerical techniques improved enough to handle more general models and get significance levels. Now we have progressed to where we can analyze very general models that require fewer assumptions, have a more flexible functional form, provide details on the distribution of the estimates in the model, use interactive exploratory techniques to make new discoveries, and present the results with graphical visualization techniques. With further refinements in statistical computing tools, the statistical community has a chance to really serve the scientific and technical world much better at a time when the Quality Movement, the Biotechnology Revolution, and the Environmental mandate has made statistics and experimentation more critically important. The Interface conference provides the forum for presentation and discussion of the latest breakthroughs in statisical computing."

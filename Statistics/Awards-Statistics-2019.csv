"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1916199","Collaborative Research: Integrating multi-dimensional omics data for quantifying disease heterogeneity","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","08/15/2019","08/07/2019","Jian Huang","IA","University of Iowa","Standard Grant","Yong Zeng","07/31/2022","$120,286.00","","jian-huang@uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1269, 7454","068Z","$0.00","Many complex diseases such as cancer demonstrate significant across-patient heterogeneity. For a better understanding of disease biology and optimally selecting treatment strategies, it is important to properly model disease heterogeneity.  This project will develop a novel framework for modeling disease heterogeneity through the effective integration of information from multiple types of highly complex omics measurements.  The proposed analysis framework and approaches will have significant broader impact.  Applications of the methods will lead to more accurate identification of heterogeneous patient groups as well as their omics characteristics, which will facilitate the identification/definition of disease subtypes, treatment selection, and clinical decision-making.   Data on skin and lung cancer will be analyzed leading to heterogeneity models that will be valuable to basic science researchers and clinicians.  The project also involves education and training of graduate students at Yale University and the University of Iowa.<br/><br/>High-dimensional omics data have been shown to be highly effective for heterogeneity analysis. Taking advantage of recent developments in multi-dimensional profiling under which data are collected on multiple types of omics measurements, the investigators will systematically develop novel integrated analysis strategies and approaches. Specifically, three sets of methods will be developed under the novel PFR (penalized fused regression) framework. Model averaging will be further developed to facilitate computation and provide additional insights into the proposed approaches. Extensive and rigorous methodological, computational, and theoretical investigations will be conducted.  This project will make fundamental contributions to high-dimensional statistics and disease heterogeneity analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916411","Novel Statistical Methods for Modeling Population Dynamical Systems","DMS","STATISTICS","09/01/2019","08/16/2021","Xiao Song","GA","University of Georgia Research Foundation Inc","Continuing Grant","Pena Edsel","08/31/2022","$124,999.00","Hanwen Huang","xsong@uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1269","","$0.00","Mathematical models have been increasingly integrated in infectious disease studies in order to provide a quantitative understanding of virus infection and disease processes. Among the mathematical models, the ordinary differential equation (ODE) is a simple but powerful framework for modeling the dynamics of complex systems. Parameters in ODE models often have scientific meanings. Most of the previous research in this area focused on estimating ODE parameters from a single subject. This is not an efficient approach, because the data are often collected on multiple subjects. The objective of this project is to develop efficient methods for analyzing ODE systems by combining data from multiple subjects. The proposed research is expected to have broad impacts and application in biomedical studies, ecology, and other scientific areas.<br/><br/>Models that can characterize the common features in the population will be considered while taking into account the variations among subjects. Efficient approaches for model selection will also be investigated. Both statistical theory and computational algorithm will be developed to tackle challenges in this area. Results from this research will provide new insight into the existing methods and inspire new lines of investigations in analyzing complex dynamic systems using ODE model. Extensive numerical studies will be conducted, which will help interested researchers better understand the proposed methods.  The research will be integrated with various educational activities that will impact teaching and learning related to dynamic modeling.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916170","Two-Phase Sampling with Interval-Censored Data","DMS","STATISTICS","09/01/2019","08/19/2019","Qingning Zhou","NC","University of North Carolina at Charlotte","Standard Grant","Yong Zeng","08/31/2023","$120,000.00","","qzhou8@uncc.edu","9201 UNIVERSITY CITY BLVD","CHARLOTTE","NC","282230001","7046871888","MPS","1269","","$0.00","Interval-censored failure time data arise when the time to a failure event is not exactly observed but known or observed only to fall within some time interval. Such data are collected in many fields, including epidemiology, medicine, demographics, economics, finance, psychology and social sciences. This research will propose new cost-effective sampling designs with interval-censored failure time data, in response to the needs of cost reduction and improved power for detecting the exposure-failure-time relationship in epidemiological and biomedical studies. The proposed designs coupled with robust and efficient statistical methods will enable study investigators to collect more informative samples and produce more precise statistical inference at a fixed budget, and they will be especially useful when the failure rate is low and the exposure variable is expensive or difficult to obtain. The graduate student support will be used for theoretical investigation of the proposed methods as well as conducting numerical studies and manuscript preparation.<br/><br/>The new designs proposed in this research are generally two-phase biased-sampling schemes where one observes the expensive exposure at phase II with a probability depending on the phase I information collected on failure times, cheap covariates, or auxiliary variables. All proposed designs will produce biased-sampled interval-censored data with missing covariates. No methods available in the literature can appropriately handle such data. This research will develop robust and efficient semiparametric likelihood-based methods that properly account for the proposed sampling schemes. Nonparametric techniques such as sieve and kernel estimation will be employed. New arguments based on modern empirical process theory will be developed for theoretical investigation of the proposed methods. Efficient computational algorithms and user-friendly software that implement the proposed designs and methods will be created and disseminated. Successful completion of the proposed research will have a significant impact on how future cost-effective epidemiological and biomedical studies be conducted and how data from these studies be efficiently analyzed. This research will also have potential impact on how to perform cost-effective studies and data analyses in other fields that produce interval-censored failure time data, such as demographics, economics, finance, psychology and social sciences.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1911715","The SRCOS Summer Research Conference in Statistics and Biostatistics","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","05/15/2019","05/09/2019","Arnold Stromberg","KY","University of Kentucky Research Foundation","Standard Grant","Pena Edsel","04/30/2020","$46,340.00","","stromberg@uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","1260, 1269","7556, 9150","$0.00","Summer Research Conference (SRC) in statistics and biostatistics, sponsored by the Southern Regional Council on Statistics (SRCOS), is held at General Butler State Resort Park in Carrolton, Kentucky on June 2-5, 2019.  The 2019 SRC is the 55th anniversary of that conference.  Its purpose is to encourage the interchange and mutual understanding of current research ideas in statistics and biostatistics, and to give motivation and direction to further research progress.  The project focuses on young researchers, placing them in close proximity to leaders in the field for person-to-person interactions in a manner not possible at most other meetings.  Speakers will present formal research talks with adequate time allowed for clarification, amplification, and further informal discussions in small groups. To enhance the recruitment and retention of under-represented groups in STEM fields, SRCOS has added an undergraduate component, Statistical Undergraduate Research Experience (SURE). The SURE encourages involvement of under-represented minorities, presents career opportunities in Statistics, provides a Real Data Analytics Workshop, as well as a graduate school information session.  Under the travel support provided by this award graduate students attended and presented their research in posters which are reviewed by more experienced researchers, while undergraduate students have been exposed to data analytics, graduate school opportunities in statistics and biostatistics, and career opportunities available.<br/><br/>The Summer Research Conference (SRC) in statistics and biostatistics  is an annual conference sponsored by the Southern Regional Council on Statistics (SRCOS). The SRCOS is a consortium of statistics and biostatistics programs in the South, stretching as far west as Texas and as far north as Maryland.  This project is funding student participation in the 2019 Summer Research Conference (SRC) sponsored by SRCOS.  The meeting is particularly valuable for students and faculty from smaller regional schools at drivable distances, affording them the opportunity to participate and interact closely with internationally-known leaders in the field without the cost of travel to distant national or international venues.  This conference is strengthening the research of the statistics and biostatistics community as a whole, and particularly focuses on reaching under-represented undergraduate students with Statistics Undergraduate Research Experience (SURE 2019).  For all details, please consult the conference link at https://www.srcos.org<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915277","Collaborative Research: Spectral Functional Principal Components on Abelian Groups with Applications to Spatial Functional Data","DMS","STATISTICS","08/01/2019","07/22/2019","Joshua French","CO","University of Colorado at Denver","Standard Grant","Yong Zeng","07/31/2023","$80,000.00","","joshua.french@ucdenver.edu","13001 E 17TH PLACE F428","AURORA","CO","800452571","3037240090","MPS","1269","","$0.00","Massive data sets on gridded 2D and 3D domains have recently become available through computer climate model outputs, records from satellite remote sensing and brain scans, among others. These data sets have both temporal and spatial dimension. For example, a state of vegetation is observed on a grid covering an agricultural area at regular time intervals, every day or every week.  Such data can be viewed as functions of time, one function per spatial grid unit. Their chief characteristic is the spatial dependence of curves observed at the grid nodes. There is an increasing need to develop statistical tools, which will allow researchers to extract useful information from such data. The PIs will develop such tools.  The data and problems that motivate this research arise in several science fields, which have important impacts on society. For example, conclusions drawn from future climate models help the government and corporations plan for the allocation of various assets. Brain research on trauma experienced by military veterans and on Alzheimer's disease are recognized as important societal goals. The statistical research the PIs will conduct will provide useful quantitative tools to help scientists in these fields. Mathematical foundations of the new approach will be created, together with domain-specific approaches. The new methods will be implemented in R packages and made available to research community, government agencies and commercial enterprises. In the course of the proposed research, two Ph.D. students will be trained. <br/><br/>The PIs will create a new framework for inference for functional data defined on domains with an additive group structure. The new dimension reduction approach will have characteristics of a multi-scale, data-driven representation, which takes into account the dependence of the functions defined on group elements, for example spatial grid nodes. The PIs will use methods of Fourier analysis on Abelian groups, spectral theory for functional data, invariance principles in Hilbert spaces, computationally efficient spatio-temporal spline representations, routines for downloading and manipulating massive data sets. The PIs will develop several inferential procedures, including bootstrap-based inference, tests for the spatial and distributional structure, and applications to the evaluation of the accuracy of computer climate models. The PIs will also develop corresponding computational techniques, which will lead to the computationally fast representation of various data structures of large to massive size.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915855","A Unifying Framework for High-Dimensional Additive Modeling","DMS","STATISTICS","09/01/2019","07/25/2019","Noah Simon","WA","University of Washington","Standard Grant","Yong Zeng","08/31/2023","$219,993.00","Ali Shojaie","nrsimon@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1269","","$0.00","Recent technological advances have provided researchers with a wealth of data. Unlocking the potential of these vast data sources often involves linking a response variable (e.g., the presence of a disease, or a positive response to a treatment) to a large number of potential features of interest (e.g. gene expression levels or activities of brain regions). It is increasingly common to measure a large number of features (thousands or more), on a small number of subjects (or patients). Generally, only a small number of these features are related to the response variable. To determine those relevant features, statistical/machine-learning modelling algorithms are used to automatically select a small subset of features that are most predictive of response. Many of these algorithms enforce strong restrictions on the models they allow; e.g. linearity. In scientific domains, where only a small number of subjects are measured, these restrictions can be useful: building more complex models requires more subjects. However, they are sometimes overly restrictive. In this project, a framework to estimate less restrictive (additive) models that employ variable selection in high-dimensional problems will be developed. This framework will help overcome a number of computational challenges. It will additionally lay a theoretical foundation for analyzing the statistical behavior of such high dimensional estimators (that will include the impact of finite computational resources). A publicly available software implementation for flexible high-dimensional modeling will also be developed.<br/><br/>This project engages seminal questions in nonparametric estimation and penalized regression. Generally, the computational and theoretical challenges of sparse nonparametric regression in high dimensions are studied separately: Iterative algorithms are constructed that eventually get within a prespecified tolerance of the minimum, while statistical properties (e.g. convergence rates) of the exact minimizer are studied. In addition, for non-parametric problems, existing theoretical studies have often focused on statistical properties when the structure implied by the objective (e.g., sparsity) holds exactly. This project aims to merge the study of computational and statistical optimality, in the setting of high-dimensional additive, and more general nonparametric, models. More specifically, it aims to analyze the statistical properties of approximate, rather than exact, minimizers; from there, it characterizes the number of descent iterations needed to obtain estimators with optimality guarantees. In addition, the project aims to extend these ideas to settings where the structure/smoothness may be misspecified. To address these challenges, the project brings together ideas from convex optimization, empirical process theory, penalized regression, and approximation theory, and will serve as a template for engaging those bodies of knowledge together.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915845","Nonparametric Curve Estimation in Presence of Missing Data","DMS","STATISTICS","09/01/2019","07/29/2019","Sam Efromovich","TX","University of Texas at Dallas","Standard Grant","Yong Zeng","08/31/2024","$190,000.00","","efrom@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1269","","$0.00","The project focuses on statistical activities motivated by medical and engineering applications in the presence of missing data, a familiar and often inevitable complication in statistical analysis. The first main activity is motivated by the analysis of functional magnet resonance images of the human brain. The project will develop and test statistical procedures that allow doctors and bioengineers to take into account missing data with potential applications in understanding the neural plasticity and developing new methods for diagnosis and treatment of Alzheimer's and Parkinson's diseases. The second main activity is the statistical analysis of radiation and drug therapy of cancer cells and finding efficient treatments for prostate and breast cancers. The third main activity is to develop new statistical methods for time series analysis with environmental applications in wastewater treatment and reducing pollution. The project has the potential for impacts on environmental, actuarial, and cancer and brain studies. Graduate students will participate in the project, and obtained results will be disseminated through publications, presentation at conferences and the distribution of free R packages.<br/><br/>The project focuses on several topics in nonparametric curve estimation in the presence of missing data. Firstly, missing may be destructive when based solely on the data no consistent estimation is possible, and then an exploratory sampling is needed to unlock information contained in missing data. The project will develop methodology, theory and methods of efficient (minimal cost) exploratory sampling that allows matching the performance of an oracle that knows the missing mechanism. Secondly, missing always decreases available information. The PI plans to develop theory and methods for the sequential estimation with assigned risk and minimal stopping time, which becomes an attractive and feasible remedy when a priori knowledge of the size of available observations is precluded by missing. Thirdly, the PI plans to develop a shrinking local minimax methodology for missing data and construct a new type of data-driven estimators that can attain the faster minimax rate. Lastly, the project will develop sharp minimax theory and efficient nonparametric estimator for survival analysis in the presence of missing data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916231","Collaborative Research: Innovations for Bayesian Tree Ensemble Methodology","DMS","STATISTICS","07/15/2019","07/10/2019","Matthew Pratola","OH","Ohio State University","Standard Grant","Yong Zeng","06/30/2024","$120,000.00","","mpratola@stat.osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269","079Z","$0.00","An essential goal of modern statistical analyses across many disciplines is to gain insight into the behavior of real-world processes both to identify important correlates of variation and to obtain improved predictions.  For example in marketing, the statistician may be interested in learning the purchasing behavior of consumers from an analysis of a database of consumer transactions that includes various consumer descriptors (e.g. age, income level, geographic location) as well as  purchase amounts. The statistician would then typically attempt to build a mathematical model that characterizes the relationship between the consumer descriptors and the expenditure amount.  In doing so, however, certain issues bear strongly on the model's value and effectiveness.  First, the validity of a model may strongly depend on prior assumptions about the nature of the modeled process, information that can be difficult to ascertain. For instance, a consumer behavior model which builds in a simple assumption that consumers with higher income levels are always expected to purchase more, may be inadvertently ignoring subtleties that violate this assumption when other factors are simultaneously taken into account.  Second, sometimes even a valid and effective model may be such a complicated object that the extraction of meaningful information can itself be very challenging.  For example, after establishing particular set of predictors as important drivers of consumer purchasing power, it will still be of key interest how to best measure their relative importance in the model.  Focusing on the powerful and flexible approach of Bayesian regression tree ensemble modeling, the main thrust of this project will be to innovate this methodology to address these and other modeling avenues.  This new methodology will enable practitioners to address their research questions in an assumption-lean framework that allows the ensemble models to make use of their data to adaptively and flexibly incorporate contextual modeling assumptions. To greatly enhance interpretability, it will also provide automatic, information based summaries of variable importance to help the practitioner understand and interpret the available descriptor information.  In addition to these and further methodological contributions, the project will develop software for the implementation of this methodology as a freely available R package, enabling practitioners to more easily leverage our developments in their practical work. This is where the graduate student supported by this award will help. <br/><br/><br/>The research will focus on three general innovations to Bayesian ensemble modeling to further enhance its ability to extract meaning from complex data within an assumption lean framework.    The first contribution will develop theoretically valid measures of variable importance.  These measures will provide computationally efficient calculation of indices which meaningfully gauge the relative importance of predictor variables, both marginally and in terms of interactions.  The second contribution will provide an approach to monotone shape constrained inference which does not require any prior assumption of monotonicity.  This multidimensional nonparametric regression approach will enable the discovery and estimation of any and all the monotone components of the regression function, and to do so with no constraint assumptions whatsoever.  The third contribution will vastly extend the applicability of Bayesian ensemble modeling by developing a generalization of BART for arbitrary response data distributions, such as dichotomous responses and count data.  This major technical innovation will be based on a conjugacy-free formulation that will extend the reach of BART to many new application areas and problem types than were previously possible.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916271","Integrative Analysis on Heterogeneous Datasets with High-Dimensional and Non-Standard Models","DMS","STATISTICS","09/01/2019","08/16/2021","Yuekai Sun","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Yong Zeng","08/31/2023","$180,000.00","Moulinath Banerjee","yuekai@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Advances in data collection technology in the past decade have enabled practitioners to collect larger and more comprehensive datasets about many natural and social phenomena. Although this trend has enabled practitioners to gain new insights, it also comes with caveats that, if not addressed, may lead to erroneous conclusions that lie at the core of the reproducibility crisis in some areas of science. The caveats include: (i) Modern datasets are growing in heterogeneity, not only as a consequence of the inherent diversity in the world, but also the trend of combining data from multiple sources to create more comprehensive datasets. Not properly accounting for this growing heterogeneity may lead practitioners to systematically biased conclusions. (ii) The size of modern datasets is a hindrance to drawing inferences from them. Fitting a standard model to a massive dataset can be computationally intractable. (iii) The comprehensive nature of modern datasets raises privacy and security concerns. This is exacerbated by integrative analysis that may uncover combinations of patterns in multiple sources that are individually innocuous, but jointly identifying.<br/><br/>The Principal Investigator aims to address the heterogeneity, size, and privacy/security concerns that arise in integrative analysis of heterogeneous datasets by designing communication avoiding methods. At a high level, the general approach is to trade local computation for communication: compute lossy summaries of each data source and perform integrative analysis on the summaries. This way, only the summaries are assembled, thereby reducing the communication costs and preserving the anonymity and security of the separate data sources. The specific aims of the PI's work include (i) effective computational strategies for distributed computing under heterogeneity in popular high-dimensional models with provable statistical guarantees, and (ii) new methodological and theoretical insights into data integration for ""non-differentiable"" statistical problems in which estimators are obtained by projecting either on the  boundaries of a convex cone or via the optimization of discontinuous criterion functions, and which arise increasingly in modern domains of research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916467","Statistical Design, Sampling, and Analysis for Large Scale Experiments","DMS","STATISTICS","09/01/2019","11/03/2021","Lulu Kang","IL","Illinois Institute of Technology","Standard Grant","Yong Zeng","08/31/2023","$129,450.00","","lulukang@umass.edu","10 W 35TH ST","CHICAGO","IL","606163717","3125673035","MPS","1269","9251","$0.00","In the big data paradigm, even the controlled experiments can become large-scale, in the sense that the sample size is massive, and the dimension of the input variables is high. Such ""big data"" problem challenges many statistical approaches and significantly increases the amount of computation in estimation and inference. In this project, the PI focuses on specific instances of large-scale experiments and develops a set of novel theories and methodologies on experimental design, sampling, and analysis. The research has two major parts. In Part 1, the PI focuses on the type of experiments that contains a large dimension of covariate variables. For example, in a clinical trial, the covariates can be patients' rich medical history. How should the treatment settings be assigned to each patient? The PI provides the answer through a general experimental design framework so that the treatment effects are estimated accurately despite the influence of the covariates. In Part 2, the PI focuses on the Gaussian Process (GP) regression, one of the most popular statistical learning tools. The computation required is prohibitive for analyzing large-scale experiments such as the climate model simulations. The PI develops a dimension reduction framework and an active learning method that significantly improves the efficiency and accuracy of the GP model.<br/><br/><br/>Three major methodologies are considered. In Parts 1-3, the PI introduces a new discrepancy-based design to achieve covariate balance for experiments with a large dimension of covariates. The discrepancy criterion also has appealing theoretical properties that lead to a more accurate estimation of the parameters including both treatment effects and covariates' effects. Optimal design algorithms are developed for both offline and online experiments. In Part 4, the PI develops a novel dimension reduction method that finds the optimal convex combination of low-dimension kernel functions for the GP model. It is shown that the proposed method is a significantly less computational and more accurate approximation of certain types of underlying functions. In  Part 5, an active learning method based on the generalized Cook's Distance is developed for the GP regression. It is more efficient than the standard random sampling method. The research is novel in ideas, rigorous in theories, and useful in practice, and will open new directions in the statistical design and analysis of experiments area. The PI has a detailed education plan to develop new course modules, tutorials, and workshops based on the research products from this project. The research outcomes are readily applicable to a variety of scientific, engineering, medicine and other fields where large-scale data collection and analysis are demanded.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916161","RUI: Partially Observed Curves, and Big-Data Virtual Bootstrap","DMS","STATISTICS","09/01/2019","07/31/2019","Majid Mojirsheibani","CA","The University Corporation, Northridge","Standard Grant","Yong Zeng","08/31/2023","$174,975.00","","majid.mojirsheibani@csun.edu","18111 NORDHOFF ST","NORTHRIDGE","CA","913300001","8186771403","MPS","1269","9229","$0.00","Many real data sets in scientific disciplines, such as biomedical, engineering, and social sciences, contain missing, censored, or partially observed values, and this can make the task of statistical estimation and inference significantly more complicated.  Part of this research project focuses on the development of new flexible statistical methods to perform accurate prediction and inference in the presence of incomplete and missing data. Here, the data could be high-dimensional as well as functional, where each data value can be a curve. In another part of this research project, the PI considers the development of new efficient computer-intensive methods to deal with Big-data scenarios, where the data size may be too large to invoke classical approaches. Big-data has been one of the current research frontiers in recent years and there has been a growing interest in  Big-data-driven decision-making procedures in both academia and the industry.  There are still many computational and theoretical challenges in this area that require new methodologies. The PI's new approaches will solve a number of important statistical problems at the intersection of machine learning and statistical inference.<br/><br/>The research deals with three broad classes of problems related to prediction and inference in some nonstandard setups. These include the problem of functional classification when the covariate curves may be unobservable on some subsets of their domain. However, unlike some of the earlier results in the literature, the PI's approach does not impose any missing-at-random (MAR) type assumptions on the mechanisms that cause the absence or censoring of information. The approach allows for incomplete covariate to appear in the new unclassified curves as well as in the data. Given the observed covariate fragments, the aim is to construct strongly consistent nonparametric classifiers based on local-averaging methods. The second class of problems deals with uniform asymptotics for kernel regression estimators in the presence of missing response variables. This is generally acknowledged to be a difficult problem. The limiting distribution of the maximal deviation of such estimators can be used to construct asymptotically correct uniform confidence bands, or to perform goodness-of-fit tests, for an unknown regression function. Here, the PI will consider both MAR and non-ignorable missing response assumptions. The third set of problems focuses on the development of new weighted bootstrap methods for Big-data scenarios. The PI's approach aims at reducing the computational burden associated with the repeated sampling of Big-data, while still retaining the benefits of bootstrap methodology. The developed methods will be used to better approximate the sampling distribution of kernel and deconvolution density estimators, as well as their important functionals (such as sup- and Lp-norms), in the Big-data scenario.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916375","Non-Parametric Estimation under Shape/Norm Constraints","DMS","STATISTICS","09/01/2019","07/29/2019","Sabyasachi Chatterjee","IL","University of Illinois at Urbana-Champaign","Standard Grant","Yong Zeng","08/31/2023","$160,000.00","","sc1706@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","079Z","$0.00","This project is about advancing methodology and theoretical understanding of certain algorithms that are widely used today in machine learning and statistics. Decision Trees are an important type of predictive modelling method. They have a long history and modern variations like random forest are among the most powerful techniques available. One part of the project would develop new and theoretically sound decision trees accompanied by software implementation. The other part of the project applies to signal processing. Total Variation Denoising is a popular method used in image processing to do noise removal. The current algorithm as it stands, is not fully automated. This project would develop a fully automated version of this algorithm which is theoretically valid. In the bigger picture, the research would generate improved versions of these time tested algorithms and our understanding of how and why they work would be refined. <br/><br/><br/>A main focus of the project is to study non parametric estimation of non smooth functions such as piecewise constant/linear/polynomial functions, in high dimensions. One major agenda here is to give theoretical guarantees for CART like estimators. These guarantees would provably demonstrate adaptivity to the number and arrangement of rectangular level sets of the regression function. Theoretical understanding of such adaptivity for CART like estimators are largely absent in the literature and this research should be a first step towards filling this gap. In this project, an extension of the Dyadic CART estimator, called Model Selection Cart (MS Cart) is proposed as a computationally and theoretically tractable method to achieve the desired adaptivity. We have already developed an algorithm (to be implemented and made publicly available), based on a dynamic programming approach, which provably computes the MS Cart estimator efficiently. We are currently working on showing theoretical guarantees for MS Cart. Another main focus of the proposal is to study the methodology of Total Variation Denoising (TVD). This technique is a non linear image denoising technique heavily used in the image processing community. The first problem talked about in this proposal, under this topic, is a step towards rigorous understanding of the statistical risk of the TVD estimator. Worst case analysis of this risk is now well understood in the literature. The research proposed here will go beyond worst case analysis and reveal the adaptivity of the TVD estimator. The second proposed problem deals with the very practical issue of choosing the tuning parameter for TVD in a fully data driven way. A new tuning parameter free estimator is proposed here whose practical performance has been thoroughly checked by us in simulations. We will show that our proposed estimator is minimax rate optimal while being fully data driven. Such an estimator is not available in the literature, as of now. This estimator has the potential to make the TVD methodology much more user friendly than it already is as choosing a tuning parameter can be a delicate issue. The PI will also address several non parametric estimation problems in settings such as quantile regression and in general exponential families. In particular, one agenda here is to study shape constrained estimation in quantile regression which would be a first step towards going beyond restrictive linear assumptions often made in the existing literature.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1853458","University of Arkansas Spring Lecture Series in the Mathematical Sciences 2019 and 2020","DMS","STATISTICS","04/01/2019","02/27/2019","Giovanni Petris","AR","University of Arkansas","Standard Grant","Pena Edsel","05/31/2022","$9,956.00","Avishek Chakraborty, Jyotishka Datta","gpetris@uark.edu","1125 W MAPLE ST STE 316","FAYETTEVILLE","AR","727013124","4795753845","MPS","1269","7556, 9150","$0.00","The University of Arkansas Spring Lecture Series 2019 and 2020 will be held in Fayetteville, AR, on April 18-20, 2019 and April 16-18, 2020, on the topics of ""Bayesian Analysis for Multivariate Dynamic Systems"" and ""Discrete Random Structures and Prediction in Bayesian Nonparametrics,"" respectively. The common theme of the two Lecture Series is how to harness the power of data using complex models within the flexible paradigm of Bayesian statistical inference. In each Lecture Series, a prominent researcher in the field will give five lectures on the topic of the conference, starting with an introductory one and reaching the boundaries of current research. Additionally, ten leading figures in the area covered by the conference will give one-hour research presentations, and there will be sessions devoted to contributed talks and poster presentations. Graduate students, recent PhDs, and new researchers are supported by this award. To facilitate the transition of graduate students to the advanced research topics that will be treated in the lectures, there will be an introductory workshop, specifically aimed at them, in the afternoon of the day before the conference begins.<br/><br/>The 2019 conference will focus on Bayesian approaches to modeling and analysis of high-dimensional multivariate time series with a broad purview over problems of statistical analysis, structure assessment, monitoring and forecasting.   Emphasis will be given on discussion of recent advances in state-space models anchored on instantiations of the decouple/recouple concept and its implied strategies that provide a powerful platform for scaling coherent statistical analysis to increasingly complex dynamic systems. State-of-the-art Bayesian modeling approaches based on this concept, as well as their broad range of applications in many different areas including financial and commercial forecasting, socio-economic, engineering and natural sciences will be covered in the lectures.  The 2020 conference will give an overview of recent developments in Bayesian nonparametrics, specifically focusing on discrete random structures that are key tools for many modern inferential goals, such as topic modeling, change-point analyses or meta-analysis. Special attention will be given to the most promising research directions such as partial exchangeability and dependent nonparametric priors, spanning state-of-the-art tools in this area. The attendees will benefit from ample opportunities to forge and foster collaborations and exchange of ideas as well as exposure to open research problems in methodological and cross-disciplinary domains. More details on these conferences are available at https://fulbright.uark.edu/departments/math/research/spring-lecture-series/index.php<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915978","Spectral Methods for High Dimensional Tensor Data","DMS","STATISTICS","09/01/2019","07/30/2019","Miaoyan Wang","WI","University of Wisconsin-Madison","Standard Grant","Yong Zeng","08/31/2023","$179,349.00","","miaoyan.wang@wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","Recent developments have made large-scale multidimensional data readily available in science and engineering applications. Examples include multi-tissue, multi-individual gene expression studies, in which gene expression profiles are collected from different individuals' tissues. Another example is the DBLP database, which is organized into a three-way tensor of author-by-word-by-venue, and each entry indicates the co-occurrence of the triplets. Despite the popularity of tensor data, there are many challenges to using statistical methods for analyzing higher-order tensors. Indeed, the classical spectral theory for matrices is not directly applicable to tensors, and the computational problem becomes NP-hard in the worst case. Therefore, analyzing tensor data with increasing dimensionality and ever-growing complexity requires the development of novel statistical methods, which is the aim of this project.<br/><br/>In this project, the PI plans to develop a framework of statistical models, scalable algorithms, and relevant theories to analyze tensor-valued data. This will allow researchers to examine complex interactions among tensor entries and between multiple tensors, thereby providing solutions to questions that cannot be addressed by traditional matrix analysis. The project will focus on three major areas: (i) spectral theory for specially-structured or random tensors; (ii) estimation of low-rank tensors from non-Gaussian observations; and (iii) joint estimation of mean and covariance for tensor-valued data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1914917","Complex Problems in Functional Data Analysis","DMS","STATISTICS","07/01/2019","08/16/2021","Jane-Ling Wang","CA","University of California-Davis","Continuing Grant","Yong Zeng","06/30/2023","$200,000.00","","janelwang@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","Functional data analysis (FDA) deals with infinite-dimensional data in the form of random functions. Such data have become increasingly common due to new technology to record and store massive data. The field has gained much traction and research has accelerated, but there remain many unsolved problems and new opportunities for research. This research focuses on four projects that address: 1) an open problem regarding the choice of the domain of interest in a regression setting with a functional covariate and scalar response, 2) implementing the RKHS (reproducing kernel Hilbert space) approach for conventional functional linear models when the functional covariates are observed sparsely, 3) dynamic modeling for multivariate functional data, and 4) challenges for the analysis of functional snippet data, for which each subject is observed in a different interval much shorter than the domain of the functional data. The developed methods will be applied to various data with functional components to evaluate the effect of pollutants on lung cancer mortality and to explore the interaction of these pollutants.  The proposed research thus has direct impacts on public health research. In addition, the proposed approaches for functional snippets have broad applications in accelerated longitudinal studies, which are common in social and health sciences. The computer code of developed algorithms will be integrated into an existing R-package, fdapace, on CRAN. The research findings will be incorporated into graduate curricula, undergraduate and graduate research projects, and short courses at workshops, and be presented at professional meetings. <br/> <br/>Project 1 is important for interpreting the influence of a functional covariate, yet to date, there is no algorithm that can reliably identify the relevant domain and the theory is incomplete. We propose to resolve these open problems through a new framework that involves a dynamic RKHS approach to overcome the challenges. This has the potential to break new ground in the well-established field of RKHS.  A weakness of the RKHS approach is that it has difficulty to handle sparsely observed functional covariates. In Project 2, we propose a solution by imputing incomplete functional covariates and show that the regression coefficient function can be recovered through the imputed functional covariates. A new line of theory will be developed to deal with the approximation errors in the Karhunen-Lo\'eve expansion for functional data.  These new results will facilitate future research that involves imputation for functional data. Project 3 aims at modeling the derivatives of multivariate functional data using the component processes as covariates. We propose a concurrent approach that avoids an ill-posed inverse problem and has the advantage to accommodate time-lags of the predictor component processes. Project 4 deals with another open problem in FDA. We propose two nonparametric approaches for functional snippets and will develop supporting theory. These new approaches provide a new frontier of research in FDA, as once the covariance can be estimated accurately, existing FDA approaches, such as principal component analysis, classification or clustering, can be readily adapted for functional snippets.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1847590","CAREER: Computational and Theoretical Investigations of Variational Inference","DMS","STATISTICS","03/01/2019","04/30/2023","Chao Gao","IL","University of Chicago","Continuing Grant","Yong Zeng","02/29/2024","$400,000.00","","chaogao@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1269","079Z, 1045","$0.00","The new era of complex and big data poses unprecedented challenges in terms of both our computational and theoretical understanding of Statistics.  One of the fundamental questions in modern data science is how to efficiently process massive data sets with minimal information loss to aid scientific discovery and decision making.  This has led the scientific community to adopt approximate statistical procedures that achieve the optimal trade-off between computational efficiency and statistical efficiency.  On the one hand, the approximation should be tractable to gain computational benefits. On the other hand, the approximation needs to be tight, so as to not compromise much in terms of the statistical optimality of the original problem. This project aims to bridge the computational and theoretical gaps in various statistical problems under complex and nonstandard settings. The results of the proposed research will significantly impact areas known for applying computationally intensive methods on a routine basis.  These include population genetics, astronomy, computer vision, political science, social science, and animal science.<br/><br/>Recent developments in high-dimensional statistics and machine learning focus on exploring the intrinsic low-dimensional structure of the problem. This poses new challenges in terms of both statistical optimality and computational efficiency.  Variational inference is a technique that addresses both challenges by seeking a variational approximation to the original problem that is not only tractable, but also tight.  However, the literature lacks a systematic investigation of variational inference from both the computational and statistical perspective.  The goals of the project include: (1) theoretical investigations of variational Bayesian procedures; and (2) variational optimization strategies in robust estimation.  The project's success will help bridge the computational and theoretical gaps in modern data science and lead to significant theoretical and computational advances in variational inference.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1914411","Variable Selection, Instrument Search and Estimation in Problems with Nonignorable Missing Data","DMS","STATISTICS","09/01/2019","08/09/2019","Jun Shao","WI","University of Wisconsin-Madison","Standard Grant","Yong Zeng","08/31/2023","$199,999.00","","shao@stat.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","Missing data are ubiquitous in many applications including sample surveys, clinical trials and medical/health studies.  Handling incomplete data is particularly challenging when the missingness is related to the unobserved data.  This project seeks to develop statistical models and methods for analyzing datasets with nonignorable missing values.  The research topics are motivated by problems encountered by survey agencies as well as in data sets from biomedical/health studies.  The results will have significant impact both in terms of new methodological development for handling incomplete data as well as in applications to real datasets from a number of scientific fields. <br/><br/>When the missing data mechanism depends on unobserved data,  the missing data are referred to as nonignorable.  Handling nonignorable missing data is a challenging problem as the unobserved values follow a distribution different from that of the observed data leading to issues of identifiability and estimability of the underlying unknown parameters.  The proposed research focuses on the following three areas: (1) Instrument search and model selection. A recent method for handling nonignorable missing data is built on the use of a covariate (called an instrument) that enables researchers to identify and estimate population parameters.  In applications, however, such an instrument must be constructed from a given set of observed covariates.  The proposed research will develop methods for finding instruments in different situations, including semiparametric propensity models, pseudo likelihood methods, and problems with both missing responses and covariate values.   Together with instrument search, model selection regarding the parametric component will be also investigated.  (2) High dimension reduction and variable selection. In many big data applications, the dimension of the covariate vector is often very large, however, only a few covariates are useful.  While there is extensive research related to dimension reduction and variable selection over the past two decades, there are no results available in the presence of nonignorable missing responses.  The proposed research focuses on covariate selection or dimension reduction under two semiparametric frameworks.  (3) Multivariate data with nonignorable missing values. The proposed research will develop methods for problems with both missing responses and covariates, survival analysis with survival-dependent missing covariates,  and personalized medicine with longitudinal data having dropouts.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916395","Collaborative Research: High-Dimensional Spatial-Temporal Modeling and Inference for Large Multi-Source Environmental Monitoring Systems","DMS","STATISTICS","09/01/2019","08/09/2019","Andrew Finley","MI","Michigan State University","Standard Grant","Yong Zeng","08/31/2023","$80,000.00","","finleya@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","","$0.00","Remote sensing technologies and Geographic Information Systems continue to bring about dramatic developments in scientific discovery. Scientists in a variety of disciplines today have unprecedented access to massive spatial and temporal databases comprising high resolution remote sensed measurements. Statistical modeling and analysis for such data often entail reckoning with spatial associations and variations at multiple levels while attempting to recognize underlying patterns and potentially complex relationships among the scientific variables. Traditional statistical hypothesis testing is no longer adequate for these inferential objectives and statisticians are increasingly turning to multi-level or hierarchical modeling structures for analyzing complex spatial-temporal data. However, there continue to remain substantial computational bottlenecks as scientists encounter the data deluge in remote-sensed data that demand specialized ""BIG DATA"" technologies. The PIs will address these problems by developing probabilistic machine learning tools for spatial-temporal BIG DATA within the context of scientific advancements in forest structure, topography, and weather-related events (e.g., storms) that can have far-reaching public health, economic, environmental, and security implications. Several innovations in statistical and computational methods and related software development are envisioned. The proposed data products will offer quantification of forest damage/change and landslide risk assessment for Puerto Rico following hurricanes Irma and Maria. Key educational components include dissemination of proposed technologies across the scientific communities including data scientists, engineers, foresters, ecologists, and climate scientists. The PIs plan to train the next generation of data scientists through dissemination efforts for undergraduate and graduate students in STEM fields.           <br/><br/>The PIs will develop a statistical framework for executing elaborate case studies and data analysis on high-dimensional remotely sensed data, where ""high dimension"" alludes to one or all of a massive number of (i) spatial locations; (ii) time points; and (iii) responses or outcomes. The PIs will introduce massively scalable multivariate spatial process models within a rich Bayesian hierarchical framework to obtain fully model-based inference for the underlying data generating process. Innovative statistical methodologies are proposed to implement hierarchical models at scales involving tens of millions of spatial locations, thousands of time points and possibly hundreds of remote-sensed variables. The massive scalability of these models will be achieved through sparsity-inducing spatial-temporal processes and other graphical models, matrix-variate low-rank models, conjugate Bayesian distribution theory, and meta-learning paradigms using approximations of a collection of posterior distributions. Theoretical results that enhance current methods will be explored as will be several proposed case studies at hitherto unprecedented scales. The PIs will develop a full suite of spatial models in a wide variety of experiments involving massive data sets. Since massive data sets are where complex relationships can be detected effectively, the proposed methods are well-suited for modeling complex scientific phenomena. Key substantive inference and statistical quantification will be offered for forest damage/change and landslide risk assessment for Puerto Rico following hurricanes Irma and Maria. The PIs will provide probability-based uncertainty quantification and will substantially enhance the scientific community's understanding of storm-related damage assessment.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916211","Combinatorial Inference: Statistical Uncertainty Assessment for Discrete Structures","DMS","STATISTICS","08/15/2019","08/31/2021","Junwei Lu","MA","Harvard University","Continuing Grant","Yong Zeng","01/31/2023","$147,258.00","","junweilu@hsph.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269","","$0.00","Discrete structures like networks, clusters and ranking are observed in many real-life applications including brain networks, protein clusters, and portfolio selection. This project will develop a unified statistical framework for inferring these unknown discrete structures from large-scale datasets in biology, neuroscience, and finance.  The replicability crisis on whether statistical inference for scientific discoveries can be trusted has drawn attention from both the scientific community and the public.  A new field, ""combinatorial inference"", developed by the PI aims to fill the gap between various existing methods designed for continuous quantities and the lack of valid reproducibility assessment for discrete ones.  This project will also train the next generation of data scientists to acquire statistical and computational skills for large-scale scientific data analysis.  <br/><br/>This project aims to develop a unified inference framework to assess uncertainty (e.g. constructing confidence intervals, testing hypotheses and controlling the false discovery rates) when inferring discrete structures include networks, hypergraphs, clustering and ranking.  Three types of problems will be considered: (1) Testing the hypothesis that the discrete structure has certain combinatorial properties (e.g., the network is connected, or the graph is triangle-free); (2) Constructing confidence sets covering the discrete quantities with given significance level (e.g., the maximum degree of a graph, the elements belong to a same cluster, top k ranking items); (3) Screening discrete structures of interest (e.g., the cycles, hubs and cliques in a graph) with the false discovery rate control.  Classical inference mainly focuses on testing hypotheses and constructing confidence intervals for continuous parameters where analytical methods and theory can be directly applied. On the other hand, exiting research on discrete structures in statistical models mainly focuses on estimation and lacks systematic inferential methods for uncertainty assessment.  This project seeks to advance the frontiers of modern statistical inference in four directions:  (a) Methodology: Developing efficient methods for hypothesis testing, confidence interval construction, and false discovery controlling procedures; (b) Theory: Developing new probabilistic tools including non-asymptotic concentration inequalities and limiting theory and universality phenomena for combinatorial random quantities; (c) Computation: Computationally efficient algorithms to implement the combinatorial inferential methods for large-scale statistical models; and (d) Fundamental limits: Developing new combinatorial information-theoretic lower bounds to justify the optimality of the proposed inferential methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916198","Non-Convex Landscapes and High-Dimensional Latent Variable Models","DMS","STATISTICS","09/01/2019","08/03/2019","Zhou Fan","CT","Yale University","Standard Grant","Pena Edsel","08/31/2022","$182,654.00","","zhou.fan@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269","","$0.00","In many fields of science and engineering, probabilistic latent variable models are a powerful and widely-used tool for drawing inferences from complex data. They provide a flexible framework by modeling the complexity in observed data as arising from interactions between simpler random and unobserved quantities. Latent variable models used in modern applications are often high-dimensional, and this leads to both statistical and computational challenges for inference: Surprising phenomena emerge in which structure in one latent variable can create spurious and problematic artifacts in classical inference procedures for another. These classical procedures also commonly lead to non-convex optimization problems over a large number of parameters, which are difficult to computationally solve.<br/><br/>This research will study a flexible framework by modeling the complexity in observed data as arising from interactions between simpler random and unobserved quantities. The aim is in answering the following questions: How and why can one source of latent variation lead to artifacts in classical statistical estimates for another? What are the geometric properties of objective function landscapes in these models that render them difficult to optimize? And, can we design improved inferential procedures that correct for these artifacts and are easier to compute? The research will apply techniques from random matrix theory, free probability theory, and statistical physics to obtain a better understanding of these questions in high-dimensional settings.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915099","Impact of Molecular Biomarkers on Survival Outcomes: Signal Detection, Model Selection, and Statistical Inference in High-dimensional Settings","DMS","STATISTICS","08/15/2019","04/19/2022","Haolei Weng","MI","Michigan State University","Continuing Grant","Yong Zeng","07/31/2024","$120,000.00","","wenghaol@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","","$0.00","The goal of this project is to provide a theoretically justifiable and computationally feasible framework for analyzing large-scale data collected from epidemiology and other biomedical sciences, social science, marketing, environmental science, and econometrics. The proposed methods will provid a convenient means to identify the gene-specific impacts on survival and quantify the uncertainty of the estimates in high-dimensional settings. As a result, the methods will help detect patients' specific characteristics that make them respond differently to treatment or more susceptible to diseases, which is key to precision medicine. The PI further plans to disseminate the proposed research in education by redesigning graduate-level courses and training graduate students. <br/><br/>The project, through its three major aims, features a series of methods to address fundamental issues arising from high-dimensional survival data analysis. The first aim is to detect gene-specific impacts on cancer patients' survival and to provide an integrated framework of detecting individual genes' relevance to survival when genes have heterogeneous patterns of influence. The second aim focuses on sequentially selecting important predictors for predicting survival outcomes. This approach is different from existing forward regression approaches, which are not applicable to handle censored outcome data with high-dimensional covariates. The third aim is to develop methods to quantify the uncertainty of survival models with high-dimensional predictors by integrating model selection, estimation, and inference steps.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1853549","Collaborative Research: Development of New Statistical Methods for Genome-Wide Association Studies","DMS","STATISTICS, MATHEMATICAL BIOLOGY","07/15/2019","07/10/2019","Marco Ferreira","VA","Virginia Polytechnic Institute and State University","Standard Grant","Zhilan Feng","06/30/2023","$146,886.00","","marf@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1269, 7334","","$0.00","Advances in high-throughput sequencing technologies now make possible cost-effective analysis of whole genomes. The genomes of any two humans are 99.9% identical, with differences in the remaining 0.1% determining the diversity of human traits. For example, DNA sequence differences account for 80% of the variability in human height. Current technology allows the identification of these sequence polymorphisms between individuals, which can then be correlated to differences in a given trait. When done on a genome wide level with a large population of individuals, such genome wide association studies (GWASes) can be a useful tool for the identification of key genes controlling specific traits. However, a requirement for this approach is the availability of powerful and accurate statistical and computational methods to search through a massive amount of sequencing data to correctly identify DNA differences associated with the phenotypic trait of interest. The outcome of the project will (1) provide statistical methods to understand relationships between DNA sequence differences and the full range of diversity observed in a population, and (2) provide corresponding computational tools suitable for use by biologists and biomedical specialists for their specific population studies. This research project will produce intermediate methodological and theoretical results that lay the foundation for the final output. This project will also apply the developed methods to real, experimental data to demonstrate their utility. In addition to these research outcomes, the project will support the training of students in the field, including women and underrepresented minorities.<br/> <br/>GWAS estimates the correlation between phenotypic traits and sequence polymorphisms to identify genetic variants highly associated with specific traits. Single nucleotide polymorphisms (SNPs) are the most common type of genetic variant, and sequencing technologies allow for large-scale collection of SNP information. The project team will develop new GWAS models and methods to find trait-affecting variants with more power and accuracy. Specifically, the new methods developed in this research project will improve existing approaches by allowing modeling of observed traits from any probabilistic distribution in the exponential family. This extension ensures statistical models are biologically meaningful and interpretable. Second, the new methods will exploit different Bayesian priors, especially contemporary Bayesian priors for ultra-high dimensional model selection, that will share information across the entire genome for stable statistical inferences. Theoretical results of Bayesian priors in these new methods will also be developed. Third, a stochastic search algorithm will be developed to efficiently search through the massively large model space for model selection. This ensures that new methods are practical and useful since analysis can be done within a reasonably short time frame. Meanwhile, this also eliminates the use of subjective thresholds of significance that are now commonly used but an embarrassing practice in GWAS, having no theoretical support. Methods will be implemented into software tools and will be freely available for statisticians, biologists, and biomedical researchers. This project is funded jointly by the Division of Mathematical Sciences Mathematical Biology Program and the Statistics Program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1914636","Collaborative Research: Uncertainty Quantification, Optimal Designs and Calibration in Computer Experiments","DMS","STATISTICS","07/01/2019","06/19/2019","Rui Tuo","TX","Texas A&M Engineering Experiment Station","Standard Grant","Yong Zeng","06/30/2023","$142,517.00","","ruituo@tamu.edu","3124 TAMU","COLLEGE STATION","TX","778433124","9798626777","MPS","1269","","$0.00","From aerospace designs to material science to biomedical studies, today's practice in engineering and physical sciences has made increasing use of computer simulations. How to design the computer simulations, analyze the computer output data, as well as to enhance the accuracy of the computer models are fundamental challenges in computer simulations. This project will focus on the development of a statistical framework for computer experiments, with the goal of developing both theoretical and methodological tools that cover the typical computer simulation pipeline from data collection to modeling and analysis to verification and validation. Specifically, the team plans to establish a new uncertainty quantification theory and foster novel methodologies for data mining, interpretation and decision making. The project will provide accurate, efficient and robust approaches that would make an impact on contemporary computer simulation practice. <br/><br/>The project has three major objectives: (i) establish a statistically and computationally efficient uncertainty quantification framework for Gaussian process regression, (ii) propose a general experimental design scheme for multi-fidelity computer experiments, (iii) study the statistical properties and suggest efficient algorithms for novel calibration methods for computer models. The proposed work should lead to methodological development of a generic nature in the design, uncertainty quantification and calibration in computer experiments. The improved uniform error bounds can potentially lead to the use of fewer experimental runs for the same precision. Their significance can go beyond computer experiments such as in spatial statistics, which heavily uses kriging method. The optimal designs for nonstationary Gaussian Process models can help stimulate further development of experimental design theory in more complex situations. Standard approaches in experimental design do not pay much attention to the nonstationary situations. The proposed algorithm can substantially enhance the value of the projected kernel calibration (PKC) method. Although PKC is known to be theoretically superior, there is no known algorithm that can effectively calculate the PKC estimates. Because calibration is used to bridge the gap between computer simulations and physical experiments, this work can be potentially significant. Theoretical and technical advances made in this project can help facilitate further interactions between statistics, applied mathematics and probability theory, through journal publications, student exchange visits and presentations in interdisciplinary conferences, etc.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1949730","Statistical Machine Learning Methods for Complex Data Sets","DMS","STATISTICS","08/01/2019","09/10/2019","Kean Ming Tan","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Pena Edsel","07/31/2022","$75,028.00","","keanming@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Recent advances in science and technology have led to the generation of massive amounts of large-scale data with complex structures, including genomics, neuroimaging, and microbiology data. These large-scale datasets pose significant statistical and computational challenges to data analysis. Firstly, widely used statistical methods yield unstable estimates and are not computationally scalable to modeling large-scale data sets. Secondly, complex data sets are often accompanied by outliers due to possibly measurement error or heavy-tailed random noise. For instance, in genomic studies, it has been observed that the distribution of gene expression levels is generally heavy-tailed, that is, the data contain a lot of extremely large values. Classical statistical methods will yield biased estimates and spurious scientific discovery if these outliers are not taken into account during model estimation and inference. This project aims to develop scalable and robust multivariate statistical methods to address the aforementioned problems. <br/><br/>In this project, the investigator uses a combination of regularization and statistical optimization techniques to develop novel multivariate statistical methods for analyzing complex high-dimensional data sets. The first part of the project concerns the sparse generalized eigenvalue problem, which arises naturally in many statistical models such as partial least squares, canonical correlation analysis, sufficient dimension reduction, and Fisher's discriminant analysis. The investigator will develop a general framework for solving the sparse generalized eigenvalue problem and make available a wide range of statistical models for analyzing high-dimensional data. Furthermore, the investigator will study the theoretical properties of sparse generalized eigenvalue problem, and this will lead to the understanding of various statistical models that are previously not well understood in the high-dimensional setting. The second part of the research project focuses on a class of robust sparse reduced rank regression models. The investigator will develop efficient algorithms and high-dimensional asymptotic analysis for the resulting estimators under the Huber loss function, and quantify the bias-robust tradeoff between using Huber loss and squared error loss. This research project will also deliver easy-to-use software packages for fitting the developed methods.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916290","Collaborative Research: Asymptotic Statistical Inference for High-dimensional Time Series","DMS","PROBABILITY, STATISTICS","08/01/2019","07/18/2019","Danna Zhang","CA","University of California-San Diego","Standard Grant","Yong Zeng","07/31/2023","$120,000.00","","daz076@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1263, 1269","","$0.00","The information era has witnessed an explosion in the collection of high dimensional time series data across a wide range of areas, including finance, signal processing, neuroscience, meteorology, seismology, among others. For low dimensional time series, there is a well-developed estimation and inference theory. Inference theory in the high dimensional setting is of fundamental importance and has wide applications, but has been rarely studied. Researchers face a number of challenges in solving real-world problems: (i) complex dynamics of data generating systems, (ii) temporal and cross-sectional dependencies, (iii) high dimensionality and (iv) non-Gaussian distributions. The goal of this project is to develop and advance inference theory for high dimensional time series data by concerning all the above characteristics. The project will provide training to graduate students and publicly avaialble statistical packages. <br/><br/>This project involves developing a systematic asymptotic theory for estimation and inference for high dimensional time series, including parameter estimation, construction of simultaneous confidence intervals, prediction, model selection, Granger causality test, hypothesis testing, and spectral domain estimation. To this end, a new methodology for the estimation of parameters and second-order characteristics for high dimensional time series will be proposed. New tools and concentration inequalities for the asymptotic analysis of high-dimensional time series will be developed. To perform simultaneous inference and significance testing, the PIs will investigate the very deep Gaussian approximation problem and the high dimensional central limit theorems by taking both high dimensionality and temporal and cross-sectional dependencies into account.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916339","Sensitivity Analysis for Informative Missingness with Semiparametric Models","DMS","STATISTICS","08/15/2019","08/06/2019","David Todem","MI","Michigan State University","Standard Grant","Yong Zeng","07/31/2023","$180,000.00","","todem@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1269","","$0.00","Missing data are common in almost all empirical research. Examples abound in epidemiologic studies, clinical trials, social science studies (for example sample surveys) and other substantive investigations of scientific inquiry. With incomplete data, a long-standing question in statistical research concerns how to conduct unbiased and reliable inferences, especially when the missingness process depends on the unobserved data. This question has become particularly relevant in this era of big data, where empirical data, albeit massive, may be subject to a selective nonresponse. Motivated by practical problems in medical imaging data, functional and longitudinal data, this research project will develop new methods and practical tools for handling incomplete data, focusing on sensitivity analysis. Results obtained from this research will have significant impacts on sensitivity analysis methods for nonrandom missing data under semiparametric estimation and inference.<br/><br/>With recent technological advances in this era of big data, many missing data models arising in various substantive applications involve infinite-dimensional parameters that are only identifiable on the grounds of unverifiable assumptions about the data generating process. A popular approach for addressing identifiability concerns, at least in the context of finite-dimensional parameter settings, has been to impose some structural constraints on the model upon which the parameters of primary interest are presumed identified. Inferences are then conducted within the framework of a sensitivity analysis to accommodate uncertainties resulting from these untestable restrictions. Despite its popularity, this approach has not been systematically studied when the working population model involves a nonparametric component in addition to the parametric component. This project proposes to fill this gap by formalizing inferences via rigorous sensitivity analyses when there are identifiability concerns under semiparametric estimation and inference. Unlike in finite-dimensional parametric models, conducting such analyses is not trivial owing to the complexity of the theoretical arguments and the computational burden. The goal of this project is to further our understanding of the theory that embeds inference in non- and weakly-identified semiparametric models arising in missing data problems within the framework of a sensitivity analysis. Conducting a sensitivity analysis under semiparametric formulations is technically not trivial owing to the inferential strategy posing substantial challenges beyond those encountered with finite-dimensional parametric models. In this project, asymptotic expansions, resampling techniques including the bootstrap, related techniques from empirical processes and computer-aided simulations will be employed to study the central theoretical properties and the finite sample performance of the proposed procedures. The foundational nature of the proposal in establishing the connection between the empirical process theory and sensitivity analysis is transformative. This connection within the class of semiparametric models arising in both classical and modern statistics will be explored and studied. A further part of the intellectual merit lies in its interdisciplinary approach with the transfer of ideas between high level computing, empirical process techniques, and subject matter science informing the model describing complex systems with many parameters.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916376","A Unified Multi-Stage Approach to Generalized Sequential Decision Making Problems with Covariates","DMS","STATISTICS","07/01/2019","06/18/2021","Wei Qian","DE","University of Delaware","Continuing Grant","Yong Zeng","06/30/2023","$165,781.00","Shanshan Ding","weiqian@udel.edu","220 HULLIHEN HALL","NEWARK","DE","197160099","3028312136","MPS","1269","9150","$0.00","Sequential decision making problems are commonly encountered optimization tasks with important modern applications. With rapid advances in data-driven technology, the diverse application examples include online service recommendation for smart phone users, intelligent implementation of intervention plans for medical service, automated financial service processing, and many others. Generally, faced with multiple decision arms, a service provider needs to choose one to be delivered for each upcoming service user, and targets to maximize the overall reward and benefits for all these service users. Furthermore, in this Big Data era, individual user covariates and metrics are often accessible to service providers, which holds great promise in personalized (mobile, medical, or business) service decision making to enhance user reward outcomes. This project will significantly advance the methods and theory for the sequential decision making problems with covariates, and address important questions that are also of interests to multiple statistics-related fields such as computer science, operations research, business analytics, health sciences, and broader machine learning communities. The promising use of personalized service will be promoted through close interdisciplinary collaborations with business and medical research communities. The graduate student supported by this grant will help with statistical theory, programming, and data analytics. <br/><br/>Under both parametric and nonparametric frameworks, a unified multi-stage approach will be developed to optimally solve a series of generalized sequential decision making problem settings formulated as multi-armed stochastic bandit problems with covariates. In particular, the investigators aim to (1) develop a new algorithm to handle high-dimensional user covariates under assumptions much relaxed from existing work while improving performance; with integration of a class of high-dimensional regression methods and new technical tools for non-i.i.d. samples inherited from the algorithm, establish rigorous finite-time regret analysis and useful statistical properties; (2) propose a new nonparametric framework as the censored bandit problem with covariates and show optimal cumulative regret and flexible use with possibly censored reward response and non-linear decision boundary; (3) study a class of high-dimensional dimension reduction methods to mitigate curse of dimensionality issues in the nonparametric regression settings and significantly extend the use of classical nonparametric methods in high dimensional problems. Both theoretical and empirical studies to incorporate complex covariate structures inspired from business and medical research questions for decision making will create valuable training and research opportunities for graduate and undergraduate students. The graduate student supported by this grant will help with statistical theory, programming and data analytics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916349","Collaborative Research: High-Dimensional Spatial-Temporal Modeling and Inference for Large Multi-Source Environmental Monitoring Systems","DMS","STATISTICS","09/01/2019","08/09/2019","Sudipto Banerjee","CA","University of California-Los Angeles","Standard Grant","Yong Zeng","08/31/2022","$199,993.00","","sudipto@ucla.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244201","3107940102","MPS","1269","","$0.00","Remote sensing technologies and Geographic Information Systems continue to bring about dramatic developments in scientific discovery. Scientists in a variety of disciplines today have unprecedented access to massive spatial and temporal databases comprising high resolution remote sensed measurements. Statistical modeling and analysis for such data often entail reckoning with spatial associations and variations at multiple levels while attempting to recognize underlying patterns and potentially complex relationships among the scientific variables. Traditional statistical hypothesis testing is no longer adequate for these inferential objectives and statisticians are increasingly turning to multi-level or hierarchical modeling structures for analyzing complex spatial-temporal data. However, there continue to remain substantial computational bottlenecks as scientists encounter the data deluge in remote-sensed data that demand specialized ""BIG DATA"" technologies. The PIs will address these problems by developing probabilistic machine learning tools for spatial-temporal BIG DATA within the context of scientific advancements in forest structure, topography, and weather-related events (e.g., storms) that can have far-reaching public health, economic, environmental, and security implications. Several innovations in statistical and computational methods and related software development are envisioned. The proposed data products will offer quantification of forest damage/change and landslide risk assessment for Puerto Rico following hurricanes Irma and Maria. Key educational components include dissemination of proposed technologies across the scientific communities including data scientists, engineers, foresters, ecologists, and climate scientists. The PIs plan to train the next generation of data scientists through dissemination efforts for undergraduate and graduate students in STEM fields.           <br/><br/>The PIs will develop a statistical framework for executing elaborate case studies and data analysis on high-dimensional remotely sensed data, where ""high dimension"" alludes to one or all of a massive number of (i) spatial locations; (ii) time points; and (iii) responses or outcomes. The PIs will introduce massively scalable multivariate spatial process models within a rich Bayesian hierarchical framework to obtain fully model-based inference for the underlying data generating process. Innovative statistical methodologies are proposed to implement hierarchical models at scales involving tens of millions of spatial locations, thousands of time points and possibly hundreds of remote-sensed variables. The massive scalability of these models will be achieved through sparsity-inducing spatial-temporal processes and other graphical models, matrix-variate low-rank models, conjugate Bayesian distribution theory, and meta-learning paradigms using approximations of a collection of posterior distributions. Theoretical results that enhance current methods will be explored as will be several proposed case studies at hitherto unprecedented scales. The PIs will develop a full suite of spatial models in a wide variety of experiments involving massive data sets. Since massive data sets are where complex relationships can be detected effectively, the proposed methods are well-suited for modeling complex scientific phenomena. Key substantive inference and statistical quantification will be offered for forest damage/change and landslide risk assessment for Puerto Rico following hurricanes Irma and Maria. The PIs will provide probability-based uncertainty quantification and will substantially enhance the scientific community's understanding of storm-related damage assessment.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915976","Collaborative Research: Subject-level Prediction and Application","DMS","STATISTICS","08/01/2019","07/30/2019","Jonnagadda Rao","FL","University of Miami School of Medicine","Standard Grant","Yong Zeng","07/31/2023","$120,000.00","","js-rao@umn.edu","1400 NW 10TH AVE","MIAMI","FL","331361000","3052843924","MPS","1269","","$0.00","Many practical problems are related to prediction, where the main interest is at the subject (for example personalized or precision medicine) or (small) sub-population (for example small community) level.  In recent years, new and challenging problems have emerged from diverse fields such as business, social sciences, and health sciences.  Examples may involve prediction of a health outcome for a new patient or perhaps prediction of a new school's response to efforts to educate children about smoking prevention.  The investigators have shown in previous work called classified mixed model prediction (CMMP) that in such cases, it is possible to make substantial gains in prediction accuracy by identifying a class that a new subject belongs to.  However, the scenarios under which CMMP currently operates are somewhat constrained and many real-life situations fall outside its scope.  Given the tremendous gains in accuracy that are possible, it would be very valuable to develop further methodology and computational advances to deepen knowledge in this area.    <br/><br/>This project aims to make methodological advances of the classified mixed model prediction method into other types of subject-level prediction problems as well as to develop new inferential methods along the CMMP idea, by making the latter truly useful in practical situations.  The basic idea of CMMP is to create a ""match"" between a group or cluster in the population for which one wishes to make prediction and a (massive) training dataset, with known groups or clusters. Once such a match is built, the traditional mixed model prediction method can be utilized to make accurate predictions. The practical challenges that will be solved in this project include i) how to deal with training data with unknown grouping; ii) how to deal with sparse, high dimensional covariates; iii) how to make better use of covariate information to improve accuracy of CMMP; and iv) how to provide accurate measures of uncertainty for CMMP-type predictions. Two important areas of application will be investigated.  One is in precision medicine and health disparities focusing on the prediction of epigenetic markers using high dimensional genotype profiles. The other comes from the area of family economics using a large survey of data from China where predictions at finer levels of resolution (e.g., households) are of primary interest. Both applications will leverage important collaborations with practitioners and thus increase the impact of the work.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1914632","Collaborative Research: Uncertainty Quantification, Optimal Designs and Calibration in Computer Experiments","DMS","STATISTICS","07/01/2019","06/19/2019","C. F. Jeff Wu","GA","Georgia Tech Research Corporation","Standard Grant","Yong Zeng","06/30/2023","$180,000.00","","jeffwu@isye.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1269","","$0.00","From aerospace designs to material science to biomedical studies, today's practice in engineering and physical sciences has made increasing use of computer simulations. How to design the computer simulations, analyze the computer output data, as well as to enhance the accuracy of the computer models are fundamental challenges in computer simulations. This project will focus on the development of a statistical framework for computer experiments, with the goal of developing both theoretical and methodological tools that cover the typical computer simulation pipeline from data collection to modeling and analysis to verification and validation. Specifically, the team plans to establish a new uncertainty quantification theory and foster novel methodologies for data mining, interpretation and decision making. The project will provide accurate, efficient and robust approaches that would make an impact on contemporary computer simulation practice. A graduate student will work on both the theoretical derivations and numerical verification for the project.<br/><br/>The project has three major objectives: (i) establish a statistically and computationally efficient uncertainty quantification framework for Gaussian process regression, (ii) propose a general experimental design scheme for multi-fidelity computer experiments, (iii) study the statistical properties and suggest efficient algorithms for novel calibration methods for computer models. The proposed work should lead to methodological development of a generic nature in the design, uncertainty quantification and calibration in computer experiments. The improved uniform error bounds can potentially lead to the use of fewer experimental runs for the same precision. Their significance can go beyond computer experiments such as in spatial statistics, which heavily uses kriging method. The optimal designs for nonstationary Gaussian Process models can help stimulate further development of experimental design theory in more complex situations. Standard approaches in experimental design do not pay much attention to the nonstationary situations. The proposed algorithm can substantially enhance the value of the projected kernel calibration (PKC) method. Although PKC is known to be theoretically superior, there is no known algorithm that can effectively calculate the PKC estimates. Because calibration is used to bridge the gap between computer simulations and physical experiments, this work can be potentially significant. Theoretical and technical advances made in this project can help facilitate further interactions between statistics, applied mathematics and probability theory, through journal publications, student exchange visits and presentations in interdisciplinary conferences, etc.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2015636","Leveraging Structural Information in Regression Tree Ensembles","DMS","STATISTICS","09/01/2019","01/30/2020","Antonio Linero","TX","University of Texas at Austin","Continuing Grant","Gabor Szekely","08/31/2020","$25,854.00","","antonio.linero@austin.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1269","8083","$0.00","A common task in statistics is prediction; for example, a practitioner may be interested in predicting the presence of a disease given genetic information about an individual. Due to recent advances in data collection, frequently one has access to datasets which contain a massive number of predictors, but with correspondingly few subjects. This setting is generally referred to as the ""big P, small n"" scenario. Drawing meaningful conclusions under such circumstances is generally impossible unless the underlying data satisfy certain structural assumptions. The simplest such structural assumption is that only a small number of the predictors are relevant; in this setting, finding the useful predictors corresponds to finding a so-called ""needle in a haystack."" The goal of this project is to construct procedures which adapt to this, and other, structural assumptions. The project will focus on methods based on decision trees, which are flowchart-like structures in which predictions are based on whether the predictors satisfy various rules. Usually an ensemble of decision trees are constructed, with the predictions for each individual tree averaged. While decision tree ensembles are frequently used with high dimensional data, it is unclear to what extent they adapt to the structural properties of the data. This project will show that, in practice, off-the-shelf decision tree ensembling methods do not adapt to common structural assumptions, and will develop new methods which do. In addition to developing methods with strong theoretical support, this project will support the development of an R package to give practitioners easy access to our methodology.<br/><br/>    <br/>The PI will develop Bayesian methods for incorporating structural information into tree-based ensemble methods, and establish theoretically the benefit of making use of this additional information. This forms a nonparametric counterpart to the parametric approaches used in linear models, such as the lasso, graphical lasso, or group lasso; Bayesian approaches in the parametric setting include the use of variable selection priors, such as spike-and-slab priors and global-local shrinkage priors. Structural information will be incorporated by modifying the commonly used priors on decision tree ensembles so that the prior is concentrated on models which satisfy the desired structure. The PI will first investigate the theoretical properties of a sparsity inducing prior which is designed to eliminate unnecessary predictors. Sparsity here is obtained by applying a sparsity inducing Dirichlet prior to the a priori probability that a given branch is associated to a given predictor. This prior will be extended to allow for grouped variable selection in a similar manner to the group lassoby considering the class of Dirichlet tree priors, and further to accommodate graphical structures in the predictors through sparsity inducing logistic normal priors. Additionally, the PI will develop computationally efficient Markov chain Monte Carlo algorithms to fit the resulting models. Compared to existing methods, these structural priors will be shown to lead to substantial gains in predictive accuracy, and to more accurate scientific discovery."
"1935729","Collaborative Research: Information-Based Subdata Selection Inspired by Optimal Design of Experiments","DMS","STATISTICS","05/10/2019","05/24/2019","John Stufken","NC","University of North Carolina Greensboro","Standard Grant","Pena Edsel","06/30/2022","$59,994.00","","jstufken@gmu.edu","1000 SPRING GARDEN STREET","GREENSBORO","NC","274125068","3363345878","MPS","1269","","$0.00","Extraordinary amounts of data are collected in many branches of science, in industry, and in government. The massive amounts of data provide incredible opportunities for making knowledge-based decisions and for advancing complicated research problems through data-driven discoveries. To capitalize on these opportunities, it is critical to develop methodology that facilitates the extraction of useful information from massive data in a computationally efficient way. Even the simplest analyses of the data can be computationally intensive or may no longer be feasible for big data. It is however often the case that valid conclusions can be drawn by considering only some of the data, referred to as subdata. This project develops optimal strategies for selecting subdata that retain, as much as possible, relevant information that was available in the massive data set. The methodology helps to identify the most informative data points, after which an analysis can proceed based on the selected subdata only. This facilitates data-driven decisions, scientific discoveries, and technological breakthroughs with computing resources that are readily available.<br/> <br/>Existing investigations for extracting information from big data with common computing power have focused on random subsampling-based approaches, which have as limitation that the amount of information extracted is only scalable to the subdata size, not the full data size. This project develops and expands the Information-Based Optimal Subdata Selection (IBOSS) method proposed by the PIs in the following directions: 1) It combines IBOSS with sparse variable selection methods in linear regression; 2) it develops subdata selection methods for generalized linear models; 3) it constructs computationally efficient algorithms for selecting the most informative subdata; and 4) it develops user-friendly software that supports the methodology.  The research is a significant addition to the field of big data science. It advances a new method for dealing with big data and has the potential to create novel research opportunities in statistical science and other quantitative fields. The results are valuable even when supercomputers are available, because cutting edge high performance computing facilities will always trail the exponential growth of data volume.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915829","Dynamic Modeling of Recurrent Events and Its Applications","DMS","STATISTICS","09/01/2019","08/19/2019","Yanqing Sun","NC","University of North Carolina at Charlotte","Standard Grant","Yong Zeng","08/31/2023","$139,868.00","","yasun@uncc.edu","9201 UNIVERSITY CITY BLVD","CHARLOTTE","NC","282230001","7046871888","MPS","1269","","$0.00","This project will develop some novel statistical methods for the recurrent events. The proposed research is motivated by the need to develop new statistical methodology for the malaria vaccine trial and other biometrical researches. The proposed statistical methods can be used to assess the risk factors of event occurrences, how the occurrences of events and event types are affected by concomitant variables, intervention, the past history, as well as the associations among different types of events. The proposed research is challenging in theoretical development and in statistical computation. The proposed statistical methods can facilitate the analysis of the malaria vaccine trial and contribute to develop efficacious malaria vaccines toward control and elimination of malaria. The development of the outlined research would enrich a collection of statistical tools for understanding the event histories and contribute to the efforts to overcome the medical and public health challenges and beyond.  The graduate student support will be used for research on the theory of malaria control. <br/> <br/>The research aims to develop nonparametric and semiparametric dynamic models for the recurrent events that will advance the existing statistical methods for recurrent events. The statistical methods are proposed to model and assess the risk factors of event occurrences, how the occurrences of events and event types are affected by concomitant variables, interventions, and the event history. New statistical methods also proposed to investigate the associations among different types of events.  The proposed research is motivated by the challenges arising from analyzing the malaria vaccine efficacy trialand has broach applications  in other fields such as biomedical, reliability and economics.  The proposed research includes two parts.  The research for the dynamic varying-coefficient intensity models of event occurrences is described in Part I which introduces three models: the nonparametric  dynamic varying-coefficient intensity model, the semiparametric dynamic varying-coefficient intensity model, and the dynamic models for multiple type of recurrent events. Research for modeling and assessing the associations among different types of recurrent events is proposed in Part II. The nonparametric dynamic varying-coefficient intensity models will be estimated based on the likelihood principle and the local linear smoothing technique. The profile estimation approach will be utilized for the semiparametric dynamic varying-coefficient intensity models. The inference procedures that are of scientific interest will be investigated. Large sample theory will be developed for the proposed estimation and hypothesis testing procedures. A two-stage estimating procedure is proposed to estimate the models for the conditional rate function of the recurrent event processes where the marginal semiparametric models are estimated in the first stage and the models for the rate ratio are estimated in the second stage. Studies are proposed to investigate a variety of models for the conditional rate ratio. New statistical methods, theory and computational algorithms will be developed. The proposed statistical methods can facilitate the analysis of the malaria vaccine trial and contribute to develop efficacious malaria vaccines toward control and elimination of malaria.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1921366","Scientific Computing Meets Machine Learning and Life Sciences","DMS","INFRASTRUCTURE PROGRAM, STATISTICS, MATHEMATICAL BIOLOGY","09/01/2019","09/07/2021","Linda Allen","TX","Texas Tech University","Standard Grant","Swatee Naik","08/31/2022","$25,500.00","Rattikorn Hewett, Linda Allen, Jingyong Su","linda.j.allen@ttu.edu","2500 BROADWAY","LUBBOCK","TX","79409","8067423884","MPS","1260, 1269, 7334","7556","$0.00","The workshop ""Scientific Computing meets Machine Learning and Life Sciences"" will be held on the campus of Texas Tech University in Lubbock, TX, from October 7 through October 9, 2019. This workshop will bring together leading experts and early career researchers from mathematics, statistics, computer science, machine learning, data sciences, and life sciences to report on cutting-edge and state-of-the-art computational algorithms in scientific computing and to identify computational and statistical challenges and open problems in machine learning and the life sciences. In addition, the workshop will provide a forum for an international and diverse group of researchers to foster communication, to facilitate new collaborative interactions, and to initiate joint research projects that will address the open and emerging issues and the computational and statistical challenges posed in machine learning and the life sciences. The three-day workshop will consist of presentations, posters, and group discussions that will stimulate an intensive exchange of ideas and foster fruitful interactions. This award supports the attendance of both researchers and graduate students, with priority given to graduate students, postdoctoral scholars, early career investigators, members of under-represented groups, and researchers who do not have other federal support. <br/><br/>Scientific computing is an increasingly important tool in many areas of science and engineering, such as biomedical imaging, genomics, proteomics, phylogeny, computer vision, and precision medicine, allowing biological data and systems to be explored that are not amenable to theoretical or experimental investigations. Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns, and make decisions with minimal human intervention. The advent of the big data era pushed machine learning to the forefront and has spurred broad interests in machine learning in recent years. The field of life sciences has advanced through a synergistic interplay between deep understanding of biology and mathematical techniques, especially from computational mathematics, probability, and statistics. Still, biologists are overwhelmed by the amount of data being generated and the new methods required for data-management. Quantitative theories are needed to help interpret and to contextualize observations. A variety of new challenges in scientific computing for machine learning have emerged in recent years that are related to the life sciences, such as developing predictive models for disorder detection, drug repurposing, toxicity prediction, electronic health record analysis, language translation, etc. These issues and many other open problems will be discussed among the diverse group of scientists participating in the workshop. More information is available at http://www.math.ttu.edu/scmlls2019/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1914556","Computer-Intensive Methods for Nonparametric Analysis of Dependent Data","DMS","STATISTICS","09/01/2019","07/29/2019","Dimitris Politis","CA","University of California-San Diego","Standard Grant","Yong Zeng","08/31/2023","$150,000.00","","dpolitis@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","","$0.00","Ever since the fundamental recognition of the potential role of the computer in modern statistics, the bootstrap and other computer-intensive statistical methods have been developed extensively for inference with independent data. Such methods are even more important in the context of dependent data where the distribution theory for estimators and test statistics may be difficult or impractical to obtain. Furthermore, the recent information explosion has resulted in datasets of unprecedented size that call for flexible, nonparametric, and, by necessity, computer-intensive methods of data analysis. Time series analysis in particular is vital in many diverse scientific disciplines, e.g., in economics, engineering, acoustics, geostatistics, biostatistics, medicine, ecology, forestry, seismology, and meteorology. As a consequence of the  proposal's development of efficient and robust methods for the statistical analysis of dependent data, more accurate and reliable inferences may be drawn from datasets of practical import resulting into appreciable benefits to society.  Examples include data from meteorology/atmospheric science (e.g. climate data), economics (e.g. stock market returns), biostatistics  (e.g. fMRI data), and bioinformatics (e.g. genetics and microarray data).<br/><br/><br/>The project focuses on the development of methods of inference for the analysis of dependent data that do not rely on unrealistic or unverifiable model assumptions. In particular, the principal investigator and his collaborators will work on: (a) Subsampling and resampling for Big Data, including bootstrap for multivariatetime series of large dimension; (b) New model fitting and resampling for ARMA (p,q) models with both p,q large; (c) New smoothing estimators of time-varying covariance matrices for locally stationary multivariate time series; (d) Resampling for time series with an (almost) periodic component; (e) Model-free Bootstrap for stationary and non-stationary data; (f) Estimating the degree of smoothness and support of the common density of stationary data; (g) Improved nonparametric estimation of a hazard rate function; (h) A bootstrap test for the null hypothesis of `overdifferencing'; (i) Markov-type resampling and Linear Process Bootstrap for stationary random fields; and (j) Different aspects of resampling of functional and high-dimensional time series.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1914882","Collaborative Research: Spectral Functional Principal Components on Abelian Groups with Applications to Spatial Functional Data","DMS","STATISTICS","08/01/2019","07/22/2019","Piotr Kokoszka","CO","Colorado State University","Standard Grant","Yong Zeng","07/31/2023","$120,101.00","","Piotr.Kokoszka@colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1269","","$0.00","Massive data sets on gridded 2D and 3D domains have recently become available through computer climate model outputs, records from satellite remote sensing and brain scans, among others. These data sets have both temporal and spatial dimension. For example, a state of vegetation is observed on a grid covering an agricultural area at regular time intervals, every day or every week.  Such data can be viewed as functions of time, one function per spatial grid unit. Their chief characteristic is the spatial dependence of curves observed at the grid nodes. There is an increasing need to develop statistical tools, which will allow researchers to extract useful information from such data. The PIs will develop such tools.  The data and problems that motivate this research arise in several science fields, which have important impacts on society. For example, conclusions drawn from future climate models help the government and corporations plan for the allocation of various assets. Brain research on trauma experienced by military veterans and on Alzheimer's disease are recognized as important societal goals. The statistical research the PIs will conduct will provide useful quantitative tools to help scientists in these fields. Mathematical foundations of the new approach will be created, together with domain-specific approaches. The new methods will be implemented in R packages and made available to research community, government agencies and commercial enterprises. In the course of the proposed research, two Ph.D. students will be trained. <br/><br/>The PIs will create a new framework for inference for functional data defined on domains with an additive group structure. The new dimension reduction approach will have characteristics of a multi-scale, data-driven representation, which takes into account the dependence of the functions defined on group elements, for example spatial grid nodes. The PIs will use methods of Fourier analysis on Abelian groups, spectral theory for functional data, invariance principles in Hilbert spaces, computationally efficient spatio-temporal spline representations, routines for downloading and manipulating massive data sets. The PIs will develop several inferential procedures, including bootstrap-based inference, tests for the spatial and distributional structure, and applications to the evaluation of the accuracy of computer climate models. The PIs will also develop corresponding computational techniques, which will lead to the computationally fast representation of various data structures of large to massive size.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916013","Efficient Inferences in Task-Evoked Functional Magnetic Resonance Imaging Studies","DMS","STATISTICS","09/01/2019","08/18/2021","Lan Liu","MN","University of Minnesota-Twin Cities","Continuing Grant","Yong Zeng","08/31/2023","$119,999.00","","liux3771@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","Functional magnetic resonance imaging (fMRI) is a popular technology that measures brain activity by detecting changes associated with blood flow. A fundamental challenge of fMRI has been the limited sample sizes. The main reason for such a small sample size is the high scanning cost. If the sample size could be shrunk without compromising the information, a great amount of money could be saved and many additional cohorts could be explored. On the other hand, the rapid technological advances in brain imaging made it possible and routine to obtain high-resolution imaging data. While traditional methods of analysis may have produced acceptable results when the imaging data was in low resolution, the high dimensional images demand better statistical methods for more precise and efficient estimations for task fMRI studies. The research stands to make a significant contribution by better identifying the key components that play an important role in the learning, memorizing process and shed light on the cognitive growth. The methods to be developed have the potential to be modified for analyzing other types of high-dimensional data with limited sample sizes in medical research, such as genomics data. The program will also significantly impact the training of undergraduate and graduate students.<br/><br/>The goal of this project is to develop a series of comprehensive statistical methodologies for conducting efficient multivariate inferences for task fMRI studies. The investigators provide efficient methods to estimate the effect of exposure and personal characteristics on multivariate responses at a single time point as well as on the time-series data. The importance of the research lies in both the statistical methodology development and the interdisciplinary application. The research will provide theoretical justification for simultaneous dimension reduction and efficient estimation in the high dimensional data setting. Specifically, this research overcomes the technical difficulty that the dimension of brain imaging data is usually hundreds or thousands of times the sample size.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916204","Statistical Modelling and Inference for Next-Generation Functional Data","DMS","STATISTICS","08/01/2019","07/08/2021","Lily Wang","IA","Iowa State University","Standard Grant","Gabor Szekely","11/30/2021","$165,269.00","","lwang41@gmu.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1269","102Z","$0.00","With the rapid growth of modern technology, many large-scale imaging studies have been or are being conducted to collect massive datasets with large volumes of imaging data, thus boosting the investigation of ""next-generation functional data"". These enormous collections of imaging data contain interesting information and valuable knowledge, which has raised the demand for further advancement in functional data analytic approaches. Although functional data analysis (FDA) has gained widespread popularity in recent years, enhancing the capability of next-generation FDA remains a long-standing challenge. This research targets integrating state-of-the-art statistical modeling devices with modern computational and inferential techniques to develop a set of flexible and intelligent statistical tools to enable learning and discovery from next-generation functional data. The efficacy of the tools developed in this research will be tested by neuroimaging studies. The proposed methods and theory are also applicable to a broader range of fields that require modeling and analysis of images and other complex data types collected over space and/or time, such as geography, environmental science and remote sensing studies. The graduate student support will be used for day-to-day research activities, including parts of the theory/methodology developments and data analysis. <br/><br/>This research will enrich the methods for dealing with functional data observed from complex data objects (high-dimensional, correlated images or shapes), which commonly arise in imaging studies, such as, health/medical imaging or remote sensing imaging. The PI aims to address some challenging research problems in analyzing next-generation functional data by: (1) innovating a statistically sound framework to extract useful information from large-scale longitudinal imaging studies; (2) developing flexible and intelligent statistical models to delineate the association between massive imaging data and covariates of interest and to characterize and visualize the spatial variability of the imaging data; and (3) developing efficient, scalable algorithms with high-performance statistical software packages to meet the challenges posed by dynamic imaging studies. In particular, the proposed research involves four projects. Project 1 provides a unifying approach to characterize the varying association between imaging responses with a set of explanatory variables. Project 2 focuses on the interface between high-dimensional and next-generation functional data to address several fundamental bottlenecks in large-scale imaging genetics studies. Projects 3 and 4 deal with longitudinal/dynamic imaging studies, and a comprehensive functional regression framework to analyze repeated functional responses from these studies will be developed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915711","High-Dimensional Inference beyond Linear Models","DMS","STATISTICS","10/01/2019","07/29/2019","Bin Nan","CA","University of California-Irvine","Standard Grant","Pena Edsel","09/30/2022","$199,994.00","","nanb@uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1269","","$0.00","Regression models are widely used in investigating the associations between a set of predicting variables, the so-called covariates, and some outcome variable. Estimates of regression coefficients and their confidence intervals provide useful information, for example, the importance of certain genetic variants to lung cancer, or brain regions associated with memory loss in an aging population. With the advent of big data era, regression models with many covariates have been commonly used to tackle many important scientific problems in areas such as genomics, neuroimaging, business, engineering, information technology, and other biomedical studies, and sometimes the number of covariates (e.g. genetic variants) is even greater than the sample size (e.g. the number of study participants). Making statistical inference (i.e. constructing confidence intervals for regression coefficients) for a large number of covariates becomes a challenging issue because the conventional methods such as the maximum likelihood estimation may either not exist or yield biased estimates. It has been shown in recent years that the regression coefficients can be estimated by using regularized methods, e.g., the lasso approach. However, it is also well-known that the regularized methods yield biased estimates, thus cannot be directly used for making statistical inference, in particular, for constructing confidence intervals. Some researchers have shown that proper statistical inference can be made in linear regression models after implementing a clever de-biasing procedure. However, it is also found that the de-biased method does not work beyond linear models. Without imposing restrictive assumptions, theory and methods will be developed for the generalized linear models and the Cox regression model with a large number of covariates, as well as for the functional regression models with applications in brain imaging studies. Proper distributional theory and confidence intervals will be provided, which will lead to more reliable results in scientific research.   <br/><br/>The existing de-biased methods do not successfully correct the bias in nonlinear models, e.g., the generalized linear models or the Cox model, leading to poor results in statistical inference. The main causes of the problem include the unrealistic sparsity assumption imposed on the inverse expected Hessian matrix, and that the ""negligible"" terms in the existing de-biased methods are in fact not negligible. In this project, two methods that further de-bias the lasso estimators without relying on the assumption of sparse inverse expected Hessian matrix will be considered: (i) directly inverting the Hessian matrix when the number of regression parameters is less than the sample size; (ii) eliminating the major bias term without using the inverse of Hessian matrix - a quadratic programming approach, which can potentially handle the case with larger number of regression parameters than the number of observations. Additional challenges arise in the Cox regression with high-dimensional covariates, where the partial-likelihood-based loss functions for all the observations are not i.i.d., and each loss function is not Lipschitz. The proposed method will be approximating the loss function to yield i.i.d. losses and extended to handling multivariate and clustered survival data with even more complicated loss functions. For the brain imaging data, functional regression model using Haar wavelet basis is investigated. The major added challenge is to characterize the impact of the approximation error using Haar wavelets on the asymptotic distribution of the refined de-biased functional estimation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916115","Collaborative Research: Generalized Fiducial Inference in the Age of Data Science","DMS","STATISTICS","08/01/2019","07/16/2019","Jan Hannig","NC","University of North Carolina at Chapel Hill","Standard Grant","Yong Zeng","07/31/2023","$150,000.00","","jan.hannig@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","Data and their use have become extremely important in modern society. This provides for an urgent need to study mathematical foundations of statistics and data science. In this project the PIs explore interaction of generalized fiducial inference with modern data science problems and techniques. There are several benefits of the proposed research. First, it is expected that the proposed research will increase our understanding of inference and relationships between the frequentist, fiducial and Bayesian paradigms and how do these paradigms fit into data science which the aim to improve better data science practice. Second, it is expected to lead to new and efficient procedures for quantifying uncertainty in a number of applications. An important example is the calibration of likelihood ratios reported by data science algorithms in forensic science that has potential implication for practical usage of likelihood ratios in courtroom. Additionally, the project will provide research opportunities to graduate students and, in particular, help train women and minority graduate students in the field that is of a great benefit to society.<br/><br/><br/>Beginning around the year 2000, the PIs and collaborators started to re-investigate the ideas of fiducial inference and discovered that Fisher's approach, when properly generalized, opens doors to solve many important and difficult problems of uncertainty quantification. After many years of preliminary investigations, the team was able to put together a coherent, well thought out plan for a systematic research program in this area. The PIs termed their generalization of Fisher's ideas as generalized fiducial inference (GFI). The PIs are now working towards applying their GFI methodology to handle data science problems that have emerged due to our ability to collect massive amounts of data rapidly. In particular the PIs propose to conduct research into the following topics: (1) In-depth investigation of fundamental issues of GFI so that they can be simply used on manifolds, with constraint, and penalties. This is essential for applicability. (2) Development of a bias free fiducial selector, so that a sparsity of the fiducial distribution is induced as a natural outcome of a minimization problem and unbiasedness is achieved using a novel de-biasing approach. (3) Interplay between objective Bayesian and fiducial solutions for covariance estimation. (4) Uncertainty quantification for graphon and regression with network cohesion. (5) Use of deep networks for computation of GFI. (6) Applications of GFI to a wide variety of practical problems; e.g., calibration of likelihood ratios used for quantifying evidence in forensic science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915842","Flexible Statistical Modelling for High Dimensional Data","DMS","STATISTICS","09/01/2019","07/09/2019","Hui Zou","MN","University of Minnesota-Twin Cities","Standard Grant","Yong Zeng","08/31/2023","$179,535.00","","hzou@stat.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","079Z","$0.00","Scientific and technology innovations have made massive high-dimensional data ubiquitous in various fields, such as biological science, medical studies, public health, social sciences, e-commerce, finance, climate studies, and so on. During the past decade statisticians have developed a rich collection of new tools for high-dimensional statistical modeling. Despite these important advances, there are still many challenges and open problems to be dealt with in high-dimensional data analysis. Their solutions require innovative ideas and techniques to handle the methodological, computational and theoretical challenges. The goal of this research is to develop mathematically solid and computationally efficient methods to address these pressing and important inferential challenges. <br/><br/>This research consists of three projects. The first project concerns measurement errors in high-dimensional M-estimation. The PI will study a new unified convex approach to solve the error-in-variables penalized M-regression including Huber regression, logistic regression, quantile regression, and the support vector machine. In the second project the PI will establish a new inference tool named composite M-estimation and demonstrate its applications in high-dimensional learning. In the third project the PI will study a flexible heterogeneity pursuit method for understanding the heterogeneity effects in high-dimensional data. Software packages will be created to make the new methods readily available to other researchers and practitioners.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916419","Optimal Bayesian Inference Under Shape Restrictions","DMS","STATISTICS","08/01/2019","07/30/2019","Subhashis Ghoshal","NC","North Carolina State University","Standard Grant","Yong Zeng","07/31/2023","$200,000.00","","subhashis_ghoshal@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1269","","$0.00","In many contexts of statistical modeling, the shape of a function used in modeling plays a key role. Prominent examples are increasing trend of the Arctic ice sheet melting and the rising sea levels under climate change. Many inverse problems such as deconvolution, or estimation under censoring also lead to shape restrictions on the concerned functions. While estimating these quantities and quantifying the uncertainty in their inference, such shape restrictions should be taken into consideration. Testing for an increasing trend or a similar shape restriction is also important for validating a theory leading to such a shape restriction. In this project, Bayesian methods, which combine prior information and observed data to make an inference, will be developed in the context of shape-restricted models. The results will be applied in various fields of interest. The proposed research, apart from developing new ideas, methods and computational techniques for answering related mathematical questions, will provide a significant impact on making decisions in various application such as climate change, tumor size monitoring, and censored data. Research findings will be disseminated through arXiv preprints, journal publications, talks in conferences and various institutions, and through special topics courses. The software will be developed and distributed for free through CRAN and PI's website. The PI is highly committed to doctoral student advising and promoting diversity, especially from women and underrepresented groups. Twenty-six doctoral students already graduated and four are currently working with him. The PI's NSF grants also supported his doctoral students to travel to conferences. The PI also has the track record of promoting the representation of women and minorities through the conference support grants he obtained. In total 21 female  researchers and 4 from under-represented groups and many young U.S. participants were supported. The PI will continue promoting diversity in research related to this proposal. The graduate student support will be used on shape-restricted inference research and on writing computer codes for the resulting formulae. <br/> <br/>Shape restricted inference has been studied well from the maximum likelihood perspective, but Bayesian methods have been less developed. In the Bayesian approach, additional information in the form of the qualitative shape restriction may be naturally blended in the prior. Uncertainty in the concerned functions can be quantified by Bayesian credible regions, which are relatively easy to obtain from posterior sampling. The frequentist coverage of such sets is important to know. In this proposal, a new computationally advantageous Bayesian approach based on a ``projection posterior'' will be adopted, which will also be easier to analyze theoretically. Suitable priors for shape restricted inference such as those obtained from step functions and B-splines series will be developed for both univariate and multivariate shape restrictions, and the projection posterior will be studied. Local and global posterior contraction rates will be established. Asymptotic frequentist coverage of Bayesian credible intervals for a regression or density function at a point under monotonicity or other shape constraints will be obtained.  A recalibration step will be used to adjust the coverage to meet a targeted value. Asymptotically optimal and computationally advantageous Bayesian tests for shape restrictions will be developed. Results will be extended to other types of univariate shape restrictions like convexity or log-concavity and to multivariate monotonicity and convexity settings in regression, density estimation, and survival analysis. The methods developed will be applied in diverse contexts including climate change and medical data. The proposed research may open up a completely new path for the Bayesian approach in shape-restricted inference and reconcile Bayesian and frequentist uncertainty quantification under shape restriction and may serve as a seed for further development in the years to come.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1914937","New Methods in High-Dimensional Causal Inference","DMS","STATISTICS","09/01/2019","08/15/2019","Debashis Ghosh","CO","University of Colorado at Denver","Standard Grant","Yong Zeng","08/31/2022","$149,779.00","","debashis.ghosh@cuanschutz.edu","13001 E 17TH PLACE F428","AURORA","CO","800452571","3037240090","MPS","1269","079Z","$0.00","With the popularity of deep learning algorithms in the scientific literature as well as by their use and development by companies such as Google, Microsoft and Facebook, their deployment in a variety of tasks is rapidly accelerating.   Because of this, better mathematical understanding of deep learning, and more broadly, machine learning algorithms in decision making processes are needed.  A key innovation that is needed is how to marry machine learning algorithms into causal modelling procedures, which will help algorithms to develop ""reasoning capabilities"" (e.g., understand why an algorithm is making the decision/prediction that it makes).   <br/> <br/>The problem that will be addressed in this project is how to model confounders so as to develop causal effect estimators that have desirable sampling properties.  In addition, it is important to have well-justified procedures that computationally scale with the number of observations.  In this project, the PI and team will focus their research in two areas.  The first will be to understand the implications of deep learning algorithms and their performance on foundational assumptions for the popular potential outcomes model.  In recent work, the PI discovered a fundamental tension between Gaussian process classification algorithms, covariate overlap and regularity of causal effect estimators.  The goal of the first aim of the research will be to see if a similar phenomenon holds for deep learning algorithms.   In addition, the interpolation properties of Gaussian process and deep learning-based classification algorithms will be explored.   The second part of the project will deal with developing scalable algorithms for causal effect estimation.   New computationally scalable algorithms for causal effect estimation will be developed as part of this research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2001433","CAREER: Hierarchical Models for Spatial Extremes","DMS","OFFICE OF MULTIDISCIPLINARY AC, STATISTICS, Division Co-Funding: CAREER","09/01/2019","12/12/2022","Benjamin Shaby","CO","Colorado State University","Continuing Grant","Yong Zeng","06/30/2025","$429,135.00","","bshaby@colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1253, 1269, 8048","1045, 1515","$0.00","Rare events can have crippling effects on economies, infrastructure, and human health and well being. But in order to make sound decisions, understanding how large the most severe events are likely to be is imperative. The PI will focus on developing statistical tools for understanding the spatial structure of the most extreme events. These new tools will improve on existing models because they will be both more realistic and more computationally tractable.  The PI will also apply these tools to help scientists and policymakers study risks posed by severe environmental phenomena like inland floods, wildfires, and coastal storm surges. Furthermore, the PI will organize workshops to foster closer integration of statistical and Earth science research, as well as develop graduate courses and a textbook focused on modern statistical methods for Earth science.<br/><br/>The PI will develop stochastic models for extreme events in space that are 1) flexible enough to transition across different classes of extremal dependence, and 2) permit inference through likelihood functions that can be computed for large datasets.  It will accomplish these modeling goals by representing stochastic dependence relationships conditionally, which will induce desirable tail dependence properties and allow efficient inference through Markov chain Monte Carlo (MCMC). The first research component will develop sub-asymptotic models for spatial extremes using max-infinitely divisible (max-id) processes, a generalization of the limiting max-stable class of processes, based on a conditional representation.  The second research component will develop sub-asymptotic spatial models for extremes based on scale mixtures of spatial Gaussian processes.  The PI will conduct closely interwoven computational development and theoretical explication of the joint tail dependence that the proposed hierarchically specified max-id and scale mixture processes induce.  Finally, the PI will apply these models to problems of high societal impact, such as extreme precipitation risk, wildfire susceptibility, and coastal storm surge exposure.  The PI will enhance connections between extreme value statisticians and communities of climate and atmospheric scientists, mitigation researchers, and stakeholders, through 1) biannual international workshops on weather and climate extremes, 2)  a Ph.D. level course in spatial statistics which will include new advances and applications of spatial extremes, and 3) writing the textbook Modern Statistics for Earth Scientists.  The PI also will add modules on extremes to Penn State's Sustainable Climate Risk Management (SCRiM) summer school, and contribute to SCRiM's electronic resources and interactive teaching materials for educators, decision makers, underrepresented groups, and the general public.  The PI will strengthen existing collaborations with government agencies which are responsible for communicating and mitigating risk to the public posed by extremal environment phenomena.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916251","Collaborative Research: Integrating Multi-Dimensional Omics Data for Quantifying Disease Heterogeneity","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","08/15/2019","08/07/2019","Shuangge Ma","CT","Yale University","Standard Grant","Yong Zeng","07/31/2023","$150,000.00","","shuangge.ma@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269, 7454","068Z","$0.00","Many complex diseases such as cancer demonstrate significant across-patient heterogeneity. For a better understanding of disease biology and optimally selecting treatment strategies, it is important to properly model disease heterogeneity.  This project will develop a novel framework for modeling disease heterogeneity through the effective integration of information from multiple types of highly complex omics measurements.  The proposed analysis framework and approaches will have significant broader impact.  Applications of the methods will lead to more accurate identification of heterogeneous patient groups as well as their omics characteristics, which will facilitate the identification/definition of disease subtypes, treatment selection, and clinical decision-making.   Data on skin and lung cancer will be analyzed leading to heterogeneity models that will be valuable to basic science researchers and clinicians.  The project also involves education and training of graduate students at Yale University and the University of Iowa.<br/><br/>High-dimensional omics data have been shown to be highly effective for heterogeneity analysis. Taking advantage of recent developments in multi-dimensional profiling under which data are collected on multiple types of omics measurements, the investigators will systematically develop novel integrated analysis strategies and approaches. Specifically, three sets of methods will be developed under the novel PFR (penalized fused regression) framework. Model averaging will be further developed to facilitate computation and provide additional insights into the proposed approaches. Extensive and rigorous methodological, computational, and theoretical investigations will be conducted.  This project will make fundamental contributions to high-dimensional statistics and disease heterogeneity analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915803","Highly Multivariate Geo-Statistics Using Graphical Models","DMS","STATISTICS","07/01/2019","06/15/2021","Abhirup Datta","MD","Johns Hopkins University","Continuing Grant","Yong Zeng","06/30/2023","$180,000.00","","abhidatta@jhu.edu","3400 N CHARLES ST","BALTIMORE","MD","212182608","4439971898","MPS","1269","","$0.00","Researchers in forestry, ecology, climate sciences, environmental health, and many other fields routinely analyze geo-tagged data collected at thousands of locations using spatial statistics. Modern Geographical Information Systems (GIS) are empowered to simultaneously measure many different variables at each location. This heralds a shift towards a multivariate paradigm in spatial statistics. A joint analysis of all the variables help identify common geographical patterns and sources for the different variables. In this project, the PI pursues statistical methodology that can adequately address the emerging complexities of such highly-multivariate geospatial datasets. The innovations include a) utilizing available scientific information about the dependence among variables, b) ensuring computational scalability of the algorithms, and c) improving interpretability of findings from the multivariate analysis. The genesis of the proposed innovations lies in substantive questions related to climate modeling, air and water quality. These research domains study some of the most threatening challenges to the human society in the twenty-first century. The statistical methods developed in this project will enable practitioners in these fields to conduct highly-multivariate spatial analysis using modest computing resources. The project also provides the opportunity to train graduate students in many diverse and essential areas of statistics as well as in advanced statistical computing.<br/><br/>Gaussian Processes (GPs) have long been used for modeling multivariate spatial surfaces. Multivariate GPs are often created by mixing univariate ones which obfuscate the individual spatial characteristics of each resultant surface. Direct constructions like the multivariate Matern GP are more interpretable but entail complex parameter constraints offering little flexibility to exploit prior information about inter-variable dependence. The PI proposes a novel procedure to create multivariate GPs that endows each surface with an interpretable GP measure with surface-specific variance, smoothness and spatial decay, but also enables incorporating the dependency network among the variables into the construction. A recurrent theme throughout is the versatile exploitation of graphical models. Graphs defined in space, time and variable domains are used to create multivariate GPs with desirable properties in terms of interpretation, computation and structure. Another accompanying theme is utilizing a standard decomposition of GPs to extend the discrete construction to well-defined continuous stochastic processes, thereby enabling predictions at any new location. Novel, simple, but efficient strategies will be explored for parameter estimation. Finally, the PI separately focuses on non-Euclidean spatial domains like estuaries and river networks. New univariate GPs will be devised that respect the complicated contours of these domains. Subsequently, harmonious application of graphical models will create multivariate locally smooth GPs to analyze multivariate spatial data on such domains.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915894","Learning Latent Graphs from Stationary Signals","DMS","STATISTICS","07/15/2019","07/10/2019","Jie Peng","CA","University of California-Davis","Standard Grant","Yong Zeng","06/30/2024","$300,000.00","Debashis Paul","jiepeng@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","079Z","$0.00","In recent years, data with many features and complicated interactions among these features and/or across time and space have become ubiquitous. How to extract meaningful information from such large complex data is one of the most pressing questions with significant scientific and societal implications. This research will generate new tools for modeling and analyzing such data through graph- or network-based representations of the data.  The analytical and computational tools derived from this research can be applied to many fields including economics, finance, neuroscience, and various sub-fields within the social and biological sciences. This project will also provide training opportunities for a new generation of researchers empowering them to contribute to the rapid development of statistics, data science and related fields. Results of this research will be disseminated through publications, conference presentations, lectures, and open source software.<br/>  <br/>This project will develop a novel Spectral Graph Models (SGM) framework which models multivariate data as graph-referenced stationary signals. The SGM framework provides new tools for network inference from a signal processing perspective and for modeling multivariate observations with complicated dependencies, including possible temporal or spatial dependence. It also provides new tools for covariance estimation through efficient graph-based representations. The SGM framework combines four key aspects - spectral representation of covariances, spectral graph theory, semiparametric modeling, and sparse parameterization. It allows for temporal dependency in graphs or parameters and can be used to model both independent and dependent multivariate observations. The SGM framework has a wide range of applications, and methods developed through this research will be applied to various types of data including brain activity and international trade data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1848579","CAREER: New Change-Point Problems in Analyzing High-Dimensional and Non-Euclidean Data","DMS","STATISTICS","07/01/2019","07/14/2023","Hao Chen","CA","University of California-Davis","Continuing Grant","Yong Zeng","06/30/2024","$400,000.00","","hxchen@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","1045","$0.00","In this big data era, massive data sequences are collected in various scientific fields for studying complicated phenomena over time and space, including neuroscience, epidemiology, social science, computer vision, and astronomy.  Change-point analysis is a crucial early step in analyzing these data sequences, such as, to raise an alarm when an abnormal event happens in online data monitoring, and to segment a long sequence into more homogeneous parts for follow-up studies.  To accommodate modern applications, the ability to deal with high throughput data and data with complicated structures is becoming a necessity.  Parametric methods usually cannot be applied to very high dimensions unless strong assumptions are made to avoid the estimation of a large number of nuisance parameters.  This project focuses on developing non-parametric change-point detection methods that are free of strong assumptions and computationally scalable to high dimensional and complex data.  This project provides students and researchers with exciting new research problems that have both statistical and scientific importance.  The training component for undergraduate and graduate students will prepare new researchers with inter-disciplinary education.<br/><br/>This project will develop a new scan statistic framework through a novel adaptation of graph-based methods.  The PI has shown that the graph-based approaches scale to high-dimensional and non-Euclidean data, and allow for universal analytic permutation p-value approximations that is decoupled from application-specific modeling, facilitating their applications to large and complicated data sets.  Despite the good properties of the graph-based methods, there are still some gaps between its current versions and many modern applications. This project aims to fill those important gaps.  In particular, this project will (1) develop new graph-based approaches to effectively integrate information from multiple sources, which is common in many application areas, such as smart homes and smart cities, and seek ways to distribute the new approaches to local centers to avoid the excessive transmission of raw data in a distributed system; (2) develop treatments from the level of constructing the graph to deal with dependent data, which is more effective than a circular block permutation framework developed by the PI earlier; and (3) develop a new framework to provide analytic power approximations to the graph-based methods that kick in for sample sizes in hundreds and thousands even for high-dimensional data and non-Euclidean data, facilitating researchers to make better decisions in real applications.  These methodological and theoretical developments will provide better understandings of modern complicated data sequences from diverse fields, which will further advance the understanding of major scientific problems in these fields.  The tools developed in this project will be distributed as open source software packages with detailed documentations.  This will enhance the collaboration between the statistics community and researchers from broader scientific fields, and make data analysis procedures more transparent.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1932751","Workshop on Risk Analysis for Extremes in the Earth System","DMS","STATISTICS","08/01/2019","07/28/2019","Benjamin Shaby","PA","Pennsylvania State Univ University Park","Standard Grant","Pena Edsel","07/31/2020","$13,000.00","","bshaby@colostate.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1269","7556","$0.00","The ""Workshop on Risk Analysis for Extremes in the Earth System"" will take place at Lawrence Berkeley National Lab in Berkeley, Califirnia, on July 22-24, 2019.  This workshop will serve two complementary purposes. The first is to provide a venue for statisticians and Earth scientists to share ideas and facilitate collaboration aimed at understanding rare, high-impact events in the Earth system. The second is to introduce graduate students, as well as others wishing to enter into the area of extreme value analysis, to the techniques and challenges that are unique to studying rare events. <br/><br/>The first day of the workshop will feature a pair of short courses, one on contemporary advances in statistical analysis of extreme events, and the second on using the NIMBLE and climextRemes software for data analysis. The following two days will feature research talks on statistical methodology and applications related to extremes, as well as talks on the science and impacts of extreme events in the Earth system. These talks will include several by researchers in both statistics and fields related to Earth science. Finally, there will be a poster session on the evening of Tuesday, July 23.  For more information, see https://cascade.lbl.gov/upcoming-workshops-conferences/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915904","Collaborative Research: Stochastic Models for Gene-based Association Analysis of Longitudinal Phenotypes with Sequence Data","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","08/01/2019","07/30/2019","Ruzong Fan","DC","Georgetown University","Standard Grant","Yong Zeng","07/31/2022","$179,890.00","","rf740@georgetown.edu","MAIN CAMPUS","WASHINGTON","DC","200570001","2026250100","MPS","1269, 7454","068Z","$0.00","Longitudinal genetic studies provide a valuable resource for exploring key genetic and environmental factors that affect complex traits over time. Genetic analysis of longitudinal data that incorporates trait variation over time is critical to understanding genetic influence and biological variations of complex diseases.  In recent years, many genetic studies have been conducted in cohorts in which multiple measures on a trait of interest are collected on each subject over a period of time in addition to genome sequence data.  These studies not only provide a more accurate assessment of disease condition but enable researchers to investigate the influence of genes on the trajectory of a trait and disease progression.   This project focuses on the development of novel association testing methods to analyze sequencing genomic data at gene levels.  The research will help provide insights into the underlying biology and progression of complex diseases.<br/><br/>In longitudinal genetic studies and data from the Electronic Medical Records and Genomics (eMERGE) network, phenotypic traits and genetic variants may be viewed as functional data.  Functional data analysis (FDA) can serve as a valuable tool for exploring key genetic and environmental factors that affect complex traits over time.  In the presence of a large number of rare variants, gene-based analysis is a more powerful tool for gene mapping than testing of individual genetic variants.  This project seeks to develop stochastic functional regression models and longitudinal sequence kernel association tests (LSKAT) to analyze longitudinal traits of population samples and pedigree or cryptically related samples, and to analyze pleiotropic traits.  FDA techniques and kernel-based approaches are utilized to reduce the high dimensionality of sequencing data and draw useful information. A variance-covariance structure is constructed to model the measurement variation and correlations of an individual's trait based on the theory of stochastic processes and novel penalized spline models are used to estimate the trajectory mean function.  The proposed methods and software will be tested and refined using real data sets and simulation studies.  User-friendly software will be developed to implement the proposed methods and will be made publicly available.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916125","Collaborative Research: Generalized Fiducial Inference in the Age of Data Science","DMS","STATISTICS","08/01/2019","07/16/2019","Thomas Chun Man Lee","CA","University of California-Davis","Standard Grant","Yong Zeng","07/31/2023","$120,000.00","","tcmlee@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","Data and their use have become extremely important in modern society. This provides for an urgent need to study mathematical foundations of statistics and data science. In this project the PIs explore interaction of generalized fiducial inference with modern data science problems and techniques. There are several benefits of the proposed research. First, it is expected that the proposed research will increase our understanding of inference and relationships between the frequentist, fiducial and Bayesian paradigms and how do these paradigms fit into data science which the aim to improve better data science practice. Second, it is expected to lead to new and efficient procedures for quantifying uncertainty in a number of applications. An important example is the calibration of likelihood ratios reported by data science algorithms in forensic science that has potential implication for practical usage of likelihood ratios in courtroom. Additionally, the project will provide research opportunities to graduate students and, in particular, help train women and minority graduate students in the field that is of a great benefit to society.<br/><br/><br/>Beginning around the year 2000, the PIs and collaborators started to re-investigate the ideas of fiducial inference and discovered that Fisher's approach, when properly generalized, opens doors to solve many important and difficult problems of uncertainty quantification. After many years of preliminary investigations, the team was able to put together a coherent, well thought out plan for a systematic research program in this area. The PIs termed their generalization of Fisher's ideas as generalized fiducial inference (GFI). The PIs are now working towards applying their GFI methodology to handle data science problems that have emerged due to our ability to collect massive amounts of data rapidly. In particular the PIs propose to conduct research into the following topics: (1) In-depth investigation of fundamental issues of GFI so that they can be simply used on manifolds, with constraint, and penalties. This is essential for applicability. (2) Development of a bias free fiducial selector, so that a sparsity of the fiducial distribution is induced as a natural outcome of a minimization problem and unbiasedness is achieved using a novel de-biasing approach. (3) Interplay between objective Bayesian and fiducial solutions for covariance estimation. (4) Uncertainty quantification for graphon and regression with network cohesion. (5) Use of deep networks for computation of GFI. (6) Applications of GFI to a wide variety of practical problems; e.g., calibration of likelihood ratios used for quantifying evidence in forensic science.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916446","Theory and algorithms for computational sufficiency","DMS","STATISTICS","09/01/2019","07/30/2019","Vincent Vu","OH","Ohio State University","Standard Grant","Yong Zeng","08/31/2022","$120,000.00","","vqv@stat.osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1269","","$0.00","The extraction of information from data is of fundamental importance. Central to this problem is defining ""information"", determining what in the data is most relevant and deciding which algorithms/procedures should be used.  Technological advances in data acquisition and storage have led to an over  abundance of data, which has only exacerbated the problem. The field of statistics has addressed these issues since its inception; the traditional point of view relies on stringent assumptions about the data and probabilistic models, but has ignored computational aspects. This project will develop a new theory and algorithms based on a concept called computational sufficiency. Rather than making stringent assumptions about the data and models, this concept assumes that many different and complementary procedures might be applied to the data. Then we consider the relevant information in the data to be that which is sufficient for obtaining the results of all the  procedures under consideration. In this way, the theory can (i) reveal hidden commonalities between procedures, (ii) identify meaningful reductions of the data, (iii) provide results that are robust to modeling assumptions, and (iv) allow us to exploit the reductions for efficient computation.<br/><br/>Statistical sufficiency has been a foundational concept of mathematical statistics since the early 20th century. Implicit in its definition is the specification of a statistical model. However, actual data analysis does not always begin with the specification of a model, and it may not even make explicit use of a statistical model. The abundance of data and advances in computing have made it easier for the data analyst to abandon a single statistical model and instead consider multiple algorithmic models. Computational sufficiency defines information in the context of a collection of procedures that share a common input domain. The basic idea is very simple: we wish to find functions of the data that contain sufficient information for computing every procedure in the collection. In other words, what are the reduced summaries of the data that are sufficient to produce the same answers as would be obtained by applying the procedures to the whole data. This project will broaden the applicability of this theory and its depth. The results will lead to both theoretical insights and practical  computational advances in data analysis. In some cases, the theory will be  able to provide a conceptual link between seemingly unrelated methods and it will also provide a basis for inferences that are less dependent on  assumptions. The computational advances provided by this research can lead to  reduction of computational costs in applying multiple, advanced procedures by an order of magnitude, thus facilitating responsive and multifaceted analysis of large-scale data.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916472","Learning Dependence Structures with Bayesian Regularization","DMS","STATISTICS","09/01/2019","07/29/2019","Feng Liang","IL","University of Illinois at Urbana-Champaign","Standard Grant","Yong Zeng","08/31/2023","$180,001.00","","liangf@uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1269","079Z","$0.00","A central issue in modern statistics and machine learning is overfitting. Many successful techniques have been developed under this mathematical framework known as regularization, which penalizes or adds constraints on the underlying model parameters to avoid overfitting. This project focuses on a Bayesian framework for effective regularization. The Bayesian framework is especially appealing for many applications since it provides a conceptually easy but principled way to coherently incorporate prior assumptions/knowledge, address overfitting and quantify uncertainty. The main goal of this project is to develop fast and scalable computational tools, as well as providing theoretical guarantees, to assist with the implementation of Bayesian inference for practical problems arising in many application areas.<br/><br/>This project focuses on a Bayesian framework for learning dependence structures among a large number of variables. In particular, the PI will develop a unified framework for Bayesian regularization for conditional Gaussian graphical models, mixed graphical models, and latent graphical models. The proposed research focuses on designing a family of sparsity inducing priors, deriving efficient algorithms to explore the posterior modes and to approximate the posterior distribution, and providing theoretical guarantees such as estimation error bounds and selection consistency for the obtained estimators. The proposed activities will generate research results that will be immediately applicable to many other statistical models beyond graphical models, and computation tools that will be made available via open-source statistical software packages in R and/or Python.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1848575","CAREER: Statistical Analysis of Nonconvex Optimization in Unsupervised Learning","DMS","STATISTICS","08/15/2019","06/26/2023","Xiaodong Li","CA","University of California-Davis","Continuing Grant","Yong Zeng","07/31/2024","$407,611.00","","xdgli@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","079Z, 1045","$0.00","Unsupervised learning techniques have been widely used in real-world applications such as searching, ranking, recommender systems, social networks, online advertisements, online transportation, virtual assistants, and so on. In many practical applications of unsupervised learning methodology, nonconvex optimization based estimation methods are convenient due to their scalability to large data. This scalability is crucial in various applications such as computer vision and natural language processing. However, nonconvex optimization is computationally unstable or even infeasible in general due to unfavorable local minima that may exist. In consequence, statistical properties for global minimum based estimates may not provide meaningful guidelines for practitioners. This project aims to study the statistical properties of local minimum based estimates for nonconvex optimization methods in a range of unsupervised learning problems. The proposed research projects are significant in identifying reliable nonconvex frameworks by understanding the trade-off between computational feasibility and statistical efficiency. A new platform will be provided for collaborations across different fields such as statistics, mathematics and computer science. The activity is planned to engage female and underrepresented minority students in the study in Science, Technology, Engineering and Mathematics (STEM) fields through both theoretical and computational training and hands-on data analysis. <br/><br/><br/>Since nonconvex optimization methods are known to be adaptive to missing data and various parameterizations, the proposed projects are focused on the statistical analysis for low-rank factorization based unsupervised learning, such as matrix completion, robust PCA, pairwise ranking and network representation. Recent developments in landscape analysis for nonconvex low-rank factorization reveal that there could be no spurious local minima if (i) the ground truth satisfies strong structural assumptions; (ii) model mismatching is not permitted; (iii) the sample size is large. In contrast, the proposed project is focused on conducting the landscape analysis in more general settings: First, model-free frameworks will be proposed to study the geometric properties of nonconvex optimization without requiring structural assumptions on the data or exact model matching; Second, the effects of model mismatching on the landscape of the nonconvex objective functions will be analyzed; Third, statistical efficiencies for local minima based estimates and their relationship with the intrinsic dimensions and sample sizes will be established; Fourth, algebraic structures of general parameterized low-rank factorization will be exploited in order to establish a unified landscape analysis for a broad class of unsupervised learning problems. Moreover, in order to test the empirical behavior of the proposed methods, the activity is planned to identify appropriate benchmark datasets in learning-to-rank, predictive network analysis and recommendation systems in order to compare the empirical performances of the proposed approaches with baseline methods in the literature.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916001","Dynamic Multivariate Normative Comparison and Risk Screening for Alzheimer's Disease Progression","DMS","STATISTICS","08/15/2019","08/06/2019","Yu Cheng","PA","University of Pittsburgh","Standard Grant","Yong Zeng","07/31/2024","$179,928.00","","yucheng@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1269","","$0.00","This project focuses on the development of statistical methods for analyzing data from the Alzheimer Disease (AD) Research Center (ADRC).  The first objective is to develop robust procedures to classify cognitive impairment over repeated visits. This has important clinical implications, as current diagnostic methods tend to falsely flag healthy subjects as impaired.  The second project focuses on systematic evaluation of high-dimensional risk factors to select promising features that can separate those subjects who will develop AD, from those who might die, and those who will be alive and disease free by a certain time point.  The completion of this project will lead to the identification of important risk factors that are predictive of both AD and survival.  These newly identified biological, clinical, and genetic markers will guide future studies developing targeted intervention for AD.  The proposed methods are relevant for disease diagnosis and risk screening but may also be applied to other areas such as economics, finance and engineering.  The project will integrate research and education through the mentoring of graduate students. <br/><br/>The first project concerns longitudinal measures of multiple domain scores of cognitive functioning modeled using multivariate mixed-effect models.  A longitudinal multivariate normative comparison (MNC) statistic is then computed to measure the distance between a subject's domain scores and the estimated norm of healthy controls. Different thresholding methods are proposed for the longitudinal MNC based on the Chi-square approximation and permutation to identify cognitive impairment from retrospective data.  Two familywise-error-rate controlling procedures are developed to dynamically screen for cognitive impairment at each ongoing visit, by comparing the p-values from the longitudinal MNC with adaptive significance levels. In the second project, a recently developed diagnostic measure of the volume under the ROC surface (VUS) is adopted as a model-free screening metric for ordinal competing endpoints. The VUS can be readily estimated as a concordance probability by some weighted U-statistics. The proposed screening procedure based on the U-type estimator of the VUS provides systematic and dynamic evaluation of markers' discriminatory capacity without any model assumptions. As the first screening method developed specifically for ordinal disease progression, the successful completion of the second project will contribute to the broader field of high-dimensional risk screening.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1914639","Longitudinal Modelling and Sequential Monitoring of Image Data Streams","DMS","STATISTICS","08/01/2019","07/19/2019","Peihua Qiu","FL","University of Florida","Standard Grant","Yong Zeng","07/31/2023","$180,000.00","","pqiu@ufl.edu","1523 UNION RD RM 207","GAINESVILLE","FL","326111941","3523923516","MPS","1269","","$0.00","In imaging applications related to earth and environmental monitoring, manufacturing industries, medical studies and many others, collected image data are often in the form of data streams in the sense that new images are acquired sequentially over time. In such applications, one fundamental task is to monitor the image sequence to see whether the underlying longitudinal process of the observed images changes significantly over time. This project aims to develop novel and effective statistical methods for answering this question. Because of the wide applications of image sequence monitoring, this project will have broader impacts on society through its applications in different disciplines and areas. Open source R packages will be developed and distributed freely for convenient use by practitioners. A web portal will also be developed for individual researchers to try the proposed methods. The PI plans to integrate the research results into educational activities, including the development of new curriculum modules, the mentoring of Ph.D. students, and outreach to local high schools students for after-school activities to raise their interests in data modeling and scientific research, and contribute to the workforce development in Science, Technology, Engineering and Mathematics. <br/><br/>This project aims to develop a flexible longitudinal modeling approach and an effective sequential monitoring scheme for analyzing image data streams, and study their statistical properties. The proposed longitudinal model for describing observed images in a given time interval is flexible, and its estimation procedure has the edge-preservation property while removing noise. It can accommodate both geometric misalignments among observed images and spatio-temporal data correlation in the observed image data. The proposed image monitoring approach can account for dynamic longitudinal patterns of the observed image data streams. To this end, image pre-processing, including image denoising and image registration, will be performed properly before image monitoring. The proposed methods will consider both cases where the observation times are equally or unequally spaced.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916496","Collaborative Research: Principal Component Analysis over Tree Spaces and Its Applications to Phylogenomics","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","10/01/2019","08/26/2019","Grady Weyenberg","HI","University of Hawaii","Standard Grant","Yong Zeng","09/30/2023","$118,807.00","","gradysw@hawaii.edu","2425 CAMPUS RD SINCLAIR RM 1","HONOLULU","HI","968222247","8089567800","MPS","1269, 7454","068Z, 9150, 9251","$0.00","Phylogenomics is a relatively new field that seeks to understand evolutionary relationships between organisms at the scale of the whole genome. One of the central questions in evolutionary biology is a better understanding of the relationships between organisms, usually summarized in the form of a phylogenetic tree. The methods in common use for developing these trees tend to work best for closely related organisms, and when the sequences are relatively short; for example, the DNA sequence for a single gene applied to a collection of mammals. When comparing more distantly related organisms, or data from large portions of the genome, current techniques can break down. Since modern technology can quickly and cheaply produce genome-scale sequence data, there is a pressing need for better analytical tools tailored to this large-scale high-dimensional data. The most popular statistical methods for finding general patterns in large-scale data, such as Principal Component Analysis (PCA), make the assumption that the space where the data lies is flat, like the plane geometry of Euclid. However, the space of possible phylogenetic trees has a decidedly non-Euclidean geometry, with a surface more akin to an origami figure made with a sheet of rubber. The goal of this project is to develop alternative types of principal components, and methods to calculate them, which take into account the unusual structural features of the mathematical space of phylogenetic trees.<br/><br/><br/>PCA is a statistical method that takes data points in a high dimensional Euclidean space into a lower dimensional plane which minimizes the sum of squares between each point in the data set and their orthogonal projection onto the plane. It has been used for clustering high dimensional data points for statistical analysis and it is one of the simplest and most robust ways of doing dimensionality reduction in a Euclidean vector space.  However, it assumes the properties of a Euclidean vector space. The space of all possible phylogenies on a fixed set of species does not form a Euclidean vector space, so PCA must be reformulated in the geometry of a tree-space. Motivated by the previous work by T. Nye in 2011 on construction of the first principal component, or principal geodesic, the PIs propose two geometric objects under different metrics which represent a k-th order principal component: (1) the locus of the weighted Frechet mean of k+1 points in a tree-space, where the weights vary over the associated probability simplex, under the Billera-Holmes-Vogtman (BHV) metric and (2) the tropical convex hull of k+1 points in a tree-space via the tropical metric in tropical geometry known as the max-plus algebra. The first aim of this project is to prove properties of the PCA under the BHV metric and the PCA under the tropical metric over tree-spaces. Then, the second aim is to develop efficient algorithms to compute/approximate them. Simulation studies will be conducted to show these algorithms perform well.  Then the PIs will apply these algorithms to empirical data sets, such as Apicomplexa, a phylum of parasitic alveolates including malaria, and African coelacanth genomes, and sequences of hemagglutinin for influenza from New York. The broader impact will include advising undergraduate students for the implementation of the algorithms and user interfaces of the software products. These research experiences will complement a new Data Science program being developed as a component of the current Hawaii EPSCoR program. A portion of the summer effort will also be used to collaborate with nearby high school science and engineering programs in the development of data analysis lesson modules.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1913149","Statistical Learning Problems with Complex Stochastic Models","DMS","STATISTICS","08/15/2019","08/12/2019","Yazhen Wang","WI","University of Wisconsin-Madison","Standard Grant","Yong Zeng","07/31/2023","$150,000.00","","yzwang@stat.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","Big data is having a profound impact on scientific research and knowledge discovery. And while big data poses many statistical and computational challenges, it also presents unprecedented opportunities for statistics and data science. The investigator will focus on emerging scientific problems through the development of novel statistical and computational means, and address the challenges that arise in solving data intensive complex problems. The research in this project on finance statistics and computational algorithms is motivated by solving practical problems, and will yield cutting-edge statistical techniques and effective computational tools. The investigator actively participates in activities to integrate research with student training and applies the research outcomes to fields like finance and deep learning. <br/><br/>The investigator will conduct novel research on stochastic gradient descent algorithms and unified models for financial data. The research goals are to develop innovative statistical methodologies, computing techniques, and theories for: 1) unified stochastic models for combined inference based on both high-frequency and low-frequency financial data, and 2) statistical and computational analysis of stochastic gradient descent algorithms with applications to machine learning in particular deep learning. The investigator intends to establish theoretically-supported statistical methodologies and computational procedures, and significantly advance computational and statistical understanding to the proposed research problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916014","Random Matrices and Related Problems","DMS","STATISTICS","09/01/2019","07/30/2019","Tiefeng Jiang","MN","University of Minnesota-Twin Cities","Standard Grant","Yong Zeng","07/31/2023","$220,000.00","Yongcheng Qi","jiang040@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1269","","$0.00","Methods for statistical analysis of high dimensional data and large random matrices have receive increased attention in recent years.  This project aims to develop new methods for these problems and extend the applicability of existing methods. The research on new methodology for high dimensional data includes both tools for ultra-high dimensional data, and the theoretical study of statistical properties for methods applicable to data with high dimensionality and fixed sample sizes.  In this project, the PIs will develop new statistical methods and extend existing methods with novel applications in many fields of statistics, physics, and biology. The efficiency of these methodologies will be demonstrated via simulations and applications to real data sets. In addition, the research results will promote teaching and learning activities. The research results will be disseminated through conference presentations and publications.<br/> <br/>This project develops new methodologies for tests on the dependence structure for high dimensional data and large random matrices. The PIs will investigate a series of topics that are closely related to high dimensional data, including tests on dependence structure for high dimensional data, tests on high-dimensional mean vectors and high-dimensional covariance matrices, and nonparametric tests for complete independence for high-dimensional data.  In addition, the PIs will study the spectral radii of large random matrices such as the limiting distribution for the maximum eigenvalue from principal minors of sample covariance matrices, the largest eigenvalues of Markov matrices, and the limiting distribution for the largest absolute value for the products of truncated unitary matrices.  The research in this project will expand the scope of the application of high-dimensional statistical methods and large random matrices.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916476","Statistical Inference and Experimental Design for Recurrent Event Data with Applications in Neuroscience","DMS","STATISTICS","09/15/2019","07/29/2019","Shizhe Chen","CA","University of California-Davis","Standard Grant","Yong Zeng","08/31/2023","$180,000.00","","szdchen@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","Modern technologies allow neuroscientists to record and control neural activities at cellular resolution in living, intact brains.  While these technologies open up a path toward understanding the brain, they also pose new challenges for neuroscientists and statisticians.  Modern neuroscience experiments produce massive recordings of neural activities in the form of recurring spikes.  Despite the large volume, missing data and measurement errors pose significant statistical challenges.  Moreover, with techniques such as optogenetics and multi-photon imaging, it is increasingly clear that statistical experimental designs should be utilized to facilitate modern neuroscience experiments.  The research focuses on answering two questions of interest to the scientific community: (1) how to rigorously analyze the newly-acquired neural data, and (2) how to optimally design neuroscience experiments to aid scientific discovery.<br/><br/>There are two main aims in this research project. The first aim focuses on the development of statistical theory and methods for analyzing neural data using a class of point process models known as the Hawkes process.  While the Hawkes process is widely used in neuroscience applications, the underlying theoretical properties are not well-studied.  The proposed research will seek to (a) develop novel theory for the Hawkes process under realistic assumptions, (b) establish a causal inference framework for partially observed networks using instrumental variables, and (c) account for calibration errors in spike detection.  In the second aim, an automatic procedure for model-based design of optogenetics experiments for mapping neural microcircuits will be developed leveraging results obtained as part of the first aim.  Computationally efficient algorithms for online learning and designing on neural data will also be developed.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1914496","Towards Efficient Bias Correction in Data Snooping","DMS","STATISTICS","09/01/2019","07/29/2019","Xuming He","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yong Zeng","08/31/2023","$250,000.00","","xmhe@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","The choice of a statistical model is often a critical part of data analysis because a useful model helps researchers extract relevant information from noisy data to reach interpretable findings.  While scientific or economic theories do help formulate models in some applications, most data analysts have to rely on empirical models.  Using the same data to select a model and then to perform model-based statistical inference is commonly known as data snooping.  Unfortunately, data snooping is intrinsically risky without a careful analysis of the potential bias resulting from such practices.  The primary goal of this project is to study how to understand and correct bias from data snooping and develop sound statistical inference methods.   The research will provide valuable tools for scientists, researchers, and policy makers who rely on data-driven models for uncertainty assessment and confirmatory data analysis.<br/><br/>This project focuses on regression-adjusted inference on treatment effects and inference on the best selected subgroup. The proposed work is motivated by the pressing need for more fundamental research related to the handling of ""post-selection bias"" in statistical analysis. The repeated data-splitting method for de-biased inference on a structural parameter (for example, the average treatment effect) enables efficient bias removal in addressing an intrinsic scientific question.  The proposed inference on the best selected subgroup provides a bias-correction to a natural estimate of the subgroup effect size, and therefore reduces the risk of data-snooping and false discoveries in subgroup analysis.  In the big data era, data-driven models and subgroup analyses are often used to take advantage of anticipated sparsity in the data structure or to explore data heterogeneity. The proposed research aims to provide insights, theory, and tools for more informed decision making in such endeavors. The project will involve collaborations with researchers investigating the risk of concussion as well as scientists in the biotechnology industry who routinely rely on subgroup analysis.  Graduate and undergraduate students will be engaged in the proposed research.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1841307","The Annual Data Institute Conference","DMS","STATISTICS","03/15/2019","03/14/2019","James Wilson","CA","University of San Francisco","Standard Grant","huixia wang","02/29/2020","$15,000.00","David Uminsky","jdwilson4@usfca.edu","2130 FULTON ST","SAN FRANCISCO","CA","941171080","4154225203","MPS","1269","7556","$0.00","From March 10th - 12th, 2019, the Data Institute at the University of San Francisco (USF) will host the Data Institute Conference at the downtown campus of USF in San Francisco.  In the past decade the explosion of available data has fundamentally changed the way researchers across disciplines approach problems. Industry is experiencing a similar disruption as data driven decision-making is driving innovation toward new skills and roles that are often coined ""data science."" While academic researchers have traditionally been the center of innovation and research, industry is pouring significant resources into developing solutions to many of these same problems. This isolated and paralleled approach to finding solutions is ripe for a conference dedicated to exchanging ideas across this traditional divide. This conference aims to draw leaders of data science in industry and academia to explore the latest theoretical advances and technological applications in data science to promote the next generation of cross-disciplinary research. This conference will make an equally important impact to the greater scientific community with a focus on recruiting and increasing diversity in data science with attendees that are not traditionally represented in STEM fields. This proposal is devoted to the support of students, post-doctoral scholars, and early-career researchers, especially those from underrepresented groups. <br/><br/>Data Science is a rapidly developing field with fluid boundaries but largely contains research areas in the statistical, mathematical and computational sciences. This conference will bring together both junior and senior researchers from industry and academia to exchange ideas on the recent advances of data science in these areas. Tracks include network analysis, experimental design, high dimensional signal processing and machine learning, as well as tracks dedicated to recent applications of these statistical topics to deep learning, social good, and marketing. There is a clear need for this novel approach to a conference in data science. Most conferences in this space have attendees numbering in the thousands and are either very broad with dozens of parallel sessions or focused on a single topic in data science. This will be of moderate size (~350) with three or fewer parallel sessions at any given time with many overlapping research threads so as to maximize the opportunity to collaborate and establish new research between academics and industry practitioners. To learn more about the Data Institute Conference, please visit the website: https://www.sfdatainstitute.org/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844695","CAREER: Unbiased Estimation with Faithful Markov Chains for Scalable Statistical Inference","DMS","STATISTICS","08/15/2019","06/08/2021","Pierre Jacob","MA","Harvard University","Continuing Grant","Yong Zeng","07/31/2024","$257,826.00","","pjacob@fas.harvard.edu","1033 MASSACHUSETTS AVE 5TH FL","CAMBRIDGE","MA","021385369","6174955501","MPS","1269","1045","$0.00","Numerical integration is a common goal in all scientific fields where complex probabilistic models need to be simulated and calibrated.  In statistics, numerical integration is used in virtually all tasks, from parameter inference to model averaging and hypothesis testing.  Among state-of-the-art numerical integration techniques, most methods are randomized algorithms that operate iteratively, generating a sequence of random states, one after the other. Unfortunately, their iterative nature stands at odds with current directions in computing hardware: increasingly parallel architectures and stagnating clock rates.  This research develops new algorithms that provide accurate estimates of integrals as a number of random quantities, that can be generated independently and in parallel, goes to infinity.  The proposed techniques are employed to address long-standing challenges in statistical inference for large models and complex data.  The proposed innovations combine applied probability, computer science and statistical computing, and apply to many fields including machine learning, statistical mechanics, computational neuroscience and epidemiology, where high-dimensional integrals abound.  The project involves the development of software and features an educational program with courses and research opportunities for students, and a broader dissemination program.<br/><br/> <br/>To numerically approximate high-dimensional integrals, Markov Chain Monte Carlo methods iteratively generate sequences that explore the landscape described by the integrand. These methods yield estimators that converge to the integrals of interest in the limit of the number of iterations. However, algorithms that rely on iterative asymptotic regimes risk becoming obsolete in the era of parallel computing hardware. The proposed research develops new Monte Carlo estimators that are unbiased for the expectations of interest, while having a finite computing cost and a finite variance. They can thus be generated independently in parallel and averaged over, paving the way for scalable numerical integration on large-scale parallel computers. The proposed estimators rely on faithful couplings of Markov chains, whereby pairs of chains coalesce after a random number of iterations. This project includes theoretical investigations on the efficiency of the proposed estimators, and the design of practical coupling strategies for various applications.  The research connects with topics in numerical methods, stochastic processes and optimal transport. Beyond parallel computing, the proposed estimators are used to tackle statistical challenges such as normalizing constant estimation and modular inference for large models made of multiple components.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1844481","CAREER: Valid and Scalable Inference for High-dimensional Statistical Models","DMS","STATISTICS","03/15/2019","03/10/2023","Adel Javanmard","CA","University of Southern California","Continuing Grant","Yong Zeng","02/29/2024","$402,189.00","","ajavanma@usc.edu","3720 S FLOWER ST","LOS ANGELES","CA","900894304","2137407762","MPS","1269","079Z, 1045","$0.00","Due to the advent of ""big data"" technologies, fine-grained data sets can be collected at unprecedented scales, bringing transformative changes to modern life ranging from healthcare, and social networks, to recommendations systems and commerce. As a result, data-driven methods are becoming de rigueur nowadays, driving the need for increasingly sophisticated algorithms that find subtle statistical patterns in massive amount of data. This trend however is a double-edged sword: on the one hand, modern statistical learning methods help researchers in various fields to discover unexpected patterns from data and to make better decisions impacting everyday life. On the other hand, the rapid growth in the size and scope of data sets as well as the complexity of the methods used has made statistical models less transparent. Employing the derived models without a proper understanding of their validity can lead to many false discoveries, incorrect predictions and massive costs. For example, suppose the medical records of patients are used to develop a model for providing personalized risk score for a chronic disease. A high-risk score can trigger an intervention, such as incentive for healthy behavior, additional tests, and medical follow-ups which are all costly. This raises the concern about the validity of the outcomes retuned by this model. Should one interpret them at an average level or an individual level? Are the model predictions biased and, if so, how much? The overarching goal of this project is to develop novel foundational perspectives on the emerging inferential and computational challenges in statistics and data science. In addition, the PI plans to develop software packages to implement the proposed methods and make them publicly available. The proposed work will benefit a broad range of researchers from various areas ranging from bioinformatics and machine learning to finance and engineering. The PI will also integrate components from this project into an advanced graduate class and use selected results to motivate undergraduate students to pursue careers in STEM (Science, Technology, Engineering and Math). <br/><br/>This project aims at developing statistical methods that are 1) scalable and 2) valid in the sense that in addition point estimation, they also quantify the statistical uncertainty that is intrinsic in the estimation and predications. These issues are among the central topics in modern statistics and it is imperative to develop solid theory and powerful methodology to address them. This project focuses on three interrelated prongs that develop fundamental insights for these ubiquitous challenges: (1) Uncertainty assessment and high-dimensional inference: the PI will develop a flexible framework for general hypothesis testing problems in high-dimensional setting using the so-called debiasing approach, and further study inference for high-dimensional models with adaptively collected samples, such as time series; (2) Online hypotheses testing: the PI will formulate the decentralized false discovery rate (FDR) control where the number of hypotheses to be tested is unknown (possibly infinite) and the decision maker should take an action on each before the next p-value is received; and (3) Optimal iterative estimation for non-linear decision regions.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1913015","Conferences for New Researchers in Statistics, Probability, and Data Science","DMS","INFRASTRUCTURE PROGRAM, STATISTICS","06/15/2019","05/04/2022","Alexander Volfovsky","NC","Duke University","Standard Grant","Yulia Gel","10/31/2022","$182,372.00","","alexander.volfovsky@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1260, 1269","7556","$0.00","The Committee on New Researchers of the Institute of Mathematical Statistics will hold its 2019 conference at Colorado State University, July 24-27, 2019. Each participant will give a brief introduction to his/her research, as well as a poster presentation. Plenary talks will be given by established researchers. There will be panel discussions on teaching and mentoring, publishing, funding, and finding collaborators. The other workshops supported by this grant will include oral and poster presentations by new researchers, plenary talks by established researchers, and open discussions of future directions for statistics, probability, and data science.<br/><br/>Colorado State University will host a conference for new researchers in statistics and probability from July 24-27, 2019. The meeting is organized by the IMS Committee on New Researchers and held for junior researchers working in different areas of Statistics and Probability. The primary objective of the conference is to provide a platform for interaction among new researchers, as well as opportunities to seek mentorship from leading researchers in the field. This conference series is explicitly aimed at training the future leaders and researchers in probability and statistics. This funding will further support new researcher participation at subject-specific workshops and conferences throughout the year, including the Atlantic Causal Inference Conference (an annual meeting of causal inference researchers held in Montreal in May 2019) and the Preparing for Careers in Teaching Statistics and Data Science Workshop (an annual workshop for early career educators in statistics and probability to be held in Colorado in July 2019).<br/><br/>Details can be found at http://groups.imstat.org/newresearchers/conferences/nrc.html.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916221","Theory for General Regression with Heavy Tails and Shape Constraints: A Multiplier Empirical Process Approach","DMS","STATISTICS","07/01/2019","05/24/2021","Qiyang Han","NJ","Rutgers University New Brunswick","Continuing Grant","Yong Zeng","06/30/2023","$180,000.00","","qh85@stat.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","Modern high-dimensional complex-structured datasets are usually very noisy and heavy-tailed. The project will address two major inter-related questions arising from the challenges due to these highly noisy datasets and unstable tuning-sensitive nonparametric methods. The first question concerns the noisy nature of high-dimensional complex-structured datasets. It addresses the extent to which we can believe in the results of estimation procedures designed for regression models with very light-tailed errors, while keeping in mind that such an ideal assumption may fail miserably in practice due to numerous outliers. The second question concerns the subtle and usually unstable tuning-sensitive multidimensional nonparametric methods. It aims at understanding how far alternative tuning-free and stable statistical estimation methods with additional shape constraints can be reliable. The graduate student will be involved in developing theory and methods for multi-dimensional shape constrained models, implementing algorithms, performing numerical experiments, and validating the theoretical properties of the statistical methods.<br/><br/>The proposed research questions share a close common tie at the level of their underlying probabilistic structures exhibited by multiplier empirical processes, a special form of the empirical processes. Unfortunately, existing tools necessarily fail to provide sharp understandings due to complex structures within the processes. The PI will develop probabilistic tools and techniques in connection with these multiplier empirical processes. These tools and techniques will play a crucial role in understanding the phase-transitional behavior of the commonly used least squares and other related frequentist and Bayes methods in regression models with heavy tails and shape constraints. The particular problems under investigation include (a) behavior of penalized least squares estimators in nonparametric models with heavy-tailed errors, (b) behavior of Bayes procedures under quasi-Gaussian likelihood with heavy-tailed errors, (c) behavior of least squares estimators in multi-dimensional isotonic and convex shape constrained models and (d) behavior of maximum likelihood estimators in general additive models with shape constraints.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916222","Multivariate Analysis for Samples of Networks","DMS","STATISTICS","08/15/2019","08/06/2019","Elizaveta Levina","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yong Zeng","07/31/2023","$249,999.00","","elevina@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","Data collected in the form of networks, often with weighted edges, have become increasingly common in practice, but classical statistical tools do not apply to such data. This project will develop a statistical toolbox of network-aware methods for several common statistical analyses.  The main motivating application is neuroimaging, which allows collecting brain connectivity network data from human subjects.   These methods can be used to identify brain connectivity patterns associated with disorders, to estimate normal brain development trajectories and deviations from it for children and adolescents, to study changes associated with normal and abnormal aging in older subjects, and to find new subtypes of known disorders.   The methods will be developed and extensively tested in close collaboration with neuroscientists.<br/><br/>This project develops network analogues for common multivariate analysis tools for vector-valued data, such as estimating the mean, classification, clustering, and principal component analysis. The overarching theme is developing network-aware methods by striking the balance between collapsing the networks to a few global summary measures and treating them as a long single vector of edge weights, with the goal to obtain methods that are not only accurate, but also scientifically interpretable.   The technical challenges in developing such methods arise, broadly speaking,  from the need to impose high-level network structure, such as communities, onto low-level network features, such as individual edge weights.  Addressing these challenges will involve sophisticated tools from modern random matrix theory, structured penalties, advanced optimization techniques, and computationally efficient algorithms, to be developed for each of the new network analysis tools proposed.    The close collaboration with neuroscientists and psychiatrists will ensure that the statistical tools developed are thoroughly tested and vetted in the neuroimaging community.    Project results will be widely disseminated through publications, presentations, and software packages, in both statistical and neuroimaging venues, and are expected to raise the current standards for statistical network analysis in the field of brain connectomics.  The project will also train graduate students in an important emerging area of modern statistics and help them develop advanced computing and  interdisciplinary collaboration skills.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916226","The Argo Data and Functional Spatial Processes","DMS","STATISTICS","08/01/2019","08/09/2019","Tailen Hsing","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Yong Zeng","07/31/2022","$297,484.00","Stilian Stoev","thsing@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1269","","$0.00","The project focuses on research problems inspired and motivated by the Argo data set. The data is the product of the multi-national Argo project that has been monitoring the temperature and salinity of the open oceans (Atlantic, Indian, and Pacific) since 2007. It consists of temperature/salinity profiles -- measurements over a dense grid of pressure levels -- of the upper ocean layer from 0 through 2,000 meters below the surface. Currently, the Argo project operates around 4,000 autonomous floats, which continuously sample such type of functional data profiles over a spatial grid covering all open oceans. The resulting rich collection of function-valued data indexed by space and time has been a major resource for basic scientific research in oceanography and climate science. In this project, the co-PIs and their research team, along with collaborating oceanographers, will focus on producing state-of-the-art statistical theory and methodology along with full-fledged algorithmic implementations to help address the scientific challenges of the Argo project. The research will also have an impact on fundamental statistical theory and methodology as well as, more broadly, on modeling and analysis of complex space-time data in other scientific domains. The graduate student support will be used for research on extreme value theory. <br/><br/>The existing theory and methodology of spatial statistics has largely focused on scalar data with stationary structure. Function-valued data with non-trivial dependence structure that varies in space and time pose novel theoretical and methodological challenges. Recently, the field of functional spatial data has seen a steady development but there remains a huge gap between theory and applications. For example, the existing analysis of the Argo data in the scientific literature is still focused on treating one pressure-level at a time using conventional spatial statistics methods. The co-PIs plan to develop a comprehensive framework of function-valued random field models that is suitable for the analysis of the Argo data. This framework will provide a principled approach to the problem by treating ocean temperature and salinity as functions of a continuous range of pressure levels. Estimators for the functional mean and covariance will be developed along with their uncertainties. Important practical challenges on computing the estimators and optimal smoothing parameters through cross-validation will be addressed using novel algorithms and scalable implementations. The research will also address the fundamental functional kriging problem, i.e., the optimal prediction of function-valued data indexed by space and time. This will involve the development of a new statistical paradigm that bridges the two fields: functional data analysis and spatial statistics. A core issue is the introduction of new models that are amenable to the objective, for which a good starting point is extending the theory of intrinsically stationary models in spatial statistics to the context of functional spatial processes. This program will involve research on the structure and representation of such type of processes required to build adequate and flexible models. This will be followed by studying functional spatial processes that are locally intrinsically stationary through a generalization of the notion of tangent field. Concrete models, estimators and their applications to the Argo project will be developed, resulting in new tools and data products for the broader scientific community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1854336","FRG: Collaborative Research: Statistical Approaches to Topological Data Analysis that Address Questions in Complex Data","DMS","OFFICE OF MULTIDISCIPLINARY AC, TOPOLOGY, STATISTICS, , CDS&E-MSS","09/01/2019","06/23/2023","Brittany Fasy","MT","Montana State University","Standard Grant","Swatee Naik","08/31/2024","$404,183.00","Alessandro Rinaldo, Larry Wasserman","brittany@cs.montana.edu","216 MONTANA HALL","BOZEMAN","MT","59717","4069942381","MPS","1253, 1267, 1269, 1798, 8069","062Z, 1206, 1616, 9150, 9251","$0.00","As both real and simulated data become increasingly complex due to improved instrumentation and deeper understanding of the underlying data-generating mechanisms, improved statistical methodology is required for proper analysis. Fields such as astronomy and biology that have spatial intricate, web-like data (e.g., the large-scale structure of the Universe, fibrin networks) can benefit from methodology that exploits the web-like information.  The field of Topological Data Analysis (TDA) has great potential for the innovations needed to address these important and challenging scientific questions.  This project will extend existing TDA algorithms, statistical theory and applications, and make the advancement easily accessible by incorporating the work into the freely available R package TDA.  Moreover, the research will train undergraduate and graduate students in an interdisciplinary and collaborative environment.<br/><br/>The goals of this project are (1) to extend existing algorithms in TDA to allow statistically rigorous inferences and improved visualization, (2) to develop the statistical theory necessary to apply hypothesis testing to sets of topological descriptors, (3) to develop justifiable algorithms for parameter selection, and (4) to apply these methods to complex data, especially to critical areas in astrophysics.  These developments will make TDA more accessible to scientists and data analysts across disciplines and will give TDA a rigorous statistical foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916320","Nonparametric Confidence Sequences and their Applications","DMS","STATISTICS","08/01/2019","07/29/2019","Aaditya Ramdas","PA","Carnegie-Mellon University","Standard Grant","Yong Zeng","07/31/2022","$160,000.00","","aramdas@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","","$0.00","Large scale sequential testing and estimation is now a daily task in the tech industry, with large internet companies running hundreds or thousands of experiments (sometimes called A/B tests) per week to understand their customer base preferences, in order to improve product performance and user experience. Such experiments are inherently sequential: visitors arrive in a stream and outcomes are typically observed quickly relative to the duration of the test. These experiments are loosely planned: there are few hurdles to starting a new test, and little oversight on how they are run, unlike clinical trials which are heavily regulated with clear formal planning due to the involvement of statisticians from the start. The experiments are also continuously monitored, and adaptive choices are made of whether to stop early and make conclusions, or to collect more data. In other words, sample sizes and budgets are rarely fixed in stone in advance and there is plenty of flexibility at the hands of the data scientist who is running the experiment. Such situations are common in the sciences as well, with either telescopes collecting astronomical data sequentially (and perhaps testing for presence of black holes or estimating sizes of galaxies), or psychologists collecting human subject data sequentially (and analyzing effect sizes along the way). However, a major drawback of such flexible, loosely planned, sequential experimentation with fluid decision making, is that it is very nontrivial to provide correct inferential guarantees, either along the way or when the experiment is terminated. Traditional confidence intervals and p-values, the bread and butter of classical statistics, are designed for fixed sample sizes, and can only be used once at that predetermined time. Using the standard CIs repeatedly at different sample sizes, or after adaptively stopping, without any correction to account for the multiple intervals constructed, completely invalidates their guarantees, leading to an increase in erroneous conclusions. The graduate student support will be used for research on sequential analysis and concentration inequalities.<br/><br/>The PI proposes to revisit a classical notion called a ""confidence sequence"" by Darling and Robbins (1967), which is a (potentially infinite) sequence of confidence intervals that is, with high probability, simultaneously valid over all times. Due to the simultaneous guarantee, an analyst may keep peeking at the data and the constructed confidence sequence, adaptively choosing to stop collecting data or to collect more, and still have correct inferential guarantees through the process including when it stops. These can be converted to always-valid p-values, that are also valid at arbitrary stopping times. Using modern martingale techniques, we have recently been able to generalize prior constructions of confidence sequences to several novel nonparametric settings, yielding both the tightest known closed-form CS expressions as well as the sharpest numerical methods in practice. This project seeks to extend the scope of the above advances both theoretically and practically. A few examples of extensions that the PI wishes to pursue include designing new confidence sequences for vector-valued mean vectors, and fully empirical bounds that do not depend on unknown parameters. We will also explore applications of these bounds to sequential testing and estimation tasks.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915932","Deep Learning and Random Forests for High-Dimensional Regression","DMS","STATISTICS","08/15/2019","06/24/2020","Jason Klusowski","NJ","Rutgers University New Brunswick","Continuing Grant","Pena Edsel","11/30/2020","$105,385.00","","jason.klusowski@princeton.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","075Z, 079Z","$0.00","This project aims to investigate two of the most widely used and state-of-the-art methods for high-dimensional regression: deep neural networks and random forests. Despite their widespread implementation, pinning down their theoretical properties has eluded researchers until recently. The proposed research aims to add to the growing body of literature on their analysis, by both developing tools of theoretical value and providing guarantees and guidance for practitioners and applied scientists who use these popular methods frequently in their work.<br/><br/>The success of multi-layer networks has largely been buoyed by their ability to generalize well despite being able to fit most datasets, given enough parameters. This phenomenon is particularly striking when the input dimension is far greater than the available sample size, as is the case with many modern applications in molecular biology, medical imaging, and astrophysics, to name a few. A major component of the proposed work will be to obtain complexity bounds for classes of deep neural networks with controls on the size of their weights, which can then be used to bound generalization error and statistical risk. These complexity bounds reveal the role of complexity penalization, which is based on certain norms of the weights of the network. Motivated by these observations, another stream of the proposed research seeks to provide statistical guarantees of certain complexity penalized estimators and their adaptive properties. Current theoretical results for random forests are either for stylized versions of those that are used in practice or are asymptotic in nature and it is therefore difficult to determine the quality of convergence as a function of the parameters of the random forest. Furthermore, the setting for the analysis of more practical implementations of random forests is limited to structured, fixed-dimensional regression function classes. Given these restrictions, the first component of the proposal aims to investigate how random forests behave in the high-dimensional regime when the number of predictors grows with the sample size. Another research objective is to isolate and study families of flexible high-dimensional regression functions for which finite sample convergence rates can be established. The final endeavor of this project is to connect popular measures of variable importance  to the bias of random forests. Since variable importance measures are used for assessing the role each predictor variable plays in influencing the output, this connection will partially explain why random forests are adaptive to sparsity. The relationship will also help to theoretically motivate variable importance measures as useful tools for model interpretability.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916002","Privacy-Preserving Bayesian Inference: Foundations and Extensions","DMS","STATISTICS","09/01/2019","07/30/2019","Ruobin Gong","NJ","Rutgers University New Brunswick","Standard Grant","Pena Edsel","08/31/2022","$100,000.00","","ruobin.gong@rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1269","","$0.00","This project provides a theoretical foundation and computational methodologies to conduct Bayesian statistical inference using differentially private data. Differential privacy is a mathematical framework that allows the release of potentially sensitive data in such a way that protects the confidentiality of individual records without unduly sacrificing its overall usefulness for statistical analysis. As our society today grapples with the privacy implications that accompany the exploding growth of large-scale datasets, the invention of differential privacy provides a solution to protect personal demographic and biological information without deterring the accumulation of public knowledge. The U.S. Census Bureau has officially adopted differential privacy as the disclosure avoidance method for the 2020 Census. Other data collectors and curators are expected to follow suit in the near future. This project answers the pressing need for new statistical theory and methods to appropriately understand and efficiently analyze differentially private data. The project will expand the repertoire of tools available to researchers, and contribute to the cause of creating a better informed and more transparent society while respecting individual privacy.<br/> <br/>   <br/>The PI will work on a theoretical formulation of the definitions of differential privacy using imprecise probability constructions, including interval of measures and coherent upper-lower probability measures. In the Bayesian context, such a formulation delivers a robust-likelihood conception of the model and allows for the computation of bounds on posterior quantities based on differentially private data for arbitrary prior specifications. The PI also proposes the differentially private approximate Bayesian computation (ABC) algorithm, a noisy ABC algorithm that delivers exact posterior inference given differentially private observations subject to arbitrary additive noise. The algorithm permits differentially private inference from large-scale Bayesian models with intractable likelihoods. The project bridges the classic theories of robust Bayes and generalized Bayes, with the novel literature on statistical privacy, and derives practical implementations based on privacy-preserving data releases. The project will supply analysts and researchers in a timely fashion with inferential methodologies tailored for differentially private input that are both theoretically sound and computationally efficient.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916246","Collaborative Research: Stochastic Models for Gene-based Association Analysis of Longitudinal Phenotypes with Sequence Data","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","08/01/2019","07/30/2019","Zuoheng Wang","CT","Yale University","Standard Grant","Yong Zeng","07/31/2022","$120,000.00","","zuoheng.wang@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1269, 7454","068Z","$0.00","Longitudinal genetic studies provide a valuable resource for exploring key genetic and environmental factors that affect complex traits over time. Genetic analysis of longitudinal data that incorporates trait variation over time is critical to understanding genetic influence and biological variations of complex diseases.  In recent years, many genetic studies have been conducted in cohorts in which multiple measures on a trait of interest are collected on each subject over a period of time in addition to genome sequence data.  These studies not only provide a more accurate assessment of disease condition but enable researchers to investigate the influence of genes on the trajectory of a trait and disease progression.   This project focuses on the development of novel association testing methods to analyze sequencing genomic data at gene levels.  The research will help provide insights into the underlying biology and progression of complex diseases.<br/><br/>In longitudinal genetic studies and data from the Electronic Medical Records and Genomics (eMERGE) network, phenotypic traits and genetic variants may be viewed as functional data.  Functional data analysis (FDA) can serve as a valuable tool for exploring key genetic and environmental factors that affect complex traits over time.  In the presence of a large number of rare variants, gene-based analysis is a more powerful tool for gene mapping than testing of individual genetic variants.  This project seeks to develop stochastic functional regression models and longitudinal sequence kernel association tests (LSKAT) to analyze longitudinal traits of population samples and pedigree or cryptically related samples, and to analyze pleiotropic traits.  FDA techniques and kernel-based approaches are utilized to reduce the high dimensionality of sequencing data and draw useful information. A variance-covariance structure is constructed to model the measurement variation and correlations of an individual's trait based on the theory of stochastic processes and novel penalized spline models are used to estimate the trajectory mean function.  The proposed methods and software will be tested and refined using real data sets and simulation studies.  User-friendly software will be developed to implement the proposed methods and will be made publicly available.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1854220","FRG: Collaborative Research: Statistical Approaches to Topological Data Analysis that Address Questions in Complex Data","DMS","OFFICE OF MULTIDISCIPLINARY AC, TOPOLOGY, STATISTICS, , CDS&E-MSS","09/01/2019","06/26/2019","Jessica Kehe","CT","Yale University","Standard Grant","Swatee Naik","08/31/2020","$368,706.00","","jjkehe@wisc.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1253, 1267, 1269, 1798, 8069","062Z, 1206, 1616","$0.00","As both real and simulated data become increasingly complex due to improved instrumentation and deeper understanding of the underlying data-generating mechanisms, improved statistical methodology is required for proper analysis. Fields such as astronomy and biology that have spatial intricate, web-like data (e.g., the large-scale structure of the Universe, fibrin networks) can benefit from methodology that exploits the web-like information.  The field of Topological Data Analysis (TDA) has great potential for the innovations needed to address these important and challenging scientific questions.  This project will extend existing TDA algorithms, statistical theory and applications, and make the advancement easily accessible by incorporating the work into the freely available R package TDA.  Moreover, the research will train undergraduate and graduate students in an interdisciplinary and collaborative environment.<br/><br/>The goals of this project are (1) to extend existing algorithms in TDA to allow statistically rigorous inferences and improved visualization, (2) to develop the statistical theory necessary to apply hypothesis testing to sets of topological descriptors, (3) to develop justifiable algorithms for parameter selection, and (4) to apply these methods to complex data, especially to critical areas in astrophysics.  These developments will make TDA more accessible to scientists and data analysts across disciplines and will give TDA a rigorous statistical foundation.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916378","Scalable Statistical Inference in Small-World Networks","DMS","STATISTICS","08/15/2019","08/09/2019","Sebastien Roch","WI","University of Wisconsin-Madison","Standard Grant","Yong Zeng","07/31/2023","$299,999.00","Karl Rohe","roch@math.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1269","","$0.00","Social networks often exhibit ""small-world features"".  For instance, friends typically share many common friends, but most individuals have a limited number of close acquaintances irrespective of the size of the network.  Another well-documented feature of such networks is the ""six degrees of separation property"", whereby most people are a small number of social connections away from one another. Despite their ubiquity, common statistical models of complex networks do not typically generate graphs with these properties. Therefore, the main goal of this project is to address the general lack of plausible and tractable statistical models of small-world networks. Specifically, the PIs will develop a novel framework for the inference of these networks, including statistical models, fast and scalable algorithms, as well as supporting theory. These models and methods will be empirically validated through the development and deployment of techniques that sample large graphs in ways that helps assess them. This new understanding will contribute to ongoing interdisciplinary collaborations in journalism, health care, and law. <br/><br/>Existing probabilistic constructions of small-world networks, i.e., random graphs exhibiting low diameter, sparsity and transitivity, tend to be ad-hoc and, hence, often not suitable for statistical inference. In this project, the PIs will formulate and analyze interpretable statistical models of small-world networks; and develop scalable statistical inference for such models based on both spectral techniques and local sampling. For this purpose, the PIs will develop and explore a family of network models with high-dimensional latent features. The PIs will analyze how traditional algorithms perform in this regime, and will develop and analyze local sampling algorithms, such as respondent-driven sampling. The information-theoretic limit of community detection will also be studied. This grant will support the development of two courses aimed at intermediate undergraduates in UW- Madison's new undergraduate data science degree. These courses will aim to broaden engagement in both data science and social network analysis. This grant will also support the training of PhD students in both Statistics and Mathematics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1847415","CAREER: A Statistical Inferential Framework for Online Learning Algorithms","DMS","STATISTICS","06/01/2019","06/09/2023","Weijie Su","PA","University of Pennsylvania","Continuing Grant","Yong Zeng","05/31/2024","$400,000.00","","suw@wharton.upenn.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","1269","079Z, 1045","$0.00","The ever-evolving and improving data acquisition techniques in science and engineering allow practitioners to measure thousands to millions of observation units in a cheap and online fashion. Examples include recommender systems, neural recordings, and influenza prediction, where decision-making is constantly updated as more data arrives and thus the problems are best represented with models capturing the online nature of data. With collective efforts from statistics, computer science, and optimization, a large toolbox has been developed to efficiently make an update to the model based on one data point at a time, whereas much less is explored about how to quantify uncertainty of the predictions in these online learning problems. For example, to what extent can we trust the predictions of an online learning algorithm, and how different would the predictions be by updating the model with more data? The investigator will develop a new framework involving theory, algorithms, and software to quantify uncertainty for a large class of online learning algorithms. The methods developed through this research project will be applied to large-scale classification problems in online learning, with the goal of enhancing interpretability of the predictions.  All methods will be implemented in software that will be broadly disseminated to practitioners who work on online learning tasks. The investigator will develop new courses at both undergraduate and graduate levels based on the research output to increase interaction across statistics, computer science, and optimization, and will mentor students with interests in these fields.<br/><br/>More specifically, this project will focus on the uncertainty arising from training on large-scale datasets with stochastic gradient descent and many of its variants, a class of immensely popular online learning algorithms that sequentially update the model parameters using computationally cheap but noisy gradients. The algorithmic randomness of stochastic gradient descent is potentially non-negligible and could even jeopardize the interpretation of predictions at worst. Taking a fully inferential viewpoint, the proposed research has a detailed research agenda that aims to obtain an in-depth understanding of uncertainty quantification for stochastic gradient descent through three fundamental topics: (1) constructing confidence intervals for online learning with convex objectives, (2) quantifying uncertainty for deep neural networks, and (3) accelerating stochastic optimization in the online setting. Taken together, the proposed research projects will build a firm foundation for integrating statistical inferential ideas into stochastic optimization using streaming data, with research output feeding back into the development of practical methodology for analyzing online datasets. The completion of this work will bring in various perspectives from statistics, optimization, and machine learning, leading to a comprehensive understanding of inferential properties of online learning algorithms and improved trustworthiness of the application of stochastic gradient descent in a wide range of scientific and engineering problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916037","Collaborative Research: Principal Component Analysis over Tree Spaces and Its Applications to Phylogenomics","DMS","STATISTICS, MSPA-INTERDISCIPLINARY","10/01/2019","08/26/2019","Ruriko Yoshida","CA","Naval Postgraduate School","Interagency Agreement","Yong Zeng","09/30/2022","$129,915.00","","ryoshida@nps.edu","1 UNIVERSITY CIR","MONTEREY","CA","939435098","8316562271","MPS","1269, 7454","068Z","$0.00",""
"1916233","Collaborative Research: Innovations for Bayesian Tree Ensemble Methodology","DMS","STATISTICS","07/15/2019","07/10/2019","Robert McCulloch","AZ","Arizona State University","Standard Grant","Yong Zeng","06/30/2022","$159,997.00","","remccul1@asu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1269","079Z","$0.00","An essential goal of modern statistical analyses across many disciplines is to gain insight into the behavior of real-world processes both to identify important correlates of variation and to obtain improved predictions.  For example in marketing, the statistician may be interested in learning the purchasing behavior of consumers from an analysis of a database of consumer transactions that includes various consumer descriptors (e.g. age, income level, geographic location) as well as  purchase amounts. The statistician would then typically attempt to build a mathematical model that characterizes the relationship between the consumer descriptors and the expenditure amount.  In doing so, however, certain issues bear strongly on the model's value and effectiveness.  First, the validity of a model may strongly depend on prior assumptions about the nature of the modeled process, information that can be difficult to ascertain. For instance, a consumer behavior model which builds in a simple assumption that consumers with higher income levels are always expected to purchase more, may be inadvertently ignoring subtleties that violate this assumption when other factors are simultaneously taken into account.  Second, sometimes even a valid and effective model may be such a complicated object that the extraction of meaningful information can itself be very challenging.  For example, after establishing particular set of predictors as important drivers of consumer purchasing power, it will still be of key interest how to best measure their relative importance in the model.  Focusing on the powerful and flexible approach of Bayesian regression tree ensemble modeling, the main thrust of this project will be to innovate this methodology to address these and other modeling avenues.  This new methodology will enable practitioners to address their research questions in an assumption-lean framework that allows the ensemble models to make use of their data to adaptively and flexibly incorporate contextual modeling assumptions. To greatly enhance interpretability, it will also provide automatic, information based summaries of variable importance to help the practitioner understand and interpret the available descriptor information.  In addition to these and further methodological contributions, the project will develop software for the implementation of this methodology as a freely available R package, enabling practitioners to more easily leverage our developments in their practical work. This is where the graduate student supported by this award will help. <br/><br/><br/>The research will focus on three general innovations to Bayesian ensemble modeling to further enhance its ability to extract meaning from complex data within an assumption lean framework.    The first contribution will develop theoretically valid measures of variable importance.  These measures will provide computationally efficient calculation of indices which meaningfully gauge the relative importance of predictor variables, both marginally and in terms of interactions.  The second contribution will provide an approach to monotone shape constrained inference which does not require any prior assumption of monotonicity.  This multidimensional nonparametric regression approach will enable the discovery and estimation of any and all the monotone components of the regression function, and to do so with no constraint assumptions whatsoever.  The third contribution will vastly extend the applicability of Bayesian ensemble modeling by developing a generalization of BART for arbitrary response data distributions, such as dichotomous responses and count data.  This major technical innovation will be based on a conjugacy-free formulation that will extend the reach of BART to many new application areas and problem types than were previously possible.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915884","Asymptotic Equivalence of Quantum Statistical Models","DMS","STATISTICS","07/01/2019","06/19/2019","Michael Nussbaum","NY","Cornell University","Standard Grant","Yong Zeng","06/30/2024","$300,000.00","","nussbaum@math.cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","7203","$0.00","A fundamental insight of quantum mechanics is that randomness is a inherent feature of the physical world at the microscopic level. Any observation made on a quantum system such as an atom or a light pulse, results  in a nondeterministic, stochastic outcome. The study of the direct map from the system?s state or preparation to the probability distribution of the measurement outcomes has been one of the core topics in traditional quantum theory. In many quantum protocols, the experimenter has incomplete knowledge and control of the system and its environment, or is interested in estimating an external field parameter which affects the system dynamics. In this case, one deals with a statistical inverse problem of inferring unknown state parameters from the measurement data obtained by probing a large number of individual quantum systems. The theory and practice arising from tackling such questions is shaping up into the field of quantum statistics, which lies at the intersection of quantum theory and statistical inference. The current project aims at  a better understanding  of statistical inference for infinite dimensional quantum systems,  an area which can be seen as a quantum counterpart of nonparametric statistics. The ultimate goal is to develop a    theory of comparison and convergence of quantum statistical models, thereby enabling  efficient estimation techniques and establishing solid statistical methodology for computing reliable error bars. This project is well aligned with NSF's Quantum Leap Big Idea. The graduate student supported by this award will carry out research on quantum statistics, under the supervision of the  PI.<br/><br/>An area of particular interest is local asymptotic equivalence, with explicitly constructed quantum channels, of different quantum statistical models of pure states with a parameter in  Hilbert space.  Some elements of such a theory have already been put forward by a recent result of the proposer and collaborators, approximating a model of a large number of independent quantum systems by a Gaussian model of coherent states (shifted vacuum states). The goal will be to further develop these notions in cases of entanglement, focusing at first on quantum analogs of Gaussian stationary sequences. Among these, two cases are singled out: a stationary model of zero mean pure Gaussian states commonly used to model the squeezed vacuum in quantum optics, and gauge invariant stationary Gaussian states which appear to exhibit classical limiting behavior.   In a related topic, results on sharp nonparametric risk asymptotics like the Pinsker bound, proved initially in classical white noise models, have motivated the development of equivalence theory. It is of interest to establish minimax risk bounds of this type for estimating pure quantum states, and their adaptive attainability. In the problem of symmetric quantum hypothesis testing, or discrimination between two quantum states, the proposer and collaborators solved the longstanding problem of the quantum Chernoff lower bound pertaining to the exponential rate of decay of the error probability. In that connection, several new problems appear, such as the optimal error exponent for discriminating composite hypotheses, and attainability of the  bound  by realizable receivers in quantum optics.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916245","Collaborative Research: Innovations for Bayesian Tree Ensemble Methodology","DMS","STATISTICS","07/15/2019","07/10/2019","Edward George","PA","University of Pennsylvania","Standard Grant","Yong Zeng","06/30/2024","$149,999.00","","edgeorge@wharton.upenn.edu","3451 WALNUT ST STE 440A","PHILADELPHIA","PA","191046205","2158987293","MPS","1269","079Z","$0.00","An essential goal of modern statistical analyses across many disciplines is to gain insight into the behavior of real-world processes both to identify important correlates of variation and to obtain improved predictions.  For example in marketing, the statistician may be interested in learning the purchasing behavior of consumers from an analysis of a database of consumer transactions that includes various consumer descriptors (e.g. age, income level, geographic location) as well as  purchase amounts. The statistician would then typically attempt to build a mathematical model that characterizes the relationship between the consumer descriptors and the expenditure amount.  In doing so, however, certain issues bear strongly on the model's value and effectiveness.  First, the validity of a model may strongly depend on prior assumptions about the nature of the modeled process, information that can be difficult to ascertain. For instance, a consumer behavior model which builds in a simple assumption that consumers with higher income levels are always expected to purchase more, may be inadvertently ignoring subtleties that violate this assumption when other factors are simultaneously taken into account.  Second, sometimes even a valid and effective model may be such a complicated object that the extraction of meaningful information can itself be very challenging.  For example, after establishing particular set of predictors as important drivers of consumer purchasing power, it will still be of key interest how to best measure their relative importance in the model.  Focusing on the powerful and flexible approach of Bayesian regression tree ensemble modeling, the main thrust of this project will be to innovate this methodology to address these and other modeling avenues.  This new methodology will enable practitioners to address their research questions in an assumption-lean framework that allows the ensemble models to make use of their data to adaptively and flexibly incorporate contextual modeling assumptions. To greatly enhance interpretability, it will also provide automatic, information based summaries of variable importance to help the practitioner understand and interpret the available descriptor information.  In addition to these and further methodological contributions, the project will develop software for the implementation of this methodology as a freely available R package, enabling practitioners to more easily leverage our developments in their practical work. This is where the graduate student supported by this award will help. <br/><br/><br/>The research will focus on three general innovations to Bayesian ensemble modeling to further enhance its ability to extract meaning from complex data within an assumption lean framework.    The first contribution will develop theoretically valid measures of variable importance.  These measures will provide computationally efficient calculation of indices which meaningfully gauge the relative importance of predictor variables, both marginally and in terms of interactions.  The second contribution will provide an approach to monotone shape constrained inference which does not require any prior assumption of monotonicity.  This multidimensional nonparametric regression approach will enable the discovery and estimation of any and all the monotone components of the regression function, and to do so with no constraint assumptions whatsoever.  The third contribution will vastly extend the applicability of Bayesian ensemble modeling by developing a generalization of BART for arbitrary response data distributions, such as dichotomous responses and count data.  This major technical innovation will be based on a conjugacy-free formulation that will extend the reach of BART to many new application areas and problem types than were previously possible.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916239","Offline and Online Change-point Analysis for Large-scale Time Series Data","DMS","STATISTICS","09/01/2019","08/18/2021","Jun Li","OH","Kent State University","Continuing Grant","Yong Zeng","08/31/2023","$100,000.00","","jli49@kent.edu","1500 HORNING RD","KENT","OH","442420001","3306722070","MPS","1269","","$0.00","Offline or online time series data often involve change points due to the dynamic behavior of the monitored systems. Identifying change points from offline time series data makes parameter estimation and statistical inference efficient by pooling homogeneous observations. Detection of change points from online time series data provides timely snapshots of the monitored system and allows for real-time anomaly detection.  Despite its importance, methods available for detecting change points in large-scale offline and online time series data are scarce. This is because a large number of parameters cannot be estimated accurately with a limited number of observations, and parametric models do not fully capture multifarious aspects of data dependence. This project will develop new non-parametric change-point detection methods that incorporate both spatial and temporal dependence without imposing restrictive structural assumptions on large-scale time series data. The proposed methods will span a wide range of topics in applications, including identifying significant genes associated with certain diseases, studying dynamic functional connectivity in resting-state functional magnetic resonance imaging data, and detecting abrupt events such as dissociation of communities, or formation of new communities from social networking platforms. This project will integrate research and education by involving students at different levels, including those from underrepresented groups, and by training the pre-college and high school teachers to improve their knowledge in statistics through new developed courses. The developed methods will be disseminated to biomedical and social scientists through interdisciplinary collaborations and the analysis of first-hand datasets. <br/><br/>This project will develop a general factor model framework for spatial and temporal dependence of large-scale time series data. By integrating the framework, this project will provide hypothesis testing and offline change-point estimation of specific parameters, including the population mean and covariance matrix. The proposed methods can be readily modified to incorporate the advantages of both sum-of-squares-norm and max-norm statistics for hypothesis testing. They can be extended from regular binary segmentation methods to other popular change-point estimation methods, such as circular binary segmentation and wild binary segmentation. This project will also provide new stopping rules for online change-point detection of large-scale time series data. An explicit expression for the average run length (ARL) will be derived, so that the level of threshold in stopping rules can be easily obtained with no need to run time-consuming Monte Carlo simulations. The proposed research will derive an upper bound for the expected detection delay (EDD), the expression of which clearly demonstrates the impact of data dimensionality and dependence. This project will extend the current knowledge about change-point detection. For offline change-point detection, the PI will study the possibility of estimating the change point near the boundary in high dimensional settings. For online change-point detection, a comparison will be made between the stopping rule based on the sum-of-squares-norm statistic and the one based on the max-norm statistic, through the derived ARLs and EDDs.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1914465","Collaborative Research: Subject-level Prediction and Application","DMS","STATISTICS","08/01/2019","07/30/2019","Jiming Jiang","CA","University of California-Davis","Standard Grant","Yong Zeng","07/31/2023","$100,000.00","","jiang@wald.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","Many practical problems are related to prediction, where the main interest is at the subject (for example personalized or precision medicine) or (small) sub-population (for example small community) level.  In recent years, new and challenging problems have emerged from diverse fields such as business, social sciences, and health sciences.  Examples may involve prediction of a health outcome for a new patient or perhaps prediction of a new school's response to efforts to educate children about smoking prevention.  The investigators have shown in previous work called classified mixed model prediction (CMMP) that in such cases, it is possible to make substantial gains in prediction accuracy by identifying a class that a new subject belongs to.  However, the scenarios under which CMMP currently operates are somewhat constrained and many real-life situations fall outside its scope.  Given the tremendous gains in accuracy that are possible, it would be very valuable to develop further methodology and computational advances to deepen knowledge in this area.    <br/><br/>This project aims to make methodological advances of the classified mixed model prediction method into other types of subject-level prediction problems as well as to develop new inferential methods along the CMMP idea, by making the latter truly useful in practical situations.  The basic idea of CMMP is to create a ""match"" between a group or cluster in the population for which one wishes to make prediction and a (massive) training dataset, with known groups or clusters. Once such a match is built, the traditional mixed model prediction method can be utilized to make accurate predictions. The practical challenges that will be solved in this project include i) how to deal with training data with unknown grouping; ii) how to deal with sparse, high dimensional covariates; iii) how to make better use of covariate information to improve accuracy of CMMP; and iv) how to provide accurate measures of uncertainty for CMMP-type predictions. Two important areas of application will be investigated.  One is in precision medicine and health disparities focusing on the prediction of epigenetic markers using high dimensional genotype profiles. The other comes from the area of family economics using a large survey of data from China where predictions at finer levels of resolution (e.g., households) are of primary interest. Both applications will leverage important collaborations with practitioners and thus increase the impact of the work.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915786","Bootstrap Methods in Modern Settings: Inference and Computation","DMS","STATISTICS","07/01/2019","07/01/2021","Miles Lopes","CA","University of California-Davis","Continuing Grant","Yong Zeng","06/30/2023","$220,000.00","","melopes@ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1269","","$0.00","Bootstrap methods play a central role in statistical methodology, and are among the most widely used tools for uncertainty quantification. Although bootstrap methods have an extensive literature, their scope of applicability is far from being fully understood in modern statistical problems. This is especially the case when data are described by high-dimensional models with many unknown parameters, or when then the amount of data exceeds computational constraints. Motivated by these challenges, the proposed research has two main objectives, which are to (1) analyze the theoretical validity of bootstrap methods in high-dimensional models, and (2) develop novel ways of using bootstrap methods to enhance large-scale computations. The graduate student will focus on the analysis of bootstrap methods for high-dimensional and large-scale data. <br/><br/>With regard to the first objective, the research will focus on overcoming certain limitations of the non-parametric bootstrap, particularly in the context of high-dimensional principal components analysis. This will be pursued through a generalization of the parametric bootstrap that is tailored to the statistics of interest. Examples of such statistics include those arising from the eigenvalues of sample covariance matrices. For the second objective, the research will explore bootstrap methods as a systematic way to estimate the errors of randomized algorithms. Given that bootstrap methods have been historically labeled as computationally intensive, this application is based on a relatively distinct perspective, since it seeks to use bootstrap methods to assist computation. In particular, bootstrap methods will be developed to obtain numerical error estimates that offer a practical alternative to worst-case error analysis.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1933743","2019 NISS Writing Workshop for Junior Researchers","DMS","STATISTICS","08/01/2019","07/26/2019","James Rosenberger","NC","National Institute of Statistical Sciences","Standard Grant","Gabor Szekely","07/31/2020","$9,991.00","Lingzhou Xue","JRosenberger@niss.org","19 TW ALEXANDER DR","RESEARCH TRIANGLE PARK","NC","277090152","9196859300","MPS","1269","7556","$0.00","The 2019 NISS Writing Workshop for Junior Researchers will be held at the Joint Statistical Meetings (JSM) on July 28 and 30, 2019, in Denver, CO.  The participants are 25 junior researchers in statistics, biostatistics, and data science, with less than 6 years since their PhD and preference to those with less than 3 years.  The writing workshop provides individual hands-on guidance and mentoring on how to write journal articles and grant proposals.  Junior researchers are paired with senior editors as mentors.  Previous workshops have been very successful with many workshop participants going on to become associate editors or co-editors of major statistical journals.  Individuals from among the 109 graduates from 2007 to 2011, the first five years of this workshop, have held at least 23 associate editorships and one co-editorship.<br/><br/>Writing well is a critical but often short-changed component of the education of statisticians and data scientists. The inability to write well can hinder not only publication of a researcher's results but also success with grant proposals.  Over the past several years, NISS has organized technical writing workshops for junior researchers in statistics, biostatistics and data science. The 2019 NISS writing workshop consists of two parts. The Sunday tutorial is an all-day session that covers scientific writing, as well as how to organize a paper. The workshop will cover ethical issues, writing grant proposals, strategies for journal choice, effective use of graphics, and understanding and responding to reviewers' comments.  At the end of this session, participants will meet with their mentor who analyzed a paper the participant submitted prior to JSM.  The Tuesday session focuses on specific issues for improving a manuscript.  The website for the workshop is: https://www.niss.org/events/2019-niss-writing-workshop-junior-researchers-jsm<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916371","Prior Calibration and Algorithmic Guarantees under Parameter Restrictions","DMS","STATISTICS","08/15/2019","08/15/2019","Debdeep Pati","TX","Texas A&M University","Standard Grant","Pena Edsel","07/31/2022","$107,000.00","Anirban Bhattacharya","debdeep@stat.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1269","","$0.00","Statistical learning of many real systems can be significantly enhanced by harnessing and translating domain knowledge into meaningful parameter restrictions. With the advent of high throughput datasets, such restrictions are often present on high-dimensional parameter spaces thereby complicating inference. This research aims to develop novel statistical methods and computational algorithms for such problems drawing motivation from a number of real applications. Working within a nonparametric Bayes framework, the first part of the research project lays emphasis on the importance of calibrating prior distributions in these constrained problems and theoretically quantifying the impact of the constraints on parameter learning. The second part aims to develop efficient Markov Chain Monte Carlo and variational algorithms and analyze their convergence behaviors for the said problems. The PIs will also propose undergraduate courses that will focus on the modeling and applied components of Bayesian methods. When teaching the courses, the PIs will use daily life as well as scientific examples across different disciplines to inspire students' learning. The Activity-Based Learning (ABL) courses aim to enrich students' academic experience and learning outcomes by connecting theory with practice and concepts with methods, using data & insights obtained through engagement with the larger world.<br/><br/><br/>The research project is motivated by statistical and computing challenges posed by a number of real scientific applications where various complex restrictions are posed on key parameters, necessitating novel statistical methods and associated computational algorithms. Operating in a Bayesian paradigm which enables incorporation of various constraints in a principled framework and provides readily available uncertainty estimates often sought after in scientific applications, a major emphasis will be laid on calibration of prior distributions under these constrained spaces. Examples will be provided where seemingly innocuous prior choices routinely used in practice can lead to biased inferences in certain specific situations. A rigorous theoretical understanding of such phenomenon will be provided along with development of alternative default priors on these constrained spaces. The methodological and theoretical developments will be accompanied by efficient computational algorithms using novel approximation techniques in the context of Markov chain Monte Carlo and variational algorithms that meet the scalability demanded by the specific applications and beyond. The algorithm development will be paralleled by novel convergence analysis, bridging ideas between the optimization and sampling literature.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916071","Stable and Robust Graph Embedding, and Related Problems","DMS","STATISTICS","08/15/2019","08/09/2019","Ery Arias-Castro","CA","University of California-San Diego","Standard Grant","Yong Zeng","07/31/2022","$140,000.00","","eariasca@math.ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1269","","$0.00","The project addresses important problems having to do with inferring the position of one or several objects in space.  Such problems are relevant to a wide range of important applications, such as in robotics (e.g., autonomous vehicles, self-driving cars), geospatial information systems or GIS (e.g., sensor network localization), and more.  As is often the case, the literature on these topics is overwhelmingly application-specific.  Contributing theory and more principled methodology can help build a foundation that enables the research community to move forward with a common, more rigorous (mathematical) language and a deeper understanding of the problem itself and its ramifications.  A particular focus will be on understanding how robust state-of-the-art methods are to gross errors in the input (ubiquitous in applications) and, for some tasks, on developing new methods that are more robust than existing ones.<br/><br/>The problem of graph embedding is a fundamental problem at the crossroads of a number of research areas including multivariate analysis and visualization in statistics, machine learning, metric geometry, robotics, and more.  The literature on the topic is vast, and yet lacks crucial elements of theory, in particular in regards to the stability to noise and robustness to outliers.  Although some robust methods are available, there is potential for improvement by leveraging existing research on the problem of robust matrix completion.  At an even more fundamental level, there is the problem of characterizing which graphs are uniquely embeddable.  This is studied in the rigidity theory literature.  The problem is largely solved and reduces to the existence of a certain matrix, called stress matrix, that includes the coordinate vectors defined by an embedding of the graph (when one exists) in its null space, and has maximum rank with that property.  However, this condition is qualitative, rather than quantitative, in that the rank is numerically unstable.  This calls for the development of a quantitative theory of rigidity, as well as new methodology with performance guarantees.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916174","Collaborative Research: A Symphony of Smoothing and Change Point Analysis","DMS","STATISTICS","08/15/2019","08/07/2019","Pang Du","VA","Virginia Polytechnic Institute and State University","Standard Grant","Yong Zeng","07/31/2022","$160,002.00","","pangdu@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1269","","$0.00","A sudden change in a real life process is often of particular interest since the change may signal an anomaly, the onset of a disease, or an out-of-control process. What has been ignored in the literature is the co-existence of a slowly-changing component that also needs to be properly modeled to make valid statistical inference. In traditional statistics, inferring a sudden change and modeling a slowly-changing trend are often perceived as two seemingly conflicting concepts since they emphasize respectively on continuity and discontinuity of the true underlying process. Motivated by real-life applications, this project aims to develop three sets of novel statistical methods where simultaneous modeling of the sudden change and the slowly-changing trend are properly introduced to accommodate the practical needs. The statistical analysis tools developed in this project will benefit practitioners in the fields of cybersecurity, environmental sciences, financial managements, and medical studies. The results from the project will be integrated into teaching and student training, and be disseminated through publications and the distribution of R packages.<br/><br/>This project aims to the development of statistical methods that are natural fusions of change point analysis and nonparametric smoothing. The proposed method will combine recent developments in functional data analysis and nonparametric smoothing inference, such as functional time series and functional Bahadur representation, and change point analysis tools to solve the challenging problems. The first aim of the project focuses on change point detection in variance functions along with a smoothly-changing mean trend. A novel subsampling approach will be designed to incorporate the large sample size involved. The other two aims of the project are to develop change point detection methods and derive corresponding optimal theoretical properties for multivariate probability densities and functional time series, respectively.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1914760","Collaborative Research: Subject-level Prediction and Application","DMS","STATISTICS","08/01/2019","07/30/2019","Thuan Nguyen","OR","Oregon Health & Science University","Standard Grant","Yong Zeng","07/31/2023","$79,931.00","","nguythua@ohsu.edu","3181 SW SAM JACKSON PARK RD","PORTLAND","OR","972393011","5034947784","MPS","1269","","$0.00","Many practical problems are related to prediction, where the main interest is at the subject (for example personalized or precision medicine) or (small) sub-population (for example small community) level.  In recent years, new and challenging problems have emerged from diverse fields such as business, social sciences, and health sciences.  Examples may involve prediction of a health outcome for a new patient or perhaps prediction of a new school's response to efforts to educate children about smoking prevention.  The investigators have shown in previous work called classified mixed model prediction (CMMP) that in such cases, it is possible to make substantial gains in prediction accuracy by identifying a class that a new subject belongs to.  However, the scenarios under which CMMP currently operates are somewhat constrained and many real-life situations fall outside its scope.  Given the tremendous gains in accuracy that are possible, it would be very valuable to develop further methodology and computational advances to deepen knowledge in this area.    <br/><br/>This project aims to make methodological advances of the classified mixed model prediction method into other types of subject-level prediction problems as well as to develop new inferential methods along the CMMP idea, by making the latter truly useful in practical situations.  The basic idea of CMMP is to create a ""match"" between a group or cluster in the population for which one wishes to make prediction and a (massive) training dataset, with known groups or clusters. Once such a match is built, the traditional mixed model prediction method can be utilized to make accurate predictions. The practical challenges that will be solved in this project include i) how to deal with training data with unknown grouping; ii) how to deal with sparse, high dimensional covariates; iii) how to make better use of covariate information to improve accuracy of CMMP; and iv) how to provide accurate measures of uncertainty for CMMP-type predictions. Two important areas of application will be investigated.  One is in precision medicine and health disparities focusing on the prediction of epigenetic markers using high dimensional genotype profiles. The other comes from the area of family economics using a large survey of data from China where predictions at finer levels of resolution (e.g., households) are of primary interest. Both applications will leverage important collaborations with practitioners and thus increase the impact of the work.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916074","Multivariate Histograms and Inference with Finite Sample Guarantees","DMS","STATISTICS","07/15/2019","07/09/2019","Guenther Walther","CA","Stanford University","Standard Grant","Yong Zeng","06/30/2023","$250,000.00","","Walther@stat.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","Data that comprise several different measurements on each subject are common for modern big data. In order to store these big data in a database as well as for other applications, it is essential to summarize these big data in a compact form without losing important information. This is known to be a difficult problem due to a phenomenon called the `curse of dimensionality'. This research will implement a concrete plan to overcome this stumbling block for a number of important data analysis tasks. Importantly, the resulting methodology will provide relevant guarantees for the accuracy of these analysis tasks as well as fast algorithms for their implementation. The award will provide support of graduate training through research.<br/><br/>Density estimation based on multivariate data is known to be a difficult problem due to the `curse of dimensionality'. But in many applications the density is not the final goal of the inference, rather it is a stepping stone to access other objectives. In particular, the histogram represents a summary of the data for the main purpose of showing important features in the data, such as modes, and for estimating probabilities of subsets of the population. This proposal will address the latter problem directly in order to derive a useful multivariate histogram. The research will develop simultaneous confidence bounds with finite sample guarantees for the probability contents of certain data-dependent subsets of the sample space. It will be shown that these bounds possess certain optimality properties and that the widths of the bounds depend essentially only on the probability content of the sets and not on the dimensionality of the space, thus avoiding the curse of dimensionality. The project will develop fast algorithms to construct a histogram that satisfies these bounds and which therefore inherits these properties. The research will investigate the performance of this histogram, also in regards to detecting important features in the distribution such as modes.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916163","Learning Decision Rules with Observational Data","DMS","STATISTICS","09/01/2019","08/08/2019","Stefan Wager","CA","Stanford University","Standard Grant","Pena Edsel","08/31/2021","$140,000.00","","swager@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","The analysis of large-scale and complex data plays an increasingly central role in society, and innovations in machine learning are yielding ever more powerful predictive technologies. However, when we use such data to guide decision making, it is important to recognize that the majority of datasets in these domains are observational rather than randomized in nature, and require careful analysis in order to draw correct conclusions about the causal effect of deploying a potential policy. The research aims to develop new methods for data-driven decision making that can harness the power and expressiveness of machine learning, all while rigorously building on best practices for causal inference from non-randomized data.<br/><br/><br/>This project is centered around the following three statistical tasks: (1) Examine the problem of heterogeneous treatment effect estimation in observational studies, and develop a flexible framework that can be used with, e.g., boosting or neural networks. The accuracy of the proposed method depends on the complexity of the causal signal that we can intervene on, not on other merely associational signals. (2) Consider welfare maximizing structured policy learning, and study an approach whose regret decays as the inverse square root of the sample size in a non-parametric setting. (3) Consider the problem of learning optimal stopping rules from sequentially randomized data, and propose a new robust yet computationally feasible approach to policy learning in this setting. A unifying theme underlying all these results is that they highlight how classical ideas from semiparametric statistics can be used to rigorously leverage accurate machine learning predictors in decision-making problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1915967","Robust Wasserstein Profile Inference","DMS","STATISTICS","07/01/2019","08/08/2021","Jose Blanchet","CA","Stanford University","Continuing Grant","Yong Zeng","06/30/2023","$250,000.00","","jose.blanchet@stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1269","","$0.00","A crucial part of any data-driven decision-making problem under uncertainty involves being able to guarantee, with a high degree of confidence, a desirable performance of estimated decision rules when actually deployed in practice. This, precisely, is the key role of the guarantees given by statistical inference. The goal of this project is to investigate a novel inference methodology that precisely builds from inception data-driven decision-making rules with enhanced out-of-sample properties. This is achieved by introducing a game-theoretic formulation, in which the decision maker optimizes against an adversary that optimally exploits potential weaknesses of a decision when adding perturbations to the data (within reasonable size, yet arbitrary directions). A statistical framework is designed to estimate an optimal amount of data perturbations to obtain robust, yet practical, decision rules. The framework naturally leads to optimal (in certain sense) confidence regions for the underlying decision making parameters. The output of this project has implications in various areas of applied decision making under uncertainty, in particular, machine learning, artificial intelligence, operations management, and transportation are applications of special interest. The graduate student will work on computational methods for large scale optimal transport.<br/> <br/>The novel inference methodology to be investigated in this project unifies and extends a large class of estimators (such as generalized Lasso and regularized logistic regression among many others), which have been successfully applied in practice. These are encompassed within a distributionally robust optimization (DRO) framework. A DRO formulation is a class of games in which the statistician chooses a parameter or an action to minimize certain expected loss and an adversary chooses a perturbation of the empirical measure against the statistician (maximizing the expected loss) within a certain size (called the distributional uncertainty size). In the context of this proposal, this perturbation is measured in terms of optimal transport costs (or Wasserstein distances). The use of the Wasserstein distance and the DRO formulation justifies the name Robust Wasserstein Profile Inference of this inference methodology. The proposal studies the optimal selection of the distributional uncertainty size and associated optimal confidence regions induced by the DRO formulation. Specific applications, for example, to shape constrained estimation and stochastic optimization problem in engineering will be studied by the PI. The proposed research provides a rich interplay between the theory of optimal transport, statistical inference and convex optimization. Finally, the PI will attempt to recruit high-quality personnel from under-represented groups. The PI will also disseminate the scientific output of this proposal via open access sites, in addition to the standard vehicles (conferences and journal publications).<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916208","Spatial-Temporal Modeling and Computation for Physical Processes and Numerical Simulations","DMS","STATISTICS","08/15/2019","08/10/2021","Joseph Guinness","NY","Cornell University","Continuing Grant","Yong Zeng","07/31/2023","$220,001.00","","guinness@cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1269","","$0.00","Every minute of every day, a swarm of satellites captures images of the scenes below, and supercomputers churn out simulations of future weather and climate, accumulating a mountain of raw information about the Earth and its atmosphere. Since a significant amount of public funding has been devoted to the collection and production of this data, it is imperative that statistical tools be up to the task of analyzing it. This project aims to sort through this information, accurately filling in gaps in the raw data, inferring meaningful quantities--such as changing wind patterns--from sequences of images, and refining our understanding of the Earth as an interconnected system through the analysis of numerical computer simulations. Statistical techniques developed during this project will be made accessible to the broader community by public dissemination of software. Students and emerging researchers will be trained to use the new methods and will be empowered with specific knowledge and independent critical thinking skills to venture out and make their own impacts.<br/><br/>The project outlines advancements for three crucial tasks in the geoscientific data analysis pipeline. (1) Observations from ground monitors and polar orbiting satellites often have gaps in space and time that must be interpolated. Thus, new Gaussian process approximations are proposed that significantly reduce computational effort while improving approximations, allowing for fast and accurate interpolations. (2) Geostationary satellite sensors afford the opportunity for fine scale, continual monitoring of the atmosphere. This proposal outlines a framework for using these data--which consist of a temporal sequence of images--for the purpose of inferring upper air wind fields. (3) The pace of supercomputing has continued to increase our ability to produce high-resolution numerical simulations, which requires new computational tools for analyzing the output. A technique is proposed for local estimation that results in a globally valid statistical model, a critical feature that enables numerical model emulation and data compression via statistical models.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916237","Binary Expansion Statistics: A Nonparametric Inference Framework for Big Data","DMS","STATISTICS","09/01/2019","08/03/2019","Kai Zhang","NC","University of North Carolina at Chapel Hill","Standard Grant","Pena Edsel","08/31/2022","$149,999.00","","zhangk@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1269","","$0.00","In the Big Data era, the large numbers of observations and variables pose unprecedented challenging problems of complex forms of dependency and high computing expenses. This situation is especially common in important problems in Astronomy, Biology, Economics, Engineering, Finance, Genetics, Genomics, Neurosciences, etc. To meet these challenges, the PI proposes to study these problems through a novel framework of binary expansion statistics. The overall objective of the project is (i) to provide an in-depth understanding of complex dependency in Big Data with new theory and methods, and (ii) to build a stronger connection between Statistics and Computer Science. The PI anticipates the achievement of his goals through an integration of research and education plans.<br/><br/><br/>The binary expansion statistics framework is able to ""divide and conquer"" any complex dependency, i.e., to approximate and decompose nonlinear dependency into interactions of Bernoulli variables in the binary expansion filtration and then aggregate the information to produce nonparametric inference of dependence. This approach connects the inference problems to important concepts in Statistics and Computer Science such as multiple testing, Hadamard transform, and bitwise operation. The research agenda is to further develop this framework and study several fundamental problems to develop optimal theory, methodologies and algorithms. The PI also has comprehensive plans on educating graduate and undergraduate students and on disseminating the research results to the broader scientific community.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916220","Interactive Methods for Data-Adaptive Multiple Testing","DMS","STATISTICS","09/01/2019","08/29/2021","William Fithian","CA","University of California-Berkeley","Continuing Grant","Yong Zeng","08/31/2022","$180,000.00","","wfithian@berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1269","","$0.00","In modern applications, very large and complex data sets are routinely collected without a specific research question in mind. Rather, the express goal is to explore the data in search of novel insights, discovering relationships and structure they may not expected to be found, then report inferences for those findings. This project provides a series of iterative algorithmic frameworks for interactive multiple testing, which blend exploratory and confirmatory analyses. The frameworks are adaptive in an unusually strong sense: at each step of the procedure, the algorithm reveals more data to the analyst who makes data-driven decisions that guide the procedure.<br/><br/>The starting point of this project is the PI's recent work on the AdaPT and STAR algorithms, which give flexible and powerful frameworks for controlling the FDR while exploiting side information (AdaPT) or enforcing constraints on the rejection set (STAR). Given predictor for each p-value, AdaPT lets analysts interactively estimate Bayes-optimal p-value weights from the data using any machine learning method, while provably controlling finite-sample FDR. The STAR likewise allows for generic interactive modeling while also guaranteeing that the rejection set satisfies natural structural constraints such as hierarchy principles in regression. Both methods guarantee FDR control by means of optional stopping arguments, giving near-total flexibility to the analyst to adaptively model the data. The PI will work on four projects that continue this research program by developing new interactive methods and applying AdaPT to current scientific problems.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1907926","Statistical Analysis of Neural Data (SAND9)","DMS","STATISTICS","03/15/2019","03/08/2019","Robert Kass","PA","Carnegie-Mellon University","Standard Grant","Gabor Szekely","02/29/2020","$20,000.00","","kass@stat.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1269","7556","$0.00","The ninth international workshop on Statistical Analysis of Neural Data (SAND9) will take place May 21-23, 2019, in Pittsburgh, PA. Its purpose is to define important challenges in the analysis of neural data and useful strategies for attacking them. SAND9 will bring together neurophysiologists, statisticians, mathematicians, engineers, physicists, and computer scientists who are interested in quantitative analysis of neural data. The meeting will include 8 keynote lectures, with 5 invited discussants, 12 lectures by junior investigators chosen competitively based on abstract submissions, a poster session, and a panel discussion of emerging challenges across several areas of brain science. SAND9 is the ninth workshop in a series that began in 2002. The workshops are held during the spring at the Center for the Neural Basis of Cognition, run jointly by Carnegie Mellon University and the University of Pittsburgh. Like past SAND meetings, SAND9 aims to foster communication between experimental neuroscientists and those trained in statistical and computational methods; to encourage young researchers to present their work and get feedback from more senior investigators; and to include as participants women, under-represented minorities, and persons with disabilities, who might benefit from the small workshop environment. <br/><br/>Experimental methods for discovering the neural basis of behavior have been advancing rapidly and, as a result, brain data sets are increasing in size and complexity. Methods for better understanding such rich data sets, which can enable design of ever-more informative experiments, are desperately needed. This workshop series is concerned with identifying, discussing, and disseminating many of the most promising approaches to analysis of neural data of all kinds, including data obtained from microscopy, electrophysiology, calcium imaging, circuit stimulation, and neuroimaging. More details are available at http://sand.stat.cmu.edu/<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2013789","CAREER: Statistical inference of network and relational data","DMS","STATISTICS, Division Co-Funding: CAREER","11/01/2019","05/06/2020","Yang Feng","NY","New York University","Continuing Grant","Pena Edsel","06/30/2022","$151,307.00","","yf31@nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1269, 8048","1045","$0.00","Technological innovations have provided a primary force in advancement of scientific research and in social progress.  Large scale network data and relational data are frequently encountered in genomics and health sciences, economics, finance and social media.  The proposed project will (1) enhance methodological and theoretical developments for statistical analysis of network and relational data. (2) advance the understanding of the community structure of social network. The research emanating from this grant will advance the frontiers of theory and methods for network data modeling.  The new developments will provide better understandings of large scale network data for researchers from diverse fields of sciences and humanities, e.g., understanding  the social behavior of individuals and the dynamic nature of social network.<br/><br/>The proposed project has the following three interrelated objectives under the theme of statistical inference for large scale network and relational data. (1) To introduce a new framework for community detection with covariate information. There have been many existing approaches to  community detection. However, a majority  of  them focus on analyzing the network without considering the covariate information, which could be valuable for achieving greater accuracy of community detection. The goal of this research is to study when and how will covariate information help in terms of  the community detection accuracy.  (2) To develop a new dynamic stochastic block model framework with applications in change point detection. The stochastic block model along with its variants are usually defined for a static network. The goal here is to define a dynamic version of the stochastic block model, with a clear interpretation of how the network evolves over time. A general dynamic spectral clustering method will be proposed and its theoretical properties established. The important problem of change point detection of the dynamic network will be studied in details.  (3) To introduce a conditional dependency measure with applications in undirected graphical models. It is of fundamental interest to ascertain variables or factors underlining the network dependency structure. The goal is to introduce a flexible conditional dependency measure, which can capture a wide range of different dependency structures. The PI will develop a new method for generating a general undirected graph with desirable features by making use of the resulting conditional dependency measure."
"1853556","Collaborative Research: Development of New Statistical Methods for Genome-Wide Association Studies","DMS","STATISTICS, MATHEMATICAL BIOLOGY","07/15/2019","07/10/2019","Tieming Ji","MO","University of Missouri-Columbia","Standard Grant","Junping Wang","10/31/2020","$200,000.00","Gary Stacey","jit@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269, 7334","9150","$0.00","Advances in high-throughput sequencing technologies now make possible cost-effective analysis of whole genomes. The genomes of any two humans are 99.9% identical, with differences in the remaining 0.1% determining the diversity of human traits. For example, DNA sequence differences account for 80% of the variability in human height. Current technology allows the identification of these sequence polymorphisms between individuals, which can then be correlated to differences in a given trait. When done on a genome wide level with a large population of individuals, such genome wide association studies (GWASes) can be a useful tool for the identification of key genes controlling specific traits. However, a requirement for this approach is the availability of powerful and accurate statistical and computational methods to search through a massive amount of sequencing data to correctly identify DNA differences associated with the phenotypic trait of interest. The outcome of the project will (1) provide statistical methods to understand relationships between DNA sequence differences and the full range of diversity observed in a population, and (2) provide corresponding computational tools suitable for use by biologists and biomedical specialists for their specific population studies. This research project will produce intermediate methodological and theoretical results that lay the foundation for the final output. This project will also apply the developed methods to real, experimental data to demonstrate their utility. In addition to these research outcomes, the project will support the training of students in the field, including women and underrepresented minorities.<br/> <br/>GWAS estimates the correlation between phenotypic traits and sequence polymorphisms to identify genetic variants highly associated with specific traits. Single nucleotide polymorphisms (SNPs) are the most common type of genetic variant, and sequencing technologies allow for large-scale collection of SNP information. The project team will develop new GWAS models and methods to find trait-affecting variants with more power and accuracy. Specifically, the new methods developed in this research project will improve existing approaches by allowing modeling of observed traits from any probabilistic distribution in the exponential family. This extension ensures statistical models are biologically meaningful and interpretable. Second, the new methods will exploit different Bayesian priors, especially contemporary Bayesian priors for ultra-high dimensional model selection, that will share information across the entire genome for stable statistical inferences. Theoretical results of Bayesian priors in these new methods will also be developed. Third, a stochastic search algorithm will be developed to efficiently search through the massively large model space for model selection. This ensures that new methods are practical and useful since analysis can be done within a reasonably short time frame. Meanwhile, this also eliminates the use of subjective thresholds of significance that are now commonly used but an embarrassing practice in GWAS, having no theoretical support. Methods will be implemented into software tools and will be freely available for statisticians, biologists, and biomedical researchers. This project is funded jointly by the Division of Mathematical Sciences Mathematical Biology Program and the Statistics Program.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1906109","University of Missouri Black Migrations Symposium","DMS","STATISTICS, Sociology, Methodology, Measuremt & Stats, Geography and Spatial Sciences","02/15/2019","02/12/2019","Christopher Wikle","MO","University of Missouri-Columbia","Standard Grant","Gabor Szekely","05/31/2020","$19,982.00","Tristan Ivory, Daive Dunkley","wiklec@missouri.edu","121 UNIVERSITY HALL","COLUMBIA","MO","652113020","5738827560","MPS","1269, 1331, 1333, 1352","7556, 9150","$0.00","The Department of Black Studies and Department of Statistics at the University of Missouri, Columbia, will host a Symposium entitled ""Black Migrations"" on February 7-8, 2019.  This symposium topic was chosen to coincide with the theme of the 2019 Black History month. Migration has played a central role in the histories of Africans and their descendants. For some, migration was entirely voluntary while others were forced to move due to violence, political destabilization, ecological degradation, or other upheavals. Black migrations have also resulted in more diverse and stratified interracial populations that have reshaped the societies of the receiving areas. In more recent periods, scholars have begun exploring the impact out-migration and return migration have had on the development and stability of various majority black societies. In addition, scholars, students, and activists have been examining the relation between relocation and conceptualizations of blackness. <br/><br/>This two-day symposium will examine black migrations to include relocations within and beyond the US. Symposium organizers will seek papers from scholars and students that discuss various periods and streams of migration that have shaped the histories and contemporary realities of African people and their descendants.  An important aspect of this conference is that it will also bring together academic statisticians and geographers who are working on the modeling of migrations and related problems (e.g., population flows).  This is a unique opportunity to link the STEM discipline of Statistics with an important and timely topic that has not traditionally made use of advanced statistical methodology.  That is, Statistics has a long history of considering models for demographic population change (e.g., flow models) but there is little cross-disciplinary interaction between statisticians and researchers who have focused on Black Migration.  This may be the first such symposium that seeks to bring these groups together.  Thus, an important goal of the symposium is to encourage the interchange and mutual understanding of research ideas in statistics, geography, and black migrations, and to give motivation and direction to further research progress, particularly for young researchers, in a manner not ordinarily possible at other meetings.  The meeting will be primarily pitched towards graduate students and early career researchers.  Symposium details can be found at https://blackstudies.missouri.edu/feature/2019-black-migrations-symposium.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1928858","Diversity Workshop and Mentoring Program at the Joint Statistical Meetings","DMS","STATISTICS","08/01/2019","07/28/2019","Donna LaLonde","VA","American Statistical Association","Standard Grant","Gabor Szekely","07/31/2020","$27,500.00","Brian Millen, Jesse Chittams, Dionne Swift","donnal@amstat.org","732 N WASHINGTON ST","ALEXANDRIA","VA","223141925","7036841221","MPS","1269","7556","$0.00","The Diversity Workshop and Mentoring Program will be held July 28-31, 2019 at the Joint Statistical Meetings (JSM) at the Colorado Convention Center. The program brings historically-underrepresented undergraduates, graduates, post-docs, and early career professionals together with more senior statisticians in academia, government, and the private sector for mentoring, networking, and leadership and skills development. The ultimate objective of this workshop is to foster enduring mentoring relationships and prepare statisticians and data scientists of diverse backgrounds for career success.<br/><br/>The program will include information sessions, small group discussions and one-on-one meetings of mentor/mentee pairs during the conference. The overarching theme for the sessions is Developing Leaders, Growing Community, and Ensuring a Diverse Profession. Participants will have an opportunity to participate in parallel sessions on topics such as Career Success: Tips and Traps, Graduate School Success, Effective Presentation Skills, Strategic Networking, and Developing Successful Mentoring Relationships. The program will conclude with a Keynote address and a 100th Birthday Celebration of David Blackwell. The program is organized by the American Statistical Association's Committee on Minorities in Statistics with additional support provided by a workshop planning committee. The workshop website is https://community.amstat.org/cmis/events/dwmp/dwmp2019<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916351","Collaborative Research: Asymptotic Statistical Inference for High-dimensional Time Series","DMS","PROBABILITY, STATISTICS","08/01/2019","07/18/2019","Wei Biao Wu","IL","University of Chicago","Standard Grant","Yong Zeng","07/31/2023","$190,000.00","","wbwu@galton.uchicago.edu","5801 S ELLIS AVE","CHICAGO","IL","606375418","7737028669","MPS","1263, 1269","","$0.00","The information era has witnessed an explosion in the collection of high dimensional time series data across a wide range of areas, including finance, signal processing, neuroscience, meteorology, seismology, among others. For low dimensional time series, there is a well-developed estimation and inference theory. Inference theory in the high dimensional setting is of fundamental importance and has wide applications, but has been rarely studied. Researchers face a number of challenges in solving real-world problems: (i) complex dynamics of data generating systems, (ii) temporal and cross-sectional dependencies, (iii) high dimensionality and (iv) non-Gaussian distributions. The goal of this project is to develop and advance inference theory for high dimensional time series data by concerning all the above characteristics. The project will provide training to graduate students and publicly avaialble statistical packages. <br/><br/>This project involves developing a systematic asymptotic theory for estimation and inference for high dimensional time series, including parameter estimation, construction of simultaneous confidence intervals, prediction, model selection, Granger causality test, hypothesis testing, and spectral domain estimation. To this end, a new methodology for the estimation of parameters and second-order characteristics for high dimensional time series will be proposed. New tools and concentration inequalities for the asymptotic analysis of high-dimensional time series will be developed. To perform simultaneous inference and significance testing, the PIs will investigate the very deep Gaussian approximation problem and the high dimensional central limit theorems by taking both high dimensionality and temporal and cross-sectional dependencies into account.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916884","Quality and Productivity Research Conference - Data and Science Is a Winning Alliance","DMS","STATISTICS","04/01/2019","03/29/2019","Michael Baron","DC","American University","Standard Grant","Gabor Szekely","03/31/2020","$25,655.00","","baron@american.edu","4400 MASSACHUSETTS AVE, NW","WASHINGTON","DC","200168002","2028853440","MPS","1269","7556","$0.00","This project funds student participation in the 36th Quality and Productivity Research Conference, or QPRC, hosted by American University in Washington, DC on June 11-13, 2019. This QPRC has a special theme devoted to the Data Revolution, ""Data and Science Is a Winning Alliance."" The conference aims to demonstrate and explore the interplay between data and science, and to discuss modern scientific approaches to handling big, multidimensional, unstructured data.  The sessions will address how data and science, the two fundamental sources of knowledge, are leveraging each other to accelerate discoveries and increase quality and productivity.  The program will include a comprehensive discussion of cutting-edge modern methodologies in diverse aspects of data science and current progress made in such computer-intensive fields as stream data mining, machine learning, functional data analysis, image reconstruction, and facial recognition.  Participating statisticians, data scientists, quantitative analysts, and representatives of different branches of industry and government will exchange novel ideas and share experiences in working with modern big data to discover knowledge and applying it in numerous fields. <br/><br/>Plenary and invited paper sessions at the 36th QPRC are scheduled in the following subjects: machine learning, applied data mining, time series and forecasting, text analytics, data science, cybersecurity, data visualization, big data analytics, statistical process control monitoring, functional data analysis, and spatial data science.  The conference also includes contributed and poster sessions and a technical tour. It is preceded by a one-day course ""Big Data Analytics: Dealing with Structured, Semi-structured, and Unstructured Data.""  Strong student participation is a vital component of the 36th QPRC. Efforts will be made to broaden participation by recruiting students from under-represented groups and engaging students from numerous universities and graduate programs across the United States. Students are invited to submit contributed and poster presentations and compete for travel scholarships. The conference details are on its web site, https://www.american.edu/cas/qprc/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2005746","Collaborative Research:  Nonparametric Bayesian Aggregation for Massive Data","DMS","STATISTICS","08/01/2019","11/08/2019","Zuofeng Shang","NJ","New Jersey Institute of Technology","Continuing Grant","Gabor Szekely","08/31/2020","$30,604.00","","zshang@njit.edu","323 DR MARTIN LUTHER KING JR BLV","NEWARK","NJ","071021824","9735965275","MPS","1269","8083","$0.00","Modern massive data appear in increasing volume and high heterogeneity. Examples include internet searches, social networks, mobile devices, satellites, genomics, medical scans, etc. Bayesian approaches are particularly useful in such context since the complex structures in the data can be naturally incorporated in Bayesian hierarchical models. Besides, uncertainty quantification can be easily executed through Bayesian computation. However, due to storage and computational bottlenecks, traditional Bayesian computation implemented in a single machine is no longer applicable to modern massive data. In this project, a set of nonparametric Bayesian aggregation procedures with theoretical justifications are developed based on a standard parallel computing strategy known as Divide-and-Conquer. This research will significantly enhance the availability of Bayesian tools and software for analyzing massive data. The educational plan of the project will be in the form of graduate student advising and offering of special topics courses. <br/><br/>This project consists of three major components. First, the PIs will establish a Gaussian approximation of general nonparametric posterior distributions which serves as a theoretical foundation for general distributed Bayesian algorithms. Second, the PIs will develop a nonparametric Bayesian aggregation procedure with theoretical guarantees that is particularly useful to handle massive data in a parallel fashion. Third, the PIs will develop an efficient parallel Markov Chain Monte Carlo (MCMC) algorithm for nonparametric Bayesian models which will perform as well as traditional MCMC with substantially less computational costs. This research will lead to an emergence of ""Splitotics (Split+Asymptotics) Theory"" providing theoretical guidelines for Bayesian practices. The smoothing spline inference results recently obtained by the PIs will be used as a promising tool for achieving the above goals."
"1848035","CAREER: Statistical Inference of Tail Dependent Time Series","DMS","STATISTICS","07/15/2019","04/21/2021","Ting Zhang","MA","Trustees of Boston University","Continuing Grant","Huixia Wang","06/30/2021","$208,663.00","","tingzhang@uga.edu","1 SILBER WAY","BOSTON","MA","022151703","6173534365","MPS","1269","1045","$0.00","The project aims to develop a new theoretical framework and statistical inference methods for the analysis of tail dependent time series, and to educate future statisticians and data scientists on its theory and practice. Tail dependent time series, as an emerging data type, has been observed and recognized in various fields including actuarial science, climate science, economics, finance, hydrology, and internet traffic engineering, among others. The task of understanding and appropriately accommodating the phenomenon of tail dependence can be of significant importance to the modeling of extreme events such as earthquakes, hurricanes, and financial crises. The results from the project will make significant impacts in scientific areas such as climate science, economics, actuarial science, finance, hydrology and internet traffic engineering. The proposal also involves an integrated education plan to expose undergraduate and high school students to the topic, to equip graduate and advanced undergraduate students with a desirable level of statistical reasoning and analytical skills for analyzing tail dependent time series, and to mentor doctoral students to become future leaders in the education and research of the area. <br/><br/>Existing methods for studying tail dependent time series often rely on certain parametric models for describing the underlying tail dependence structure. This is particularly due to the lack of a convenient and rigorous framework that one can use to obtain desired limit theorems for a general class of tail dependent time series. The project aims to address this fundamental problem by proposing a new framework based on the causal representation and the technique of adversarial tail coupling. Using the newly proposed framework, the project will develop meaningful results toward a tail m-dependent approximation scheme, which can then be used as a powerful tool to obtain limit theorems for statistics of tail dependent data. Compared with the conventional m-dependent approximation, the current setting can be more challenging due to the double asymptotics where the quantile index is allowed to approach either zero or one as the sample size increases to reflect extreme risks. The project will study several statistical inference problems for tail dependent time series, including high quantile estimation and its associated confidence interval construction, tail dependence visualization and testing, inference of extremely high quantiles using the extreme value theory, extensions to high and extremely high quantile regression models, and high-dimensional nonstationary settings. The results to be developed are expected to be useful in identifying undiscovered features in certain climate science and economic data, and applicable to other scientific problems that involve the analysis of tail dependent time series.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"1916448","Markov Random Fields, Geostatistics and Matrix-Free Computation","DMS","STATISTICS","09/01/2019","07/29/2019","Debashis Mondal","OR","Oregon State University","Standard Grant","Pena Edsel","10/31/2021","$120,000.00","","mondal@wustl.edu","1500 SW JEFFERSON ST","CORVALLIS","OR","973318655","5417374933","MPS","1269","","$0.00","In the past few decades, spatial statistics has become increasingly important in agriculture, epidemiology, geology, image analysis and environmental science. PI's prior research provided new perspectives in connecting two major branches of spatial statistics, namely the Markov random fields and geostatistics and in advancing fast statistical computations.  At present, many important scientific applications demand use of complex spatial models and their multivariate and spatial-temporal versions. However, statistical computations of these complex spatial models have remained a challenge. The project derives new mathematical understanding on these complex spatial and spatial-temporal models, which then opens up the possibility of advancing various scalable statistical computations with minimal storage.  The project will contribute to obtaining enhanced scientific understanding in studies such as arsenic and magnesium contamination and hydro-chemical analysis of groundwater and spatial and spatial temporal variations in opioid overdose cases in the United States.<br/><br/>The project brings together mathematical and computational knowledge from different scientific fields to develop principled frameworks for spatial statistics and inference. The research aims to provide new understanding on (i) constructions of higher neighborhood order Gaussian Markov random fields, (ii) joint modeling of two or more spatial variables, and (iii) complex spatial-temporal models. Novel matrix-free computations are proposed to advance statistical inference. These computations include not just best linear unbiased predictions and residual maximum likelihood estimation, but also scalable Hamiltonian Monte Carlo methods. Applications will include mapping (1) heavy metal contamination in groundwater and (2) geographic variations in drug overdose cases across the United States. The project also aims to integrate research and educational activities through developing short courses and case studies on spatial statistics and scalable computation, and through providing valuable training and learning opportunities for graduate students.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."
"2006475","Higher Order Asymptotics for Some Nonstandard Problems in Time Series and in High Dimensions","DMS","STATISTICS","09/01/2019","11/14/2019","Soumendra Lahiri","MO","Washington University","Continuing Grant","Gabor Szekely","06/30/2021","$92,407.00","","s.lahiri@wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","MPS","1269","","$0.00","Correlated and high dimensional data appear routinely in many areas of sciences, including atmospheric sciences, finance, and molecular genetics, as well as in an ever increasing number of everyday activities such as social networking. While a vast amount of data are being generated and are available for analyses, traditional methods often fail to elicit information in such applications. This research project has two major goals.  First, it seeks to develop new mathematical tools for analyzing a recent complex statistical approach for correlated data that has been known to produce astonishingly accurate results in empirical studies, but lacks any theoretical justification. It is hoped that the new theoretical tool will lead to further refinements of existing statistical methodology for correlated data. The second part of the project is concerned with complex inferential issues for high dimensional data where the number of unknown parameters far exceeds the sample size, such as determining the role of a few important genes among a collection of several thousand genes from data on a few hundred patients. The project seeks to develop theoretical and methodological statistical tools to enable researchers to address important inference questions without stringent model assumptions. <br/><br/>The project aims to develop some critical theoretical tools and nonparametric statistical methodology for the analysis of time series and high dimensional data. Specifically, this project will focus on (i) developing asymptotic expansion results for the ""fixed-b"" asymptotic approach in time series that has shown significant improvement over traditional methods in several empirical studies but with very little theoretical underpinning; (ii) investigating higher order properties of some general classes of statistical tests (e.g., Wald tests) and of some more recently proposed nonstandard empirical likelihood tests, both under the ""fixed-b"" formulation; (iii) developing new pivotal quantities for block bootstrap in time series that nearly match the accuracy of bootstrap under independence; (iv) developing asymptotic expansion results in high dimensions under sparsity by exploiting some novel tools from approximation theory and Banach space theory; (v) applying the asymptotic expansion results from (iv) to investigate the ""phase transition"" phenomenon in asymptotic properties of statistical methods in high dimensions, and (vi) investigating properties of resampling methods for post-variable selection inference in high dimensions."
"1903479","International Conference on Design of Experiments 2019","DMS","STATISTICS","05/01/2019","05/03/2019","Manohar Aggarwal","TN","University of Memphis","Standard Grant","Gabor Szekely","04/30/2020","$10,000.00","E Olusegun George, Dale Bowman","maggarwl@memphis.edu","Administration 315","Memphis","TN","381523370","9016783251","MPS","1269","7556","$0.00","The International Conference on Design of Experiments (ICODOE 2019) will be held at the Department of Mathematical Sciences, University of Memphis, during May 18-21, 2019.  The design and analysis of experiments plays a critical role in the process of discovery and improvement in science, medicine, engineering and manufacturing and has enormous impact on the daily lives of just about every person in the United States and throughout the world.  The goals of the ICODOE 2019 conference are (a) to encourage and support early career researchers, graduate students, and underrepresented groups (minority and women), and give them the opportunity to form networks for future research interaction, (b) to provide these researchers with state of the art knowledge in the area of experimental design, and (c) to bring outstanding researchers from across the world to present their active research in the field and its various applications.  The conference will have a significant impact on the education of junior researchers and will help in the advancement of knowledge and cross fertilization of techniques of experimental designs across disciplines.<br/><br/>Rapid developments have taken place in research on design and analysis of experiments, and the conference will have sessions on a wide variety of topics of current importance, including the following issues. (i) Computer Experiments which have an ever growing impact on science, engineering and technology. The complexity of scientific questions emerging from successful interdisciplinary collaborations, make the development, assessment and utilization of complex computer models of critical importance; (ii) The rapid development of science and technology has enabled researchers to generate and collect data with considerable size and complexity in all fields from academia to industry. These data provide remarkable opportunities for discovering new knowledge and insights, yet these opportunities have not yet been fully explored due to the computational burden. A systematic experimental design thinking for large-scale statistical analysis is essential; (iii) With the increasing popularity of social media, designs for online experiments are becoming vitally necessary.  Other sessions will include the design of simulation experiments needed in manufacturing, new developments in factorial designs for industry, design of discrete choice experiments for marketing, psychology, and medical studies, adaptive designs for clinical trials, optimal design of dose-finding studies for pharmaceutical research, computational aspects of large-scale optimal experimental designs and Bayesian nonlinear designs, designs for experiments with multiple objectives or multiple stages, and issues of robustness including the interface of design with machine learning.  The website for ICODOE 2019 is http://www.memphis.edu/msci/icodoe2019/.<br/><br/>This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria."

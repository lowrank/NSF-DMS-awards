"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"9973321","Algorithms for Nonsmooth Equations","DMS","COMPUTATIONAL MATHEMATICS","08/01/1999","08/02/1999","Stephen Billups","CO","University of Colorado at Denver-Downtown Campus","Standard Grant","Michael Steuerwalt","07/31/2003","$100,000.00","","sbillups@carbon.cudenver.edu","1380 LAWRENCE ST STE 300","DENVER","CO","802042055","3037240090","MPS","1271","0000, 9263, OTHR","$0.00","This project will develop algorithms for solving nonsmooth systems of<br/>equations, addressing areas where methods for nonsmooth equations lag<br/>behind those for smooth equations.  The first of these areas involves<br/>solving highly nonlinear problems where an associated merit function<br/>may have local minima that are not solutions.  Such problems cause<br/>difficulties for descent-based methods, which may converge only to a<br/>local minimum of this merit function.  The second area focuses on<br/>solving ill-posed problems (that is, those with singular, or<br/>ill-conditioned Jacobian matrices at a solution).  The third area<br/>addresses developing limited memory methods for solving very<br/>large-scale problems.  In the smooth domain, a number of tools are<br/>available for solving such very large-scale problems.  These include<br/>preconditioned conjugate gradient methods, limited-memory quasi-Newton<br/>methods, Chebyshev methods, and truncated Newton methods.  Possible<br/>extensions of these methods to solving nonsmooth systems of equations<br/>will be explored.  In addition to the pursuit of these natural<br/>extensions of smooth methods to the nonsmooth problem domain,<br/>practical algorithms for solving generalized equations will be<br/>developed.  The ultimate goal of these four research areas is the<br/>development of high quality computer software for solving nonsmooth<br/>equations, which will be made publicly available.<br/><br/>In less technical terms, this project is aimed at developing high<br/>quality computer software for solving certain types of nonsmooth<br/>equations for which current computational tools are inadequate.<br/>Nonsmooth equations provide a unifying framework for many important<br/>mathematical problems and have applications in a wide variety of<br/>fields.  Some examples include international trade, development<br/>planning, financial options pricing, robotics, contact mechanics,<br/>traffic equilibria, and economic equilibria.  Because of this wide<br/>array of applications, the development of software that improves<br/>our ability to solve nonsmooth equations will provide a valuable <br/>tool for solving many problems of national interest."
"9973328","Construction of Accurate, Robust and Efficient Numerical Techniques for Partial Differential Equations","DMS","COMPUTATIONAL MATHEMATICS, NUMERIC, SYMBOLIC & GEO COMPUT","08/01/1999","07/29/1999","James Bramble","TX","Texas A&M Research Foundation","Standard Grant","Michael Steuerwalt","07/31/2003","$289,842.00","Raytcho Lazarov, Joseph Pasciak","james.bramble@math.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1271, 2865","0000, 9263, OTHR","$0.00","9973328<br/><br/>The main objectives of this research are the construction and study of finite element approximations to steady-state and transient problems of mechanics and engineering, including convection-diffusion-reaction problems, Stokes and Navier-Stokes equations, equations of linear elasticity, and the construction, analysis, and testing of fast methods for solving the resulting discrete systems.  The emphasis will be on the development of new accurate, robust and efficient computational techniques.  Approximation strategies involving standard Galerkin finite element methods,  least-squares methods, and mixed finite element methods will be  studied especially from the standpoint of their accuracy, stability, and efficient implementation.  The solution techniques which will be developed will emphasize preconditioning via domain decomposition or multigrid/multilevel methods.  Local grid refinement  and error control coupled with multilevel solution methods will be used in ground water and petroleum applications.<br/><br/>Technological development is becoming increasingly dependent on large scale scientific computation.  Such computations are often limited by accuracy of the numerical method and the amount of computational effort required for its solution.  The research funded by this proposal will provide better numerical methods and new techniques for the efficient solution of the resulting system of linear and non-linear equations.  Such advances will improve our ability to accurately model more complex processes (devices) as well as our ability to understand complex systems such as the behavior of a contaminant in a multi-component groundwater system.<br/>"
"9996416","Inverse Diffraction Problems in Optics","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/16/1999","09/13/1999","Gang Bao","MI","Michigan State University","Standard Grant","Deborah Lockhart","06/30/2002","$44,000.00","","bao@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1266, 1271","9146, 9263, MANU","$0.00",""
"9971798","Interpretation of Computer Simulations and Experimental Data from Chaotic Processes","DMS","COMPUTATIONAL MATHEMATICS","08/01/1999","07/26/1999","Timothy Sauer","VA","George Mason University","Standard Grant","Michael Steuerwalt","07/31/2002","$59,091.00","","tsauer@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1271","0000, 9263, OTHR","$0.00","Sauer<br/>9971798<br/><br/>The investigator studies computational aspects of nonlinear dynamical systems, with emphasis on questions that have multidisciplinary implications in the sciences and engineering.  The first main area involves the validity of long-term computer simulations of typical nonhyperbolic chaotic systems.  The question is whether long simulation trajectories can be assumed to approximately match true system behavior.  This is a far-reaching question that goes to the heart of numerical computation.  Numerical analysis is typically concerned with algorithm properties that are local in time, but the possibility of chaotic trajectories in nonlinear systems is causing this view to shift markedly.  The project involves the development of quantitative laws for the expected time between breakdowns, or mismatches, between computer simulation trajectories and the correct trajectories of computer models, and investigation of statistical quantities that may be severely miscalculated because of the mismatch.  The second major area is ongoing work by the investigator on the interpretation of laboratory experiments that generate aperiodic data streams.  Using univariate or multivariate time series or inter-event time intervals produced by a deterministic physical process, the phase space of the process can be faithfully reconstructed as the basis for applications such as system identification, filtering, prediction and control.  The mathematical foundations of these applications has been the topic of ongoing research by the investigator and coworkers.  The project attempts to widen the areas of validity of these methods, and increase their power for the study of complex systems, especially in the presence of observational noise.<br/><br/>Computer simulations are an essential part of modern science.  The importance of the correct interpretation of long-term computer simulations of nonlinear  models is increasing as modeling replaces experiments and expensive traditional methods of design in areas as diverse as biotechnology, rational drug design, meteorology, satellite orbit trajectory design, wind tunnel testing, and neurophysiology.  The questions explored in this project are critical to the fundamental understanding of the results of complex simulations of nonlinear processes.  One goal of this project is to explore these questions in physically relevant models, and in particular to isolate and quantify the limitations of these representations, especially for the purpose of long-term modeling.  The second focus of this research is the interpretation of data collected from chaotic systems in laboratory experiments and nature.  Complex deterministic time series are being explored as key information in physical, chemical, engineering and biological/medical settings.  As an example, neuron firing data from hippocampal cells of mammals and other small neural systems are studied by the investigator in conjunction with a group of medical researchers who have recently relocated at George Mason University, with the purpose of detecting deterministic information processing in the brain.  The investigator has done previous work on expanding these conceptual foundations and developing related computational implementations, and plans to widen their areas of validity and increase their power for the study of complex systems in natural, experimental, and engineering-related contexts.  New techniques for these applications are developed in this project, involving new computation techniques used in conjunction with existing signal processing methods.<br/>"
"9973926","Noninvertible Dynamical Systems: A Computer-Assisted Study","DMS","COMPUTATIONAL MATHEMATICS","09/01/1999","10/27/2003","Bruce Peckham","MN","University of Minnesota-Twin Cities","Standard Grant","Junping Wang","08/31/2004","$140,000.00","","bpeckham@d.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1271","0000, 9139, 9229, 9263, HPCC, OTHR","$0.00","     The investigator studies the dynamics and bifurcations of<br/>discrete-time systems featuring nonunique reverse-time behavior.<br/>He and his colleague investigate two-dimensional endomorphisms<br/>and aspects of three-dimensional endomorphism dynamics, and<br/>develop scientific computation, stability, and visualization<br/>tools necessary for these studies.  The basic tool for the study<br/>of noninvertible maps is the notion of a ""critical set"": the<br/>locus of points where the linearized map becomes singular.<br/>Understanding the forward and backward iterates of the critical<br/>set is the key to understanding the full dynamics of<br/>noninvertible maps.  Because trajectories backward in time are<br/>not unique, definitions (e.g. unstable manifolds of saddle<br/>points) and computations (e.g. stable manifolds of saddle points)<br/>involving backward iterates must be extended from the<br/>(invertible) diffeomorphism case to the (noninvertible)<br/>endomorphism case.  The interactions of these sets and the system<br/>attractors with the critical set give rise to a class of<br/>important dynamical phenomena distinct from those arising in<br/>diffeomorphisms, and the dependence of such interactions on<br/>parameters gives rise to new bifurcations.  Current computational<br/>tools used for simulation, computation of invariant sets, and<br/>study of bifurcations are correspondingly extended, and new tools<br/>are constructed to deal with multiple backward trajectories and<br/>their consequences.  The possible geometric explosion of the<br/>number of distinct preimages of a point (or a set) requires new<br/>programming approaches, and scientific visualization becomes<br/>crucial in numerical exploration of global dynamics and their<br/>parameter dependence.  Thr project involves work in software<br/>development, numerical investigation, and mathematical analysis.<br/>The interplay of all three is necessary to have any hope of<br/>understanding the dynamics of nontrivial systems.  The<br/>investigator plans a systematic computer-assisted study of the<br/>dynamic behavior of noninvertible maps, carried out in a<br/>continuous dialogue between computation and theory, and with<br/>illustrative and technologically relevant applications in mind.<br/>     The investigator and his colleague continue studying the<br/>dynamical behavior of typical systems of equations that are used<br/>to model a variety of physical, chemical, and ecological systems.<br/>The goal is to study changes, or ""bifurcations,"" in the<br/>qualitative behavior of these systems as parameters are changed.<br/>The class of models being studied is called ""noninvertible<br/>discrete dynamical systems."" A discrete dynamical system is one<br/>which provides a rule for predicting some quantity or quantities<br/>at some point in the future (say, one year from now) based only<br/>on the value of that quantity now.  By iterating the rule, we can<br/>predict what the quantity will be at any year in the future.<br/>When the rule can be ""inverted"" to produce a rule to go backward<br/>in time, the system is called invertible.  Otherwise it is<br/>noninvertible.  The project includes software development,<br/>numerical investigation, and mathematical analysis.  The behavior<br/>of these systems is typically so complicated that there is no<br/>hope of understanding them without the aid of computers.  On the<br/>other hand, computer investigation alone, without the acompanying<br/>mathematical analysis, generally leads to a very incomplete<br/>understanding.  Consequently, all three activities must be done<br/>in concert with each other.  With respect to all three activites,<br/>there has been much previous work on one-dimensional (the<br/>dimension is the number of quantities that is being monitored)<br/>invertible and noninvertible systems and on higher dimensional<br/>invertible systems, but work on two-dimensional and higher<br/>noninvertible systems is still in relative infancy.  It is this<br/>area on which they focus.  Advances will lead to a greater<br/>understanding of this class of models, and therefore the<br/>phenomena that are modeled by them.  In the process, they develop<br/>software tools that are of use to others for continued study of<br/>these systems, as well as tools for scientifically visualizing<br/>objects, whether they come from dynamical systems applications or<br/>from any area of science, engineering or mathematics.<br/>"
"0096312","Modeling and Computational Studies of Cell and Tissue       Movement","DMS","PLANT FUNGAL & MICROB DEV MECH, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/1999","08/03/2000","Hans Othmer","MN","University of Minnesota-Twin Cities","Continuing Grant","Michael Steuerwalt","08/31/2002","$375,474.00","Dean Bottino, Eirikur Palsson","othmer@math.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1118, 1266, 1271","0000, 9263, OTHR","$0.00",""
"9971800","Fast, High Order Vortex Methods Based on Deforming Basis Functions","DMS","COMPUTATIONAL MATHEMATICS","08/01/1999","07/27/1999","Louis Rossi","MA","University of Massachusetts Lowell","Standard Grant","John C. Strikwerda","07/31/2001","$71,000.00","","rossi@math.udel.edu","600 SUFFOLK ST STE 212","LOWELL","MA","018543624","9789344170","MPS","1271","0000, OTHR","$0.00","9971800<br/><br/>This project expands and enhances existing knowledge of a new category of vortex method based on deforming basis functions.  Vortex methods approximate the vorticity field of a flow as a linear combination of localized basis functions.  These basis functions move with the flow velocity at the basis function centroid, and the velocity field is calculated from the vorticity field through a Biot-Savart integral.  For flows dominated by isolated regions of vorticity, vortex methods offer considerable advantages because they are naturally adaptive in the sense that computational elements are dedicated exclusively to regions that have vorticity.  Most vortex methods use rigid, axisymmetric basis functions though there are some exceptions.  This project studies vortex methods that use basis functions which deform with local flow deviations, and builds upon the recent development of a high spatial order vortex method based on deforming elliptical Gaussians.  Elliptical Gaussians are special in the sense that they represent self-similar, exact solutions to the relevant advection-diffusion equation.  To implement such a scheme, one must develop methods for evaluating the Biot-Savart integral for an elliptical Gaussian and identify the relevant convergence parameters for the method as a whole.  Though a two-dimensional scheme has already been developed, the Principle Investigator will enhance this method by developing a fast summation algorithm.  The Principle Investigator will extend this method to three dimensions, providing a crucial link between vortex stretching and the geometry of vortex filaments.  The new method will be used to perform careful calculations of a variety of problems including vortex dipole collisions and jet transients.  Finally, the Principle Investigator will extend this approach to moisture transport through unsaturated porous media.<br/><br/>This work provides a means of quickly and accurately calculating fluid flow properties on a wide range of problems including but not limited to aerospace applications, industrial processes, ocean currents and atmospheric flows of all sorts.  The Principal Investigator will also extend these concepts along new lines to develop methods that can accurately calculate the motion and diffusion of moisture and contaminants in unsaturated soils.  This type of method, called a ""vortex method"", is unusual in that these schemes are naturally adaptive. Naturally adaptive methods dedicate computational resources exclusively to the dominant regions of the flow.  These methods simulate flows by calculating the evolution of the local angular momentum in the fluid.  Often, the angular momentum is restricted to a small fraction of the total volume of the fluid.  For instance, weather systems are driven by a collection of storm systems that represent concentrated regions of angular momentum.  Using such a method, one can reconstruct the entire flow field based solely on the evolution of the angular momentum which means that the computer only need perform calculations over a small area to capture the entire flow field.  Vortex methods have the added advantage of being easily parallelized on multiple processor computers so that one can take full advantage of supercomputing facilities and networks of connected computers.  Also, the Principal Investigator will apply these techniques to an entirely new type of problem involving flow through unsaturated porous media such as dry soils, clays or concrete.  Similar to vortex methods, this new technique takes advantage of the fact that the movement of moisture is dominated by narrow ""preferred paths"" occupying a relatively small fraction of the total domain of interest.  These paths are created by moisture-media interactions, and the computer need only dedicate its resources to those regions containing moisture.  Finally, many aspects of these activities make excellent undergraduate research projects.  These research activities will enhance students' interests and knowledge in mathematics, environmental science and high performance computing.  Thus, in addition to its scientific merits, this project will give students at the University of Massachusetts Lowell meaningful research experiences and enhance collaboration across several disciplines.<br/>"
"9973331","Numerical Analysis of Qualitative Dynamics in Flows","DMS","COMPUTATIONAL MATHEMATICS","08/01/1999","07/27/1999","William Kalies","FL","Florida Atlantic University","Standard Grant","Michael Steuerwalt","07/31/2002","$33,411.00","","wkalies@comcast.net","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","MPS","1271","0000, 9263, OTHR","$0.00","Kalies<br/>9973331<br/><br/>The investigator develops numerical methods for determining the qualitative dynamical behavior of solutions to ordinary differential equations.  For these methods, input requires only a system of differential equations, a region of the phase space, and no a priori knowledge of the dynamics.  The output is a Morse decomposition of the invariant set into regions containing nontrivial dynamics.  Information about the behavior in these regions is given in terms of the Conley index, and connecting orbits between Morse sets can be detected.  Such an algorithm is beneficial as a first step in the study of systems of differential equations for which little information is known, particularly in higher dimensions where visualization of numerically simulated solutions is difficult.  The type of computation envisioned here provides a somewhat coarse, qualitative description of a dynamical system.  However, the Conley index, a tool from dynamical systems theory, is an algebraic topological index, which also allows for the development of algorithms for rigorously proving the existence of specific dynamical behavior.  Therefore, a reliable algorithm can be used both as a tool for analyzing unfamiliar systems and as a tool for obtaining computer-assisted proofs of specific dynamics; both would have a wide variety of applications.  The algorithms developed here also have potential applications in computational topology, in particular the computation of the homology of cellular complexes, and connections to mesh generation and Delaunay triangulations.<br/><br/>Differential equations are used to model many phenomena in the physical sciences: mechanical systems, chemical reactions, and biological systems.  Understanding the dynamics of such inherently nonlinear systems is a problem in global analysis.  When the underlying space has dimension higher than three, the study of such systems is often beyond the scope of current analytic techniques.  Computers are capable of performing many calculations of solutions to such equations, but most current numerical methods are not well suited for extracting asymptotic dynamical behavior from systems in higher dimensions.  One problem is that computers can generate a large quantity of data that is difficult to analyze; often a graphical analysis is performed, which is impractical in higher dimensions and can miss unstable objects.  The purpose of this project is to design and implement algorithms to study the asymptotic dynamics of differential equations in higher dimensions.  The output of the algorithm is an algebraic index (designed by C.  Conley) that could be thought of as a tool for extracting the essential dynamical information (including unstable objects) from computational data and that does not require visualization to be understood.<br/>"
"9905454","Propagation of Uncertainty in the Field Extrapolation of Laboratory Experiments:  Application to Dioxin Contaminated Sediments","DMS","COMPUTATIONAL MATHEMATICS, ENVIRO GEOCHEM AND BIOGEOCHEM","09/01/1999","08/20/1999","Pierre Goovaerts","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Michael Steuerwalt","08/31/2002","$250,000.00","Peter Adriaens","goovaert@engin.umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1271, 1584","1584, 9198, 9263, EGCH","$0.00","9905454<br/><br/>The investigators develop a Monte-Carlo based approach to account for major sources of uncertainty that affect the extrapolation (""scaling up"") of laboratory results to the field.  These include the uncertainty attached to the parameters of laboratory-based predictive models, and the uncertainty about the spatial distribution of environmental factors that are inputs to these numerical models.  The methodology is applied to the simulation and mapping of dioxin degradation (dechlorination) in general, and 2,3,7,8-TCDD fluxes in particular, in estuarine sediments.  The project involves the development of models relating the dioxin dechlorination rate to various input variables (salinity and respiratory conditions, dioxin concentrations, methane production, sulfate reduction), the geostatistical modeling of uncertainty about the spatial distribution of input variables through multivariate nonparametric stochastic simulation, and the analysis of uncertainty propagation through Latin hypercube sampling of the probability distributions of model parameters and input variables.  These risk assessments are accounted for in decision-making processes, such as identification of locations of largest uncertainty where additional samples should be collected, and in the long-term prediction of temporal trends in dioxin concentrations.<br/><br/>Dioxin-contaminated sediments pose serious threats for the environment and can entail costly clean-up to maintain the nation's waterways.  To avoid unnecessarily stringent remediation efforts, it is critical to map these contaminated sediments and to predict the current concentrations and how they could evolve in the near future.  Much research has been devoted so far to laboratory experimentation, which allowed a better understanding of the mechanisms governing the degradation of these contaminants under relevant ambient conditions as a function of sediment biogeochemistry.  Extrapolation of these results to the field is not straightforward because of the spatial variability of the contaminant concentrations and of the environmental factors (e.g. salinity, respiratory conditions) controlling their degradation.  This research develops a methodology to simulate the spatial distribution of dioxin concentrations and their degradation in time.  Multiple equally-likely scenarios are generated to assess the uncertainty arising from our imperfect knowledge of degradation mechanisms and of the spatial distribution of contaminants, in particular when sampling is sparse.  The approach is illustrated for the Lower Passaic River/Newark Bay estuary because there is an extensive temporal and spatial data base for this estuary.  Success of this research will help scientists and engineers to meet the increasing challenge to combine various sources of information acquired at field and laboratory scales with different levels of uncertainty (measurement and prediction errors).  It may prove useful in determining long-term risks posed by dioxin-contaminated sediments, and aid in the development of risk-based sediment management strategies.<br/><br/>"
"9973291","Numerical Methods for Multimaterial and Multiphase Flows","DMS","COMPUTATIONAL MATHEMATICS","07/15/1999","05/06/2005","Smadar Karni","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Junping Wang","06/30/2005","$139,171.00","","karni@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1271","0000, 9263, OTHR","$0.00","9973291<br/><br/>This proposal concerns the development and implementation of algorithms for <br/>compressible multimaterial and multiphase flows.  In both types of flows, a major numerical stumbling block is accounting correctly for thermodynamical relaxation processes and equilibrium between the fluid components.  Failure to do so is the reason why state-of-the-art single fluid algorithms often do not work in multifluid flows.  Karni was among the first to realize that the key to robust multifluid algorithms is good control over the pressure field, and has been active in developing and implementing algorithms for complex multimaterial flows.  It is proposed: (i) to develop multimaterial algorithms for stiff fluids with surface tension.  Numerical issues such as stiffness, singular source treatment, interface sharpening and efficient time integration will be studied.  Comparison with asymptotic theories and experiments of oscillating bubbles will be conducted; (ii) to extend multimaterial algorithms to multiphase flow models, in particular to the 2-velocity 2-pressure multiphase model which is sufficiently general and is time hyperbolic.  Numerical aspects such as global conservation, positivity, efficient integration of stiff source terms will be addressed.  The methods will be validated against numerical benchmark tests. (iii) to use multimaterial computations to 'validate' multiphase flow models by numerical averaging. Studying mean flow properties might provide numerical Hugoniot curves for the multiphase (nonconservative) systems, and might shed light on the controversial issue of constitutive closures.  This part will be done in collaboration with H.M. Glaz of the University of Maryland.<br/><br/>Multimaterial and multiphase flows are flows consisting of several 'pure' gas or liquid components.  For example, hydrogen and air, air in water, liquid droplets in air or even dust in air.  Depending on the application, the interest may be to follow the motion of the interface that separates the different fluids and study its dynamics and stability.  For example, computing the dynamics of a an oscillating gas bubble in water is useful for understanding the propagation of sound waves underwater, and for studying underwater explosions.  In order to burn fuel efficiently in combustion chambers one needs to understand mixing processes, which in turn are governed by the dynamics of, say, a hydrogen jet in air.  In some other applications, the number of interfaces may be huge and following the dynamics of individual interfaces is not only impossible but is also not interesting. For example, in bubbly liquids such as soda cans, in liquid suspensions such as in sprays or in dusty gases, the motion of a single bubble or droplet is not of interest.  Rather, the interest is in understanding how the fluid mixture behaves 'on average'.  Both types of flows lead to mathematical models that need to be solved on computers, and are computationally very intensive.  They are also not easy to compute. The 'average' models are also mathematically less well understood.  Experiments, while available, are difficult to conduct and are restricted by the accuracy of the instruments and their often large margins of error.  Computer simulations provide therefore a complementary tool which may shed light and give insight into the complex problems of interest.  The past few years have seen a growing interest in developing methods suitable for computing multimaterial/multiphase flows and in their efficient implementation to studying complex flow phenomena.  This project concerns further development and implementation of such methods.<br/>"
"9970310","Nonlinear Waves in One-Dimensional and Multi-Dimensional Conservation Laws","DMS","COMPUTATIONAL MATHEMATICS, ANALYSIS PROGRAM","08/01/1999","11/05/2004","Suncica Canic","TX","University of Houston","Standard Grant","Joe W. Jenkins","09/30/2005","$173,378.00","","canics@berkeley.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271, 1281","0000, 9178, 9251, 9263, OTHR, SMET","$0.00","The focus of this research proposal is on the study of solutions of<br/>one-dimensional and multi-dimensional systems of conservation laws.<br/>In spite of an extensive experimental and numerical exploration of problems<br/>that are governed by MULTI-DIMENSIONAL conservation laws, there is no theory<br/>that would describe properties of nonlinear waves that arise as solutions of<br/>these problems. Recent results by the PI and co-worker Keyfitz indicate that<br/>for a large class of two-dimensional problems, including standard equations of<br/>compressible flow, an analysis of their self-similar solutions leads to many<br/>results that are shared by the entire class of problems. They include an existence and uniqueness theorem that holds in the region where the flow is supersonic, and a description of possible singularities that may arise at the boundary where the flow changes from supersonic to subsonic. To complete the analysis, a theory of free-boundary problems for the positions of transonic shocks is needed. Preliminary results in this vein are encouraging, and the PI, plans to continue research in this direction. Tools of study include analysis of free-boundary problems for which a degenerate elliptic equation of novel type has to be solved, and asymptotic analysis of singularities. When completed, the results of this project will contribute to solving open problems in the field of multi-dimensional wave interactions and shock reflection problems, which include an existence theorem for oblique shock reflection by a ramp, bifurcation criteria for two-dimensional elementary waves, and the correct function space for a general existence theory that will capture possible singularities in the solution. In ONE-DIMENSIONAL conservation laws several novel phenomena, recently discovered by the PI, call for further exploration and understanding. They include nonexistence of classical weak self-similar Riemann solutions and the presence of bounded amplitude, high frequency, oscillatory solutions replacing the classical ones. Using compensated compactness methods these solutions were shown to satisfy the system of conservation laws in a measure-valued sense. Understanding what causes nonexistence, and what is the physical meaning of the oscillations, is one of the goals of  this research project.<br/><br/>Conservation laws are mathematical equations that describe processes central to<br/>technology, such as high-speed flows, supersonic jets, as well as flows through<br/>porous media which arise in environmental engineering and reservoir simulation.<br/>Understanding the structure of solutions of conservation laws is crucial for a<br/>successful simulation of these phenomena. The main difficulty lies in the fact<br/>that solutions to conservation laws admit ``shock waves'', which correspond to<br/>the sudden, abrupt changes in the flow properties. Shock waves influence the<br/>stability of high-speed flows, and are crucial in the simulation of oil recovery. In addition, based on the recent PI's findings, multi-dimensional conservation laws admit ``singularities'' in the solutions that have not yet been theoretically understood and which are difficult to resolve numerically. Their presence influences the outcome of the simulations. This project proposes an original approach towards understanding these singularities. Even more generally, in this project the PI proposes an organized approach towards the development of a general theory that would describe the structure of solutions of multi-dimensional conservation laws such as the equations that describe high-speed flows. Because the methods used to study one-dimensional conservation laws cannot be generalized to more than one space dimension, this research area is wide open. However, since almost all processes in technology are multi-dimensional, the development of such a theory is an imperative. In addition to the research aspects of this proposal, aspects of the work involving nonlinear conservation laws have been used successfully by the investigator in graduate student education, and continuation of this effort in similar directions is proposed.<br/><br/><br/>"
"9973318","Computational Methods for the Direct Simulation of Particulate Flow of Newtonian and Non-Newtonian Incompressible Viscous Fluids","DMS","COMPUTATIONAL MATHEMATICS","08/01/1999","07/27/1999","Roland Glowinski","TX","University of Houston","Standard Grant","Michael Steuerwalt","07/31/2003","$171,000.00","B. Montgomery Pettitt, Edward Dean, Tsorng-whay Pan","roland@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271","0000, 9263, OTHR","$0.00","9973318<br/><br/>The main goal of this project is to address the numerical solution of the Navier-Stokes equations modeling the flow of incompressible Newtonian and non-Newtonian viscous fluids and to combine the resulting methodology with appropriate domain embedding methods to investigate the simulation of particulate incompressible viscous fluid flows, e.g., flow of mixtures of incompressible fluids and solid particles moving, typically, under the effect of gravity and hydrodynamical forces.  The numerical methodology will rely on the following basic basic ingredients:<br/><br/>(1) Time discretization by operator splitting,<br/>(2) Treatment of advection by the method of characteristics or reduction to a second order wave-like equation,<br/>(3) Space approximation by finite elements,<br/>(4) Use of a distributed Lagrange multiplier/fictitious (embedding) domain methodology to localize the solid particles and force their rigid body motion.<br/><br/>Typical non-Newtonian fluids to be investigated include Maxwell and <br/>Oldroyd B.<br/><br/>Our goal in this project is to develop efficient methods, able to achieve the numerical simulation of three-dimensional flow of mixtures of incompressible viscous fluids with several hundreds to thousands of solid particles.  Such large scale simulations have the characteristics of a Grand Challenge in Scientific Computing and success will rely on advanced computational methods implemented on powerful parallel computers.  With these simulation tools the scientific community will be able to better understand  the complicated mechanisms taking place for example in chemical reactors and in fractured oil reservoirs.  Among the consequences of these investigations we can expect for example improved manufacturing techniques for rubber materials and pharmaceutical drugs, and also improved efficiency for drilling techniques used in oil Industry.<br/>"
"9972210","Evolution of Obliquely Interacting Wavetrains in Deep Water","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, PHYSICAL OCEANOGRAPHY","09/01/1999","06/19/2001","Diane Henderson","PA","Pennsylvania State Univ University Park","Continuing Grant","Michael Steuerwalt","08/31/2003","$325,125.00","Joseph Hammack","DMH@MATH.PSU.EDU","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1266, 1271, 1610","0000, 9216, 9263, HPCC, OTHR","$0.00","9972210<br/><br/>The investigators conduct analytical, numerical, and experimental studies of three-dimensional water waves.  They construct a state-of-the-art, precisely controlled, experimental facility that will enable the generation of nonlinear, three-dimensional wavefields and the direct measurement of their surface patterns.  The goal is to use experiments to test and guide development of theoretical models and thereby obtain a unified experimental, theoretical, and numerical understanding of the evolution of nonlinear, three-dimensional surface waves over deep water.  In particular they study (1) the dynamics of two obliquely interacting Stokes wavetrains, and (2) the dynamics of many obliquely interacting Stokes wavetrains that form resonantly interacting quartets.<br/><br/>The investigators are building a precision facility in which to generate three-dimensional water waves with any desired shape, speed and complexity.  One reason for these experiments is that an understanding of the fundamental physics of such waves greatly enhances the ability to model ocean waves.  Models of ocean waves are necessary for a variety of applications, including shipping, construction, and defense.  Models presently in use rest upon some mathematical assumptions that have never been tested and are possibly incorrect.  The experiments of this project are designed to test these assumptions.  A second reason for these experiments is that water waves are a physical application of the solutions to a special class of partial differential equations.  These equations arise in many fields: oceanography, meteorology, plasma physics, and optics.  Water waves provide a manageable experiment in which to study the application of the mathematics and to test the underlying assumptions and regimes of validity.  If these equations model well the water wave experiments, then investigators in many fields have more confidence in applying them to other fields and in developing more such models.  The project is supported by the Applied Mathematics and Computational Mathematics programs in the Division of Mathematical Sciences and by the Physical Oceanography program in the Division of Ocean Sciences.<br/>"
"9976754","Workshop on Discrete Optimization '99","DMS","COMPUTATIONAL MATHEMATICS, OPERATIONS RESEARCH","05/01/1999","04/16/1999","Peter Hammer","NJ","Rutgers University New Brunswick","Standard Grant","Jong-Shi Pang","04/30/2000","$15,000.00","Endre Boros","hammer@rutcor.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1271, 5514","0000, 9263, OTHR","$0.00","9976754<br/><br/>Discrete optimization is one of the most important areas of operations research, and deals with the methodology of finding  optimal solutions to problems involving variables which must take integer (i.e. non-fractional) values.  Its applications occur in  numerous areas of science, technology, management, etc., and include problems as diverse as those of determining the optimal locations of warehouses, VLSI chip design, airline crew scheduling, etc.  The main goal of DO'99 is to provide an overview of the current state of the art in the field of discrete optimization.  For this purpose, 23 of the most prominent researchers active in this field will provide state of the art presentations of various subareas of this discipline.  The current project's objective is to make it possible for a larger number of junior researchers, post-doctoral fellows and graduate students to participate in the meeting and attend the lectures of these world-renowned experts.  At the same time, these participants will also have the opportunity to present papers about their own research, and to establish contacts both with the senior participants and with their peers.  The funds made available through this project will be used entirely to provide stipends for participants. <br/><br/>"
"9974289","Kinetic Theory Method for Large Eddy Simulation of Turbulence","DMS","COMPUTATIONAL MATHEMATICS","09/01/1999","08/23/1999","Steven Orszag","CT","Yale University","Standard Grant","Michael Steuerwalt","07/31/2003","$180,000.00","","steven.orszag@yale.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1271","0000, 9263, OTHR","$0.00","9974289<br/><br/>In this work, we plan to explore and apply a fundamentally new approach to large-eddy simulation (LES) of turbulence, viz. the use of a Boltzmann kinetic level representation of the flow.  Conventional approaches to LES are based on solving a system of modified Navier-Stokes equations at the continuum level.  While such an approach has achieved many interesting results, it also encounters intrinsic difficulties (e.g. in providing faithful turbulent behavior at boundaries).  Owing to recent successes with the lattice Boltzmann method, we believe there is significant potential in approaching the problem from a new angle.  That is, we propose the use of the Boltzmann equation representation of fluid dynamics. Such a representation is not only suitable for describing particle motions, but may also be a useful way of studying large-scale fluid turbulence properties once some appropriate averaging procedure is constructed.  In this work, we plan to carry out an extensive series of theoretical and computational studies of turbulence dynamics by formulating such a Boltzmann description.  The specific tasks include model formulation, simulation results, memory and history effects, reformulation of wall models and boundary conditions, and initial applications to multiphase turbulent flows.<br/><br/>The work to be done here may have long-lasting impact on our ability to simulate fluid flows in real-world systems.  The huge range of eddy sizes in turbulence precludes direct solution of the flow equations at all scales for full-scale systems.  The novelty of the lattice Boltzmann method is that it circumvents some of the intrinsic limitations of conventional computers in order to achieve realistic flow simulations.  By coupling of this method to advanced ideas on large-eddy simulation, we expect to achieve progress that would not otherwise be possible on these difficult flow problems.  In particular, the lattice Boltzmann method for large-eddy simulation allows the efficient formulation of near-wall boundary conditions that have so far escaped analysis.  It is also possible that the new formulations to be established here will suggest new computer architectures that will enable full-scale flow simulations.<br/>"
"9973231","Advanced Computational Stochastic Dynamic Programming for Continuous Time Problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/1999","08/24/1999","Floyd Hanson","IL","University of Illinois at Chicago","Standard Grant","Michael Steuerwalt","08/31/2002","$189,050.00","","hanson@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1271","9189, 9216, 9263, EGCH, HPCC","$0.00"," The investigator and associates develop large scale<br/>computational procedures for optimal stochastic control problems.<br/>Massively parallel processor and parallel algorithm advances are<br/>essential for managing massive computational and memory demands,<br/>with particular emphasis on large scale applications in ground<br/>water pollution remediation and manufacturing systems.<br/>Implementation of advanced computational techniques, such as<br/>parallelization, graphical visualization, and efficient data<br/>structures make possible the solution of larger dimensional<br/>problems.  The emphasis here is on formally justified and useful<br/>computational methods rather than limited highly rigorous<br/>results.  The objective is to develop fast algorithms coupled<br/>with massively parallel processing for the optimal feedback<br/>control of general continuous-time nonlinear stochastic dynamical<br/>systems.  These stochastic systems include both Gaussian and<br/>distributed Poisson white noise, thus hybrid stochastic models.<br/>Advanced computational treatment of these systems, especially<br/>with Poisson noise for modeling disastrous events, is a<br/>particularly unique feature of this research.  The computational<br/>methods are tested on ground water pollution models and<br/>multistage manufacturing systems, but are applicable to a wide<br/>variety of applications, yielding a higher level of robustness<br/>for the computational methods.  The numerical approach directly<br/>treats the partial differential equation of stochastic dynamic<br/>programming.  New algorithms, including finite element and random<br/>simulations, are developed to alleviate both memory and<br/>computationally intensive demands from the ""curse of<br/>dimensionality.""<br/>     The investigator and associates develop optimal<br/>computational solutions to ground water pollution and<br/>manufacturing problems perturbed by uncertainty.  Ground water<br/>and manufacturing problems are examples of grand and national<br/>challenge problems, respectively.  In the case of ground water<br/>the cause of uncertainty is the unexpected introduction of<br/>pollution and varying environmental parameters, while in the case<br/>of the manufacturing systems it is the failure and repair of<br/>machines with fluctuating manufacturing parameters.  The reason<br/>for seeking optimal solutions is to minimize costs.  Large scale<br/>computations are necessary due to the complexity of the governing<br/>physical equations and essential numerical approximations.  Since<br/>the cleanup of polluted ground water sites can cost billions<br/>where feasible, optimal solutions could save millions over<br/>nonoptimal methods.  Similar savings can be obtained for<br/>just-in-time manufacturing systems that are managed to minimize<br/>costs, enhancing our globally competitive capabilities.  Advanced<br/>computing techniques are developed to get optimal solutions using<br/>high performance computation with massively parallel computers.<br/>Graphical visualization is important to make the immense output<br/>usable to the environmental resource or manufacturing plant<br/>manager who would be the prime user."
"9975354","Computation in Nonlinear Filtering","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","07/15/1999","07/15/1999","Stephen S. Yau","IL","University of Illinois at Chicago","Standard Grant","Thomas W. Fogwell","06/30/2002","$155,000.00","","yau@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1266, 1271","0000, 9263, OTHR","$0.00","9973247<br/><br/>We suggest problems in computational and applied linear algebra that come under three headings. (i) Under the first heading come two problems on parallel computations.  The first concerns a divide and conquer approach to the problem of computing the mean first passage matrix for Markov chains, work that was started in the case when the process is a random walk with an underlying cutpoint graph structure, e.g., a tree.  The second problem is to investigate the speed-up due to the addition of processors, when the number of blocks (natural partitions) is kept fixed, in a model of asynchronous iterations.  One factor here is the type of coefficient matrix to which the model is applied.  <br/>(ii) Under the second heading  comes a problem encountered when working on<br/>convergence of infinite products of matrices.  It is known that in iterative methods for computing a solution to singular systems, e.g., in computing the stationary distribution for a Markov process, the magnitude of the subdominant eigenvalue determines the asymptotic rate of convergence.  We have noticed that for random stochastic matrices the subdominant eigenvalue seems to decay exactly as the square root of n.  We suggest investigating this behavior.  (iii) Here we suggest two problems on approximating the algebraic connectivity of a graph which is the second smallest eigenvalue of its Laplacian matrix.  Estimates of this quantity are used in various graph-based algorithms such as the spectral separator method.  In the first problem we suggest investigating bounds derived from the generalized group inverse of the Laplacian which seems to yield good lower bounds. In the second problem we propose studying the algebraic connectivity of random graphs on n vertices.  Throughout the proposal we give examples from the literature for the applications of the problems set forth.<br/><br/>Because many practical problems are very intricate, we need to build models to represent them and then develop a way of understanding and handling the models.  Such modeling frequently results in large arrays of numbers or, what is often associated with the arrays, a system with many equations and many unknowns.  The numbers in the array can be of a random nature if the model represents a physical situation (scientific, economic, social or statistical) in which there is some randomness involved. If the model represents a more deterministic physical situation, there will be definite numbers with some relations between them.  Once we have a model,  there may be many parameters and features which we need to measure and compute. This proposal suggests seven problems.  Two of the problems concern certain high-performance computing with these models.  The computations are to be performed in parallel so as to achieve speed-up of the computation and, if possible, a reduction in the number of computations.  Two examples of questions are: If a problem partitions naturally into a number of subproblems which are called blocks, and if we begin to increase beyond the number of blocks, the number of computers/processors applied to solve in parallel the entire problem, do we necessarily continue to increase the speed of calculations, or is a point of saturation reached after a while and what do we do then?  A common situation in which such a problem arises is in data-fitting, sometimes known as the least-squares problem.  Another problem for computation in parallel is in physical systems which have states with a probability of going from one state to another.  An example is various types of population concentrations such as urban, suburban, rural, etc. with a probability of moving from one type of region to another.  We may be interested in the distribution of the population in the different conurbations after a short term.  It turns out that such predictions  may be done in parallel under certain underlying assumptions.<br/>"
"9972490","Preconditioning Techniques for Algebraic Equations Arising from Partial Differential Equations","DMS","COMPUTATIONAL MATHEMATICS","07/15/1999","01/08/2002","Howard Elman","MD","University of Maryland, College Park","Standard Grant","Michael Steuerwalt","06/30/2003","$130,000.00","","elman@cs.umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1271","0000, 9263, OTHR","$0.00","Elman<br/><br/>This project concerns the development, analysis and testing of techniques<br/>of numerical linear and nonlinear algebra for solving systems of equations<br/>arising from discretization of a collection of partial differential equations.<br/>The emphasis will be on preconditioning strategies and their combination <br/>with Krylov subspace iterative methods for linear and nonlinear systems.<br/>These ideas will be applied to several examples of differential equations,<br/>primarily coming from models of fluid dynamics.  Our focus will be on two <br/>types of problems, the saddle point problems produced by direct discretization <br/>of the primitive formulation of the incompressible Navier-Stokes equations, <br/>and the convection-diffusion equation.  These have been chosen because they <br/>are used in numerous engineering models of fluid flow, and because they contain<br/>challenging mathematical and computational qualities that make them difficult <br/>to solve.  In particular, accurate discretization in two and three dimensions <br/>leads to large sparse systems of algebraic equations that are nonsymmetric,<br/>and for the first problem, nonlinear and indefinite.  In addition, they have <br/>associated with them fundamental parameters such as Reynolds numbers and <br/>discretization mesh widths that, for the solution of problems of practical <br/>interest, cause many algorithms to converge slowly and require large-scale <br/>systems to attain accuracy.  Our goals are to extend and analyze certain <br/>preconditioning techniques designed for saddle point problems.  Plans include <br/>demonstration of the effectiveness of these preconditioners for general mixed <br/>finite element discretizations, development of rigorous mathematical techniques<br/>for analyzing convergence, demonstration of their usefulness in realistic <br/>settings involving general meshes and domains, and incorporation of fast<br/>algorithms for the convection-diffusion equation into the solution process.<br/><br/>The purpose of developing computational algorithms is to enable the <br/>efficient numerical solution of differential equations used in mathematical <br/>modelling.  The use of such models is becoming an increasingly important<br/>component in manufacturing and design (for example, of aerospace vehicles,<br/>automobiles, cooling devices for nuclear devices) and in enhancing our<br/>understanding of critical natural phenomena (for example, blood flows and<br/>dispersal of environmental pollutants).  Understanding these phenomena<br/>through purely experimental techniques is prohibitively expensive or<br/>impossible, whereas the use of mathematical models introduces a basic <br/>understanding of the physics by providing approximations to quantities<br/>such as flow rates and pressures.  This leads to the identification of <br/>promising design features, such as wing shape in airplane manufacturing.  <br/>(For example, many of the design decisions for the Boeing 777 jet were made <br/>using computational studies.)  Accurate solution of the mathematical models <br/>is only feasible, however, if reliable and fast solution algorithms are <br/>available.  The goal of this project is to develop such algorithms.<br/><br/>"
"9971164","Studies in Numerical Solution of Ordinary Differential Equations","DMS","COMPUTATIONAL MATHEMATICS","08/01/1999","03/06/2003","Zdzislaw Jackiewicz","AZ","Arizona State University","Standard Grant","Michael Steuerwalt","07/31/2003","$105,000.00","","jackiewicz@asu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1271","0000, 9263, OTHR","$0.00","<br/>Jackiewicz<br/><br/>It is the purpose of this project to study various acceleration <br/>techniques for continuous and discrete waveform relaxation<br/>iterations. These techniques include exponential, polynomial or<br/>Toeplitz preconditioning and/or overlapping components of the <br/>system. We will also study the construction and implementation <br/>of novel numerical methods for ordinary differential equations<br/>which are special cases of general linear methods. Such methods <br/>are appropriate for nonstiff or stiff differential systems in a<br/>sequential or parallel computing environment. These methods have<br/>also higher potential to preserve qualitative properties of the <br/>solution than classical methods. This potential will be exploited<br/>to integrate wave propagation problems with absorbing boundary<br/>conditions.<br/><br/>The research supported by this proposal will lead to the development<br/>of modern software for the solution of many problems in science and<br/>engineering which are governed by large differential systems. This<br/>software will be based on novel numerical methods developed in the<br/>past few years by the principal investigator and his coworkers and <br/>will utilize modern computer architecture by splitting the underlying<br/>large problem into smaller subsystems or subproblems which can then be <br/>solved in parallel by allocating them to different processors. <br/>Numerical experiments on many problems, for example, acoustic wave<br/>propagation, chemical reactions, movements of a plate under the load<br/>of a car passing through it, catastrophe model for the nerve impulse<br/>mechanism, model that describes the dynamics of a system with large<br/>number of particles which can form clusters, and reaction-diffusion<br/>systems, indicate that these novel methods are more efficient and <br/>robust and better suited to take advantage of the state-of-the-art<br/>computer architecture than the methods which are currently in use for<br/>these problems.<br/>"
"9973308","Collaborative Research:  Large-Scale Optimization:  Matrix-free Algorithms, Data Parallelism, and Applications in Seismic Inversion","DMS","COMPUTATIONAL MATHEMATICS","09/01/1999","06/15/2001","Mark Gockenbach","MI","Michigan Technological University","Continuing Grant","Michael Steuerwalt","08/31/2002","$82,908.00","","msgocken@udel.edu","1400 TOWNSEND DR","HOUGHTON","MI","499311200","9064871885","MPS","1271","9216, 9263, HPCC","$0.00","     In this collaborative interdisciplinary project, the<br/>investigators Mark Gockenbach, Anthony Kearsley, and William<br/>Symes develop and implement algorithms for large-scale<br/>optimization problems that arise in a variety of applications,<br/>focusing on techniques particularly suited to parallel<br/>architectures, and apply the methods to the seismic velocity<br/>estimation problem.  Large-scale optimization problems often<br/>present difficulties to standard algorithms and software.  Many<br/>of these difficulties arise in the seismic velocity estimation<br/>problem.  First, the sheer data volume makes it impractical to<br/>explicitly form and factor matrices, as required by many standard<br/>optimization algorithms.  To address this, a new matrix-free<br/>Sequential Quadratic Programming (SQP) algorithm is developed,<br/>based on recent advances in matrix-free algorithms for the<br/>so-called trust region subproblem.  Second, the data structures<br/>and interfaces required for seismic data processing are not<br/>easily adapted to those required by ""off-the-shelf"" optimization<br/>software.  The Hilbert Class Library (HCL), an object-oriented<br/>optimization package, can solve optimization problems involving<br/>data structures and interfaces of arbitrary complexity.  The SQP<br/>algorithm, along with the necessary seismic data structures and<br/>simulators, is implemented in HCL.  Third, large-scale<br/>simulations often require the use of parallel computation.  The<br/>use of parallelism in simulation and optimization is addressed<br/>through the development of HCL classes that automatically<br/>distribute data over a network of distributed workstations.  The<br/>above innovations in optimization methods and software are used<br/>to study in detail a new formulation of the seismic inverse<br/>problem.  The investigators have recently introduced this<br/>formulation in order to overcome certain optimization-theoretic<br/>difficulties inherent in standard formulations.<br/>     Optimization problems arise in science and engineering,<br/>where one often wishes to find the best design, the best<br/>mathematical model, the best strategy, and so forth.  Large-scale<br/>optimization problems, which involve many variables, present<br/>special challenges, including the choice of algorithm, the<br/>representation of data, and the interface between optimization<br/>software and programs written by the application scientist.  This<br/>project addresses these challenges in the context of an important<br/>application, seismic exploration.  The investigator and his<br/>colleagues develop new optimization algorithms to identify<br/>geological features of the subsurface of the earth.  Included is<br/>a general method for solving large-scale optimization problems;<br/>this algorithm is applicable to other science and engineering<br/>problems.  Moreover, they also develop an innovative software<br/>package, called the Hilbert Class Library (HCL), that allows<br/>optimization algorithms to be used with problems of arbitrary<br/>complexity; the software adapts to different data structures and<br/>software interfaces.  Finally, they extend HCL to automatically<br/>take advantage of parallel computers, making it easier to take<br/>advantage of high performance hardware.  The seismic exploration<br/>problem is important to the petroleum industry; a precise<br/>knowledge of geological structures is essential for efficient<br/>utilization of petroleum reserves.  In addition, the HCL software<br/>addresses the important issue of technology transfer as it<br/>pertains to numerical algorithms; too often algorithmic advances<br/>are unavailable to application scientists because optimization<br/>software and application software have incompatible interfaces."
"9981458","Collaborative Research:  Non-Linear Population Dynamics:    Mathematical Models, Biological Experiments and Data        Analysis","DMS","POPULATION DYNAMICS, POP & COMMUNITY ECOL PROG, APPLIED MATHEMATICS, STATISTICS, COMPUTATIONAL MATHEMATICS","09/01/1999","07/05/2001","Brian Dennis","ID","Regents of the University of Idaho","Continuing Grant","Michael Steuerwalt","08/31/2002","$84,000.00","","brian@uidaho.edu","875 PERIMETER DR MS 3020","MOSCOW","ID","838449803","2088856651","MPS","1174, 1182, 1266, 1269, 1271","9169, 9263, EGCH","$0.00","Dennis<br/>9981458<br/>     Understanding the fluctuations in animal numbers is a<br/>central issue in population biology that has far-reaching impact<br/>on and implications for problems ranging from food production to<br/>the conservation of species.  Nonlinear dynamics opens the way to<br/>a new phase of population research in which theory and<br/>experimentation focus on phenomena such as cycles and<br/>quasi-periodicity, chaos and strange attractors, multiple<br/>attractors and complicated basins of attraction, saddle sets and<br/>their stable manifolds, and so on.  This interdisciplinary<br/>project, in which Robert Costantino, Jim Cushing, Brian Dennis,<br/>Robert Desharnais, and Shandelle Henson collaborate, covers a<br/>spectrum of activities essential to testing nonlinear population<br/>theory: the translation of the biology into the language of<br/>mathematics and back again, the analysis of deterministic and<br/>stochastic models, the development and application of statistical<br/>techniques for the analysis of data, and the design and<br/>implementation of biological experiments.  A series of experiments<br/>with flour beetles of the genus Tribolium provide rigorous<br/>experimental tests of nonlinear phenomena.  Topics include<br/>nonlinearity in the context of stochasticity, chaos and<br/>population control, the impact of periodic environments on animal<br/>abundance, demographic dynamics and natural selection,<br/>nonequilibrium species interactions, and statistical questions<br/>concerning parameterization and validation of models.<br/>     With a sound understanding of the underlying dynamics of<br/>animal populations, ecologists can anticipate the consequences of<br/>environmental degradation and learn better ways to manage natural<br/>populations and control populations of pests.  For example, one of<br/>the experiments suggests that, by taking advantage of the<br/>""sensitivity to initial conditions"" that is the hallmark of a<br/>chaotic system, managers can dramatically decrease pest<br/>population numbers without the use of chemical pesticides by<br/>making small perturbations at critical times.  The marriage of<br/>ecological theory and experiments by the interdisciplinary<br/>research team leads to new techniques for the application of<br/>mathematics to ecological problems.  The project is supported by<br/>the Applied Mathematics, Computational Mathematics, and<br/>Statistics programs in MPS and the Population Biology and Ecology<br/>programs in BIO.<br/>"
"9974389","Modeling Host-Parasite Systems","DMS","POP & COMMUNITY ECOL PROG, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, AMERICAS PROGRAM","08/15/1999","08/06/1999","Zhilan Feng","IN","Purdue Research Foundation","Standard Grant","Michael Steuerwalt","07/31/2003","$221,000.00","Fabio Milner, Dennis Minchella","zfeng@math.purdue.edu","1281 WIN HENTSCHEL BLVD","WEST LAFAYETTE","IN","479064182","3174946200","MPS","1182, 1266, 1271, 5977","0000, 5913, 9169, 9263, EGCH, OTHR","$0.00","Feng<br/>9974389<br/>     The investigators and her colleagues study and analyze the<br/>complex interactions between parasitic trematodes and their<br/>invertebrate and vertebrate hosts by developing mathematical<br/>models that allow the prediction of changes in the population<br/>dynamics of both hosts and parasites.  Continuous deterministic<br/>models based on systems of differential equations are proposed<br/>and analyzed using the theory of dynamical systems and integral<br/>and differential equations.  Data gathered for the validation of<br/>the models is fitted using standard statistical techniques.<br/>Validation of the simpler initial models is done by taking the<br/>prevalence values of infection of human hosts at two or more<br/>points in time, and estimating prevalence of infection at other<br/>points in time for which data is available.<br/>     A collaborative study is undertaken that combines the<br/>theoretical and empirical experience of a parasitologist and the<br/>modeling and theoretical expertise of mathematicians to<br/>investigate models of host-parasite interactions.  The scope of<br/>this project is threefold:<br/>     *First, to model host-parasite interactions, with<br/>increasingly complex models motivated by biological<br/>considerations;<br/>     *second, the study of mathematical properties of each model<br/>and their biological and ecological implications such as<br/>population control or potential species extinction;<br/>     *third, the validation of the models through comparison of<br/>model results with field data.  This project significantly<br/>advances our knowledge and understanding of the dynamics of<br/>host-parasite systems, by providing usable models to study<br/>natural systems, by analyzing their sensitivity to the parameters<br/>affecting their dynamics, and by allowing the structuring of the<br/>complexities of the systems into hierarchies.  A specific example<br/>is the determination of the proportion of cases of<br/>schistosomiasis that need to be treated for the disease to be<br/>eradicated, as well as the effect of drug-induced parasite<br/>mutation and the existence of more than one species of definitive<br/>hosts on the dynamics of transmission of this ailment.  Just as<br/>important is the possibility of extending the models developed in<br/>this project to more complex systems involving three or more<br/>species, or stochastic environmental variability.<br/>"
"9973212","Discrete-Time Models for Biological Invasions:  Variability and Multispecies Interactions","DMS","POPULATION DYNAMICS, POP & COMMUNITY ECOL PROG, OFFICE OF MULTIDISCIPLINARY AC, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/1999","08/26/1999","Michael Neubert","MA","Woods Hole Oceanographic Institution","Standard Grant","Michael Steuerwalt","08/31/2003","$380,000.00","","mneubert@whoi.edu","266 WOODS HOLE RD","WOODS HOLE","MA","025431535","5082893542","MPS","1174, 1182, 1253, 1266, 1271","9169, 9263, EGCH","$0.00","Neubert<br/>9973212<br/>     The investigators systematically develop and analyze<br/>mathematical models of biological invasions.  Their aim is to<br/>understand how three key factors (species interactions,<br/>environmental heterogeneity, and demographic stochasticity)<br/>affect the invasion process.  The models take the form of systems<br/>of nonlinear integrodifference equations.  They test their models<br/>with data from a nascent, biologically simple invasion: the<br/>spread of lupine and its caterpillar herbivore into the Pumice<br/>Plains region of Mount St. Helens following its 1980 eruption.<br/>     Invasions by exotic species -- species like zebra mussels,<br/>gypsy moths, and purple loosestrife -- are occurring more<br/>frequently than ever before.  In one recent year, 456 million<br/>exotic plants were imported into the United States.  The<br/>biological and economic consequences of these invasions can be<br/>dramatic; estimates of the damage done to agricultural crops by<br/>exotic species run in the billions of dollars.  A key to<br/>controlling the spread of exotic species is a scientific<br/>understanding of the factors that determine how fast they spread<br/>across the landscape.  One way to obtain this understanding is<br/>through the use of mathematical models of the invasion process.<br/>The investigators construct and analyze invasion models that<br/>incorporate an important biological reality that has often been<br/>missing in other studies: the fact that invaders interact both<br/>with their prey and often with simultaneously introduced<br/>predators and competitors.  They test the predictions of their<br/>models with data from actual biological invasions that occurred<br/>in the wake of the eruption of Mount St. Helens.  The insight<br/>they gain from these studies will help environmental managers<br/>deal more effectively with invading nonnative pest species.<br/>The project is supported by the programs of Applied Mathematics<br/>and of Computational Mathematics and the Office of<br/>Multidisciplinary Activities in MPS and by the programs of<br/>Population Biology and of Ecology in BIO.<br/>"
"9974551","Interfaces for the Twenty-First Century Conference, August 16-18, 1999, Monterey, California","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, PMP-Particul&MultiphaseProcess, FD-Fluid Dynamics","04/15/1999","04/07/1999","Marc Smith","GA","Georgia Tech Research Corporation","Standard Grant","Hans Engler","03/31/2000","$20,000.00","","marc.smith@me.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1266, 1271, 1415, 1443","0000, OTHR","$0.00","The award will support a conference on mathematical models for interfaces in fluid <br/>mechanics and materials processing, to be held on August 16-18, 1999 in Monterey, CA.  <br/>The meeting will bring together leading researchers in interface problems in order to <br/>discuss the current state of the art and to assess the commonality of approaches used <br/>to model interfaces and solve for their evolution.  The participants will include applied <br/>mathematicians, engineers, and scientists from academia, government laboratories, and <br/>industry who are working on these problems. This conference will present a unique and <br/>timely opportunity for these communities to come together in an informal atmosphere to <br/>assess the current state of the art and to identify the important research problems for the future.<br/><br/>Interfaces are free or moving boundaries that occur between two different material phases.<br/>They are found in a wide range of fluid mechanics and materials processing problems in a<br/>variety of different industrial processes and everyday phenomena, ranging from water drops <br/>in a hot pan to coating films on a solid surface. A better understanding and the successful <br/>manipulation of such boundaries could lead to process improvements with wide benefits.<br/>"
"9973108","A Qualitative and Quantitative Study of Self-Adjoint and Non-Self-Adjoint Sturm-Liouville Problems","DMS","COMPUTATIONAL MATHEMATICS","08/15/1999","07/28/1999","Anton Zettl","IL","Northern Illinois University","Standard Grant","Junping Wang","07/31/2003","$130,000.00","Hongyou Wu, Qingkai Kong","zettl@math.niu.edu","1425 W LINCOLN HWY","DEKALB","IL","601152828","8157531581","MPS","1271","0000, 9263, OTHR","$0.00","Zettl<br/><br/>Technical description. In a series of papers over the last five years or so, <br/>the three authors of this proposal, together with a number of collaborators, <br/>embarked on a systematic study of the dependence of the spectrum of self-adjoint <br/>Sturm-Liouville problems on the problem. This project is a natural<br/>continuation of the above work. It is planned to enlarge the class of self-adjoint<br/>problems covered (e.g. by including the so called ""left-definite"" problems) and,<br/>especially, extend the above work to the non-self-adjoint case. The latter case is<br/>formidable: there is no general theory that can be compared with the well developed<br/>theory for the self-adjoint case. A new approach introduced by the authors for<br/>the self-adjoint case, using tools from algebraic geometry, has yielded considerable<br/>insight for this case and is expected to do so also for the non-self-adjoint case.<br/><br/>General description. Sturm-Liouville problems have a celebrated history dating back<br/>to the seminal papers of Sturm and Liouville in 1836-37. Thousands of papers have <br/>been written on these problems by mathematicians and by scientists, and engineers.<br/>Yet the field is still intensely active today with dozens of papers published every year. <br/>These problems have a wide range of applications in pure mathematics, applied <br/>mathematics, quantum mechanics, the sciences and engineering. For example, they <br/>can be used to study the spectrum of the hydrogen atom, eddies in the atmosphere, <br/>neutron transport, etc. etc. The main purpose of this project is to study, both <br/>qualitatively and quantitatively, some major and difficult classes of such problems <br/>about which little is known.<br/>"
"9973126","Collaborative Research:  Nonlinear Population Dynamics:     Mathematical Models, Biological Experiments, and Data       Analyses","DMS","POPULATION DYNAMICS, POP & COMMUNITY ECOL PROG, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/1999","08/13/1999","Jim Cushing","AZ","University of Arizona","Standard Grant","Michael Steuerwalt","08/31/2002","$195,000.00","Shandelle Henson","cushing@math.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1174, 1182, 1266, 1271","9169, 9263, EGCH","$0.00","Cushing<br/>9973126<br/>     Understanding the fluctuations in animal numbers is a<br/>central issue in population biology that has far-reaching impact<br/>on and implications for problems ranging from food production to<br/>the conservation of species.  Nonlinear dynamics opens the way to<br/>a new phase of population research in which theory and<br/>experimentation focus on phenomena such as cycles and<br/>quasi-periodicity, chaos and strange attractors, multiple<br/>attractors and complicated basins of attraction, saddle sets and<br/>their stable manifolds, and so on.  This interdisciplinary<br/>project, in which Robert Costantino, Jim Cushing, Brian Dennis,<br/>Robert Desharnais, and Shandelle Henson collaborate, covers a<br/>spectrum of activities essential to testing nonlinear population<br/>theory: the translation of the biology into the language of<br/>mathematics and back again, the analysis of deterministic and<br/>stochastic models, the development and application of statistical<br/>techniques for the analysis of data, and the design and<br/>implementation of biological experiments.  A series of experiments<br/>with flour beetles of the genus Tribolium provide rigorous<br/>experimental tests of nonlinear phenomena.  Topics include<br/>nonlinearity in the context of stochasticity, chaos and<br/>population control, the impact of periodic environments on animal<br/>abundance, demographic dynamics and natural selection,<br/>nonequilibrium species interactions, and statistical questions<br/>concerning parameterization and validation of models.<br/>     With a sound understanding of the underlying dynamics of<br/>animal populations, ecologists can anticipate the consequences of<br/>environmental degradation and learn better ways to manage natural<br/>populations and control populations of pests.  For example, one of<br/>the experiments suggests that, by taking advantage of the<br/>""sensitivity to initial conditions"" that is the hallmark of a<br/>chaotic system, managers can dramatically decrease pest<br/>population numbers without the use of chemical pesticides by<br/>making small perturbations at critical times.  The marriage of<br/>ecological theory and experiments by the interdisciplinary<br/>research team leads to new techniques for the application of<br/>mathematics to ecological problems.  The project is supported by<br/>the Applied Mathematics, Computational Mathematics, and<br/>Statistics programs in MPS and the Population Biology and Ecology<br/>programs in BIO.<br/>"
"9970086","Mathematical Studies of Excitability in Physiological Systems","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/15/1999","08/03/1999","James Keener","UT","University of Utah","Standard Grant","Michael Steuerwalt","07/31/2003","$266,000.00","","keener@math.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","MPS","1266, 1271","9183, 9263, BIOT","$0.00","Keener<br/>9970086<br/>     The investigator develops and uses mathematical models and<br/>tools to study a variety of problems in physiology organized<br/>around the theme of excitability and wave propagation.  The three<br/>main interconnected thrusts for this work are: 1) to develop<br/>mathematical models that can be used to describe specific<br/>physiological phenomena; 2) to develop analytical techniques,<br/>numerical algorithms, and computational tools to study the model<br/>equations; 3) to use analytical and numerical tools to understand<br/>the behavior of these systems and to thereby better understand<br/>the underlying physiological processes.  The physiological<br/>problems are drawn from four specific areas: 1) cardiology and<br/>the problems of fibrillation onset and defibrillation; 2) calcium<br/>dynamics and excitation-contraction coupling in cardiac cells; 3)<br/>the role of microtubule networks in the transport of chemicals<br/>throughout a cell; 4) the involvement and spatial spread of<br/>bacteria in chemical reactions, such as mine tailings dumps,<br/>water treatment plants, oil spills, etc.<br/>     The common thread running through this work is that chemical<br/>signals underlie motion, and this work is directed toward an<br/>understanding of how chemical signals are generated, controlled,<br/>and how they mediate motion.  Such an understanding is a crucial<br/>first step toward solving several important health-related and<br/>environmentally related problems.  Specifically, an improved<br/>understanding of the heartbeat and how and why it fails could<br/>give new clues as to how to prevent or cure some dangerous<br/>cardiac disorders.  Similarly, an improved understanding of how<br/>bacteria (and other living cells) move and regulate their motion<br/>could lead to improved organic control of toxic chemical wastes.<br/>"
"9973247","Computations and Applications for Stochastic Matrices, Linear Systems, and Graphs","DMS","COMPUTATIONAL MATHEMATICS","08/01/1999","04/01/2003","Michael Neumann","CT","University of Connecticut","Standard Grant","Junping Wang","01/31/2003","$63,000.00","","neumann@math.uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1271","0000, 9263, OTHR","$0.00","9973247<br/><br/>We suggest problems in computational and applied linear algebra that come under three headings. (i) Under the first heading come two problems on parallel computations.  The first concerns a divide and conquer approach to the problem of computing the mean first passage matrix for Markov chains, work that was started in the case when the process is a random walk with an underlying cutpoint graph structure, e.g., a tree.  The second problem is to investigate the speed-up due to the addition of processors, when the number of blocks (natural partitions) is kept fixed, in a model of asynchronous iterations.  One factor here is the type of coefficient matrix to which the model is applied.  <br/>(ii) Under the second heading  comes a problem encountered when working on<br/>convergence of infinite products of matrices.  It is known that in iterative methods for computing a solution to singular systems, e.g., in computing the stationary distribution for a Markov process, the magnitude of the subdominant eigenvalue determines the asymptotic rate of convergence.  We have noticed that for random stochastic matrices the subdominant eigenvalue seems to decay exactly as the square root of n.  We suggest investigating this behavior.  (iii) Here we suggest two problems on approximating the algebraic connectivity of a graph which is the second smallest eigenvalue of its Laplacian matrix.  Estimates of this quantity are used in various graph-based algorithms such as the spectral separator method.  In the first problem we suggest investigating bounds derived from the generalized group inverse of the Laplacian which seems to yield good lower bounds. In the second problem we propose studying the algebraic connectivity of random graphs on n vertices.  Throughout the proposal we give examples from the literature for the applications of the problems set forth.<br/><br/>Because many practical problems are very intricate, we need to build models to represent them and then develop a way of understanding and handling the models.  Such modeling frequently results in large arrays of numbers or, what is often associated with the arrays, a system with many equations and many unknowns.  The numbers in the array can be of a random nature if the model represents a physical situation (scientific, economic, social or statistical) in which there is some randomness involved. If the model represents a more deterministic physical situation, there will be definite numbers with some relations between them.  Once we have a model,  there may be many parameters and features which we need to measure and compute. This proposal suggests seven problems.  Two of the problems concern certain high-performance computing with these models.  The computations are to be performed in parallel so as to achieve speed-up of the computation and, if possible, a reduction in the number of computations.  Two examples of questions are: If a problem partitions naturally into a number of subproblems which are called blocks, and if we begin to increase beyond the number of blocks, the number of computers/processors applied to solve in parallel the entire problem, do we necessarily continue to increase the speed of calculations, or is a point of saturation reached after a while and what do we do then?  A common situation in which such a problem arises is in data-fitting, sometimes known as the least-squares problem.  Another problem for computation in parallel is in physical systems which have states with a probability of going from one state to another.  An example is various types of population concentrations such as urban, suburban, rural, etc. with a probability of moving from one type of region to another.  We may be interested in the distribution of the population in the different conurbations after a short term.  It turns out that such predictions  may be done in parallel under certain underlying assumptions.<br/>"
"9970480","Research in Representation Theory and Automorphic Forms","DMS","COMPUTATIONAL MATHEMATICS, ANALYSIS PROGRAM","08/01/1999","07/23/2001","Nolan Wallach","CA","University of California-San Diego","Continuing Grant","Joe W. Jenkins","07/31/2003","$188,073.00","","nwallach@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271, 1281","0000, OTHR","$0.00","Abstract<br/>Wallach<br/><br/>This project aims to use the methods of analysis, homological algebra and geometry in order to study the structure and applications of representations of real reductive groups. In particular the principal investigator plans to continue his development of what he calls ""transfer"" between real forms of reductive groups over the complex numbers. A new ingredient involves determining explicit multiplicity formulas (analogous to Blattner's formula) for restriction of representations of semisimple  groups to symmetric subgroups (noncompact). The theory also provides a new method for proving the unitarizability of representations that are difficult to analyze analytically. This work uses basic invariant theory and computationally intensive algebraic geometry. For the latter, Matt Clegg and the author will be completing the code for the Groebner array which is now in place. This hardware has also been helpful in solving several problems in combinatorics, representation theory, and quantum computing.<br/><br/>The relatively inexpensive access to fast computational tools has changed the landscape of ""pure mathematics"". In fact, it has blurred the distinction  between pure and applied mathematics. Mathematicians can now do experiments involving massive calculations to help predict the form of theorems to be proved by mathematical reasoning. This proposed research will make use of these tools. It will also work to enhance the usefulness of massive calculations whose output can be mathematical objects that are described by files that are billions of bytes long . Although the basic research aims of this project are outside the scope of short term applications the methodology is expected to lead to practical applications.  For example, methods to help in deciding which aspects of an immense file that describes a complicated object are most important without having the ability to read the full file.<br/>"
"9973276","Scalable Parallel Multilevel Algorithms for the Solution and Optimization of Partial Differential Equations","DMS","COMPUTATIONAL MATHEMATICS","07/15/1999","07/14/1999","Randolph Bank","CA","University of California-San Diego","Standard Grant","Thomas W. Fogwell","06/30/2002","$245,000.00","Michael Holst, Philip Gill","rbank@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271","0000, 9263, OTHR","$0.00","9973276<br/><br/>This proposal focuses on several fundamental computational issues involved in the parallel implicit solution of partial differential equations arising from optimal control and the variational formulation of physical problems.  The goals of the research include, but are not limited to, the following:<br/><br/>    (1) The formulation and analysis of provably efficient, robust and scalable multilevel algorithms for the optimization and solution of systems defined by partial differential equations.<br/><br/>    (2) The development of software embodying the above algorithms and its dissemination within the manufacturing, engineering and scientific community. The proposed software will extend the scope and effectiveness of the existing codes PLTMG, MC, and SNOPT that have been developed by the investigators at UC San Diego.<br/><br/>    (3) The attraction of advanced graduate students into this research area, which generally concerns the analysis and computational study of systems arising in the sciences and engineering.<br/><br/>The investigators have combined expertise in optimization, numerical methods for partial differential equations, and parallel computation.  This research program is intended to provide graduate students with a broad educational experience in an environment in which these different aspects of scientific computation are integrated.  A key aim of the research is the transfer of new results to the manufacturing, engineering and scientific community.  Mathematical software is one of the most effective means of technology transfer.  Conversely, a crucial component of effective algorithm development is close association with engineers and scientists facing real-world problems.  The investigators will continue and broaden their collaboration with researchers in other areas of applied science and technology, including manufacturing, environmental modeling, electrical power optimization, biomolecular modeling, computer graphics, solid mechanics, aerospace and finance.  All these applications have benefited from and contributed to existing algorithms and software.<br/><br/>"
"9875856","CAREER: Adaptive multilevel finite element methods with applications to biomolecules and gravitation","DMS","COMPUTATIONAL MATHEMATICS","08/15/1999","08/05/1999","Michael Holst","CA","University of California-San Diego","Standard Grant","Junping Wang","07/31/2004","$200,000.00","","mholst@math.ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271","1045, 9216, 9263, HPCC","$0.00","Holst<br/>9875856<br/>     The investigator designs and analyzes effective numerical<br/>techniques for solving certain classes of nonlinear elliptic and<br/>parabolic partial differential equations on complicated domains,<br/>arising in mathematical biology and gravitation.  The work<br/>focuses on adaptive multilevel finite element methods, a powerful<br/>class of numerical approximation techniques for dealing with<br/>irregular domain geometries, degenerate coefficients, and<br/>complicated nonlinearities, in an optimal or nearly optimal way.<br/>Specific target applications for this work are the nonlinear<br/>Poisson-Boltzmann equation arising in continuum models of<br/>biomolecules, the Hamiltonian and momentum constraints in the<br/>Einstein equations, and some related elliptic and parabolic<br/>equations that arise in the topological classification of<br/>surfaces in geometry.  While this work has a large theoretical<br/>component, to have an impact in biology and physics requires a<br/>substantial amount of software development, as well as<br/>interdisciplinary collaboration with scientists in the relevant<br/>areas.  The required implementations are developed by the<br/>investigator and his students as class library extensions to his<br/>software package MC (Manifold Code), and the resulting software<br/>is made available to researchers in the biology and physics<br/>communities.  This project is an NSF CAREER grant.<br/>     The computational methods developed in this project have a<br/>direct impact on the modeling capabilities of rational drug<br/>design researchers using structure-based models of biomolecules.<br/>The Poisson-Boltzmann equation is a well-established model for<br/>electrostatic interactions of biomolecules, allowing one to<br/>predict the effectiveness of a candidate drug molecule in<br/>negotiating the onion-like layers of the electrostatic field on<br/>its journey to bind to the active site in a target biomolecule.<br/>For example, through the use of these types of structure-based<br/>numerical models, one can predict the effectiveness of a<br/>candidate HIV proteinase inhibitor in binding to the active site<br/>in the HIV proteinase.  However, the Poisson-Boltzmann equation<br/>presents severe technical difficulties for solution by numerical<br/>methods on computers.  By developing improved multilevel<br/>numerical techniques to reduce the computer time required to<br/>solve the Poisson-Boltzmann equation, and at the same time<br/>producing more accurate solutions through the use of adaptive<br/>finite element methods, the investigator gives drug designers<br/>access to models that produce more accurate results in less time.<br/>In addition, a critical ingredient for the success of this and<br/>related projects is the education and training of the next<br/>generation of computational mathematicians.  Therefore, a fully<br/>integrated involvement in undergraduate and graduate<br/>computational mathematics education is an integral part of the<br/>project.  In particular, the investigator develops two new<br/>project-oriented, interdisciplinary, computational mathematics<br/>courses at UC San Diego, modeled after a successful experimental<br/>course he taught at UC Irvine in 1997-1998.  This project is a<br/>National Science Foundation CAREER grant.  NSF strongly<br/>encourages the early development of academic faculty as both<br/>educators and researchers.  The Faculty Early Career Development<br/>(CAREER) Program is a Foundation-wide program that provides for<br/>the support of junior faculty within the context of their overall<br/>career development.  It combines in a single program the support<br/>of quality research and education in the broadest sense and the<br/>full participation of those traditionally underrepresented in<br/>sciences and engineering.  This program enhances and emphasizes<br/>the importance the Foundation places on the development of full,<br/>balanced academic careers.<br/>"
"9974116","Models for Wave-Bottom Interaction and the Formation of Sand Bars and Sand Ridges","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, SCEC","08/15/1999","08/13/1999","Jerry Bona","TX","University of Texas at Austin","Standard Grant","Michael Steuerwalt","07/31/2001","$90,600.00","","jbona@uic.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1266, 1271, 1571","9196, 9263, EGCH","$0.00","Bona<br/>9974116<br/>     The overall technical objective of the project is to develop<br/>three-dimensional topographical models that describe seabed<br/>dynamics in wave-dominated near-shore zones and shelf regions<br/>where single and multiple sand bars or sand ridges form a central<br/>morphological feature.  The investigator models and analyses the<br/>effects of waves and currents on coastal dynamics.  This is<br/>challenging because of the presence of two free surfaces, the<br/>water surface and the movable bottom, both of which change over<br/>time, and which interact with each other through complex,<br/>nonlinear processes.  The model features nonlinear, dispersive,<br/>dissipative partial differential equations to describe the<br/>evolution of waves coming in to the shore from deep water.  These<br/>equations, which are of Boussinesq type, are coupled to a model<br/>for run-up and reflection at the shoreline.  Once the evolution of<br/>the water surface is approximately known, the velocity field in<br/>the bulk of the fluid may then be determined via potential<br/>theory.  This velocity field is then viewed as driving a viscous,<br/>sediment-laden boundary layer in which the sediment transport is<br/>taking place.  The velocity distribution in this layer is<br/>determined using ideas similar to those pioneered by<br/>Longuet-Higgins.  By applying conservation of sediment mass in an<br/>infinitesimal form, a differential equation is thereby inferred<br/>that serves to determine the bottom deformation.  Thus the entire<br/>model is somewhat complicated, consisting of a nonlinear<br/>initial-boundary-value problem for the wave motion coupled in a<br/>nonlocal way to a conservation law for the bottom movement.  This<br/>model is developed in detail, analysed and implemented as a<br/>computer code.  Model predictions are compared with field data.<br/>It is also proposed to link the computer code with input from a<br/>Geographical Information System, to allow easier deployment of<br/>the model in real situations.<br/>     The underlying motivation for the project is two-fold.<br/>First, there is a desire to understand both qualitatively and<br/>quantitatively some of the fundamental processes that lead to the<br/>formation and maintenance of bottom structure in coastal zones<br/>and on continental shelves.  This involves a thorough knowledge<br/>of both the wave environment and the dynamics of the bed as it is<br/>influenced by waves and by currents.  Second, and not less<br/>important, the project responds to the need for new and more<br/>sophisticated tools to aid in the development of strategies to<br/>deal with real coastal engineering problems.  The project has the<br/>potential to contribute to the study of a number of important<br/>scientific and environmental issues.  The ocean margins in North<br/>America are vast and in many places, troubled.  Indeed, erosion<br/>of unconsolidated coastlines is a world-wide phenomenon that is<br/>not well understood.  Global warming will aggravate these<br/>problems by increasing sea levels, thus causing shore retreat and<br/>providing a better base for the scour and erosion associated with<br/>wave activity.  Fragile Arctic coasts and low-altitude regions<br/>are already displaying unmistakable signs of deterioration caused<br/>by these global changes.  Severe erosion may well spread to other<br/>coast regions within the next half century, and thus there will<br/>be an increased interest in effective prevention methods.  In<br/>addition to ""hard"" protection schemes such as the construction of<br/>seawalls and the like, there will be an increased demand for<br/>""soft"" protection strategies that take advantage of natural<br/>processes.<br/>"
"9973218","Minimization of Piecewise Quadratic Functions and Applications","DMS","COMPUTATIONAL MATHEMATICS","07/01/1999","05/14/2003","Wu Li","VA","Old Dominion University Research Foundation","Standard Grant","Junping Wang","09/30/2003","$71,806.00","","wli@odu.edu","4111 MONARCH WAY","NORFOLK","VA","235082561","7576834293","MPS","1271","0000, 9263, OTHR","$0.00","9973218<br/><br/>The proposer's main objective is to resolve some theoretical and computational issues in developing innovative and efficient numerical methods for solving constrained minimization problems that can be reformulated as unconstrained minimization of piecewise quadratic functions.  More specifically, the proposer's objectives are the following: (i) identifying constrained minimization problems that can be converted to the unconstrained minimization of convex piecewise quadratic functions; (ii) designing numerical algorithms for solving the unconstrained reformulation problem that take advantage of the structure of the original problem; (iii) developing matrix factorization and matrix updating techniques for solving symmetric (but possibly singular) systems of linear equations; and (iv) establishing error bounds that are useful for convergence analysis of numerical algorithms for solving certain minimax problems for convex quadratic functions.  The potential impact of the project is an innovation on how quadratic programming problems should be solved in sequential quadratic programming methods as well as in trust region methods for solving constrained minimization problems.  The theory on new merit functions will provide a completely new perspective on how constraints should be handled in solving constrained minimization problems.  The new penalty function theory also serves as a theoretical basis for developing new methods for solving constrained minimization problems.  New techniques for finding a descent Newton direction when the Hessian of an objective function is only positive semidefinite will significantly advance the theory on Newton methods and allow people to use Newton methods to find a minimizer of a convex piecewise quadratic function in finitely many iterations, even though the set of all minimizers might be unbounded.  New error bounds will provide new ways to establish convergence results for some numerical algorithms.<br/><br/>Quadratic programming is the key for solving constrained minimization problems that have significant applications in many areas such as bio-engineering, chemical engineering, aircraft design, etc.  For example, a faster and more accurate way of solving certain constrained minimization problems means a shorter and more efficient design cycle in developing new aircraft for Boeing.  This project aims at using an innovative approach for solving large-scale quadratic programming problems that can be used to obtain accurate solutions of constrained minimization problems efficiently. This will lead to better software tools for solving many engineering application problems.<br/>"
"9971972","A Proposal for Research in Waves in Random Media and Applications","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/15/1999","08/17/1999","George Papanicolaou","CA","Stanford University","Standard Grant","Michael Steuerwalt","07/31/2003","$294,000.00","","papanico@math.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1266, 1271","0000, 9169, 9263, EGCH, OTHR","$0.00","Papanicolaou<br/>9971972<br/>     The goal of this project is to develop and use analytical<br/>and computational methods for geophysical wave propagation.  The<br/>investigator and his colleagues have developed two general<br/>theories: one based around radiative transport for elastic wave<br/>propagation in the crustal waveguide, and the other based on<br/>waves in nearly layered random media.  The mathematical issues<br/>involved in radiative transfer are (i) the derivation of boundary<br/>conditions (ii) the introduction and use of good Monte Carlo<br/>methods (iii) the solution of realistic inverse problems for<br/>parameter identification.  In the case of nearly layered media an<br/>important mathematical issue is the robustness of wave<br/>localization, known to hold for perfectly layered random media.<br/>Another direction of study is the assessment of the effect of<br/>random inhomogeneities on time reversal imaging, called wave<br/>migration in geophysics.  The mathematical analysis requires time<br/>domain statistics for time reversed pulses, which have to be<br/>constructed by Fourier analysis.  Theoretical explanations are<br/>provided for the increased resolution in time reversal imaging<br/>due to the inhomogeneities.<br/>     Geophysical wave propagation, the propagation of seismic<br/>waves in the earth's crust for example, requires mathematical<br/>modeling at several levels depending on the accuracy and the<br/>resolution required.  It is impossible to simulate numerically<br/>seismic wave propagation over a few hundreds of kilometers of the<br/>earth's crust while taking into account the detailed<br/>inhomogeneities in the earth's crust.  Radiative transport theory<br/>can be used very effectively and at much lower computational<br/>cost, and it can provide estimates for the codas (tails) of<br/>seismograms with enough accuracy to interpret and analyze many<br/>continental seismic events.  The project provides the mathematical<br/>methodology and infrastructure for using radiative transport in<br/>geophysical problems.  The investigator looks carefully at what is<br/>needed to deal with discontinuities and interfaces in the crustal<br/>environment, what is needed for large scale imaging from surface<br/>seismic measurements, what is needed for assessing the waveguide<br/>effects that appear in long distance seismic wave propagation,<br/>and what is needed for the effective numerical simulation of long<br/>distance seismic wave propagation.<br/><br/>"
"9973285","Numerical Approximation of Liquid Crystal Flows","DMS","COMPUTATIONAL MATHEMATICS","08/01/1999","07/22/1999","Noel Walkington","PA","Carnegie-Mellon University","Standard Grant","Michael Steuerwalt","07/31/2003","$165,000.00","","noelw@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1271","0000, 9263, OTHR","$0.00","9973285<br/><br/>Technical Description:<br/><br/>This project centers around the construction and analysis of algorithms for the simulation of the motion of nematic liquid crystals.  Such algorithms will facilitate simulation of the manufacture and operation of devices (such as computer displays) built from liquid crystals.  Equations characterizing the motion of liquid crystals were proposed by Ericksen and Leslie in the 1960's; however, until recently there were very few general results concerning the existence, uniqueness, and stability of these equations.  Recent discoveries by F. Lin and C. Liu of new energy estimates enables many of these fundamental questions to be answered, which, in turn, facilitates the development and analysis of algorithms for the approximation of these equations.  Preliminary results by the author, in collaboration with C. Liu, for a specific instance of the Ericksen--Leslie equations have been very promising, and this proposal outlines a program of research that will result in robust and reliable codes for the simulation of these equations in their full generality.<br/><br/>Non-Technical Description:<br/><br/>Over the past half century computers have stimulated a major change in the design and manufacture of virtually every industrial product produced in the U.S. Computer simulation of product strength, operation, and performance has replaced much of the expensive prototyping and testing phase required before any product is brought to market.  In order to further reduce design and development costs better computer algorithms are required for each step of the manufacturing process, ranging from the simulation of advanced materials to the operation of complex systems.  This project plans to draw upon recent theoretical breakthroughs to predict the behavior of materials like those found in modern liquid crystal computer displays.  These materials exhibit many complex phenomena, and simulation and prediction of their properties requires modern high performance computers.<br/><br/>"
"9873275","KDI:  Multiscale Modeling and Simulation in Scientific Inference:  Hierarchical Methods for Parameter Estimation in Porous Flow","DMS","COMPUTATIONAL MATHEMATICS, MECHANICS, Hydrologic Sciences, KDI OPPORTUNITY FUND","05/01/1999","09/10/1998","John Trangenstein","NC","Duke University","Standard Grant","Michael Steuerwalt","04/30/2003","$2,300,000.00","Zbigniew Kabala, David Higdon","johnt@math.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1271, 1441, 1579, 8876","0000, 1337, 1339, OTHR","$0.00","Trangenstein<br/>9873275<br/><br/>The investigator and his colleagues develop new spatial models and numerical methods for flow in porous media.  Conventional simulation models for these problems require very high-dimensional parameters as inputs, yet the determination of these parameters is radically ill-posed.  Reliable and relevant additional data sources are scarce.  The problem is multi-scaled, since fine-scale variations in the defining parameters can have key large-scale effects.  Hence this project will develop models and numerical methods on a hierarchy of scales.  However, the relationship between different scale sub-models goes beyond simple averaging.  The investigators develop statistical models, methodology and computation for data synthesis and high-dimensional inference and predictions.  Computations for fluid flow incorporate hierarchical simulation involving adaptive mesh refinement and streamtube methods.  The large size of the computations requires distributed computing and the construction of a network of workstations running custom software.  The methods are validated against new analytical work in stochastic partial differential equations.  Finally, the resulting methodology is applied to field studies of contaminant cleanup involving surfactants and to oil production problems.<br/><br/>Uncertainty in porous flow has a large impact on society and the economy.  For example, the cost of discovery and efficient production of petroleum affects the cost of transportation and energy production.  Also the cost of location and removal of contaminants from ground water affects the cost and quality of safe water resources.  One goal of this project is to develop new statistical and computational methods to assess and reduce the uncertainty in modeling flow in porous media.  This goal requires new research into modeling, uncertainty, computation and data measurement.  To address this wide range of issues, this project involves a collaboration of applied mathematicians, statisticians and engineers.  As a result, this project is directed toward the Knowledge and Distributed Intelligence area of interest at NSF, and especially the New Computational Challenges aspect of this initiative.  In order to assign confidence to computer simulations, the statisticians in this project develop concepts, models and methods to integrate data and to incorporate and measure uncertainty.  Together, the statisticians, applied mathematicians and engineers use a variety of sophisticated analytical and numerical techniques to reduce the size and cost of the computations.  The engineers apply the resulting methods to various field studies, and communicate the results to their industrial affiliates.  Even with these new concepts and algorithms, the size of the computations is so large that many computers have to work on the problem simultaneously.  The applied mathematicians build a network of workstations using fast communication boards to perform these large calculations, and dedicate this machine to these calculations and to student instruction.<br/>"
"9973071","A Computational Framework for Multi-Rigid-Body Dynamics with Contact and Friction","DMS","COMPUTATIONAL MATHEMATICS, ROBOTICS","09/01/1999","08/05/1999","Mihai Anitescu","PA","University of Pittsburgh","Standard Grant","Junping Wang","08/31/2002","$75,000.00","","anitescu@mcs.anl.gov","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1271, 6840","0000, 9263, OTHR","$0.00","This research investigates fundamental challenges in creating<br/>multi-rigid-body simulation environments that have a robust and<br/>realistic behavior for a large class of configurations and<br/>interactions. The problem specifically targeted by this proposal is<br/>the difficulty, due to the apparent lack of solutions of the resulting<br/>model, in consistently formulating and efficiently solving the<br/>constrained Newton laws of mechanics when Coulomb friction is<br/>involved.  Recent results, partly developed by the author have shown<br/>that a well-defined numerical approach is possible in an<br/>impulse-velocity complementarity formulation. The proposed research<br/>addresses the need to integrate recent developments in the study of<br/>models of friction with advanced numerical integration and<br/>optimization techniques.  The complementarity formulation will play a<br/>central part in this project but alternative methods will also be<br/>explored. The algorithms developed will accommodate numerical issues<br/>rarely approached in a complementarity setting such as stiffness and<br/>constraint drift control. Several specialized optimization methods<br/>will be designed to address distinguished problems that appear in a<br/>constrained simulation, such as constraint stabilization in presence<br/>of contact degeneracy.<br/><br/>This research will overcome essential difficulties in all areas of<br/>investigation that depend on the correct simulation of the motion of<br/>objects in contact with friction such as robotics, virtual reality,<br/>biomechanics, manufacturing design.  In all these areas, contact with<br/>friction is a fundamental component, whether it is related to the user<br/>pushing or pulling a stack of virtual objects in frictional contact<br/>with one another in virtual reality, to a robot grasping and<br/>manipulating a load, or to the correct simulation of human walking<br/>where friction is necessary to prevent slipping.  Yet in spite of the<br/>omni presence of contact and friction in everyday life its simulation<br/>has proved to be very difficult, partly because of the lack of a<br/>consistent and efficient theoretical and computational framework. It<br/>is the aim of this proposal to extend the recent results in frictional<br/>contact modeling in directions that will answer the requirements of<br/>high-quality, high-performance multi-rigid-body dynamics simulation for use <br/>in robotics, virtual reality, manufacturing design and other fields.<br/>This will enable the development of software that will become <br/>an important component of the manufacturing infrastructure of the future."
"9972622","Numerical Analysis of Large Eddy Simulation","DMS","COMPUTATIONAL MATHEMATICS","08/01/1999","07/16/1999","William Layton","PA","University of Pittsburgh","Standard Grant","Michael Steuerwalt","12/31/2002","$105,000.00","","wjl+@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1271","0000, 9263, OTHR","$0.00","9972622<br/><br/>This proposal is to begin the mathematical development of large eddy simulation. Large eddy simulation (or LES) tries to predict the motion of the large structures in the turbulent flow of a fluid.  LES has been highly developed by the engineering computational fluid dynamics community since its inception in 1970. The present proposal is to provide a mathematical and numerical analytic foundation for the field. In particular, it includes: modeling-derivation of improved space filtered flow models, asymptotics- improved boundary conditions for such models, analysis-rigorous analytical study of the modeling error, numerical analysis-derivation and validation of algorithms for space filtered models, direct simulation- a new approach to LES of direct simulation of large eddies, simulation- computational testing and benchmarking of the models and algorithms under study.<br/><br/>Understanding turbulent flow is central to many important problems including environmental and energy related applications (global change, mixing of fuel and oxidizer in engines and drag reduction), aerodynamics (maneuvering flight of jet aircraft) and biophysical applications (blood flow in the heart, especially the left ventricle).  Turbulent flow is composed of coherent patches of swirling fluid called eddies.  These range in size from large storm systems such as hurricanes to the little swirls of air shed from a butterfly's wings. Large Eddy Simulation (LES for short) seeks to predict the motion of the largest and most important eddies uncoupled from the small eddies.  This uncoupling is important<br/>because the large eddies are resolvable on a computational mesh (a collection of chunks of the physical problem) which can be handled by a supercomputer.  The proposed research centers on modeling the large eddies (such as storm fronts, hurricanes and tornadoes in the atmosphere) in turbulent flow, predicting their motion in computational experiments and validating mathematically the large eddy models and algorithms developed.  Current approaches to LES seem to be presently confronting some barriers to resolution, accuracy and predictability.  It seems likely that many of these barriers can be traced to the mathematical foundation of the models used, the boundary conditions imposed and the algorithms employed for the simulations.  The research undertaken is to develop these mathematical foundations as a guide for practical high performance computation.  This research promises to make it possible to extend the range of accuracy and reliability of predictions important to applications, such as those described above, where technological progress requires confronting turbulence.<br/>"
"9996349","Efficient, High Resolution, Numerical Methods for           Free-boundry Problems with Surface Tension","DMS","COMPUTATIONAL MATHEMATICS","07/01/1999","07/26/1999","Mark Sussman","FL","Florida State University","Standard Grant","John C. Strikwerda","07/31/2001","$26,600.00","Elbridge Puckett","sussman@math.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1271","9148, 9216, 9263, HPCC, MANU","$0.00",""
"9803752","DYNAMIC HETEROGENEITY: Bifurcations, Patterns and a New Mechano-Chemical Instability","DMS","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL MATHEMATICS, Catalysis","08/15/1999","07/03/2001","Philip Holmes","NJ","Princeton University","Continuing Grant","Michael Steuerwalt","07/31/2003","$400,001.00","Yannis Kevrekidis","pholmes@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1253, 1271, 1401","9162, 9216, 9263, AMPP, HPCC","$0.00","9803752<br/><br/>The investigators model and analyze a mechano-chemical instability recently discovered in the study of catalytic chemical reactions on very thin crystals: spatiotemporal variation in the reaction rate coupled with heat release causes spatially varying thermal expansion and buckling of the catalyst surface.  This changes the catalytic properties, closing the loop between reaction, heat release and deformation; it has been observed to lead to sustained mechano-chemical oscillations.  The project builds on experience in studying pattern formation and instability independently in mechanical and in chemically reacting systems.  Theoretical and computational studies of nonlinear buckling of beams under constraints are extended to account for static as well as dynamic variations of the beam properties, including thermally-induced strains.  Earlier computer-assisted studies of pattern formation on catalytic surfaces are used to drive, through the temperature fields thereby generated, mechanical deformations of the catalyst.  Finally, the loop is closed by allowing the deformation to change the surface catalytic properties, thus attempting to reproduce the oscillations observed in experiments.  The modeling, stability analysis and computer-assisted work are performed in constant dialogue with experimental groups in Princeton (on inhomogeneous beams) and in Berlin (on deforming catalytic crystals).<br/><br/>Heterogeneous catalytic reactions constitute the backbone of chemical processes of industrial and environmental relevance:  the CO oxidation reaction the project addresses occurs, for example, in every automotive catalytic converter.  The development of techniques for analyzing in fine spatial and temporal detail what occurs on the catalyst in situ under reaction conditions is revolutionizing the way modeling of these processes is done and enabling the discovery of new phenomena.  This will in turn improve the design of catalytic processes.  Specifically, this project aims at understanding a phenomenon only recently observed in CO oxidation:  oscillations involving mechanical deformations of the catalyst coupled to temperature and chemical concentrations.  Computer-aided analyses of the process lead to better, predictive models, which can be exploited in optimizing both the material properties and the operating conditions of the process.  Funding for the project is provided by the program of Computational Mathematics and the Office of Multidisciplinary Activities in MPS and by the Chemical Reaction Processes program in ENG.<br/>"
"9973427","Adaptive Wavelet Methods for Boundary Integral Equations","DMS","COMPUTATIONAL MATHEMATICS","07/15/1999","07/12/1999","Yuesheng Xu","ND","North Dakota State University Fargo","Standard Grant","Thomas W. Fogwell","11/30/2001","$120,725.00","Charles Micchelli","y1xu@odu.edu","1310 BOLLEY DR","FARGO","ND","581055750","7012318045","MPS","1271","0000, 9263, OTHR","$0.00","9973427<br/><br/>This proposed research aims at developing adaptive fast algorithms using wavelet bases in the Galerkin, Petrov-Galerkin, collocation, product integration and quadrature methods for both linear and nonlinear boundary integral equations whose solutions may have singularities.  We will also consider efficient evaluation of potentials which relates the solution of a boundary integral equation with the solution of the original boundary value problem.<br/><br/>The primary purpose of the proposed research is to understand how wavelet bases can be employed to improve the computational efficiency of the commonly used conventional numerical methods for boundary integral equations while preserving the attractive features of the conventional methods such as high convergence order and stability.  The ultimate goal of this project is to provide computationally efficient algorithms for high-performance computing in science and engineering.<br/>"
"9973310","Collaborative Research:  Large-scale Optimization:  Matrix-free Algorithms, Data Parallelism, and Applications in Seismic Inversion","DMS","COMPUTATIONAL MATHEMATICS","09/01/1999","08/27/1999","Anthony Kearsley","PA","Carnegie-Mellon University","Standard Grant","Junping Wang","08/31/2002","$58,500.00","","anthonyk@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1271","9216, 9263, HPCC","$0.00","     In this collaborative interdisciplinary project, the<br/>investigators Mark Gockenbach, Anthony Kearsley, and William<br/>Symes develop and implement algorithms for large-scale<br/>optimization problems that arise in a variety of applications,<br/>focusing on techniques particularly suited to parallel<br/>architectures, and apply the methods to the seismic velocity<br/>estimation problem.  Large-scale optimization problems often<br/>present difficulties to standard algorithms and software.  Many<br/>of these difficulties arise in the seismic velocity estimation<br/>problem.  First, the sheer data volume makes it impractical to<br/>explicitly form and factor matrices, as required by many standard<br/>optimization algorithms.  To address this, a new matrix-free<br/>Sequential Quadratic Programming (SQP) algorithm is developed,<br/>based on recent advances in matrix-free algorithms for the<br/>so-called trust region subproblem.  Second, the data structures<br/>and interfaces required for seismic data processing are not<br/>easily adapted to those required by ""off-the-shelf"" optimization<br/>software.  The Hilbert Class Library (HCL), an object-oriented<br/>optimization package, can solve optimization problems involving<br/>data structures and interfaces of arbitrary complexity.  The SQP<br/>algorithm, along with the necessary seismic data structures and<br/>simulators, is implemented in HCL.  Third, large-scale<br/>simulations often require the use of parallel computation.  The<br/>use of parallelism in simulation and optimization is addressed<br/>through the development of HCL classes that automatically<br/>distribute data over a network of distributed workstations.  The<br/>above innovations in optimization methods and software are used<br/>to study in detail a new formulation of the seismic inverse<br/>problem.  The investigators have recently introduced this<br/>formulation in order to overcome certain optimization-theoretic<br/>difficulties inherent in standard formulations.<br/>     Optimization problems arise in science and engineering,<br/>where one often wishes to find the best design, the best<br/>mathematical model, the best strategy, and so forth.  Large-scale<br/>optimization problems, which involve many variables, present<br/>special challenges, including the choice of algorithm, the<br/>representation of data, and the interface between optimization<br/>software and programs written by the application scientist.  This<br/>project addresses these challenges in the context of an important<br/>application, seismic exploration.  The investigator and his<br/>colleagues develop new optimization algorithms to identify<br/>geological features of the subsurface of the earth.  Included is<br/>a general method for solving large-scale optimization problems;<br/>this algorithm is applicable to other science and engineering<br/>problems.  Moreover, they also develop an innovative software<br/>package, called the Hilbert Class Library (HCL), that allows<br/>optimization algorithms to be used with problems of arbitrary<br/>complexity; the software adapts to different data structures and<br/>software interfaces.  Finally, they extend HCL to automatically<br/>take advantage of parallel computers, making it easier to take<br/>advantage of high performance hardware.  The seismic exploration<br/>problem is important to the petroleum industry; a precise<br/>knowledge of geological structures is essential for efficient<br/>utilization of petroleum reserves.  In addition, the HCL software<br/>addresses the important issue of technology transfer as it<br/>pertains to numerical algorithms; too often algorithmic advances<br/>are unavailable to application scientists because optimization<br/>software and application software have incompatible interfaces."
"9973032","Efficient Description, Modeling, and Recognition of Natural Imagery via a Local Basis Library","DMS","COMPUTATIONAL MATHEMATICS","08/15/1999","08/24/1999","Naoki Saito","CA","University of California-Davis","Standard Grant","Michael Steuerwalt","07/31/2003","$70,100.00","","saito@math.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1271","9216, 9263, HPCC","$0.00","9973032<br/><br/>The investigator develops adaptive, hierarchical, and computationally efficient algorithms for description, modeling, and recognition of natural imagery by tightly combining computational harmonic analysis techniques and the insight derived from the study of biological vision systems.  The specific study aims are to: 1) Investigate the elementary image features that efficiently characterize a given image class, and develop computationally efficient stochastic image models using these features for simulation; 2) Develop hierarchical algorithms to extract high-level complex features by integrating the elementary features for building better stochastic models; and 3) Investigate the effectiveness of such stochastic models for recognizing and classifying different image classes.  Throughout this study the investigator fully utilizes the so-called local basis library as a basic tool to extract elementary features that are the expansion coefficients (or nonlinear functions of such coefficients) of an image with respect to the bases contained in this library.  This local basis library consists of a collection of the local basis dictionaries (such as wavelet packets, local cosine, local Fourier, and brushlets) whose basis elements are localized in both space and spatial frequency.  Each dictionary in turn contains a huge number of orthogonal or biorthogonal bases, and is equipped with a fast numerical algorithm to select a best basis tailored to a given task and to a given set of images by optimizing task-specific criterion.  In Aim 1, the investigator specifically selects the local features from the library by explicitly reducing the redundancy and statistical dependence among them, and investigate the importance of the sparsity and statistical independence for image representation and description.  Then, he analyzes the effectiveness of such features by constructing simple stochastic models by assuming mutual independence or pairwise dependence among them.  In Aim 2, the investigator integrates local features studied in Aim 1 to extract high-level complex features by applying the localized versions of the Karhunen-Loeve transform directly on those local features, and incorporates such features into the stochastic models.  In Aim 3, the investigator examines the effectiveness of combining the localized versions of the linear discriminant analysis and the models obtained by Aims 1 and 2, for extracting the key discriminant local features.<br/><br/>Results of the project will be useful for those who routinely work on image-based diagnostics such as radiologists and geologists.  Radiologists use very subtle features of mammograms to determine whether the patients have benign or malignant tumors.  Exploration geologists use images of multiple scales ranging from actual rock samples to seismic images to determine whether a specific subsurface layer contains oil or gas.  Although these experts have highly trained eyes and have accumulated large databases of images in their minds through training and experience, their diagnostic rules are often of qualitative nature.  The investigator believes that the above feature extraction tools will help these experts move their diagnostic methods from a qualitative regime to a quantitative one.  In fact, given a specific class of images (e.g., a collection of mammograms representing malignant tumors), the above algorithms may allow these experts to build stochastic models that specify the statistical dependency of critical features of the images and to create new sample images from these models.  This ability to create new simulated images at will is indispensable for assessing the variability of the features and images.  If an expert can give a correct diagnosis for these simulated images, then this implies that the implicit diagnostic rules in their minds are encoded as an explicit stochastic model built on features computed by these feature extraction tools.  This may lead to the development of a distributed digital image diagnostic system, where imaging devices and the diagnostic engines/knowledge bases are remotely located.  One can easily imagine the usefulness of such systems in medicine, geophysical exploration, and other fields requiring image diagnostics.<br/><br/>"
"9973290","Fast Integral Methods for Adaptive Incompressible Flow Simulations","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/15/1999","08/13/1999","Michael Minion","NC","University of North Carolina at Chapel Hill","Standard Grant","Michael Steuerwalt","01/31/2003","$91,000.00","","minion@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1266, 1271","0000, 9263, OTHR","$0.00","The proposed research is dedicated to the development and implementation of integral equation methods for linear equations based on the Fast Multipole Method (FMM) for use with adaptive finite difference methods for incompressible flow.  The scientific objective of this course of research is to produce new methods for the study of phenomena in incompressible flow systems which are not easily approached with existing methods.  For incompressible flow systems, the divergence constraint requires the solution of global elliptic equations which greatly increases the complexity and computational cost of simulation. Many fluid systems also contain a large range of relevant length scales which necessitates the use of spatially and temporally adaptive methods.  When applicable, integral equation methods offer the possibility of efficient, adaptive, high-order methods which are readily applicable to complex computational geometries and are also highly parallel.  Three distinct applications of FMM will be developed and coupled with adaptive fluid solvers:<br/>solution of the Poisson equation, solution of the surface Laplacian,<br/>and the fast summation of impulse potentials, each in the context of a<br/>specific physical problem.  The numerical study of Boussinesq<br/>convection, in which strong localized fronts develop, requires that a<br/>standard Poisson equation be solved.  Simulating front formation of<br/>the quasigeostrophic thermal scalar and internal waves in two fluid<br/>systems requires that a psuedo-differential operator equivalent to a<br/>surface Laplacian be inverted.  A new approach to modeling thin<br/>flexible membranes in incompressible flows requires that membrane<br/>forces be evaluated at all grid points as well as a Poisson equation<br/>be solved to enforce incompressibility.  For all of these physical<br/>problems, adaptive methods offer the opportunity to study problems<br/>which are unapproachable with uniform mesh methods.<br/><br/>The range of important fluid flow applications which the proposed<br/>research could impact is quite diverse.  Examples include modeling the<br/>ocean or climate, predicting oil recovery or contaminant flows in the<br/>ground, simulating combustion or nuclear reactions, and modeling the<br/>flow of blood in organs like the heart or kidneys.  For these types of<br/>applications, the solution of linear equations represents the most<br/>computationally intensive part of the overall computer model.  The use<br/>of integral equation methods to solve the linear equations within<br/>these models represents a distinct change from the majority of current<br/>computer simulations.  Any increase in the efficiency in which these<br/>equations can be solved translates directly into the ability for<br/>scientists to run larger and more accurate models.  The <br/>mathematical and computational techniques necessary to make integral<br/>equation methods an attractive alternative to more standard approaches<br/>has only been fully developed in the last 10 years.  Because of the<br/>complexity and newness of these methods, little work has been done to<br/>exploit their power in applications involving fluid flow.  The<br/>applications that will be pursued as test cases for the new methods<br/>represent problems for which current numerical techniques are<br/>inadequate for answering fundamental questions of interest to<br/>scientists.  The algorithms developed will also serve as a stepping<br/>stone for future methods applicable to more complicated problems."
"9973373","Numerical Methods for Digital Signal Reconstruction","DMS","COMPUTATIONAL MATHEMATICS","08/15/1999","08/23/1999","Thomas Strohmer","CA","University of California-Davis","Standard Grant","Michael Steuerwalt","07/31/2002","$81,000.00","","strohmer@math.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1271","9216, 9263, HPCC","$0.00","     The investigator develops efficient numerical methods for<br/>multi-dimensional signal reconstruction using tools from harmonic<br/>analysis such as local trigonometric transforms and wavelets.<br/>The reconstruction problems that are considered range from<br/>approximation of nonstationary signals from scattered data to<br/>recovery of missing data in digital image sequences.  Successful<br/>solution of these reconstruction problems requires a priori<br/>information about the signal, otherwise these problems are<br/>ill-posed.  Often this a priori knowledge can be derived from<br/>physical properties of the process, for instance in terms of the<br/>""local frequency behavior"" of the signal.  Local trigonometric<br/>transforms and wavelets have become powerful methods for<br/>analysis, compression and denoising of digital signals.  The<br/>investigator derives a mathematical framework for the application<br/>of these concepts to signal and image reconstruction (beyond the<br/>well-traversed area of denoising).  The design of these<br/>reconstruction methods is accompanied by the development of<br/>multi-level regularization techniques in order to provide a<br/>flexible framework for the incorporation of different kinds of a<br/>priori knowledge about the signal.  The investigator applies the<br/>new numerical methods to the approximation of geophysical signals<br/>from scattered data and to error correction in received coded bit<br/>streams.<br/>     This project is motivated by a number of multi-dimensional<br/>signal reconstruction problems arising in areas as diverse as<br/>exploration geophysics, medical imaging, and telecommunication.<br/>In order to optimally compress, transmit or analyze an incomplete<br/>or distorted signal, it is necessary to first reconstruct the<br/>original signal from the available data.  This reconstruction<br/>process includes the restoration of lost data as well as the<br/>recovery of a signal from scattered noisy measurements.  The<br/>mathematical framework being developed here allows the design of<br/>flexible and robust algorithms for the reconstruction of<br/>multi-dimensional signals.  Due to huge amount of data and<br/>possibly required real-time processing the reconstruction<br/>algorithms must also be fast and efficient.  The derived<br/>numerical methods are important for reliable error correction in<br/>data transmission, for efficient reconstruction of geophysical<br/>signals, and for other applications."
"9972147","Innovative Finite Element Methods for Modeling Multiphase Contaminant Flows in Porous Media","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, GLOBAL CHANGE, Hydrologic Sciences","09/01/1999","06/13/2001","Richard Ewing","TX","Texas A&M Engineering Experiment Station","Continuing Grant","Michael Steuerwalt","08/31/2002","$294,000.00","","richard-ewing@tamu.edu","3124 TAMU","COLLEGE STATION","TX","778433124","9798626777","MPS","1266, 1271, 1577, 1579","0000, 1317, 9197, 9216, 9263, EGCH, HPCC, OTHR","$0.00","Ewing<br/>9972147<br/>     Complex physical phenomena involving multiphase contaminant<br/>flows and transport in porous media are often modeled by coupled<br/>systems of nonlinear partial differential equations.  Recent<br/>advances in computational capabilities (particularly with the<br/>advent of new parallel architectures) have greatly expanded the<br/>potential for incorporating more physics into the differential<br/>equations.  While such changes to the model can increase its<br/>ability to represent accurately the underlying real process, this<br/>must be verified in each phase of the modeling process utilizing<br/>physical, mathematical, numerical, and computational concepts.<br/>The investigator and his colleague develop analytical and<br/>numerical methods for the differential equations describing<br/>multiphase contaminant flows and transport in porous media.  The<br/>first objective is a theoretical analysis of these nonlinear<br/>partial differential equations.  Special topics to be treated are<br/>regularity, stability, and bifurcation phenomena.  The second<br/>objective is to study and develop finite element methods capable<br/>of capturing sharp solution fronts, producing accurate fluid<br/>velocities, conserving mass, giving high-order approximations in<br/>time, and being efficiently adapted in local grid refinement.<br/>Here the investigators develop finite element methods for<br/>treating systems of nonlinear transport-diffusion equations via<br/>concepts of Eulerian-Lagrangian localized adjoint methods and<br/>mixed finite element methods coupled with high-order<br/>time-stepping procedures.  The third objective is to develop<br/>efficient parallelizable iterative solution techniques for the<br/>resulting discrete problems.<br/>     This project continues ongoing development of accurate<br/>numerical techniques, efficient computational codes, and<br/>supporting mathematical analysis for modeling of multiphase flows<br/>and transport in groundwater hydrology that has applications in<br/>design of remediation and clean-up technologies.  The development<br/>of improved methods for assessing groundwater contamination by<br/>hazardous wastes has become increasingly important.  Indeed,<br/>groundwater supplies are increasingly threatened by organic,<br/>inorganic, and radioactive contaminants introduced into the<br/>environment by improper disposal or accidental release.  Estimates<br/>of remediation costs at U.S. government sites alone range into<br/>the hundreds of billions of dollars.  Protecting the quality of<br/>groundwater supplies is a grand challenge problem of broad<br/>societal importance.  There are also a number of industrial<br/>applications of the proposed research.  The production and<br/>managment of oil reservoirs are governed by equations and<br/>technologies that are nearly identical to those exploited in<br/>groundwater hydrology.  The manufacture of high quality fiber<br/>reinforced plastic composites for automotive and aerospace<br/>application is controlled by technology similar to that studied<br/>in this project.  Design of semiconductors uses equations very<br/>analogous to those for groundwater transport.  In these areas,<br/>technology transfer can be easily carried out.  In particular,<br/>because of their intellectual content and strong links to<br/>industry, this variety of problems provides an excellent vehicle<br/>for students to learn techniques in applied and computational<br/>mathematics.<br/><br/>"
"9972865","Nonlinear Phenomena in Fluid Dynamics and Related PDE's with Applications to Atmosphere/Ocean Science","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, LARGE-SCALE DYNAMIC METEOROLOG, PHYSICAL OCEANOGRAPHY","09/01/1999","06/09/2003","Andrew Majda","NY","New York University","Continuing Grant","Michael Steuerwalt","08/31/2005","$750,000.00","","jonjon@cims.nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1266, 1271, 1527, 1610","0000, 4444, 9263, OTHR","$0.00","Majda<br/>9972865<br/>     The investigator continues studies of the theory and<br/>applications of partial differential equations to problems in<br/>turbulence and atmosphere/ocean science.  The project emphasizes<br/>problems in atmosphere/ocean science involving four research<br/>areas:<br/>1)  Stratified Mixing, Instability and Turbulence;<br/>2)  Novel Statistical Theories for Open Ocean Convection;<br/>3)  Nonlinear Waves in Tropical Meteorology;<br/>4)  Nonlinear Stochastic Modelling for Geophysical Flows.<br/>The approach to all of these issues involves a sophisticated<br/>combination of asymptotic analysis, numerical computation, and<br/>theoretical mathematical analysis to gain insight into these<br/>complex and important phenomena.  Inherently nonlinear or<br/>statistical methods are emphasized throughout.  The project,<br/>especially in the latter three topics, stresses<br/>multi-disciplinary collaboration and exchange of ideas between<br/>applied mathematicians, atmospheric scientists, and<br/>oceanographers.<br/>     Behaviors of the atmosphere and ocean are complex and not<br/>well understood.  The project involves using novel and<br/>sophisticated mathematical theories and techniques to improve<br/>our understanding of important features of the climate, such<br/>as El Nino, and the poleward transport of heat in the North<br/>Atlantic.  Such an understanding could lead to improved<br/>predictions of weather, climate, and environmental phenomena.<br/>The approach involves multi-disciplinary collaboration and<br/>exchange of ideas between applied mathematicians, atmospheric<br/>scientists, and oceanographers.  The project is supported by the<br/>Applied Mathematics and Computational Mathematics programs in the<br/>Division of Mathematical Sciences, by the Physical Oceanography<br/>program in the Division of Ocean Sciences, and by the Large-Scale<br/>Dynamic Meteorology program in the Division of Atmospheric<br/>Sciences.<br/><br/>"
"9973219","Computational and Applied Linear Algebra:  Asynchronous Parallel Methods, Multiplicative Schwarz and Other Problems","DMS","COMPUTATIONAL MATHEMATICS","07/15/1999","08/10/1999","Daniel Szyld","PA","Temple University","Standard Grant","Jong-Shi Pang","06/30/2001","$58,000.00","","szyld@temple.edu","1801 N BROAD ST","PHILADELPHIA","PA","191226003","2157077547","MPS","1271","0000, 9263, OTHR","$0.00","99732219<br/><br/>Three areas of study in computational and applied mathematics will be studied.<br/><br/>The first deals with asynchronous parallel methods. In these iterative methods the computational effort is distributed among the processors.<br/><br/>The processors proceed with their computation and exchange information only when their own computational step is completed. This is done without waiting for all processors to complete their tasks, that is, without synchronization points. In this way, processor idle time is minimized, and the overall computational time can be reduced.<br/><br/>The second topic relates to the linear algebra representation of multiplicative Schwarz methods for the iterative solution of discretized partial differential equations. A weighted max norm will be used to study the convergence of these methods in several circumstances, including cases when there is no underlying grid, and the case with a ""coarse grid"" correction.<br/><br/>The third problem to be addressed is the comparison of the rate of convergence of two different iterative stationary methods for the solution of singular linear systems of algebraic equations. Several authors have shown that the usual hypothesis in similar comparisons for the nonsingular case do not apply here. A different partial order will be used to try to obtain similar comparison theorems.<br/><br/>Under the High Performance Computing and Communication initiative, computers with several thousands of processors are being designed.  Problems requiring a large computational effort could be solved in these machines by distributing the work among the processors and exchanging information as the computation proceeds. In order to increase the computational efficiency, this exchange of information can be done asynchronously, that is, without waiting for all processors to reach a certain predetermined point. In this project, several aspects of these asynchronous methods will be studied. For example, it is important to know for which kind of problems this approach will be advantageous. One needs to know how to formulate the problems so they conform to the conditions needed for these methods to work.  Asynchronous methods are possibly the kind of methods which will allow the next generation of parallel machines to attain their expected potential.  Another part of the project relates to singular linear systems.  These systems arise in numerous applications, such as queuing models of telecommunication networks. Given two methods to solve the equations representing these models, one wants to know which is faster. Tools to that effect will be developed.<br/>"
"9971813","Nonlinear Dynamics of Two Extended Systems: (i) Nonlinear Dispersive Waves, (ii) Primary Visual Cortex","DMS","COMPUTATIONAL NEUROSCIENCE, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/15/1999","04/25/2003","David McLaughlin","NY","New York University","Continuing Grant","Michael Steuerwalt","08/31/2003","$304,202.00","","david.mclaughlin@nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1162, 1266, 1271","0000, 9125, 9263, BIOT, OTHR","$0.00","McLaughlin<br/>9971813<br/>     The investigator and his colleagues study complex nonlinear<br/>dynamics for two extended systems: (i) Nonlinear Dispersive Waves<br/>and (ii) Primary Visual Cortex.  Project (i) concerns the random<br/>behavior of nonlinear waves -- specifically chaos for<br/>near-integrable soliton partial differential equations.  For these<br/>equations temporal chaos is now rather well understood; however,<br/>far less is known about the more severe behavior of<br/>spatial-temporal chaos.  Both its onset, as well as the<br/>description of its macroscopic temporal evolution, are studied<br/>through a combination of asymptotic averaging and numerical<br/>experiments.  Project (ii) is to develop a theoretical and<br/>computational model of the primary visual cortex (V1).  A point<br/>neuron model of V1 is developed, with a realistic coupling<br/>architecture between cortical neurons.  The model is constrained,<br/>whenever possible, by physiological and anatomical measurements.<br/>It has a multi-layered architecture and focuses upon dynamical<br/>behavior of the cortical network.<br/>     Nonlinear systems can have solutions that behave randomly,<br/>even though the system itself is strictly deterministic.  When<br/>these systems describe waves that are extended spatially, this<br/>chaotic behavior is not well understood today.  Such extended<br/>waves are prevalent throughout nature -- including the turbulent<br/>behavior of air masses and ocean currents that influence both<br/>weather and climate.  Clearly, the prediction of dynamical<br/>behavior for such chaotic waves has fundamental importance.  The<br/>investigator carries out mathematical studies of fundamental<br/>chaotic behavior for idealized nonlinear wave equations.  The<br/>brain is an extremely complex extended system whose dynamical<br/>response to stimulation is a central problem for modern neural<br/>science.  He and his colleagues, an interdisciplinary group of<br/>neural scientists and mathematicians at New York University, are<br/>developing a computational model of area V1 of the Primary Visual<br/>Cortex of the Macaque monkey, the first cortical region along the<br/>visual pathway at which cortical processing of visual information<br/>is thought to occur.  Their numerical experiments are designed to<br/>simulate specific laboratory observations of V1 -- with the<br/>results of both numerical simulation and laboratory experiments<br/>under continual comparison.  It is easier to identify possible<br/>mechanisms for cortical processing through numerical experiments<br/>than in the laboratory experiments.  Such interdisciplinary<br/>collaborations between experimental biologists and computational<br/>applied mathematicians unveil how the cortex processes visual<br/>information, and will lead to a deeper understanding of the<br/>functioning of the brain.  The project is supported by the Applied<br/>Mathematics program in the Division of Mathematical Sciences and<br/>by the Computational Neuroscience program in the Division of<br/>Integrative Biology and Neuroscience.<br/>"
"9973266","Some Approximation Problems in Differential Equations","DMS","COMPUTATIONAL MATHEMATICS","08/01/1999","07/28/1999","Luca Dieci","GA","Georgia Tech Research Corporation","Standard Grant","Michael Steuerwalt","07/31/2002","$129,499.00","","dieci@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1271","0000, 9263, OTHR","$0.00","Dieci<br/>9973266<br/><br/>The investigator and his collaborators analyze and implement techniques to tackle several problems in differential equations and matrix analysis.  The following topics are studied: (1) smooth orthonormal factorizations of parameter dependent matrices and connected applications,(2) computation of Lyapunov exponents with applications, (3) stability and bifurcations of invariant tori, (4) computation of matrix exponential and other functions of a matrix, and Riccati equations.  In all cases, theoretical analysis and extensive computational testing are undertaken, and codes for the outlined tasks are developed.  Specific aims include implementation of techniques to compute QR and SVD of fundamental matrix solutions, techniques to update invariant subspaces factorizations and SVDs of parameter dependent matrices, and applications connected to such techniques; implementation of methods to compute Lyapunov exponents and related stability information of continuous dynamical systems; techniques to compute the Lyapunov type numbers of Fenichel and use these quantities to monitor continuability and bifurcations of invariant tori; approximation of the exponential of a matrix in case the matrix is block triangular, and other aspects of computation of functions of a matrix.<br/><br/>To model physical phenomena in a compact way, differential equations are probably the most powerful tool we have.  Differential equations can model complicated chemical interactions and biological phenomena, manufacturing plants, robot and satellite motions, transport properties of materials, and a host of other phenomena of interest in the applied sciences.  However, typically one can only guarantee that differential equations have a solution, but it cannot be provided in closed form.  Moreover, in a typical situation, there will be uncertainties in the phenomenon under study, and these will show up as parameters in the differential equation model.  Loosely speaking, the fundamental question is then to understand the stability of solutions (as parameters change).  Now, suppose that we have obtained (at a high price) detailed information on the solution to our model for a certain parameter value in the model.  We may hope that if we change the parameter just slightly, we will be able to obtain detailed information for the new parameter value at a fraction of the cost we previously paid.  Basically, this expectation forms the principle of so-called ""continuation techniques.""  The investigator studies approximation techniques to solve differential equations which aim at assessing the stability of the solutions and at exploiting continuation techniques.  The goal is to eventually obtain approximation of a differential equation model in a way which will give the complete picture of solutions of the model as its parameters vary inside a certain (physical) range.  This goal finds its concrete realization in the study of quantities which serve as indicators of stability (the so-called Lyapunov exponents) and in computer programs which approximate these quantities.  The investigator also uses these Lyapunov exponents for predicting bifurcation phenomena and assessing error propagation when solving differential equations.  Finally, the investigator studies continuation techniques for matrix factorizations and produces computer programs for continuation techniques.<br/>"
"9818171","SIAM Interdisciplinary Conferences in Applied Mathematics","DMS","INFRASTRUCTURE PROGRAM, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","05/15/1999","05/17/2001","James Crowley","PA","Society For Industrial and Applied Math (SIAM)","Continuing Grant","Lloyd E. Douglas","04/30/2002","$191,700.00","","jcrowley@siam.org","3600 Market St.","Philadelphia","PA","191042688","2153829800","MPS","1260, 1266, 1271","0000, OTHR","$0.00","This project consists of a series of interdisciplinary conferences in  applied  and computational mathematics conducted by the Society for Industrial and Applied Mathematics (SIAM). The conferences are designed to bring together mathematicians, engineers, and scientists who are developing and applying new mathematical concepts, methods,  and algorithms. A key feature of this series is the inclusion of focused, interdisciplinary, academic/industry workshops in which mathematical, statistical and computational scientists from academia <br/>can interact with mathematicians, statisticians, and computational scientists, engineers, and scientists in academia and in industry on specific areas and problems of practical interest. The primary objective of the conferences is to initiate new interaction among the creators and users of mathematics and thereby contribute to the understanding of mathematics and encourage its application to problems of society."
"9971543","Theory and Applications of Random Cellular Automata and Related Models","DMS","PROBABILITY, COMPUTATIONAL MATHEMATICS","07/01/1999","07/06/1999","David Griffeath","WI","University of Wisconsin-Madison","Standard Grant","K Crank","06/30/2002","$84,000.00","","griffeat@math.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1263, 1271","0000, 9263, OTHR","$0.00","9971543<br/>Griffeath<br/><br/> On the theoretical side, the present proposal will further develop the investigator's rigorous theory of threshold growth, and related dynamics, for deterministic and probabilistic cellular automata (CA).  The theory of growth models has played a key role in the analysis of spatial complex systems, dating back to the middle of the century.  During the present funding period, Griffeath will seek to further that theory by focusing on five main research projects: scaling laws for critical monotone solidification, shape theory for random perturbations of threshold growth, a prototype of spinodal clustering, quantitative analysis of 'small-world networks', and asymptotics for von Koch - type CA solidification.  These studies are primarily theoretical, their goal being rigorous mathematical results.  On the experimental side, Griffeath has begun to examine more complex interactions which exhibit a wide variety of intriguing self-organizational effects not present in the simplest ""linear"" spatial systems.  Preliminary empirical modeling projects at present include digital biofilms, nonlinear spatial modeling of social interaction, CA pattern dynamics, and a simple CA paradigm for traffic jams.<br/><br/> In broad strokes, the ongoing research of David Griffeath combines mathematical analysis and computer visualization in the study of complex spatial systems.  The investigator exploits this interplay for the theoretical and empirical study of deterministic and random cellular automata, local lattice dynamics which serve as prototypes for spiral formation in excitable media, the morphology of crystals, and various other nonlinear processes such as nucleation, flocking, host-parasite interactions, dendritic growth, and phase separation by surface tension.  The analysis of aggregation, shape, interfaces, and droplet interaction for such basic growth models sheds new light on many areas of applied science.  The investigator's research contributes to the mathematical foundations of such processes.<br/>"
"9973048","Adaptive Methods for Systems of Reaction-Diffusion Equations in Three Space Dimensions","DMS","COMPUTATIONAL MATHEMATICS","07/01/1999","07/06/1999","Peter Moore","LA","Tulane University","Standard Grant","Jong-Shi Pang","02/28/2001","$73,914.00","","pmoore@mail.smu.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","MPS","1271","0000, 9263, OTHR","$0.00","9973048<br/><br/>Systems of reaction-diffusion equations occur frequently in scientific and engineering applications.  Some important examples include the bidomain model with the Beeler-Reuter ionic membrane kinetics in cardiac electrophysiology, the Brusselator model of the Belousov-Zhabotinsky reaction and Turing models of pattern formation Adaptive finite element methods are particularly well-suited to these problems and indeed have proved effective in solving a wide variety of partial differential equations without requiring user intervention.  This proposal aims to develop software for solving reaction-diffusion systems in three dimensions efficiently and accurately.  Adaptive codes depend on two building blocks, a posteriori error estimates and adaptive grid strategies.  The finite element method will employ hp-adaptivity where h and p will be allowed to vary in different directions (anisotropic refinement).  A new data structure for storing anisotropic grids will be implemented.  The adaptive strategy will be coupled with new a posteriori error indicators based on successful indicators developed by the proposer for linear elements.  The large system of linear equations arising from the temporal and spatial discretization process will be solved using preconditioned GMRES.  An adaptive preconditioning selection strategy will be used to choose the appropriate preconditioner during program execution.<br/><br/>Many physical and biological processes can be modeled by reaction-diffusion equations.  Some important examples include pattern formation in biological systems and voltage propagation in the heart.  Specifically models for voltage propagation in the heart aid in understanding the onset of cardiac arrhythmias and may lead to better therapies.  These models involve a large number of equations over both simple and complex geometries.  The goal of this proposal is to provide researchers, such as biomedical engineers, with the accurate and efficient computational tools necessary in order to analyze their models in the case of simple geometries.  These tools can then be used in solving problems on more complex geometries.  This work is being carried out in conjunction with the work of a biomedical engineer at Tulane University.<br/>"
"9971846","RUI:  Applications of Approximation Theory to Neural Networks and Wavelets","DMS","COMPUTATIONAL MATHEMATICS","07/15/1999","07/15/1999","Hrushikesh Mhaskar","CA","California State University-Los Angeles","Standard Grant","Michael Steuerwalt","03/31/2003","$120,000.00","","Hrushikesh.Mhaskar@cgu.edu","5151 State University Dr","Los Angeles","CA","900324221","2132240111","MPS","1271","0000, 9229, 9263, OTHR","$0.00","Mhaskar will continue his investigations on the complexity problem in<br/>the theory of neural networks and construction of multiscales based on<br/>orthogonal polynomial coefficients as well as samples of the target<br/>functions. Variations of such classical methods from approximation<br/>theory as summability methods and  polynomial inequalities will be<br/>developed to provide a unified theory of these apparently diverse areas.<br/>The work will be applied to the numerical construction of orthogonal<br/>polynomials, approximation on the sphere using scattered data, and the<br/>theory of system identification.<br/><br/>Although approximation theory is a classical area of mathematics, active<br/>for more than a hundred years, technological progress in the areas of<br/>neural networks and wavelets has given rise to many new possibilities<br/>for applications of approximation theory. Neural networks and wavelets<br/>are both useful in high speed parallel computing, and naturally involve<br/>the approximation of functions. Approximation theory techniques used<br/>with neural networks have produced dramatically better results in<br/>certain applications to array antenna technology, pattern recognition,<br/>and financial time series prediction."
"9971244","Combinatorial and Geometric Problems in Knot Theory","DMS","TOPOLOGY, COMPUTATIONAL MATHEMATICS","08/01/1999","07/30/1999","Morwen Thistlethwaite","TN","University of Tennessee Knoxville","Standard Grant","Benjamin M. Mann","07/31/2003","$69,927.00","","morwen@math.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","MPS","1267, 1271","0000, OTHR","$0.00","Proposal:  DMS-9971244<br/><br/>PI:  Morwen Thistlethwaite<br/><br/>Abstract: Thistlethwaite's research is in classical knot theory, the study of embeddings of smooth simple closed curves in the 3-sphere.  The computer is an essential tool in his research, in that the problems he addresses are often suggested by careful observation of data.  Thistlethwaite will continue to investigate the rate of growth of the number of knots, following his collaboration with C. Sundberg, where the exact growth exponent was determined for prime, alternating links.  He will study optimal configurations of symmetric knots, and will investigate other geometric problems associated with knot complements, in particular hyperbolic knot complements containing essential4-punctured spheres.  He will investigate the possibility that hyperbolic geometry might be used to provide a new, completely geometric proof of the Tait flyping conjecture.  The knot tables will be extended well beyond the 1,701,936 knots currently listed, and these new tables will be used to search for interesting examples.  Development will continue on the freely available software package ""Knotscape"", which provides a graphical interface to the knot tables and access to many invariants.<br/><br/>Thistlethwaite's specialty is classical knot theory, a branch of 3-dimensional topology with origins in the nineteenth century.  A knot is a closed curve in 3-dimensional space; one can imagine a knotted rope with its ends joined together.  Two knots are considered to be equivalent if one can be deformed continuously to the other, and a knot is deemed to be trivial, or unknotted, if it is equivalent to a flat circle.  Knot theory is a rich subject, as it interfaces with geometry, topology, algebra and combinatorics.  In recent years it has aroused the interest of chemists and molecular biologists, particularlyin relation to the knotting of DNA. Knot theory abounds with problems which are simple to state, but hard to solve; for example, it is often difficult in practice to prove that two given knots are inequivalent, as it is hard to rule out the existence of some ingenious method of deforming one to the other.  It can even be hard to decide whether a given knot is trivial.  If a knot is laid down on a flat surface with as few crossovers as possible, the resulting number of crossovers is called the crossing-number of the knot.  To date, Thistlethwaite has classified by computer the 1,701,936 knots of up to 16 crossings; this classification was confirmed by an independent tabulation carried out by J. Hoste and J. Weeks.  As part of this project, he will extend the tables to 17 or 18 crossings, thereby expecting to find many new examples with exciting properties.  He will continue to investigate the fundamental problem as to how fast the number of knots grows in relation to crossing-number.  Thistlethwaite will use hyperbolic geometry, a form of non-Euclidean geometry, to investigate hidden symmetries of knots, and to determine the structure of alternating knots (a knot is alternating if it can be arranged so that the rope goes alternately over and under at successive crossings.)  He will continue to develop the software package ""Knotscape"", which provides a graphical interface to the knot tables.<br/>"
"9972769","Computational Discrete Conformal Geometry and Applications","DMS","GEOMETRIC ANALYSIS, COMPUTATIONAL MATHEMATICS","08/15/1999","08/03/1999","Kenneth Stephenson","TN","University of Tennessee Knoxville","Standard Grant","Michael Steuerwalt","07/31/2003","$170,000.00","","KENS@MATH.UTK.EDU","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","MPS","1265, 1271","9216, 9263, HPCC","$0.00","9972769<br/><br/>The investigator further develops the software tool CirclePack and uses it to explore mathematical questions that arise in several areas of application.  Circle packing was introduced by Thurston, first in the context of orbifold constructions and then in the approximation of conformal mappings.  The topic has been developed to the point that it provides a surprisingly comprehensive discrete theory that both parallels and approximates the classical theory of analytic functions, i.e., conformal mappings in two dimensions.  Developments have been driven largely by classical connections --- discrete analyticity, extremal length, harmonic measure, Riemann mappings, and so forth --- and in turn those same connections account for circle packing's utility in a variety of other topics.  In function theory, the research focuses on circle packing techniques in studies of a conjecture of Smale, conformal tilings, and construction of Grothendieck's dessins d'enfants.  In applications, circle packing provides a unique new tool for graph embedding, and particular emphasis is placed on collaborations in brain-flattening and 2-dim quenching.  Computability of circle packings is a central strength, but growing demands have placed considerable stress on the software package ""CirclePack.""  The project addresses porting and modularization issues, and investigates strategies for implementing parallel packing algorithms.<br/><br/>Many areas of science, technology, and mathematics rely in fundamental ways on combinatorial information, that is, patterns, both concrete and abstract --- crossing patterns in a twisted strand of DNA, atomic locations in a crystal, activation sites in the visual cortex.  In 2-dimensional settings such patterns are often represented as planar graphs, which then need to be presented visually.  A pleasing new approach has emerged from abstract topology.  A ""circle packing"" is a configuration of circles having a specified pattern of tangencies.  It has dual natures: ""combinatoric"" in the prescribed pattern, but ""geometric"" in the radii of the actual circles.  One should think of the circles as bringing a ""spontaneous"" geometry to what began as a purely abstract combinatorial situation.  Fortuitously, this geometry inherits key features of an important classical mathematical topic, conformal geometry, so it brings to applications over 150 years of theoretical development.  However, unlike the classical theory, circle packings are both concrete and computable.  This project involves the development and application of circle packing algorithms and display software, with particular emphasis on collaboration with an NIH project on ""brain mapping,"" on the physics of 2-dimensional quenching, and on discrete analytic function theory in mathematical analysis.<br/><br/>"
"9973252","Applied Nonadaptive Group Testing and Superimposed Codes","DMS","COMPUTATIONAL MATHEMATICS","08/01/1999","07/26/1999","Anthony Macula","NY","SUNY College at Geneseo","Standard Grant","Jong-Shi Pang","07/31/2001","$52,720.00","","macula@geneseo.edu","1 College Circle","Geneseo","NY","144541401","5852455547","MPS","1271","0000, 9263, OTHR","$0.00","DMS 9973252<br/><br/>Abstract: Given a finite ground set of elements which can be uniquely characterized as positive or negative, the object of group testing is to optimize the identification of the positive objects by performing 0, 1 tests on  subsets or pools of the ground set.  Group testing algorithms are search algorithms.  If one thinks of a search algorithm as a battery of tests, then no test in a nonadaptive group testing algorithm can be modified by the results of any other test.  Thus the order in which the tests are performed is irrelevant.  Depending upon the real world problem that is being modeled, it is often the case that the tests can be carried out simultaneously.  Hence, nonadaptive search algorithms are well suited for parallel processing and computing when memory or data storage capability is limited.  Every parallel algorithm is nonadaptive.  In this project, we propose to use and develop combinatorial and algebraic codes to construct nonadaptive group testing algorithms from the matrix representations of these codes.  A specific aim of this proposal is to develop more efficient high throughput group testing algorithms that find a large number of positives when testing error rates can range up to 10%.<br/><br/>From an applied point of view, the ability to effectively and efficiently search for unknown objects is essential to many endeavors in the areas of biotechnology, telecommunications, and national security.  The unknown<br/>objects may be the locations of particular DNA sequences or a group of computer hackers.  Because of their computational power, group testing algorithms play a pivotal role in the Human Genome Project and they are essential in the screening many different kinds of biological and chemical data bases.  Group testing algorithms have also been applied to computer network protocols and can be used in the development of computer intrusion detection software. <br/><br/>"
"9971169","Modeling Chromosome Aberrations","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/1999","06/09/2003","Rainer Sachs","CA","University of California-Berkeley","Continuing grant","Michael Steuerwalt","08/31/2005","$537,615.00","","sachs@math.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1266, 1271","9139, 9183, 9251, 9263, BIOT","$0.00","Sachs<br/>9971169<br/>     Ionizing radiation produces chromosome aberrations, through<br/>breakage and large-scale reshuffling of chromosomes.  Using<br/>fluorescent in situ hybridization techniques to paint various<br/>chromosomes different colors reveals a rich variety of simple and<br/>complex aberrations, each putatively indicative of two or more<br/>DNA double strand breaks involved in one over-all misrepair<br/>reaction.  The investigator undertakes calculations on the<br/>biophysical implications of current data on aberrations, and of<br/>pulsed-field gel-electrophoresis data on DNA double strand break<br/>clustering along chromosomes, relevant to the early stages of<br/>aberration formation.  Three basic molecular models for DNA break<br/>misrejoining are compared.  There are so many combinatorial<br/>possibilities for this misrejoining that theorems, equations, and<br/>computer simulations are needed for systematically interpreting<br/>the extensive, elaborate current data.  The main methods are: (1)<br/>suitably adapting standard results of probability theory and<br/>stochastic process theory, mainly on Poisson cluster point<br/>processes; and (2), extending CAS (chromosome aberration<br/>simulator) computer software.  Collaborations with many different<br/>experimental groups world-wide are continued.  Models that assume<br/>randomness of break induction and misrepair can characterize with<br/>reasonable accuracy, using only two adjustable parameters, the<br/>relative frequencies of many different types of aberrations and<br/>their dependence on genomic content of the chromosome arms<br/>involved.<br/>     Understanding the effects of ionizing radiation on cells is<br/>an important part of fundamental biology.  Chromosome aberrations<br/>are complex cellular endpoints, copiously produced by ionizing<br/>radiation and informative about basic mechanisms of DNA damage<br/>production, repair, and misrepair.  Aberrations are also robustly<br/>implicated in the main areas of applied radiobiology, including<br/>cancer risk assessment, estimating cell killing during<br/>radiotherapy, and biodosimetry (i.e. judging past exposure of an<br/>individual to radiation, e.g. after a radiation accident or an<br/>astronaut's mission).  Due to increased current interest and to<br/>improved experimental techniques, there has been a flood of new<br/>data on radiation-induced aberrations.  The investigator studies<br/>the underlying biophysics of chromosome aberration formation,<br/>using state-of-the-art mathematical and computer methods.  His<br/>methods discriminate between the three current molecular models<br/>of aberration formation, which are needed mainly to predict<br/>levels of radiation damage in situations not yet measured<br/>experimentally, such as doses too low to be readily tractable in<br/>laboratory analyses but nonetheless of possible concern for large<br/>populations.  Mathematical and computer modeling is a<br/>comparatively inexpensive way to maximize biological information<br/>obtainable from the rapidly increasing databases and to help<br/>guide experiments.<br/><br/>"
"9972826","Mathematical Models of Molecular Motors","DMS","ADVANCES IN BIO INFORMATICS, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/1999","09/28/2001","George Oster","CA","University of California-Berkeley","Continuing grant","Michael Steuerwalt","08/31/2003","$420,000.00","Charles Peskin","goster@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1165, 1266, 1271","9183, 9216, 9263, BIOT","$0.00","Oster<br/>9972826<br/>     The investigator and his colleagues continue studies of<br/>biomolecular motors and mechanochemical phenomena at the<br/>microscale.  They focus attention on two particular protein<br/>motors: ATP synthase and kinesin.  The former is a rotary motor<br/>that combines in one protein the two major mechanism of energy<br/>transduction: The F1 portion hydrolyzes ATP to generate a rotary<br/>torque, and the Fo portion utilizes the energy stored in a<br/>transmembrane electrochemical gradient to generate a rotary<br/>torque in the opposite direction.  Kinesin is a molecular motor<br/>that hydrolyzes ATP to generate linear force and displacement<br/>along its microtubule track.  Both of these motors have been<br/>characterized in great detail, mechanically and biochemically as<br/>well as structurally.  The challenge is to incorporate this<br/>diverse body of information into a comprehensive model that<br/>provides a consistent explanation for each motor's behavior.  The<br/>investigators have formulated models for both motors comprising<br/>ATP synthase.  These models contain several phenomenological<br/>components that must now be modeled at a more detailed level to<br/>provide mechanistic explanations.  In particular, the process of<br/>binding ATP to the catalytic site is the heart of the energy<br/>transduction mechanism.  They model this process in detail.  The<br/>starting point for the proposed kinesin work is their 1995 model,<br/>which is tested against more recent data, such as trajectory<br/>variance as a function of load (an experiment that they<br/>suggested).  The role of elasticity in molecular motor function,<br/>which they have demonstrated in the context of imperfect Brownian<br/>Ratchet motors, is studied in kinesin (which is connected to its<br/>load through a flexible protein linkage), and also in chromosome<br/>transport during cell division, in protein translocation across<br/>intracellular membranes, and in the elastic coupling that<br/>connects the F1 and Fo motors comprising ATP synthase.  The<br/>influence of the water environment on the operation of molecular<br/>motors is investigated by developing a new<br/>mathematical/computational framework for mesoscale fluid<br/>dynamics.  This methodology combines a traditional continuum<br/>description of the fluid with random forces that simulate<br/>Brownian motion.  They use similar methods to simulate<br/>osmotically driven water transport within biological cells.<br/>     Mathematical modeling and computer simulation of living<br/>systems is needed in order to make sense out of the vast body of<br/>multidisciplinary data that is being generated at an ever<br/>increasing pace by experimental biologists.  Biological cells are<br/>now known to be teeming with protein motors, which, like tiny<br/>robotic machines, conduct the business of life through mechanical<br/>processes driven by the expenditure of chemical energy.  The<br/>mechanisms by which these motors operate are fundamental secrets<br/>of nature, vital to the understanding of life itself.<br/>Experimental investigations suggest possible mechanisms, but it<br/>is only through mathematical modeling and computer simulation<br/>that one can tell whether a proposed mechanism will account, in a<br/>quantitative way, for the observed behavior of any particular<br/>molecular motor.  The goal of this project is a detailed<br/>blueprint for the operating principles of these molecular<br/>machines.<br/><br/>"
"9973233","Computational Group Theory","DMS","COMPUTATIONAL MATHEMATICS, CONNECTIONS, NUMERIC, SYMBOLIC & GEO COMPUT","04/15/1999","03/27/2001","Gilbert Baumslag","NY","CUNY City College","Continuing grant","Michael H. Steuerwalt","03/31/2003","$450,000.00","Alexei Miasnikov","gilbert@groups.sci.ccny.cuny.edu","Convent Ave at 138th St","New York","NY","100319101","2126505418","MPS","1271, 1922, 2865","9139, 9263, HPCC","$0.00","Baumslag<br/>9973233<br/>     The principal investigator and his colleagues continue<br/>developing the software package Magnus, which has been designed<br/>to carry out computations in and experiments with, in the main,<br/>finitely presented groups.  This software has been set up so that<br/>it can be used remotely, via the internet.  This means that<br/>researchers world-wide can use the software by logging on to the<br/>Magnus server.  The project uses Internet-based facilities to<br/>maintain a list of outstanding open problems and to set up<br/>several discussion groups focusing on related mathematical areas.<br/>These support collaboratory interactions among users, including<br/>researchers in Europe, Australia, New Zealand, Hong Kong, and<br/>China.  Software development includes the introduction of genetic<br/>algorithms as well as measures to allow users to add their own<br/>routines to Magnus, extending the scope of the tool.  The<br/>investigators also use Magnus to study problems in pure group<br/>theory and in commutative algebra.  The project includes the<br/>training of students.<br/>     In mathematics, group theory aims to capture and study the<br/>essence of symmetry, particularly in patterns.  Such patterns<br/>occur frequently in nature, for instance as crystals.  Its<br/>connection with symmetry makes group theory important in physics,<br/>in biology, and in the study of our physical world, as well as in<br/>geometry and other mathematical fields such as the theory of<br/>computation and cryptography.  The investigators' goal in this<br/>project is to build an easy-to-use computational tool that makes<br/>it possible for a user to carry out experiments and solve<br/>problems about the most ubiquitous of the mathematical objects<br/>involved here, the so-called finitely presented groups.  These are<br/>groups that can be described completely by a finite set of rules;<br/>examples include the group of rigid geometric motions of an<br/>object and the groups of quantum mechanics.  The computational<br/>tool, Magnus, has a simple but limited interface.  Nevertheless,<br/>Magnus has already been used to explore and solve a number of<br/>difficult problems in group theory.  The present project focuses<br/>on algorithmic developments and techniques to allow users to add<br/>their own routines to Magnus, extending the scope of the tool.<br/>The project also provides Internet-accessible computation and<br/>information resources in a collaboratory setting, fostering<br/>Internet-based interchanges and collaborations among researchers.<br/>Finally, the project helps train students in both computation and<br/>mathematics.<br/>"
"9972228","Evolution of Co-dimension k&gt;1 Manifolds Embedded in R^n and Applications to Medical Image Analysis","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, SIGNAL PROCESSING SYS PROGRAM","08/15/1999","07/25/2001","Oliver Faugeras","MA","Massachusetts Institute of Technology","Continuing grant","Michael H. Steuerwalt","07/31/2003","$202,500.00","Eric Grimson","faugeras@ai.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1266, 1271, 4720","0000, 9183, 9216, 9263, BIOT, HPCC, OTHR","$0.00","Faugeras<br/>9972228<br/>     The investigators and their colleagues study the problem of<br/>automatically clustering sets of points that lie on smooth<br/>manifolds of arbitrary co-dimension.  They use the geodesic snake<br/>approach in which they set up a Partial Differential Equation<br/>(PDE) that drives the evolution of a manifold in some potential<br/>field created by the data points, such that the solution of the<br/>PDE converges to a manifold that is very close to the data points<br/>(ideally contains them).  The ""shape"" of this manifold (including<br/>its co-dimension) characterizes the set of points.  They<br/>investigate computationally efficient implementations for<br/>evolving manifolds of arbitrary co-dimension by level sets<br/>methods, because of their robustness and flexibility.  These<br/>implementations are being used to study the Euclidean and affine<br/>mean-curvature motion of curves in three-dimensional spaces.  Two<br/>applications are being pursued by the investigators.  The first<br/>application is the detection of blood vessels in Magnetic<br/>Resonance Angiography (MRA) images.  In MRA images, blood vessels<br/>appear as bright and noisy curve-like patterns, possibly with<br/>gaps.  It is important to detect those patterns independently of<br/>the noise and to bridge the gaps.  The detection of blood vessels<br/>can be considered, as a first approximation, as an instance of<br/>the three-dimensional case, co-dimension two.  The second<br/>application is the clustering of feature points in higher<br/>dimensional spaces with an eye on two specific problems, the<br/>description of the ""shape"" of a cluster and the design of<br/>automated content-based image retrieval systems.  A given pattern,<br/>e.g. an image, can be represented by a vector of features in some<br/>n-dimensional space.  Patterns in the same class usually fall into<br/>clusters that may have some very tricky ""shapes"".  The<br/>investigators and their collegues believe that their methods can<br/>provide ways of automatically obtaining shape descriptions from<br/>sets of sample feature vectors thereby facilitating the design of<br/>recognition algorithms and improving their performances.  A target<br/>application is the design of automated content-based image<br/>retrieval systems.<br/>     The investigators and their colleagues study the problem of<br/>automatically fitting a set of points in a possibly<br/>high-dimensional space to a set of smooth manifolds.  This is<br/>important because in many practical applications data can be<br/>represented as points in high-dimensional spaces.  They are<br/>currently pursuing two applications of these techniques.  The<br/>first one is the detection of blood vessels in Magnetic Resonance<br/>Angiography (MRA) images.  In MRA images, blood vessels appear as<br/>bright and noisy curve-like patterns, possibly with gaps.  It is<br/>important to detect those patterns independently of the noise and<br/>to bridge the gaps.  The results of this detection can be used in<br/>the diagnosis of diseases and in planning surgery.  The second<br/>application is the description of the ""shape"" of a cluster of<br/>points in a high-dimensional space.  The investigators and their<br/>colleagues are investigating means of automatically obtaining<br/>such descriptions from a set of samples.  They believe that this<br/>will facilitate the design of recognition algorithms and increase<br/>their performances.  A target application that is particularly<br/>relevant to the average citizen is the design of automated<br/>content-based image retrieval systems.  In such systems each image<br/>can be represented by a point in some high-dimensional (10 to 20)<br/>feature space, and images in the same class, e.g. images of cars,<br/>cluster on some manifold, e.g a ""car-manifold"".  The description<br/>of the shape of this manifold (which is simpler than the real<br/>shapes of the objects in the class) can then be used to retrieve<br/>all images of, e.g. cars, in the data base because their<br/>representations in the high-dimensional space will be closer to<br/>the corresponding manifold than to any other.<br/>"
"9973339","Computational Study of the Maximal Inscribing Ellipsoid Problem and Applications to Integer Programming","DMS","COMPUTATIONAL MATHEMATICS","07/01/1999","06/30/1999","Yin Zhang","TX","William Marsh Rice University","Standard Grant","Michael H. Steuerwalt","06/30/2003","$153,680.00","","yzhang@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271","0000, 9263, OTHR","$0.00","9973339<br/><br/>The problem of finding the maximum-volume ellipsoid inscribing a given polytope in n-dimensional space has many important application potentials.  However, the lack of practically efficient algorithms and software for this problem has so far hampered its actual applications in practice.  The proposed project will attempt to rectify this situation.  Firstly, we plan to develop and implement efficient, practical algorithms and portable library software for solving the problem.  The algorithms to be studied will be variants of infeasible primal-dual interior-point methods.  Secondly, we will utilize the resulting software to develop and implement algorithms for integer programs that have non-binary variables varying in wide ranges and are difficult to solve for conventional branch-and-bound techniques.<br/><br/>Given a polygon on the plane, what is the largest ellipse that can fit inside the polygon?  This question can be easily generalized from the two-dimensional space (the plane) to higher-dimensional spaces, resulting in the so-called maximum-volume ellipsoid problem.  In many cases, the amount of computing time required for solving this problem determines how fast we can solve more complicated problems in applications, for example, problems in manufacturing and managerial decision-making processes.  In this project, we will develop fast methods and build efficient computer code for solving the maximum-volume ellipsoid problem, and then apply the code to solving application problems.<br/><br/>"
"9973393","Approximation and Computation of Lyapunov Exponents, Global Errors, and Functional Traveling Waves","DMS","COMPUTATIONAL MATHEMATICS","08/01/1999","07/16/1999","Erik Van Vleck","CO","Colorado School of Mines","Standard Grant","Michael H. Steuerwalt","07/31/2003","$150,000.00","","erikvv@ku.edu","1500 Illinois","Golden","CO","804011887","3032733000","MPS","1271","0000, 9263, OTHR","$0.00","Van Vleck<br/>9973393<br/><br/>The investigator and his colleagues study issues in computational differential equations: shadowing and related topics of global error analysis for initial value problems, computation of stability information and Lyapunov exponents, and analysis and computation of lattice differential equations, i.e., differential equations that are discrete in space and continuous in time.  This project has as a central theme the combination of dynamical systems ideas with numerical analysis ideas.  The focus is on the development and analysis of efficient, accurate numerical techniques and on analytical and computational techniques that help identify interesting dynamic properties of differential equations.<br/><br/>The investigator and his colleagues study methods for understanding the propagation of errors that occur during the numerical computation of differential equations.  This understanding is fundamental because models of physical and biological systems are often solved using computational techniques.  The research on lattice differential equations has applications in the area of materials modeling.  The models being considered allow for the inclusion of both microscopic and macroscopic effects, which will become more important as advances in computer technology allow for larger, more sophisticated models of materials.<br/>"
"9973423","Collaborative Research: Large Scale Optimization: Matrix Free Algorithms, Data Parallelism, and Applications in Seismic Inversion","DMS","COMPUTATIONAL MATHEMATICS","09/01/1999","08/27/1999","William Symes","TX","William Marsh Rice University","Standard Grant","Michael H. Steuerwalt","08/31/2003","$118,000.00","","symes@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271","9216, 9263, HPCC","$0.00","     In this collaborative interdisciplinary project, the<br/>investigators Mark Gockenbach, Anthony Kearsley, and William<br/>Symes develop and implement algorithms for large-scale<br/>optimization problems that arise in a variety of applications,<br/>focusing on techniques particularly suited to parallel<br/>architectures, and apply the methods to the seismic velocity<br/>estimation problem.  Large-scale optimization problems often<br/>present difficulties to standard algorithms and software.  Many<br/>of these difficulties arise in the seismic velocity estimation<br/>problem.  First, the sheer data volume makes it impractical to<br/>explicitly form and factor matrices, as required by many standard<br/>optimization algorithms.  To address this, a new matrix-free<br/>Sequential Quadratic Programming (SQP) algorithm is developed,<br/>based on recent advances in matrix-free algorithms for the<br/>so-called trust region subproblem.  Second, the data structures<br/>and interfaces required for seismic data processing are not<br/>easily adapted to those required by ""off-the-shelf"" optimization<br/>software.  The Hilbert Class Library (HCL), an object-oriented<br/>optimization package, can solve optimization problems involving<br/>data structures and interfaces of arbitrary complexity.  The SQP<br/>algorithm, along with the necessary seismic data structures and<br/>simulators, is implemented in HCL.  Third, large-scale<br/>simulations often require the use of parallel computation.  The<br/>use of parallelism in simulation and optimization is addressed<br/>through the development of HCL classes that automatically<br/>distribute data over a network of distributed workstations.  The<br/>above innovations in optimization methods and software are used<br/>to study in detail a new formulation of the seismic inverse<br/>problem.  The investigators have recently introduced this<br/>formulation in order to overcome certain optimization-theoretic<br/>difficulties inherent in standard formulations.<br/>     Optimization problems arise in science and engineering,<br/>where one often wishes to find the best design, the best<br/>mathematical model, the best strategy, and so forth.  Large-scale<br/>optimization problems, which involve many variables, present<br/>special challenges, including the choice of algorithm, the<br/>representation of data, and the interface between optimization<br/>software and programs written by the application scientist.  This<br/>project addresses these challenges in the context of an important<br/>application, seismic exploration.  The investigator and his<br/>colleagues develop new optimization algorithms to identify<br/>geological features of the subsurface of the earth.  Included is<br/>a general method for solving large-scale optimization problems;<br/>this algorithm is applicable to other science and engineering<br/>problems.  Moreover, they also develop an innovative software<br/>package, called the Hilbert Class Library (HCL), that allows<br/>optimization algorithms to be used with problems of arbitrary<br/>complexity; the software adapts to different data structures and<br/>software interfaces.  Finally, they extend HCL to automatically<br/>take advantage of parallel computers, making it easier to take<br/>advantage of high performance hardware.  The seismic exploration<br/>problem is important to the petroleum industry; a precise<br/>knowledge of geological structures is essential for efficient<br/>utilization of petroleum reserves.  In addition, the HCL software<br/>addresses the important issue of technology transfer as it<br/>pertains to numerical algorithms; too often algorithmic advances<br/>are unavailable to application scientists because optimization<br/>software and application software have incompatible interfaces."
"9971157","Conference on Discrete and Computational Geometry","DMS","GEOMETRIC ANALYSIS, COMPUTATIONAL MATHEMATICS, NUMERIC, SYMBOLIC & GEO COMPUT","06/15/1999","06/10/1999","Jacob Goodman","NY","CUNY City College","Standard Grant","Michael H. Steuerwalt","05/31/2000","$21,000.00","","jegcc@cunyvm.cuny.edu","Convent Ave at 138th St","New York","NY","100319101","2126505418","MPS","1265, 1271, 2865","9216, 9263, HPCC","$0.00","9971157<br/><br/>The principal investigator and his colleagues, Richard Pollack (Courant Institute, New York University) and Emo Welzl (ETH Zurich) organize a research conference on discrete and computational geometry, which will take place during the week of June 27, 1999 through July 2, 1999 at the ETH Zurich seminar center in Monte Verita, Ascona, Switzerland. This is the third in a series of international conferences on discrete and computational geometry going back to 1986. Areas to be discussed include enumerative properties of arrangements, k-sets, geometric transversal theory, the complexity of Betti number computation, a new proof of the bellows conjecture, and the recently-announced proof of the Kepler conjecture, among many others.<br/><br/>The fields of discrete (or combinatorial) geometry and of computational geometry both deal with configurations of points, lines, planes, balls, polyhedra, etc. in 2- and 3-dimensional space, as well as in higher dimensions; i.e., with fundamental questions about simple (but usually many) geometric objects. Discrete geometry seeks a basic understanding of such geometric structures, while computational geometry investigates the inherent complexity (difficulty) of algorithmic problems. Many questions in applied areas can be reduced to problems about discrete configurations of many geometric objects. Hence advances in mathematical understanding of the geometric structures and in computational methods for dealing with them can have significant consequences for applications. Such applications arise, for example, in computer aided design and manufacturing, solid modeling, textile layout, medical imaging, computer vision, optimization, computer graphics and virtual reality, robot motion planning, geographic information systems, and molecular biology."

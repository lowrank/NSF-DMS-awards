"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1620216","Three dimensional deep wavelet scattering for quantum energy interpolation","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","09/08/2016","Matthew Hirn","MI","Michigan State University","Standard Grant","Leland Jameson","08/31/2020","$191,775.00","","mhirn@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","9263","$0.00","Physical quantities are often computed as solutions to a system of complex mathematical equations, which may require huge computations for intricate physical states. Quantum chemistry calculations of molecular energies is such an example. Indeed, computing the energy of a molecule, given the charges and positions of its nuclei, is a central issue in computational chemistry with important applications in molecular dynamics, materials science, and drug discovery. Machine learning algorithms do not simulate the physical system but estimate solutions by learning from a training set of known examples. However, such learning algorithms may require a number of examples that is exponential in the system dimension, and are thus intractable; this phenomena is referred to as the ""curse of dimensionality."" This proposal will develop a novel approach for the estimation of molecular energies based on the ""scattering transform."" The scattering transform estimates molecular energies, and circumvents the curse of dimensionality, by utilizing a multiscale, multilevel architecture that takes advantage of physical invariants. The resulting algorithms have the potential to significantly speed up the computation of highly accurate molecular energy estimates, leading to large scale atomistic simulations with greatly improved accuracy, speed, and adaptability, thus shifting the paradigm of multiscale modeling. The PI will additionally mentor an undergraduate student and train a graduate student in this field, thus setting up the potential for dissemination of the core ideas to a broader audience.<br/><br/>The scattering transform has the structure of a deep convolutional network, but is composed of iterated wavelet transforms and complex modulus operators. Such networks have been used in computer vision for the analysis and classification of two dimensional images and audio tasks involving one dimensional signals. A multiscale three dimensional scattering transform network is novel both in practice (multiscale 3D) and design (for quantum chemistry), and has the chance to influence these types of architectures moving forward. A systematic approach will attack the primary object on several fronts: (1) development of efficient 3D filters with the appropriate symmetry and stability properties; (2) rigorous error analysis of the scattering regression algorithm for various components of the molecular energy functional; (3) deeper understanding of the scattering network via provable relations with fast multipole methods. The methods used to carry out these objectives will include: (i) wavelet filter design and efficient signal processing algorithms; (ii) utilization of Littlewood-Paley Theory in conjunction with polynomial (Taylor) approximation theory; (iii) multiscale analysis; (iv) numerical experiments to validate methods. By rigorously linking deep learning architectures with physical chemistry, the research in this proposal will take place at the interface of data science and scientific computation, for the mutual gain of both."
"1621798","Asynchronous parallel stochastic frameworks with convergence guarantee for solving large-scale fixed point problems","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","09/01/2016","08/30/2016","Ming Yan","MI","Michigan State University","Standard Grant","Christopher Stark","08/31/2019","$150,000.00","","yanm@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271, 8069","8083, 9263","$0.00","In the last two decades, the size of data sets in a large number of areas has grown quickly. In many applications of machine learning, there are massive amounts of training data sets and the data sets may be collected and stored at different locations. Learning a model from these data sets imposes high demands for computation, memory, and data transfer on algorithms. Asynchronous parallel algorithms are applied to solve these large-scale problems via high performance computing and reduced communication and idle time. The performance of asynchronous parallel algorithms is improved largely comparing to synchronous parallel algorithms, especially when the number of cores is large. However, theoretical analysis on the convergence and convergence rates of these algorithms still investigation. <br/> <br/>In this proposal, the PI will develop fast and robust generic asynchronous parallel stochastic frameworks with provable convergence for solving large-scale fixed point problems that have applications in a large number of areas. One objective is to develop asynchronous stochastic algorithms for finding a zero point of a random operator, the sum of a random operator and a deterministic operator, and the sum of two random operators and show the convergence of these algorithms. Another objective is to couple coordinate updates into these asynchronous stochastic algorithms and show their convergence. The last objective is to implement these algorithms and develop software to help people without knowledge about parallel computing run asynchronous algorithms. The research in solving fixed point problems is motivated by problems in various computational sciences and engineering, and its development benefits all these fields by providing fast and robust algorithms. Areas impacted by the proposed work include machine learning, optimization, optimal control, statistics, finance, signal and image processing, compressive sensing, as well as other lines of research involving large data sets and distributed data."
"1619960","Collaborative Research:  Multiscale Study of Active Cellular Matter: Simulation, Modeling, and Analysis","DMS","COMPUTATIONAL MATHEMATICS","08/15/2016","08/16/2016","Tong Gao","MI","Michigan State University","Standard Grant","Leland Jameson","07/31/2020","$194,999.00","","gaotong@egr.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","9263","$0.00","Active cellular matter is the basis of novel synthetic active fluids made of mixtures of suspended cytoskeletal filaments and molecular motors. By consuming chemical fuel, the molecular motors (e.g., kinesins) can bind to and create actively moving crosslinks between the biofilaments (e.g., microtubules) to drive their relative motion, which leads to large-scale collective motions in the filament/motor mixture through hydrodynamic coupling. Synthetic active suspensions made of small numbers of components reveal how higher-order aspects of assembly and organization are built in living cells. These systems also present new challenges to our understanding, design, and analysis of materials, and have the potential to provide valuable new technologies such as autonomously moving and self-healing materials.<br/><br/>In this work, the investigators study active cellular matter composed of microtubules and molecular motors through multiscale methods, and tightly coupled modeling, analysis, and simulation. The project aims to understand the fundamental interactions underlying stress generation within bundles of rigid/flexible biofilaments that undergo dynamic instability, as well as the nonlinear dynamics and hierarchical pattern formation in large-scale collective motions. The project will also predict key material properties including its coherent structures, local heterogeneity, time- and length-scales, and material rheology. To resolve the physics at different length- and time-scales, several methods will be developed and integrated: (1) microtubule-motor interactions will be simulated using a kinetic Monte Carlo method; (2) the hydrodynamic interactions between objects of various shapes will be modeled using a nonlocal slender body/boundary integral method, together with fast summation methods; (3) a pseudo-spectral method will be implemented to simulate the collective motion through a continuous active liquid-crystal type model."
"1620273","Higher Order Variational Inequalities: Novel Finite Element Methods and Fast Solvers","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/19/2018","Susanne Brenner","LA","Louisiana State University","Continuing Grant","Leland Jameson","06/30/2020","$356,807.00","Li-yeng Sung","brenner@math.lsu.edu","202 HIMES HALL","BATON ROUGE","LA","708030001","2255782760","MPS","1271","9150, 9263","$0.00","Variational inequalities appear in areas that involve differential equations and optimization.  They are fundamental tools for the modeling of phenomena in science, engineering and finance that involve inequality constraints.  The goal of this project is to design, analyze and implement reliable and efficient numerical algorithms for variational inequalities that involve higher order differential equations, with applications to optimal control and materials science.<br/><br/>Novel finite element methods for higher order elliptic and parabolic variational inequalities will be developed together with fast solution techniques (adaptive, parallel and multilevel) for the resulting discrete problems. The emphasis is for problems on nonsmooth and nonconvex domains where the regularity of the solutions of the variational inequalities is much more subtle, and for problems on three dimensional domains where numerical computations are much more demanding.  An important application is to optimal control problems constrained by elliptic partial differential equations. By reformulating these optimal control problems as fourth order variational inequalities for the state variable, various finite element methodologies(classical conforming and nonconforming finite element methods, discontinuous Galerkin methods, partition of unity methods, mixed finite element methods, etc.) and techniques (error estimators, local mesh refinements, inclusion of singularities in local approximation spaces, etc.) can be employed in their numerical solutions.  The new numerical methods designed from this approach will be fundamentally different from the ones obtained by the traditional approach where the emphasis is on the control variable."
"1619993","Development and analysis of high-order partitioned schemes for fluid-structure interaction problems","DMS","COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","06/15/2016","06/30/2017","Martina Bukac","IN","University of Notre Dame","Continuing Grant","Leland Jameson","05/31/2020","$187,946.00","","mbukac@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","1271, 7334","7334, 9263","$0.00","Fluid-structure interaction (FSI) problems arise in many applications, such as aerodynamics, geomechanics and biomedical engineering. In hemodynamics, FSI models have been used to describe the interaction between blood and arterial walls. More precisely, numerical algorithms for fluid-structure interaction problems can provide predictions in many cardiovascular diseases, such as aneurysms or atherosclerosis. Since combining state-of-the-art algorithms with non-invasive clinical measurement tools provides an innovative approach to medical diagnosis and surgical decision making, there is an increasing demand for fast and efficient numerical schemes to solve FSI problems. The PI will develop and analyze a class of stable and robust high-order partitioned numerical methods for FSI problems. The development of stable and efficient algorithms for fluid-structure problems is crucial for performing patient specific diagnostic tests, and this work will make a major contribution in biomedical research.<br/><br/>The goal of this research is a development and analysis of a class of higher-order partitioned numerical methods for interactions between an incompressible, viscous fluid and a thin, elastic structure. The discretization in space will be performed using the finite element method, and different partitioned methods will be proposed based on the time discretization. In particular, algorithms will be developed based on the kinematically coupled scheme (Project 1), the Strang operator splitting approach (Project 2) and the Crank-Nicolson and Leapfrog method (Project 3). Energy estimates and convergence rates will be derived for each proposed algorithm. A comparison of all the proposed methods based on their performance, stability and convergence properties will be made, allowing other researchers to identify optimal algorithms for specific choices of parameter values. The proposed algorithms, numerical analysis and simulation results will be published and made available to the community. In that way, they will serve as a set of reference data that can be used for model validation by other researchers."
"1619807","Meshfree Finite Difference Methods for Nonlinear Elliptic Equations","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","09/02/2016","Brittany Froese Hamfeldt","NJ","New Jersey Institute of Technology","Standard Grant","Leland Jameson","08/31/2021","$149,974.00","","bdfroese@njit.edu","323 DR MARTIN LUTHER KING JR BLV","NEWARK","NJ","071021824","9735965275","MPS","1271","9263","$0.00","Nonlinear elliptic equations describe problems as varied as the design of lenses and reflectors, mapping the subsurface of the earth, interpretation of medical images, and modelling complex weather phenomena. However, existing tools for solving these equations are practical only in very simple settings, and can fail when faced with realistic data. This project will introduce a new class of methods for solving nonlinear elliptic equations when the data is unstructured and non-smooth. The new mathematical and computational techniques developed in this project will lead to fast, reliable methods for solving equations in the realistic settings required for further progress in current applications.<br/><br/>This project will introduce a new class of meshfree finite difference methods for solving nonlinear degenerate elliptic equations in two- and three-dimensions. Whereas existing convergent methods for fully nonlinear equations often require computations to be performed on a uniform grid in a rectangular domain, this framework will allow equations to be posed on unstructured point clouds. Methods will rely on unusually large search neighborhoods in order to construct approximations that align with the structure of the underlying PDE operator. The resulting schemes will correctly approximate weak (viscosity) solutions, while allowing for adaptivity and complicated geometries. This project will also introduce new formulations of several non-classical boundary conditions, which will be used to produce meshfree implementations. Fast solution techniques for the resulting algebraic systems will be developed."
"1620288","Adaptive Discontinuous Galerkin Methods and Applications","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","07/12/2021","Ohannes Karakashian","TN","University of Tennessee Knoxville","Standard Grant","Yuliya Gorb","08/31/2022","$196,453.00","","okarakas@utk.edu","201 ANDY HOLT TOWER","KNOXVILLE","TN","379960001","8659743466","MPS","1271","1303, 9263","$0.00","Scientific computing and especially computational mathematics are recognized as crucial to the advancement of science. By replacing expensive and sometimes even impossible laboratory experiments (e.g., supernova explosions), numerical simulations have grown steadily in importance as effective alternatives to these. Therefore, the development of state of the art algorithms and codes, with the concomitant increase in confidence in their predictions, is important to progress in all these fields. The research program in this project aims at the development, analysis, and computer implementation of numerical methods designed to approximate the solutions of some partial differential equations that govern key phenomena in the fields of applied mathematics, biomedicine, engineering, and physics. In particular, this research program will be applied to generate efficient numerical algorithms for the effective simulation of the functioning of the heart by focusing on its electro-mechanical nature and the resulting fluid flow patterns. Another application of importance to society will be to apply the newly developed algorithms to the simulation of the Earth's climate by studying the influence of various chemicals including greenhouse gases, as well as pollutants such as aerosols, dust and black carbon. The knowledge gained will be communicated to other researchers through seminars and conferences. New courses will be developed and students will participate in all phases of the project. The latter aspect is crucial in developing the new generation of researchers.<br/><br/>The numerical methodologies described in this proposal namely the finite element method and in particular the discontinuous Galerkin method have been studied for several decades now. Yet there is ample need for growth in both the analytical and application arenas. Research on traditional as well as recently introduced a posteriori error estimates will be conducted extending these estimates to classes of equations that had stayed beyond the reach of such methods. These will lead to improved and flexible adaptive methods that will be applied to solving problems of practical importance. An additional application of adaptive methods will be to answer questions of analytical nature concerning the existence, uniqueness, and stability properties of solitary wave solutions of nonlinear dispersive equations. The crucial problem of solving the resulting systems of linear and nonlinear equations will form an integral component of the proposed research with multilevel domain decomposition and multigrid methods forming the backbone of the thrust in this context. Implementation on massively parallel computers will be pursued actively given that most of the problems studied involve three spatial dimensions as well as time."
"1547357","RTG: Algebraic Topology and Its Applications","DMS","PROBABILITY, TOPOLOGY, COMPUTATIONAL MATHEMATICS, Special Projects - CCF, WORKFORCE IN THE MATHEMAT SCI, Combinatorics","06/01/2016","05/27/2022","Matthew Kahle","OH","Ohio State University","Continuing Grant","Joanna Kania-Bartoszynska","05/31/2024","$1,879,711.00","Michael Davis, Tamal Dey, Yusu Wang, Facundo Memoli","kahle.70@osu.edu","1960 KENNY RD","COLUMBUS","OH","432101016","6146888735","MPS","1263, 1267, 1271, 2878, 7335, 7970","7301, 7556, 8084, 9251","$0.00","Recently, algebraic topology has found a number of applications within mathematics and beyond. For example, topology provided powerful new methods for detecting and describing shape and structure in complex and high-dimensional data. This award supports the Research Training Group in Algebraic Topology and its Applications at Ohio State University. This work will be performed in a highly collaborative and active interdisciplinary research environment that has recently emerged, involving investigators from the Departments of Mathematics and Computer Science & Engineering. The main objective of the project is to provide early-stage mathematicians and computer scientists with opportunities to learn about both theoretical and applied aspects of topology and related fields. Exposure to these topics will allow students and junior faculty to move fluently across a range of research topics, to enter research communities in traditional core areas, and to work in new interdisciplinary directions.<br/><br/>This award supports postdoctoral researchers, graduate students, and undergraduate researchers studying such topology-related fields as geometric group theory, metric geometry, computational geometry and topology, knot theory, and stochastic topology. The award also supports the development of new courses, as well as working groups, seminars, and workshops in these areas. At the heart of the project is an emphasis on mentoring at all levels. The program also places a strong emphasis on developing writing and communication skills, particularly for postdoctoral researchers and graduate students involved in the research group."
"1620047","Development and Application of Efficient High-order Semi-Lagrangian Schemes","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","09/06/2016","Wei Guo","MI","Michigan State University","Standard Grant","Leland Jameson","05/31/2018","$79,713.00","","weimath.guo@ttu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","8012, 9263","$0.00","Understanding behaviors of plasmas plays an increasingly important role in modern science and engineering such as thermo-nuclear fusion, satellite amplifier, and computer chip manufacturing. A fundamental model in plasma physics is the Vlasov-Maxwell system, which is a nonlinear kinetic transport model describing the dynamics of charged particles due to the self-consistent electromagnetic forces. As predictive simulation tools in studying the complex kinetic system, efficient, reliable and accurate transport schemes are of fundamental significance. The main numerical challenges in such studies lie in the high dimensionality, nonlinear coupling, and inherent multi-scale nature in both space and time. Another application concerned in this project is in atmospheric science. One example is the chemistry-climate model in the study of evolution of stratospheric ozone and many other chemical constituents. The present generation of global climate models include hundreds of tracer species in order to adequately represent complex physical and chemical processes, resulting in huge computational cost in computer simulations. <br/> <br/>The PI will develop and analyze a class of efficient, reliable and highly accurate numerical methods for transport problems in plasma physics and atmospheric science. A semi-Lagrangian framework will be devised by employing a high order discontinuous Galerkin spatial discretization to take advantage of its many attractive properties, such as flexibility, compactness, and excellent ability to resolve features involving multiple scales. By a careful design in the scheme formulation, the proposed scheme is free of splitting error and able to conserve total mass of the system. Motivated by the work of PI on developing a fast asymptotic preserving Maxwell solver that is capable of recovering the magneto-static limit, the temporal scale separation issue associated with the Vlasov-Maxwell simulations will be addressed. For the applications in the global chemistry-climate modeling, the new schemes can be conveniently adapted to the spherical transport simulations based on the cubed-sphere geometry. Theoretical issues including the stability analysis and error estimates will be investigated."
"1619821","Reduced order models for imaging and inversion with waves and diffusive fields","DMS","COMPUTATIONAL MATHEMATICS","06/15/2016","06/19/2018","Alexander Mamonov","TX","University of Houston","Continuing Grant","Leland Jameson","05/31/2020","$209,879.00","","mamonov@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271","9263","$0.00","The project's focus is on development of novel techniques for computational inversion and imaging problems arising in medical imaging, geophysical exploration, nondestructive evaluation and testing and other applications where the internal structure and properties of objects must be determined without direct access to the object's interior. The object is subjected to probing fields and the measurements of its response are made. Electromagnetic fields and acoustic waves are most often used for probing. Conventionally an inverse problem of determining an object's properties from the measurements is formulated as a minimization of a misfit between the measured data and the prediction of a forward model. Such problems are notoriously difficult to solve due to a highly nonlinear dependence of measurements on model properties. We propose an approach to inversion and imaging based on the theory of reduced order models (ROMs). This approach aims to alleviate the nonlinearity of the inverse problem thus making it much easier to solve. It allows one to obtain images free from various types of artifacts that conventional methods often struggle to remove. <br/> <br/>The proposed framework is general enough and can be applied to inversion and imaging both with waves and in diffusive regimes. First, a ROM is constructed as a projection of the partial differential equation (PDE) operator on subspaces of PDE solution snapshots either in the time or the frequency domain. This ensures that the ROM response interpolates the measured data. Even though neither the PDE operator nor the solution snapshots are directly accessible in inversion, projections can be computed from the measured data using the tools of linear algebra. After the ROM is constructed it may be used in at least two ways. First, it can be used in an imaging algorithm. Since the ROM is a projection of the PDE operator, an image can be constructed from the back-projection of a ROM. Model reduction takes into account nonlinear interactions between the reflectors and thus allows one to eliminate artifacts caused by multiple reflections. This is a vast improvement over conventional imaging approaches that are often based on linearizations (Born, Kirchhoff) which miss or misinterpret the nonlinear effects. Second, the ROM can be used to reformulate conventional optimization problems to minimize ROM misfit instead of data misfit. Such optimization objective is expected to be more convex which makes inversion less prone to local minima. Another consequence is accelerated convergence. The following specific aspects of ROMs for inversion and imaging are proposed: (1) new imaging functionals;(2) backscattering and non-collocated source/receiver data measurement settings; (3) nonlinear data preprocessing; (4) reformulations of conventional optimization approaches; (5) non-iterative inversion methods."
"1555072","CAREER:  Uncertainty Quantification and Big Data Analysis in Interconnected Systems: Algorithms, Computations, and Applications","DMS","COMPUTATIONAL MATHEMATICS, Division Co-Funding: CAREER","07/01/2016","07/06/2020","Guang Lin","IN","Purdue University","Continuing Grant","Yuliya Gorb","06/30/2023","$400,759.00","","lin491@purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1271, 8048","1045, 9263","$0.00","Uncertainty quantification (UQ) and big data analysis have received increasing attention in recent years. Extensive research effort has been devoted to these topics, and novel numerical methods have been developed to efficiently deal with large-scale data sets and complex problems with uncertainty. Both UQ and big data analysis enable us to better understand the impacts of various uncertain inputs (boundary and initial data, parameter values, geometry, network etc.) to numerical predictions. UQ and big data analysis are thus critical to many important practical problems such as climate modeling, weather prediction, ocean dynamics, and smart grids. As the data size and dimensions of parameter space increase, one of the biggest challenges in UQ computations and big data analysis is the computational cost for analyzing the data and running the simulations. For large-scale complex interconnected systems, deterministic simulations can be very time-consuming, and conducting UQ simulations further increases the simulation cost and can be prohibitively expensive. This project aims to address these critical challenges. A novel set of highly efficient UQ and big data analysis algorithms will be developed to make big data analysis and UQ simulations amenable for large-scale complex interconnected systems. The new algorithms will significantly advance the current state of the art of UQ and big data analysis methods. The project also integrates educational opportunities, including exposing a range of undergraduate students to UQ and big data, giving graduate students the advanced skills needed to apply them, and mentoring Ph.D. students to be leaders in UQ and big data education and research. <br/><br/>The approach under development in this research project is based on scalable algorithms for multivariate Bayesian-treed Gaussian process and power network reduction; high-dimensional UQ algorithms; dynamic state estimation and model calibration for non-Gaussian noisy data; and advanced stochastic contingency analysis. The new algorithms will be based on building multi-fidelity models in both network models and probability space. Such algorithms can accommodate big data in linear time. In addition, while current contingency analysis allows only assessment of a static power grid status without considering uncertainty, the new approaches will allow analysis of contingency dynamically and probabilistically for cascade failures. The new algorithms will allow investigators to establish an efficient framework to rigorously quantify the uncertainty, analyze big data, and endow smart grid simulations with a composite error bar."
"1620472","Randomized Algorithms for Matrix Computations","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","08/17/2016","Per-Gunnar Martinsson","CO","University of Colorado at Boulder","Standard Grant","Leland Jameson","04/30/2019","$249,979.00","","pgm@ices.utexas.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1271","9263","$0.00","This project will develop mathematical techniques for accelerating computational tasks such as simulating electromagnetic scattering,  medical imaging, extracting useful information from large datasets, machine learning, and many others. In all these computations, the step that tends to be the most time-consuming, and which therefore limits how large problems can be solved, concerns the manipulation of large square or rectangular arrays of numbers, called ""matrices"". Many of the matrices that arise in practical applications have redundancies, and can be compressed to enable them to be stored using less space. Using the compressed format, computations involving the matrix can also be greatly accelerated. The problems that will be addressed are deterministic in nature, but the algorithms that will be developed are probabilistic. It turns out that by exploiting certain mathematical properties of large ensembles of independent random numbers, one can build algorithms for compressing matrices that are much faster than traditional deterministic techniques. The new randomized algorithms can in theory fail, but the likelihood of failure can be shown to  be lower than 1 time out of 10,000,000,000 runs in typical applications. Randomized algorithms of this type have recently attracted much interest due to the fact that they perform  particularly well on emerging computing platforms such as mobile computing (where conserving energy is the key priority), computing using graphical processor units (where the vast numbers of computational cores create challenges), and distributed memory parallel computers. The methods also perform very well when applied  to massively large datasets that must be stored on hard drives, or on large server farms. The project will train one doctoral student, and will lead to the release of a publicly available software package that implements the methods that will be developed. <br/><br/>From a technical point of view, the objective of the project is to develop efficient algorithms for factorizing matrices and for solving large linear systems of algebraic equations. The algorithms will be based on randomized sampling, and will exploit remarkable mathematical properties of random matrices and random orthogonal projections. Such randomized algorithms require less communication  than traditional methods, which makes them particularly attractive for modern applications involving multicore processors, distributed computing, out-of-core computing, etc. Specifically, the project will address the following problems: (1) Computing full matrix factorizations (e.g. the so called ""column pivoted QR factorization"") which are core building blocks in scientific computing. Preliminary numerical experiments demonstrate speed-ups of close to an order of magnitude compared to state-of-the-art software packages. (2) Solving linear systems involving many unknowns and many equations. We expect to achieve substantial practical acceleration, and are cautiously optimistic about the possibility to develop solvers with substantially better asymptotic complexity than the cubic complexity achieved by standard techniques. (3) Developing randomized methods for accelerating computational simulations of phenomena such as electro-statics, composite materials, biochemical processes, slow fluid flows, Gaussian processes in 2 and 3 dimensions, etc. Technically, this will be achieved by developing randomized methods for compressing so called ""data-sparse"" or ""rank-structured"" matrices."
"1620003","Collaborative Research:   Multiscale Study of Active Cellular Matter: Simulation, Modeling, and Analysis","DMS","COMPUTATIONAL MATHEMATICS","08/15/2016","08/16/2016","Matthew Glaser","CO","University of Colorado at Boulder","Standard Grant","Leland Jameson","07/31/2019","$130,000.00","Meredith Betterton","matthew.glaser@colorado.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1271","9263","$0.00","Active cellular matter is the basis of novel synthetic active fluids made of mixtures of suspended cytoskeletal filaments and molecular motors. By consuming chemical fuel, the molecular motors (e.g., kinesins) can bind to and create actively moving crosslinks between the biofilaments (e.g., microtubules) to drive their relative motion, which leads to large-scale collective motions in the filament/motor mixture through hydrodynamic coupling. Synthetic active suspensions made of small numbers of components reveal how higher-order aspects of assembly and organization are built in living cells. These systems also present new challenges to our understanding, design, and analysis of materials, and have the potential to provide valuable new technologies such as autonomously moving and self-healing materials.<br/><br/>In this work, the investigators study active cellular matter composed of microtubules and molecular motors through multiscale methods, and tightly coupled modeling, analysis, and simulation. The project aims to understand the fundamental interactions underlying stress generation within bundles of rigid/flexible biofilaments that undergo dynamic instability, as well as the nonlinear dynamics and hierarchical pattern formation in large-scale collective motions. The project will also predict key material properties including its coherent structures, local heterogeneity, time- and length-scales, and material rheology. To resolve the physics at different length- and time-scales, several methods will be developed and integrated: (1) microtubule-motor interactions will be simulated using a kinetic Monte Carlo method; (2) the hydrodynamic interactions between objects of various shapes will be modeled using a nonlocal slender body/boundary integral method, together with fast summation methods; (3) a pseudo-spectral method will be implemented to simulate the collective motion through a continuous active liquid-crystal type model."
"1620070","Collaborative Research:  Algorithms for Large-scale Stochastic and Nonlinear Optimization","DMS","COMPUTATIONAL MATHEMATICS","08/01/2016","06/17/2016","Richard Byrd","CO","University of Colorado at Boulder","Standard Grant","Leland Jameson","07/31/2020","$136,371.00","","richard@cs.colorado.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1271","9263","$0.00","The promise of artificial intelligence has been a topic of both public and private interest for decades. Starting in the 1990s the field has been benefited from the rapidly evolving and expanding field of machine learning. The intelligent systems that have been borne out of machine learning, such as search engines, recommendation platforms, and speech and image recognition software, have become an indispensable part of modern society.  Rooted in statistics and relying heavily on the efficiency of numerical algorithms, machine learning techniques capitalize on increasingly powerful computing platforms and the availability of very large datasets.  One of the pillars of machine learning is mathematical optimization, which, in this context, involves the computation of parameters for a system designed to make decisions based on yet unseen data. The goal of this project is to develop new optimization algorithms that will enable the continuing rise of the field of machine learning.<br/>  <br/>The research consists of two projects, which are thematically related and address the solution of optimization problems that are nonlinear, high dimensional, stochastic, involve very large data sets and in some cases are non-convex. Two families of algorithms will be developed to garner the benefits of both stochastic gradient methods and batch methods, while avoiding their shortcomings. One of these algorithms uses a gradient aggregation approach that re-uses gradient values computed at previous iterations. The challenge is to design an algorithm that is efficient in minimizing testing error, not just training error. The second approach employs adaptive sampling techniques to reduce the noise in stochastic gradient approximations as the optimization progresses. An important aspect of this research is the design of an efficient strategy for incorporating second-order information that captures curvature of the optimized loss function, even in the case when Hessian estimates are based on inaccurate gradients. In all cases, the goal is research is to design and implement algorithms in software, and test them on realistic machine learning applications."
"1620082","Accurate Preconditioing for Computing Eigenvalues of Large and Extremely Ill-conditioned Matrices","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","06/19/2018","Qiang Ye","KY","University of Kentucky Research Foundation","Continuing Grant","Leland Jameson","08/31/2020","$225,000.00","","qye3@uky.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","1271","9150, 9263","$0.00","Computations of eigenvalues of large matrices arise in a wide range of scientific and engineering applications, including, for example, page ranking of the Google Search Engine. Large scale eigenvalue problems are often inherently ill-conditioned which implies that their eigenvalues differ vastly in  magnitude. This poses a significant challenge to the existing eigenvalue algorithms in the sense that smaller eigenvalues computed may have a poor accuracy, caused by roundoff errors in computer arithmetic. This project will develop new algorithms to address this numerical difficulty. The research results will have applications in a variety of problems where extreme ill-conditioning arises. In particular, a notable ill-conditioning problem is the biharmonic differential operator, which has been used in modeling and design of rigid elastic structures such as beams, plates, or solids, in constructions of multivariate splines, as well as in geometric modeling and computer graphics. A discrete version of the biharmonic operator has also found applications in circuits, image processing, mesh deformation, and manifold learning. With the discretized biharmonic operators easily becoming extremely ill-conditioned, this research will resolve the numerical accuracy issues of  the existing algorithms for these applications.<br/><br/>Computing smaller eigenvalues of large and extremely ill-conditioned matrices is an important and intellectually challenging task. Indeed, the effect of ill-conditioning on accuracy is often regarded as an unsolvable problem that is attributable to the formulation of the eigenvalue problem itself. While recent research results have shown that this may be mitigated by exploring structures of matrices, the main objective of this project is to propose an innovative use of preconditioning as a new general methodology to solve the accuracy issue caused by ill-conditioning. We will develop new methods that combine preconditioning with accurate structured inversion methods to accurately compute smaller eigenvalues of an extremely ill-conditioned matrix. As an application, we will also study various discretization schemes and derive suitable structured preconditioners for biharmonic differential operators."
"1606334","Algebraic Theory of Differential and Functional Equations: from Foundations to Computation","DMS","ALGEBRA,NUMBER THEORY,AND COM, FOUNDATIONS, COMPUTATIONAL MATHEMATICS","05/01/2016","03/17/2016","Alexey Ovchinnikov","NY","CUNY Queens College","Standard Grant","James Matthew Douglass","04/30/2017","$35,000.00","Alice Medvedev","aovchinnikov@qc.cuny.edu","6530 KISSENA BLVD","FLUSHING","NY","113671575","7189975400","MPS","1264, 1268, 1271","7556","$0.00","The Differential Algebra and Related Topics (DART) VII conference will be held at the CUNY Graduate Center and City College, 09/30-10/04/2016, in New York City. The subject of differential algebra concerns the study of differential and difference equations from an algebraic point of view. The study of these types of equations has many applications outside of mathematics, including biology (population dynamics and cellular biology), chemical reactions, and physics. The aim of the conference is to bring together researchers who develop theory for these types of equations and those who develop algorithms for working with them.<br/><br/>The DART international conference series started in 2000 and has become an annual activity in the recent years. This research area includes differential and difference Galois theories, differential and difference elimination, symbolic computation in commutative algebra and algebraic geometry, and model theory. The conference will foster the new interactions between researchers in differential and difference algebra and researchers in model theory, algebraic geometry, and commutative algebra. Special attention will be paid to mentoring the participants who are junior researchers. Additional information can be found at the conference website: http://qcpages.qc.cuny.edu/~aovchinnikov/DART7/index.html"
"1620128","Micro-Macro Decomposition Numerical Schemes for Multiscale Simulation of Plasma","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","08/01/2018","James Rossmanith","IA","Iowa State University","Continuing Grant","Leland Jameson","08/31/2020","$196,035.00","","rossmani@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1271","9263","$0.00","Plasma physics is the study of the dynamics of ionized gases. Because plasma, often referred to as the fourth state of matter, is the most abundant form of ordinary matter in the universe, there exist many important application problems for which an understanding of the plasma dynamics is critical. Some examples of these applications include understanding the dynamics of stars, the affect of the solar wind on the Earth's magnetosphere, the dynamics of magnetically confined fusion devices, and the dynamics of laser-plasma devices that could be used in medical imaging applications. The objective of this research is to develop highly accurate and efficient computational tools for simulating the dynamics of the electrons (negatively charged particles) and the ions (positively charged particles) that constitute the plasma of interest. This research aims to develop novel computational techniques based on novel multiscale methods that divide the underlying equations into macroscopic (large scale phenomena) and microscopic (small scale phenomena), and couple these scales in some appropriate manner. These methods will be implemented in computer code that will take advantage of modern parallel computer architectures. The resulting methods and codes will be used to simulate various application problems in order to verify and validate the approach.<br/><br/>The primary objective of this research is to develop accurate and efficient computational methods for solving nonlinear differential equations used to model plasma dynamics. The goal is to solve a class of multiscale models of plasma using a novel micro-macro decomposition approach. The idea behind the micro-macro decomposition is to start with a general enough model that contains the coupled dynamics of macroscopic and microscopic phenomena, to then write this model into two parts with appropriate coupling terms, and finally to apply potentially different numerical techniques to each part in order to optimize efficiency. The specific schemes that will be used in this research are based on high-order discontinuous Galerkin finite element methods with novel time-stepping strategies to achieve computational efficiency. This research will develop new computational tools for simulating plasma dynamics. In particular, new micro-macro decomposition techniques will be designed and implemented that allow for the efficient numerical solution of multi-species plasma systems. New strategies will be developed to adaptively turn on and off the microscopic solvers. As part of the research project, novel computational methods and software will be produced that in the future could be applied to a wide range of both laboratory and astrophysical plasma problems. The software developed will be made freely available on the web as part of the DoGPack software project."
"1620406","Collaborative Research: Efficient Modeling of Incompressible Fluid Dynamics at Moderate Reynolds Numbers by Deconvolution LES Filters Analysis and Applications to Hemodynamics","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","07/20/2018","Alessandro Veneziani","GA","Emory University","Continuing Grant","Leland Jameson","06/30/2019","$180,001.00","","ale@mathcs.emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1271","9263","$0.00","Computational fluid dynamics has emerged as a powerful tool to study the physiopathology of the cardiovascular system and for patient-specific Surgical Planning (SP) for cardiovascular diseases. Recently, clinical trials - the standard procedure for understanding diseases and assessing the impact of therapies and devices in the clinical practice - have been supported by a massive use  of numerical simulations to improve the knowledge extracted from measured data, leading to Computer Aided Clinical Trials (CACT). For a large variety of pathologies involving the aorta - the major artery of the circulation - this requires to work with turbulent flows. While Direct Numerical Simulation in this context can be appropriate for a proof of concept, for the large number of patients involved in CACT and SP we need different numerical tools to provide the appropriate trade-off between accuracy and reliability needed by clinical applications and computational efficiency needed by tight deadlines. As CACT and SP are new emerging concepts in cardiovascular mathematics, an appropriate numerical modeling of turbulent physiological flows for clinical applications is now an unmet need that we intend to solve in this proposal.<br/><br/>A possible way to limit the computational costs associated with Direct Numerical Simulations without sacrificing accuracy is to solve the flow average and model properly the effects of the small scales (not directly solved) at the medium and large scales (solved). We intend to investigate carefully new cutting-edge methods for disturbed flows based on Large Eddy Simulation (LES) Deconvolution filtering techniques with the ultimate goal of enabling practical use of numerical tools to improve knowledge extraction and clinical practice through CACT and SP. The main objective of this research is the development and the analysis of a robust and accurate LES based approach requiring no or minimal user's set-up for realistic incompressible flow problems with application to computational hemodynamics. We articulate the project in the following points: (a) Sensitivity analysis of key parameters involved in the method to understand their impact on the solution, leading to an automated parameter set-up through physical and numerical arguments. (b) Development and analysis of high-order in time methods, particularly for the computation of the pressure, with consequent improvement of the mass conservation properties. (c) Analysis of the impact of our LES approach on non-Dirichlet boundary conditions and the possible backflow stabilizing effects.  We plan to test the method on both academic and real bioengineering problems. Finally, we plan to deliver a finite element open source library incorporating the findings of our research, available for CACT and SP."
"1620346","Geometric and algebraic multigrid solvers for coupled systems of PDEs and PDE eigenvalue problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","09/06/2016","James Brannick","PA","Pennsylvania State Univ University Park","Standard Grant","Leland Jameson","08/31/2019","$160,000.00","","brannick@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1271","9263","$0.00","The primary goal of this project is to develop a bootstrap multigrid algorithmic framework that has the potential to make an appreciable and broad impact on computational methods for numerically solving partial differential equations (PDEs).  The intellectual merit of the project derives from its potential to make several distinct theoretical advances in the design and analysis of geometric and algebraic multigrid methods and to integrate those advances into algorithms and software for large-scale scientific applications that require solving coupled PDE systems and PDE eigenvalue problems. The broader impact of the project will be realized by applying these new algorithms to various problems in science and engineering. The graduate student and post doc involved in the project will engage in interdisciplinary research led by the PI and will have opportunities to visit and work with colleagues from industry and DOE labs. <br/><br/>This research project builds on recent advances by the PI in the development of multigrid solvers for systems of and PDE eigenvalue problems: (1) the successful development of adaptive and bootstrap algebraic multigrid as fast solvers for the Wilson-Dirac and Wilson-clover discretizations of the coupled Dirac PDE in lattice quantum chromodynamics (QCD); (2) the design and analysis of a robust bootstrap MG solver for the Laplace-Beltrami eigenvalue problem. Specifically, the project team will focus on two interrelated research goals: (1) to extend the bootstrap algebraic MG methods currently being used to solve various discretizations of the Dirac PDE to a general approach for solving systems of coupled PDEs and generalized algebraic eigenvalue problems; (2) to design and analyze new finite element bootstrap MG methods for solving PDE eigenvalue problems on surfaces."
"1620403","MULTISCALE STOCHASTIC REACTION-DIFFUSION ALGORITHMS","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/20/2018","Hye Won Kang","MD","University of Maryland Baltimore County","Continuing Grant","Leland Jameson","06/30/2020","$199,993.00","","hwkang@umbc.edu","1000 HILLTOP CIR","BALTIMORE","MD","212500001","4104553140","MPS","1271","9263","$0.00","Stochastic reaction-diffusion processes are widely used in the modeling of cancer, immune systems, and developmental biology. These processes describe the concentration changes of the chemically reacting species that diffuse through space. Numerical simulation of stochastic reaction-diffusion processes is challenging since the underlying biochemical systems are large in general and naturally involve various scales in the speed of chemical reaction and diffusion and their quantities in different locations. The multiscale nature can slow down the speed of the numerical simulation. Moreover, stochastic simulation requires a set of repeated performance of the simulations to obtain averaged behavior of chemical species. The goal of this project is to develop efficient numerical algorithms for stochastic reaction-diffusion processes by coupling different numerical schemes based on scales and by applying optimal strategies to increase accuracy and efficiency. The ability to simulate large stochastic systems will provide an efficient tool for modeling and understanding of complex biochemical systems. A graduate-level course in the field related to this project will be redesigned. The undergraduate and graduate students will participate in the project and will receive mentorships.<br/> <br/>This project focuses on the development and the analysis of multiscale numerical algorithms for stochastic reaction-diffusion processes combining different numerical schemes. Markov chain models are widely used to model chemically reacting species with diffusion, but the exact simulation of Markov chain models for large systems are computationally expensive when the systems involve multiscale phenomena. There are many studies to develop and to understand multiscale methods for stochastic reaction-diffusion processes using Markov chain models, but the major drawback in the existing methodologies is that they do not fully account for significant changes in the abundances of chemical species in time and space, which reduce the accuracy of the approximations. In this project, a spatial domain of interest will be divided into several subsets based on the abundance of chemical species and Markov chain models and stochastic partial differential equations will be respectively applied to the different regions. Then, the method will be extended to incorporate moving interfaces between the numerical schemes, which change in time, and to use optimal strategies to find a location of the next reaction. The fast simulation skills of large stochastic reaction-diffusion systems will increase efficiency in the study of experimental science and engineering. Moreover, the results of the proposal will be applied to explore the impact on human development or disease, for example, to find key signaling pathways in cancer or immune systems and to find how the tissues and organs develop to have different functions in the embryo."
"1547399","RTG: Coding Theory, Cryptography, and Number Theory","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS, WORKFORCE IN THE MATHEMAT SCI, EPSCoR Co-Funding","08/01/2016","06/09/2020","Felice Manganiello","SC","Clemson University","Continuing Grant","Andrew Pollington","07/31/2023","$2,126,971.00","Felice Manganiello, Felice Manganiello, Shuhong Gao, Kevin James, Gretchen Matthews","manganm@clemson.edu","201 SIKES HALL","CLEMSON","SC","296340001","8646562424","MPS","1264, 1271, 7335, 9150","7301, 7433, 7795, 9150, 9251","$0.00","The goal of this project is to enhance the development of students in the fields of coding theory, cryptography, and number theory. Activities with strong vertical integration components are introduced at the undergraduate, graduate, and postdoctoral levels to improve recruitment, retention, and training of students in these areas, including those from historically black colleges and universities and small colleges in the region, and to provide research growth and facilitate professional development for postdocs and early career faculty.  Activities supported by this grant will give broader access to the latest research methods and results in coding theory, cryptography, and number theory, areas replete with theoretical advances and practical applications.<br/><br/>The research problems in this proposal center on automorphic forms and codes, modular forms and algebraic geometry codes, and cryptography and computational number theory. These topics allow entry for students via computation and exploration and offer the opportunity for both theoretical advances and practical applications. Activities in this Research Training Group project include curriculum and condensed research experiences designed to better prepare students transitioning from coursework to research; exam preparation modules to support students entering the graduate school admissions phase as well as those applying for candidacy; and working groups to broaden the research expertise of advanced graduate students and postdocs beyond their dissertation topics in addition to professional development and networking opportunities. The training through research involvement provided by this Research Training Group in Coding Theory, Cryptography, and Number Theory will develop a stronger and more diverse workforce."
"1620014","Real algebraic and combinatorial structures in matrix spaces","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","08/30/2016","Cynthia Vinzant","NC","North Carolina State University","Standard Grant","Leland Jameson","08/31/2020","$150,000.00","","vinzant@uw.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1271","9263","$0.00","This project aims to apply mathematical tools to study objects from optimization, engineering, and statistics. The research involved will lead to a better understanding of the underlying mathematics and further the development and analysis of algorithms. Development of such theory benefits both the mathematical and broader scientific communities and strengthens the connections between them. Another important part of this project involves mentoring undergraduate and graduate students. Participation in this project will expose students to a wide range of fields in mathematics, impress upon them the importance of interdisciplinary research, and train them in the theory and computational skills needed to carry it out.<br/><br/>The broad goals of the proposed research are to develop computational techniques in real algebraic geometry and combinatorics in order to study objects arising in optimization and other applications. Real algebraic geometry is the study of solutions to polynomial equations and inequalities, which can describe a vast array of systems from engineering and science. Tropical geometry is a powerful tool for extracting discrete data from real algebraic sets as well as for constructing examples of real algebraic sets with desired properties. The proposed research applies these powerful mathematical tools to problems in linear algebra and convex optimization. Stable polynomials, determinants, and linear spaces of matrices are fundamental objects in these fields. A better understanding of their underlying geometry would advance these fields and the connections between them."
"1620335","Positivity preserving limiter and new development on elliptic interface problems","DMS","COMPUTATIONAL MATHEMATICS","06/15/2016","06/15/2016","Jue Yan","IA","Iowa State University","Standard Grant","Leland Jameson","12/31/2020","$150,000.00","","jyan@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1271","9263","$0.00","This research will contribute to the fundamental understanding of algorithm development that are widely applied in vehicles designs and aerospace engineering. The PI and her collaborators will perform mathematical studies on the performance and properties of the algorithms. Specifically the proposed method assists to stabilize the algorithm implementation such that numerical studies of physical properties, for example the pressure distributions around fast flying airplanes, can be carried out. Comparing with lab experiments, computer simulations are way less expensive and more efficient. In this project, the PI also develops a new idea and new algorithms for interface problems. One application is the compound materials studies in which highly accurate and efficient solvers are demanded. The investigator will integrate research with education activities and communicate the research in a broader context.<br/>  <br/>The PI will develop third order maximum principle satisfying limiter for general convection diffusion equations. The objective is to prove the high order polynomial solutions staying in the given bounds without losing accuracy. The PI further extends the studies to obtain positivity preserving limiter for compressible Navier-Stokes equations. In this project, the PI also develops new methods to solve elliptic interface problems with mesh either aligned to or cut through the interface. The research is based on the direct discontinuous Galerkin methods previously designed by the PI. With the extra flexibility on the numerical flux formula, the PI manages to prove the quadratic polynomial numerical solutions satisfying strict maximum principle on unstructured triangular meshes with at least third order of accuracy. There is no geometric restriction on the meshes and obtuse triangles are allowed. The PI will prove the density and pressure approximations to compressible Navier-Stokes equations being maintained positive at all time levels. As a by-product, bounding the polynomial solutions or preserving the solution's positivity can be considered as a strong stability result. The findings of this research will improve the capability of a numerical method to those challenging problems from computational fluid dynamics. For elliptic interface problems, the PI will modify the numerical fluxes defined at element edges to implicitly enforce the interface solution jump and flux jump conditions."
"1554907","CAREER: Computational tools for the analysis of large stochastic networks","DMS","COMPUTATIONAL MATHEMATICS, Division Co-Funding: CAREER","06/01/2016","07/06/2020","Maria Cameron","MD","University of Maryland, College Park","Continuing Grant","Leland Jameson","08/31/2021","$400,000.00","","cameron@math.umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1271, 8048","1045, 9263","$0.00","The contemporary development of communications, information technologies and powerful computing resources has made networks a popular tool for data organization, representation and interpretation. Networks allow us to create mathematically tractable models that preserve important features of the underlying systems and avoid problems associated with high dimensionality and complex geometry. In particular, networks have demonstrated a strong promise as a modeling and analysis tool of complex physical processes such as protein folding, self-assembly of clusters of interacting particles, and walks of molecular motors. The development of computational methods for analysis and construction of complex networks has a potential to advance the understanding of crystal growth and lead to industrial self-assembly based design of structures consisting of interacting particles. The proposed work will help in the scientific development of undergraduate, graduate and post-doctoral students as well as help in the enhancement of the curriculum at the PI's home institution. The project will also make originally developed software through a website for public use. <br/><br/>The proposed research program is concerned with the development of computational tools for analysis and construction of stochastic networks with exponentially small pairwise transition rates. The parameter in the formula for the pairwise transition rates is the absolute temperature in the physical context which is usually small relative to important energetic barriers present in the system under consideration. Typically, networks coming from modeling complex physical systems are large (e.g. a million states), sparse and unstructured, and their pairwise transition rates vary by tens of orders of magnitude. As a result, their analysis is difficult due to their complexity and severe issues associated with the floating point arithmetic. The goal of the research program is the development of computational methods for the following four problems: (1) asymptotic analysis of large stochastic networks (in particular, finding asymptotic estimates for the eigenvalues and eigenvectors, extracting quasi-invariant and metastable subsets of states, and building coarse-grained models); (2) finite temperature continuation of specific eigenpairs describing particular transition processes of interest in the considered network; (3) building stochastic networks representing aggregation processes of interacting particles; and (4)finding the quasi-potential, the function quantifying the dynamics of time-irreversible systems driven by small thermal noise and allowing to convert continuous non-gradient systems with multiple attractors to stochastic networks. The analytical components of the proposed research lie in the interface of the Large Deviation theory and the graph theory. The numerical components involve network algorithms, numerical linear algebra, optimization methods, methods for finding saddle points, Monte-Carlo methods, and solvers for the Hamilton-Jacobi type equations."
"1620318","Adaptive Multiscale Simulation Framework for Reduced-Order Modeling in Perforated Domains","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","05/11/2021","Yalchin Efendiev","TX","Texas A&M University","Continuing Grant","Leland Jameson","06/30/2021","$210,000.00","Raytcho Lazarov","efendiev@math.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1271","9263","$0.00","Processes in perforated domains occur in many important applications. These include complex processes in soil, membranes, and filters. With current imaging techniques, detailed microscale geometries of these perforated materials can be constructed. However, it is prohibitively expensive to solve complex processes in these perforated domains due to a rich hierarchy of scales. For this reason, some types of reduced-order computational techniques are needed. The goal of this project is to develop and analyze novel computational techniques for solving challenging multiscale problems in perforated domains. The new approaches will bring the information from the detailed geometries to large-scale simulations and will improve the predictions in the simulations. This will further allow deigning new materials and optimize processes.<br/> <br/>Many current approaches for multiscale methods for problems in perforated domains have been restricted to homogenization, which is applicable when the media has scale separation. However, in many realistic perforated media, there is no scale separation, i.e., pore sizes can have a wide variety of scales. The multiscale methods of this project develop a general framework that allows rigorous and systematic reduction. The PI's goals are: (1) to develop systematic local model reduction tools for computing multiscale basis functions; (2) to develop and analyze new finite element techniques using these basis functions; (3) to study the interplay between localization of the basis functions and the global coupling mechanism; (4) to apply the developed methods to a wide variety of flows with nonlinearities and multiphysics in 3D; (5) to test and demonstrate their capabilities for solving problems in engineering and geosciences."
"1619640","Low-rank Diagonally Compensated Matrix Decompositions in the Design of Solvers and Partitioners","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","08/30/2016","Panayot Vassilevski","OR","Portland State University","Standard Grant","Leland Jameson","08/31/2019","$159,998.00","","panayot@pdx.edu","1600 SW 4TH AVE","PORTLAND","OR","972015522","5037259900","MPS","1271","9263","$0.00","The computational simulation of many problems in science and engineering requires the use of black-box matrix solvers. The main challenge in the development of reliable black-box solver lies on the unknown origin of the problems that need to be solved. This project will provide a new approach to identify the nature and the origin of the problems and then develop efficient black-box matrix tools. The results will enable researchers to make their computational simulators more feasible at large scale. In particular, problems formulated on large-scale graphs such as those arising from social and information network sciences will be more successfully approached. <br/><br/>This project offers a unifying step towards building black-box solvers and preconditioners for sparse symmetric positive definite matrices at large scale that have the potential to have desired optimal complexity and be efficient in practice. More specifically, this project proposes the use of a general matrix decomposition into a sum of an easily invertible matrix minus a sum of low-rank symmetric positive semi-definite matrices. The decomposition offers several directions for research. In the case of sparse symmetric positive definite matrices the decomposition naturally defines components for genuine black-box sparse matrix solvers including alternatives to previously studied support graph preconditioners as well as natural components for algebraic multigrid (or AMG) methods, such as convergent smoothers and tools for creating accurate coarse vector spaces. Also, the project offers new algorithms for partitioning (of graphs and finite element meshes), which are dependent on the given matrix entries. For example, if the matrix represents a discrete analog of some physical phenomenon (such as subsurface flow, electromagnetic field, elastic body), the resulting mesh partitioners will reflect the underlined physical process. The broader impact of the project is that its results can have the potential to enable researchers, including ones outside the area of numerical PDEs that are at the interface of discrete mathematics and informatics, to make their simulations more feasible at large scale. In particular, problems formulated on large-scale graphs (arising in sciences and social and information networks) could be more successfully approached. By providing the (algorithmic) tools  with rigorous theoretical foundation developed by the project, it will have the potential for solid impact in practice, both in education and research."
"1619907","Inverse Problems in Optical Imaging","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","07/03/2016","John Schotland","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Leland Jameson","06/30/2019","$300,000.00","","john.schotland@yale.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1271","8990, 9263","$0.00","Advances in imaging technologies such as computed tomography (CT) and magnetic resonance imaging (MRI) have transformed the practice of clinical medicine and basic biomedical research. Although the development of such technologies is well known to depend upon progress in physics and engineering, it is less well known that applied and computational mathematics has also played an essential role. This research project studies mathematical questions that arise in new medical imaging modalities that make use of near-infrared light either alone, or in combination with ultrasound. The research will study novel mathematical algorithms that will lead to improvements in optical imaging both with respect to resolution (visualizing structures at smaller scales) and computational speed. In particular, the project aims to devise robust and accurate image reconstruction algorithms that may lead to the detection and characterization of disease at much earlier stages than is currently possible.<br/><br/>The objective of this research is to investigate inverse problems that arise in biomedical optical imaging. The project involves two components. (i) Development of mathematically-justified image reconstruction algorithms for optical tomography with diffuse light. The analysis will be carried out within the framework of radiative transport theory. The project aims to develop reconstruction methods based on topological reduction of the inverse Born series, for recovering the absorption and scattering coefficients of the radiative transport equation from boundary measurements, and to characterize the convergence and approximation error of these methods. It is planned to employ these results to develop and test fast reconstruction algorithms suitable for use with a noncontact optical tomography system.  (ii) Study of inverse problems that arise in acousto-optic imaging. This recently developed imaging modality combines the high-resolution of ultrasound imaging with the physiologically important contrast of optical imaging. The work includes theoretical analysis of related inverse scattering problems, together with numerical tests of associated image reconstruction algorithms. In particular, the project aims to develop reconstruction methods for recovering the absorption and scattering coefficients of the radiative transport equation from acousto-optic measurements."
"1619901","OP: COLLABORATIVE RESEARCH: Integrated Simulation of Non-homogeneous Thin-film Photovoltaic Devices","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/08/2016","Akhlesh Lakhtakia","PA","Pennsylvania State Univ University Park","Standard Grant","Leland Jameson","06/30/2020","$210,000.00","","akhlesh@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1271","8396, 8399, 8990, 9263","$0.00","Solar cells are desirable as energy sources that neither use fossil fuels nor produce greenhouse gases. They must be designed to efficiently absorb sunlight and convert it to electricity. The solar cell needs to be sufficiently thick to absorb light across the solar spectrum. To reduce this thickness, and so reduce manufacturing cost, several layers of materials are used: first, to help light penetrate the solar cell, and then to trap it inside. Some of these layers are semiconductors in which electricity is generated, while others (for example, a periodically corrugated metallic back layer) may help absorption by trapping light near their surface. However, the amount of electricity generated by a solar cell does not just depend on absorption, but also on the transport of electrons within the layers of the solar cell. If the density of electrons decreases during transport, thereby trumping any gain in sunlight absorption, then a chosen light-management strategy will not be fruitful. The project team will develop an integrated pair of computer models that simultaneously predict the absorption of sunlight and the consequent electrical performance of the solar cell using modern techniques from numerical analysis. The codes will extend current simulation technology to allow for semiconductor layers with properties that vary from place to place and allow fully three-dimensional models of the device. Using their codes, the PIs will optimize device designs for best electricity generation. Definitive predictions will be provided about thin-film photovoltaic solar cells, thereby providing significant progress towards inexpensive and sustainable production of electricity. These codes will be made available to other photovoltaic researchers.<br/><br/>The overall model of the thin-film photovoltaic solar cell will have a photonic submodel and an electrical submodel. In the photonic model, the quasi-periodic Maxwell's equations will be solved using edge finite elements. To improve flexibility, the PIs will analyze and implement a non-standard mortaring technique to take care of quasi-periodicity. Compatible electrical models will be analyzed and implemented using the Hybridizable Discontinuous Galerkin (HDG) method on hexahedral elements for the non-linear convection and diffusion problem governing drift and diffusion of electrical charge carriers. The HDG scheme will require a novel analysis to understand this non-linear convection-diffusion problem. Stability and convergence will be explored first for linear convection-diffusion problems, and the methodology will then be extended to an implicit-explicit time-stepping scheme for the drift-diffusion system. Besides a full 3D model, the PIs will develop a 2D model assuming translation invariance of the photovoltaic device in one transverse direction.  The second step of the proposed research is to use the new simulation capability to design optimal nonhomogeneous thin-film photovoltaic solar cells via the Differential Evolution Algorithm. This will optimize for maximal photovoltaic electricity-generation efficiency. Additionally, domain and coefficient derivatives will be characterized and implemented to allow the computation of sensitivities and the use of gradient-based optimization. Detailed photonic-and-electrical modeling with doubly periodic back-reflectors and non-homogeneous light-absorbing layers will permit a major expansion of solar-cell design methodologies, besides yielding optimal designs for maximal photovoltaic electricity-generation efficiency."
"1619661","Data-driven Modeling of Equilibrium and Non-equilibrium Statistics","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","08/29/2016","John Harlim","PA","Pennsylvania State Univ University Park","Standard Grant","Leland Jameson","08/31/2019","$300,867.00","Xiantao Li","jharlim@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1271","8012, 9263","$0.00","An important issue in applied and computational sciences is to find the essential reduced models to predict variables of interests from high-dimensional complex dynamical systems. Given our advanced capability to collect big data, an important challenge is to leverage the information carried by the data to improve the modeling effort. Computationally, this requires adequate inference of appropriate parameters such that their uncertainties are quantifiable. A much more challenging yet important issue is to be able to make prediction in the presence of external disturbances. This problem has a wide range of applications such as in climate change science where one is interested to predict the climate change statistics corresponding to exogenous forcing such as the volcanic eruptions or even the anthropogenic factor such as the human activities. The projects in this proposal are to address these issues. While the developed methodology is aimed for general modeling of multi-scale phenomena, our focus will be to improve the understanding and prediction of the deformation behavior of graphene.<br/> <br/>Two projects are proposed: 1. Data-driven reduced modeling paradigms to capture coarse grained statistical solutions of the underlying dynamics. The methodology involves the Mori-Zwanzig formalism, a precise description of the memory effect to take into account the interactions between processes occurring on different physical scales, and a data-driven numerical scheme for estimating the parameters of the stochastic reduced model. 2. Estimation of parameters in the reduced models to predict changes on the statistical solutions in the presence of small external disturbances. This project involves employing the Pad approximation on appropriate integral operators and designing efficient algorithm to solve a system of nonlinear equations that respect appropriate equilibrium statistics of the unperturbed data, leveraging the formulation from the fluctuation-dissipation theory."
"1648171","Innovative Weak Galerkin Finite Element Methods with Application in Fluorescence  Tomography","DMS","COMPUTATIONAL MATHEMATICS","05/16/2016","07/13/2016","Chunmei Wang","TX","Texas State University - San Marcos","Standard Grant","Leland Jameson","12/31/2018","$63,998.00","","chunmei.wang@ufl.edu","601 UNIVERSITY DR","SAN MARCOS","TX","786664684","5122452314","MPS","1271","8990, 9263","$0.00","Fluorescence tomography (FT) is an emerging three-dimensional optical imaging modality that uses in vivo noninvasive depth-resolved localization and quantification of fluorescent-tagged inclusions. FT techniques have been extensively employed in early cancer detection, guidance of tumor resection, and drug monitoring and discovery. This research project aims to develop new numerical methods for computational problems arising in FT imaging. Besides fluorescence tomography, the numerical methods can be applied to solve partial differential equations that arise in various other disciplines. The models, theory, and computational methods under development in this project will be of great value in the fields of digital image processing, medical imaging, and numerical analysis, with direct applications in broad areas such as digital image and even construction industries. Students will be trained in this project.<br/><br/>The goal of this research project is to develop and analyze efficient numerical methods -- weak Galerkin (WG) finite element methods -- to address challenges posed by fourth-order problems arising in fluorescence tomography theory. The research will explore algorithmic advancements, new convergence theory, and imaging technology improvement, by: (1) developing robust finite element methods for a fourth order partial differential equation in the primal variable formulation for which no existing method works; (2) establishing a stability and convergence, including superconvergence, theory for the newly developed finite element methods; (3) validating and verifying the methods through collaboration with domain-specific researchers; (4) analyzing a new weak Galerkin mixed finite element method for a fourth order partial differential equation; and (5) developing application-oriented software packages, tested and validated with collaborators in the area of FT."
"1620316","Collaborative Research: Algorithm and Theory for Interface Computations","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/08/2016","Yoichiro Mori","MN","University of Minnesota-Twin Cities","Standard Grant","Leland Jameson","06/30/2019","$149,998.00","","y1mori@sas.upenn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1271","9263","$0.00","Phenomena in which a fluid interacts with an immersed elastic structure abound in nature and in every day life. Such fluid-structure interaction (FSI) problems include the swimming of microorganisms, the flying of birds, blood flow in the heart and the deformation of leaves to the wind. One powerful way to understand such FSI problems is through computer simulation. Many FSI problems lead to challenging computational problems that require significant improvements over currently available algorithms. The immersed boundary (IB) method is a popular computational method for FSI problems, and one major goal of this project is to understand the mathematical  properties of the IB method to aid in the development of faster and more robust numerical algorithms. Another goal is to adapt the IB method so that it can handle problems in which fluid (water) can flow through an elastic structure. Such problems are particularly important for the understanding of movement and shape changes of biological cells. This cell-biological aspect of our work will be performed in collaboration with experimental biophysicists. The project will introduce and train undergraduate and graduate students in these problems and more broadly in the mathematical and computational sciences. <br/> <br/>This project consists of two major aims in theory and algorithmic development for computational problems with moving membrane interfaces. On the theoretical side, the PIs will establish a convergence theory for the immersed boundary (IB) method. The IB method is a widely used numerical method for fluid structure interaction problems, but despite its popularity, its convergence properties are poorly understood. Convergence analysis for the IB method will be one of the first to be established for a fluid structure interaction algorithm in which a dual grid is used; one for the fluid and another for the elastic structure. Such an analysis will clarify the effect of grid and time discretization parameters on the stability properties of the IB method. On the algorithmic side, the PIs will develop a numerical scheme to handle electrodiffusion of ions and transmembrane water flow in the presence of deformable elastic membranes. A novel feature of the osmotic water flow problem in contrast to conventional fluid structure interaction problems is that the interfacial membrane does not move with respect to the local fluid velocity and that this slip velocity is controlled by the jump in concentration of a diffusing chemical across the membrane interface. The fluid structure interaction will be treated with the IB method whereas chemical diffusion will be treated using a cartesian embedded boundary method. This algorithm will be applied to study the interplay between electrophysiology/osmotic water flow and cell mechanics, an area that is poorly explored theoretically but whose importance is becoming increasingly clear."
"1619911","The 4th International Workshop on Fluid Structure Interaction Problems","DMS","COMPUTATIONAL MATHEMATICS","06/15/2016","06/08/2016","Sheng Xu","TX","Southern Methodist University","Standard Grant","Leland Jameson","05/31/2017","$15,000.00","","sxu@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1271","7556, 9263","$0.00","The 4th International Workshop on Fluid-Structure Interaction Problems (http://www2.ims.nus.edu.sg/Programs/016wfluid/index.php) will be hosted by the Institute for Mathematical Sciences (IMS) at National University of Singapore during 30 May - 3 June 2016. The workshop aims to bring together mathematicians, computational scientists, and engineers having a common interest in solving fluid-structure interaction problems. The ultimate goal is to initiate new research collaborations that improve on existing techniques and generate ideas for new approaches. Funding from the NSF is sought to support the travel of ten US participants, including graduate students, post-docs and junior faculty. <br/><br/>This workshop will address many challenging and important computational and modeling techniques for fluid-structure interaction problems in biological flows, multiphase flows, material sciences, and manufacturing processes. It will provide a platform to exchange, promote and generate ideas across a wide range of research areas in scientific computing and applications. The workshop will also introduce US researchers to a broad international community, expands and creates research collaborations with their international peers, and provides research and career counseling for junior US researchers."
"1619969","Mathematical and Computational Studies in Poroelasticity","DMS","COMPUTATIONAL MATHEMATICS","09/15/2016","07/21/2020","Amnon Meir","TX","Southern Methodist University","Standard Grant","Leland Jameson","08/31/2021","$199,993.00","","ajmeir@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1271","9263","$0.00","The PI will study mathematical models which describe the swelling and shrinking of fluid-saturated, elastic, porous media (elastic solids) and the interactions between the fluid and the elastic porous structures. The significance of the research is due to the fact that many natural substances, e.g., rocks, soils, and even biological tissues, as well as man-made materials such as foams, gels, concrete, and ceramics are such elastic porous media. Thus, research findings will be applicable in a multitude of areas (biomechanics, pharmacology, energy technology, geomechanics, geophysics, and materials science). The models developed can be used to describe various man-made materials and manufacturing processes. They can also be used to model the biomechanics of soft tissues and biological porous media, enabling the modeling and optimization of new therapies, development of new diagnostics tools, and advancing our understanding of human physiology. This research will also improve our capability to model, analyze, and assess the environmental impact of processes associated with hydraulic fracturing, wastewater injection, and carbon capture and storage, and activities associated with production of geothermal energy. Graduate and undergraduate students will be trained and will participate in the work. Research findings, and experience from it, will be incorporated into the graduate and undergraduate curricula.<br/><br/>This research focuses on mathematical and computational issues arising in continuum models of poroelasticity and electroporoelasticity. These provide a unified and systematic treatment of various porous materials and processes which arise in diverse areas of science and engineering and a variety of applications. Additional physical phenomena, such as the electromechanical response of the medium, as well as chemical and/or thermal effects, may also be accounted for. Thus, poroelasticity, electroporoelasticity, or even electro-chemo-thermo-poroelasticity, are all complex coupled, multiphysics, multiscale, phenomena, where the swelling and shrinking of an elastic or viscoelastic deforming porous medium is coupled to the electromechanical (and/or the thermal, and chemical) response of the medium and saturating fluid. Poroelasticity also involves multiple scales; the micro-scale corresponding to the molecular scale, and the scale of continuum mechanics, the macro-scale. The PI will develop mathematical and computational tools and study, analytically and computationally, various mathematical problems arising in poroelasticity. Among the issues considered will be the well posedness of mathematical pde models and the derivation and analysis of efficient and accurate numerical algorithms for approximating solutions of these partial differential equations."
"1620112","From Non-Asymptotic to Nonlocal Homogenization of Electromagnetic Metamaterials","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/15/2016","Igor Tsukerman","OH","University of Akron","Standard Grant","Leland Jameson","06/30/2021","$210,000.00","","itsukerman@uakron.edu","302 BUCHTEL COMMON","AKRON","OH","443250002","3309722760","MPS","1271","8990, 9263","$0.00","The last two decades have witnessed an explosion of interest in metamaterials (MM) -- artificial structures judiciously designed to control wave propagation and to produce physical effects not available in natural materials. These unusual physical phenomena include perfect lensing, negative refraction, and cloaking. According to Google Scholar, over 90,000 research papers, book chapters and books have been devoted to the science and applications of MM. Analysis and design of MM relies on their large-scale (""macroscopic"") parameters and on effective medium theories (""homogenization"") needed to obtain these parameters. The behavior of typical MM is nonlocal: that is, their response at a given point in space may depend on the excitation at a different point. Yet mathematical and numerical homogenization methods for nonlocal regimes are scarce and insufficient. This project will remove this critical impediment to further progress in MM. Existing homogenization theories, some of which date back to classical physics of the 19th century, will thereby be significantly extended. The broad technical impact will be in the field of MM, not necessarily electromagnetic. Nonlocal homogenization will allow us to establish unambiguously which applications of metamaterials are practically feasible and to optimize the performance of metamaterial devices. The broader impact will also be in the science of nonlocal media, for example nonlocal electrostatics of biomolecules in solvents, which has applications in protein modeling and drug discovery.<br/><br/>The chief objective of this research is to advance the analysis, simulation and applications of electromagnetic metamaterials by developing a nonlocal two-scale homogenization theory. This theory involves Trefftz approximations of the electromagnetic fields on both coarse and fine scales, and a judiciously chosen set of degrees of freedom. This is a substantial, and necessary, extension not only of traditional theories, but also of the non-asymptotic but local theory the PI has developed in recent years. For canonical examples such as layered media or photonic crystals, the PI will demonstrate a consistent order-of-magnitude accuracy improvement in the transmission/reflection coefficients. The intellectual merit of the this research is in the development of a new paradigm of nonlocal homogenization, of new computational methods related to it, and in the application of new methodology to electromagnetic metamaterials."
"1620022","Collaborative Research:   Algorithms for Large-Scale Stochastic and Nonlinear Optimization","DMS","OE Operations Engineering, COMPUTATIONAL MATHEMATICS","08/01/2016","06/17/2016","Jorge Nocedal","IL","Northwestern University","Standard Grant","Leland Jameson","07/31/2020","$270,000.00","","j-nocedal@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","006Y, 1271","071E, 072E, 073E, 077E, 078E, 9102, 9263","$0.00","The promise of artificial intelligence has been a topic of both public and private interest for decades. Starting in the 1990s the field has been benefited from the rapidly evolving and expanding field of machine learning. The intelligent systems that have been borne out of machine learning, such as search engines, recommendation platforms, and speech and image recognition software, have become an indispensable part of modern society.  Rooted in statistics and relying heavily on the efficiency of numerical algorithms, machine learning techniques capitalize on increasingly powerful computing platforms and the availability of very large datasets.  One of the pillars of machine learning is mathematical optimization, which, in this context, involves the computation of parameters for a system designed to make decisions based on yet unseen data. The goal of this project is to develop new optimization algorithms that will enable the continuing rise of the field of machine learning.<br/>  <br/>The research consists of two projects, which are thematically related and address the solution of optimization problems that are nonlinear, high dimensional, stochastic, involve very large data sets and in some cases are non-convex. Two families of algorithms will be developed to garner the benefits of both stochastic gradient methods and batch methods, while avoiding their shortcomings. One of these algorithms uses a gradient aggregation approach that re-uses gradient values computed at previous iterations. The challenge is to design an algorithm that is efficient in minimizing testing error, not just training error. The second approach employs adaptive sampling techniques to reduce the noise in stochastic gradient approximations as the optimization progresses. An important aspect of this research is the design of an efficient strategy for incorporating second-order information that captures curvature of the optimized loss function, even in the case when Hessian estimates are based on inaccurate gradients. In all cases, the goal is research is to design and implement algorithms in software, and test them on realistic machine learning applications."
"1620342","Collaborative Research:   Prediction, Optimization and Control for Information Propagation on Networks: A Differential Equation and Mass Transportation Based Approach","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","09/05/2016","Xiaojing Ye","GA","Georgia State University Research Foundation, Inc.","Standard Grant","Leland Jameson","08/31/2019","$99,770.00","","xye@gsu.edu","58 EDGEWOOD AVE NE","ATLANTA","GA","303032921","4044133570","MPS","1271","9263","$0.00","Have you ever been wondering how fast news spreads or a topic becomes trendy in online social networks? It is actually very challenging to answer such questions quantitatively and accurately. The difficulty, in mathematical language, is that the process takes place in extremely large heterogeneous networks, and the spreads exhibit pervasive randomness. For example, a Twitter user may retweet a post at literally any time, or just ignore it. Therefore, understanding and predicting the spread of trendy topics are among the most emerging problems in social networking. The study also has applications in smartphone/computer malware outbreak and epidemiology of infectious disease since the spreads share similar mathematical underpinning. Therefore, we use a general notion of information propagation on networks to describe the dynamic nature of those problems. The 'information' being propagated on networks can be a trendy topic, a new computer malware, or an infectious disease; the nodes can be users of social networking sites, computers on the internet, or human hosts; and links in the networks can be the followee and follower relationships, the network connections of computers, or the proximity or physical contact between people. In this project, we aim at developing new theory and efficient computational methods for several important problems about information propagation in networks. We advocate a new approach to model the propagation as continuous-time discrete-space stochastic processes, and propose to address these problems by novel theory and algorithms rooted in modern optimal transport theory and Fokker-Plank equations on graphs. In particular, we focus on three closely related problems which are fundamental in information propagation: influence prediction, propagation optimization, and propagation control. We will develop efficient numerical methods based on the novel approach to tackle these problems, and expect the results can greatly advance our ability to understand and control information propagation.<br/><br/>The focus of this project is on theoretical analysis and computations of information propagation on large-scale heterogeneous networks. The research has extensive applications in the real-world including social networking, cyber security and epidemics of infectious diseases. We concentrate on the investigation of three key problems on prediction and decision-making related to information propagation on networks. 1) Influence prediction: for a given source set of active nodes in the network, predict the influence, i.e. expected number of activated nodes (nodes which receive the information) in the future. 2) Optimal source distribution: select an optimal source set of nodes to achieve maximal influence. 3) Network control: change and manipulate resource distribution and network topology dynamically to achieve the desirable outcomes for information propagation on networks. These problems are difficult to solve due to many factors, such as large scale and heterogeneous structure of networks, uncertainties in propagation, incomplete knowledge of propagation dynamics, and noise in datasets. To overcome these difficulties, we take a novel and effective approach which is different from any existing method. In particular, we establish systems of differential equations, based on recently developed Fokker-Planck equations on graphs, to describe and compute the time evolution of the probability density functions for the activation states of the network and estimate the influence. We design graph-based stochastic optimization methods, which are closely related to the recent advancements on optimal transport theory, to effectively find optimal source distribution and propagation control strategy. The proposed methods are efficient, accurate, and can tackle those problems on large-scale real-world networks."
"1620384","Collaborative Research: Efficient Modeling of Incompressible Fluid Dynamics at Moderate Reynolds Numbers by Deconvolution LES Filters - Analysis and Applications to Hemodynamics","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","07/20/2018","Annalisa Quaini","TX","University of Houston","Continuing Grant","Leland Jameson","06/30/2021","$179,857.00","","quaini@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271","9263","$0.00","Computational fluid dynamics has emerged as a powerful tool to study the physiopathology of the cardiovascular system and for patient-specific Surgical Planning (SP) for cardiovascular diseases. Recently, clinical trials - the standard procedure for understanding diseases and assessing the impact of therapies and devices in the clinical practice - have been supported by a massive use  of numerical simulations to improve the knowledge extracted from measured data, leading to Computer Aided Clinical Trials (CACT). For a large variety of pathologies involving the aorta - the major artery of the circulation - this requires to work with turbulent flows. While Direct Numerical Simulation in this context can be appropriate for a proof of concept, for the large number of patients involved in CACT and SP we need different numerical tools to provide the appropriate trade-off between accuracy and reliability needed by clinical applications and computational efficiency needed by tight deadlines. As CACT and SP are new emerging concepts in cardiovascular mathematics, an appropriate numerical modeling of turbulent physiological flows for clinical applications is now an unmet need that we intend to solve in this proposal.<br/><br/>A possible way to limit the computational costs associated with Direct Numerical Simulations without sacrificing accuracy is to solve the flow average and model properly the effects of the small scales (not directly solved) at the medium and large scales (solved). We intend to investigate carefully new cutting-edge methods for disturbed flows based on Large Eddy Simulation (LES) Deconvolution filtering techniques with the ultimate goal of enabling practical use of numerical tools to improve knowledge extraction and clinical practice through CACT and SP. The main objective of this research is the development and the analysis of a robust and accurate LES based approach requiring no or minimal user's set-up for realistic incompressible flow problems with application to computational hemodynamics. We articulate the project in the following points: (a) Sensitivity analysis of key parameters involved in the method to understand their impact on the solution, leading to an automated parameter set-up through physical and numerical arguments. (b) Development and analysis of high-order in time methods, particularly for the computation of the pressure, with consequent improvement of the mass conservation properties. (c) Analysis of the impact of our LES approach on non-Dirichlet boundary conditions and the possible backflow stabilizing effects.  We plan to test the method on both academic and real bioengineering problems. Finally, we plan to deliver a finite element open source library incorporating the findings of our research, available for CACT and SP."
"1620278","Collaborative Proposal: Density-enhanced data assimilation for hyperbolic balance laws","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","08/30/2016","Ilya Timofeyev","TX","University of Houston","Standard Grant","Leland Jameson","08/31/2020","$190,000.00","","ilya@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271","9263","$0.00","The research addresses the urgent need to develop efficient computational tools to process the dramatically increasing amounts of observational data. Management of many complex systems (e.g., traffic) has to confront the uncertainty in both their current and future state. This uncertainty typically increases with time, leading to less accurate and useful predictions. Thus, it is important to develop practical methods for ""adjusting"" the probabilistic state of the system and reducing uncertainty using observational data. This approach is broadly referred to as data assimilation. We will develop novel techniques for incorporating observational data to reduce uncertainty in predictions in two particular areas  of national interest: fluid dynamics (e.g., flood forecasting) and traffic management. Both are of vital importance to sustainable development of our society.<br/><br/>We propose to develop a novel data assimilation framework for physical processes whose time-dynamics is described by hyperbolic conservation laws. This framework takes advantage of a kinetic representation of hyperbolic systems and, thus, availability of explicit deterministic equations for the time evolution of probability density function for dependent variables. These equations can often be derived and solved exactly, yielding explicit analytical solutions for the marginal and joint probability density functions. For systems of hyperbolic conservation laws an appropriate closure assumption is needed. Thus, the proposed framework relies on the kinetic representation, which takes the form of linear equations for joint probability density functions. Bayesian updating is utilized to incorporate observations into the prediction."
"1620484","Collaborative Research: New Algorithms for Group Isomorphism","DMS","COMPUTATIONAL MATHEMATICS","06/15/2016","06/08/2016","Joshua Grochow","NM","Santa Fe Institute","Standard Grant","Leland Jameson","09/30/2017","$111,583.00","","jgrochow@colorado.edu","1399 HYDE PARK RD","SANTA FE","NM","875018943","5059462727","MPS","1271","9150, 9263","$0.00","Symmetry reduces large complex systems to manageable quantities of information. Identifying those symmetries and understanding their structure helps to solve a wide range of problems, from improving engineering tasks to disrupting the mechanisms of disease. The century-old problem of deciding whether two sets of symmetries have the same structure is known today as the Group Isomorphism Problem. This problem is fundamental to both computational algebra and computational complexity, and has implications for fields as diverse as material science, particle physics, and chemistry. The primary goal of this project is to develop significantly better approaches to testing isomorphism of finite groups of symmetries. It supports a new multidisciplinary collaboration between researchers at four universities, including students and early-career mathematicians and computer scientists. <br/> <br/>The Group Isomorphism Problem asks for an algorithm to decide whether two finite groups are equivalent. Both the problem itself, and the techniques designed to improve upon it, have implications for other computational problems, including the better-known problems of Graph Isomorphism and P versus NP.  Our team's approach goes beyond existing static recursions such as working sequentially down a derived or lower central series. Using a new dynamic strategy we prioritize the optimal stages of the problem, thereby improving the performance of later stages. To achieve this we are investigating the use of nonassociative rings, spectral sequences, modular representation theory, and p-local cohomology. We are also inspecting recently developed data structures in computational algebra that seem well-suited to our approach, as well as investigating applications to geometric complexity theory."
"1613164","Self-Exciting Point Processes and Their Applications","DMS","PROBABILITY, COMPUTATIONAL MATHEMATICS","06/01/2016","06/03/2016","Lingjiong Zhu","FL","Florida State University","Standard Grant","Tomek Bartoszynski","05/31/2019","$100,057.00","","zhu@math.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1263, 1271","8060, 8083, 9263","$0.00","In mathematics, point processes are pure-jump stochastic processes, which means they are time evolutions that jump at random times and may have random jump sizes.  These processes are useful to model the complex systems that arise in the study of sociology, biology, criminology, seismology, finance, and many other fields. The most standard point process is the Poisson process that has independent time increments, which means that future jumps are independent of what has occurred in the past. However, one does not often observe independent time increments in real-world data. For example, as seen during the 2008 financial crisis, credit defaults have a contagion effect -- a default of one company can trigger more companies to default. In social networks, the possibility for users to re-share the content posted by their social connections may cascade across the system. In seismology, after an earthquake hits, the area often experiences aftershocks. In criminology, gang-related violence has the property that a murder or shooting by one gang often provokes retaliation by another gang. All these examples have in common the self-exciting property: an event can trigger more events to come. Self-exciting point processes, the subject of this research project, constitute a class of point processes that can describe such phenomena. Despite their importance in applications, many key central facts are still unknown. This project will investigate the theoretical aspects of self-exciting point processes, as well as their applications. It aims to advance understanding of how complex and large stochastic systems self and mutually excite, interact, cluster, and effect contagion. The results derived from this framework will be applied to better understand the big data sets arising from complex systems in the real world. <br/><br/>More specifically, this project studies a class of self-exciting point processes, with a focus on the Hawkes process and its extensions, including the nonlinear Hawkes process. To date, the limit theorems for this model are restricted to large time asymptotics, and they have been well studied except the multivariate nonlinear case, which will be investigated in this research. The investigator intends to understand the large asymptotics on a fixed time interval, which will be very useful in the context of many applications. These large asymptotics will differ from the Poisson process and phase transitions are anticipated. The investigator will also study the fluctuations and large deviations of the mean-field limit of a multivariate Hawkes process when the dimension is large, which is useful in applications to neural networks, financial networks, and others. The project will also explore the spatial Hawkes processes, the space-time Hawkes processes, and other types of self-exciting point processes that have been suggested for modeling but still lack some key theoretical results. The investigator also aims to study applications of the self-exciting point processes, including theoretical applications to queueing theory and to fitting these models to real world big data sets, which requires a better understanding of some theoretical aspects of the simulations and calibrations of self-exciting point processes."
"1620396","Parallel Multiscale Algorithms for Dynamical Systems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","09/03/2016","Bjorn Engquist","TX","University of Texas at Austin","Standard Grant","Leland Jameson","08/31/2019","$349,983.00","Yen-Hsi Tsai","engquist@math.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1271","9263","$0.00","The proposed research will open important problem classes for realistic computer simulation on future massively parallel computer architectures. The targeted areas are dynamical phenomena with strong variations in time. Typical processes that will benefit from this improved simulation capability are, for example, molecular dynamics for chemical and biological systems, vibrating mechanical systems, systems biological phenomena, atmospheric flow and seismic wave propagation. The novel computational methods and theoretical understanding will introduce new paradigms in the numerical solutions of challenging dynamical systems with a potential for many future realistic applications. The training of students in these fields is also very important, as they will shape the future of the development of multiscale modeling and high performance computing in academia and industry.<br/><br/>Time dependent systems that have highly oscillatory solutions can be found in many important fields of science and engineering, and they present great challenges both in analysis and in scientific computation. A major focus of research activities has been on the development of multiscale methods for such systems that focus on the effective behavior of these systems without resolution of all details. The PIs propose to develop a new framework addressing some of the core problems of scientific computing for the coming years. This will be done in two directions. In one the earlier techniques will be generalized to infinite dimensional oscillatory systems with application to seismic wave propagation. The other direction will exploit next generation massively parallel computer systems. The multiscale models for effective behavior will be used as coarse solvers to resolve important obstacles in the challenging parallel-in-time or parareal computation. The results will facilitate further synergistic advancement in the field of multiscale modeling in general."
"1552903","CAREER: Mesoscale computational modeling of intracellular soft matter","DMS","COMPUTATIONAL MATHEMATICS, Division Co-Funding: CAREER","06/01/2016","07/06/2020","Anastasios Matzavinos","RI","Brown University","Continuing Grant","Yuliya Gorb","05/31/2021","$400,000.00","","anastasios_matzavinos@brown.edu","1 PROSPECT ST","PROVIDENCE","RI","029129127","4018632777","MPS","1271, 8048","1045, 9150, 9263","$0.00","Intracellular networks of entangled or cross-linked polymers, such as the actin and microtubule cytoskeleton, interact with and respond to mechanical cues from the environment, influencing how cells grow and divide, how stem cells differentiate into specific cell types, and how cancer cells proliferate, among other important molecular and cellular phenomena. In many cases, the structure of these networks is dynamically altered by the mechanical feedback of lipid membranes and cytoplasmic flows. However, the majority of current modeling and computational approaches focus on computational tractability at the expense of these mechanical feedbacks. A central theme of this project is that the development of physically realistic models of intracellular polymer networks (and other biological matter) requires a computational approach that efficiently integrates the mechanics of the network with the hydrodynamics of intracellular fluid flows and the mechanical interactions with biological membranes. Such integrative computational efforts are key to building a better understanding of intracellular phenomena. In particular, they hold the potential to help diagnose and treat cytoskeleton-related diseases and guide the development of stem cell techniques. The proposed work will help with the scientific and professional development of both graduate students and undergraduate students. It will also integrate excellent educational opportunities including the development of open education resources; promoting computer-coding literacy as a means to investigate physical phenomena through the development of an educational website with specialized computer coding activities pertaining to the proposed project; conducting boot camps to train undergraduate students on ""Mesoscale computational modeling of intracellular soft matter"" and;  working with the Science Centre at Brown University to engage high school students from underrepresented communities in scientific computing and the physics of soft matter.<br/><br/>The PI proposes a synergistic combination of approaches consisting of dissipative particle dynamics (DPD) simulations and stochastic homogenization methods. DPD can accurately model the molecular-scale motion of polymers and fluids without the computational burden that normally makes alternative simulations (e.g., molecular dynamics) unviable. Stochastic homogenization methods, on the other hand, provide the means of efficiently extracting bulk mechanical properties from the mesoscale DPD description. Specific aims of the project include (i) the development of specialized particle-based methods for investigating the mechanical interactions between cytoplasmic flows and intracellular matter, (ii) a computational and analytic investigation of stochastic homogenization approaches to developing mesoscopic mechanical descriptions of intracellular matter, (iii) applications of the developed algorithms to the computational modeling of specific examples of biological matter, such as the actin cytoskeleton and DNA macromolecules, and (iv) the clustering and classification of simulation data in order to characterize the dynamics of biological matter in the systems under investigation. Given the recent interest in intracellular polymer networks  (e.g., the actin cytoskeleton) as therapeutic targets in a broad array of pathological conditions, including cancer, neurodegenerative diseases,  and kidney disease, the proposed computational approach  is expected to advance significant biomedical applications, especially therapeutic perturbations of cytoskeletal dynamics."
"1615444","Verified Computation and Proof","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","08/29/2016","Jeremy Avigad","PA","Carnegie-Mellon University","Standard Grant","Leland Jameson","08/31/2018","$149,818.00","","avigad@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1271","9263","$0.00","Mathematics is exceedingly complex, and avoiding mistakes is crucial to keeping our mathematics correct, meaningful, and reliable. This project involves the use of logic-based computational methods to support mathematical reasoning. The PI will contribute to the development of a theorem prover called Lean, which can be used to verify complex proofs and calculations. The system enables us to build mathematical libraries that are verified on the basis of a formal axiomatic system, with the computer checking each and every claim on the basis of a small set of axioms and rules. The system will support exploration and the discovery of new mathematics, and can also be used to verify properties of complex systems in computer science, engineering, and finance.<br/><br/>The development of Lean is based at Microsoft Research, Redmond, but it is an open-source, community-based project that currently involves researchers at Carnegie Mellon University, Stanford, and the University of Washington as well. The PI will develop Lean's libraries and automation to support applications in fields such as engineering and mathematical finance. Specifically, the PI and his collaborators and students at Carnegie Mellon will extend the libraries for analysis and measure theory and measure-theoretic probability, as well as parts of algebra and homotopy type theory. They will also contribute to other types of automation currently under development in Lean, and develop decision procedures and algorithms for special domains. Finally, they will continue to develop interactive online course material, based on Lean, to provide better educational resources for logic and formal methods."
"1555222","CAREER:   Numerical Methods For Liquid Crystals And Their Optimal Design","DMS","COMPUTATIONAL MATHEMATICS, Division Co-Funding: CAREER","08/01/2016","07/06/2020","Shawn Walker","LA","Louisiana State University","Continuing Grant","Yuliya Gorb","07/31/2022","$400,000.00","","walker@math.lsu.edu","202 HIMES HALL","BATON ROUGE","LA","708030001","2255782760","MPS","1271, 8048","1045, 8990, 9150, 9263","$0.00","Liquid crystals are commonplace in modern technological devices, and are most famously used for their optical properties in electronic displays (for example, LCDs).  What makes them so useful is that they are easily manipulated and controlled by applying voltages or magnetic fields.  Moreover, liquid crystals can interact with fine-scale ""material particles.""  Thus, liquid crystals enable the fine-scale manipulation of matter.  The goal of this research project is to create new mathematical methods/algorithms for simulating liquid crystal phenomena, and for designing new materials that utilize liquid crystals.  In other words, the research will provide the groundwork for developing functionalized and switchable materials that are ""driven"" by liquid crystal physics. The research will impact local communities, education, and liquid crystal scientists: (i) Middle School Science Fair Projects.  The PI will mentor science fair projects on liquid crystals at minority serving middle schools (the visual appeal of liquid crystals makes them ideal for project topics).  PI will involve post-docs and graduate students in the mentoring. (ii) Public Library Engagements.  PI will interact with middle/high school students through local libraries by creating a ""sit-with-a-scientist"" program.  Each session takes place at a library branch location and includes a short, introductory presentation followed by hands-on activities to allow the students to actively learn about the physics and mathematics of liquid crystals.  PI will involve post-docs and graduate students by having them assist during the sessions. (iii) Education. PI will continue to mentor undergraduates through REUs and senior projects, and create a graduate course ""Computational Methods For Liquid Crystals"" based on this research. (iv) Open Software. PI will create software implementations, tutorials, and demos of the research.<br/><br/>The research will create new mathematical algorithms to correctly and efficiently simulate liquid crystal (LC) phenomena above the scale of molecules. Current algorithms do not do this.  They either make ad-hoc changes to the underlying model for mathematical convenience, or they are expensive to compute (or both). Efficiency is crucial to facilitate ""plugging"" these methods into high level design and optimization procedures for, say, material design. Molecular simulations of LCs are too expensive for iterative design work.  The Q-tensor model is better, but can still be expensive and hard to solve.  The research does the following: (i) creates new methods for the Q-tensor model that are cheaper to solve and more faithful to the physical model by taking advantage of the discrete maximum principle (DMP); (ii) extend our method to handle arbitrary geometry through a ""cut"" finite element method (cutFEM); (iii) extend our cutFEM to do optimal shape design of LC systems, including self-assembly of colloidal inclusions.  The research will create new methods to simulate and optimize liquid crystal systems that capitalize on delicate mathematical tools, such as Gamma-convergence and the DMP.  This fits well within our core expertise, such as prior work in LCs, 3-D mesh generation/implementation, multi-physics geometric flows, and shape optimization.  Consulting with LC modeling experts will help validate our results.  An added benefit of the research is that it will further the development of finite element methods for non-linear degenerate partial differential equations, and combine shape optimization with cutFEMs."
"1620158","Collaborative Research: RUI: Three-Dimensional Multiphysics Simulation of Multi-phase Flows with Magnetic Fluids","DMS","COMPUTATIONAL MATHEMATICS","09/15/2016","09/15/2016","Philip Yecko","NY","Cooper Union","Standard Grant","Leland Jameson","08/31/2021","$109,699.00","","yecko@cooper.edu","30 COOPER SQUARE 2ND","NEW YORK","NY","100037120","2123534138","MPS","1271","9229, 9263","$0.00","Advances in the synthesis of Ferrofluids (engineered fluids that respond to magnetic fields and have a number of well-established industrial applications) have increased the scope of potential applications of these fluids to new areas. Emerging applications of ferrofluids include: magnetic targeting of drugs, cell sorting in biomedical systems and magnetically driven contaminant removal. In each of these applications the use of ferrofluids enables new techniques that depend on the use of magnetic fields for 'remote control' of the ferrofluid. However, the realization of such technologies is hampered by complexities simulation of these systems for further development and design. The proposed research program includes the development of effective, robust computational tools that will enable such simulations. In particular, the computing codes will include the particular magnetic physics of ferrofluids as well as the forces resulting from the magnetic fields, which serve as the means of control in these applications. The development of these effective simulation tools will support and accelerate innovation in these emerging these technologies. Moreover, the proposed program of code development, simulations and integrated physical experiments will serve as a proof-of-concept for the inclusion of realistic multiphysics fluid simulations for complex scientific and  engineering applications. The proposed research program will involve undergraduate and masters-degree students in leading-edge research, including students who are members of groups under-represented in STEM disciplines such as women and first-generation college students.<br/><br/>In magnetic drug targeting, a ferrofluid whose constituent nanoparticles have been functionalized to carry theraputic drugs is directed to a tumor or other localized site (e.g., in the eye); sorting of(nonmagnetic) biological cells by immersion in a ferrofluid so that the force of an applied magnetic field depends on cell size; purification of a polluted fluid by adsorbption of contaminants to magnetic nanoparticles, which are then separated from the fluid by magnetic forces. However, advances in these applications are stymied by the complex, multi-scale and multi-physics nature of the fluid-dynamical systems in which they occur. In particular, because contemporary fluid-dynamics codes are not designed to incorporate the additional physics of magnetic-fluid systems, effective simulation with these codes is difficult. The proposal describes a plan to develop and test a new parallel, multi-phase code for fully three-dimensional flows. This project will lead to a flexible and efficient, multi-phase magnetic-fluid simulation code that is fully three-dimensional and parallelized for high-performance computing. Hence, the code will enable realistic simulations relevant to the significant applications addressed. Specifically, in order to address the above-noted applications, the code will model and simulate flows with dynamic interfaces between the ferrofluid and other fluids. Moreover, the code will implement models of viscosity effects (magnetoviscosity) as well as driving forces that result from applied magnetic fields (magnetophoresis) in a flexible manner that simplifies adjustment and updating of the models."
"1522737","Collaborative Research: Computation of instantons in complex nonlinear systems","DMS","COMPUTATIONAL MATHEMATICS","02/01/2016","01/29/2016","Tobias Schaefer","NY","CUNY College of Staten Island","Standard Grant","Leland Jameson","01/31/2020","$99,554.00","","tobias@math.csi.cuny.edu","2800 VICTORY BLVD","STATEN ISLAND","NY","103146609","7189822254","MPS","1271","9263","$0.00","A wide variety of systems exhibit rare events -- events far from the average system behavior with low probability of occurring.  Rare events can have significant consequences, and improved understanding of their occurrence can aid in the design or management of such systems. The characterization of the likelihood of rare events is essential in all systems in which stochasticity plays an important role, as it allows us to take advantage of such events if they are desirable and to avoid them if they present a threat. The outcomes of this project will contribute to the understanding of rare events in complex systems, in particular, fluid dynamics and related geophysical systems. Further applications include the characterization of extreme events in the context of epidemics, population dynamics, and molecular biology. <br/><br/>The goal of this project is to develop efficient computational methods to characterize the most likely way rare events occur in complex stochastic systems and to estimate the tails of their probability distributions. For this purpose, the investigators will develop efficient algorithms to compute the so-called ""instantons"" that are minimizers of the action functional that large deviation theory associates with the stochastic differential equation describing the system's evolution. Numerical methods to calculate instantons will first be developed in the context of turbulence (in particular Burgers equation, magneto-hydrodynamics (MHD), Navier-Stokes equations, and the surface-quasi-geostrophic (SQG) equation) driven by diffusive processes. Then the investigators will extend the methods to stochastic equations that are driven by non-Markovian noise (fractional Brownian motion) or jump-processes, which play an important role in physics, biology, and chemistry. Finally, the investigators will calculate fluctuations around the instantons to get finer estimates of their probability of occurrence via prefactor calculations."
"1619908","Workshop for Women in Computational Topology","DMS","INFRASTRUCTURE PROGRAM, TOPOLOGY, COMPUTATIONAL MATHEMATICS, Algorithmic Foundations","06/15/2016","06/13/2016","Lori Ziegelmeier","MN","Macalester College","Standard Grant","Leland Jameson","01/31/2018","$30,000.00","Erin Chambers, Brittany Fasy","lziegel1@macalester.edu","1600 GRAND AVE","SAINT PAUL","MN","551051801","6516966062","MPS","1260, 1267, 1271, 7796","7556, 7929, 9263","$0.00","This award will support participants attending the first Workshop for Women in Computational Topology.  Computational topology is a new and exciting field emerging at the intersection of mathematics and computer science, with applications in areas such as big data, image recognition, and medical imaging, among many others.  Traditionally, representation of women in these research areas is quite low, and attrition rates are high at all levels.  This award will allow the creation of a network of junior and senior women in this research area, a strategy which has proven invaluable to broadening participation in such areas.  Participants will spend a week at the Institute for Mathematics and its Applications, providing a critical opportunity for research collaboration as well as networking and mentoring for women.<br/><br/>Computational topology brings together tools from topology, geometry, algorithms, and statistics in order to tackle a range of problems from applications that include graphics, sensor networks, medical imaging, materials engineering, GIS, big data analysis, and many others. This workshop will form a series of working groups that will tackle open problems posed by senior women researchers in computational topology on topics ranging from computing optimal mappings between surfaces to investigating open problems related to persistent homology to applying techniques to new applications.  Results from the workshop will be published in an AWM Springer volume, and follow-up events will be organized in conjunction with AWM to continue to grow the network."
"1548520","U.S. Participation in Newton Institute Program on Stochastic Dynamical Systems in Biology:  Numerical Methods and Applications","DMS","APPLIED MATHEMATICS, STATISTICS, COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","01/01/2016","07/29/2015","Samuel Isaacson","MA","Trustees of Boston University","Standard Grant","Junping Wang","12/31/2016","$23,530.00","","isaacson@math.bu.edu","1 SILBER WAY","BOSTON","MA","022151703","6173534365","MPS","1266, 1269, 1271, 7334","7556, 9263","$0.00","From January 4 through June 24, 2016, the Isaac Newton Institute for Mathematical Sciences  at Cambridge University in the UK will  host a thematic program on Stochastic Dynamical Systems in Biology: Numerical Methods and Applications. Stochastic dynamical systems are prevalent throughout biology and medicine, and include many key processes in gene regulation, cell signaling, tissue development, and cancer proliferation. Mathematical and numerical methods are key tools in understanding both the function of these systems, and how to control and manipulate their behavior. The program will bring together a diverse set of international researchers from across the mathematical and biological sciences, each working on the design and application of such methods. The program offers a unique opportunity to exchange ideas and work collaboratively in order to tackle the many challenging open problems of the field by enabling a large number of researchers to participate for extended periods of time. To introduce graduate students and postdoctoral fellows to the field, the opening workshop will offer tutorials and plenary talks focused on providing a broad overview of current research  problems. This grant provides travel funding for early career researchers and graduate students from the United States to make extended visits during the program and to participate in the program workshops.<br/><br/>It has become clear that to effectively understand stochasticity in biological systems, a combination of modern numerical analysis, estimation and sampling techniques, and rigorous analysis of stochastic dynamics is required. To highlight open problems and current work in these areas, the Isaac Newton Institute for the Mathematical Sciences is running a thematic program on Stochastic Dynamical Systems in Biology: Numerical Methods and Applications. The program will include an introductory, tutorial-based workshop and three research workshops, broadly focused on: multi-scale methods; well-mixed systems; and spatially distributed systems. In addition, each week of the program twenty to thirty long-term participants will be in residence, each visiting the institute for at least two weeks. The interactions between these researchers are expected to advance the field through the development of new cross-disciplinary collaborations. In the mathematical sciences these collaborations will lead to new mathematical techniques and numerical methods for studying ordinary and partial differential equations, discrete and continuous-time stochastic processes, and a variety of statistical models. To enable the broader scientific community to benefit from the program, all lectures and seminars will be available to view through the  institute website (with the speaker's permission). This grant specifically supports the travel costs to visit the institute and engage in the activities of the program of several early career and trainee mathematical scientists from the United States. Further information on the  program is available on the website, http://www.newton.ac.uk/event/sdb."
"1619630","Feature-Based Data Assimilation and Uncertainty Quantification for Complex Systems in Science and Engineering","DMS","COMPUTATIONAL MATHEMATICS","08/01/2016","06/19/2018","Matthias Morzfeld","AZ","University of Arizona","Continuing Grant","Leland Jameson","07/31/2019","$249,999.00","","mmorzfeld@ucsd.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1271","1303, 8396, 8399, 9263","$0.00","The basic idea of data assimilation is to update a computational model with information from sparse and noisy data so that the updated model can be used for predictions. Data assimilation is at the core of computational geophysics, most notably in numerical weather prediction, oceanography, and geomagnetism, and is used widely in engineering applications, ranging from robotics to reservoir modeling. In the usual approach one attempts to refine a computational model such that its outputs match data. However, matching model outputs directly to data is often unnecessary or even undesirable. In this project, data assimilation is extended so that computational models can be updated based on features in the data, rather than the raw data themselves. The feature approach reduces an intrinsic dimension and is applicable to large scale problems in geosciences and engineering, with specific applications in geomagnetic dipole reversals, cloud modeling, and uncertainty quantification for solar cells.<br/><br/>The primary technical aim of this project is to extend data assimilation such that computational models can be calibrated against features observed in the data, rather than the raw data. This can be achieved within a Bayesian framework by replacing the data with a suitable low-dimensional feature, computed from the data. The resulting feature-based likelihood can be used to assimilate selected aspects of fine-scale data into coarse, low-dimensional models. More generally, the use of features reduces the dimension of the likelihood, which in turn reduces the computational requirements of feature-based data assimilation by Monte Carlo methods. The mathematical foundations of the feature-based approach will be explored by rigorous analysis. New computational methods for feature-based data assimilation will be created, which combine machine learning techniques with Monte Carlo sampling. The efficiency of these methods will be assessed by interdisciplinary collaboration with scientists in geosciences and engineering in three specific applications. Specifically, feature-based data assimilation algorithms will be developed for the study of superchrons of Earth's magnetic dipole field, to determine the geophysical relevance of low-dimensional cloud models, and for uncertainty quantification of thin-film polymeric reflectors for solar power generation. These applications will collaboratively connect scientists (faculty, postdocs and students) across several disciplines (geosciences, engineering, mathematics). Undergraduate and graduate students at the University of Arizona will be trained as part of the project and will aid in producing and disseminating key results. The research activities will be accompanied by an outreach plan, implemented as part of the G-Teams program within the Department of Mathematics at the University of Arizona. A central outreach theme is to demonstrate, for K-12 teachers and their students, mathematics ""in action"" by applying mathematical concepts to problems relevant to our society."
"1619755","Computational Methods and Consistency for Dirichlet Graph Partitions","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/19/2018","Braxton Osting","UT","University of Utah","Continuing Grant","Leland Jameson","06/30/2020","$180,000.00","","osting@math.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","MPS","1271","9263","$0.00","This project will investigate theoretical properties, computational methods, and applications for a particular method of graph partitioning. Generally speaking, graph partitioning is the mathematical problem of subdividing a set into smaller components with desirable properties. This problem has diverse applications in image analysis (medical, satellite, and material), surveillance, social network analysis, and topic modeling, among many others. The results of this project could significantly impact these areas and due to the multidisciplinary nature of this activity, all involved parties will gain awareness and literacy outside their own respective fields. The project provides opportunities for the principal investigator to continue advising and mentoring students. <br/><br/>This project will prove fundamental theoretical results and develop computational methods for the Dirichlet graph partitioning problem, formulated as minimizing the sum of the principle Laplace-Dirichlet eigenvalues for each partition component. This graph partitioning problem has rich and exploitable mathematical structure: it is motivated by a geometric problem and has both a variational characterization and an associated stochastic process. The project will have three goals. The first goal is to prove, via Gamma convergence with a suitable metric, the consistency result that Dirichlet partitions of geometric graphs, obtained by sampling from a probability space, converge to Dirichlet partitions of that probability space as the number of sampled points tends to infinity. This result will yield a better understanding of the relationship between the graph and continuum partitioning problems as well as possibly suggest subsampling strategies for extremely large datasets. The second goal addresses important computational aspects of the Dirichlet graph partitioning problem. Although two different relaxations of this problem have been identified, computational methods based on these relaxations necessarily have increased computational complexity. The PI will seek provably convergent new methods that combine variational arguments with ideas from geometry, partial differential equations, and Markov processes in order to extend and overcome these shortcomings. The third goal is to address concrete, real-world problems and to engage the developed algorithms in practical applications."
"1620103","Collaborative Research: Density-enhanced data assimilation for hyperbolic balance laws","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","08/30/2016","Daniel Tartakovsky","CA","University of California-San Diego","Standard Grant","Leland Jameson","12/31/2017","$200,000.00","","tartakovsky@stanford.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271","9263","$0.00","The research addresses the urgent need to develop efficient computational tools to process the dramatically increasing amounts of observational data. Management of many complex systems (e.g., traffic) has to confront the uncertainty in both their current and future state. This uncertainty typically increases with time, leading to less accurate and useful predictions. Thus, it is important to develop practical methods for ?adjusting? the probabilistic state of the system and reducing uncertainty using observational data. This approach is broadly referred to as data assimilation. We will develop novel techniques for incorporating observational data to reduce uncertainty in predictions in two particular areas  of national interest: fluid dynamics (e.g., flood forecasting) and traffic management. Both are of vital importance to sustainable development of our society.<br/><br/>We propose to develop a novel data assimilation framework for physical processes whose time-dynamics is described by hyperbolic conservation laws. This framework takes advantage of a kinetic representation of hyperbolic systems and, thus, availability of explicit deterministic equations for the time evolution of probability density function for dependent variables. These equations can often be derived and solved exactly, yielding explicit analytical solutions for the marginal and joint probability density functions. For systems of hyperbolic conservation laws an appropriate closure assumption is needed. Thus, the proposed framework relies on the kinetic representation, which takes the form of linear equations for joint probability density functions. Bayesian updating is utilized to incorporate observations into the prediction."
"1620135","Multiscale computational methods in kinetic theory and optimal transport","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/19/2018","Li Wang","NY","SUNY at Buffalo","Continuing Grant","Leland Jameson","11/30/2018","$200,000.00","","wangli1985@gmail.com","520 LEE ENTRANCE STE 211","AMHERST","NY","142282577","7166452634","MPS","1271","9263","$0.00","Kinetic equations with multiple scales arise in diverse applications such as rarefied gas dynamics, plasma physics, semiconductors, and biology; they often introduce severe numerical challenges due to the stiffness that comes from the small scales. Optimal transport plays a fundamental role in image registration, video restoration, urban transport, kinetic theory and many others. However, numerical methods for it have not reached their full capacity to meet the most demanding practical applications. This project aims at both advancing the multiscale computational methods - particularly the asymptotic preserving (AP) schemes - in new prospects for kinetic equations including multi-stage and fractional asymptotic limit, and developing fast parallelizable algorithms for optimal transport via advanced optimization technique. <br/><br/>Specifically, the following topics will be investigated in this project: (1) theoretically study the AP schemes for semiconductor Boltzmann equation with two-scale collisions at a deeper depth and generalize them to implicit/high order schemes and to capture the hierarchy of macroscopic models; (2) extend the AP scheme for kinetic equation with fractional diffusion limit to a broader scope including anisotropic scattering, degenerate collision, and Levy-Fokker-Planck interaction (applications to nonclassical photon transport in clouds will be addressed); (3) develop efficient algorithms for optimal transport problems and conduct convergence analysis and apply it to practical problems especially for human crowd dynamics in panic situations. With increasing interest in multiscale kinetic equations and optimal transport, the computational methods developed here will impact beyond the particular applications in this proposal. The dynamics of electron transport in semiconductor devices are one of the main concerns in physics and engineering; the developed methods from this proposal will be equally applicable in a broader context such as gas discharges and multi-group radiative transfer. Nonclassical transport that leads to a fractional diffusion has attracted much attention in plasma physics and economy; it has now been applied in climate science to model the photon transport in clouds as well as in criminology to model the hotspots in residential burglaries. Optimal transport has become a useful tool in image processing, urban transport, computer vision and etc; the development of fast parallelizable algorithms will substantially advance these areas and the application in modeling human crowds is crucial for better preparation of safe mass events."
"1620366","Numerical Methods for Geometric PDE on Manifolds with Arbitrary Topology","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","07/23/2018","Michael Holst","CA","University of California-San Diego","Continuing Grant","Leland Jameson","06/30/2020","$214,547.00","Lee Lindblom","mholst@math.ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271","9263","$0.00","This project is concerned with the approximate solution of systems of stationary and evolution partial differential equations (PDE) arising at the intersection of mathematical physics and geometric analysis. Such systems of equations, known as Geometric PDE, appear in a wide range of physical and mathematical problems; one of the primary motivations for this project is the Einstein, which are of central importance to gravitational wave science. One of the most challenging features of this class of problems, for both mathematical analysis and computational simulation, is the underlying spatial domain which has the structure of a potentially complicated manifold rather than a simple shape in 3-space. Moreover, both the geometry and the topology of this manifold may evolve over time, depending on the particular model. The research results will have a broad impact on areas of mathematics such as geometric analysis, as well as in astrophysics and general relativity. The methods developed will contribute to the advancement of numerical methods for complex three-dimensional constrained nonlinear dynamical simulations. The simulation technology the PIs produce will provide powerful tools for the exploration of models in astrophysics and relativity as well as in some areas of pure mathematics such as geometric analysis. The two graduate students involved in the project will be co-trained by both investigators; this will involve regular interaction between all four members of the team.<br/><br/>The primary technical aim of this project is to develop a general approximation theory framework, together with reliable and provably  convergent adaptive methods, for the intrinsic discretization of a general class of nonlinear geometric elliptic and evolution PDE on  Riemannian 2- and 3-manifolds.  While the solution theory for this class of PDE has been intensively studied over the last thirty years, progress on the development of robust numerical methods with a corresponding approximation theory has been a more recent development. Most of the approaches to date, such as surface finite element methods for two-dimensional problems, are based on exploiting the embedding of the surface into 3-dimension. For applications such as general relativity, a more general approach is needed that does not rely on the existence of such an embedding. In this project, the PIs will study the development of truly intrinsic discretizations that use no extrinsic information to produce a discretization, to allow for the development of numerical methods on Riemannian 2- and 3-manifolds with arbitrary topology.  The PIs' approach is to develop an atlas-based discretization using techniques such as the multi-cube framework and the local simplex approximation techniques developed by the project team. To develop a corresponding error analysis framework, the PIs will exploit the variational crimes framework for methods in surfaces, such as methods based on finite element exterior calculus."
"1619973","Computational Methods for Symmetric Tensor Problems","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/13/2016","Jiawang Nie","CA","University of California-San Diego","Standard Grant","Leland Jameson","06/30/2020","$150,000.00","","njw@math.ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271","9263","$0.00","This project targets at symmetric computational problems. Tensors are generalizations of matrices. The entries of symmetric tensors obey symmetric patterns. Computational problems about them become more and more important in big data time. Like the case of matrices, basic problems about symmetric tensors are computing their decompositions over the real and complex fields, determining their ranks, computing low-rank approximations, and applying them in relevant applications. This project devotes to the research of computational problems about symmetric tensors.<br/><br/>Tensor is a powerful tool in computational mathematics. Symmetric tensors have beautiful algebraic and geometric properties. A problem of fundamental importance is to write a tensor as a sum of rank one tensors, with minimum length. This is the so-called tensor decomposition problem. Tensor decompositions can be over either the real or complex field. Although they are related, the decomposition over the real field is very different from the case of complex field. In applications, tensors can be very large, but their ranks may be small. People often need to approximate a symmetric tensor by a low rank one, as close as possible. Generating polynomial is an efficient tool for solving symmetric tensor computational problems. It uses the algebraic properties elegantly.  A symmetric tensor can be viewed as a symmetric multi-linear functional, which can be expressed by a multivariate polynomial. This project uses mathematical knowledge from computational algebra, polynomial systems, matrix computations, complex and real algebraic geometry, and optimization. The results produced by this project have potential applications in multilinear algebra, signal processing, blind source separation, numerical analysis, higher order Markov chains. The project is going to provide training for students and young researchers who are interested in the subject. Produced results will be promptly disseminated to the scientific community."
"1620487","Numerical Methods for Fluctuating Motion of Interface","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","09/03/2016","Bo Li","CA","University of California-San Diego","Standard Grant","Leland Jameson","08/31/2020","$300,000.00","Li-Tien Cheng","bli@math.ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271","9263","$0.00","Interfacial fluctuations are common in many physical and biological systems. Understanding the principles that underlie such fluctuations has far-reaching scientific and technological consequences. For instance, the manipulation of interfacial fluctuations in the so-called molecular beam epitaxy of growing nanometer-scale semiconductor materials can largely improve the quality and functionality of such materials that are widely used for high-technology electronic and military sensor devices. Effective treatment of some fatal diseases relies critically on our knowledge of anomalous water-protein interfacial structures that result from fluctuations and biological mutations and that characterize such diseases. This project develops a state-of-the-art computer program to investigate how the fluctuation affects the structures and long-time dynamics of interfaces, with a particular application to the binding of a drug molecule to a target protein that is a crucial step in the computer-aided drug design. The success of this project can therefore potentially help reduce the high cost often needed for laboratory experiments and speed up the process of drug discovery. In addition, this highly interdisciplinary research brings unique opportunities for students at different levels, particularly those from under represented groups, to receive training at the interface of mathematical and biological sciences. Such training is critical to keeping our nation's strength in scientific research in a highly competitive international environment.  <br/><br/>Computationally tracking the motion of fluctuating interface is in general rather challenging, as such motion involves multiple but correlated spatial and temporal scales, high energy barriers between one stable interfacial structure to another, and the coupling of interface and bulk processes. The PIs construct two methods to overcome some of these difficulties. One is the stochastic level-set method that describes the fluctuating interface by solving a stochastic differential equation. The noise in the equation is spatially localized on or near the interface. Rigorous stochastic analysis is carried out to reformulate such an equation for accurate and efficient computations. The other is a stochastic lattice-phase method that treats the coupling of both interfacial and bulk fluctuations. This method describes the interface geometry by assigning a binary value on each of the discrete sites, and minimizes a Hamiltonian of all possible discrete binary fields using a Monte Carlo simulation method. This Hamiltonian mimics the continuum one with spatial gradient-square term and a double-well potential. The mathematical analysis using the notion of Gamma-convergence reveals the interplay between the numerical grid size and the interfacial width, and directly guides the design of fast algorithms. The PIs also develop a parallel computational algorithm with the GPU implementation to speed up their computations. They combine their new techniques with a molecular solvation theory to study molecular recognition, particularly the binding of a small drug molecule to a target protein. The computational models, numerical algorithms, and computer codes developed in the project can be incorporated into existing software that are used on a daily basis to study biomolecular interactions, and in particular, for computer-aided drug design"
"1620473","Hybrid Imaging with Acoustic and Optics: Efficient Computation via Constructive Analysis","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","08/03/2018","Kui Ren","TX","University of Texas at Austin","Continuing Grant","Leland Jameson","06/30/2020","$238,956.00","","kr2002@columbia.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1271","8990, 9263","$0.00","This project is concerned with the development of mathematical theory and computational tools for some hybrid methods in medical imaging. By hybrid methods we refer to imaging techniques where two modalities with distinct underlying physics are coupled together to achieve the contrast and resolution not attainable separately by each of the modalities involved. This project is concerned with a specific class of hybrid imaging modalities that couple ultrasound imaging with optical imaging to achieve high-resolution imaging of optical properties of heterogeneous media. Our objectives are (i) to develop fast and robust image reconstruction algorithms for these hybrid modalities for practical use, and (ii) to train advanced undergraduate students, graduate students, and postdoctoral researchers on the mathematical and computational research in this field. <br/> <br/>The main proposed research efforts in this project include, but not limited to: (i) to develop efficient inversion algorithms for multi-coefficient inverse problems in photoacoustic tomography (PAT) with unknown ultrasound properties; (ii) to develop efficient reconstruction algorithms for inverse problems in quantitative fluorescence PAT (fPAT) and two-photon PAT (TP-PAT) for molecular imaging; (iii) to perform mathematical analysis on inverse problems in ultrasound-modulated fluorescence optical tomography (UMfOT) and to develop robust image reconstruction strategies in the same setting; and (iv) to develop fast algorithms for the transport equation for optimization-based iterative image reconstruction strategies for PAT, fPAT, TP-PAT and UMfOT with partial measurements. The mathematical ideas developed in the proposed projects are expected to provide insights on the mathematical and numerical solutions of many similar inverse coefficient problems to partial differential equations that appear in many fields of science and technology."
"1620352","A Scalable High-Order Discontinuous Finite Element Framework for Partial Differential Equations: with Application to Geophysical Fluid Flows","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","09/01/2016","09/08/2016","Tan Bui-Thanh","TX","University of Texas at Austin","Standard Grant","Leland Jameson","08/31/2019","$149,917.00","","tanbui@ices.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1271, 8069","8083, 9263","$0.00","Partial differential equations (PDEs) are pervasive in engineering and science, and their numerical solutions are of paramount importance in understanding complex, natural, engineered, and societal systems. Though the past decades have seen tremendous advances in both theories and computational algorithms for PDEs, scalable numerical methods for complex, coupled, and multiphysics systems that fully exploit the extreme-scale computing systems remain challenging. This becomes the major impediment for future exascale systems unless dramatic changes in mathematical methods and computational algorithms take place. However, not much work has been done for this critical component of computational mathematics. Thus, there is a critical need to develop advanced mathematical discretizations that can enable scientific applications to harness the potential of extreme-scale computing in order to continue the pace of scientific discoveries and to promote the progress of science. <br/> <br/>The PI will develop a high-order discontinuous finite element (FE) framework, including the weak Galerkin and the hybridized discontinuous Galerkin methods, and its scalable solver for geophysical fluid dynamic applications. In particular, the PI will design and rigorously analyze an abstract high-order discontinuous FE framework for a large class of PDEs including elliptic, parabolic, and hyperbolic types. The PI will also develop, analyze, and implement scalable solvers. Both hydrostatic and non-hydrostatic models will be developed and served as test beds for the developments. The advanced computational and mathematical methods developed in this project will potentially impact the computation of geophysical fluid dynamics. The project will contribute a competitive extreme-scale discretization for the dynamical cores of future high-resolution earth system models."
"1620390","RUI:  Efficient Algorithms for Compressed Sensing and Matrix Completion","DMS","COMPUTATIONAL MATHEMATICS","12/15/2016","12/13/2016","Jeffrey Blanchard","IA","Grinnell College","Standard Grant","Leland Jameson","11/30/2019","$116,309.00","","blanchaj@grinnell.edu","733 BROAD ST","GRINNELL","IA","501122227","6412694983","MPS","1271","9229, 9263","$0.00","Traditionally, a signal is measured by acquiring every component in the signal and then compressing the signal with an appropriate computational algorithm. For example, digital cameras capture an image with a huge number of pixels and then a compression scheme such as JPEG is used to reduce the size of the digital image for storage or dissemination. In many applications, the costs and challenges associated with acquiring measurements are considerable. In compressed sensing and matrix completion, the measurement process is altered in order to drastically reduce the number of measurements, but the signal reconstruction process is necessarily more difficult. Compressed sensing and matrix completion transfer the workload from the measurement process to computational resources dedicated to the signal reconstruction. Typical applications include compressive radar, geophysical data analysis, medical imaging, and computer vision. This project will take a holistic approach to data acquisition and algorithm development for compressed sensing and matrix completion where theoretical guarantees often rely on computationally expensive subroutines and apply to computationally burdensome measurement processes. Increased efficiency can be achieved through sparse measurement operators, relaxed subroutine requirements in iterative greedy algorithms, and the implementation of these algorithms on computation accelerating hardware.<br/><br/>Compressed sensing combines the acts of signal acquisition and compression into a single operation. Computationally efficient algorithms then produce accurate approximations to sparse signals by exploiting the underlying simplicity that the signal has relatively few important components. Matrix completion similarly exploits the simplicity of the target matrix having only a few independent columns; in other words, one recovers a low rank matrix from a limited number of measurements. While leading greedy algorithms for compressed sensing and matrix completion have theoretical guarantees defining the number of measurements required for accurately recovering the underlying low dimensional signal, these guarantees require many more measurements than practical for applications. Furthermore, many of the algorithms employ theoretically useful but computationally expensive subroutines. Observed performance of more computationally efficient measurement operators encourages the adoption of techniques in practice that lack worst case, uniform guarantees for acquisition and reconstruction. This project seeks to balance the competing desires for theoretical guarantees and fast, efficient algorithms. The project will pursue theoretically viable algorithms which are also practically useful and provide solutions to linear inverse problems in reasonable amounts of computational effort including power, time, and affordable hardware. At the same time, establishing empirical performance characteristics for computationally efficient measurement operators and recovery algorithms which lack precise guarantees will help guide practitioners and theorists in future research. To provide near real time solutions to these computationally intensive algorithms, the project will also further accelerate computation by designing and disseminating algorithm implementations which exploit the massively parallel computations available on high performance computing graphics processing units."
"1619818","Closing the Duality Gap:  Decomposition of High-Dimensional Nonconvex Optimization","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","07/12/2017","Mengdi Wang","NJ","Princeton University","Continuing Grant","Leland Jameson","08/31/2019","$200,000.00","","mengdiw@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1271","9263","$0.00","In the modern computerized age, nonconvex optimization remains a critical computational challenge. Nonconvexity of an optimization problem implies a combinatorial structure, which often makes the computation problem fundamentally hard. Efficient computation tools with global approximation guarantees are in high demand. The principal investigator will study a class of nonconvex optimization problems that naturally arise from distributed intelligence systems, sparse estimation and data analysis.  The proposed research will contribute new computation tools for data analytics, statistic and machine learning, distributed and parallel computing, and multi-agent intelligence systems. The project will also develop two new courses for both undergraduate and graduate students at Princeton and also will involve undergraduate students in the research project via the Princeton undergraduate summer research program.<br/><br/>This research project aims to tackle an important class of nonconvex problems utilizing their geometric structure via a systematic dualization approach. The result is expected to advance the non-convex optimization theory as well as to provide algorithmic solutions to a large variety of distributed systems. Specifically, the principal investigator plans to study the non-convex duality for a class of non-convex optimization problems that admit a near-separable structure, with extensions to minimax problems and variational inequalities, and to develop computation tools that produce approximate global optimal solutions with complexity guarantees. In addition to the fundamental aspects, the principal investigator aims to investigate practical algorithms tailored to specific problems in high-dimensional structural estimation, sparse learning, and distributed optimization. The theoretical results and new methodology are expected to advance the theory of non-convex optimization as well as to provide algorithmic solutions to a variety of computational challenges."
"1620152","Collaborative Research: RUI: Three-Dimensional Multiphysics Simulation of Multi-phase Flows with Magnetic Fluids","DMS","COMPUTATIONAL MATHEMATICS","09/15/2016","09/15/2016","A. David Trubatch","NJ","Montclair State University","Standard Grant","Leland Jameson","08/31/2021","$98,248.00","","david.trubatch@montclair.edu","1 NORMAL AVE","MONTCLAIR","NJ","070431624","9736556923","MPS","1271","9229, 9263","$0.00","Advances in the synthesis of Ferrofluids (engineered fluids that respond to magnetic fields and have a number of well-established industrial applications) have increased the scope of potential applications of these fluids to new areas. Emerging applications of ferrofluids include: magnetic targeting of drugs, cell sorting in biomedical systems and magnetically driven contaminant removal. In each of these applications the use of ferrofluids enables new techniques that depend on the use of magnetic fields for 'remote control' of the ferrofluid. However, the realization of such technologies is hampered by complexities in the simulation of these systems for further development and design. The proposed research program includes the development of effective, robust computational tools that will enable such simulations. In particular, the computing codes will include the particular magnetic physics of ferrofluids as well as the forces resulting from the magnetic fields, which serve as the means of control in these applications. The development of these effective simulation tools will support and accelerate innovation in these emerging technologies. Moreover, the proposed program of code development, simulations and integrated physical experiments will serve as a proof-of-concept for the inclusion of realistic multiphysics fluid simulations for complex scientific and  engineering applications. The proposed research program will involve undergraduate and masters-degree students in leading-edge research, including students who are members of groups under-represented in STEM disciplines such as women and first-generation college students.<br/><br/>In magnetic drug targeting, a ferrofluid whose constituent nanoparticles have been functionalized to carry theraputic drugs is directed to a tumor or other localized site (e.g., in the eye); sorting of(nonmagnetic) biological cells by immersion in a ferrofluid so that the force of an applied magnetic field depends on cell size; purification of a polluted fluid by adsorption of contaminants to magnetic nanoparticles, which are then separated from the fluid by magnetic forces. However, advances in these applications are stymied by the complex, multi-scale and multi-physics nature of the fluid-dynamical systems in which they occur. In particular, because contemporary fluid-dynamics codes are not designed to incorporate the additional physics of magnetic-fluid systems, effective simulation with these codes is difficult. The proposal describes a plan to develop and test a new parallel, multi-phase code for fully three-dimensional flows. This project will lead to a flexible and efficient, multi-phase magnetic-fluid simulation code that is fully three-dimensional and parallelized for high-performance computing. Hence, the code will enable realistic simulations relevant to the significant applications addressed. Specifically, in order to address the above-noted applications, the code will model and simulate flows with dynamic interfaces between the ferrofluid and other fluids. Moreover, the code will implement models of viscosity effects (magnetoviscosity) as well as driving forces that result from applied magnetic fields (magnetophoresis) in a flexible manner that simplifies adjustment and update of the models."
"1636610","Graduate Student Participation in National Workshops to Encourage Women's Engagement in Mathematics Research","DMS","INFRASTRUCTURE PROGRAM, ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","07/15/2016","07/12/2016","Ami Radunskaya","RI","Association for Women in Mathematics, Inc.","Standard Grant","James Matthew Douglass","06/30/2018","$49,962.00","Magnhild Lien, Kathryn Leonard","aer04747@pomona.edu","201 Charles St","Providence","RI","029042213","4014554086","MPS","1260, 1264, 1271, 7334","7556","$0.00","This award supports graduate student participation at four workshops presented by the Association for Women in Mathematics (AWM) during major US mathematics meetings in 2016-2018. These meetings are the annual meeting of the Society for Industrial and Applied Mathematics in 2016 (held in Boston, MA) and 2017 (held in Pittsburgh, PA), and the Joint Mathematics Meetings in 2017 (held in Atlanta, GA) and 2018 (held in San Diego, CA).  These Meetings Workshops will bring the rising women stars and current mathematical leaders together with graduate students and recent graduates from a variety of fields in pure and applied mathematics. By creating a rich intellectual community centered on shared research interests, the activities promise to generate new mentoring relations, new research collaborations, and increased activity in the associated research areas. Such relationships can lead to a lifetime of professional success. Moreover, these events at highly visible national meetings benefit the entire mathematical community by showcasing the excellent mathematical work done by women.<br/><br/>Each Meetings Workshop has a research focus derived from a prior AWM Research Collaboration Workshop, a week-long event where participants work together on open research problems. At the Meetings Workshops, leaders in each research field will introduce younger mathematicians to a deeper understanding of the field and its connections with their own work. Graduate students will, in turn, share their research during poster sessions and small group interactions. The research topics that will be highlighted in the next four workshops include Dynamical Systems with applications to Medical Problems;  Number Theory;  Numerical Partial Differential Equations; and Noncommutative Algebra. These topics span a broad spectrum of current mathematical problems, and will correspondingly enhance the future careers of many young women mathematicians.  For more information on the first two conferences, please see https://sites.google.com/site/awmmath/programs/workshops/SIAM-workshop and https://sites.google.com/site/awmmath/programs/workshops/jmm-workshop.  This award is jointly funded by the Infrastructure, Algebra and Number Theory, Mathematical Biology, and Computational Mathematics Programs within the NSF Division of Mathematical Sciences."
"1620168","Novel numerical methods for fully nonlinear second order elliptic and parabolic Monge-Ampere and Hamilton-Jacobi-Bellman equations","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/19/2018","Xiaobing Feng","TN","University of Tennessee Knoxville","Continuing Grant","Leland Jameson","06/30/2021","$269,989.00","","xfeng@utk.edu","201 ANDY HOLT TOWER","KNOXVILLE","TN","379960001","8659743466","MPS","1271","9263","$0.00","Fully nonlinear second order elliptic partial differential equations (PDEs) arise from many scientific and engineering applications such as differential geometry, antenna design, astrophysics, geophysical fluid dynamics, image processing, mathematical finance, optimal mass transport, and stochastic optimal control. These PDEs are among most difficult PDEs to study analytically and to solve numerically. Two major and distinct classes of fully nonlinear second order PDEs often arise from applications, namely, the Monge-Ampere (MA) type PDEs and the Hamilton-Jacobi-Bellman (HJB) type PDEs. They have very different structures and arise from distinct application fields. However, a recent discovery by the PI's research team finds that these two classes of PDEs are intimately related. This finding opens a door for utilizing and adapting the relatively wealthy numerical methods and techniques for HJB-type PDEs to solve MA-type PDEs and enables a possibility for bridging the gap on numerical methods between those two major classes of fully nonlinear PDEs. It also provides a deeper understanding about the strength and weakness of the existing numerical methods for both classes of fully nonlinear PDEs. The education component of this research project is to engage and train two graduate students in developing necessary applied and computational mathematics knowledge and skills so that they can pursue a successful career in either academia or industry in the near future.<br/><br/>In this project, the PI will develop efficient numerical methods for both MA-type and HJB-type fully nonlinear PDEs. The PI will achieve the following goals in this project: (1) to establish equivalent (in the viscosity sense) HJB-reformulations for general MA-type equations, in particular, for the MA-type PDEs from optimal mass transport and for parabolic MA-type PDEs; (2) to systematically develop a high order semi-Lagrangian methodology and framework, which take the advantages of wide-stencil finite difference methods and unstructured triangular finite element and discontinuous Galerkin (DG) methods, for HJB-type and MA-type PDEs. (3) to develop convergent narrow-stencil finite difference, finite element and DG methods and framework for HJB-type and MA-type fully nonlinear PDEs based on some new and generalized numerical monotonicity concept; (4) to incorporate uncertainty into fully nonlinear PDE models by considering and developing efficient numerical methods for stochastic MA-type and HJB-type PDEs; (5) to apply the anticipated numerical methods to fully nonlinear PDE application problems arising from optimal mass transport, semigeostrophic flow, and stochastic optimal control from mathematical finance. By addressing the challenging numerical PDE problems and establishing fundamental numerical fully nonlinear PDE methodologies and theories, this project will have a significant theoretical and practical impact to the emerging field of numerical fully nonlinear PDEs and to computational and applied mathematics at large. The new numerical techniques can be used to solve various fully nonlinear PDE problems arising from differential geometry, antenna design, astrophysics, geophysical fluid dynamics, image processing, mathematical finance, optimal mass transport, and stochastic optimal control."
"1620293","Collaborative Research:   Multiphysics Modeling and Analysis of Thermo-Visco-Acoustic Equations with Applications to the Design of Trace Gas Sensors","DMS","COMPUTATIONAL MATHEMATICS","09/15/2016","09/09/2016","John Zweck","TX","University of Texas at Dallas","Standard Grant","Leland Jameson","08/31/2020","$149,157.00","Susan Minkoff","zweck@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1271","9263","$0.00","Trace gas sensors can be used to detect and identify very small quantities of gases for applications in such diverse fields as atmospheric chemistry, environmental and  industrial emissions monitoring, explosives detection, industrial  process control, and non-invasive medical diagnostics. The large-scale adoption of trace gas sensors requires sensor systems that are compact, portable, efficient, sensitive, cost-effective and highly reliable. Quartz Enhanced Photoacoustic Spectroscopy (QEPAS) sensors hold promise as a technology that may achieve many of these goals. In particular, QEPAS sensors can be as small as several cubic millimeters, whereas sensors based on other sensitive spectroscopic techniques require large cell volumes of tens to hundreds of cubic centimeters. QEPAS sensors use a quartz tuning fork to detect weak sound waves that are generated when a beam of light from a laser interacts with a trace gas. A major  engineering challenge to overcome before QEPAS  sensors can be widely deployed is to increase their sensitivity and lower their production cost. The overall goal of this project is to develop a computational model for QEPAS sensors that is a significant enhancement over existing models, and to then use this model to determine cost-effective designs that increase the sensitivity of QEPAS sensors. The major mathematical challenge of the project is to develop efficient computational methods to solve the multiphysics equations that form the basis of the model. The project will provide broad training in computational science for two mathematics graduate students from faculty mentors with complementary expertise in the  physics and engineering of the application, mathematical modeling, numerical analysis, and parallel computing.<br/><br/>QEPAS sensors employ a resonantly vibrating quartz tuning fork to detect  weak acoustic pressure waves and thermal disturbances which are generated when optical radiation from a laser beam interacts with a trace gas. The project will involve the development and analysis of computational methods to solve a system of Helmholtz equations that describes the interaction between a thermo-visco-acoustic fluid and a resonantly vibrating mechanical structure (a quartz tuning fork). The model will be used to numerically  optimize the QEPAS signal as a function of the geometric parameters of the sensor. The cumulative effect of the damping of the tuning fork by the viscous fluid will be computed in terms of the geometric parameters of the system and physical constants. Consequently, the model will allow for realistic optimization of QEPAS sensors by varying the tuning fork geometry. Furthermore, in some situations, the thermal diffusion wave can dominate the acoustic pressure wave on the surface of the tuning fork, in a phenomenon known as Resonant Opto-Thermo-Acoustic DEtection (ROTADE). Current mathematical descriptions of these sensors cannot capture both QEPAS and ROTADE phenomena simultaneously, although experimental data indicates that depending on the position of the laser beam along the tuning fork axis, both types of trace gas sensing may occur. The new model will allow for simultaneous simulation of both types of sensor systems.  Preliminary analytical and computational results show that standard finite element methods for solving the equations in the model are ineffective due to small parameters in the equations and  the high wave number of the solution. The small parameters produce an ill-conditioned linear system resulting from the finite element discretizations of the equations, while the high wave numbers can cause large phase errors in the computed solution (pollution error). This project will advance knowledge in computational mathematics by developing and analyzing block preconditioners for the multiphysics Helmholtz system. In addition, methods for reducing the pollution error will be developed  by extending higher-order finite element and interior penalty stabilization methods originally proposed for scalar Helmholtz equations to the multiphysics Helmholtz system. The techniques developed will be relevant for more general coupled Helmholtz systems such as those which arise in the study of thermal phenomena near thin bodies, the design of hearing aid transducers and micro-electrical-mechanical <br/>devices."
"1620027","Collaborative Research:  Mathematical Methods for Optimal Polynomial Recovery of High-Dimensional Systems from Sparse and Noisy Data","DMS","COMPUTATIONAL MATHEMATICS","09/15/2016","09/09/2016","Guannan Zhang","AL","Auburn University","Standard Grant","Leland Jameson","08/31/2019","$38,913.00","Yanzhao Cao","gzz0005@auburn.edu","321-A INGRAM HALL","AUBURN","AL","368490001","3348444438","MPS","1271","9150, 9263","$0.00","Current problems in approximation that are driven by applications in science and engineering, are typically formulated in very high dimensions. This project involves the study of different problems related to high-dimensional approximation, that arise in a large number of applications including neutron, tomographic and magnetic resonance image reconstruction, uncertainty quantification, optimal control and parameter identification, as well as in important areas of energy and material science.  The approaches used in this work will result in substantially improved and mathematically well-founded methodologies for computer simulations of solutions to real-world problems. The project will be centered around the interdisciplinary training of graduate students in computational data science and engineering. The results obtained will be disseminated through journal articles, conference talks, and a collaborative website. <br/><br/>In this effort we propose to develop novel mathematical techniques  for approximation of high-dimensional systems from a limited amount of sparse and noisy data.  The results of this effort will enable scientists to understand what are the number realizations of a nonlinear manifold that required to recover the entire high-dimensional solution map, with optimal approximation guarantees and minimal computational cost. Our rigorous mathematical approach includes: Novel weighted convex optimization and iterative thresholding techniques for optimal polynomial recovery, established via an improved estimate of the restricted isometry property; and Advanced multi-index methods that alleviate complexity and accelerate convergence of solutions by constructing model hierarchies with the use of reduced-basis techniques."
"1620150","Adaptive and high order PDF methods for nonlinear filtering problems","DMS","PROBABILITY, COMPUTATIONAL MATHEMATICS","07/01/2016","06/16/2016","Yanzhao Cao","AL","Auburn University","Standard Grant","Leland Jameson","06/30/2020","$160,000.00","","yzc0009@auburn.edu","321-A INGRAM HALL","AUBURN","AL","368490001","3348444438","MPS","1263, 1271","9150, 9263","$0.00","This research project investigates efficient and fast numerical algorithms of nonlinear filtering problems.  The goal of nonlinear filtering is to obtain best statistical estimate of the state, which can be the trajectory of an unmanned aerial vehicle, of a stochastic dynamical system based on noisy partial observations of the state.  As a key tool for data assimilation, nonlinear filtering has applications in vastly diverse research areas including biology, mathematical finance, signal processing and target tracking. A particular example of target tracking is the guidance and surveillance of unmanned aerial vehicles (UAV) which have been playing an essential role in national security and defense.  Through nonlinear estimates  of target location based on the observations from multiple sensors, nonlinear filtering forms a core component in unmanned aerial vehicle targeting and navigation systems.  Because of vast amount data at an increasing rate of availability, traditional nonlinear filtering methods such as Kalman filter and particle filter are often inadequate to handle high dimensional and highly nonlinear problems.  This research project aims to develop fast and adaptive numerical algorithms to attack such nonlinear filtering problems. Several graduate students will participate in this project. <br/><br/>This project is to conduct research in developing high order and adaptive numerical algorithms and the corresponding numerical analysis on nonlinear filtering problems.  The focus is on the PDF filter, which solves the nonlinear filtering problem by solving the conditional probability density function (PDF) for the optimal filter through a stochastic partial differential equation or a backward stochastic differential equation.   We will develop two classes of algorithms: the first solves the Zakai equation on adaptively constructed computational domains and on sparse grids; and the second solves a class of backward stochastic differential equations. Both algorithms overcome three difficulties for the PDF filter: i) high dimensionality; ii) low regularity; iii) unbounded domains. The overarching objective of this proposal is to make the PDF filter a highly competitive numerical method for nonlinear filtering problems. In particular, the novel BSDE based PDF filter studied in this proposal achieves a high order of convergence, which is much faster than all other existing nonlinear filtering methods.<br/>"
"1719461","Fast algorithms for large-scale nonlinear algebraic eigenproblems","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","08/01/2016","12/02/2016","Fei Xue","SC","Clemson University","Standard Grant","Leland Jameson","07/31/2019","$71,870.00","","fxue@clemson.edu","201 SIKES HALL","CLEMSON","SC","296340001","8646562424","MPS","1271, 8069","7433, 8084, 8396, 8609, 9150, 9263","$0.00","This project concerns development and analysis of new numerical algorithms for large-scale algebraic eigenproblems with nonlinearity in eigenvalues, eigenvectors, and parameters. These eigenproblems arise in electronic structure calculation, design of accelerator cavities, delay differential equations, vibration analysis of complex structures, and many more. Structure-preserving linearization techniques that have been developed recently are competitive for small or medium polynomial and rational eigenproblems, but they entail high computational costs for large-scale simulations due to the significantly enlarged dimension of linearized problems. In addition, linearization introduces considerable complications for the development of preconditioners, and it is not applicable to eigenproblems with full nonlinearity. <br/><br/>The PI shall develop novel iterative projection methods that are accurate, robust and efficient, for the solution of large-scale truly nonlinear eigenproblems. This goal can be achieved in part by exploration of special properties of different types of nonlinear eigenproblems that enable solution strategies similar to those for linear eigenproblems.  This investigation is focused on ( 1) new preconditioned eigensolvers, including conjugate-gradient-like and minimal-residual-like methods, for efficient solution of a large number of extreme and interior eigenvalues of problems with nonlinearity in eigenvalues, with and without the variational principle;  (2) fast inexact Newton-like methods to solve parameter-dependent degenerate eigenproblem for the study of (in)stabilities of dynamical systems;  (3) efficient algorithms for solving eigenproblems with nonlinearity in eigenvectors arising from condensed matter physics and electronic structure calculation. The research will develop a systematic and unified treatment of mathematical theory and development of numerical software."
"1620362","Collaborative Resesarch: New Algorithms For Group Isomorphism","DMS","COMPUTATIONAL MATHEMATICS","06/15/2016","04/27/2021","Peter Brooksbank","PA","Bucknell University","Standard Grant","Leland Jameson","05/31/2022","$78,058.00","","pbrooksb@bucknell.edu","ONE DENT DR","LEWISBURG","PA","178372005","5705773510","MPS","1271","9263","$0.00","Symmetry reduces large complex systems to manageable quantities of information. Identifying those symmetries and understanding their structure helps to solve a wide range of problems, from improving engineering tasks to disrupting the mechanisms of disease. The century-old problem of deciding whether two sets of symmetries have the same structure is known today as the Group Isomorphism Problem. This problem is fundamental to both computational algebra and computational complexity, and has implications for fields as diverse as material science, particle physics, and chemistry. The primary goal of this project is to develop significantly better approaches to testing isomorphism of finite groups of symmetries. It supports a new multidisciplinary collaboration between researchers at four universities, including students and early-career mathematicians and computer scientists. <br/> <br/>The Group Isomorphism Problem asks for an algorithm to decide whether two finite groups are equivalent. Both the problem itself, and the techniques designed to improve upon it, have implications for other computational problems, including the better-known problems of Graph Isomorphism and P versus NP.  Our team's approach goes beyond existing static recursions such as working sequentially down a derived or lower central series. Using a new dynamic strategy we prioritize the optimal stages of the problem, thereby improving the performance of later stages. To achieve this we are investigating the use of nonassociative rings, spectral sequences, modular representation theory, and p-local cohomology. We are also inspecting recently developed data structures in computational algebra that seem well-suited to our approach, as well as investigating applications to geometric complexity theory."
"1620337","Regularization of Hypersensitive Problems for Numerical Computation with Empirical Data","DMS","COMPUTATIONAL MATHEMATICS","09/15/2016","09/09/2016","Zhonggang Zeng","IL","Northeastern Illinois University","Standard Grant","Leland Jameson","08/31/2021","$179,984.00","","Z-Zeng@neiu.edu","5500 N SAINT LOUIS AVE","CHICAGO","IL","606254625","7734424671","MPS","1271","9263","$0.00","The aim of this project is the development of regularization theories, robust numerical algorithms, and a software package for problems that are known to be highly sensitive to data perturbations. Some of the fundamental problems in algebraic computation that remain at the frontier in numerical analysis, and where reliable algorithms and software are in demand, are of this nature. Extending on novel theories and algorithms/software developed under previous NSF support, the PI proposes to design algorithms for defective eigenvalue problems, to develop a numerical elimination strategy for polynomial systems, to validate the regularization theories, and to produce  software, NAClab. <br/><br/>This research attempts to bridge scientific fields of numerical analysis, computer algebra, algebraic geometry, and differential topology. Hypersensitive problems are known to be formidable challenges in practical computation particularly when empirical data are inevitably used. Advances in attacking those problems will enable wide range of applications.  The intellectual merit of this project lies in an innovative geometric analysis, proven regularization theory and an effective computational methodology for striking out the dreaded hypersensitivity in fundamental algebraic problems. This project is multidisciplinary in nature along with a major outcome in a robust, blackbox-type, and publicly available software toolbox NAClab to solve highly sensitive algebraic problems arising in sciences/engineering and to serve as building blocks for future algorithmic development.  The software will supply critical tools for application areas such as robotics, molecular conformation, chemical equilibrium, Nash equilibria, automatic control, as well as other branches of mathematics such as algebraic geometry."
"1620250","Fast algorithms for nonlinear kinetic models","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/19/2018","Jingwei Hu","IN","Purdue University","Continuing Grant","Leland Jameson","06/30/2020","$205,650.00","","hujw@uw.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1271","7237, 9263","$0.00","In multiscale modeling hierarchy, the Boltzmann and related kinetic equations serve as a building block that bridges atomistic and continuum models. These equations describe the non-equilibrium dynamics of a gas or system comprised of a large number of particles in random motion and constantly colliding with each other, and have found applications in various fields such as rarefied gas/plasma dynamics, radiative transfer, semiconductor modeling, etc. The prominent challenges associated with numerically approximating the Boltzmann-like equations are the expense of evaluating the collision term - a high-dimensional, nonlinear, nonlocal integral operator. Fast algorithms developed in this project will greatly advance the state-of-the-art simulation of collisional kinetic equations, and will enable scientists and engineers to effectively handle more complex systems that have previously not been feasible, due to the enormous expense of evaluating the collision term.<br/><br/>The new algorithms will be based on spectral approximation which stands out for its superior accuracy among the available Boltzmann solvers. To reduce the huge computational cost of conventional spectral methods, the main idea is to exploit and leverage the convolutional and low-rank structure in the collision integral. Four specific aims will be addressed: 1) fast algorithms for the Boltzmann collision operator with general collision kernel that will allow efficient simulation of particle interactions beyond hard sphere model; 2) fast algorithms for the multi-species Boltzmann equation, which would be invaluable for describing gaseous mixtures; 3)fast algorithms for the inelastic Boltzmann equation, which have immediate application in modeling granular materials; 4) a fast deterministic solver for the Schrodinger-quantum Boltzmann system modeling the kinetics of Bose-Einstein condensate at finite temperature."
"1620454","Collaborative Research: New Algorithms for Group Isomorphism","DMS","COMPUTATIONAL MATHEMATICS","06/15/2016","06/27/2016","James Wilson","CO","Colorado State University","Standard Grant","Leland Jameson","05/31/2020","$129,999.00","","james.wilson@colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1271","9263","$0.00","Symmetry reduces large complex systems to manageable quantities of information. Identifying those symmetries and understanding their structure helps to solve a wide range of problems, from improving engineering tasks to disrupting the mechanisms of disease. The century-old problem of deciding whether two sets of symmetries have the same structure is known today as the Group Isomorphism Problem. This problem is fundamental to both computational algebra and computational complexity, and has implications for fields as diverse as material science, particle physics, and chemistry. The primary goal of this project is to develop significantly better approaches to testing isomorphism of finite groups of symmetries. It supports a new multidisciplinary collaboration between researchers at four universities, including students and early-career mathematicians and computer scientists. <br/> <br/>The Group Isomorphism Problem asks for an algorithm to decide whether two finite groups are equivalent. Both the problem itself, and the techniques designed to improve upon it, have implications for other computational problems, including the better-known problems of Graph Isomorphism and P versus NP.  Our team's approach goes beyond existing static recursions such as working sequentially down a derived or lower central series. Using a new dynamic strategy we prioritize the optimal stages of the problem, thereby improving the performance of later stages. To achieve this we are investigating the use of nonassociative rings, spectral sequences, modular representation theory, and p-local cohomology. We are also inspecting recently developed data structures in computational algebra that seem well-suited to our approach, as well as investigating applications to geometric complexity theory."
"1620016","Multiscale Weak Galerkin Methods for Flows in Highly Heterogeneous Media","DMS","COMPUTATIONAL MATHEMATICS","08/01/2016","07/30/2018","Xiu Ye","AR","University of Arkansas Little Rock","Continuing Grant","Leland Jameson","07/31/2021","$216,522.00","","xxye@ualr.edu","2801 S UNIVERSITY AVE","LITTLE ROCK","AR","722041000","5015698474","MPS","1271","8396, 8611, 9150, 9263","$0.00","Fluid flow in porous media is important in many areas, including oil extraction and recovery, environmental protection, energy conservation, and the design and operation of fuel cells, solar cells, and batteries. Development of accurate, efficient, and reliable numerical schemes to simulate such fluid flow has received considerable attention in mathematics and engineering communities over the past decade.  However, mathematical modeling and numerical simulation of fluid flows in heterogeneous media and realistic settings remain a challenge. Much of the difficulty in porous media flow simulations is due to the involvement of different length scales, from macroscopic scale to microscopic scale. This research project aims to develop accurate, efficient, and reliable numerical algorithms for flows in porous media.  Weak Galerkin finite element methods (WGFEMs) will be developed for flows in highly heterogeneous domains, porous media, and complex flows in heterogeneous media. The methods under development are anticipated to significantly advance the utility of numerical analysis for realistic scientific and engineering applications.  Graduate students are involved in the project.<br/><br/>This project aims to develop new weak Galerkin (WG) finite element methods (FEMs) with excellent flexibility in element construction and mesh generation, suited to dealing with heterogeneous physical parameters. Additionally, it is envisioned that the new multiscale WGFEMs will be applicable in other fields, such as structural analysis, electromagnetic wave scattering, image processing, and computer vision. Collaboration with petroleum industry partners is planned in this research project."
"1620497","RUI: Development of Fast Scalable Adaptive High Order Methods for Solving the Boltzmann Equation","DMS","COMPUTATIONAL MATHEMATICS","06/15/2016","06/13/2016","Alexander Alekseenko","CA","The University Corporation, Northridge","Standard Grant","Leland Jameson","05/31/2020","$299,947.00","","alexander.alekseenko@csun.edu","18111 NORDHOFF ST","NORTHRIDGE","CA","913300001","8186771403","MPS","1271","033Z, 7237, 8396, 9229, 9263","$0.00","This project's goal is to advance one's ability to use computer simulations to address scientific and technological challenges by employing modeling at microscopic scales using the kinetic Boltzmann equation. Applications of this proposal span the dynamics of gas, plasma, self-organizing systems, networks, and bacterial dynamics. The project will focus on a bottleneck issue in kinetic modeling --- the development of fast methods for high fidelity simulations of particle interactions in rarefied gases. The project's most immediate impact is in the development of novel aerospace technologies and in important U.S. initiatives in the development of clean energy, biotechnology, and new materials. This will be through its applications to computer simulation of devices that either operate in rarefied gas or are manufactured in vacuum. The project will provide training for the STEM workforce by engaging students in research. <br/> <br/>Despite of being studied intensely in the last decades, deterministic numerical solutions of the Boltzmann equation continue to be evasive. To achieve a full three-dimensional solution suitable for use in applications, fast scalable adaptive numerical approaches for evaluating the five-fold Boltzmann collision integral need to be devised. This proposal will address these shortcomings by developing convolution formulations of the Boltzmann collision integral based on nodal discontinuous Galerkin (nodal-DG) discretizations in the velocity variable, by developing adaptable nodal-DG wavelet discretizations of the collision operator on octree meshes, and by developing fast algorithms for evaluating the convolution form of the collision integral based on an application of the Fourier transform. The new methods will require at most O(n^6) operations for a fully deterministic evaluation of the Boltzmann collision integral, and will require O(n^5) memory units to store the pre-computed collision kernels, where n is the number of discretization points in one dimension in the velocity space. The new methods will be implemented on parallel architectures and will be scalable. Implementation of this proposal will result in the development of capabilities for producing high-fidelity solutions to the Boltzmann equation, capabilities for producing benchmark solutions and methods for validation of kinetic models. The research activities will result in a new application of nodal-DG wavelets to the approximation of the Boltzmann collision integral."
"1620345","Collaborative Research:   Prediction, Optimization and Control for Information Propagation on Networks: A Differential Equation and Mass Transportation Based Approach","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","09/01/2016","03/11/2020","Hao-Min Zhou","GA","Georgia Tech Research Corporation","Standard Grant","Leland Jameson","08/31/2020","$164,825.00","Hongyuan Zha","hmzhou@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1271, 8069","8084, 9263","$0.00","Have you ever been wondering how fast news spreads or a topic becomes trendy in online social networks? It is actually very challenging to answer such questions quantitatively and accurately. The difficulty, in mathematical language, is that the process takes place in extremely large heterogeneous networks, and the spreads exhibit pervasive randomness. For example, a Twitter user may retweet a post at literally any time, or just ignore it. Therefore, understanding and predicting the spread of trendy topics are among the most emerging problems in social networking. The study also has applications in smartphone/computer malware outbreak and epidemiology of infectious disease since the spreads share similar mathematical underpinning. Therefore, we use a general notion of information propagation on networks to describe the dynamic nature of those problems. The 'information' being propagated on networks can be a trendy topic, a new computer malware, or an infectious disease; the nodes can be users of social networking sites, computers on the internet, or human hosts; and links in the networks can be the followee and follower relationships, the network connections of computers, or the proximity or physical contact between people. In this project, we aim at developing new theory and efficient computational methods for several important problems about information propagation on networks. We advocate a new approach to model the propagation as continuous-time discrete-space stochastic processes, and propose to address these problems by novel theory and algorithms rooted in modern optimal transport theory and Fokker-Plank equations on graphs. In particular, we focus on three closely related problems which are fundamental in information propagation: influence prediction, propagation optimization, and propagation control. We will develop efficient numerical methods based on the novel approach to tackle these problems, and expect the results can greatly advance our ability to understand and control information propagation.<br/><br/>The focus of this project is on theoretical analysis and computations of information propagation on large-scale heterogeneous networks. The research has extensive applications in the real-world including social networking, cyber security and epidemics of infectious diseases. We concentrate on the investigation of three key problems on prediction and decision-making related to information propagation on networks. 1) Influence prediction: for a given source set of active nodes in the network, predict the influence, i.e. expected number of activated nodes (nodes which receive the information) in the future. 2) Optimal source distribution: select an optimal source set of nodes to achieve maximal influence. 3) Network control: change and manipulate resource distribution and network topology dynamically to achieve the desirable outcomes for information propagation on networks. These problems are difficult to solve due to many factors, such as large scale and heterogeneous structure of networks, uncertainties in propagation, incomplete knowledge of propagation dynamics, and noise in datasets. To overcome these difficulties, we take a novel and effective approach which is different from any existing method. In particular, we establish systems of differential equations, based on recently developed Fokker-Planck equations on graphs, to describe and compute the time evolution of the probability density functions for the activation states of the network and estimate the influence. We design graph-based stochastic optimization methods, which are closely related to the recent advancements on optimal transport theory, to effectively find optimal source distribution and propagation control strategy. The proposed methods are efficient, accurate, and can tackle those problems on large-scale real-world networks."
"1620262","Fast spectral methods and their applications","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","07/05/2018","Jie Shen","IN","Purdue University","Continuing Grant","Leland Jameson","12/31/2019","$179,969.00","","shen@math.purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1271","7237, 9263","$0.00","Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations. Solutions for many problems of interest exhibit local singular behaviors which contaminate the accuracy of usual spectral/spectral-element methods. Many complex systems exhibiting anomalous diffusion can be better modeled with fractional partial differential equations, which are numerically challenging due to their nonlocal nature. Spectral/spectral-element methods usually lead to dense or block dense and ill-conditioned matrices that are difficult and expensive to solve. The focus of this project is to design, analyze, and implement fast and robust spectral methods for a class of numerically challenging problems. The numerical simulations will enable us to handle challenging problems having stringent accuracy and/or memory requirements with a reasonable cost in CPU and turn-around time, and will contribute to a better understanding of some fundamental issues in materials science and fluid dynamics through fast and accurate numerical simulations. Another important goal of this project is to engage graduate students in learning necessary skills of computational and applied mathematics so that they can pursue a successful career in sciences and engineering.<br/><br/>The PI will address these issues with the following tasks: (i) develop effective Muntz Galerkin method to deal with problems with singular solutions; (ii) develop efficient and accurate spectral methods for solving a class of fractional differential equations by constructing special basis functions with generalized Jacobi and Laguerre functions, and derive corresponding error estimates; (iii) develop direct structured solvers with optimal computational complexity for dense or block dense linear systems arising from spectral/spectral-element discretization; and (iv) develop efficient spectral algorithms for solving the phase-field model of electro-magnetic couplings in ferroelectric and multiferroic nanostructures. The research will result in fast and stable direct spectral/spectral-element solvers for a class of partial differential equations as well as fast Jacobi/spherical harmonic transforms. This project will also result in a set of computational modules to efficiently and accurately solve the coupled nonlinear system for the phase-field model of electro-magnetic couplings in ferroelectric and multiferroic nanostructures."
"1620038","Computing Spectral Distributions for Graph Analysis and Classification","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/22/2016","David Bindel","NY","Cornell University","Standard Grant","Leland Jameson","06/30/2020","$149,953.00","","bindel@cs.cornell.edu","341 PINE TREE RD","ITHACA","NY","148502820","6072555014","MPS","1271","9263","$0.00","Modern network science analyzes and interprets relationships such as friendships between people in a social network, interactions between proteins in a biological network, or connections between machines in a computer network.  A key challenge in large-scale network analysis is understanding how the network is composed of basic building blocks or motifs, such as communities in social networks or groups of proteins that function together in a biological network.  Once such building blocks have been identified, one would like to identify what things in the network play key roles in them.  This project will create novel methods for decomposing networks into basic building blocks using the mathematical tool of graph spectra. Just as physical scientists use the spectrum of light frequencies absorbed or emitted by an object to determine the atomic composition of the material, graph spectra let network scientists determine how networks are composed from basic atomic building blocks.  The project will produce fast software tools to compute graph spectra for networks involving relations between millions of entities, along with new analysis techniques to extract network building blocks from these spectral signals.<br/><br/>The main objective of this research is to extend spectral density analysis methods common in spectral geometry and physics to gain new insights into the structure and composition of complex networks. The research extends prior work in computational material science, introducing novel stochastic and deterministic estimators of local and global spectral densities with different tradeoffs with respect to cost and accuracy.  These methods will take advantage of the structure of localized eigenvectors associated with interior eigenvalues in the spectra of complex networks  and to find network motifs. New graph decomposition techniques based on these local spectral density estimates will enable new and practical tools for graph analysis, and spectral node classification techniques based on this approach will generalize current centrality-based measures of node importance to provide more detailed insights into the role nodes play in a network. In addition to providing new mathematical insights, the project will result in new software tools for use by a broad audience of network scientists, and will expose undergraduate researchers to spectral theory in network science."
"1615800","Modeling Tissue Division Patterns and Loss of Cell Polarity in Cancer","DMS","COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","08/01/2016","06/29/2018","Alexandra Jilkine","IN","University of Notre Dame","Continuing Grant","Zhilan Feng","07/31/2021","$180,184.00","","ajilkine@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","1271, 7334","9251, 9263","$0.00","Cancer represents one of the biggest problems for modern societies. In 2015, there were 1.6 million new cancer cases and 590,000 cancer-related deaths in the US. Mathematical and computational models enable critical examination of mechanisms that may be involved in the initiation and development of tumors. Due to ongoing renewal of tissue, some individual human cells divide as many as 5000 times during a lifetime, accumulating deleterious mutations that may result in a cell becoming cancerous. The principal investigator will test how the probability of tissues accumulating mutations depends on the spatial organization of cells, and the developmental stage of the cell where the initial cancer-causing mutation occurs. Determining which cellular division patterns slow down mutation accumulation and delay the onset of cancer will advance our understanding of mechanisms driving tumorigenesis and provide quantitative predictions that can be validated experimentally. The project will train undergraduate and graduate students through research involvement in an interdisciplinary field.  The mathematical models developed in this research project will be incorporated into a summer course on Cancer Dynamics in the q-bio Summer School that introduces students from diverse disciplines such as mathematics, physics, and computer science to modeling in biology.  <br/><br/>Dynamical models will be used to study the effects of cellular differentiation hierarchy on the estimated time to cancer development and treatment outcomes in various cancer types. Trade-offs between symmetric cell division and the production of more differentiated cells via asymmetric division will be explored in a stochastic model of different cell types and their spatial locations. On an intracellular level, a model for unequal distribution of cellular components (cellular polarity) that regulates asymmetric divisions will be developed.  Computational tests will examine the effects of mutations of different key proteins in the polarity biochemical network and relate them to macroscopic behavior in a population level model of tissue. This project will aid in understanding how disruption of asymmetric division mechanisms can act as a cancer promoting mechanism."
"1620198","Collaborative Research: Algorithm and Theory for Interface Computations","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/08/2016","Lingxing Yao","OH","Case Western Reserve University","Standard Grant","Leland Jameson","09/30/2018","$162,703.00","","lyao@uakron.edu","10900 EUCLID AVE","CLEVELAND","OH","441061712","2163684510","MPS","1271","9263","$0.00","Phenomena in which a fluid interacts with an immersed elastic structure abound in nature and in everyday life. Such fluid-structure interaction (FSI) problems include the swimming of microorganisms, the flying of birds, blood flow in the heart, and the deformation of leaves in the wind. One powerful way to understand such FSI problems is through computer simulation. Many FSI problems lead to challenging computational problems that call for significant improvements over currently available algorithms. The immersed boundary (IB) method is a popular computational method for FSI problems, and one major goal of this project is to understand the mathematical properties of the IB method to aid in the development of faster and more robust numerical algorithms. Another goal is to adapt the IB method so that it can handle problems in which fluid (water) can flow through an elastic structure. Such problems are particularly important for the understanding of movement and shape changes of biological cells. This cell-biological aspect of the work will be performed in collaboration with experimental biophysicists. The project will train undergraduate and graduate students in the mathematical and computational sciences through research on these problems.<br/><br/>This project consists of two major aims in theory and algorithmic development for computational problems with moving membrane interfaces. On the theoretical side, the PIs will establish a convergence theory for the immersed boundary (IB) method. The IB method is a widely used numerical method for fluid structure interaction problems, but despite its popularity, its convergence properties are poorly understood. Convergence analysis for the IB method will be one of the first to be established for a fluid structure interaction algorithm in which a dual grid is used; one for the fluid and another for the elastic structure. Such an analysis will clarify the effect of grid and time discretization parameters on the stability properties of the IB method. On the algorithmic side, the PIs will develop a numerical scheme to handle electrodiffusion of ions and transmembrane water flow in the presence of deformable elastic membranes. A novel feature of the osmotic water flow problem in contrast to conventional fluid structure interaction problems is that the interfacial membrane does not move with respect to the local fluid velocity and that this slip velocity is controlled by the jump in concentration of a diffusing chemical across the membrane interface. The fluid structure interaction will be treated with the IB method whereas chemical diffusion will be treated using a Cartesian embedded boundary method. This algorithm will be applied to study the interplay between electrophysiology/osmotic water flow and cell mechanics, an area that is poorly explored theoretically but whose importance is becoming increasingly clear."
"1620083","Robust Stability of Linear Dynamical Systems: Algorithms, Theory and Applications","DMS","COMPUTATIONAL MATHEMATICS","06/15/2016","06/19/2018","Michael Overton","NY","New York University","Continuing Grant","Leland Jameson","05/31/2020","$350,135.00","","overton@cs.nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1271","7433, 9263","$0.00","Dynamical systems are ubiquitous in our modern world, with embedded control systems in everything imaginable, from cars and airplanes to medical devices. Designing controllers for these systems based on feedback is a paradigm of increasingly great importance. Stability is the most important property of a dynamical system, so it is essential that controllers be designed so that systems are stable even in the presence of uncertain feedback. This research project aims to extend the theory and develop new computational algorithms in this important area.  The project will bring the tools of algorithms for stability analysis and controller synthesis to a wide community of scientists and engineers, most effectively through the provision of freely available software. The open-source software toolbox HIFOO, developed by the principal investigator, was designed for this purpose, stabilizing a given system with a fixed-order controller that locally optimizes appropriate objectives, such as the stability radius, over the controller variables. HIFOO has been used successfully in a wide variety of applications, including synchronization of heterogeneous multi-agent systems and networks, design of motorized gimbals that stabilize an angular motion of an optical payload around an axis, controllers for aircraft flight systems, and minimally invasive surgery. <br/><br/>We consider linear dynamical systems with uncertain feedback depending linearly on the output. This paradigm leads to an ordinary differential equation whose system matrix is a linear fractional map. The associated stability radius measures the size of perturbations that can be tolerated while still guaranteeing system stability, i.e., so that the eigenvalues of the system matrix are in the left half of the complex plane for all perturbations that are norm-bounded by a given quantity. A key goal of the project is to develop an efficient, scalable, accurate algorithm for computing the stability radius that is applicable to the case where the system matrix is large and sparse. Without loss of generality, the perturbations under consideration can be assumed to be matrices with rank one, so the algorithm under development depends on efficiently iterating with rank-one perturbations, exploiting the rank-one structure efficiently with existing eigensolvers.  An important aspect of the algorithm will be to ensure that the stability radius is computed accurately by making use of a theorem about the relationship between imaginary eigenvalues of a Hamiltonian matrix and singular values of the associated transfer matrix, using a novel approach that is specifically designed to be robust with respect to errors in the computation of these eigenvalues. The new algorithm under development in this project will allow the design of low-order controllers for much larger systems than was previously possible, including control of discretized systems of partial differential equations."
"1522767","Collaborative Research: Computation of instantons in complex nonlinear systems.","DMS","COMPUTATIONAL MATHEMATICS","02/01/2016","01/29/2016","Eric Vanden-Eijnden","NY","New York University","Standard Grant","Leland Jameson","01/31/2020","$100,000.00","","eve2@cims.nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1271","9263","$0.00","A wide variety of systems exhibit rare events -- events far from the average system behavior with low probability of occurring.  Rare events can have significant consequences, and improved understanding of their occurrence can aid in the design or management of such systems. The characterization of the likelihood of rare events is essential in all systems in which stochasticity plays an important role, as it allows us to take advantage of such events if they are desirable and to avoid them if they present a threat. The outcomes of this project will contribute to the understanding of rare events in complex systems, in particular, fluid dynamics and related geophysical systems. Further applications include the characterization of extreme events in the context of epidemics, population dynamics, and molecular biology. <br/><br/>The goal of this project is to develop efficient computational methods to characterize the most likely way rare events occur in complex stochastic systems and to estimate the tails of their probability distributions. For this purpose, the investigators will develop efficient algorithms to compute the so-called ""instantons"" that are minimizers of the action functional that large deviation theory associates with the stochastic differential equation describing the system's evolution. Numerical methods to calculate instantons will first be developed in the context of turbulence (in particular Burgers equation, magneto-hydrodynamics (MHD), Navier-Stokes equations, and the surface-quasi-geostrophic (SQG) equation) driven by diffusive processes. Then the investigators will extend the methods to stochastic equations that are driven by non-Markovian noise (fractional Brownian motion) or jump-processes, which play an important role in physics, biology, and chemistry. Finally, the investigators will calculate fluctuations around the instantons to get finer estimates of their probability of occurrence via prefactor calculations."
"1620100","Topics in Finite Element Analysis","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","08/13/2018","Johnny Guzman","RI","Brown University","Continuing Grant","Leland Jameson","06/30/2020","$210,000.00","","Johnny_Guzman@brown.edu","1 PROSPECT ST","PROVIDENCE","RI","029129127","4018632777","MPS","1271","9150, 9263","$0.00","The over-arching goal of this project is to design and develop numerical methods to solve important problems arising from engineering and biological sciences. In particular, three important problems will be investigated in the research. The first is to come up with efficient numerical methods that can split a region in two in a balanced way. This problem is known as Cheeger's problem, and the PI will develop numerical methods that do this efficiently and accurately.  The second problem is to analyze robust numerical methods to solve a class of elliptic interface problems. Interface problems have a numerous applications in fluid flow simulation, fluid-structure interaction modeling.  Finally, the PI will analyze new promising numerical methods for important and classical fluid flow problems.<br/><br/>For the first project, the problem boils down to solving a minimization problem for the L^1 norm. The approach the PI will take is to consider the minimization problem in L^p norm and let p tend to 1. In simple terms, one is taking a regularization of the original problem.  The L^p minimization problem will take the form of a p-Laplacian eigenvalue problem.  The advantage of this approach is that regularity results are available for the corresponding equations.  In the second problem, the PI will study immersed boundary finite element methods for second order elliptic interface problems. The goal of the project is to prove estimates for the full flux approximation in order to reveal the convergence behavior of the method.  Finally, the PI will study H(div) conforming and discontinuous Galerkin methods using upwinded fluxes for incompressible Euler's equations in two and three dimensions. The PI will prove optimal error estimates for the numerical methods so that they can provide a reliable guidance for computational simulations."
"1632111","International Workshop on Numerical Analysis of Singularly Perturbed Differential Equations","DMS","COMPUTATIONAL MATHEMATICS","06/15/2016","06/13/2016","James Adler","MA","Tufts University","Standard Grant","Rosemary Renaut","05/31/2017","$10,000.00","","jadler3@gmail.com","169 HOLLAND ST FL 3","SOMERVILLE","MA","021442401","6176273696","MPS","1271","7556, 9263","$0.00","Computational Mathematics, in general, and Numerical Analysis and Scientific Computing (NASC), in particular, are now well-recognized as providing a ""third mode"" of scientific investigation, complementing traditional theoretical and experimental approaches.  While the successes of NASC in both basic science and industry are many, one class of problems that remains challenging are those with rapid changes in the simulated quantities, such as arise with boundary and interior layers in fluid dynamics and other disciplines.  This grant supports US participation in an international workshop and short course on ""Numerical Analysis for Singularly Perturbed Differential Equations"", to be held on the campus of St. Mary's University in Halifax (Canada), from July 25-29, 2016.  NSF funds will allow US-based participants to benefit from learning both the basic tools in this area and the state-of-the-art, developing new collaborations with researchers from Canada and abroad. The PI will prioritize support for participants from underrepresented groups.<br/><br/>Problems with boundary and interior layers arise in many areas of scientific and industrial simulation, including mathematical biology, fluid dynamics, and electromagnetics.  Efficient parameter-robust simulation tools are needed in these fields to support the ever-increasing complexity of applied problems of interest to scientists and engineers.  Broadly, these problems can be categorized as ``singular perturbation problems"", where a small parameter is identified that controls the width of these critical layers.  In the limit as these parameters approach zero, nearly all standard discretization and mesh-refinement strategies fail to yield robust error control; thus, a specialized subfield of NASC has arisen to address efficient and robust simulation of singularly perturbed differential equations. This two-day short course will focus on finite-difference and finite-element discretizations on fitted-layer meshes, as well as adaptive mesh refinement strategies for these problems. The remainder of the workshop is aimed at collaborative research sessions among workshop participants to apply these tools to real-world problems.  There are many opportunities for broader impacts of the proposed short course and workshop, including the development of new interdisciplinary research connections."
"1620063","Robust Solvers for Coupled Problems with Applications to Electromagnetism and Poromechanics","DMS","COMPUTATIONAL MATHEMATICS","09/15/2016","07/20/2018","Xiaozhe Hu","MA","Tufts University","Standard Grant","Leland Jameson","08/31/2019","$160,658.00","James Adler","Xiaozhe.Hu@tufts.edu","169 HOLLAND ST FL 3","SOMERVILLE","MA","021442401","6176273696","MPS","1271","9251, 9263","$0.00","The goal of this project is to develop, implement, and study robust and efficient computational (Partial Differential Equation) solvers for large-scale systems of equations that describe coupled physical problems.  In particular, we aim to investigate applications in the computation of electromagnetic phenomena around obstacles, as well as poromechanic applications, such as the study of groundwater flow in porous media. Designing these solvers represents an important class of challenging problems in computational mathematics, because those coupled systems usually describe complex multiphysics phenomena across different time and spatial scales.  Currently, most efficient and robust solvers are developed for single-physics problems, whereas each tool we develop will strongly consider the importance of the inherent coupling.  The research will provide new computational paradigms for electromagnetics and poroelasticity, both of which have crucial applications in physics and engineering, such as fusion energy applications, shale gas recovery, carbon dioxide consolidation, and cardiac muscle behavior, to name a few. Finally, the project supports one graduate student. Through training and collaboration with investigators and other experts in the field, they will become involved in the broader research communities of scientific computing and engineering. <br/><br/>Each of the applications described above corresponds to a discretized coupled system of partial differential equations (PDEs).  Due to the complexity of the multi-physics and multi-scale phenomena described by such models, an essential component is efficient and robust nonlinear and linear solvers, due to the fact that the computational time needed to simulate complex physics is in many cases dominated by solving the large-scale linear systems of equations representing the discretized PDEs.  Therefore, this research focuses on developing, analyzing, and implementing efficient iterative methods and preconditioners for coupled PDE systems. More precisely, this is achieved by two possible approaches. When structure-preserving discretizations are applied, properties of the PDE models that are carried over to the discrete model are used to derive an exact block factorization and we will design novel preconditioners without approximating the Schur complements. For general numerical schemes, monolithic multigrid methods will be developed by generalizing the multigrid theoretical framework for indefinite coupled systems. Finally, by applying these new iterative solvers and studying their interplay with accurate discretization schemes, the investigators will create robust and efficient numerical simulations for systems such as Maxwell's equations and those describing poromechanics. More generally, the new solvers developed here will provide insight on how to use analytic tools in the design of algebraic solvers and novel theoretical foundations for the design of robust solvers for general coupled PDEs will be considered."
"1619904","OP: COLLABORATIVE RESEARCH: Integrated Simulation of Non-homogeneous Thin-film Photovoltaic Devices","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/08/2016","Peter Monk","DE","University of Delaware","Standard Grant","Leland Jameson","06/30/2020","$499,999.00","","monk@udel.edu","220 HULLIHEN HALL","NEWARK","DE","197160099","3028312136","MPS","1271","8396, 8399, 8990, 9150, 9263","$0.00","Solar cells are desirable as energy sources that neither use fossil fuels nor produce greenhouse gases. They must be designed to efficiently absorb sunlight and convert it to electricity. The solar cell needs to be sufficiently thick to absorb light across the solar spectrum. To reduce this thickness, and so reduce manufacturing cost, several layers of materials are used: first, to help light penetrate the solar cell, and then to trap it inside. Some of these layers are semiconductors in which electricity is generated, while others (for example, a periodically corrugated metallic back layer) may help absorption by trapping light near their surface. However, the amount of electricity generated by a solar cell does not just depend on absorption, but also on the transport of electrons within the layers of the solar cell. If the density of electrons decreases during transport, thereby trumping any gain in sunlight absorption, then a chosen light-management strategy will not be fruitful. The project team will develop an integrated pair of computer models that simultaneously predict the absorption of sunlight and the consequent electrical performance of the solar cell using modern techniques from numerical analysis. The codes will extend current simulation technology to allow for semiconductor layers with properties that vary from place to place and allow fully three-dimensional models of the device. Using their codes, the PIs will optimize device designs for best electricity generation. Definitive predictions will be provided about thin-film photovoltaic solar cells, thereby providing significant progress towards inexpensive and sustainable production of electricity. These codes will be made available to other photovoltaic researchers.<br/><br/>The overall model of the thin-film photovoltaic solar cell will have a photonic submodel and an electrical submodel. In the photonic model, the quasi-periodic Maxwell's equations will be solved using edge finite elements. To improve flexibility, the PIs will analyze and implement a non-standard mortaring technique to take care of quasi-periodicity. Compatible electrical models will be analyzed and implemented using the Hybridizable Discontinuous Galerkin (HDG) method on hexahedral elements for the non-linear convection and diffusion problem governing drift and diffusion of electrical charge carriers. The HDG scheme will require a novel analysis to understand this non-linear convection-diffusion problem. Stability and convergence will be explored first for linear convection-diffusion problems, and the methodology will then be extended to an implicit-explicit time-stepping scheme for the drift-diffusion system. Besides a full 3D model, the PIs will develop a 2D model assuming translation invariance of the photovoltaic device in one transverse direction.  The second step of the proposed research is to use the new simulation capability to design optimal nonhomogeneous thin-film photovoltaic solar cells via the Differential Evolution Algorithm. This will optimize for maximal photovoltaic electricity-generation efficiency. Additionally, domain and coefficient derivatives will be characterized and implemented to allow the computation of sensitivities and the use of gradient-based optimization. Detailed photonic-and-electrical modeling with doubly periodic back-reflectors and non-homogeneous light-absorbing layers will permit a major expansion of solar-cell design methodologies, besides yielding optimal designs for maximal photovoltaic electricity-generation efficiency."
"1620026","Nonlinear Instability of Navier-Stokes equations from a probabilistic point of view: Numerics and Simulations","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/20/2016","Xiaoliang Wan","LA","Louisiana State University","Standard Grant","Leland Jameson","06/30/2020","$183,001.00","","xlwan@math.lsu.edu","202 HIMES HALL","BATON ROUGE","LA","708030001","2255782760","MPS","1271","9150, 9263","$0.00","During the last two decades, there has been a widespread interest in uncertainty quantification from academia to industry, where stochastic models and approaches have been developed to effectively describe the propagation of uncertainty of different sources in complex systems, and the interplay of mathematical modeling and experimental data. For example, classical deterministic differential equations have been relaxed to a random one by taking into account the uncertainty in physical parameters, initial/boundary conditions, etc. This strategy is being employed more and more in industrial design to enhance robustness and efficiency. Bayesian inference has been applied to inverse problems in a more natural way to deal with the noisy observations, which actually turns an ill-posed deterministic inverse problem into a well-posed one from the probabilistic point of view. In this project the Principal Investigator considers a stochastic formulation of a classical problem in dynamical system - nonlinear instability of wall-bounded parallel shear flows.  The strategy will be based on a very general observation: under the excitation of noise, no matter how small the amplitude of noise is, states that are impossible for a deterministic mathematical model can be explored by a stochastic model. The issue of particular interest is the transitions to the anomalous states that occur rarely but have major impact, such as system failure, loss of stability, etc. <br/> <br/>The main mathematical tool in this project is the Freidlin-Wentzell theory of large deviations for small random perturbations of dynamical systems. One typical scenario of nonlinear instability in many wall-bounded flows is the subcritical bifurcation of Navier-Stokes equations with respect to the Reynolds number, where the equation can have at least two stable solutions for a certain Reynolds number. The Principal Investigator will recast nonlinear instability in a stochastic setting and regard it as a rare event triggered by small noise. Mathematically, a question considered is how the nonlinear instability develops as the amplitude of noise goes to zero. The large deviation principle asserts that such a transition will occur mainly following a path given by the minimizer of the action functional.  The mail goal of this project is twofold: 1) Develop efficient numerical algorithms to seek the most probable transition path that is critical for the development of nonlinear instability; and 2) Extensive numerical studies of the subcritical bifurcation of wall-bounded parallel shear flows. The first task will be achieved by hp adaptive finite element discretization in time direction and spectral method for spatial discretization incorporating with parallel computing. In particular, the pressure will be removed from the formula using a divergence-free space such that one can reduce the number of degrees of freedom and just focus on the instability. In the second task, the focus is on the relation between the action-based stability theory and classical stability theories such as linear stability theory, nonlinear stability theory, nonmodal theory, edge state and minimal energy perturbation study. More specifically, small noise will be added to some classical deterministic models in literature to seek extra information that cannot be given by deterministic stability theories."
"1619884","Efficient Methods for Large-Scale Self-Concordant Convex Minimization","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/19/2018","Quoc Tran-Dinh","NC","University of North Carolina at Chapel Hill","Continuing Grant","Leland Jameson","06/30/2020","$239,840.00","","quoctd@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1271","9263","$0.00","The literature on the formulation, analysis and applications of convex optimization is ever expanding due to its broad applications in signal processing, machine learning, statistics, and other fields of data science. In theory, many convex problems have a well-understood structure, and hence state-of-the-art first order and interior-point methods can obtain high accurate solutions. In practice, however, modern applications present a host of increasingly larger-scale and nonsmooth optimization problems that can render these methods impractical. Fortunately, recent advances in convex optimization offer a surprising new angle to fundamentally re-examine the theory and practice of large-scale convex optimization models in a unified fashion. Successful development of the PI's ideas will have several broad impacts in data analysis and computational science. While existing state-of-the-art approaches focus on certain classes of convex problems, the PI strongly believes that the approach proposed in this project can be expand to cover a wide range of unexploited convex optimization applications. The obtained theory and methods can be specified and customized to solve various problems in different fields, including massive data analysis, machine learning, high-resolution imaging science, operations research, networks, and control. Successful real-world applications and software development can create a major impact to practicians in academic and industry. The PI's broad collaborations are expected to make a significant progress in the application of convex optimization techniques. Several research topics in this project will be integrated into graduate training programs through special topic courses and PhD research directions, whereas undergraduate training activities can also benefit from this research via internship training and interdisciplinary collaborations.<br/><br/>This project focuses on exploiting and generalizing a prominent concept so-called self-concordance to develop new efficient convex optimization techniques to attack two classes of large-scale convex optimization problems, and will be integrated into three work packages (WPs). WP1. Composite self-concordant convex optimization: While existing convex optimization methods essentially rely on the Lipschitz gradient assumption which unfortunately excludes many important applications such as Poisson and graphical learning models, the PI instead focuses on the self-concordance structure and its generalizations. Such a concept is key to the theory of interior-point methods, but has remained unexploited in composite minimization. Grounded in this structure, the PI will develop novel and provable convex optimization algorithms for solving several subclasses of large-scale composite convex problems. He also plans to generalize this self-concordant notion to other subclasses of problems such as logistic loss functions and entropy models to cover a broader range of applications.  WP2. Constrained convex optimization involving self-concordant barriers: Various constrained convex applications are integrated with a self-concordant barrier structure, while other convex constraints often have a ""simple"" structure. Existing general-purpose convex algorithms solve these problems by mainly employing either a standard interior-point method or an augmented Lagrangian framework. The PI alternatively concentrates on exploiting special structures of these problems using the theory of self-concordant barriers and combining them with both the interior-point idea and the proximal framework to develop new and scalable algorithms equipped with a rigorous convergence guarantee, while offering a parallel and distributed implementation. WP3. Implementation and applications: This WP aims at investigating the implementation aspects of the proposed algorithms and upgrading the PI's SCOPT solver. The methods developed in this project will be validated through three concrete real-world applications: image processing involving Poisson models, graphical learning problems, and large-scale max-cut and graph clustering problems."
"1620455","Harmonic analysis, non-convex optimization, and large data sets","DMS","COMPUTATIONAL MATHEMATICS","10/01/2016","09/13/2016","Thomas Strohmer","CA","University of California-Davis","Standard Grant","Leland Jameson","09/30/2019","$179,979.00","","strohmer@math.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1271","9263","$0.00","Future scientific and technological progress will depend heavily on the generation of new information technology capabilities and novel methods from signal and image processing to deal with today's massive volumes of data. A research effort is proposed to create mathematical concepts and computational methods to address some of the key challenges in this important area. In particular, the PI will focus on the areas of imaging, high-dimensional data analysis, machine learning, and information theory. The project uses tools from computational harmonic analysis, operator theory, random matrix theory, and optimization yielding efficient numerical algorithms with rigorously-established properties under carefully stated conditions. The payoffs for society at large are many, including new information technology capabilities, improved methods for signal- and image processing, as well as better understanding of data mining tools for Big Data.<br/><br/>Two concrete topics of this research effort are:(i) Fast and reliable algorithms of non-convex problems: When dealing with massive data sets, many tasks involve the use of a heuristic algorithm to solve a non-convex optimization problem. Often these heuristic algorithms get stuck in local minima, that are far away from the global minimum. We will develop fast numerical algorithms that come with theoretical performance guarantees for a range of important data analysis tasks; (ii) Efficient algorithms for heterogenous and high-dimensional data: Existing methods for high-dimensional data are often computationally rather expensive and rely on stationarity and homogeneity of the data, thus limiting their use for massive, heterogenous data sets. The PI will derive a framework of computationally efficient methods for properly fusing and efficiently processing heterogeneous, high-dimensional data."
"1620331","Collaborative Research:  Multiscale Study of  Active Cellular Matter: Simulation, Modeling, and Analysis","DMS","COMPUTATIONAL MATHEMATICS","08/15/2016","08/16/2016","Michael Shelley","NY","New York University","Standard Grant","Leland Jameson","07/31/2019","$109,814.00","","shelley@cims.nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1271","9263","$0.00","Active cellular matter is the basis of novel synthetic active fluids made of mixtures of suspended cytoskeletal filaments and molecular motors. By consuming chemical fuel, the molecular motors (e.g., kinesins) can bind to and create actively moving crosslinks between the biofilaments (e.g., microtubules) to drive their relative motion, which leads to large-scale collective motions in the filament/motor mixture through hydrodynamic coupling. Synthetic active suspensions made of small numbers of components reveal how higher-order aspects of assembly and organization are built in living cells. These systems also present new challenges to our understanding, design, and analysis of materials, and have the potential to provide valuable new technologies such as autonomously moving and self-healing materials.<br/><br/>In this work, the investigators study active cellular matter composed of microtubules and molecular motors through multiscale methods, and tightly coupled modeling, analysis, and simulation. The project aims to understand the fundamental interactions underlying stress generation within bundles of rigid/flexible biofilaments that undergo dynamic instability, as well as the nonlinear dynamics and hierarchical pattern formation in large-scale collective motions. The project will also predict key material properties including its coherent structures, local heterogeneity, time- and length-scales, and material rheology. To resolve the physics at different length- and time-scales, several methods will be developed and integrated: (1) microtubule-motor interactions will be simulated using a kinetic Monte Carlo method; (2) the hydrodynamic interactions between objects of various shapes will be modeled using a nonlocal slender body/boundary integral method, together with fast summation methods; (3) a pseudo-spectral method will be implemented to simulate the collective motion through a continuous active liquid-crystal type model."
"1619892","HIGH-ORDER INVARIANT DOMAIN PRESERVING NUMERICAL METHODS FOR NONLINEAR HYPERBOLIC SYSTEMS","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","08/29/2016","Bojan Popov","TX","Texas A&M University","Standard Grant","Leland Jameson","08/31/2021","$249,084.00","Jean-Luc Guermond","popov@math.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1271","9263","$0.00","Many important physical phenomena are modeled by nonlinear systems of hyperbolic conservation laws. When approximating such problems one encounters sharp interfaces, contact discontinuities, shocks and other nonlinear wave interactions. In such regions high order approximation methods are not stable and exhibit oscillations. It is very important that these oscillations be controlled because preservation of mass, positivity or boundedness of the solution is critical in numerical simulations. Moreover, spurious oscillations are known to promote convergence to nonphysical weak solutions or simply lead to failure to produce an approximation. Any advance in this direction will have a broad impact insofar the class of problems we want to address touches many fields in engineering (mechanical, aerospace, nuclear, ocean, etc.), in environmental sciences, in geophysics, in petroleum engineering, etc. Proposing a novel robust approximation technique for solving nonlinear hyperbolic problems developing shocks or sharp interfaces will have impact in every areas of science and engineering where controlling or dealing with this type of phenomenon is still an enormous challenge.<br/><br/>The main focus of this proposal to investigate and develop new high-order approximation techniques for nonlinear hyperbolic systems on unstructured meshes in any space dimension. The project will be organized around three main objectives: (1) Development of high-order maximum principle preserving methods for scalar conservation equations. The emphasis will be on the design and analysis of numerical methods that are at least third-order in space and time. Convergence estimates for some of these methods will be established; (2) Design of invariant domain preserving methods for any hyperbolic system in any space dimension on non-uniform meshes. The goal is to construct invariant domain preserving methods that are at least formally second-order accurate both in space and time. The objective is to have methods that preserve all the invariant domains of the underlying physical system; (3) The last part of the project will consists of extending the new methodology to systems with source terms like the shallow water equations and radiative transport."
"1620222","Collaborative Research:   Multiphysics modeling and analysis of thermo-visco-acoustic equations with applications to the design of trace gas sensors","DMS","COMPUTATIONAL MATHEMATICS","09/15/2016","09/09/2016","Robert Kirby","TX","Baylor University","Standard Grant","Leland Jameson","08/31/2019","$90,000.00","","Robert_Kirby@baylor.edu","700 S UNIVERSITY PARKS DR","WACO","TX","767061003","2547103817","MPS","1271","9263","$0.00","Trace gas sensors can be used to detect and identify very small quantities of gases for applications in such diverse fields as atmospheric chemistry, environmental and  industrial emissions monitoring, explosives detection, industrial  process control, and non-invasive medical diagnostics. The large-scale adoption of trace gas sensors requires sensor systems that are compact, portable, efficient, sensitive, cost-effective and highly reliable. Quartz Enhanced Photoacoustic Spectroscopy (QEPAS) sensors hold promise as a technology that may achieve many of these goals. In particular, QEPAS sensors can be as small as several cubic millimeters, whereas sensors based on other sensitive spectroscopic techniques require large cell volumes of tens to hundreds of cubic centimeters. QEPAS sensors use a quartz tuning fork to detect weak sound waves that are generated when a beam of light from a laser interacts with a trace gas. A major  engineering challenge to overcome before QEPAS  sensors can be widely deployed is to increase their sensitivity and lower their production cost. The overall goal of this project is to develop a computational model for QEPAS sensors that is a significant enhancement over existing models, and to then use this model to determine cost-effective designs that increase the sensitivity of QEPAS sensors. The major mathematical challenge of the project is to develop efficient computational methods to solve the multiphysics equations that form the basis of the model. The project will provide broad training in computational science for two mathematics graduate students from faculty mentors with complementary expertise in the  physics and engineering of the application, mathematical modeling, numerical analysis, and parallel computing.<br/><br/>QEPAS sensors employ a resonantly vibrating quartz tuning fork to detect  weak acoustic pressure waves and thermal disturbances which are generated when optical radiation from a laser beam interacts with a trace gas. The project will involve the development and analysis of computational methods to solve a system of Helmholtz equations that describes the interaction between a thermo-visco-acoustic fluid and a resonantly vibrating mechanical structure (a quartz tuning fork). The model will be used to numerically  optimize the QEPAS signal as a function of the geometric parameters of the sensor. The cumulative effect of the damping of the tuning fork by the viscous fluid will be computed in terms of the geometric parameters of the system and physical constants. Consequently, the model will allow for realistic optimization of QEPAS sensors by varying the tuning fork geometry. Furthermore, in some situations, the thermal diffusion wave can dominate the acoustic pressure wave on the surface of the tuning fork, in a phenomenon known as Resonant Opto-Thermo-Acoustic DEtection (ROTADE). Current mathematical descriptions of these sensors cannot capture both QEPAS and ROTADE phenomena simultaneously, although experimental data indicates that depending on the position of the laser beam along the tuning fork axis, both types of trace gas sensing may occur. The new model will allow for simultaneous simulation of both types of sensor systems.  Preliminary analytical and computational results show that standard finite element methods for solving the equations in the model are ineffective due to small parameters in the equations and  the high wave number of the solution. The small parameters produce an ill-conditioned linear system resulting from the finite element discretizations of the equations, while the high wave numbers can cause large phase errors in the computed solution (pollution error). This project will advance knowledge in computational mathematics by developing and analyzing block preconditioners for the multiphysics Helmholtz system. In addition, methods for reducing the pollution error will be developed  by extending higher-order finite element and interior penalty stabilization methods originally proposed for scalar Helmholtz equations to the multiphysics Helmholtz system. The techniques developed will be relevant for more general coupled Helmholtz systems such as those which arise in the study of thermal phenomena near thin bodies, the design of hearing aid transducers and micro-electrical-mechanical <br/>devices."
"1619759","Workshop for Women in Shape Analysis","DMS","COMPUTATIONAL MATHEMATICS","06/15/2016","06/13/2016","Erin Chambers","MO","Saint Louis University","Standard Grant","Leland Jameson","05/31/2017","$9,000.00","Kathryn Leonard","erin.chambers@slu.edu","221 N GRAND BLVD","SAINT LOUIS","MO","631032006","3149773925","MPS","1271","7556, 9150, 9263","$0.00","This grant supports travel for US women to the Workshop for Women in Shape Analysis (WiShT), to be held at the Nesin Mathematics Village in Istanbul, Turkey, in the week June 5-12, 2016.  The workshop is designed to strengthen the shape modeling community by bringing together women researchers at various stages in their careers (from graduate student to senior researcher) and from across the world, to foster research collaboration and mentorship. Because shape modeling is interdisciplinary in nature, drawing from mathematics, computer science, and biotechnology, collaborations are especially crucial. Furthermore, since no single country has many women working in shape modeling, international collaborations are essential to forming robust networks of women researchers. Hosting the workshop in Turkey reflects a conscious effort to make the workshop more accessible for women from the Middle East and Africa.  Participants will spend one week working together in small groups to solve one of a selection of open questions in shape modeling. Instead of the more typical workshop structure where participants watch presentations of established results, WiShT participants will begin generating new results in collaboration with other participants. Following the workshop, the research network will be maintained and strengthened by publishing a proceedings volume, establishing a website and listserv, and organizing follow-up conferences and reunions for participants. Mentoring and professional development will happen both formally and informally. For more information about the workshop see http://nesinkoyleri.org/eng/events/2016-wis2/index.php<br/><br/>Shape analysis is a critical component in many applications areas, including image recognition, medical imaging, biomedical engineering, graphics, and computer animation. The workshop will tackle three very different challenging problems in shape modeling and analysis in groups led by experienced researchers, and the broad range of participants will allow an interdisciplinary team of scholars to make substantial progress in each area. Group 1 will explore the many possible definitions of shape complexity and will study the relationships between them in the hope of deriving an overarching mathematical theory of shape complexity and an understanding of when each notion of complexity is most appropriate. The second group will focus on combinatorial optimization for shape-based segmentation models, both discrete and continuous.  Each model presents challenges: continuous models may not converge to optimal solutions or may be much slower, while in the discrete setting, integrating shape information or targeting structures with a particular predefined prior is quite challenging. Group 2 will combine these approaches by exploring the mapping of variational formulations to the discrete domain and tackle the challenges associated with optimizing the resultant discrete models. Group 3 will focus on convolution skeletons in both two and three-dimensions. The goal is to develop a method to transform an input shape and associated convolution skeleton into a new skeleton that is robust, compact, and reproducible that gives a smooth parameterization of the original shape. Results from each group will be published in an AWM-Springer volume, as well as disseminated via conference and journal publications."
"1732434","Collaborative Research: An Integrated Approach to Convex Optimization Algorithms","DMS","COMPUTATIONAL MATHEMATICS","10/21/2016","01/31/2017","Anne Gelb","NH","Dartmouth College","Standard Grant","Leland Jameson","08/31/2018","$27,735.00","","annegelb@math.dartmouth.edu","7 LEBANON ST","HANOVER","NH","037552170","6036463007","MPS","1271","1271, 9251, 9263","$0.00","Image reconstruction and feature extraction have been important aspects in various applications such as medical resonance imaging (MRI) and synthetic aperture radar (SAR). However, these procedures involve challenges. Different applications may vary in data acquisition (sampling) domains, levels of detail required, and processing domains for the features of interest. The data acquisition is usually under-prescribed and noisy. The sampling domains and/or processing domains may not be well suited for the underlying question. All of these make the problems ill-posed, and various regularization techniques are necessary to study the problems by formulating them as convex optimization models. This project will develop an integrated framework of investigating such convex optimization models. The project will provide graduate students with opportunities for training through research involvement and will prepare them for careers in science and engineering. <br/><br/>The PIs aim to propose a systematic way of evaluating various regularization techniques in such models, conduct a rigorous numerical analysis of the models, and develop efficient numerical algorithms of solving the models. Specifically, the PIs will address the following technical questions: (1) What constraints must be placed on the collected data in order to construct a numerically robust approximation to the underlying function? (2) How quickly and in what sense does the approximation converge? (3) Are the corresponding numerical algorithms developed for the fidelity and regularization terms viable? (4) How well are perturbations from the original data tolerated? The project aims to provide answers to all of these questions."
"1620231","A Hierarchical Multiscale Method for Nonlocal Fine-scale Models via Merging Weak Galerkin and VMS Frameworks","DMS","COMPUTATIONAL MATHEMATICS, Leadership-Class Computing","09/01/2016","05/15/2019","Arif Masud","IL","University of Illinois at Urbana-Champaign","Standard Grant","Leland Jameson","08/31/2020","$164,274.00","","amasud@uiuc.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1271, 7781","9263","$0.00","Many problems in the natural sciences and engineering involve phenomena that possess a spectrum of material, spatial, and temporal scales which correspond to coarse and fine scale physics. Very often fine scale physics is also nonlocal and therefore these problems pose a great challenge to the current computational techniques. Nonlocality of fine-scales and advecting sharp gradients are two key ingredients in the modeling of several classes of fluid dynamics problems. This research effort focuses on the development of variationally based methods for problems with steep gradients and discontinuities in the underlying fields and wherein fine scales have a nonlocal feature. Of specific interest are propagating steep fronts for which interacting discontinuities challenge the stability of the numerical methods. Typical examples are chemically reacting fluids permeating through porous elastic solids where fast reaction rates produce steep concentration fronts. Such problems arise in (i) high temperature injection molding of fibrous composites in micro and nanomaterials engineering, and (ii) enhanced oil recovery and secondary shale gas recovery processes in petroleum engineering. Another class of problems from mathematical physics is advection dominated viscous flows leading to anisotropic turbulence. <br/><br/>The PI will develop a unique and novel, simultaneous top-down and bottom-up multiscale approach for consistent representation of both the hierarchy of scales as well as the structure of inter-scale coupling operators for complex fluid mechanics problems. It blends ideas from the Variational Multiscale (VMS) method that helps decompose the governing system of equations into coarse-scale and fine-scale sub-problems and then employs Weak Galerkin (WG) ideas at the fine-scale variational level to extract models for the finer physics. Weak continuity of functions that is facilitated by the Weak Galerkin method results in fine-scale models that are nonlocal. A further generalization of WG via Discontinuous Galerkin (DG) ideas provides a framework to develop methods for problems with sharp gradients on sound mathematical basis. This notion leads to variationally derived Discontinuity Capturing (DC) methods that are independent of the user-defined or user-designed parameters. Emphasis is placed throughout on variationally consistent interscale coupling with rigorous treatment of the continuity conditions that are critical for the mathematical and algorithmic stability. The resulting computational algorithms will be ideal for massively parallel computing on distributed systems where message passing traditionally has been a bottle neck. The variational structures underlying the new methods will increase local-solves that are cost effective because of local resident memory on the new generation of processors while substantially reducing global communication between processors, thereby leading to efficient and economic computations. The mathematical frameworks and computational algorithms emanating from this work will be broadly disseminated by publication in high-quality archival journals, and by presentations at high impact conferences."
"1622453","Collaborative Research:  Towards an Accurate, High-Fidelity Modeling System for Multiphysics and Multiscale Coastal Ocean Flows","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","09/15/2016","09/12/2016","Yingjie Liu","GA","Georgia Tech Research Corporation","Standard Grant","Christopher Stark","08/31/2020","$75,000.00","","yingjie@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1271, 8069","7433, 8012, 9263","$0.00","In correspondence with global warming and fast expansion of human activities in coastal waters, it is now becoming urgently needed to advance our modeling capabilities to a level of direct, high-fidelity simulation of many emerging, real-world, multiphysics, multiscale coastal ocean flow problems. Examples of these problems include oil spill at bottom of oceans, tidal energy extraction in coastal waters, etc. Currently, essentially there is no model at this level of simulation. The objective of this project is to develop computational methods and an unprecedented computer modeling system, which will be not only applicable to such problems but also a new platform to study many other problems.<br/><br/>In particular, a geophysical fluid dynamics model and a fully 3D fluid dynamics model are integrated via domain decomposition and two-way coupling into a single modeling system, in which the former captures mesoscale [O(10) km - O(10,000) km], background ocean currents and the latter resolves sub-mesoscale [O(1) m - O(10) km], complicated, local phenomena. The objective will be realized via three tasks: 1) Methods and algorithms with regard to solution accuracy, computational efficiency, and transmission condition. 2) Numerical experiments on the methods and algorithms, their validation in test problems, and a showcase application. 3) Development of a robust, efficient software package of the modeling system. The educational component aims at increasing awareness of students, especially those from underrepresented groups, on new computational problems and necessity for new methods of their solution, stimulating their interest in numerical algorithms and model development. The activities of this component include a series of seminars, teaching material development, etc."
"1620194","Fractional Partial Differential Equations and Related Nonlocal Models:   Fast Numerical Methods, Analysis, and Application","DMS","COMPUTATIONAL MATHEMATICS","10/01/2016","09/22/2016","Hong Wang","SC","University of South Carolina at Columbia","Standard Grant","Leland Jameson","09/30/2020","$249,999.00","Guiren Wang","hwang@math.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","MPS","1271","9150, 9263","$0.00","The project proposes to develop a novel mathematical modeling of micro- and nano-fluidics, which intersects engineering, biochemistry, nanotechnology, and biotechnology. The study of micro-and nano-fluidics has great potential to revolutionize the methods in biological and chemical applications, which has wide applications to the design of systems in which low volumes of fluids are processed to achieve multiplexing, automation, and high-throughput screening. Micro- and nano-fluidics is used widely in the development of inkjet printheads, DNA chips, lab-on-a-chip technology, micro-propulsion, and micro-thermal technologies. The project will also provide advanced interdisciplinary training to graduate and undergraduate students. All of these activities will have broad and long-lasting impacts and contribute directly to the intellectual infrastructure of the nation.<br/><br/>Nonlocal models such as fractional partial differential equations (FPDEs), fractional Laplacian, and peridynamics are emerging as powerful tools for modeling challenging phenomena including anomalous transport and long-range time memory or spatial interactions in a wide range of fields such as biology, physics, chemistry, finance, engineering, and solute transport in groundwater. These models provide more appropriate description of many important problems in applications than integer-order PDE models do. Two of the main reasons that nonlocal models have not been used extensively so far are as follows: (1) They generate numerical schemes with dense matrices and solutions with strongly local behavior, which are significantly more expensive to solve numerically than traditional integer-order PDE models. A naive simulation of a three-dimensional linear problem with a moderate number of grid points may take state of the art supercomputers hundreds of years to finish and so deemed unrealistic. (2) Nonlocal models introduce mathematical difficulties, which were not encountered in the context of integer-order PDEs. It is proposed to effectively address both points at this juncture. The fast numerical methods proposed herein will provide significant computational benefits for nonlocal models, and will facilitate their applications. Preliminary numerical experiments of a simple three-dimensional fractional PDE showed that the proposed method reduced the CPU time from 2 months and 25 days by a traditional method to 5.74 seconds and reduced storage significantly. The proposed mathematical and numerical analysis will provide a solid theoretical foundation for nonlocal models and related numerical approximations. The fast and accurate numerical methods and rigorous mathematical analysis results will be applied in the development of a novel mathematical modeling of micro- and nano-fluidics. The resulting mathematical model will be utilized in the study of micro- and nano-fluidics."
"1620271","Logic, Topology and Genomics","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","09/05/2016","Saugata Basu","IN","Purdue University","Standard Grant","Leland Jameson","08/31/2020","$200,000.00","","sbasu@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1271","9263","$0.00","The goal of the project is to apply methods of logic and topology to several important problems in genomics and medicine. The first application is mining large scale clinical databases for information that would be usable for clinicians and/or biological researchers. The PI plans to design and implement a method applying ideas from persistent homology theory in a logic-based framework.  The starting point of this approach is the notion of ""redescriptions"" introduced by Parida (senior consultant for the project) and Ramakrishnan in the context of knowledge discovery. A mathematical reformulation leads to  certain filtered complexes arising from set systems, which are then amenable to analysis using tools from topology.  A second application will be in the area of phylogenetics. Studying population admixtures is a very active area of  research   in population genomics. The PI will use topological methods to not only detect but also discriminate  ancient from recent admixture, and validate the  approach by testing it on large simulated populations where the  admixtures are known in advance. Generating such populations pose unique challenges that have been tackled by Parida  and her group recently.<br/><br/>A common practical difficulty encountered in many applications  of topological data analysis is computing persistent homology groups of filtrations of very large simplicial complexes. The sizes of these complexes makes the computations of their persistent homology bar-codes using current generation publicly available software impossible. The second part of the project will address this shortcoming.  The PI will investigate a new approach towards improving efficiency of computing persistent homology groups over existing algorithms. This approach will be useful in a wide variety of applications where topological data analysis  is currently being used. The PI plans to implement this algorithm and develop it into a general purpose software-package for computing approximations of persistent homology invariants of filtrations of large complexes. The project will bring together tools from two different areas of mathematics --  logic and topology -- in a novel way, as a method towards analyzing large data-sets. In addition, the PI will also study the underlying mathematical problems that come up -- on the interface of logic and topology which are fundamentally interesting in their own rights, and should have other applications as well. The PI also intends to work with a graduate student and involve them in all aspects of the proposed research."
"1632364","International Conference on Computational Mathematics and Inverse Problems","DMS","COMPUTATIONAL MATHEMATICS","08/01/2016","07/21/2016","Jiguang Sun","MI","Michigan Technological University","Standard Grant","Leland Jameson","07/31/2017","$24,700.00","","jiguangs@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","MPS","1271","7556, 9263","$0.00","The main goal of the International Conference on Computational Mathematics and Inverse Problems is to bring together an international collection of established specialists as well as junior researchers in the area of computational mathematics and inverse problems. This type of scientific gathering and interaction can have long-lasting impact on junior researchers and graduate students. A significant broader impact will be to bring young researchers into the field, and establish new connections between disparate groups. The conference will also build a bridge between computational mathematics and inverse problems. The exchange of ideas will be beneficial to researchers in both areas. In addition, there will be a significant attendance of graduate students. The conference is a great opportunity for them to access the cutting-edge research, find interesting research topics, learn from world leading researchers, and build the network/collaboration for future academic careers. Geographically, Michigan Technological University is a little isolated from major cities. The conference will provide a boost to the research activities and give the department more visibility in the mathematical sciences community.<br/><br/>On the basis of current research trends and the wide spectrum of applications, the conference focuses on the following topics: iterative methods for large sparse eigenvalue problems with applications in big data sciences, finite element methods for eigenvalue problems, computational electromagnetic, direct techniques in inverse scattering theory, full wave inversion. The conference will feature plenary talks given by international experts on the aforementioned topics, invited talks from junior researchers, and contributed talks mainly from graduate students. The topics of the conference involve rapidly developments at the frontiers on today's research in computational mathematics and inverse problems. On one hand, the demand of computational methods in inverse problems is ever increasing. On the other hand, the computational mathematicians can find interesting new problems in inverse problems. Applicability of the conference topics extends to many real world applications including big data sciences, high performance computing, medical imaging, non-destructive testing, underground imaging and exploration."
"1619778","Multiscale Computation in Kinetic Theory","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","07/09/2018","Qin Li","WI","University of Wisconsin-Madison","Continuing Grant","Leland Jameson","06/30/2020","$250,001.00","","qinli@math.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1271","7237, 8396, 8611, 9263","$0.00","Physical systems are modeled at different scales and to different approximations with different equations. The Schrodinger equation at the quantum level models the molecular and atomic scale.  Newton's laws in classical mechanics model the macroscopic scale.  The Boltzmann equation applies at the statistical level, where systems of many particles are studied, and the Navier-Stokes equation and others model distributed systems such as fluids in the continuum regime. A central question in applied mathematics and physics is to understand the relationships between the different models, and many tools (both analytical and numerical) have been developed for this task through the years. However, most of them idealize the systems under study and cannot tackle practical problems that have emerged in the study of complicated systems in chemistry, physics, and engineering. This project focuses on two longstanding challenges concerning these connections: the characterization of quantum information in the classical regime when chemical reactions are present, and the coupling between the statistical and the fluid description. The project aims to develop improved methods for the modeling of multiscale systems. <br/> <br/>Despite their fundamental importance in physics and engineering, effective mathematical analysis and computational techniques for multiscale problems in kinetic theory have remained rather elusive. The multiple scales inherent in many physical systems have posed notorious computational challenges.  This project concerns development of multiscale numerical methods in kinetic theory, including numerical capture of the hydrodynamic limit of Boltzmann-type equations and the semi-classical limit of the Schrodinger equation. Both have long been regarded as fundamental problems in kinetic theory. More specifically, the project focuses on capturing the non-adiabatic transition in the classical regime derived from quantum mechanics, and boundary layer effects that connect the fluid description with the statistical mechanical description. Both problems emerge in transition regimes, the multi-physics phenomena can be captured by none of currently available mathematical treatments, and the computation is far from being efficient. The project aims to develop and analyze efficient computational tools for these problems, focusing on treatment of boundary layers and interfaces and design of asymptotic-preserving schemes.  Besides leading to improved understanding of physical systems of these types, it is expected that the new tools under development could inspire treatments of similar problems emerging in other areas, for example, hyperbolic type problems with random media."
"1620465","Numerical and Analytical Investigations on Nonlocal Dispersive Wave Equations","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/15/2016","Yanzhi Zhang","MO","Missouri University of Science and Technology","Standard Grant","Leland Jameson","06/30/2020","$180,000.00","","zhangyanz@mst.edu","300 W 12th Street","Rolla","MO","654096506","5733414134","MPS","1271","9150, 9263","$0.00","Nonlocal dispersive wave equations have been recently applied in many areas such as electromagnetism, acoustics, cosmology, elasticity, biology, hydrodynamics, viscoelasticity, seismics, water wave, plasma, quantum mechanics, brain and consciousness, and so on. However, their nonlocality introduces considerable challenges in both mathematical analysis and numerical simulations. This project seeks to address fundamental issues related to mathematical modeling and numerical simulations of nonlocal dispersive wave equations as well as their solution properties. The proposed project will bridge the gap between different areas, enhance interdisciplinary research, and advance the application of fractional differential equations in practice. Since nonlocal wave equations have broad applications in physics, chemistry, biology and engineering, the research in this project has great potentials to advance the research and technology in relevant areas.<br/><br/>The main objectives of this research are to build mathematical and numerical treatments for the nonlocal Schroedinger wave equations, and to provide a deeper understanding of the modeling with long-range interactions, so as to advance their application to problems with nonlocality. In this project, both the discrete nonlinear Schroedinger (DNLS) equation with long-range interactions and the fractional nonlinear Schroedinger (fNLS) equation with the fractional Laplacian will be investigated. Integrating the discrete and continuous models offers a new opportunity for a deeper understanding of the nonlocality of the Schroedinger wave equations. On the one hand, accurate algorithms will be developed to improve the efficiency and reduce the computational costs in simulating the DNLS with large lattice sites, especially in two- or three-dimensional lattices. On the other hand, efficient and accurate numerical methods for discretizing the fractional Laplacian will be designed and applied to study the properties of the stationary states and dynamics of fNLS. The study on the DNLS and fNLS will provide a deeper understanding on modeling and properties of long-range interactions, as well as is benefitting the development of numerical algorithms for fractional differential equations.<br/>"
"1620434","A Framework for Multiscale/Multiphysics Mathematical Modeling of Cerebral Aneurysm Rupture","DMS","COMPUTATIONAL MATHEMATICS, Engineering of Biomed Systems, MSPA-INTERDISCIPLINARY","08/01/2016","06/13/2016","Yue Yu","PA","Lehigh University","Standard Grant","Leland Jameson","07/31/2020","$235,000.00","","yuy214@lehigh.edu","Alumni Building 27","Bethlehem","PA","180153005","6107583021","MPS","1271, 5345, 7454","8007, 8091, 9263","$0.00","Cerebral aneurysm (CA) is a diseased dilatation of an intracranial artery, and its rupture is the leading cause of subarachnoid bleeding. However, the mechanisms behind aneurysm formation, growth and rupture remain an enigma. While the current clinical technology cannot yet provide a lot of mechanistic details of these processes in vivo, many efforts have been devoted in modeling the biomechanics of the cerebral aneurysms. Specifically, numerical simulations have elucidated some of the physics associated with the arterial wall damage and aneurysm rupture. The proposed work aims to provide a multiphysics/multiscale mathematical model along with a numerical framework to understand the mechanism of cerebral aneurysm rupture. The new knowledge will be introduced into both graduate and undergraduate level courses. The resultant software will be ready for classroom use as friendly and free opensource routines for instructors and students.<br/><br/>This project aims to develop a new methodology for addressing fundamental open questions in multiscale and multiphysics modeling of brain aneurysms, and to study the interactions between the arterial wall and the blood flow with an emphasis on simulating the rupture phenomena. To be specific, the computational domain is composed of three regions: the fluid (blood) simulated as incompressible Newtonian flow, the fracture solid (aneurysm fundus wall) modeled by the nonlocal peridynamic theory, and the solid (arterial wall) described by a viscoelastic model. These three subregions will be numerically coupled to each other with proper interface boundary conditions. In preliminary work, the PI has: (1) developed new schemes for fluid-structure interaction (FSI) to stabilize and accelerate the coupling between the fluid solver and the classical solid solver; (2) designed an efficient long-term integration method for fractional-order PDEs (FPDEs) and found that the fractional order might serve as an indicator for the aneurysm wall strength; (3) investigated the Dirichlet-Dirichlet boundary condition for the peridynamic-classical theory coupling. In the next three years, the PI will work on both theoretical and numerical aspects. For the theoretical part, new models will be addressed to describe the viscoelastic behavior of the arterial walls and to capture the material failure near the aneurysm fundus. Regarding the numerical effort, high-performance computational tools based on high-order continuous/discontinuous Galerkin methods will be developed, which could accurately simulate the new models as well as provide a coupling framework for problems composed of heterogeneous domains with multiscale/multiphysics dynamics. Technically, the PI will: (1) further validate the fractional-order PDE models that better describe the viscoelastic behavior of cerebral aneurysm walls; (2) for the first time develop two-component peridynamic theory for modeling the aneurysm rupture, and develop high-order numerical solvers for this model based on the discontinuous Galerkin method; (3) design partitioned approaches for coupling the 3D continuum formulations of peridynamics and classical theory, by investigating proper mathematical interface conditions directly derived from conservation laws. The coupling techniques the PI has investigated in preliminary work (FSI coupling) would also be adopted into the multiscale coupling problem here. This project is co-funded by the Computational Mathematics Program of the Division of Mathematical Sciences, the BioMAPS Initiative and the Biomedical Engineering program of the Division of Chemical, Bioengineering, Environmental and Transport Systems Division (CBET)."
"1620058","Advanced numerical methods for multiphysics Magnetohydrodynamics","DMS","COMPUTATIONAL MATHEMATICS","08/15/2016","08/30/2016","Jean-Luc Guermond","TX","Texas A&M University","Standard Grant","Leland Jameson","07/31/2021","$270,000.00","","guermond@math.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1271","9263","$0.00","The objective of this project is to develop innovative numerical methods capable of solving energy-related problems in the context of renewable and alternative energies. The numerical techniques developed in this project will help design grid-scale liquid metal batteries capable of storing large quantities of renewable energies. This research will also help improve the performance of large power electric transformers cooled by environment-friendly vegetable-based oils containing ferromagnetic particles. Finally, by facilitating the understanding of magneto-hydrodynamic instabilities in liquid metals, this project will help to ascertain the integrity and the efficiency of the electromagnetic pumps that will be used to extract energy from the next generation of Liquid-Metal Fast-Breeder Reactors and Tokamaks. This project will be done in collaboration with an European team; the project will foster diversity, international exchanges, and multidisciplinarity. The educational component of the project will contribute to increase the competitiveness of the STEM workforce in the US in computational magnetohydrodynamics.<br/><br/>The research program will be organized into four areas: (1) Development of new efficient semi-implicit algorithms to solve partial differential equations with variable material properties (density, electric conductivity, magnetic permeability) using spectral or very high-order methods; (2) Modeling of ferromagnetic fluids and development of new numerical techniques to solve the magneto-static equations in the context of liquid metals and ferromagnetic fluids; (3) Development of level set techniques to account for more than two phases, and development of new high-order level set techniques to guarantee mass conservation and maximum principle; (4) Integration of the mathematical models and numerical techniques developed in (1)-(2)-(3) into a massively parallel open source code to test the proposed methods on realistic applications (liquid metal batteries, thermo-convection of ferromagnetic oil in high-voltage transformers, liquid metal dynamos). This project will involve the Principal Investigator, one post-doctoral collaborator, one graduate student, and European collaborators."
"1708602","Statistical Learning for High-Dimensional Stochastic Dynamical Systems","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","07/01/2016","08/22/2017","Mauro Maggioni","MD","Johns Hopkins University","Continuing Grant","Leland Jameson","08/31/2018","$300,000.00","","mauro.maggioni@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","MPS","1266, 1271","9263","$0.00","High-dimensional stochastic dynamical systems arise in a wide variety of scientific fields and applications, including models for the dynamics of molecules, of multi-agent systems (such as cars or animals), of activity of neurons, etc. The high-dimensionality of these systems corresponds to the large number of variables (atoms, agents, neurons, respectively, in the preceding examples) and typically makes these systems very expensive to simulate and hard to understand even when simulations are possible. This research project aims to develop novel ideas and algorithms for the model reduction of such systems. The investigator will develop automatic learning algorithms that, by collecting a number of short simulations in parallel, output a much lower-dimensional model of the original system that yields faster simulations, with provable accuracy, in a much reduced number of variables. This will make the simulations less expensive, allowing one to perform more and longer simulations, and make the extraction of useful information from simulated data easier. He will apply these constructions to molecular dynamics simulations, expecting to significantly reduce the computational time needed to simulate small biomolecules.<br/><br/>Large data sets in high dimensional spaces appear in a wide variety of scientific fields and applications. The PI focuses here on data sets that originate from the simulation of high-dimensional stochastic systems that arise in a wide variety of applications (e.g. molecular dynamics), with the goal of producing a much lower dimensional stochastic system with similar statistical properties as the full system, at least at a certain time scale. This is possible for a wide variety of dynamical systems with separation of time scales when the structure of the forces acting on the system and the stochastic perturbations are such that the trajectories of the system accumulate, in state or phase space, around low-dimensional sets (at the appropriate time scale and accuracy). The approach only requires access to a simulator S for the original system that, given initial conditions and the shortest time scale of interest as inputs, produces as output a (stochastic) path of the system starting at that initial condition and stops at the specified time. A call to S is typically expensive, but after a small number of carefully designed calls that yield a relatively small number of short paths, the algorithm learns and outputs a low-dimensional representation of the system, that is, a low-dimensional stochastic system whose trajectories are (in a suitable statistical sense), at the requested time scale, close to those of the original system. This construction may be performed in an online setting, as new regions of state space are explored, and in a multiscale fashion, where the time scale at which the system is reduced varies. These techniques will be adaptive to the assumed low intrinsic dimension of the simulated data, the timescale of interest, and the accuracy, leading to a new generation of results and algorithms for learning and approximating high-dimensional stochastic systems. While the techniques to be developed are applicable to a large family of stochastic systems, the main application considered in this project is Molecular Dynamics (MD). These techniques are expected to dramatically speed up the exploration of the state space of these molecules and of MD simulations as a whole. At the same time, they are general enough that they are applicable to a wide variety of stochastic systems, and the framework sets the stage for a novel approach to automatic learning of dynamical systems that is amenable to further generalizations."
"1620108","High order accuracy WENO methods for high dimensional problems on sparse grids","DMS","COMPUTATIONAL MATHEMATICS","06/15/2016","06/16/2016","Yongtao Zhang","IN","University of Notre Dame","Standard Grant","Leland Jameson","05/31/2020","$193,338.00","","yzhang10@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","1271","9263","$0.00","High order accuracy numerical methods are especially efficient for solving mathematical models in computational fluid dynamics and computational biology which contain complex solution structures. The computational cost increases significantly when the number of grid points is large or the spatial dimension of the problem is high, due to the ""curse of dimensionality"". How to achieve fast computations by high order accuracy methods is a very important question especially for long-time simulations. This research project aims to develop efficient high order accuracy numerical methods on sparse grids for high spatial dimensional problems. The new methods have the potential to be applied to a broader class of applications in quantum electronic systems, molecular motors, finance, collective cell motions in biology, gene regulatory network, etc. <br/><br/>The PI will design, analyze and implement novel high order Krylov integration factor (IF) weighted essentially nonoscillatory (WENO) algorithms for solving hyperbolic or convection-diffusion partial differential equation (PDE) problems on sparse grids by using the sparse-grid combination technique to deal with the high dimensional challenge. The sparse-grid method is a powerful approximation tool for high dimensional problems. It has been successfully used in many scientific and engineering applications. Discretizations on sparse grids involve much fewer degrees of freedom than that on single grids. Efficient numerical simulations of these high dimensional systems will help in studying interesting biological questions in this area. The proposed research will contribute in the active area of dealing with the ""curse of dimensionality"". A suite of powerful computational tools for solving high dimensional nonlinear PDEs will be developed. These techniques are expected to make positive contributions to computer simulations of complicated phenomena in biological and physical systems. The proposed activity will also provide excellent training and education opportunities for both graduate and undergraduate students interested in research at the interface of mathematics, computation, and applications."
"1724979","Structured Dictionary Models and Learning for High Resolution Images","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","01/26/2017","Mauro Maggioni","MD","Johns Hopkins University","Standard Grant","Leland Jameson","07/31/2018","$105,494.00","","mauro.maggioni@jhu.edu","1101 E 33rd St","Baltimore","MD","212182686","4439971898","MPS","1271","9263","$0.00","We will develop novel techniques for the multi-resolution analysis of high-resolution images, to obtain novel efficient and information representations. These representations will take into account natural invariances in images, and will lead to novel dictionary learning constructions and algorithms for images and in signal processing in general. These representations will then be used to analyze, search, and recognize similar objects or features in collections of (scans of) paintings, in particular a large collection by the baroque artist Jan Brueghel. The distances between images and portions thereof, the features learned by the extensions of dictionary learning we will construct, and the associated statistical similarities, together with labels provided by experts to be used to train classifiers and algorithms that learn similarities among items to match those provided by expert, will enable us to enrich the current set of capabilities in building these large networks of paintings, to search through them more easily and with more general search patterns, and to visualize them according to different metrics by using dimensionality reduction techniques.<br/><br/>The automatic learning of templates and patterns, and their statistical relationships, in images and signals in general is crucial in a wide variety of applications, such as automating object recognition, and in defining visually meaningful similarities between images, needed to enable searches in large image databases. We will both develop novel techniques for automatically learning good templates for images, that incorporate natural invariances such as translations and scalings, and novel ways of exploiting these templates for analyzing large collections images, measuring the similarities  between then, and finding and characterizing recurrent patterns in them. These novel techniques will be applied to the data on the Jan Brueghel Research site, that allows scholars to investigate and conceptualize a very different notion of old master pictures. Instead of creating absolute categories of genuine and not-genuine, the team will be drawing a map of interconnections between the thousands of paintings produced in the workshops of early modern Antwerp. These pictures were made over several generations, in the shops of masters ranging from world-famous (Pieter Brueghel, Rubens) to utterly obscure. The website will chart how ideas were generated, exchanged, reused and retooled by different artists, mapping networks of creation and production well beyond those traceable through archival documents."
"1645445","Advancements in the Ultraspherical Spectral Method","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","09/09/2016","Alex Townsend","NY","Cornell University","Standard Grant","Leland Jameson","07/31/2018","$104,033.00","","townsend@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1271","9263","$0.00","The numerical solution of real-world fluid flow and airfoil problems needs an accurate, flexible, and fully-adaptive spectral element method. The so-called ultraspherical spectral method, with its sparsity and regularity preserving discretizations, is promising to overcome many of the traditional computational barriers. This research project will exploit and investigate the remarkable properties of the ultraspherical spectral method with the aim of producing a high quality and industrial-strength spectral element solver for partial differential equations. One key feature will be its robustness to pinching boundary features, typical with airfoils, that will alleviate the current tremendous burden on mesh generation algorithms. The project will radically alter the perception of spectral methods in the computational mathematics and engineering communities by extensively demonstrating that, when done carefully, they can be a flexible, general, and powerful numerical tool.<br/><br/>Today's pseudospectral methods deliver both convenience and spectrally accurate discretizations for the solution of differential equations. However, they lead to dense discretizations, numerical instability, and a severe limitation to simple geometries. The novel ultraspherical spectral method is an alternative that retains the same accuracy and convenience, but leads to almost banded well-conditioned discretizations that faithfully preserves the regularity of the underlying differential operator while also being amenable to specialized fast linear algebra routines. Based on this new spectral method, the PI will derive a new mathematically-grounded fully-adaptive spectral element method for meshed geometries. Key novel computational features will include: (1) A high accuracy on mesh elements that is independent of the aspect ratio; (2) True hp-adaptivity that allows for essentially arbitrarily large element degree p and small average mesh element size h (without concern of ill-conditioning); and (3) The flexibility to solve a wide range of differential equations with general boundary constraints; and (4) Local refinement and mesh coarsening for the resolution of corner singularities. This new spectral element method will be applied to challenging partial differential equations for the state-of-the-art numerical simulation of advection-dominated fluid flow problems."
"1620449","Theoretical and Numerical Studies of Nonlocal Equations Derived from Stochastic Differential Equations with Levy Noises","DMS","COMPUTATIONAL MATHEMATICS","09/15/2016","09/09/2016","Xiaofan Li","IL","Illinois Institute of Technology","Standard Grant","Leland Jameson","08/31/2020","$199,578.00","Jinqiao Duan","lix@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","MPS","1271","9263","$0.00","Stochastic effects are ubiquitous in complex systems in science and engineering.  Although random  mechanisms may appear to be very small or very fast, their long time impact on the system evolution may be delicate or even profound, which has been observed in, for example, stochastic bifurcation, stochastic resonance and noise-induced pattern formation.  The research team will study the complex systems under uncertainty by developing numerical methods to be simulated on computers and answer fundamental questions about the average quantities of the systems.  The investigators will deliver the following broader impact outcomes: (1) Two graduate students (including one female underrepresented minority) will receive education and training and (2) they will continue to recruit and nurture underrepresented students in STEM.  Additionally, the resulting computational tools have broad applications in areas ranging from biology to geophysics.  The software resulting from the proposed project will be made publicly available.<br/><br/>Mathematical modeling of complex systems under uncertainty often leads to stochastic differential equations (SDEs).  Fluctuations appeared in the SDEs are often non-Gaussian (e.g., Levy motions) rather than Gaussian (e.g., Brownian motion).  Compared with systems with Gaussian noises, quantifying the impacts of non-Gaussian Levy fluctuations are much less understood.  The researchers will develop convergent and efficient numerical techniques for investigating the deterministic macroscopic quantities that can help understand the dynamics of SDEs with Levy noises, in particular the mean exit time, escape probability, and probability density function.  They will also answer theoretical questions with regard to the well-posedness and the regularity of the solutions to these nonlocal equations.  Building upon previously developed methods in the one-dimensional case, the team will focus its effort on two-dimensional systems.  The proposed project will provide not only broadly applicable computational techniques to solve integro-differential equations with singular integrands, but also theoretically address the central questions pertaining to the solutions.  The theory will help to provide insight into fundamental issues in quantifying the impacts of non-Gaussian Levy fluctuations in dynamical systems."
"1656459","Efficient Algorithms for Uncertainty Quantification in High Dimensions","DMS","COMPUTATIONAL MATHEMATICS","08/16/2016","09/08/2016","Dongbin Xiu","OH","Ohio State University","Standard Grant","Leland Jameson","07/31/2018","$105,833.00","","xiu.16@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","MPS","1271","8396, 8609, 9150, 9263","$0.00","Uncertainty quantification (UQ) has become an integral part of today's scientific computing, as it is essential to the understanding of the impacts of various uncertain inputs (boundary and initial data, parameter values, geometry, etc.) to numerical predictions. UQ is thus critical to many important practical problems such as climate modeling, weather prediction, ocean dynamics, bio-chemical reactions, etc. One of the biggest challenges in UQ computations is the simulation cost, as UQ makes the traditional computations in much higher dimensional parameter spaces. For large and complex systems, the standard baseline deterministic simulations can be very time consuming, and conducing UQ simulations will further increase the simulation cost and can be prohibitively expensive. This is precisely the core issue this project intends to address and study. A novel set of highly efficient UQ algorithms will be developed to make UQ simulations amenable for large and complex systems. The new algorithms will significantly advance the current state-of-the-art of UQ methods. One prominent feature of the new algorithms is that they are designed to produce mathematically optimal UQ simulation results based on given affordable simulation capacity. While the traditional UQ methods seek to provide the smallest cost at fixed accuracy, the new methods will provide the best results at fixed affordable cost. This new feature thus makes the new algorithms ideally suited for practical UQ simulations of large and complex systems, and will have a profound impacts in various multidisciplinary fields where UQ is critical.<br/><br/>The core group of the new algorithms will be based on stochastic collation (SC). In order to provide optimal prediction under given and limited simulation capacity, this project is to develop a set of novel and efficient stochastic collocation methods, with a focus on high dimensional problems using very few number of samples. An important and  fundamental assumption is made: the number of the sample runs and the location of the samples are arbitrary and given by practitioners.  The goal is then to seek the best approximation in the potentially very high dimensional parameter space, based on the given samples.  The proposed research consists of three major approaches: (1) arbitrary interpolation type SC; (2) sparse regression type SC; and (3) multi-fidelity SC. In all cases, the number of high-fidelity simulations is assumed to be limited and given by the available simulation capacity, and methods are constructed to produce the best possible UQ simulations. All methods will be mathematically rigorous, as their constructions rely heavily on approximation theories in high dimensions; and also easy to implement, as they are the non-intrusive SC methods."
"1620109","RUI:   Computational algebraic geometry and combinatorial algorithms for neuroscience and biological networks","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","09/08/2016","Elizabeth Gross","CA","San Jose State University Foundation","Standard Grant","Leland Jameson","08/31/2019","$133,547.00","","egross@hawaii.edu","210 North Fourth Street","San Jose","CA","951125569","4089241400","MPS","1271","9229","$0.00","This project is focused on applications of computational mathematics to problems arising in neuroscience and systems biology.  Beyond utilizing currently available methods in creative ways, the main aim is to develop new computational methods to meet the needs of specific biological applications, particularly in systems biology and neuroscience.  In systems biology, the goal is to understand complicated biological processes by studying interactions in a network setting rather than in isolation. While in neuroscience, the goal is to better understand neural connections in the brain, and how the brain interacts with it's external environment.  On the systems biology side, the work in the proposed project will help make sense of two types of networks: biochemical reaction networks, deterministic models for molecular interactions, and protein-protein interaction networks, relational data collected, confirmed, and refined over the past decade. On the neuroscience side, the focus will be on neuronal networks, schematics that record connections between neurons, and combinatorial neural codes, a form of discretized cell firing data.<br/><br/>The techniques employed will come from combinatorics and computational algebraic geometry, two subfields with applications in an array of fields including statistics, physics, engineering, and biology.  The proposal has three main research components. First, techniques and theory from computational algebraic geometry, such as toric ideals and Groebner bases, will be used to understand and visualize neuroscience data.  Second, new algorithms for sampling random graphs with fixed properties will be developed for testing statistical hypotheses about protein-protein interaction and neuronal networks.  Third, new algorithms for computing elimination ideals will be developed with the goal of applying these new methods to model selection in biochemical reaction network theory."
"1619713","High Order and Efficient Numerical Methods for Simulating Electromagnetic Phenomena","DMS","COMPUTATIONAL MATHEMATICS","09/01/2016","09/05/2016","Wei Cai","NC","University of North Carolina at Charlotte","Standard Grant","Leland Jameson","01/31/2018","$170,000.00","Duan Chen","cai@smu.edu","9201 University City Boulevard","CHARLOTTE","NC","282230001","7046871888","MPS","1271","8037, 9263","$0.00","New materials with special properties are necessary in the search for new clean energy sources and advanced medical devices. Electromagnetic phenomena play a key role in the design of new materials such as meta-materials and conducting materials. Meta-materials, assembled with blocks of meta-atoms of naturally available components, have provided a wide range of new possibilities to design man-made materials with special properties. Novel devices using meta-materials have been proposed including perfect lens and sub-diffraction-limited imaging for medical applications, light harvest in clear energy solar cells. In addition, understanding the conducting flow of a charged system is essential for studying confined nuclear thermal reactions for the exploration of new clean energy sources.<br/><br/>The computational simulation of electromagnetic phenomena is challenging, owing to the demand of highly accurate and efficient numerical methods that not only represent the correct physics in the magnetic induction equation but also resolve the multiple scattering and local field enhancements from random objects in meta-materials. To meet these requirements, the PIs will accomplish the following two tasks in this project: (1) to develop a highly efficient volume integral equation method for Maxwell equations for very accurate computation of multiple scatterings of large number of regular or random objects employed in the construction of meta-materials; (2) to devise a high order constrained transport finite element method for the magnetic induction equations in the magneto-hydrodynamics  problem so the global divergence free condition on the magnetic field is preserved. The research findings will be disseminated through journal publications and software tool development."
"1620471","Computational Paradigm for Simulating Free Boundary Diblock Copolymers","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","06/17/2016","Frederic Gibou","CA","University of California-Santa Barbara","Standard Grant","Leland Jameson","06/30/2019","$120,000.00","","fgibou@ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","MPS","1271","8037, 9263","$0.00","This project combines modeling with numerical methods for the investigation of the self-assembly of block copolymers. In particular, this project will result in the development of efficient and predictive computational tools that predict the self-assembly of block copolymers that present a free boundary. In addition, this formalism will be used to develop a methodology for solving the inverse problem of finding the geometry of a mask component that will direct the self-assembly of copolymer towards a target design. Copolymers are ubiquitous in science and engineering, they provide unique characteristics that modern industrial processes depend on to keep up with Moore's law and constitute an excellent model system for scientific studies of self-assembly. Therefore, the research has a broader impact in the multiscale modeling and computation of more general self-assembly processes that arise in other physical and biological sciences. The specific objectives of the project are: 1) to develop an innovative, effective computational framework for predicting the self-assembly of block copolymer in both two and three spatial dimensions in the case of free surfaces; 2) to use this framework to understand the coupling between thermodynamic, kinetic and surface tension forces on the self-assembly and on the geometry of the free surface; 3) to apply this framework to the prediction of a template's geometry that will direct the self-assembly towards a target design.<br/> <br/>Diblock copolymers are melts made of molecular chains with two chemically different species along their backbone that self-assemble into ordered structures used in high density hard drives, drug delivery systems, magnetic dots, nano-pores, nano-wires, membranes with tailored nano-scales porosity, in battery fuel cells and silicon capacitors. Since the surface of the melt plays a crucial role in the self-assembly, this research will develop a computational paradigm that enables the simulation of the self-assembly of free boundary diblock copolymers. This paradigm combines the level-set methodology for dynamic interfaces with the self-consistent field theory describing the self-assembly of diblock copolymers at equilibrium. These studies will be carried out by the PI who works in an interdisciplinary environment with close collaborations with experts in the field of diblock copolymers that bring forth a synergy of modeling and computational ideas. The broader impacts of the work also include 1) the training of computational students in an interdisciplinary, international team, 2) the integration of undergraduate from underrepresented groups, and 3) potential industrial relevance."
"1620138","Analysis and Computation for Inverse Problems in Differential Equations","DMS","COMPUTATIONAL MATHEMATICS","07/01/2016","07/20/2018","William Rundell","TX","Texas A&M University","Continuing Grant","Leland Jameson","12/31/2020","$300,000.00","","Rundell@math.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1271","9263","$0.00","Many objects of physical interest cannot be studied directly. Examples include the following: imaging the interior of the body, the determination of cracks within solid objects, and material parameters such as the conductivity of inaccessible objects. When these problems are translated into mathematical terms they take the form of partial differential equations, the lingua franca of the mathematical sciences.  However, since one may have additional unknowns in the model, these introduce unknown parameters in the equations that have to be resolved by means of further measurements. Specific problems addressed in this project include the recovery of the location and shape of interior objects from surface measurements or the determination of obstacles from acoustic or electromagnetic scattering data.  <br/><br/>In this project the PI deals with the practical aspects of such ""inverse problems"" from a mathematical and computational perspective. The main challenge is when a unique determination can be made from a given amount of data, but as these inverse problems are characterized by often severe ""ill-conditioning"", meaning that even when there is only one solution to the problem, two very different objects may produce data sets that are infinitesimally close. This lack of stability aspect makes designing and analyzing algorithms for the efficient numerical recovery of the unknowns extremely challenging. The PI will concentrate on developing extremely fast algorithms designed to detect significant features utilizing only minimal data. The PI also looks at inverse spectral problems, and a classic example of which is to be given the vibrational frequencies of a body and seek to determine its internal construction. Here the body can be a metal beam or a star such as the sun. A central theme of this proposal is the investigation of inverse problems for so-called anomalous diffusion models. Classical diffusion is based on Brownian motion and has its roots in 19th century physics together with Einstein's 1905 random walk model. Here a very localised disturbance spreads with the characteristic shape of a Gaussian and, further, the process is Markovian; at a given time step the state depends only on that at the previous time step.  While this serves well for a wide range of models, it fails for those that exhibit a ""history"" or ""memory"" effect.  This includes many materials that been developed over the last twenty years as well as economic forecasting such as stock and commodity market modeling.  It turns out that degree of ill-conditioning in anomolous diffusion inverse problems can be very different from those of the classical case suggesting that indeed fundamental new physics is involved. From a mathematical and computational standpoint this comes at a price; the resulting analysis is considerably more complex and challenging. The project also has a significant educational component in the training of graduate and undergraduate students."
"1620212","Hybrid Computational Methods and Algorithms for Complex Biological Systems","DMS","Cellular Dynamics and Function, COMPUTATIONAL MATHEMATICS, Cross-BIO Activities, MSPA-INTERDISCIPLINARY, Systems and Synthetic Biology","09/01/2016","06/13/2016","Yi Sun","SC","University of South Carolina at Columbia","Standard Grant","Leland Jameson","08/31/2020","$165,741.00","","yisun@math.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","MPS","1114, 1271, 7275, 7454, 8011","7465, 8007, 8037, 9150, 9179, 9263","$0.00","In this project the PI will develop new hybrid computational methods and algorithms to study complex biological systems and problems arising in biomaterials and tissue engineering:  (a) cellular aggregate fusion via cell self-assembly; (b) bacterial patterns formation in biofilm growth. This research will positively impact the applications in biomaterials, biotechnology, and biomedical sciences, which include bio-printing technology for fabricating tissues and organs, study of cell motions, drug design and ultimately regenerative medicine. The hybrid computational tools and algorithms developed in this project will help us to better understand complex mechanisms in the cellular system and the bacterial system. The computational framework developed here will be broadly applicable to build up similar hybrid models in (bio)material sciences and (bio)fluid dynamics. Broader impacts of the education plan is to increase the representation of underrepresented minority groups in computational mathematics and mathematical biology. In addition to the training of doctoral graduate students, undergraduate students will also be integrated into the proposed research projects. <br/><br/>For the thrust (a), the PI proposes to investigate how the adhesion molecules effect the fusion processes by integrating hybrid models for molecular signaling pathways with those for mechanical motion. In particular, signaling pathways, actomyosin dynamics and the cellular level mechanical polarization are modeled by continuum variables, whose evolution and transport are governed by systems of reaction and reaction-diffusion (RD) equations. Whereas, mechanical motion of the cellular system is described by an on-lattice model based on the kinetic Monte Carlo (KMC) algorithm. Communication between the lattice model and the continuum scale RDs is carried out via a suite of multiscale protocols. For the thrust (b), the PI proposes to study several major factors that affect the formation of bacterial patterns in biofilms: bacterial chemotaxis, motility, and interactions. In the proposed hybrid model, each of bacteria is characterized by an individual off-lattice particle or an elongated rod shape with its location, orientation, and its state of stress exerted by local environment, while the dynamics of extracellular polymeric substances (EPS) in the environment is described by continuously changing fields. The hybrid model is described by a system of ordinary and partial differential equations.<br/>"
"1620280","Collaborative Research:   Mathematical Methods for Optimal Polynomial Recovery of High-Dimensional Systems from Sparse and Noisy Data","DMS","COMPUTATIONAL MATHEMATICS","09/15/2016","09/09/2016","Clayton Webster","TN","University of Tennessee Knoxville","Standard Grant","Leland Jameson","08/31/2019","$65,752.00","","webstercg@math.utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","MPS","1271","9263","$0.00","Current problems in approximation that are driven by applications in science and engineering, are formulated in very high dimensions. This project involves the study of different objectives related to high-dimensional approximation, that arise in a large number of applications including neutron, tomographic and magnetic resonance image reconstruction, uncertainty quantification, optimal control and parameter identification for engineering and science applications, as well as important energy and material science applications.  The approaches used in this work will result in substantially improved and mathematically well-founded methodologies for computer simulations of solutions to real-world problems. The project will also involve the interdisciplinary training of graduate students on computational data sciences and engineering. The results obtained will be disseminated through journal articles, conference talks, a collaborative website, and by the research and training activities of the junior participants. <br/><br/>In this effort we propose to develop novel mathematical techniques  for approximation of high-dimensional systems from a limited amount of sparse and noisy data.  The results of this effort will enable scientists to understand what are the number realizations of a nonlinear manifold that required to recover the entire high-dimensional solution map, with optimal approximation guarantees and minimal computational cost. Our rigorous mathematical approach includes: Novel weighted convex optimization and iterative thresholding techniques for optimal polynomial recovery, established via an improved estimate of the restricted isometry property; and Advanced multi-index methods that alleviate complexity and accelerate convergence of solutions by constructing model hierarchies with the use of reduced-basis techniques."
"1565738","Workshop on Functional Analytic Methods in Error Prediction with Applications","DMS","COMPUTATIONAL MATHEMATICS","05/01/2016","04/26/2016","Victor Ginting","WY","University of Wyoming","Standard Grant","Rosemary Renaut","04/30/2017","$29,810.00","Farhad Jafari","vginting@uwyo.edu","1000 E. University Avenue","Laramie","WY","820712000","3077665320","MPS","1271","7556, 9150, 9263","$0.00","This project provides travel and subsistence support for participants in the workshop ""Functional Analytic Methods in Error Prediction with Applications"" held at the University of Wyoming on June 13-17, 2016. The workshop is part of an annual activity spearheaded by the Rocky Mountain Mathematics Consortium (RMMC). The workshop is aimed at exposing graduate students and junior researchers to recent progress in computational sciences, particularly in error estimation techniques utilized in a class of multi-physics and multi-scale problems inheriting various levels of uncertainty. The workshop is designed to be strongly interdisciplinary. Workshop presentations and lectures are a blend of colloquium type and more in-depth style, with the intention of allowing junior participants to gain knowledge that are potentially useful in their future endeavors, and at the same time giving more experienced participants ample opportunity to initiate research collaboration.<br/><br/>Any numerical approximation/simulation for solving mathematical problems contains some errors. These errors are often functionally dependent on some parameters associated with the method. Formally, a robust numerical method should exhibit a behavior in which the errors vanish in the limiting process associated with the parameters. A standard practice is to estimate the errors in terms of the parameters and some regularity assumptions of the original problems. Unfortunately, many applications prevent imposing those theoretical assumptions, thereby creating a need to monitor the errors in real time simulation. Having this availability will enhance the robustness of the approximation in the predictive simulation. Furthermore, a closely related aspect to error estimation is quantifying the uncertainty that can be present both in the original problem and in the techniques employed to approximate it. The appropriate strategy to tackle this is multifaceted, involving reliance on sophisticated mathematical and statistical tools. This workshop provides a forum for researchers to address all these issues. More information can be found at the workshop website: <br/>www.uwyo.edu/math/additional-learning-opportunities/rmmc-summer-school/"

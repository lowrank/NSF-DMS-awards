"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"0104009","Solving polynomial systems by polyhedral homotopies","DMS","COMPUTATIONAL MATHEMATICS","08/01/2001","09/17/2001","Tien-Yien Li","MI","Michigan State University","Standard Grant","Junping Wang","07/31/2005","$167,239.00","","li@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","0000, 9263, OTHR","$0.00","In the last two decades, the homotopy continuation method for<br/>solving polynomial systems has been established and proved to<br/>be reliable and efficient. Resulting from a previous project,<br/>supported by NSF Grant DMS-9804846, a source code, HOM4PS, was<br/>produced. Excellent performance of this code on a large collection<br/>of polynomial systems in a wide variety of applications provides<br/>practical evidence that the newly developed methods constitute a<br/>powerful general purpose solver. Nontheless, there are still<br/>numerous models of polynomial systems in applications which do<br/>not have a satisfactory line of attack. Those models provide a rich<br/>source of interesting and challenging problems with strong mathematical<br/>content. The essence of the proposed project is the advance development<br/>of the solver based on the conduct of further research<br/>to greatly enlarge the scope of its applications. The ultimate goal<br/>is a more complete high-quality block-box solfware which will<br/>incorporate the best state of the art to provide the general scientific<br/>community a reliable source for solving polynomial systems in practice.<br/><br/>The problem of solving polynomial systems has been, and will continue<br/>to be, one of the most important subjects in both pure and applied<br/>mathematics.  The need to solve systems of polynomial equations arises<br/>very frequently in various fields of science and engineering, such as,<br/>formula construction, geometric intersection, inverse kinematics,<br/>robotics, vision and the computation of equilibrium states of chemical<br/>reaction equations, etc. In recent years (1993-1999), a considerable<br/>research effort in Europe had been directed to this problem in two<br/>consecutive major projects, PoSSo (Polynomial System Solving) and FRISCO<br/>(FRamework for Integrated Symbolic/numerical COmputation), supported by<br/>European Commission with thirteen university teams in seven European<br/>countries involved. Those research projects focused on the development<br/>of the already well-established Groebner basis methods within the<br/>framework of computer algebra.  Their reliance on symbolic manipulation<br/>makes those methods seem somewhat limited to relatively small problems.<br/>In contrast, the approch by the homotopy continuation method in this<br/>project is numerical and exhibits much powerful application results."
"0089881","Tenth International Conference on Approximation Theory","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, ANALYSIS PROGRAM","02/01/2001","01/30/2001","Charles Chui","MO","University of Missouri-Saint Louis","Standard Grant","Peter Polyakov","01/31/2002","$20,000.00","Joachim Stoeckler","chui@cs.umsl.edu","1 UNIVERSITY BLVD","SAINT LOUIS","MO","631214400","3145165897","MPS","1266, 1271, 1281","0000, 9263, OTHR","$0.00","This award is for the partial support of the conference in the field of Approximation Theory at the University of Missouri, St. Louis. The conference will concentrate on various topics including classical approximation theory, wavelets, mutivariate splines and nonlinear approximation. The conference will be the tenth in a traditional triennial series of such conferences, held earlier at UT Austin, Texas A&M and Nashville."
"0106743","Development of Numerical Methods for Semiconductor Device Simulation and Electron Microscopy","DMS","COMPUTATIONAL MATHEMATICS","09/01/2001","08/22/2001","Sigal Gottlieb","MA","University of Massachusetts, Dartmouth","Standard Grant","Junping Wang","08/31/2004","$50,000.00","","sgottlieb@umassd.edu","285 OLD WESTPORT RD","NORTH DARTMOUTH","MA","027472356","5089998953","MPS","1271","0000, 9263, OTHR","$0.00","This study will focus on two numerical techniques for discontinuous problems and their physical applications. Particular attention will be paid to the development of a numerical method and its impact on refining the mathematical model of the physical system. Sharp gradients and discontinuities are characteristic of semiconductor device simulations. While numerical methods have been developed to handle these characteristics, these methods need to be refined and tailored to capture the features exhibited by carrier flow in semiconductor device simulations.  On the other hand, the mathematical models that are used to describe carrier transport in semiconductors are constantly evaluated and changed.   Steady-state weighted essentially non oscillatory methods will be refined and used to determine the validity of macroscopic models of current transport and deposition in semiconductor devices. Discontinuities are also a problem in the determination of protein structure by electron microscopy. The inherently discontinuous nature of physical structures, and the assumption that repetition of the structure in a gridlike formation is a periodic function leads to slow decay of the Fourier coefficients. Fourier coefficient extrapolation and Gegenbauer polynomial methods will be further developed and applied to the field of electron microscopy to achieve better resolution protein structures. This has the potential to be added to any electron microscopy software as a postprocessing step, and provide better resolution structures.<br/><br/>Numerical methods for semiconductor device simulation models allow efficient and inexpensive simulation of the processes involved in semiconductor device production. However, these processes have many discontinuities that require sensitive numerical methods to capture the sharp changes in density and pressure without smearing them. Such methods, known as Weighted Essentially Non-Oscillatory methods, have been developed for use in similar problems, but are not efficient for the long time scales necessary for semiconductor simulations. The aim of this project is to further develop these numerical methods, and make them efficient for semiconductor device simulation on computers. Efficient numerical methods will also serve to compare different models, which attempt to describe the physical problem, and to evaluate which models best compare to reality. Another aspect of this project deals with the effects of discontinuities in protein structures studied by electron microscopy. Mathematical methods have been recently developed to solve the underlying problem by adding a smoothing step, which smoothes away numerical artifacts while keeping the real discontinuities. These methods have never been used on protein structures, and need to be tailored to it. These methods may improve the resolution of protein structures determined by electron microscopy.  <br/>"
"0104003","Local Regularization Methods for Ill-Posed Inverse Problems:  Fast Algorithms and Adaptive Parameter Selection","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2001","07/18/2003","Patricia Lamm","MI","Michigan State University","Standard Grant","Henry Warchall","08/31/2005","$99,000.00","","lamm@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1266, 1271","0000, 9263, OTHR","$0.00","Local regularization methods show great promise for the solution of a number of different classes of inverse problems, often retaining the special structure of the original problem as well as leading to very fast algorithms (especially in the case of Volterra problems).  In addition, numerical tests show that local regularization methods can work well to resolve sharp features of solutions without having to rely on nondifferentiable or nonquadratic optimization schemes.  To date, the convergence theory for the local regularization of Volterra problems has been limited to only mildly ill-posed inverse problems.  In the case of more severely ill-posed problems, there is numerical evidence that certain local methods may suffer from lack of stability and/or convergence.  Because of the cost efficiencies of local regularization methods, an important question is therefore whether new variations of these methods can be developed for which stability/convergence can be proven in the general case.  The PI proposes to develop two new variations of local regularization methods which show promise in numerical tests and for which there is hope of establishing a general stability/convergence theory.  The PI also proposes to develop adaptive schemes for the selection of variable regularization parameters in local regularization methods.  Variable parameters are of use in applying more smoothing in some parts of the domain and less in others.  In numerical tests adaptive local regularization techniques have been shown to be effective in determining the variable regularization parameter at the same time that local parts of the solution are recovered.  Because no convergence theory exists at the present time for such an approach, the PI proposes to study such adaptive schemes and develop a theory which will be useful in making recommendations for adaptive parameter selection methods.  The PI also proposes to extend these ideas to nonlinear Volterra problems and to linear non-Volterra problems.<br/><br/>Inverse problems occur widely in many applications, including problems of biomedical imaging (CT scans and X-rays), image reconstruction (from satellites or other sources), and geophysical exploration.  The Volterra class of inverse problems arises in the determination of the surface temperature of a space vehicle as it re-enters the earth's atmosphere; additionally, Volterra inverse problems appear as models for remote sensing problems.  While classical methods exist for for solving such problems, classical methods are often very inefficient and lead to overly expensive solution techniques.  A second disadvantage of classical solution methods is readily seen in imaging applications where reconstructed images may have blurred edges and inadequately detailed features.  The PI proposes to address both of these difficulties with the development of new solution methods based on the ideas of local regularization.  The use of these newer methods can lead to a significant decrease in cost for the solution of a wide class of practical inverse problems, with improved resolution of detailed features of solutions."
"0100662","Modular Representations of Finite Groups","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS","07/15/2001","05/21/2004","Jon Carlson","GA","University of Georgia Research Foundation Inc","Standard Grant","Tomek Bartoszynski","06/30/2005","$144,237.00","","jfc@math.uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1264, 1271","0000, 9263, OTHR","$0.00","The project is an investigation into the representation theory and cohomology of finite groups over fields of prime characteristic.  The Principal Investigator is particularly interested in the homological properties of representations which underlie the basic module theory.  He plans to consider a question open for more than 20 years on the classification of a specific type of modules that play an important role in the larger category theory of the modules, and also to look the structure of the cohomology ring of the group which acts on the fundamental homological constructions.  Carlson and his collaborators have shown that many facets of the module category are controlled by the group cohomology of p-subgroups.  The proposed work would build on this foundation. Other projects involve investigations of the structure of module categories of finite groups and the general theory of extensions of modules.  Results from the project could be of interest in the area of algebraic topology as well as in representation theory.  Professor Carlson plans to continue his development of computer algebra systems for experimentation with modules and homomorphisms.  He intends to expand his collection of programs for the computation of group cohomology and other aspects of the module theory.  The programs are also being rewritten for more general applications in the area of the representation theory of algebras. <br/><br/>In basic terms the Principal Investigator will look at certain types of algebraic systems together with the actions of operators.  Such a system is called a module and it might have many dimensions in the sense of depending on many variable. The operations may represent something like the geometric rotation of points on a space.  The project will concentrate on the classification and properties of modules whose associated operators have a preset collection of interactions.  A significant part of the project is the development of computational techniques and software for analyzing the structure and properties of modules.  Groups of transformations on modules and spaces are basic objects in modern mathematics and arise in many applications of the mathematics.  Some of the methods of the study are closely related to geometric techniques used in topology."
"0094179","CAREER: Regularization methods for fluid/filament interactions in three dimensions","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2001","09/22/2006","Ricardo Cortez","LA","Tulane University","Standard Grant","Junping Wang","08/31/2007","$344,000.00","","rcortez@tulane.edu","6823 SAINT CHARLES AVE","NEW ORLEANS","LA","701185665","5048654000","MPS","1266, 1271","0000, 1045, 1187, 9150, 9250, 9263, OTHR","$0.00","Cortez<br/>0094179<br/>     The investigator develops, analyzes, and applies<br/>computational techniques for the motion of elastic filaments<br/>embedded in an incompressible fluid.  This general situation<br/>arises in many physical contexts, including the motion of<br/>microorganisms, swimming of aquatic animals, fluid flow around<br/>elastic bodies, bubble motion and more.  The project encompasses<br/>the development of numerical methods for flows corresponding to a<br/>wide range of length scales.  The methods are based on the use of<br/>a force field along filaments made of localized but smooth terms<br/>rather than delta distributions.  Regular expressions for the<br/>fluid and filament velocities induced by the forces are then<br/>derived as the basis for the methods.  Important properties of<br/>this approach are that it eliminates the singularities that would<br/>normally be present if the forces are assumed to be delta<br/>distributions, the volume of fluid within elastic boundaries is<br/>conserved extremely well, and high accuracy can be achieved.  The<br/>work is aimed at extending well-known numerical techniques to<br/>three-dimensional flows driven by forces along filaments and to<br/>the full range of length scales.  Analysis of the methods is<br/>carried out to establish and improve their stability, convergence<br/>and accuracy properties.  The applications that are pursued<br/>include a wide variety of flagellar motions, the motion of<br/>flexible membranes around obstacles, and others.<br/>     The motion of microorganisms in a liquid and the flow of<br/>blood in capillaries are two examples of phenomena that can be<br/>studied with computational methods.  Computer techniques that can<br/>accurately simulate motions of this type are very valuable<br/>because they can be used to determine how arteries become<br/>obstructed or how flagellated organisms perform specific<br/>functions in the human body and the effects of defects in this<br/>mechanism.  From a mathematical point of view, these computer<br/>simulations are not ready for scientists to use; there is a need<br/>to improve the accuracy with which the underlying equations are<br/>being solved and increase the reliability of the methods.  This<br/>project is aimed at developing computational methods for fluid<br/>flows interacting with elastic structures.  The goal is to develop<br/>high-accuracy methods, improve on their mathematical properties,<br/>and apply them to problems from biological sciences.<br/>"
"0107210","Computational Methods for Most-Energetic Traveltimes of Seismic Waves","DMS","COMPUTATIONAL MATHEMATICS","09/01/2001","08/30/2001","Seongjai Kim","KY","University of Kentucky Research Foundation","Standard Grant","Junping Wang","08/31/2004","$38,590.00","","skim@math.msstate.edu","500 S LIMESTONE","LEXINGTON","KY","405260001","8592579420","MPS","1271","0000, 9263, OTHR","$0.00","The work is concerned with the development of computational methods for the most-energetic traveltimes (METTs) of seismic waves and their applications to various physical problems.  The METT corresponds to the highest energy level in data and has been recognized as one of most important components for image/inversion processes in highly heterogeneous media.  However, METTs often develop discontinuities in wavefronts.  Thus, computational principles that have been utilized for the computation of continuous first-arrival traveltimes can hardly be adopted.  The upwind direction of the METT is determined by the energy level (amplitudes) of wavefronts and, therefore, the amplitude accuracy is very crucial for a successful METT simulation.  Since the transport equation, which solves the amplitude, incorporates the traveltime Laplacian, the traveltime should be solved by a non-oscillatory high-order numerical algorithm which can provide a mechanism for an accurate traveltime Laplacian near the discontinuities.  The investigator proposes the development of accurate and efficient algorithms for the METT and the corresponding amplitude in 3D heterogeneous media, incorporating second- and third-order ENO schemes and their local extensions for intersecting wavefronts near discontinuities.  A full wavefield solver of negligible numerical dissipation is to be developed and implemented for a numerical verification of the conjecture: the METT can be computed in such a way that the corresponding amplitude is continuous over the whole domain.  Applications are planned for the anisotropic METT and realistic geophysical problems such as tomography in oil exploration, earthquake analysis, and shallow seismic reflection.  The proposed research subjects are vitally important to energy, economic, and environmental concerns.  In particular, the theoretical understanding and numerical algorithms for the METT and the corresponding amplitude will open other promising research areas.<br/><br/>For various applications of sound waves traveling through a substance, it is necessary to compute the traveltime of the sound waves.  Since the acquired data are dominated by the highest energy levels of the waves, the most reliable results can be obtained by incorporating information from the most-energetic traveltime (METT).  The proposal is concerned with the development of accurate and efficient computational algorithms for the METT and applications to important geophysical image processing problems.  Image processing methods allow geoscientists to find interesting underground features.  In oil exploration, for example, geophysicists first compute images of rock/soil structures and then detect the spots where oil reservoirs are.  Thus the oil reservoir detection can be successful only with accurate and well-focused images, for which the newly developed algorithms provide the most useful information.  The same arguments can be applied to the shallow sound wave reflection, the image processing techniques for the subsurface not deeper than 100 meters.  These techniques have been developed for detailed images; examples can be found in waste-disposal site characterization, archaeological search, and ground security analysis.  Many cracks and break-downs on buildings or roads are due to an ignorance or under-estimation of the importance of ground security analysis.  The proposed work is also applicable to earthquake analysis.  Since a horrible destruction happens with a close relation to the most-energetic components of the earthquake wave, the computation of the METT is necessary for a deeper understanding of the earthquake.  The new algorithms are expected to provide an effective mechanism that helps geoscientists simulate seismic ground motion and helps geologists predict earthquake destruction areas, with high accuracy and efficiency.  The above mentioned problems are vitally important to energy, economic, and environmental concerns and can be solved more reliably by the proposed work."
"0075239","Iterative Methods in Image Reconstruction","DMS","COMPUTATIONAL MATHEMATICS","04/01/2001","03/28/2001","James Nagy","GA","Emory University","Standard Grant","Junping Wang","03/31/2004","$130,000.00","","jnagy@emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1271","0000, 9263, OTHR","$0.00","     The investigator develops efficient and reliable iterative<br/>methods for reconstructing an image from recorded, noisy data.<br/>To obtain better resolved images, he develops methods that can<br/>effectively handle complicated operators (e.g., spatially variant<br/>kernels) and methods that enforce a nonnegativity constraint.<br/>     The emergence of increasingly sophisticated imaging devices<br/>has produced a new generation of very difficult computational<br/>problems in which an image is to be reconstructed from recorded<br/>data.  Such problems arise, for example, in breast cancer<br/>detection, where there is a tradeoff between the needs to obtain<br/>high resolution images and to limit the radiation dose to the<br/>patient.  New devices have also been recently proposed for<br/>3-dimensional imaging, where high dimensionality and resolution<br/>requirements result in nontrivial computational complexity.  Yet<br/>another example occurs in new ground-based telescopes, which use<br/>adaptive optics techniques.  To extract detailed information and<br/>take advantage of the increase in resolution that can be obtained<br/>from new imaging devices, it is important to develop methods that<br/>take into account spatial changes of properties in the imaging<br/>mechanisms, which are not amenable to standard fast Fourier<br/>transform based methods.  New fully 3-dimensional medical imaging<br/>devices require a new generation of computational methods to<br/>efficiently reconstruct images from recorded data, and in order<br/>for these new computational methods to find their way to clinical<br/>use, it is important to provide software packages that allow for<br/>easy implementation and experimentation.  Methods that enforce<br/>nonnegativity in the computed solution, and that can efficiently<br/>handle spatially variant kernels, are developed.  Many of the<br/>computational tools developed in this project should be<br/>applicable to a wide class of iterative image reconstruction<br/>methods, including linear, nonlinear and statistical based<br/>methods."
"0107159","The Barrett Lectures May, 2001 ""New Directions and Developments in Computational Mathematics","DMS","COMPUTATIONAL MATHEMATICS","05/01/2001","05/24/2001","Xiaobing Feng","TN","University of Tennessee Knoxville","Standard Grant","Catherine Mavriplis","04/30/2002","$9,320.00","Ohannes Karakashian, Charles Collins","xfeng@utk.edu","201 ANDY HOLT TOWER","KNOXVILLE","TN","379960001","8659743466","MPS","1271","0000, 9263, OTHR","$0.00","The investigator and his colleagues organize the 2001 John H. Barrett <br/>Memorial Lectures in the University of Tennessee at Knoxville from <br/>May 10-12, 2001. The Barrett Lectures have been held annually since 1972. <br/>Each year a different topic is chosen, representing the research interests <br/>of the mathematics faculty of the University of Tennessee. <br/>Since 1993, the lectures have consisted of three<br/>one-hour survey talks by each of three leading researchers<br/>representing different themes/directions in a single field. The topic of <br/>the 2001 Barrett Lectures is: ``New Directions and Developments in <br/>Computational Mathematics"". The focus of the lectures is parallel <br/>numerical algorithms for partial differential equations and their <br/>implementations and applications. The main speakers of the 2001 Barrett <br/>Lectures are Jinchao Xu of Penn State University, David Keyes of <br/>Old Dominion University and ICASE, and Mary Wheeler of University of <br/>Texas at Austin. Each of them will deliver three one-hour survey lectures <br/>on recent developments on parallel numerical algorithms with three <br/>different emphases: theory (Xu), computer implementation (Keyes), <br/>and application (Wheeler). In addition to the main speakers, six<br/>speakers are also invited to give one-hour talks on topics related <br/>to one of the main lectures.<br/><br/>The Barrett Lectures are partly funded by a grant from Tennessee Science <br/>Alliance, and have often received additional support from the National <br/>Science Foundation. They attract wide interest, with an audience of between <br/>40 and 50 participants from the whole country, in addition to faculty and <br/>students from Knoxville and the Oak Ridge National Laboratory. They represent <br/>one of the few long standing lecture series in mathematics in the southeastern <br/>United States. The main objective of the 2001 Barrett Lectures is to provide <br/>the participants with an exposition of parallel numerical algorithms for <br/>partial differential equations arising from various <br/>scientific/engineering/industrial applications, through in-depth survey<br/>lectures and informal discussions with the above three leading researchers <br/>in the field. Additional goals are to foster interdisciplinary collaboration, <br/>particularly with researchers in the Computer Science Department and in the <br/>College of Engineering at the University of Tennessee and several other <br/>southeastern institutions, and to generate a set of written surveys in the <br/>subject, which the organizing committee will endeavor to have published <br/>in book form. The fund being requested from the NSF will be spent<br/>providing partial support towards travel and accommodation for $20$ graduate<br/>students and recent Ph.D.'s who do not have research grants. <br/><br/>"
"0107389","Multiblock numerical methods for multiphase flow and transport in porous media","DMS","COMPUTATIONAL MATHEMATICS","09/01/2001","08/28/2001","Ivan Yotov","PA","University of Pittsburgh","Standard Grant","Junping Wang","08/31/2004","$99,999.00","","yotov@math.pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1271","0000, 9263, OTHR","$0.00","The main objective of this research is to develop accurate, efficient, and robust numerical methods for modeling subsurface flow and transport.  The research approach is based on a recently developed multiblock domain decomposition methodology.  The equations hold with their usual meaning on the subdomains, with physically meaningful boundary conditions imposed on the interfaces via mortar finite elements.  The formulation provides great flexibility for multiphysics and multinumerics couplings and treatment of irregular geometries and internal boundaries. It is very suitable for efficient parallel implementations.  This research will study discretization issues - multiblock schemes for advection dominated problems and multilevel adaptive techniques, efficient solvers - parallel domain decomposition-Newton-Krylov-multigrid methods, and modeling issues - multiphysics couplings.<br/><br/>Computer modeling of fluid flow and transport in the subsurface has a major economic impact on environmental and energy industries.  It can provide dependable and cost-effective solutions to global problems like contaminant groundwater remediation and enhanced oil recovery.  Groundwater supplies are often contaminated by organic, inorganic, and radioactive sources due to improper disposal. Remediation costs at U.S. government sites alone range into the hundreds of billions of dollars.  Hydrocarbons contribute almost two-thirds of the nation's energy supply.  Moreover, recoverable reserves are being increased twice as fast by enhanced oil recovery techniques as by exploration.  Current computer simulators are limited in their ability to incorporate fine scale geological data and to resolve the physical processes occurring on a wide range of spatial (microns to kilometers) and temporal (microseconds to thousands of years) scales.  The goal of this research is to develop new and efficient computational methods for modeling multiphase flow and transport in porous media on multiprocessor computers.  These new methods will allow for more accurate resolution of the underlying multiscale phenomena and thus improve the dependability of the computer simulators."
"0114473","Multiphase Mechanics of Tumor Encapsulation & Multilobulation","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2001","08/08/2001","Trachette Jackson","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Michael Steuerwalt","08/31/2004","$101,000.00","","tjacks@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1266, 1271","0000, 9263, OTHR","$0.00","Jackson<br/>0114473<br/>     The investigator uses mechanical models to investigate the<br/>mechanisms involved in tumor encapsulation, multiple lobe<br/>formation, and transcapsular spread.  Mass and momentum balance<br/>equations are written for the normal cells, neoplastic cells, the<br/>extracellular matrix (ECM) on which they are anchored, and the<br/>interstitial fluid in which they are bathed.  This system is<br/>closed by suitable constitutive relations for the mass supply,<br/>the partial stress tensor, and the momentum supply of each<br/>constituent.  The former is defined on the basis of<br/>phenomenological observations of tumor cell growth and the latter<br/>is based on the mechanical properties of each phase.  The model<br/>equations, consisting of a set nonlinear conservation and<br/>evolution equations, are analyzed using asymptotic analysis,<br/>bifurcation analysis, and perturbation theory in order to<br/>quantify the relative importance of chemical processes (such as<br/>ECM production and degradation) and mechanical properties of the<br/>tissue (such as ECM density and stiffness) in influencing capsule<br/>formation.  A further objective is to characterize the<br/>bifurcation that leads to multiple lobe formation.  Through<br/>analysis and simulation of the model, the investigator aims to<br/>discover which factors (mechanical and chemical) determine<br/>successful capsule formation and to quantify their influence.<br/>     The mechanisms by which a tumor becomes encapsulated as a<br/>continuum of cells or as several lobes of different sizes,<br/>separated by connective tissue, is an interesting, important, and<br/>unsolved phenomenon in tumor biology.  In fact, the presence (or<br/>absence) of a dense capsule surrounding a neoplastic mass is a<br/>major determinant of prognosis and the ultimate survival of the<br/>host.  Despite the importance of capsule formation, little is<br/>known about the process by which capsules arise.  The<br/>incestigator develops a mathematical modeling framework that<br/>describes tumor growth, encapsulation, multiple lobe formation,<br/>and transcapsular spread based on the physical forces and<br/>cellular interactions involved.  The specific aims are to use<br/>mechanical models to assist in understanding i) the role tumor<br/>cell, normal cell, and extracellular matrix (ECM) interactions in<br/>capsule formation, ii) the effects of tumor induced ECM<br/>production and degradation on the formation of tumor capsules,<br/>iii) the role of tissue properties such as ECM density and<br/>stiffness in slowing or impeding the process of tumor<br/>encapsulation, and iv) the bifurcation that allows a simple<br/>encapsulated continuum of cells to make the transition to a<br/>multi-nodular form.  There are implications for clinical diagnosis<br/>as well as for prognosis.<br/><br/>"
"0105190","Minimization of actuator hysteresis under varying load and temperature conditions","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/15/2001","01/10/2003","William Galinaitis","VA","Ferrum College","Continuing Grant","Henry Warchall","09/30/2005","$100,000.00","","galinait@rose-hulman.edu","215 FERRUM MOUNTAIN RD","FERRUM","VA","240882612","5403655574","MPS","1266, 1271","9178, 9229, 9263, SMET","$0.00","The accuracy of positioning systems that use piezoelectric actuators is limited by a hysteresis relationship between the input to the actuator and the resulting displacement.  The positioning accuracy of the system can be improved by minimizing the effect of the hysteresis.  To reach this goal it is necessary to have an accurate representation of the hysteresis, and a control strategy that provides compensation for the hysteresis.  Recent work has demonstrated the feasibility of using the KP operator to model hysteresis.  This work also demonstrated the use of the inverse KP operator to create an open loop control that minimized the positioning error caused by the hysteresis.  The results of that work are restricted to an actuator working under a fixed set of operating conditions.  As these conditions change the performance of the model and the control degrade.  To remedy this situation the proposed three year research program will focus on the modification of the KP operator so that it is capable of modeling hysteresis over a wide range of operating conditions (temperature and load).  As before, the inverse operator will be used as part of a controller designed to minimize positioning error over a range of operating conditions.  The major program tasks are: 1) The determination of the conditions necessary for data that is used to identify the nonnegative measure of the KP operator; 2) The modification of the KP operator to explicitly include variables associated with the temperature of the actuator and the load acting on the actuator; 3) The inversion of the modified KP operator and the development of a control based on this inverse.  Computer simulations and experimental data will be used to evaluate the relative performance of the KP and modified KP operators.  Similarly, the performance of controllers based on the inverses of both operators will be compared to determine if the positioning accuracy of the actuator can be improved by incorporating load and temperature variables into the KP operator.  <br/><br/>A key component of many mechanical systems is the positioning subsystem.  This subsystem typically consists of a controller that uses sensor information to drive an actuator such that a desired position is attained by a system component.  For a satellite-tracking antenna, it may be the system that aims the parabolic antenna dish at the desired point in the sky, or it is the system that is used to position a specimen under the probe tip in a scanning probe microscope (SPM).  Whether it is a system for pointing an antenna or imaging microscopic structure, the accuracy of the positioning directly impacts the overall effectiveness of the system.  For an SPM a piezoceramic type actuator is used for positioning.  A relationship between the actuator input and the resulting position, called hysteresis, limits the accuracy of this type of actuator.  The hysteresis introduces a positioning error that cannot be effectively minimized with conventional control strategies.  However, over the past decade several methods have been developed that are capable of minimizing this error for fixed operating conditions.  The proposed program is directed at extending a method based on the KP operator so that it can be used to reduce the hysteresis error over a range of actuator operating conditions.  The two major benefits of the program are: 1) The results can be used to improve the accuracy of currently installed positioning systems such as those used for microscopic imaging and integrated circuit production and 2) The program will actively engage a diverse undergraduate student population to state of the art research.  The second of these benefits is as significant as the first, for this effort will include the development of an experimental test facility at Ferrum College for both computer simulation work and experimental testing.  Students at the college will be active participants in the design, implementation and execution of experiments directed at the development and evaluation of the hysteresis models and the actuator controllers.  Additionally, they will apply material learned in their undergraduate courses to analyze experimental data and system performance.  The program will provide a rare opportunity for undergraduate students in this region to experience advanced research typically only found at a major research university or industrial research center.  This program will also be an introduction for this region of the state to advanced technology."
"0104891","Analysis, Algorithms and Computations for Model Problems in Material Sciences","DMS","COMPUTATIONAL MATHEMATICS","08/01/2001","07/30/2001","Qiang Du","IA","Iowa State University","Standard Grant","Catherine Mavriplis","10/31/2001","$203,796.00","","qd2125@columbia.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1271","0000, 9263, OTHR","$0.00","There has been an increasing trend to conduct scientific research using numerical simulations on modern high performance computers in recent years.  Considerable progress has been made in the area of computational material sciences. Computational tools have been used in the design of new materials as well as in the study of their properties. The central objectives of this project are: 1) to develop or refine certain mesoscale and macroscale models, so to enlarge the range of physical problems for which such  models are valid; 2) to analyze these models in order to gain further understanding of their properties and solutions; 3) to develop, analyze, and implement algorithms, in particular, parallel and adaptive algorithms, for the numerical simulation of these models; and 4) to use our algorithms and codes to study some interesting phenomena in material sciences.<br/><br/>In the proposed work, the principal investigator will study models and develop numerical algorithms for some interesting  material sciences problems that involve multiscale (mesoscale and macroscale) and stochastic effects,  such as problems related to vortices and other defects in superconductivity and magnetism. A major part of the project is aimed at increasing the range of applications for the mesoscale codes and allow more comparative studies between the mesoscale and macroscopic models through the use of domain and scale decomposition/integration and adaptive computation techniques. The codes for mesoscale models can be of use in gaining information and insight about the physical behavior and interaction of the fine structures (such as vortices) with, for example, boundaries, interfaces, impurities, currents, and thermal fluctuations. They can be of indirect use to device designers, in particular, when connections with macroscopic properties can be identified. Models based on the stochastic partial differential equations and their numerical simulations will also be given emphasis, so as to gain insight to the macroscopic effect of thermal fluctuations and impurities in the materials like superconductors and liquid crystals. The work will be aimed at making the computational codes robust, efficient, flexible, accurate, scalable and user-friendly. It is hoped that these codes can be used by physicists, material scientists, and engineers in laboratories, universities, and industrial organizations as a tool for studying some specific material properties and also a tool in designing devices.  <br/><br/>"
"0106511","Finite-Volume Methods for Hyperbolic Problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2001","08/17/2001","Randall LeVeque","WA","University of Washington","Standard Grant","Junping Wang","08/31/2007","$516,326.00","","rjl@uw.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1271","0000, 9263, OTHR","$0.00","This proposal concerns the development of multidimensional high-resolution finite-volume methods for solving hyperbolic partial differential equations, the development of software implementing these methods, and the application of these methods to particular problems. These methods are implemented in the CLAWPACK software package, which is freely available on the web and allows students and researchers studying a wide range of phenomena to use the technology of high-resolution methods and adaptive mesh refinement. These algorithms and the software will be further developed and brought to bear on a wider variety of problems. Particular problems of interest include: further development of adaptive refinement base on a new tree-structured code; inclusion of Cartesian-grid techniques for complex geometries; development of a general methodology for solving hyperbolic equations on curved manifolds; and wave-propagation problems (e.g., elastodynamics) in heterogeneous material, including nonlinear problems with spatially-varying flux functions.<br/><br/>A wide range of practical problems in science and engineering involve the propagation of waves or the transport of substances in fluid flow. Examples arise in problems as diverse as the study of ultrasound waves in human tissue, the transport of contaminants in groundwater or the atmosphere, and the study of gravitational waves arising from the collision of black holes. Mathematically all of these problems lead to similar sets of partial differential equations. Solving these equations numerically requires special techniques that can deal with discontinuous functions, since often either the coefficients describing the problem or the solution (or both) are discontinuous. Examples include discontinuities in material properties at the interface between tissue and bone in an ultrasound problem, or the shock waves that arise in most nonlinear wave-propagation problems. Over the past few decades, a powerful class of numerical methods has been developed for solving such problems that have been much more heavily used in some applications areas than others. A primary goal of this project is to facilitate the transfer of this technology to new areas. The software package CLAWPACK, developed by the P.I. and coworkers, is designed to make it relatively easy to for students to learn about these methods and for researchers to apply them.  There are still numerous mathematical challenges that arise in applying these methods to new situations. Research will be conducted to further improve these methods as a variety of new applications are explored. The subjects covered in this proposal are fertile ground for graduate education in computational mathematics. The P.I. is actively involved in training students and postdocs at the University of Washington as well as at other institutions by hosting visiting graduate students. The P.I. has also taught several short courses elsewhere and developed lecture notes, textbooks, software, and other educational material based on this research.  <br/>"
"0107187","Particle Simulations in Fluid Dynamics and Molecular Dynamics","DMS","COMPUTATIONAL MATHEMATICS","09/01/2001","08/21/2001","Robert Krasny","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Junping Wang","02/28/2006","$239,500.00","","krasny@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1271","0000, 9178, 9251, 9263, OTHR, SMET","$0.00","Krasny<br/>0107187<br/>     The investigator develops new computer algorithms for<br/>particle simulations in fluid dynamics and molecular dynamics,<br/>and he employs the algorithms to study important application<br/>problems.  The main technical tool is a new adaptive treecode<br/>algorithm for solving the N-body problem in the context of<br/>long-range particle interactions.  In this algorithm, the particle<br/>forces or velocities are evaluated using Taylor approximation in<br/>Cartesian coordinates, and the necessary Taylor coefficients are<br/>computed by a recurrence relation.  The adaptive features include<br/>a divide-and-conquer evaluation strategy, nonuniform rectangular<br/>clusters, variable order approximation, and a run-time choice<br/>between Taylor approximation and direct summation.  The<br/>investigator develops an optimal strategy for choosing the<br/>parameters in the code, in order to maximize the accuracy and<br/>efficiency of the algorithm.  The application problems to be<br/>studied in fluid dynamics include the onset of chaotic dynamics<br/>in vortex cores, high precision computation of vortex sheet<br/>roll-up and spiral formation in the Kelvin-Helmholtz problem, and<br/>breakdown and transition in vortex rings.  An important component<br/>of the work is to compare regularized particle simulations with<br/>experiments and with genuine viscous flows computed by finite<br/>difference schemes.  The application problems to be studied in<br/>molecular dynamics involve potential energy and force field<br/>evaluation for electrostatic interactions.<br/>     The results of this research affect several areas of<br/>strategic national interest, including civil and military<br/>aviation and biotechnology.  One outstanding problem in aviation<br/>is the simulation of the trailing vortex wake that forms behind<br/>an aircraft on takeoff and landing.  The vortex wake is a serious<br/>hazard to other aircraft, especially in crowded airports.  It<br/>imposes a minimum time and distance between takeoffs and landings<br/>on a given runway, and it is a source of wind shear, which has<br/>been implicated in accidents.  The project sheds light on the<br/>structure and dynamics of vortex cores, and this helps<br/>aeronautical engineers in designing vortex wake remediation<br/>systems.  In the area of biotechnology, one far-reaching problem<br/>is to simulate protein folding in a physically accurate way.  This<br/>is in fact the next step required to make use of the data<br/>collected in the human genome project.  The project contributes by<br/>enhancing the computer algorithms that are used by biochemists to<br/>study protein folding.  In the area of education and human<br/>resources, the investigator directs an applied honors calculus<br/>course for freshman science and engineering students, develops a<br/>new graduate course on scientific computing, and mentors the<br/>professional development of undergraduate and graduate students<br/>and postdocs.<br/>"
"0108540","Solutions and Grids from Genuinely Multidimensional Residual Distribution Schemes","DMS","COMPUTATIONAL MATHEMATICS","09/15/2001","08/30/2001","Philip Roe","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Junping Wang","08/31/2005","$160,000.00","Bram van Leer, Kenneth Powell","philroe@engin.umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1271","0000, 9263, OTHR","$0.00","The Fluctuation-splitting method for solving first-order partial differential equations is a distributive scheme based on evaluating residuals over piecewise linear simplex elements. The nature of the distribution step matches as closely as possible the local physics, thereby achieving minimal numerical dissipation. It is known how to implement this program is two space dimensions but new physics arises in three dimensions. In supersonic flow this is due to the replacement of characteristic lines by bicharacteristic surfaces, and in both supersonic and subsonic flow by the appearance of helicity, the streamwise component of vorticity, which interacts with the acoustics. We will attempt to design distribution schemes around the system version of the linear wave equation, which is the simplest model problem to exhibit these features, and which is embedded in the Euler equations. Application to the Euler equations themselves will be by means of a local linearization that is well established.<br/><br/>Many phenomena in nature and technology are well enough understood that the mathematical equations describing them can be written down, but to actually solve these equations requires huge computer resources. Examples that could be cited include the flow round an aircraft, the evolution of a galaxy, or the future of the weather. Especially in the last decade, the computer resources available for such projects has grown enormously, but the scientific appetite for computing power remains unsatisfied. When new machines become available, they may initially lie idle for some fraction of the day, but are soon fully utilized, typically within a month. To extract the most information from given resources, it is important that attention be paid to the set of instructions (the algorithm) by which the computer processes the data, both from the viewpoint of efficient arithmetic and also with regard to the process that converts the mathematical equations into arithmetical form (the scheme). Numerous schemes of varying sophistication exist, but new ones are continually sought, and this research seeks to extend the scope of one particular scheme that is based on trying to mimic the physics as directly as possible with few arbitrary decisions. It is hoped that the result of this will be to reduce the amount of computer storage space needed to model a given physical situation with given accuracy. Focus will on complex fluid flows in which an important role is played by rotating vortices. Such flows are at present very difficult to compute efficiently, Advances in this area would be especially beneficial to the prediction of flow round an aircraft in its take-off or landing configuration, and to computing the flows around helicopters.  <br/>"
"0196522","Analysis, Algorithms and Computations for Model Problems in Material Sciences","DMS","COMPUTATIONAL MATHEMATICS","08/01/2001","12/02/2004","Qiang Du","PA","Pennsylvania State Univ University Park","Standard Grant","Michael Steuerwalt","07/31/2005","$203,796.00","","qd2125@columbia.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1271","0000, 9263, OTHR","$0.00",""
"0107233","Numerical Solution of Partial Differential Equations and Applications","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2001","08/20/2001","Douglas Arnold","PA","Pennsylvania State Univ University Park","Standard Grant","Catherine Mavriplis","10/31/2001","$180,000.00","","arnold@umn.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1266, 1271","0000, 9263, OTHR","$0.00","The investigator will devise, improve, and analyze methods for the numerical simulation of complex physical phenomena modeled by partial differential equations, emphasizing three main areas: mixed methods for elasticity equations, discontinuous Galerkin methods, and computational general relativity.  For elasticity, the investigator will build on a recent breakthrough that enabled the construction of the first stable mixed finite element methods for the displacement-stress formulations with polynomial trial functions, and also work towards the development of simpler nonconforming mixed finite element methods and extensions to three dimensional elasticity problems.  Concerning discontinuous Galerkin methods--finite element methods in which the approximating piecewise polynomial functions are discontinuous, with modifications incorporated into the variational formulation to achieve consistency--the investigator will work with his collaborators to build on recent work on the unification and classification of such methods to develop a unified approach to the analysis of and discrimination among a wide class of discontinuous Galerkin methods for elliptic equations.  Application will also be made to the numerical simulation of elastic plates incorporating shear.  The third and largest effort concerns the numerical solution of Einstein's field equations relating mass and the curvature of space-time.  The emphasis here will be on understanding the fundamental properties of the Einstein equations most relevant to their numerical solution and the basic difficulties that have beset previous attempts at numerical simulations of them.  The work will be guided by the goal of simulating the coalescence of inspiralling pairs of black holes and the resulting emission of gravitational radiation, which is a problem of fundamental importance to gravitational physics and also because such simulations will be essential to realization of a new generation of observatories based on gravitational wave detectors.<br/><br/>Computer simulation is a key tool for the design and testing of complex engineering structures.  In recent decades computer simulation has also joined experiment and theory as one of the main paradigms of scientific investigation.  In both areas, many of the most complex systems are first modeled by systems of partial differential equations--in which the language of calculus is used to express the variations of the relevant physical quantities in space and time--and then these systems of differential equations must be approximated by numerical algorithms, which harness the power of modern computers to perform billions of arithmetical operations a seconds to extract the solutions to the equations to the required degree of accuracy. In recent decades the principles for the design and validation of such algorithms have been developed for many of the basic systems of differential equations encountered in science in technology, but many more complex systems have so far resisted effective computation, and that is the thrust of this research.  A particular emphasis will be on numerical algorithms for accurate determination of the stresses internal to elastic structures, which is essential to building safe and economic engineering structures.  A second emphasis will be on developing methods to simulate Einstein's equations of general relativity, especially for predicting the output of gravitational radiation--minute ripples that propagate on the curved surface of space-time--from massive cosmological events such as black hole collisions.  Computer codes capable of making such predictions are needed to realize the effectiveness of a new type of observatory based on gravitational radiation currently being constructed, which will provide mankind with its first window on the dark matter that makes up 90% of the universe. <br/>"
"0109132","Support of The Interface Of Three Areas OF Biomedical Science With The Mathematical Sciences","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","06/01/2001","05/22/2001","Michelle Schwalbe","DC","National Academy of Sciences","Standard Grant","Michael Steuerwalt","05/31/2002","$25,000.00","","mschwalbe@nas.edu","2101 CONSTITUTION AVE NW","WASHINGTON","DC","204180007","2023342254","MPS","1266, 1271","0000, 9263, OTHR","$0.00","Weidman<br/>0109132<br/>     There are tremendous opportunities to increase our<br/>understanding of biological and biomedical research challenges<br/>through mathematical and statistical methods.  But it is not easy<br/>for researchers to collaborate on these topics because of a<br/>historical gulf that separates the disciplines.  In order to<br/>overcome that gulf, the Board on Mathematical Sciences of the<br/>National Research Council, in collaboration with the NRC's Board<br/>on Life Sciences, holds a workshop, involving some 70 people<br/>across the disciplines, to (1) stimulate some new collaborations<br/>and insights on the spot and (2) provide an introduction for<br/>mathematical scientists (disseminated via print and the Internet)<br/>of some mathematical research directions that contribute to<br/>important ongoing research in the biological and biomedical<br/>sciences.<br/>     At the workshop, leading researchers in dynamical modeling<br/>of cellular functions, in modeling of disease states, and in<br/>neuroscience describe some of the major research challenges in<br/>their fields.  They do not limit themselves to ""mathematical""<br/>challenges; workshop discussions are focused on identifying the<br/>mathematical and statistical research that should be undertaken<br/>in order to advance the biological and biomedical research<br/>agendas.  By involving a broad range of mathematical scientists,<br/>the workshop brings a number of different perspectives and ideas<br/>to bear on the open questions raised by the presentations.  The<br/>aim is to stimulate a wide-ranging discussion and distill out the<br/>best ideas.  The workshop's summary will be widely disseminated<br/>in order to give many more mathematical scientists the basic<br/>understanding needed to become engaged in biological and<br/>biomedical research.<br/><br/>"
"0107450","Interior Point Methods for Nonconvex Nonlinear Programming","DMS","COMPUTATIONAL MATHEMATICS","08/01/2001","07/30/2001","David Shanno","NJ","Rutgers University New Brunswick","Standard Grant","Junping Wang","07/31/2005","$216,700.00","","shanno@rutcor.rutgers.edu","3 RUTGERS PLZ","NEW BRUNSWICK","NJ","089018559","8489320150","MPS","1271","0000, 9263, OTHR","$0.00","The work will continue to develop the algorithm of the LOQO <br/>interior point code for general continuous mathematical <br/>programming problems, and expand the range of problems to <br/>which it can be applied. Areas of research include better<br/>detection of infeasibility and unboundedness, better detection <br/>of linear dependence of the constraints, and better estimates <br/>of initial estimates to the optimum point. Work on higher <br/>order methods, which can greatly improve algorithmic efficiency, <br/>will be continued, with particular interest in applying filter <br/>methods. In an attempt to expand the scope of the types of <br/>problems that the algorithm can solve, the research will be <br/>concerned with solving discrete problems using interior point <br/>methods. On type of problem considered will be mixed integer <br/>quadratic programming. Problems of this type commonly arise in <br/>finance. Mixed integer linear programming problems will also <br/>be studied, with an initial focus on quickly finding feasible <br/>solutions. Further work will be concerned with general <br/>complementarity problems, especially those that arise in <br/>economics and engineering. In all cases, appropriate collections <br/>of problems arising from real applications will be modeled <br/>in AMPL. These will be made available via the internet, as will <br/>extensions to the LOQO program.<br/><br/>The LOQO code for solving a variety of types of mathematical <br/>programming problems has been under development for the past <br/>four years as a joint research effort of the principal investigator <br/>and Professor Robert Vanderbei of Princeton University. The code <br/>is made freely available via the internet, and to date has been <br/>downloaded several thousand times. It is used to solve problems <br/>as diverse as engineering design, portfolio optimization, airline <br/>scheduling problems, and is currently being adapted for various <br/>projects in medical research. The proposed research is <br/>concentrated both on improving the efficiency of the code and <br/>increasing the set of problems for which the code can be used. <br/>The new types of problems for which it is hoped the code can <br/>be adapted efficiently are problems with variables that must be <br/>an integer, generally 0 or 1. These problems are extremely common, <br/>and of great practical value. For example, when assigning aircraft <br/>to routes, a whole plane must be assigned to a specific route, <br/>not some fraction of a plane. Including integer variables within <br/>the context an algorithm of this type is very new, and will be <br/>approached initially through important specific applications."
"0113649","ITR/AP:  Optimal Nonlinear Estimation in the Geosciences","DMS","COMPUTATIONAL MATHEMATICS, ITR SMALL GRANTS","09/01/2001","05/13/2004","Gregory Eyink","AZ","University of Arizona","Standard Grant","Junping Wang","08/31/2005","$254,552.00","Shlomo Neuman, Juan Restrepo","eyink@jhu.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1271, 1686","0000, 1686, 9263, OTHR","$0.00","  This project develops a new technique for estimation of the state of a stochastic dynamical system, given some partial and imperfect information from measurements.  Unlike traditional linear estimation methods, such as least-squares variational methods or Kalman Filter approach, this new technique is capable of handling highly nonlinear dynamics--and thus non-Gaussian statistics-- in a way that approaches the optimality of the formally exact solution by Kushner, Stratonovitch, Pardoux (KSP).  Just as KSP, the new method computes the conditional statistics of the system given the measurements.  However, when applied to problems of interest in information technology, such as large-scale geoscience or environmental data assimilation, the new method does not lead to functional stochastic equations that are hopelessly intractable to solve, as does KSP.  The approach pursued in this project is instead to approximate the conditional statistics by means of a variational formulation, which yields the conditional mean as the minimizer of an appropriate cost function and the covariance as its Hessian.  The cost function proposed is calculated by a Rayleigh-Ritz or moment-closure scheme, which should render the problem tractable to numerical computation.  The main research that will be done is to develop suitable statistical techniques to model the system for the Rayleigh-Ritz calculation, to work out efficient algorithms for the numerical optimization of the cost function, and to compare with existing suboptimal estimation schemes. <br/> <br/>  In many areas of the geosciences of practical importance, it is crucial to combine information from observations with the results of a sophisticated numerical model to produce the best estimate of the past or future state of the system. In the case of a chemical or radioactive spill observed by monitoring a few well sites, one wants to track the contaminant plume backward in time to its source.  This must be accomplished with only statistical knowledge about the properties of the aquifer and groundwater flow field and with the measurements themselves subject to error. In numerical weather prediction, the goal is instead to combine the latest observations from satellite arrays with the output of large-scale, meteorological computer models to predict the future state of the weather.  Again, the models contain stochastic or chaotic elements which prevent perfect prediction based upon partial and imperfect information.  The modeling methods and numerical algorithms developed in this project should provide a practical scheme to compute the best estimate in the face of such randomness in the model and uncertainty in the observations."
"0107609","A-Posteriori-Error-Estimates-Based Numerical Methods for Shallow Water and Hamilton-Jacobi Equations","DMS","COMPUTATIONAL MATHEMATICS","09/01/2001","08/22/2003","Bernardo Cockburn","MN","University of Minnesota-Twin Cities","Standard Grant","Junping Wang","08/31/2004","$156,607.00","Dominik Schoetzau","cockburn@math.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1271","0000, 9263, OTHR","$0.00","This project is devoted to devising and studying, theoretically as well as computationally, efficient methods for numerically solving problems in which convection plays a significant role. A wide range of situations falls into this category and, although what we propose to develop can be easily applied to most of them, we are going to focus our efforts into two of them. The first, modeled by the shallow water equations, is the study of hurricane forecasting and of environmental studies in ports, and the second, modeled by the Hamilton-Jacobi equations, is the study of terrain navigation (computation of minimum time transit paths) of robotic vehicles and material etching in integrated circuit fabrication. The main focus of the project is the devising of the so-called a posteriori error estimates that are the basis for mathematically sound hp-adaptive algorithms.<br/><br/>We consider the problem of how to efficiently obtain highly accurate computer simulations of several physical phenomena of practical interest, namely, hurricane forecasting in the Gulf of Mexico and environmental studies in ports, and the computation of minimum time transit paths of robotic vehicles and material etching in integrated circuit fabrication. Since the exact solution of these complex problems is not known, in order to guarantee a given accuracy of the simulation, special techniques have to be suitably devised in order to assess its quality. Moreover, these techniques can be employed to automatically let the computer know when and where to increase or decrease the computational effort to obtain the simulation; in this way, the efficiency of its computation can be significantly enhanced. <br/>"
"0196436","Computational Problems in Multicomponent Materials and Multiphase Fluids","DMS","COMPUTATIONAL MATHEMATICS","02/01/2001","08/06/2001","John Lowengrub","MN","University of Minnesota-Twin Cities","Standard Grant","Michael Steuerwalt","07/31/2003","$117,286.00","","lowengrb@math.uci.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1271","0000, 9263, OTHR","$0.00",""
"0101339","FRG: Collaborative Research-Computational Conformal Mapping and Scientific Visualization","DMS","COMPUTATIONAL NEUROSCIENCE, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/15/2001","09/12/2001","David Rottenberg","MN","University of Minnesota-Twin Cities","Standard Grant","Junping Wang","08/31/2004","$170,000.00","","dar@pet.med.va.gov","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1162, 1266, 1271","0000, 1616, 9263, OTHR","$0.00","     This Focused Research Group is composed of pure<br/>mathematicians, computational mathematicians, and<br/>neuroscientists.  They develop implementations of discrete<br/>conformal mapping for multidisciplinary use, both within<br/>mathematics itself where complex analysis is being reinvigorated<br/>by new discrete techniques, and in the larger scientific context<br/>with visualization and analysis of scientific data.  The Riemann<br/>Mapping Theorem guarantees unique conformal maps between any pair<br/>of conformal 2-discs (or conformal 2-spheres); the conformal<br/>geometry preserved by such maps carries valuable mathematical<br/>structure.  Such surfaces arise naturally in many scientific<br/>contexts as piecewise flat (from data) or smoothly embedded (from<br/>theory) surfaces in 3-space.  Recently the new computational<br/>technique of circle packing has allowed computational<br/>approximations to these conformal maps.  Implementing such<br/>approximations for large scientific datasets faces both<br/>theoretical and computational challenges.  The investigator and<br/>his colleagues work on three related topics: theoretical<br/>superstructure of the circle packing technique, refinement and<br/>parallelization of the circle packing algorithm for use on large<br/>datasets, and the application of these conformal maps to<br/>visualization and analysis of scientific data.  The main<br/>application focuses on conformal flattening of human brain<br/>cortical surfaces.  The investigators use uniqueness of conformal<br/>maps to install surface-based coordinate systems on these<br/>surfaces; these coordinate systems allow localization of<br/>activation foci in Positron Emission Tomography (PET) and<br/>functional Magnetic Resonance Imaging (fMRI) brain scans.<br/>Conformal flattening has wider applicability as a visualization<br/>and graph embedding technique, and these connections inform the<br/>research.<br/>     This Focused Research Group develops algorithms to bring a<br/>classical mathematics theorem (the Riemann Mapping Theorem, 1854)<br/>to bear on problems of visualization of data.  The Riemann Mapping<br/>Theorem guarantees the existence of unique conformal<br/>(angle-preserving) maps between surfaces, but does say how to<br/>compute these maps.  Modern computers and new algorithms have<br/>changed all that, because our new computational ability can<br/>breathe life into classical existence theorems of mathematics,<br/>turning theory into computational tools.  This project develops<br/>algorithms to implement the computation of conformal maps on<br/>complex spatial surfaces.  The main application is the flat<br/>mapping of human brain cortical surfaces.  The brain surface is<br/>highly convoluted and folded in space, and most of the brain<br/>surface is folded up and hidden from view.  If one flattens the<br/>surface, one can simultaneously see down into all the folds.  The<br/>mathematically unique conformal maps produced by the algorithms<br/>allow surface-based coordinate systems to be computed on the<br/>brain surface so that surface positions can be precisely<br/>determined.  Moreover, if one puts foci of functional activation<br/>onto the flattened surface, one can then visualize and measure<br/>the relationship between brain function and brain anatomy.  These<br/>new surface-mapping techniques and their application to the brain<br/>surface permit biomedical researchers and clinicians to rapidly<br/>and accurately map and compare the locations of physiological and<br/>pathological ""events"" in the brains of research subjects and of<br/>patients with a variety of neurological and psychiatric<br/>disorders.  The project is supported by the Computational<br/>Mathematics, Applied Mathematics, and Geometric Analysis programs<br/>and the Office of Multidisciplinary Activities in MPS and by the<br/>Computational Neuroscience program in BIO.<br/>"
"0196549","Numerical Solution of Partial Differential Equations and Applications","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2001","12/06/2005","Douglas Arnold","MN","University of Minnesota-Twin Cities","Standard Grant","Junping Wang","08/31/2006","$180,000.00","","arnold@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1266, 1271","0000, 9263, OTHR","$0.00",""
"0106880","Computational Methods for Equilibrium Problems with Micro-Level Data","DMS","COMPUTATIONAL MATHEMATICS","09/15/2001","09/12/2001","Steven Gabriel","MD","University of Maryland, College Park","Standard Grant","Junping Wang","08/31/2005","$239,131.00","","stevenagabriel@gmail.com","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1271","0000, 9263, OTHR","$0.00","The objective of this work is to examine systems with micro-level data for which an equilibrium of some sort is to be reached.  One important example is the Gas Systems Analysis Model (GSAM), a modular, reservoir-based model of the North American natural gas system developed for the U. S. Department of Energy.  In its current form, GSAM is a large-scale nonlinear program that computes estimates of market equilibrium prices, quantities, flows, and other values based on the notion of maximizing total surplus less transportation costs.  Unlike the classical approach in which supply curves are known in closed form, GSAM builds supply curves from the ""bottom up"" using a data base of over 17,000 natural gas reservoirs taking into account both the interregional as well as intertemporal interdependence of these curves.  While this ""bottom up"" feature provides a good deal of realism, it renders the equilibrium computations much more difficult due to the lack of closed form supply curves.  The proposed work has two main objectives.  First, analyze the GSAM market equilibrium problem more generally by noting the functional relationships between seasonal market prices (Lagrange multipliers) and demand for gas, storage activity levels, investment decisions, etc. using the variational inequality problem (VIP) and nonlinear complementarity problem (NCP) formats.  The second main task is to develop efficient methods to reach a solution to GSAM-type problems exploiting the particular problem structure.  Iterative methods from optimization and equation solving is used to develop appropriate algorithms for this task.<br/><br/>Due to recent advances in information technology, it is now possible to model the activities of individual agents in rather complicated systems.  Examples of applications in scientific and engineering settings using micro-level data abound.  While simulations of these systems can be rather elaborate using for example, complicated ""if-then"" type rules, determining equilibrium behavior of the system in a rigorous manner can be challenging.  Part of the difficulty is due to a lack of closed form expressions for describing the system.  The proposed work will examine one such system in its general form and develop both a theory for equilibrium as well as efficient mathematical algorithms to compute such a solution.  The anticipated impact of this work is to greatly advance the state of the art in solving large-scale equilibrium problems that use micro-level data for modeling the economic behavior of individual agents.  This is significant since many similar systems are now modeled that contain no closed form expressions for key elements but for which an equilibrium solution is desirable."
"0102878","Workshop on Preservation of Stability Under Discretization","DMS","COMPUTATIONAL MATHEMATICS","05/15/2001","05/11/2001","Donald Estep","CO","Colorado State University","Standard Grant","Catherine Mavriplis","04/30/2002","$12,600.00","Simon Tavener","donald.estep@colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1271","0000, 9263, OTHR","$0.00","Stability is a term used to describe a wide variety of issues revolving around the physical observability of a solution of a differential equation.  Stability lies at the very heart of the ability to make predictions about physical situations from mathematics and are particularly relevant when numerical approximation is involved, since discretization causes perturbation of both the model and the data.  Numerical stability issues are typically complex because addressing such questions often requires a wide range of mathematical techniques, involving not only standard methods of numerical analysis, but ideas from dynamical systems, geometry, functional analysis, and the theory of differential equations.  This raises barriers both to young researchers trying to learn about numerical stability and to communication between researchers working in different areas yet facing similar stability problems.  The goal of the Workshop on the Preservation of Stability under Discretization is twofold:  (1) to increase the accessibility of numerical stability issues for young researchers and (2) provide an opportunity for the exchange of information and ideas between specialists in different application areas.  The Workshop will host a series of lectures by leading experts, each of whom will each address a separate aspect of stability under discretization.  The lectures will be aimed towards an audience of advanced graduate students and non-specialists and will be collected into a permanent archive. In addition, the invited speakers will host a series of discussion and analysis sessions for students and young researchers.<br/><br/>Stability is the term used to describe the situation in which a physical quantity is affected by small disturbances taking place at remote distances and times.  The ""butterfly"" effect in chaos is one well-known stability issue, but stability issues arise in nearly every kind of physical situation from the flow of fluids and gases to computing rocket trajectories to other planets.  Stability is particular relevant to numerical modeling of physical situations on computers because the modeling itself induces widespread error/disturbances.  Accounting for the effects of these discretization errors is difficult because it typically requires mathematical ideas from many areas, raising barriers both to learning about how deal with stability and to communication between different areas of specialization.  The goal of the Workshop on the Preservation of Stability under Discretization is twofold:  (1) to increase the accessibility of numerical stability issues for young researchers and (2) provide an opportunity for the exchange of information and ideas between specialists in different application areas. The Workshop will host a series of lectures by leading experts, each of whom will each address a separate aspect of stability under discretization.  The lectures will be aimed towards an audience of advanced graduate students and non-specialists and will be collected into a permanent archive. In addition, the invited speakers will host a series of discussion and analysis sessions for students and young researchers.<br/><br/>"
"0107832","Computational Error Estimation and Adaptive Error Control for Multiscaled Differential Equations","DMS","COMPUTATIONAL MATHEMATICS","08/15/2001","08/25/2004","Donald Estep","CO","Colorado State University","Standard Grant","Junping Wang","07/31/2005","$182,122.00","","donald.estep@colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1271","0000, 9263, OTHR","$0.00","Multiscaled physical systems involving processes acting on vastly different scales arise in a variety of important applications. Analyzing such systems present overwhelming challenges to mathematical analysis and therefore numerical simulation has become a centrally important tool. However, multi- scaled problems also strain the limit of current computational abilities and resources because of the resolution required to approximate behavior on the small scales accurately. The principle investigator will attack this problem by means of computational error estimation and adaptive error control. He will develop residual-based a posteriori estimates of user-chosen functionals of the solution given in terms of the computed numerical solution. The proposed approach accounts for the global effects of stability through a variational analysis and the dual problem to the original problem. The computational estimates will be used to guide the discretization resolution in order to efficiently compute numerical solutions of a desired accuracy. The research in this proposal will encompass development of computational error estimates, analysis of the reliability and accuracy of the estimates, implementation of adaptive finite element codes, and the use of these codes to investigate physical systems. The underlying applications driving this research will be investigations into the behavior of a small number of particles suspended in a low-Reynolds number flow and reaction- diffusion systems arising in shear flow problems, general relativity, modeling of pattern formation in biological systems, and population dynamics.<br/><br/>Some physical systems involve processes that act on vastly different scales. An example is given by the motion of small particles suspended in a slow flowing fluid, as for example occurs in riverbeds. In this situation, the interactions of the particles can occur on a very rapid time scale as they approach each other compared to motion of the fluid. Other multi-scaled systems include pattern formation in biological systems like stripes on Zebras, the dynamics of diseases in populations, and problems involving general relativity. Computer simulations of such systems have become a main tool for understanding and predicting their behavior. Yet the multi-scaled aspects strain current computational ability because of the resolution needed to approximate the behavior on small scales accurately. The principle investigator will attack this problem by developing computational error estimates to be used to guide the resolution needed at each point in space and time to obtain simulations of a desired accuracy. The resolution will be adjusted through a feedback mechanism as a system evolves. The proposed research encompasses development of techniques to estimate the error and adjust the resolution of the approximation, implementation of these techniques into computer programs that will be made available to the public, and the use of these computer programs to investigate real physical systems including those mentioned above. <br/>"
"0104087","Applications of Nonlinear Dynamics","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, MATHEMATICAL PHYSICS","08/01/2001","09/02/2004","James Yorke","MD","University of Maryland, College Park","Continuing Grant","Henry Warchall","07/31/2006","$540,000.00","Brian Hunt","yorke@ipst.umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1266, 1271, 1287","0000, 9263, OTHR","$0.00","NSF Award Abstract - DMS-0104087<br/>Mathematical Sciences: Applications of Nonlinear Dynamics<br/><br/>Abstract<br/><br/>DMS-0104087<br/>Yorke<br/><br/><br/>This project concerns the theory of chaotic dynamical systems.  The research explores mathematical phenomena that are relevant to the modeling of nonlinear physical systems and the analysis of experimental data.  We consider the method commonly used by scientists to ""reconstruct"" the dynamics of a nonlinear physical system from experimental data, and we study the mathematical relationship between the reconstructed dynamics and the true dynamics.  We will also develop methods for forecasting the behavior of spatiotemporally chaotic systems, such as the Earth's weather, given limited measurements of the state of the system.  Further, we will study fractal measures that arise in chaotic fluid flow and their application to problems of mixing and magnetic field generation. <br/><br/>Chaotic dynamics has been observed in many aspects of nature; in weather, it has been called the ""butterfly effect.""  Chaos can be an obstacle in some cases (limiting the predictability of the weather) and an asset in others (such as industrial mixing applications).  Our research concerns the mathematics behind the methods scientists use to model and forecast chaotic systems.  Our intent is both to examine the validity of apparently useful but unproven methods, and to develop new and improved methods.  Long-term benefits may range from better weather forecasts to more efficient procedures for mixing chemicals. <br/>"
"0107218","Efficient Numerical Methods for Viscous Incompressible Flows","DMS","COMPUTATIONAL MATHEMATICS","09/01/2001","06/18/2003","Jian-Guo Liu","MD","University of Maryland, College Park","Continuing Grant","Junping Wang","08/31/2005","$220,001.00","","jliu@phy.duke.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1271","0000, 9263, OTHR","$0.00","The investigator proposes to continue the development of efficient and accurate numerical methods for unsteady incompressible flow, with particular emphasis on (i) finding new equivalent formulations of Navier-Stokes equations, often inspired by physical considerations, better suited to numerical computation; (ii) proper handling of boundary conditions, a key difficulty in the subject since important physical interactions frequently occur near the boundary; (iii) rapid computation of three dimensional flows and generalization of schemes for flows in complex geometries; (iv) development of efficient schemes for complex fluids such as magneto-hydrodynamics, liquid crystal polymers, geodynamo, climate modeling, and large eddy turbulence simulations; (v) design of numerical schemes that preserves numerically conserved quantities such as energy and helicity for inviscid flow, and accurately captures their dissipation rates at high Reynolds number.  It is widely believed that different dissipation rates for energy and helicity is the key mechanism leading to the formation of large coherent structures of turbulent flows.<br/><br/>The numerical simulation of incompressible flows, which plays an important role in numerous scientific and industrial applications of current interest, is a challenging task for both numerical analysts and computational fluid dynamicists.  The proposed fast and accurate numerical methods for incompressible flow is expected to become an important tool for simulation and analysis of complex turbulent flow phenomena including vortex breakdown, (massive) flow separation, vortex shedding, transient jets in cross-stream, wake-body interaction, high-swirl flow, etc.  It is also an essential tool for the design of advanced flow control mechanisms used, for example, to reduce flow-induced noise and vibration, and to improve lift/drag performance at reduced energy consumption rates.  Examples include flow over bluff bodies such as ground or under-water vehicles; in engines; in/around rotating machinery or in data storage units with rotating and moving parts.<br/>"
"0112321","Interphase 2001:  Numerical Methods for Free Boundary Problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2001","09/20/2001","Ricardo Nochetto","MD","University of Maryland, College Park","Standard Grant","Thomas W. Fogwell","12/31/2002","$16,900.00","","rhn@math.umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1271","0000, 9263, OTHR","$0.00","INTERPHASE is a series of meetings, devoted mainly to numerical and computational aspects of free boundary problems, which have been held successfully once a year in Europe for the past 8 years.  The apparent misspelling of INTERPHASE is supposed to emphasize the chief purpose of this meeting: facilitate the communication and exchange of ideas and methods between communities (`phases') of scientists and engineers that otherwise may not interact and thus profit from each other.  In fact, interfaces arise in a variety of applications from materials science and fluid dynamics, to image processing and finance, which may not have any common roots and goals but however are described by similar partial differential equations and thereby present analogous difficulties, challenges, and methodology.<br/><br/>The 9th conference of the series will be held in College Park, MD, in September 2001 with the intention of encouraging more participation of US researchers and stimulating the exchange of ideas between American and European peers in the area of problems where there is an interface between regions of interest.  We plan to have about 30 senior speakers, 12 junior speakers, and a poster session."
"0110920","QEIB: Spatially-distributed Population Models with External Forcing and Spatial Control","DMS","POPULATION DYNAMICS, OFFICE OF MULTIDISCIPLINARY AC, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2001","01/18/2006","Louis Gross","TN","University of Tennessee Knoxville","Standard Grant","Mary Ann Horn","02/28/2006","$842,492.00","Suzanne Lenhart","lgross@utk.edu","201 ANDY HOLT TOWER","KNOXVILLE","TN","379960001","8659743466","MPS","1174, 1253, 1266, 1271","0000, 9178, 9251, 9263, OTHR, SMET","$0.00","Gross<br/>0110920<br/>     The investigators develop methods to carry out spatially<br/>explicit control to link natural and anthropogenic forces that<br/>influence the demand for biological resources with the dynamics<br/>of those resources.  While there have been numerous alternative<br/>approaches taken to add spatial components to population models,<br/>few of these have attempted to deal with spatial control.  The<br/>investigators evaluate extensions of optimal control methods for<br/>dynamical systems to spatial problems, with a focus on general<br/>mathematical and computational issues for spatial control linked<br/>with abiotic forcing of populations.  The effort is motivated by<br/>two specific examples.  The first concerns black bears in the<br/>southern Appalachians, a population with explicit environmental<br/>forcing associated with acorn mast production, which varies<br/>considerably in time and space.  This interacts with hunting,<br/>which is spatially restricted.  The second example involves an<br/>invasive exotic plant species, Lygodium microphyllum (Old World<br/>Climbing Fern), in the Everglades of South Florida.  This species<br/>can completely cover the native tree islands of the region and<br/>spatial control of hydrology may be applied to limit its growth.<br/>     Many of the current problems in regional management involve<br/>issues of spatial control -- what to do, where to do it and for<br/>what time periods.  Examples include alternative plans for land<br/>use, forest harvesting, location of dams, and water control.  A<br/>wide variety of the major environmental public policy issues in<br/>the US require scientific assessments of the impacts of<br/>alternative management.  The investigators develop new<br/>mathematical and computational approaches that can aid managers<br/>and the public to compare alternative management plans of land<br/>and regional resources.  The objective also is to allow for<br/>comparisons of alternative criteria, arising from different<br/>stakeholders, to judge the public utility of different spatial<br/>management plans.  The project is supported by the Applied<br/>Mathematics, Computational Mathematics, and Population Biology<br/>programs and by the MPS Office of Multidisciplinary Activities.<br/><br/>"
"0109086","Structure and Dynamics of Social Networks and Other Networked Systems","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2001","06/13/2002","Mark Newman","NM","Santa Fe Institute","Continuing Grant","Michael Steuerwalt","08/31/2002","$71,052.00","","mejn@umich.edu","1399 HYDE PARK RD","SANTA FE","NM","875018943","5059462727","MPS","1266, 1271","0000, 9263, OTHR","$0.00","Newman<br/>0109086<br/>     The principal investigator and his colleagues study the<br/>structure and function of real-world networked systems,<br/>particularly but not exclusively social networks.  An empirical<br/>component is concerned with the discovery and analysis of the<br/>structure of networks, including networks of collaboration<br/>between scientists, networks of company directors, networks of<br/>personal preferences, and networks of citations between academic<br/>publications.  Studied quantities include local observables such<br/>as transitivity and degree distribution, and nonlocal ones such<br/>as centrality and community structure.  The investigator develops<br/>models to aid in the understanding of the effects of network<br/>structure.  Of particular interest are random graph models of<br/>networks, percolation models of network resilience, and models of<br/>epidemics taking place on social networks.  The investigator<br/>develops new algorithms for extracting and visualizing network<br/>structure, particularly the existence of communities in networks<br/>and structural properties related to network resilience, such as<br/>path counts and centrality measures.<br/>     A knowledge of the structure of networks of acquaintance is<br/>crucial to the understanding of how information, such as news,<br/>rumors, consumer trends, etc., spreads through society.<br/>Similarly, networks of physical contact between people govern the<br/>way in which diseases spread.  A proper understanding of the<br/>nature and progress of epidemics is impossible without good<br/>network models.  In this project the investigator determines what<br/>the structure of the networks in question is, and also models the<br/>effect of that structure on, among other things, the spread of<br/>information and disease.  As well as enhancing basic<br/>understanding of these problems, the project points to ways in<br/>which network structure or dynamics can be changed in order to<br/>either improve network transmission (in the case of information)<br/>or slow it down (in the case of epidemics).  For disease<br/>transmission, for instance, it may be able to suggest effective<br/>targets for immunization or education campaigns to slow disease<br/>spread.  The new data resources and analysis techniques developed<br/>can be used to study other problems in which network-structured<br/>processes arise.<br/><br/>"
"0109022","Cheap Sturm-Liouville Spectral Functions","DMS","COMPUTATIONAL MATHEMATICS","08/15/2001","08/22/2001","Charles Fulton","FL","Florida Institute of Technology","Standard Grant","Junping Wang","07/31/2006","$85,000.00","Steven Pruess","cfulton@fit.edu","150 W UNIVERSITY BLVD","MELBOURNE","FL","329018995","3216748000","MPS","1271","0000, 9263, OTHR","$0.00","The software package SLEDGE (Sturm-Liouville Estimates Determined by Global Error-control) is currently the only Sturm- Liouville code that can compute approximations to the singular spectral function of singular problems having a simple continuous spectrum. The present research will greatly reduce the amount of computation required, and thereby the computer time, by making use of a new real-variable formula for the spectral function. The new formula requires only that the zeros of the one solution that comes into the expansion formula, and its quasi-derivative at these zeros, be computed. The new code produced will be able to handle the following two cases of singular Sturm-Liouville problems having simple spectrum: (1) left endpoint regular and right endpoint OSC-NONOSC with finite cut-off value and (2) left endpoint of NONOSC type (Limit Circle or Limit Point) and a Regular Singular Point and right endpoint OSC-NONOSC with finite cut-off value.<br/><br/>The continuing development of software for Sturm-Liouville problems to be done under this project will yield much more rapid routines for the spectral density functions for Sturm-Liouville problems that have a continuous spectrum. The existence of such a rapid computational capability is bound to stimulate activity and aid research in a wide range of disciplines where such problems arise: (1) Problems of the above type frequently arise in the Schroedinger wave mechanics which physicists developed to study the nature of atoms and atomic particles. (2) Similarly, in the study of more complicated molecules quantum chemists make use of quantum mechanical theory to describe the nature of energy exchange in molecules. (3) In the area of ocean dynamics the theory of acoustic waves in the ocean gives rise for some very interesting applications of Sturm-Liouville problems. (4) Certain types of nozzles for spraying water involve fluid flow problems for high speed jets from the nozzle which involve the above type of Sturm-Liouville problem over a finite range. In addition to a wide variety of applications in disciplines outside of mathematics, the current research can also be expected to stimulate the activity of pure and applied mathematicians and theoretical physicists who work on various related topics such as ""Inverse Spectral Theory"" (the attempt to reconstruct potential functions from scattering data), ""Resonance Problems"" in quantum theory (the problems of identifying and modelling positive energy bound states), and ""Periodic Potentials and Band Spectra"" which arise in the theory of crystals. The ability to run spectral function calculations quickly on small to medium size computers will greatly aid in the teaching and training of mathematicians, physicists and computer scientists interested in the theoretical and computational aspects of Sturm-Liouville problems and their applications.      <br/>"
"0105739","Collaborative Research:  Software for Decomposing Solution Sets of Polynomial Systems","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS","08/01/2001","07/31/2001","Jan Verschelde","IL","University of Illinois at Chicago","Standard Grant","Junping Wang","07/31/2005","$140,005.00","","janv@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1264, 1271","0000, 9263, OTHR","$0.00","The project is on the development of efficient numerical algorithms and high quality software to solve systems of polynomials.  A basic problem is to decompose the positive dimensional subsets of solutions into irreducible components.  The numerical approach of Sommese, Verschelde, and Wampler is based on generic slicing with linear spaces, generic projections into lower dimensional linear spaces, and use of classical interpolation techniques to numerically do what elimination theory does in symbolic programs.  Given a polynomial system with parameters, a goal of the project is to find equations on the parameters that need to be satisfied for the system to have a positive dimensional component of solutions.  Two applications targeted by this project are factoring multivariate polynomials and finding overconstrained mechanisms.<br/><br/>A major outcome of this work will be publicly available software to solve polynomial systems that arise in science and engineering.  This work is carried out in the research fields of numerical analysis and computer algebra whose mission is to provide the scientific community with software to solve mathematical problems.  Since polynomial systems are used as models in application areas as far apart as chemical reaction systems, the design of mechanisms, or economic equilibria to name but a few areas, the focus of the project on such basic models as polynomial systems is appropriate.  Besides the technology transfer of advanced mathematical tools into science and engineering, an important aspect of this project is to introduce students in the design and use of the developed software.<br/>"
"0104531","Microscopic Properties and Physical Behavior of Materials and Media","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/01/2001","07/01/2003","Oscar Bruno","CA","California Institute of Technology","Continuing Grant","Henry Warchall","07/31/2004","$217,546.00","","obruno@caltech.edu","1200 E CALIFORNIA BLVD","PASADENA","CA","911250001","6263956219","MPS","1266, 1271","0000, 9263, OTHR","$0.00","DMS Award Abstract<br/>Award #:   0104531<br/>PI:    Bruno, Oscar<br/>Institution:   California Institute of Technology<br/>Program:   Applied Mathematics<br/>Program Manager:  Catherine Mavriplis<br/><br/>Title: Microscopic Properties and Physical Behavior of Materials and Media<br/><br/>The present text proposes research in two distinct areas, namely 1) estimation of the overall (homogenized) behavior of inhomogeneous materials containing microscopic misfits, and, 2) evaluation of wave propagation and scattering by inhomogeneous media. Microscopic misfits are certain types of deformations and electrical/magnetic polarizations that are experienced by the microscopic components of a composite material.  In a ceramic-metal composite (cermet), for example, the two basic components of the mixture have different coefficients of thermal expansion; heating, therefore, gives rise to misfits - i.e., microscopic deformations which would be mismatched, were it not for constraints of continuity of elastic displacements at the boundaries between ceramic and metal.  Misfits thus produce microscopic stresses that determine, to a substantial extent, the macroscopic composite behavior.  The proposed efforts in these regards will seek to provide a theoretical understanding of misfits, so that materials with improved properties can be designed. One of our goals, for example, seeks to design ceramics that do not shatter at elevated temperatures, and are thus appropriate for use in turbine blades at high temperatures.  Problems of scattering by inhomogeneous media, on the other hand, play central roles in a wide variety of applications, including radar, sonar and remote sensing, medical microscopy, optical communications and non-invasive evaluation.  The work proposed here will seek to develop fast high-order solvers for a variety of configurations arising in engineering practice. In view of recent results, it is envisioned that the proposed techniques will significantly enhance our prediction capabilities in both materials science and computational wave propagation.<br/><br/>The proposed work impacts a number of areas of federal strategic interest, including high-performance computing, materials research, medicine, biology and national security. The specific research is in two distinct areas, namely 1) estimation of the overall behavior of inhomogeneous materials containing microscopic misfits, and, 2) evaluation of wave propagation and scattering by inhomogeneous media. These efforts seek to produce highly efficient computational methods for engineering of materials with specified properties, for the solution of problems in optics and microscopy, and for remote sensing problems related to radar and sonar. <br/><br/>Date: June 18, 2001<br/>"
"0125789","International Conference on Hyperbolic Problems: Theory, Numerics & Applications","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2001","09/17/2001","Thomas Hou","CA","California Institute of Technology","Standard Grant","Deborah Lockhart","08/31/2002","$18,000.00","Eitan Tadmor","hou@acm.caltech.edu","1200 E CALIFORNIA BLVD","PASADENA","CA","911250001","6263956219","MPS","1266, 1271","0000, 9263, OTHR","$0.00","The International Conference on ""Hyperbolic Problems: Theory, Numerics and Applications"" will be held in CalTech on March 25-29, 2002. The planned Hyp2002 conference will be the ninth meeting in the bi-annual international series which became one of the highest quality and most successful conference series in Applied mathematics. There have been many new developments in hyperbolic and nonlinear evolution PDEs since the last international hyperbolic PDEs conference was held in the US at Stony Brook in 1994. In the 2002 hyperbolic PDEs conference to be held at CalTech, we would like to bring young and active scientists together with leading researchers from different disciplines to address the theoretical, modeling, and computational issues in solving hyperbolic PDEs and more generally nonlinear evolution equations arising from various application areas. In fact, we would like to broaden further the scope of our next hyperbolic conference to include new and exciting research areas such as multiscale modeling and simulations (deriving and simulating meso-scale or nanoscale material properties in micro devices), geophysical applications (wave propagation in random media, coarsening of multi-phase flows through multiscale porous media), computational biology and computational chemistry, free boundary problems arising from materials science and multi-component fluid dynamics (thin films, crystal growth, multi-fluid interfaces, solid/liquid interfaces). The conference will provide a forum to exchange and to stimulate new ideas from different disciplines, and to formulate new challenging problems that will have important physical and industrial impacts.<br/><br/>Theoretical and numerical studies of hyperbolic problems have made tremendous impact in the developments of US economy and technology. Many high-resolution methods developed in the hyperbolic community have helped design faster airplanes, new materials, and improve our space exploration program. The hyperbolic conference series and its proceedings have become a focal point for active research in a growing field, where theory, numerics and applications complement each other. The earlier conferences were focused more on theoretical aspects of hyperbolic conservation laws. As computers became more and more powerful in the late eighties and the nineties, many effective numerical methods have been developed in simulating hyperbolic systems arising in aerodynamics and fluid dynamics applications. The hyperbolic meetings in this series have been very successful in steering the development of new high-resolution algorithms for solving complex physical systems. Many of these developments found new applications outside their traditional area of Computational Fluid Dynamics, including materials science, multiphase/multicomponent flows, combustion/detonation, flows with free boundary, geometrically based motions for image processing and more. We believe that hosting such a high level conference in the US will greatly benefit the US scientists working in this general area. It is especially helpful in educating our graduate students and postdocs, providing them with the opportunity to be in touch with the leading experts working on the frontier of this field. We will take several measures to attract young scientists. An effort will be made to increase the number of women <br/>"
"0125047","Cross-Disciplinary Research Training in Mathematical Biology","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2001","08/22/2001","James Keener","UT","University of Utah","Standard Grant","Junping Wang","08/31/2005","$99,960.00","","keener@math.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","MPS","1266, 1271","0000, 9237, 9263, OTHR","$0.00","     The investigator and his colleagues develop an<br/>interdisciplinary training program that would give graduate<br/>students substantive training in both mathematics and biology.<br/>The goal is to produce students competent in both disciplines,<br/>with genuine expertise in applied mathematics and some specific<br/>area of biology: biofluids, ecology and evolutionary biology,<br/>neuroscience, or physiology.  Training is built on the Special<br/>Interest Group, which includes faculty members from both<br/>mathematics and biology, students and postdocs, and<br/>industry-based researchers where appropriate.<br/>     Modern biology is increasingly mathematical and<br/>computational.  Education and training in components of both<br/>disciplines is needed to prepare people for the 21st century<br/>workforce, for leading-edge research, and for teaching.  This<br/>pilot project includes innovative aspects in its thematic<br/>organization, flexible arrangement of both common and<br/>area-specific content, interdisciplinary teaming, combination of<br/>theoretical and experimental training, and involvement of<br/>industrial partners.  The combination is promising for fostering<br/>long-term changes by growing a generation of scientists,<br/>mathematicians, and engineers who share language and skills well<br/>enough to take up and solve key problems."
"0102476","Mathematical modeling, analysis and computation arising in continuum mechanical descriptions of DNA","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/15/2001","08/03/2001","Oscar Gonzalez","TX","University of Texas at Austin","Standard Grant","Michael Steuerwalt","07/31/2005","$102,000.00","","og@math.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1266, 1271","0000, OTHR","$0.00","Gonzalez<br/>0102476<br/>     The investigator studies constitutive relations for<br/>continuum rod models of DNA and develops new tools for analyzing<br/>the supercoiling and packaging of DNA and other material<br/>filaments.  The main objectives of the project are to apply the<br/>theories of statistical mechanics and stochastic differential<br/>equations to develop constitutive relations for continuum rod<br/>models of DNA that are provably consistent with detailed,<br/>sequence-dependent structural information at the base-pair level;<br/>to apply the new concept of global curvature to develop a<br/>geometrically exact and computationally tractable formulation of<br/>the self-contact constraint that plays a central role in the<br/>supercoiling and packaging of DNA and other biological filaments;<br/>and to develop efficient and provably accurate computational<br/>methods for continuum rod models that faithfully respect global<br/>geometric constraints as well as qualitative features of the<br/>underlying system.<br/>     Biologists and chemists now believe that the linear sequence<br/>of base-pairs along a DNA molecule contains not only a genetic<br/>code, but also a structural code that governs the global<br/>organization of the molecule and its susceptibility to<br/>interactions with other molecules.  An understanding of this<br/>structural code requires realistic mechanical models of DNA over<br/>a wide range of length scales.  The research project pursued by<br/>the investigator is aimed at the further refinement of medium- to<br/>long-scale continuum models.  In particular, methods are<br/>developed for estimating local, sequence-dependent material<br/>parameters and for describing global, finite-thickness effects.<br/>Results from this research will be useful in developing a greater<br/>understanding of the physical and mechanical properties of DNA,<br/>in modeling how DNA may twist, bend and supercoil upon itself,<br/>and in studying how DNA and other material filaments may be<br/>optimally packed in confined geometries.<br/><br/>"
"0107247","Adaptive multinumeric finite element methods for shallow water flow","DMS","COMPUTATIONAL MATHEMATICS","09/01/2001","08/28/2001","Clinton Dawson","TX","University of Texas at Austin","Standard Grant","Junping Wang","08/31/2005","$169,888.00","","clint@ices.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1271","0000, 9263, OTHR","$0.00","Recent progress in coastal ocean modeling has emphasized two main themes: 1) the use of relatively large computational domains which encompass much larger areas than the region of specific interest, the main concept being to place the open ocean boundaries far away in deep water non-resonant ocean basins, and 2) strategically providing computational resolution using unstructured grids in order to maintain an approximately constant level of localized error throughout the domain.  This large domain/local grid refinement strategy has led to certain computational difficulties.  First, the range of flow regimes varies dramatically from the deep ocean to the shallow near shore and inland regions which include inlets, rivers, and flood plains with surrounding levee systems.  Not only are the depths dramatically different, but the force balances in the descriptive equations vary dramatically as well.  Various algorithms perform very differently within these widely disparate flow regimes in terms of stability, accuracy and localized mass conservation properties. Second, the high level of grid resolution provided in localized high flow gradient and/or very shallow water depth regions actually degrades the stability properties of many algorithms that worked quite well with coarser discretizations, and work very well in regions with smoother solutions.  The main focus of this project is to overcome these difficulties through the use of suitably coupled, finite element hp-adaptive algorithms, which are based on mathematically sound error estimates.  The investigators have an extensive history in developing continuous Galerkin finite element methods for shallow water problems, and have recently investigated the use of discontinuous Galerkin methods for these problems.  By exploiting the strengths of these two approaches, they plan to develop simulation tools for solving shallow water problems which can model large domains with locally refined, unstructured grids, can accurately resolve high gradient flow regions, can locally adapt to changes in flow characteristics, and which honor local mass conservation principles where necessary.  Specifically, under this project, the investigators will (1) further develop and analyze discontinuous Galerkin methods for shallow water flows in two and three dimensions (2) thoroughly compare continuous and discontinuous Galerkin methods for some model problems, and (3) investigate novel multi-algorithmic approaches based on coupling the two methodologies for shallow water equations and related mathematical models.<br/><br/>Accurate mathematical and computer modeling of coastal ocean circulation and transport of chemical species in shallow waters has significant implications from an economic, environmental and public health perspective.  Major inter-related issues include coastal inundation, navigation, sediment movement, pollutant transport and fisheries.  Accurate prediction of hurricane storm surges can help save lives and property in many low lying regions throughout the United States and the world.  The prediction of coastal currents and water levels is also of major significance in commercial and military navigation, e.g. in the design of harbors and navigation channels.  Current computer simulation tools are lacking in their ability to reliably and efficiently model these complex flow regimes.  The investigators on this project, through the use of advanced mathematical modeling, numerical algorithms and distributed computing technology, will develop state-of-the-art simulation tools for these applications."
"0101572","FRG: Modeling Waves and Sediment Transport in Coastal Zones","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, PHYSICAL OCEANOGRAPHY","09/01/2001","08/07/2001","Jerry Bona","TX","University of Texas at Austin","Standard Grant","Thomas W. Fogwell","02/28/2002","$865,749.00","Edward Thornton","jbona@uic.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1266, 1271, 1610","0000, 1616, OTHR","$0.00","Proposed here is a study featuring models for the interaction of surface and internal oceanic waves with sediment-laden bottom topography.  The project includes derivation and mathematical analysis of models, development of algorithms for the approximation of solutions, implementation of the algorithms as computer codes, comparisons of the output of numerical simulations with field data, the use of the models for prediction and their development as a tool for effective coastal engineering.  The basic issue under consideration is challenging in that it involves the temporal evolution of two free surfaces whose dynamics are connected in a complex and nonlinear way.  The wave motion involves the deformation of the water's  surface, or in the case of internal waves, the pycnoclines.  Even over a fixed bottom, these issues are difficult in their exact formulation, and consequently model equations are typically used.  In the present conception, the bottom is not fixed, and this added complexity is what makes the issues proposed here for study scientifically very interesting.  In view are three-dimensional models that are initiated by incoming wave fields from deep water.   These models will allow for long-shore variation and take account of reflection.  It is planned to develop such models and to test them extensively against both laboratory experiments and field observation.  Comparisons are in view at several sites around the world, including the Field Research Facility of the US Army Corps of Engineers at Duck, NC, a site of the Polish Academy of Sciences on the Baltic Sea, the North Coast of the Magdelan Islands in the Gulf of St. Lawrence, the Gold Coast of Australia, the Island of Djerba off the Tunisian coast and two regions in Morocco (Alger in the Gibraltar Strait and Agadir on the Atlantic coast).      <br/><br/>The motivation for carrying out the study outlined above is several-fold.  First is the pure science of the subject.  The project involves interesting and substantial issues from hydrodynamics, partial differential equations, and sediment transport theory.  Secondly, it is an ideal venue in which to acquaint students with interdisciplinary research.  There is a strong practical reason for this study as well.  The erosion and retreat of coastlines is a worldwide phenomenon.  Global warming will increase the activity of storms, raise the sea level, and further degrade the present, sometimes catastrophic state of many beaches.  Fragile Arctic coasts, beaches on the Great lakes and on many coastal regions already display the unmistakable signs of deterioration caused by these global changes and aggravated by social developmental pressure.  Severe erosion is likely to spread to many coastal areas within half a century and thus increase the demand for effective prevention methods.  The present project aims at deepening our understanding of fundamental wave-bottom interaction processes, especially as regards sediment dispersal. Going beyond the science, the project also grapples with the intelligent use of this kind of knowledge in designing coastal protection strategies.  With experience already in hand, it is clear that there are a variety of protection options and that it is not always smart to simply build some permanent, often-ugly structure.  So-called 'soft' protection methods are a useful addition to the reperatoire of the coastal engineer.  These are much less expensive when they can be made to work.   The assessment of the practicality of a protection plan, be it 'hard' or 'soft' relies upon the kind of knowledge investigated under the auspices of this project. <br/><br/>"
"0101476","Collaborative Research: Mathematical and Computational Methods for High Data-Rate Optical Fiber Communications","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/01/2001","08/05/2002","William Kath","IL","Northwestern University","Standard Grant","Henry Warchall","07/31/2005","$359,292.00","Gino Biondini","kath@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1266, 1271","0000, 1616, 9251, OTHR","$0.00","NSF Award Abstract - DMS-0101476 <br/><br/>Mathematical Sciences: FRG: Mathematical and Computational Methods for High-Data-Rate Optical Fiber Communications    <br/><br/>Abstract<br/>DMS-0101476 <br/>Kath   <br/><br/><br/>The goal of this research project is to develop new methods that can be used to determine the behavior of optical transmission systems under realistic circumstances.  This will be accomplished by a combination of various techniques.  One approach will exploit the mathematical structure of fiber transmission models in order to eliminate unessential degrees of freedom.  The reduced models that will result will be more tractable mathematically and also much more computationally efficient.  Another approach that will be used is the application of linearization and importance sampling techniques to enable the simulation of systems at realistic data error rates.  These methods will be combined to study the main sources of impairment in optical fibers in order to achieve an accurate evaluation of system performance.  All the techniques to be developed will be carefully validated by comparison to more computationally time-consuming models and to experiments.  <br/><br/>The development of high-data-rate optical fiber communications is one of the great technological achievements of the late 20th century; in the last decade alone, data rates have increased by four orders of magnitude.  This enormous increase has made possible the growth of the global Internet that promises to continue to revolutionize day-to-day communications.  Because demand for further growth continues unabated, however, system capacity is becoming limited by fiber transmission effects.  It has therefore become crucial to accurately model and calculate the impairments due to non-ideal fiber properties when designing systems.  Due to the tremendous data capacity that will be required of future transmission systems (terabits per second of aggregate capacity) and the need for extremely small transmission error rates (less than one error per trillion bits), realistic attempts to model and predict the effects of these impairments as they appear in practical systems present a number of difficult mathematical and computational challenges.  The techniques that will be developed in this collaborative research project are expected to yield large reductions in the computational time required to model optical communication systems, and at the same time produce new insights into system behavior.  Because these methods will be capable of providing detailed information about system performance at realistic data error rates, we believe they will lead to significant changes in the way in which optical transmission systems are modeled, and, ultimately, in the way that they are built.   <br/>"
"0107747","Physical Knots","DMS","COMPUTATIONAL MATHEMATICS","10/01/2001","09/24/2001","Gregory Buck","NH","Saint Anselm College","Standard Grant","Junping Wang","09/30/2004","$124,000.00","","gbuck@anselm.edu","100 SAINT ANSELMS DR","MANCHESTER","NH","031021310","6036417174","MPS","1271","0000, 9141, 9229, 9263, OTHR","$0.00","Knots are self-entangled filaments.  In mathematics, they have been most commonly studied as purely topological objects.  But physical knots are knots made of real physical stuff, from rope to DNA or other large flexible molecules; or purely mathematical knots endowed with physical-like properties such as energy or thickness.  The goals of physical knot theory are to mathematically model and help understand real physical systems, and to use physically inspired measures of knot complexity to develop novel methods for knot recognition/classification and gain deeper understanding of configuration spaces of knots.  The investigator also casts a wider net, looking for connections between physical knot theory and polymer theory, as well as analogies between physical knot theory and other important optimization and configuration problems such as protein folding.<br/><br/>Modern laboratory technology lets scientists see the tiniest structures of life.  Modern computers let researchers visualize and simulate how the structures interact.  This combined technology has helped make geometry and topology an essential tool in medicine, polymers, and other areas of chemistry, physics, and biology.  Knotting, or other tangling of filaments, is one of the fundamental ways that matter behaves, and is a key phenomenon in this scientific interaction.  Knotting and tangling happen at every scale studied by science, from microscopic DNA loops to everyday rope to tangled magnetic field loops in the solar corona.  The investigator, collaborators, and students study fundamental problems that arise in all these physical systems:  How are knots and tangles created?  What properties of the various systems cause essentially different kinds of knots and tangles?  Can the structure be simplified or completely untangled?  If so, how?  How do the mathematical properties of different kinds of knots or tangles influence physical behavior?  The project contributes to the effort of finding good data structures and good manipulation and visualization tools for topological and geometric objects -- essential tools for work in the realm of the very small.  These efforts are at the interface of information technology, nanotechnology, and biotechnology.  In materials and manufacturing, as well as in biotechnology, the work could have considerable impact by providing effective models of the topological and geometric behavior of polymers in general, and DNA in the specific.<br/>"
"0106694","Collaborative Research:  High Order Numerical Schemes for Multi-Dimensional Systems of Conservation Laws and for Simulations of Multi-Phase Fluids","DMS","COMPUTATIONAL MATHEMATICS","08/15/2001","08/20/2001","Ron Fedkiw","CA","Stanford University","Standard Grant","Junping Wang","07/31/2004","$75,740.00","","fedkiw@cs.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1271","0000, 9263, OTHR","$0.00","The main theme of the proposed project is the construction of high order accurate numerical schemes for solving multi-dimensional hyperbolic systems of conservation laws, and in particular the construction of numerical schemes for simulations of multi-phase fluid flows.  This includes numerical methods for compressible flow, incompressible flow and heat transfer.  Recently, the PI's introduced a boundary condition capturing method for variable coefficient Poisson equation in the presence of interfaces.  The method is implemented using a standard finite difference discretization on a Cartesian grid making it simple to apply in several spatial dimensions.  Furthermore, the resulting linear system is symmetric positive definite allowing for straightforward application of standard ""black box"" solvers, for example, multi-grid methods.  Most importantly, this new method does not suffer from the numerical smearing.  Using this method, the PI's extended the Ghost Fluid Method to treat two-phase incompressible flows, in particular those consisting of water and air.  The numerical experiments show that the new numerical method performs quite well in both two and three spatial dimensions.  Currently, they are working on extending this method to treat a wide range of problems, including for example combustion.  Of particular interest is the extension of this method to include interface motion governed by the Cahn-Hilliard equation which models the non-zero thickness interface with a molecular force balance model. <br/><br/>This proposed research on computational fluid dynamics is focused on the design, implementation and testing of new methods for simulating fluids such as water and gas using the computer.  In particular, this work addresses problems where more than one type of one phase of fluid exist, e.g. mixtures of water and air.  Our interest lies in improving the current state of the art algorithms so that they are better able to treat the interface that separates two fluids such as oil and water.  The results of this research should be of interest to both the military (e.g. many naval applications involve the study of water and air mixtures) and to private industry.  A particularly interesting example involves the interaction of water and oil in an underground oil recovery process.  The research covered in this proposal has implications for math and science education as well.  Not only will the PI's be working with and training graduate students in applied mathematics and engineering, but their research in extending these techniques to other fields, such as computer graphics, can play a role attracting the next generation of young scientists.  For example, figure 7 in ""Foster and Fedkiw, Practical Animation of Liquids, SIGGRAPH 2001"" shows the lovable character ""Shrek"", from the feature film of the same name, taking a bath in mud.<br/>"
"0101364","FRG: Topological methods in data analysis","DMS","PROBABILITY, TOPOLOGY, STATISTICS, COMPUTATIONAL MATHEMATICS","07/01/2001","06/19/2001","Gunnar Carlsson","CA","Stanford University","Standard Grant","Joanna Kania-Bartoszynska","06/30/2006","$996,396.00","Persi Diaconis, Joshua Tenenbaum","gunnar@math.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1263, 1267, 1269, 1271","0000, 1616, 9263, OTHR","$0.00","DMS-0101364<br/>Gunnar Carlsson<br/><br/>The overall goal of this project is to develop flexible <br/>topological methods which will allow the analysis of data <br/>which is difficult to analyze using classical linear methods. <br/>Data obtained by sampling from highly curved manifolds or <br/>singular algebraic varieties in Euclidean space are typical <br/>examples where our methods will be useful.  We intend to <br/>develop and refine two pieces of software which have been <br/>written by members of our research group, ISOMAP (Tenenbaum) <br/>and PLEX (de Silva-Carlsson).  ISOMAP is a tool for dimension <br/>reduction and parameterization of high dimensional data sets, <br/>and PLEX is a homology computing tool which we will use in <br/>locating and analyzing singular points in data sets, as well <br/>as estimating dimension in situations where standard methods <br/>do not work well.  We plan to extend the range of <br/>applicability of both tools, in the case of ISOMAP by <br/>studying embeddings into spaces with non-Euclidean metrics, <br/>and in the case of PLEX by building in the Mayer-Vietoris <br/>spectral sequence as a tool  Both ISOMAP and PLEX will be <br/>adapted for parallel computing. We will also begin the <br/>theoretical study of statistical questions relating to <br/>topology.  For instance, we will initiate the study of <br/>higher dimensional homology of subsets sampled from <br/>Euclidean space under various sampling hypotheses.  <br/>The key object of study will be the family of Cech <br/>complexes constructed using the distance function in <br/>Euclidean space together with a randomly chosen finite <br/>set of points in Euclidean space.  <br/><br/>The goal of this project is to develop tools for <br/>understanding data sets which are not easy to understand <br/>using standard methods.  This kind of data might include <br/>singular points, or might be strongly curved.  The <br/>data is also high dimensional, in the sense that each <br/>data point has many coordinates.  For instance, we might <br/>have a data set whose points each of which is an image, <br/>which has one coordinate for each pixel.  Many standard <br/>tools rely on linear approximations,  which do not work <br/>well in strongly curved or singular problems.  The kind <br/>of tools we have in mind are in part topological, in <br/>the sense that they measure more qualitative properties <br/>of the spaces involved, such as connectedness, or the <br/>number of holes in a space, and so on.  This group of <br/>methods has the capability of recognizing the number <br/>of parameters required to describe a space, without <br/>actually parameterizing it.   These methods also have the <br/>capability of recognizing singular points (like points <br/>where two non-parallel planes or non-parallel lines <br/>intersect), without actually having to construct <br/>coordinates on the space.  We will also be further <br/>developing and refining methods we have already <br/>constructed which can actually find good parameterizations <br/>for many high dimensional data sets.  Both projects will <br/>involve the adaptation for the computer of many methods <br/>which have heretofore been used in by-hand calculations <br/>for solving theoretical problems.  We will also initiate <br/>the theoretical development of topological tools in a setting <br/>which includes errors and sampling.  <br/><br/>"
"0106781","A Posteriori Error Analysis and Adaptive Algorithms for Variational Inequalities","DMS","COMPUTATIONAL MATHEMATICS","08/15/2001","08/08/2001","Weimin Han","IA","University of Iowa","Standard Grant","Junping Wang","07/31/2004","$137,041.00","","weimin-han@uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1271","0000, 9263, OTHR","$0.00","The objective of this project is to develop a systematic theory on a posteriori error estimation and adaptive algorithms for finite element solutions of variational inequalities. A posteriori error estimates play an essential role in assessing the reliability of numerical solutions and in developing efficient adaptive algorithms. Although several standard techniques have been developed to derive and analyze a posteriori error estimates for finite element solutions of differential equation problems, they do not work directly for a posteriori error analysis of numerical solutions of variational inequalities. In the proposed research, the duality theory of nonlinear analysis will be employed as a basic mathematical tool for the development of a posteriori error estimates of numerical solutions of variational inequalities.<br/><br/>Variational inequalities form an important family of nonlinear problems arising in a wide range of applications.  A partial list of the applications where variational inequalities arise include contact mechanics, plasticity, non-Newtonian fluid flows, metal forming, metal extrusion, financial mathematics.  A systematic approach will be developed to derive a posteriori error estimates and to form and test efficient, reliable adaptive solution algorithms for finite element approximations of variational inequalities. The research results will have a direct impact on many areas of science and technology. <br/>"
"0107209","Physical Knots","DMS","COMPUTATIONAL MATHEMATICS","10/01/2001","09/24/2001","Jonathan Simon","IA","University of Iowa","Standard Grant","Junping Wang","09/30/2006","$177,000.00","","jsimon@math.uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1271","0000, 9263, OTHR","$0.00","The investigator and his colleagues study ""Physical Knots,"" bridging the gap between purely topological properties of knots and links, and physically realistic systems in which properties such as thickness, curvature, repelling forces, and randomness are evident.  Focus areas include: understanding how random tangling increases with filament length in different kinds of systems; determining existence, uniqueness, and geometric properties of optimal conformations of knots; modeling gel electrophoresis of knotted DNA loops; understanding how the ""symmetric energy"" of knots models physical behavior such as accessibility of molecules to enzymes and self-irradiation of filaments that emit; determining relationships between various knot energies; understanding how knots, such as folding proteins, form in filaments with free ends.<br/><br/>This project contributes to the understanding of one of the fundamental ways that matter behaves: a solid object occupies space; a sheet of material separates one part of space from another; and a string tangles with itself or other strings.  There is growing scientific awareness that knotting and tangling happen, and are physically important, at every scale of size, from molecules such as DNA and other polymers, to magnetic field lines in the sun.  But there are many basic questions that are not yet answered:  How are knots and tangles created, or destroyed, in various settings?  What happens when one pulls a knot tight?  Why do mathematically different kinds of knots behave the way they do in physical situations?  How can we model knots on the computer, and how well do the computer simulations reflect actual behavior?  The combination of importance of the phenomenon, together with substantial open questions, makes this area fascinating and valuable for research.  In particular, the project seeks to contribute to areas of national interest, including increasing understanding of the behavior of DNA molecules, and helping to elucidate the process of protein folding.<br/>"
"0072873","Experiment-Based 3-D Computational Studies of Blood Flow in Stenotic Carotid Arteries with Dynamic Wall Properties","DMS","COMPUTATIONAL MATHEMATICS, EAST ASIA AND PACIFIC PROGRAM","09/15/2001","09/12/2001","Dalin Tang","MA","Worcester Polytechnic Institute","Standard Grant","Junping Wang","08/31/2005","$163,000.00","Homer Walker","dtang@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","MPS","1271, 5978","0000, 9200, 9263, OTHR","$0.00","    The principal investigator and his colleagues use an<br/>experiment-based computational approach to model blood flow in<br/>large stenotic arteries and investigate critical flow and artery<br/>wall behaviors that may lead to artery compression, plaque cap<br/>rupture, stroke and heart attack.  Dynamic properties of the<br/>stenotic arteries with plaque stiffness and geometry variations<br/>are determined experimentally using hydrogel tubes whose<br/>mechanical properties are close to those of bovine carotid<br/>arteries.  This leads to a series of 3-D nonlinear computational<br/>models with fluid-wall interactions based on the experimental<br/>measurements.  Arbitrary Lagrangian-Eulerian formulation is used<br/>to deal with free moving boundaries.  Implicit methods including<br/>the SIMPLER algorithm and fully coupled methods based on<br/>mesh-free generalized finite differences with staggered grids,<br/>upwind techniques, and a consistent physical interpolation<br/>technique are used to solve the fluid model.  A sequence of<br/>experiment-based thin- and thick-wall models are introduced to<br/>model the dynamic nonlinear properties of the stenotic tube wall<br/>with large strain, deformation, compression and collapse.  An<br/>incremental boundary iteration method and an under-relaxation<br/>technique are used to handle the fluid-wall interactions.<br/>Validated by experimental data, results obtained are<br/>physiologically relevant and may provide information helpful for<br/>early detection, diagnosis and prevention of related<br/>cardiovascular diseases.  The models and numerical methods<br/>developed are applicable to a wide range of problems with<br/>fluid-structure interactions and can be extended to include mass<br/>transfer, structures of arteries and plaques, endothelial<br/>responses, and arterial remodeling.<br/>     Stenosis, a constriction in blood vessels, is one of the<br/>leading causes of death in the western world.  The investigators<br/>and their colleagues couple experiments with computations to<br/>model blood flow in large stenotic arteries and investigate<br/>critical flow and artery wall behaviors that may lead to artery<br/>compression, plaque cap rupture, stroke and heart attack.  The<br/>problem is difficult because of the high complexity of artery<br/>structure and its nonlinear mechanical properties, strong blood<br/>and artery interactions, and critical flow conditions caused by<br/>severe stenosis.  Dynamic properties of the stenotic arteries are<br/>determined experimentally using bovine carotid arteries and<br/>hydrogel tubes, whose properties are close to those of bovine<br/>carotid arteries.  A series of 3-D nonlinear computational models<br/>with fluid-wall interactions, based on the experimental<br/>measurements, are solved by a novel numerical method to quantify<br/>conditions under which artery compression and plaque cap rupture<br/>may occur.  Validated by experimental data, these results can be<br/>helpful for early detection, diagnosis and prevention of related<br/>cardiovascular diseases.  The models and numerical methods<br/>developed are applicable to a wide range of problems with<br/>fluid-structure interactions and complex geometries.  The models<br/>can be extended to include mass transfer, structures of arteries<br/>and plaques, grafts and stents, endothelial responses and<br/>arterial remodeling.  As applications of biotechnology, the<br/>results obtained can be used to improve the design of medical<br/>devices such as grafts and stents."
"0107893","Automated Morphometry of Dendritic Spines","DMS","COMPUTATIONAL NEUROSCIENCE, OFFICE OF MULTIDISCIPLINARY AC, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2001","08/10/2001","W.Brent Lindquist","NY","SUNY at Stony Brook","Standard Grant","Michael Steuerwalt","08/31/2004","$181,000.00","","lindquis@ams.sunysb.edu","W5510 FRANKS MELVILLE MEMORIAL L","STONY BROOK","NY","117940001","6316329949","MPS","1162, 1253, 1266, 1271","0000, 9263, OTHR","$0.00","Lindquist<br/>0107893<br/>     The investigator creates quantitative computational tools<br/>for the automated analysis of neural images, including software<br/>algorithms to provide automated morphology measurements both of<br/>neuron dendrites and of dendritic spines (the primary synaptic<br/>receivers).  These tools are developed in close collaboration<br/>with the neural imaging research programs of two groups, led by<br/>Prof. K. Svoboda at Cold Spring Harbor Lab. and by Profs. P. Hof<br/>and S. Wearne at Mt. Sinai School of Medicine.  In preliminary<br/>work, automated algorithms have been developed and verified<br/>(against manual measurements) to identify and extract<br/>morphological measurements of dendritic spine length and density<br/>from 3D images.  This project develops algorithms to<br/>  i) provide spine volume measurements, which are currently<br/>available only through manually intensive, scanning electron<br/>microscopy measurements;<br/>  ii) analyze (two-channel) images containing two dyes that<br/>fluoresce at different frequencies.  One dye is a marker for<br/>spine morphology, the second is a marker for specific<br/>functionality.  This allows for direct correlation studies of<br/>form and function.  The primary application of these algorithms<br/>is in structural genetics work.<br/>  iii) determine higher moment (shape) measures both of spines<br/>and dendrite cross sections.  The primary application of these<br/>algorithms is in developing models of neural integration behavior<br/>and age-related deficits in short-term memory.<br/>     Biological science technologies are now capable of producing<br/>data at a rate so fast that only with computers and sophisticated<br/>numerical and statistical algorithms can biologists analyze these<br/>data.  In the past decade, laser scanning fluorescence<br/>microscopy, enabling non-invasive, three dimensional imaging of<br/>living neurons, has produced a revolution in neural research.<br/>The wealth of information available from a time sequence of three<br/>dimensional neuron images is outstripping our current information<br/>processing capacity, which has been based upon manual<br/>identification and tracing of structures of interest.  Although<br/>it uses the superb recognition capability of human sight, manual<br/>measurement is tedious, susceptible to systematic and hard to<br/>characterize human biases, restricted to measurements involving<br/>only length and counting, and effective only for small data sets.<br/>The purpose of this project is to develop computer based tools<br/>for the automated analysis of neuron images, eliminating manual<br/>analysis.  The resultant computer tools will make practical such<br/>studies as<br/>  i) the relationship between neuron structure and age-related<br/>deficits in memory as well as some forms of mental retardation;<br/>  ii) where in neurons certain proteins concentrate and how<br/>their presence and concentration correlates to the signal<br/>processing role of neurons;<br/>  iii) large scale screening of drug treatments designed to<br/>correct genetic- or trauma-induced deficiencies in neuron<br/>structure.<br/>The project is supported by the DMS/Computational Mathematics,<br/>DMS/Applied Mathematics, and IBN/Computational Neuroscience<br/>programs and by the MPS/Office of Multidisciplinary Activities.<br/>"
"0107621","Persistence, Combinatorial Morse Functions and Shape Spaces and their Applications","DMS","TOPOLOGY, COMPUTATIONAL MATHEMATICS, ROBOTICS","08/15/2001","08/30/2002","John Harer","NC","Duke University","Standard Grant","Michael Steuerwalt","07/31/2003","$99,996.00","","john.harer@duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1267, 1271, 6840","0000, 9237, 9263, OTHR","$0.00","This proposal concerns the exploration of new approaches to computational problems that arise in robotics, geographic information systems, and in biological applications.  These approaches involve techniques that lie at the intersection of mathematics and computer science and the application of mathematical perspectives in the development on new algorithms to solve a variety of problems.  The primary goal is to use techniques from Morse theory, homology theory, geometric group theory and combinatorics to study problems of shape, configuration, motion planning, and structure in computer science.  Applications include computer graphics, visualization of scientific data, computational analysis of molecular docking problems and robotics.  A key element is that this work will be done within an interdisciplinary team that includes researchers in computer science, biochemistry and chemistry at Duke, UNC and Stanford.<br/><br/>The research in this project will be important to advance our understanding in a number of important areas.  Techniques developed will give new approaches to analyzing noise in data sets such as those from x-ray Crystallography, brain scans, satellite images, and ocean temperature.  The robotics work will enhance our ability to create automated devices that perform tasks in dangerous or remote environments.  And the biology work has as its long term goal the development of better computational tools to help biomedical researchers find appropriate molecules that can interact with"
"0109895","Quantitative Insight into Responses of Cell Populations to Radiation Exposure","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2001","08/28/2001","Leonid Hanin","ID","Idaho State University","Standard Grant","Mary Ann Horn","08/31/2005","$132,000.00","","hanin@isu.edu","921 S 8TH AVE","POCATELLO","ID","832015377","2082822592","MPS","1266, 1271","0000, 9263, OTHR","$0.00","Hanin<br/>0109895<br/>     The goal of the project is to gain a new quantitative<br/>insight, on the basis of advanced mathematical methods, into<br/>kinetics of biological processes in irradiated cell populations.<br/>The investigator formulates and analyzes a comprehensive<br/>mechanistic mathematical model of these processes, including<br/>formation of radiation-induced lesions and their repair,<br/>misrepair and fixation, cell cycling and cell death.  Mathematical<br/>modeling techniques include general age-dependent branching<br/>stochastic processes, queueing systems, stochastic simulation,<br/>and modern methods of statistical inference.  Parameters of the<br/>model are estimated from experimental data on kinetics and<br/>survival of cells in synchronized cultures of S3 HeLa and V79<br/>cells exposed to continuous irradiation at various dose rates.<br/>Parameter estimation is based on a comprehensive stochastic<br/>simulation model of the underlying biological processes and on<br/>methods of stochastic optimization in the maximum likelihood<br/>setting.  The model is applied to stationary and exponential cell<br/>cultures that serve as experimental counterparts of tumor cell<br/>populations at different stages of tumor development.  The<br/>distribution of the number of surviving clonogenic cells at the<br/>end of radiation exposure, obtained from the comprehensive<br/>simulation model of cell population kinetics, is compared with<br/>explicit formulas for this distribution within simpler birth and<br/>death Markov models that resulted from the preliminary studies.<br/>The most important biological outcome of this work consists of<br/>the set of estimates of unobservable model parameters, which are<br/>interpretable in biologically appealing terms of repair processes<br/>and cell kinetics.  The project has a significant impact on<br/>evaluation, prediction and optimization of the efficacy of<br/>radiation cancer treatment and associated methods of cure rate<br/>estimation.<br/>     A remarkable progress in cell biology and radiation biology<br/>since the 1920's has resulted in a prodigious amount of<br/>experimental data comprising all aspects of cell responses to<br/>ionizing radiation.  These data represent an invaluable source of<br/>information for furthering our knowledge about intracellular<br/>processes in normal and irradiated cells.  However, to a large<br/>extent, this gold mine of experimental findings amassed over<br/>years remains unclaimed.  The investigator and his collaborators<br/>attempt to understand and describe mathematically the most basic<br/>processes in normal and irradiated cells.  This is accomplished on<br/>the basis of advanced mathematical, statistical and computational<br/>methods applied to the original experimental data describing<br/>responses of several cell lines to radiation exposure at variable<br/>dose rate.  The project was triggered by and has far-reaching<br/>implications for cancer radiotherapy.<br/>"
"0103104","Non-uniform sampling and reconstruction:Theory and algorithms","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2001","08/13/2001","Akram Aldroubi","TN","Vanderbilt University","Standard Grant","Michael Steuerwalt","08/31/2005","$147,500.00","","akram.aldroubi@vanderbilt.edu","110 21ST AVE S","NASHVILLE","TN","372032416","6153222631","MPS","1266, 1271","0000, 9263, OTHR","$0.00","Aldroubi<br/>0103104<br/>     The investigator and his colleagues develop a mathematical<br/>framework and fast computational schemes for the reconstruction<br/>of functions, signals or images from noisy, very large sampled<br/>data sets, acquired on nonuniform grids, by nonideal acquisition<br/>devices.  The problem of nonuniform sampling and reconstruction<br/>is treated in the context of shift-invariant subspaces, Besov<br/>spaces, and in arbitrary dimensions.  The theory is developed for<br/>the case when the samples are obtained from weighted averages.<br/>Density conditions for exact reconstruction are established.  When<br/>the data are noisy, incomplete, or when the assumptions needed<br/>for exact reconstruction are not satisfied, bounds on the error<br/>between the reconstructed and original signal are derived in<br/>terms of the sampling densities, the averaging functionals, and<br/>the noise statistics.  The development of the mathematical<br/>framework and the computational schemes requires a new set of<br/>techniques and ideas, and involves several areas of mathematics<br/>including wavelet theory, frame theory, functional analysis, and<br/>harmonic analysis.<br/>     The project is motivated by problems arising in data<br/>transmission, geophysical exploration, astronomy, spectroscopy,<br/>and biomedical imaging.  The problem of reconstructing a signal or<br/>an image from a set of nonuniform samples is encountered in many<br/>applications of signal or image processing.  For example, the loss<br/>of data packets during transmission through internet or from<br/>satellites can be viewed as a nonuniform sampling and<br/>reconstruction problem.  In geophysical exploration, the earth's<br/>magnetic field is measured by a combination of airborn, fast<br/>moving acquisition devices, as well as scattered stationary<br/>devices resulting in highly nonuniform sampling patterns, and a<br/>huge data set.  The goal is to reconstruct the magnetic field and<br/>use it to reveal geological features.  In fact, modern digital<br/>data processing of signals or images always uses a sampled<br/>version of the original analog signals or images.  However, the<br/>sampling devices are never ideal, and the collected data consist<br/>of average samples.  Moreover these data are often very large,<br/>incomplete, and corrupted by noise.  The question then arises<br/>whether and how the original signal can be recovered from the<br/>data.  Therefore the investigator aims to 1) quantify the<br/>conditions under which it is possible to reconstruct a signal<br/>exactly from different sets of nonuniform average-samples; 2) use<br/>these analytical results to develop explicit, and computationally<br/>efficient reconstruction schemes; and 3) analyze the performance<br/>of the algorithms under adverse conditions, or when the data are<br/>incomplete or corrupted by noise.  The development of a theory and<br/>algorithms that perform well under stringent and realistic<br/>situations will help the analysis, processing and management of<br/>very large data sets obtained digitally by new acquisition<br/>modalities, and transmitted or received by communication networks<br/>such as the internet, cellular phones, and other distributed<br/>communication systems.<br/><br/>"
"0125000","Meeting Support:  Modeling Across the Scales - Atoms to Organisms to be held January 5-10, 2002, in Santa Fe, New Mexico","DMS","COMPUTATIONAL MATHEMATICS","09/15/2001","08/28/2001","De Witt Sumners","FL","Florida State University","Standard Grant","Michael Steuerwalt","08/31/2003","$19,710.00","","sumners@math.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1271","0000, 9263, OTHR","$0.00","Sumners<br/>0125000<br/>     The investigator and his colleagues in the  Program in<br/>Mathematics and Molecular Biology (PMMB) organize an<br/>international conference on the subject of multiscale phenomena<br/>at the interface of the biological and mathematical sciences.<br/>This meeting includes a broadly multidisciplinary set of<br/>participants, students and researchers active at the interface<br/>between mathematics (broadly defined) and biology.  This project<br/>supports the participation of approximately 15 domestic students<br/>and young scientists, selected from a national pool of applicants<br/>generated by web and journal advertisements for the meeting.  An<br/>unusual feature of the meeting is an opening day of tutorials,<br/>which prepare the attendees for the lectures, with mathematical<br/>scientists introducing basic principles to biologists and<br/>biologists giving tutorials for the quantitative scientists.  Two<br/>sessions focus on issues of importance to students working at the<br/>interface between the computational sciences and biology.<br/>Students are especially welcome to attend the meeting and present<br/>posters on their work.<br/>     The Program in Mathematics and Molecular Biology (PMMB)<br/>conducts research at the interface between molecular biology and<br/>the mathematical sciences and trains students and postdocs in<br/>this area.  An essential part of this effort is a series of<br/>conferences on mathematics and molecular biology.  The meetings<br/>provide a comfortable but challenging environment where students<br/>and scientists from each discipline can learn about the other<br/>discipline and about multidisciplinary approaches to the solution<br/>of important problems in modern biology.  This fosters<br/>interdisciplinary training, catalyzes new interdisciplinary<br/>collaborations, and in an important scientific area helps develop<br/>more people with both mathematics and biology expertise.  The<br/>project supports students and junior scientists.<br/>"
"0108672","Numerical Methods for Microscale and Nanoscale Multiphase Flow in General Geometries","DMS","COMPUTATIONAL MATHEMATICS","09/01/2001","12/03/2001","Mark Sussman","FL","Florida State University","Standard Grant","Junping Wang","08/31/2005","$100,000.00","Mohammed Hussaini","sussman@math.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1271","0000, 9263, OTHR","$0.00","The research is for the development of mathematical and numerical models for the full numerical simulation of jetting phenomena.  The principal investigators develop new mathematical and numerical models for phase change, vapor bubble formation, vapor bubble collapse, and ""explosive evaporation.""  New numerical methods for including compressibility in low speed flow are being developed.  The resulting models are cast in an adaptive mesh framework in order to gain in CPU runtime and memory efficiency.<br/><br/>This research program, the numerical simulation of jetting devices, unravels basic physical mechanisms underlying the jetting processes.  This leads to improvement in design of microscale jetting devices.  The simulation substantially reduces the design cycle period of such devices as ink-jet printers, soldering, and the application of DNA strands.<br/>"
"0101329","FRG: Collaborative Research: Computational Conformal Mapping and Scientific Visualization","DMS","COMPUTATIONAL NEUROSCIENCE, OFFICE OF MULTIDISCIPLINARY AC, GEOMETRIC ANALYSIS, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/15/2001","09/12/2001","De Witt Sumners","FL","Florida State University","Standard Grant","Junping Wang","08/31/2005","$410,000.00","Philip Bowers, Monica Hurdal","sumners@math.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1162, 1253, 1265, 1266, 1271","0000, 1616, 9263, OTHR","$0.00","     This Focused Research Group is composed of pure<br/>mathematicians, computational mathematicians, and<br/>neuroscientists.  They develop implementations of discrete<br/>conformal mapping for multidisciplinary use, both within<br/>mathematics itself where complex analysis is being reinvigorated<br/>by new discrete techniques, and in the larger scientific context<br/>with visualization and analysis of scientific data.  The Riemann<br/>Mapping Theorem guarantees unique conformal maps between any pair<br/>of conformal 2-discs (or conformal 2-spheres); the conformal<br/>geometry preserved by such maps carries valuable mathematical<br/>structure.  Such surfaces arise naturally in many scientific<br/>contexts as piecewise flat (from data) or smoothly embedded (from<br/>theory) surfaces in 3-space.  Recently the new computational<br/>technique of circle packing has allowed computational<br/>approximations to these conformal maps.  Implementing such<br/>approximations for large scientific datasets faces both<br/>theoretical and computational challenges.  The investigator and<br/>his colleagues work on three related topics: theoretical<br/>superstructure of the circle packing technique, refinement and<br/>parallelization of the circle packing algorithm for use on large<br/>datasets, and the application of these conformal maps to<br/>visualization and analysis of scientific data.  The main<br/>application focuses on conformal flattening of human brain<br/>cortical surfaces.  The investigators use uniqueness of conformal<br/>maps to install surface-based coordinate systems on these<br/>surfaces; these coordinate systems allow localization of<br/>activation foci in Positron Emission Tomography (PET) and<br/>functional Magnetic Resonance Imaging (fMRI) brain scans.<br/>Conformal flattening has wider applicability as a visualization<br/>and graph embedding technique, and these connections inform the<br/>research.<br/>     This Focused Research Group develops algorithms to bring a<br/>classical mathematics theorem (the Riemann Mapping Theorem, 1854)<br/>to bear on problems of visualization of data.  The Riemann Mapping<br/>Theorem guarantees the existence of unique conformal<br/>(angle-preserving) maps between surfaces, but does say how to<br/>compute these maps.  Modern computers and new algorithms have<br/>changed all that, because our new computational ability can<br/>breathe life into classical existence theorems of mathematics,<br/>turning theory into computational tools.  This project develops<br/>algorithms to implement the computation of conformal maps on<br/>complex spatial surfaces.  The main application is the flat<br/>mapping of human brain cortical surfaces.  The brain surface is<br/>highly convoluted and folded in space, and most of the brain<br/>surface is folded up and hidden from view.  If one flattens the<br/>surface, one can simultaneously see down into all the folds.  The<br/>mathematically unique conformal maps produced by the algorithms<br/>allow surface-based coordinate systems to be computed on the<br/>brain surface so that surface positions can be precisely<br/>determined.  Moreover, if one puts foci of functional activation<br/>onto the flattened surface, one can then visualize and measure<br/>the relationship between brain function and brain anatomy.  These<br/>new surface-mapping techniques and their application to the brain<br/>surface permit biomedical researchers and clinicians to rapidly<br/>and accurately map and compare the locations of physiological and<br/>pathological ""events"" in the brains of research subjects and of<br/>patients with a variety of neurological and psychiatric<br/>disorders.  The project is supported by the Computational<br/>Mathematics, Applied Mathematics, and Geometric Analysis programs<br/>and the Office of Multidisciplinary Activities in MPS and by the<br/>Computational Neuroscience program in BIO.<br/>"
"0130107","Collaborative Research:  Focused Research Group: Analysis and Simulation of Magnetic Devices","DMS","COMPUTATIONAL MATHEMATICS","08/15/2001","08/20/2001","Weinan E","NJ","Princeton University","Standard Grant","Junping Wang","07/31/2004","$299,427.00","","weinan@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1271","0000, 1616, 9263, OTHR","$0.00","Nanoscale magnetic devices are of critical technological importance.  This project will advance our understanding of their properties through a coordinated program of modeling, analysis, simulation and experiment.  Topics to be addressed include (a) development of improved numerical methods for the simulation of magnetic materials and devices; (b) exploration of the micromagnetic ""energy landscape"" and the role of noise in thermally activated switching; and (c) investigation of specific nanoscale effects such as configurational anisotropy and geometrically constrained walls.  Mathematics has much to contribute and much to gain.  The study of appropriate limits leads to challenging problems of analysis whose solution will shed light on the essential physics.  The analysis of noise and switching leads to the study of the energy landscape and to physically relevant examples of stochastic partial differential equations.  Modeling coordinated with laboratory experiments will refine our understanding of the relevant phenomena.  This Focused Research Group activity will draw expertise from a multidisciplinary group of mathematicians, physicists and computational scientists.  The project includes a collaboration with IBM and training of postdoctoral scientists and graduate students. <br/><br/>Magnetic storage devices lie at the foundation of modern computing.  Their modeling, simulation, analysis and design raise fundamental questions of physics and mathematics, many still unanswered.  As device size decreases, the relevant science changes: defects, spatial disorder and thermal fluctuations become crucial in the nanoscale regime.  Mathematics has much to contribute and much to gain.  The study of appropriate limits leads to challenging problems of analysis whose solution will shed light on the essential physics.  The analysis of noise and switching will be studied in a three-pronged approach: by mathematical analysis, numerical modeling and experimental investigation.  Modeling coordinated with laboratory experiments will refine our understanding of the relevant phenomena.  This Focused Research Group activity will draw expertise from a multidisciplinary group of mathematicians, physicists and computational scientists.  The project includes a collaboration with IBM and training of postdoctoral scientists and graduate students. <br/>"
"0109427","Support for the Center for BioDynamics at Boston University","DMS","COMPUTATIONAL NEUROSCIENCE, ADVANCES IN BIO INFORMATICS, OFFICE OF MULTIDISCIPLINARY AC, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","09/01/2001","06/20/2005","Nancy Kopell","MA","Trustees of Boston University","Continuing Grant","Michael Steuerwalt","08/31/2007","$1,409,910.00","Tasso Kaper, James Collins, John White","nk@bu.edu","1 SILBER WAY","BOSTON","MA","022151703","6173534365","MPS","1162, 1165, 1253, 1266, 1271, 7334","0000, 9236, 9263, OTHR","$0.00","     The investigator and her colleagues collaborate in a group<br/>project at the Center for BioDynamics (CBD) to provide<br/>interdisciplinary education and training for graduate students<br/>and postdoctoral-level investigators in the context of a vigorous<br/>interdisciplinary research program that focuses on areas of<br/>mutual interest in mathematics (especially dynamical systems),<br/>biology, and engineering.  Disciplines include mathematics,<br/>biomedical engineering, aerospace/mechanical engineering,<br/>biology, psychology, and physics.  Training extends beyond the<br/>usual classroom activities by engaging participants in a variety<br/>of research projects as well.  One of the major topics is dynamics<br/>of the nervous system.  The projects, which involve experiments,<br/>modeling, and analysis, all deal with the variety of rhythms in<br/>the nervous system and the potential functions of these rhythms<br/>in key cognitive states and processes such as attention,<br/>awareness, learning, and recall.  A second major topic is<br/>dynamics of gene expression.  Progress in genomic research is<br/>leading to maps of the building blocks of biology and fueling the<br/>study of gene regulation, where proteins often regulate their own<br/>production or that of other proteins in a complex web of<br/>interactions.  CBD projects focus on using techniques from<br/>nonlinear dynamics, statistical physics, control theory, and<br/>molecular biology to model, design, and construct synthetic gene<br/>regulatory networks, and to probe naturally occurring gene<br/>regulatory networks.  The third major topic is the dynamics of<br/>patterns and waves.  Training activities include two weekly<br/>working seminars, extra journal clubs and reading groups,<br/>seminars to educate the CBD members in the research going on<br/>within the Center, and a CBD-initiated team-taught course.<br/>     The Center for BioDynamics (CBD) helps to advance<br/>understanding of difficult interdisciplinary problems at the<br/>intersection of mathematics, biology, and engineering, and it<br/>trains mathematicians, scientists, and engineers for the 21st<br/>century workforce.  It does this by combining traditional<br/>classroom education with significant engagement of students and<br/>postdocs in interdisciplinary teams working on current problems.<br/>The disciplines involved are mathematics, biomedical engineering,<br/>aerospace/mechanical engineering, biology, psychology, and<br/>physics.  One of the major topics is dynamics of the nervous<br/>system.  The projects in this topic seek to shed light on the<br/>origin of the electrical activity in the brain, and how the brain<br/>uses this activity to process sensory information, to think, and<br/>to regulate movement.  A second major topic is dynamics of gene<br/>expression.  The web of interactions among the proteins that are<br/>produced by genes is complex; the projects associated with this<br/>topic involve the design and construction of artificial gene<br/>regulatory networks, and techniques to better understand<br/>naturally occurring gene regulatory networks.  The third major<br/>topic is the dynamics of patterns and waves, occuring in a<br/>variety of applications.  Training activities include two weekly<br/>working seminars, regular sessions to read scientific journals,<br/>seminars to educate the CBD members in the research going on<br/>within the Center, and a CBD-initiated team-taught course.  The<br/>project is supported by the Computational Mathematics, Applied<br/>Mathematics, Computational Neuroscience, and Biological Databases<br/>and Informatics programs and by the MPS Office of<br/>Multidisciplinary Activities.<br/>"
"0100042","Combinatorics, Probability and Computation of Finite Groups","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS","08/01/2001","05/16/2003","Igor Pak","MA","Massachusetts Institute of Technology","Continuing Grant","Andrew Pollington","07/31/2004","$108,499.00","","pak@math.ucla.edu","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","MPS","1264, 1271","0000, 9263, OTHR","$0.00","The investigator will study finite groups from Combinatorial, Probabilistic and Computational point of view.  The research will proceed in three major directions.  First, the problem of generating random group elements is studied. The two major venues: Babai algorithms and the product replacement algorithm - both will be attacked by the investigator.  Second problem involves recognition of the finite groups based on the random elements. Finally, third problem deals with property testing of groups is studied, by introducing random subproducts as pseudo random elements in the finite group.<br/><br/>Finite groups can be viewed as sets of symmetries of finite objects; they are central in understanding of our universe.  Finite groups are often unimaginably large, which represents both theoretical and computational difficulties for working with all its elements.  Thus the information about the group is often stored in a small set of elements (generators), so that all other group elements can be obtained from these.  Now the difficult problem is reversing this encoding and recovering information about the whole group from the generators.  The current proposal aims at developments of the new algorithms and improvement of the existing procedures.<br/>"
"0296024","Adaptive Wavelet Methods for Boundary Integral Equations","DMS","COMPUTATIONAL MATHEMATICS","09/15/2001","10/22/2001","Yuesheng Xu","WV","West Virginia University Research Corporation","Standard Grant","Thomas W. Fogwell","06/30/2002","$47,196.00","Charles Micchelli","y1xu@odu.edu","886 CHESTNUT RIDGE ROAD","MORGANTOWN","WV","265052742","3042933998","MPS","1271","0000, 9263, OTHR","$0.00",""
"0101387","Collaborative Research:  Mathematical and Computational Methods in High Data-Rate Optical Fiber Communications","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/01/2001","07/30/2001","Curtis Menyuk","MD","University of Maryland Baltimore County","Standard Grant","Henry Warchall","07/31/2005","$322,109.00","","menyuk@umbc.edu","1000 HILLTOP CIR","BALTIMORE","MD","212500001","4104553140","MPS","1266, 1271","0000, 1616, 9263, OTHR","$0.00","NSF Award Abstract - DMS-0101387 <br/><br/>Mathematical Sciences: FRG: Mathematical and Computational Methods for High-Data-Rate Optical Fiber Communications    <br/><br/>Abstract<br/>DMS-0101387 <br/>Menyuk   <br/><br/><br/>The goal of this research project is to develop new methods that can be used to determine the behavior of optical transmission systems under realistic circumstances.  This will be accomplished by a combination of various techniques.  One approach will exploit the mathematical structure of fiber transmission models in order to eliminate unessential degrees of freedom.  The reduced models that will result will be more tractable mathematically and also much more computationally efficient.  Another approach that will be used is the application of linearization and importance sampling techniques to enable the simulation of systems at realistic data error rates.  These methods will be combined to study the main sources of impairment in optical fibers in order to achieve an accurate evaluation of system performance.  All the techniques to be developed will be carefully validated by comparison to more computationally time-consuming models and to experiments.  <br/><br/>The development of high-data-rate optical fiber communications is one of the great technological achievements of the late 20th century; in the last decade alone, data rates have increased by four orders of magnitude.  This enormous increase has made possible the growth of the global Internet that promises to continue to revolutionize day-to-day communications.  Because demand for further growth continues unabated, however, system capacity is becoming limited by fiber transmission effects.  It has therefore become crucial to accurately model and calculate the impairments due to non-ideal fiber properties when designing systems.  Due to the tremendous data capacity that will be required of future transmission systems (terabits per second of aggregate capacity) and the need for extremely small transmission error rates (less than one error per trillion bits), realistic attempts to model and predict the effects of these impairments as they appear in practical systems present a number of difficult mathematical and computational challenges.  The techniques that will be developed in this collaborative research project are expected to yield large reductions in the computational time required to model optical communication systems, and at the same time produce new insights into system behavior.  Because these methods will be capable of providing detailed information about system performance at realistic data error rates, we believe they will lead to significant changes in the way in which optical transmission systems are modeled, and, ultimately, in the way that they are built.   <br/>"
"0104282","Second-order Cone Programming : Algorithms and Applications","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/01/2001","05/14/2003","Donald Goldfarb","NY","Columbia University","Continuing Grant","Henry Warchall","07/31/2005","$240,018.00","Garud Iyengar","goldfarb@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1266, 1271","0000, OTHR","$0.00","Large Second-order Cone Programming: Algorithms and <br/>Applications.<br/><br/>PI.  Donald Goldfarb and Garud Iyengar.<br/><br/>Abstract. <br/><br/>Second-order cone programs (SOCPs) are convex optimization <br/>problems in which a linear function is optimized over the <br/>intersection of an affine linear manifold with the Cartesian <br/>product of second-order (Lorentz) cones.  Linear programs (LPs), <br/>convex quadratic programs (QPs),  and quadratically constrained  <br/>convex quadratic programs can all be formulated as SOCPs, as <br/>can many  other problems that do not fall into these three problem <br/>classes. On the other hand, since a second-order cone constraint <br/>is equivalent to a linear matrix inequality, semidefinite <br/>programs (SDPs) include SOCPs as a special case.  Computationally <br/>speaking, an SOCP falls  between an LP or a QP and an SDP. Interior <br/>point methods solve all of these problems in polynomial time. <br/>Although, the  computational effort required to solve an SOCP is <br/>greater than that required to solve an LP or a QP, it is <br/>substantially less than that required to solve an SDP of similar <br/>size and structure. However, in many ways, an SOCP is closer to <br/>an SDP than an LP or QP since its feasible set is non-polyhedral. <br/>The proposed research focuses on several aspects of  SOCPs, <br/>including the development of numerically stable algorithms for <br/>SOCPs that take advantage of sparsity in the data, the study of <br/>SOCP-based approximation algorithms for hard combinatorial optimization <br/>problems, the study of computational aspects of cut generation methods <br/>for mixed 0-1 SOCPs and the applications of SOCPs in robust financial <br/>optimization. <br/><br/>SOCPs are excellent models for applications that arise in a broad range <br/>of fields from  engineering, control, and finance, to robust and <br/>combinatorial optimization. The wide applicability of SOCPs and the <br/>need for efficient, numerically stable algorithms to solve them makes <br/>their study worthwhile.  <br/>"
"0196356","Fast, High Order Vortex Methods Based on Deforming Basis Functions","DMS","COMPUTATIONAL MATHEMATICS","01/15/2001","06/08/2001","Louis Rossi","DE","University of Delaware","Standard Grant","Michael Steuerwalt","12/31/2002","$38,726.00","","rossi@math.udel.edu","220 HULLIHEN HALL","NEWARK","DE","197160099","3028312136","MPS","1271","0000, OTHR","$0.00",""
"0112653","ITR/AP:  Collaborative Research:  Sampling Methods for Optimization and Control of Subsurface Contamination","DMS","COMPUTATIONAL MATHEMATICS, ITR SMALL GRANTS","10/01/2001","08/30/2001","Cass Miller","NC","University of North Carolina at Chapel Hill","Standard Grant","Junping Wang","09/30/2004","$166,667.00","","casey_miller@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1271, 1686","0000, 9263, OTHR","$0.00","The objectives of this project are (1) to simultaneously design optimization methods and subsurface flow and transport simulators for a variety of subsurface contaminant remediation problems, (2) to implement the optimization-simulation combination in a way that exploits parallelism in both the optimization and simulation, including some ideas for grid-based implementations of search-poll type methods, (3) to apply the new algorithms to two demonstration problems, (4) to train a generation of students in this multidisciplinary setting.  Two of the PIs, Dennis and Kelley, have expertise in a wide range of optimization technology, both gradient-based methods for smooth problems and sampling methods for problems for which gradients are not available.  Kelley and Miller have a long-standing collaboration in numerical methods for simulation of multi-phase flow.  The PIs will exploit this expertise and design the simulators and optimization algorithms simultaneously.  There will be IT-centered activity in software.  Two codes, FOCUS and IFFCO, are under development by the principal investigators.  The most mature of these, IFFCO an implicit filtering optimization code, will be ported to MPI from PVM, packaged for ease of installation and testing, documented as a book, and released in final form.  The PIs will also continue to develop FOCUS, an implementation of the surrogate management framework ideas.<br/><br/>Remediation of subsurface contamination is an important issue world-wise. The goal is to clean up, or render immobile, underground contaminants such as fuel spills from underground tanks or industrial waste from leaking drums, and do this at minimum cost.  Management decisions must be made, and the potential financial penalty of bad decisions is great.  Mathematical models of subsurface systems are often used to assist in making such decisions and these models are increasingly linked with optimization approaches to aid design and management decision making.  Subsurface remediation strategies involve decisions such as the restoration approach to use, the selection of locations to install devices, such as wells, and the rates of extraction from or injection into these wells.  There are open scientific problems in simulation related to fundamental modeling and choice of numerical methods that need to be addressed as part of the design of software to optimize the remediation methods.  The three PIs have expertise in simulation, modeling, and experimental work in multi-phase flows as well as broad expertise in optimization methods.  The objectives of this project are to design models, simulators, and optimization methods simultaneously with the goal of more efficient optimal design based on more accurate models."
"0107841","Collaborative Research on Quadrature and Orthogonal Polynomials in Large Scale Computation","DMS","COMPUTATIONAL MATHEMATICS","09/15/2001","08/31/2001","Daniela Calvetti","OH","Case Western Reserve University","Standard Grant","Junping Wang","08/31/2005","$96,000.00","","dxc57@case.edu","10900 EUCLID AVE","CLEVELAND","OH","441061712","2163684510","MPS","1271","0000, 9263, OTHR","$0.00","The inexpensive computation of upper and lower bounds for functionals of large, possibly sparse, symmetric matrices has received a lot of attention in the last few years. This proposal is concerned with new methods and new applications, and discusses extensions that allow the matrices to be nonsymmetric. The computation of upper and lower bounds for matrix functionals is based on the evaluation of pairs of Gauss-type quadrature rules. The outlined work proposes to study new quadrature rules of Gauss-type with properties which make them suitable for estimating matrix functional of nonsymmetric matrices. The measure associated with these quadrature rules may be indefinite or complex valued. Applications of these quadrature rules to the estimation of the norm of the error in the approximate solutions determined by iterative methods for linear systems of equations with nonsymmetric matrices will be pursued. Furthermore, applications to the iterative solutions of nonlinear problems will also be studied.<br/><br/> An important aspect of scientific computations addresses the reliability of the results. In particular, it is important to know the accuracy, measured by the error, of a computed result. One class of problems ubiquitous in scientific computing is the solution of large systems of algebraic equations. Since the solution of these kinds are equations is so widespread, they represent a class of problems for which knowledge of the numerical accuracy of the results is of great importance. This project addresses the issue by developing theory for computing the upper and lower bounds for certain measures of a system of equations. One particular application is to get the upper and lower bounds on the accuracy of approximate solutions of large systems of equations."
"0112542","ITR/AP:  Collaborative Research: Sampling Methods for Optimization and Control of Subsurface Contamination","DMS","COMPUTATIONAL MATHEMATICS, ITR SMALL GRANTS","10/01/2001","08/30/2001","Carl Kelley","NC","North Carolina State University","Standard Grant","Junping Wang","09/30/2005","$166,666.00","","tim_kelley@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1271, 1686","0000, 9263, OTHR","$0.00","The objectives of this project are (1) to simultaneously design optimization methods and subsurface flow and transport simulators for a variety of subsurface contaminant remediation problems, (2) to implement the optimization-simulation combination in a way that exploits parallelism in both the optimization and simulation, including some ideas for grid-based implementations of search-poll type methods, (3) to apply the new algorithms to two demonstration problems, (4) to train a generation of students in this multidisciplinary setting.  Two of the PIs, Dennis and Kelley, have expertise in a wide range of optimization technology, both gradient-based methods for smooth problems and sampling methods for problems for which gradients are not available.  Kelley and Miller have a long-standing collaboration in numerical methods for simulation of multi-phase flow.  The PIs will exploit this expertise and design the simulators and optimization algorithms simultaneously.  There will be IT-centered activity in software.  Two codes, FOCUS and IFFCO, are under development by the principal investigators.  The most mature of these, IFFCO an implicit filtering optimization code, will be ported to MPI from PVM, packaged for ease of installation and testing, documented as a book, and released in final form.  The PIs will also continue to develop FOCUS, an implementation of the surrogate management framework ideas.<br/><br/>Remediation of subsurface contamination is an important issue world-wise. The goal is to clean up, or render immobile, underground contaminants such as fuel spills from underground tanks or industrial waste from leaking drums, and do this at minimum cost.  Management decisions must be made, and the potential financial penalty of bad decisions is great.  Mathematical models of subsurface systems are often used to assist in making such decisions and these models are increasingly linked with optimization approaches to aid design and management decision making.  Subsurface remediation strategies involve decisions such as the restoration approach to use, the selection of locations to install devices, such as wells, and the rates of extraction from or injection into these wells.  There are open scientific problems in simulation related to fundamental modeling and choice of numerical methods that need to be addressed as part of the design of software to optimize the remediation methods.  The three PIs have expertise in simulation, modeling, and experimental work in multi-phase flows as well as broad expertise in optimization methods.  The objectives of this project are to design models, simulators, and optimization methods simultaneously with the goal of more efficient optimal design based on more accurate models."
"0113852","ITR/AP(DMS):  Semidefinite Programming for Electronic Structure","DMS","COMPUTATIONAL MATHEMATICS, ITR SMALL GRANTS","09/01/2001","12/02/2004","Bastiaan Braams","NY","New York University","Standard Grant","Junping Wang","08/31/2005","$360,107.00","Michael Overton","bbraams@emory.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1271, 1686","0000, 9263, OTHR","$0.00","This project aims to advance the state of the art in fast iterative algorithms for large-scale semidefinite programming and eigenvalue optimization, guided by applications to first-principles computation of properties of systems of many electrons.  Semidefinite optimization codes will be rewritten to replace dense linear algebra and direct matrix factorizations by iterative solution processes suited to large and sparse problems.  Numerical experiments will be carried out with different mathematical formulations and implementations of the sparse optimization procedures.  The guiding application is a variational formulation of the electronic structure ground state problem in which the unknowns are the one-body and two-body reduced density matrices of a many electron system.  This application gives rise to a large semidefinite program having sparse solution matrices and an extremely sparse constraint set.  The codes that are developed under this grant will be written and documented with the intent of wide dissemination.<br/><br/>Semidefinite programming is a valuable framework for many scientific and engineering applications, including systems control, structural analysis, combinatorial optimization, statistical estimation and VLSI design, just to name a few well-established ones besides electronic structure theory.  This project aims to increase our ability to compute solutions to problem instances whose size places them beyond the reach of current numerical methods.  As to the specific application in this project, the computation of properties of many-body systems on the basis of quantum mechanics is fundamental to the understanding of molecular and solid state structure, chemical and elementary biochemical processes, and mechanical and electromagnetic properties of condensed matter.  Potential benefits to technology and society from advances in the accuracy and scale of many-body quantum mechanical computations include, for example, better design of semiconductors, magnetic storage materials, high-temperature superconductors and chemical catalysts, as well as improvements in the emerging areas of rational drug design and the design of carbon nanostructures. <br/>"
"0074276","Mathematical Analysis of the Compositional Structure of Images","DMS","GEOMETRIC ANALYSIS, STATISTICS, COMPUTATIONAL MATHEMATICS, ROBOTICS","07/01/2001","07/08/2003","David Mumford","RI","Brown University","Continuing Grant","Junping Wang","06/30/2006","$1,830,000.00","Ulf Grenander, Stuart Geman, Basilis Gidas, Donald McClure","mumford@dam.brown.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1265, 1269, 1271, 6840","0000, 9263, OTHR","$0.00","The investigator and his colleagues study mathematical, statistical, and computational questions motivated by the problem in computer vision of defining and computing ""structural scene description,"" descriptions of the objects in a scene and their relations to each other.  The first goal is to obtain a theoretical understanding of the problem of inferring objects and their compositional structure, both in standard optical images and in laser range imagery and motion images.  The second goal is to test this understanding by developing effective algorithmsfor the statistical inference of this structure, algorithms that work with real data at reasonable speed on current computers.  The general approach is to formulate mathematical representations of patterns, typically compositional with a hierarchy of structures, to encode the variability of these structures in stochastic models, and to use the models to infer information about the image.  Standard stochastic models are rarely adequate, prompting the development of new classes of probability measures or completely new directions for traditional models.  Additionally, in the context of this application the team aims to develop mathematical, statistical, and computational ideas that are of broader use.  For example, compositional issues in vision are similar to ones in the grammars of language.  One aspect of the project studies this connection.<br/><br/>The investigator and his five colleagues continue their mathematical, statistical, and computational investigations of a range of problems motivated by image processing and computer vision.  The underlying question is simple enough: Here's an image, what is it an image of? Despite its simplicity, this is a hard question to answer.  The approach taken here is to decompose the image into different components in some hierarchical structure and to use statistical analysis to infer information about the image from the relationships of the components within the structure.  There are similarities with the grammatical structure of language, which the project explores.  Recognition of objects in an image is a fundamental problem for both computer systems and biological systems.  Advances are important for engineers developing computer vision applications, computer scientists seeking efficient algorithms in problems related to intelligent behavior of machines, and cognitive scientists studying human vision and language skills.  Results of the project are published on CD-ROM, allowing demonstration of the dynamic behavior of algorithms in ways impossible through traditional publication modes and offering new ways to exploit multi-media communications for both education and research purposes.<br/>"
"0107858","Collaborative Research on Quadrature and Orthogonal Polynomials in Large-Scale Computation","DMS","COMPUTATIONAL MATHEMATICS","09/15/2001","10/22/2004","Lothar Reichel","OH","Kent State University","Standard Grant","Leland Jameson","08/31/2005","$162,000.00","","reichel@math.kent.edu","OFFICE OF THE COMPTROLLER","KENT","OH","442420001","3306722070","MPS","1271","0000, 9263, OTHR","$0.00","The inexpensive computation of upper and lower bounds for functionals of large, possibly sparse, symmetric matrices has received a lot of attention in the last few years. This proposal is concerned with new methods and new applications, and discusses extensions that allow the matrices to be nonsymmetric. The computation of upper and lower bounds for matrix functionals is based on the evaluation of pairs of Gauss-type quadrature rules. The outlined work proposes to study new quadrature rules of Gauss-type with properties which make them suitable for estimating matrix functional of nonsymmetric matrices. The measure associated with these quadrature rules may be indefinite or complex valued. Applications of these quadrature rules to the estimation of the norm of the error in the approximate solutions determined by iterative methods for linear systems of equations with nonsymmetric matrices will be pursued. Furthermore, applications to the iterative solutions of nonlinear problems will also be studied.<br/><br/> An important aspect of scientific computations addresses the reliability of the results. In particular, it is important to know the accuracy, measured by the error, of a computed result. One class of problems ubiquitous in scientific computing is the solution of large systems of algebraic equations. Since the solution of these kinds are equations is so widespread, they represent a class of problems for which knowledge of the numerical accuracy of the results is of great importance. This project addresses the issue by developing theory for computing the upper and lower bounds for certain measures of a system of equations. One particular application is to get the upper and lower bounds on the accuracy of approximate solutions of large systems of equations."
"0107146","Temporally Uniform Grid Convergence of Discrete Approximations and Numerical Simulations in the Problems of Wave Propagation over Unbounded Domains","DMS","COMPUTATIONAL MATHEMATICS","08/15/2001","08/20/2001","Semyon Tsynkov","NC","North Carolina State University","Standard Grant","Junping Wang","01/31/2005","$94,989.00","","tsynkov@math.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1271","0000, 9263, OTHR","$0.00","Two major well-recognized difficulties in numerical simulation of waves<br/>propagating over unbounded domains are the accumulation of error during <br/>long time intervals and necessity to truncate the domain and subsequently <br/>set the artificial boundary conditions (ABCs) at the external artificial<br/>boundary. These two issues turn out to be closely related. In the previous<br/>work of the PI with collaborators that has been done in the framework of<br/>the scalar wave equation, we have used the inherently three-dimensional <br/>phenomenon of lacunae and developed a methodology that modifies any <br/>appropriate discrete scheme so that the long-term error buildup is fully <br/>eliminated while all of the original properties of the scheme (e.g., order <br/>of accuracy) are preserved. Moreover, the procedure allows one to replace <br/>the original infinite domain by a finite computational domain, which leads <br/>to obtaining highly accurate non-local unsteady ABCs. These ABCs are built <br/>directly for the discrete formulation of the problem, their temporal non-<br/>locality is fixed and limited, and they possess full geometric universality. <br/>The key objective of the proposed project is to extend the aforementioned <br/>methodology to wave-type models of practical interest, in particular, the <br/>Maxwell's equations (electromagnetic waves) and the linearized Euler's and <br/>full-potential equations (acoustic waves). The attainability of this goal <br/>is accounted for by the fact that the solutions to these equations have <br/>sharp aft fronts of the waves (manifestation of lacunae), which is the <br/>exact same behavior as displayed by the solutions to the wave equation.<br/><br/>Numerical simulation of waves on unbounded domains has numerous applications <br/>that range from scattering of electromagnetic waves from aircraft and ground <br/>vehicles (radar technology) to antenna design to calculation of the acoustic<br/>fields produced by the airframe and airplane's jet engines for the purpose of <br/>reducing the noise levels around airports, as well as inside the passenger <br/>compartments. The results that we expect to obtain are going to benefit the <br/>foregoing applied areas. Indeed, we anticipate the creation of a universal <br/>framework that would allow one to modify a wide class of already existing and <br/>proven methods so that to enable two additional crucial capabilities -- long-<br/>term integration and accurate computation of infinite-domain wave fields on <br/>truncated domains. Besides, as the proposed research unfolds, it will <br/>necessarily include communication and collaboration with physicists and <br/>engineers, as well as preparation of course materials and training young <br/>researchers.<br/>"
"0107761","Numerical Modeling for Liquid Crystal Devices","DMS","COMPUTATIONAL MATHEMATICS","07/15/2001","07/10/2001","Eugene Gartland","OH","Kent State University","Standard Grant","Junping Wang","08/31/2004","$171,151.00","","gartland@math.kent.edu","OFFICE OF THE COMPTROLLER","KENT","OH","442420001","3306722070","MPS","1271","0000, 9263, OTHR","$0.00","<br/>Numerical modeling tools have been used in the liquid crystal arena for several years.  They are typically used interactively to explore effects and to optimize the performance of devices with respect to controllable material and device parameters.  Until recently, satisfactory results could be achieved by using one-dimensional (1-D) models, which treat the transverse dimensions of the structures as infinite and uniform.  Current developments (smaller feature sizes, new technologies) are forcing more realistic 2-D and 3-D modeling. The numerical algorithms utilized in current modeling tools require several minutes on contemporary platforms to analyze a 2-D device; for 3-D, they take several hours.  Significant improvements in the numerical approaches are needed to produce acceptable performance in these higher dimensional settings. The principal investigator will work with device-modeling physicists and experts in the applied mathematics and numerical analysis communities to adapt and develop the advanced numerical methods necessary to model realistic optical devices based upon liquid crystal materials.  Modern numerical algorithms hold the promise of enabling 3-D modeling in times that are comparable to the current generation of 1-D tools (seconds).  These tools are expected to have a significant impact on the development of liquid crystal optics technologies.<br/><br/>This project is concerned with the development and analysis of numerical modeling tools for devices based on liquid crystal materials.  Liquid crystals are important in technology because they can be used to control light.  Numerous applications exist (e.g., lap-top and flat-panel displays), and new ones are being developed, including tablet document readers (e-books, electronic newspapers) and beam-steering devices (for soft, programmable, non-mechanical applications in communications and switching). Modern numerical algorithms hold the promise of enabling full modeling of liquid crystal devices in a matter of seconds.  Such tools will be of immense practical utility and can be expected to have a big impact in the development of technologies in this and related areas.<br/>"
"0128285","An International Conference: Computational Mathematics","DMS","COMPUTATIONAL MATHEMATICS","10/01/2001","09/18/2001","Mikhael Gromov","","Institut des Hautes Etudes Scientifques","Standard Grant","Junping Wang","09/30/2002","$41,911.00","","gromov@cims.nyu.edu","Le Bois-Marie,","Bures-sur-Yvette","","91440","0160926670","MPS","1271","0000, 9263, OTHR","$0.00","     The investigator organizes an international conference at<br/>Institut des Hautes Etudes Scientifiques to study folding and<br/>self-assembly of macromolecules.  Self-organization of<br/>macromolecules is a fundamental question in molecular biology.<br/>How molecules fold plays a major role in their organization.  The<br/>meeting fosters the creation of new tools by bringing together<br/>scientists from several domains to consider the problem from<br/>different perspectives.<br/>     DNA is like a long zipper whose teeth are pairs of<br/>molecules, called base pairs, one molecule on each side of the<br/>zipper.  Links between the base pairs close the zipper.  The zipper<br/>is about six feet long in human DNA; to fit inside a cell, it<br/>coils up.  It partially uncoils to expose sections --- genes ---<br/>that are read to produce an intermediate molecule, called<br/>messenger RNA.  Ribosomes read the messenger RNA and produce a<br/>corresponding amino acid chain.  The chain folds up spontaneously;<br/>this is a protein.  Proteins in turn perform their functions by<br/>partially unfolding to interact with other folded macromolecules.<br/>Proteins, genes, and DNA all fold and unfold to do their work.  So<br/>folding and self-assembly are central issues in molecular<br/>biology.  A goal of the conference is to stimulate synergistic<br/>interchanges between different disciplines by bringing together<br/>biologists, chemists, physicists, mathematicians, and computer<br/>scientists to discuss these issues from different perspectives."
"0101324","Collaborative Research: Computational Conformal Mapping and Scientific Visualization","DMS","GEOMETRIC ANALYSIS, COMPUTATIONAL MATHEMATICS","09/15/2001","02/01/2006","Kenneth Stephenson","TN","University of Tennessee Knoxville","Standard Grant","Junping Wang","08/31/2006","$410,000.00","Charles Collins","KENS@MATH.UTK.EDU","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","MPS","1265, 1271","0000, 1616, 9263, OTHR","$0.00","     This Focused Research Group is composed of pure<br/>mathematicians, computational mathematicians, and<br/>neuroscientists.  They develop implementations of discrete<br/>conformal mapping for multidisciplinary use, both within<br/>mathematics itself where complex analysis is being reinvigorated<br/>by new discrete techniques, and in the larger scientific context<br/>with visualization and analysis of scientific data.  The Riemann<br/>Mapping Theorem guarantees unique conformal maps between any pair<br/>of conformal 2-discs (or conformal 2-spheres); the conformal<br/>geometry preserved by such maps carries valuable mathematical<br/>structure.  Such surfaces arise naturally in many scientific<br/>contexts as piecewise flat (from data) or smoothly embedded (from<br/>theory) surfaces in 3-space.  Recently the new computational<br/>technique of circle packing has allowed computational<br/>approximations to these conformal maps.  Implementing such<br/>approximations for large scientific datasets faces both<br/>theoretical and computational challenges.  The investigator and<br/>his colleagues work on three related topics: theoretical<br/>superstructure of the circle packing technique, refinement and<br/>parallelization of the circle packing algorithm for use on large<br/>datasets, and the application of these conformal maps to<br/>visualization and analysis of scientific data.  The main<br/>application focuses on conformal flattening of human brain<br/>cortical surfaces.  The investigators use uniqueness of conformal<br/>maps to install surface-based coordinate systems on these<br/>surfaces; these coordinate systems allow localization of<br/>activation foci in Positron Emission Tomography (PET) and<br/>functional Magnetic Resonance Imaging (fMRI) brain scans.<br/>Conformal flattening has wider applicability as a visualization<br/>and graph embedding technique, and these connections inform the<br/>research.<br/>     This Focused Research Group develops algorithms to bring a<br/>classical mathematics theorem (the Riemann Mapping Theorem, 1854)<br/>to bear on problems of visualization of data.  The Riemann Mapping<br/>Theorem guarantees the existence of unique conformal<br/>(angle-preserving) maps between surfaces, but does say how to<br/>compute these maps.  Modern computers and new algorithms have<br/>changed all that, because our new computational ability can<br/>breathe life into classical existence theorems of mathematics,<br/>turning theory into computational tools.  This project develops<br/>algorithms to implement the computation of conformal maps on<br/>complex spatial surfaces.  The main application is the flat<br/>mapping of human brain cortical surfaces.  The brain surface is<br/>highly convoluted and folded in space, and most of the brain<br/>surface is folded up and hidden from view.  If one flattens the<br/>surface, one can simultaneously see down into all the folds.  The<br/>mathematically unique conformal maps produced by the algorithms<br/>allow surface-based coordinate systems to be computed on the<br/>brain surface so that surface positions can be precisely<br/>determined.  Moreover, if one puts foci of functional activation<br/>onto the flattened surface, one can then visualize and measure<br/>the relationship between brain function and brain anatomy.  These<br/>new surface-mapping techniques and their application to the brain<br/>surface permit biomedical researchers and clinicians to rapidly<br/>and accurately map and compare the locations of physiological and<br/>pathological ""events"" in the brains of research subjects and of<br/>patients with a variety of neurological and psychiatric<br/>disorders.  The project is supported by the Computational<br/>Mathematics, Applied Mathematics, and Geometric Analysis programs<br/>and the Office of Multidisciplinary Activities in MPS and by the<br/>Computational Neuroscience program in BIO.<br/>"
"0104861","A Direct Reconstruction Algorithm for the 2-D Inverse Conductivity Problem","DMS","COMPUTATIONAL MATHEMATICS","09/01/2001","07/30/2004","Jennifer Mueller","CO","Colorado State University","Standard Grant","Junping Wang","02/28/2005","$84,983.00","","mueller@math.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1271","0000, 1271, 9263, OTHR","$0.00","This proposal addresses a direct numerical reconstruction algorithm for the 2-D inverse conductivity problem.  The 2-D inverse conductivity problem is to determine an unknown conductivity distribution on a bounded region  from knowledge of the Dirichlet-to-Neumann map.  Physically, knowledge of the Dirichlet-to-Neumann map is tantamount to knowing the current density distribution on the boundary of the region resulting from any given voltage distribution applied on the boundary.  The problem is modeled mathematically by the generalized Laplace's equation with the conductivity as an unknown parameter.  In 1995 A. Nachman proved that knowledge of the Dirichlet-to-Neumann  map uniquely determines the conductivity in the interior of a smooth bounded  region in 2-D.  An important feature of Nachman's proof is that it outlines a direct method for solving for the conductivity without iteration.  The proof is based on techniques of inverse scattering and the d-bar method, which is a method of solution for scattering problems, not a numerical technique.  The primary goals of this proposal are to solve the inverse conductivity problem numerically using the d-bar method, develop a practical reconstruction algorithm for medical applications, and to test the implementation on physically relevant conductivity distributions. <br/><br/>The 2-D inverse conductivity problem has applications in geophysics, nondestructive testing, and a medical imaging technique known as electrical impedance tomography (EIT).  One application of EIT is the imaging of heart and lung function in real time.  In this application, electrodes are placed around the circumference of the patient's torso, current is applied on the electrodes and the resulting voltage is measured.  The resulting 2-D inverse conductivity problem is then solved numerically to reconstruct how the electricity passes through the interior and to form a cross-sectional image of the patient's chest.  Other applications include the detection of breast cancer, monitoring for internal bleeding, and the diagnosis of pulmonary embolis (a blood clot in the lung).  The proposed approach represents a new class of image reconstruction algorithm for the EIT problem.  Work thus far has indicated that the algorithm yields more accurate images than the existing fast algorithms, since it solves the full set of equations rather than a a more simplified version of the problem.  This is particularly important in medical applications such as breast cancer detection, where the measured values distinguish between the presence of a tumor or a benign cyst.  The algorithm will be tested on real data and compared to existing algorithnms in terms of accuracy and efficiency."
"0107242","Auto Validating Methods in Dynamical Systems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2001","08/28/2001","Warwick Tucker","NY","Cornell University","Standard Grant","Junping Wang","08/31/2004","$67,119.00","","warwick@math.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1271","0000, 9263, OTHR","$0.00","Tucker<br/>0107242<br/>     The investigator studies the effects of errors imposed by<br/>numerical computations carried out in most applied environments<br/>(e.g. commercial computers).  He focuses on rigorous control of<br/>errors in the numerical solution of systems of ordinary<br/>differential equations, particularly chaotic dynamical systems.<br/>The goal is to provide accurate estimates of the errors of the<br/>actual calculations, and to introduce an accurate means of<br/>dealing with rounding/truncation errors.<br/>     Auto-validating methods for the numerical solution of<br/>mathematical problems or of mathematical models of science and<br/>engineering problems, especially chaotic dynamical systems, is<br/>becoming increasingly important.  Rigorous error estimates,<br/>valuable for any computational model, are especially important in<br/>chaotic systems, which arise in modeling weather, neural<br/>networks, and so on.  The investigator <br/>"
"0107179","RUI: Group Testing for Complexes","DMS","COMPUTATIONAL MATHEMATICS, CCLI-EDUCATIONAL MATERIALS DEV","09/01/2001","08/06/2003","Anthony Macula","NY","SUNY College at Geneseo","Standard Grant","Junping Wang","08/31/2005","$94,248.00","","macula@geneseo.edu","1 College Circle","Geneseo","NY","144541401","5852455547","MPS","1271, 7427","0000, 9178, 9229, 9251, 9263, OTHR, SMET","$0.00","The screening of data sets is essential to modern technology.  <br/>Whenever the objective is to find ""positive elements"" in a <br/>data set, a test indicating whether at least one positive is <br/>an element of a specific part of the data set can greatly <br/>facilitate their isolation.  Such tests are called binary <br/>group tests and the general mathematical method behind the <br/>identification of the positives using such tests is known as <br/>classical group testing.  In many applied settings, the use <br/>of classical group testing to isolate objects that are <br/>individually positive has become standard experimental <br/>procedure.  However, very little work has been done in <br/>applying group testing techniques to the identification of <br/>objects that are collectively positive.  Let C be an unknown <br/>collection of subsets or complexes in a population and let P <br/>be a pool taken from the population.  A pool P is said to be <br/>positive if and only if a member of C is also a subset of P.  <br/>The identification of C by the application of these binary <br/>tests is called group testing for complexes.<br/><br/>The primary aim of this proposal is the development of efficient <br/>group testing methods that lead to the identification of <br/>positive combinations of objects, namely the positive complexes.  <br/>There are many scientific and technical areas where the <br/>identification of positive combinations of objects is important. <br/>Computationally feasible methods of finding combinations of <br/>entities that produce some measurable outcome or are measurably <br/>linked to some function or disfunction would have important <br/>applications to medical genetics, computer security, software <br/>testing, data mining, communications, and marketing.<br/><br/>"
"0107495","Numerical Methods for Layered Models of Ocean Circulation","DMS","COMPUTATIONAL MATHEMATICS","09/15/2001","09/21/2001","Robert Higdon","OR","Oregon State University","Standard Grant","Junping Wang","08/31/2005","$140,000.00","","higdon@math.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","MPS","1271","0000, 9263, OTHR","$0.00","The goal of this project is to develop numerical methods for layered (isopycnal) models of the general ocean circulation.  In such models, the vertical coordinate is not linear distance, but instead is potential density or some other related quantity.  When an isopycnal model is discretized in the vertical, the effect is to represent the ocean as a stack of layers that are approximately immiscible.  This corresponds to the observation that the upper and lower regions of the ocean are approximately isolated thermodynamically from each other.  However, this isolation is not complete, and in an isopycnal model the exchanges that do occur are under the control of the modeler.  This type of model shows great promise for representing the long-term behavior of ocean dynamics.  In the present project, the principal investigator will develop and incorporate methods for solving the momentum equation in a flux form for which momentum, not velocity, is the dependent variable.  It appears that the momentum formulation may give more reliable results, such as in situations where layer thicknesses tend to zero because of layer interfaces intersecting the top or bottom of the fluid domain.  This approach may also be valuable in modeling vertical transport in the ocean.  Non-oscillatory or nearly non-oscillatory advection algorithms will be used to solve the momentum equation.  Such methods will be analyzed and incorporated into a two-level time-stepping algorithm that the investigator has recently developed for formulations of the governing equations in which the fast and slow time scales are split into separate subsystems.  The resulting algorithms will be tested on model problems for which the solution behavior is known.  As time permits, the PI will then pursue the implementation and testing of these methods in existing, operational models of ocean circulation.  During this project the PI will be in regular contact with ocean modelers at Oregon State University and at Los Alamos National Laboratory.<br/><br/>The long-term objective of this work is to contribute to an understanding of the global climate system.  An important tool towards understanding this system consists of computer simulations involving coupled models of the atmosphere, ocean, sea ice, and terrestrial effects.  With such simulations one can test the response of the earth's climate to changes in external forcing, such as increased emissions of greenhouse gases.  The world's oceans store and transport tremendous amounts of heat energy, so the oceans play a crucial role in our climate system.  When the circulation of the ocean is simulated on a computer, one is using the computer to obtain approximate solutions to mathematical equations that describe the evolution of fluid flows.  The goal of the present project is to improve the methods that are used to solve those equations."
"0107628","Polyhedral Approximation and Other Computational Aspects of Geometric Problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2001","08/28/2001","Mario Lopez","CO","University of Denver","Standard Grant","Junping Wang","08/31/2006","$158,000.00","","mlopez@cs.du.edu","2199 S. University Blvd.","Denver","CO","802104711","3038712000","MPS","1271","0000, 9178, 9251, 9263, OTHR, SMET","$0.00","    The investigator and his colleague study computational<br/>aspects of various geometric problems in two directions: (1)<br/>Design and implementation of efficient algorithms for the<br/>approximation of various convex and nonconvex objects in<br/>multidimensional space by polytopes, and research of the<br/>computational complexity as well as practical efficiency of these<br/>algorithms. (2) Application of high performance computing as a<br/>tool to solve or advance toward a solution of open theoretical<br/>problems in convex geometry, among them, Kneser's problem<br/>concerning the relationship between volumes of intersections or<br/>unions of balls in multidimensional Euclidean space and their<br/>mutual distances.  Because of its practical importance in many<br/>application areas, the approximation of both convex and nonconvex<br/>polytopes by ""simpler"" polytopes is given special attention, and<br/>fully constructive solutions are developed for these cases.<br/>Extensions and variants such as approximation under various<br/>metrics, requiring the approximating object to enclose or be<br/>contained within the approximated object, and finding minimal<br/>enclosing polytopes of a specific type (like parallelotopes, for<br/>example), are also considered.  Some of these variants find<br/>important applications in mobile computing and multidimensional<br/>databases.<br/>     The investigators develop efficient computational solutions<br/>for the problem of approximating multidimensional bodies by<br/>polytopes (solids formed by flat faces) of a prescribed size.<br/>Such approximation is an important tool in many disciplines,<br/>including molecular modeling, optimal control, computer-aided<br/>design, and computer visualization.  They also investigate the<br/>problem of enclosing and approximating multidimensional bodies by<br/>polytopes of a prescribed type, such as ""boxes"" (parallelotopes),<br/>for example.  Solutions to these problems find important<br/>applications in the rapidly growing areas of mobile computing and<br/>multidimensional databases.  Furthermore, due to their simplicity,<br/>polytopes are by far the most widely used form of model<br/>representation.  Thus, the work is also important because it<br/>facilitates the use of the large body of methods already<br/>available for polytopes, provided that the resulting<br/>approximation is good and can be performed efficiently.<br/>Development of these algorithms produces tools of high<br/>performance computing.  The investigators use these tools in turn<br/>to study long-standing geometric problems."
"0107419","Collaborative Research: High Order Numerical Schemes for Multi-Dimensional Systems of Conservation Laws and for Simulations of Multi-Phase Fluids","DMS","COMPUTATIONAL MATHEMATICS","08/15/2001","08/20/2001","Xu-Dong Liu","CA","University of California-Santa Barbara","Standard Grant","Junping Wang","07/31/2004","$129,065.00","","xliu@math.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","MPS","1271","0000, 9263, OTHR","$0.00","The main theme of the proposed project is the construction of high order accurate numerical schemes for solving multi-dimensional hyperbolic systems of conservation laws, and in particular the construction of numerical schemes for simulations of multi-phase fluid flows.  This includes numerical methods for compressible flow, incompressible flow and heat transfer.  Recently, the PI's introduced a boundary condition capturing method for variable coefficient Poisson equation in the presence of interfaces.  The method is implemented using a standard finite difference discretization on a Cartesian grid making it simple to apply in several spatial dimensions.  Furthermore, the resulting linear system is symmetric positive definite allowing for straightforward application of standard ""black box"" solvers, for example, multi-grid methods.  Most importantly, this new method does not suffer from the numerical smearing.  Using this method, the PI's extended the Ghost Fluid Method to treat two-phase incompressible flows, in particular those consisting of water and air.  The numerical experiments show that the new numerical method performs quite well in both two and three spatial dimensions.  Currently, they are working on extending this method to treat a wide range of problems, including for example combustion.  Of particular interest is the extension of this method to include interface motion governed by the Cahn-Hilliard equation which models the non-zero thickness interface with a molecular force balance model. <br/><br/>This proposed research on computational fluid dynamics is focused on the design, implementation and testing of new methods for simulating fluids such as water and gas using the computer.  In particular, this work addresses problems where more than one type of one phase of fluid exist, e.g. mixtures of water and air.  Our interest lies in improving the current state of the art algorithms so that they are better able to treat the interface that separates two fluids such as oil and water.  The results of this research should be of interest to both the military (e.g. many naval applications involve the study of water and air mixtures) and to private industry.  A particularly interesting example involves the interaction of water and oil in an underground oil recovery process.  The research covered in this proposal has implications for math and science education as well.  Not only will the PI's be working with and training graduate students in applied mathematics and engineering, but their research in extending these techniques to other fields, such as computer graphics, can play a role attracting the next generation of young scientists.  For example, figure 7 in ""Foster and Fedkiw, Practical Animation of Liquids, SIGGRAPH 2001"" shows the lovable character ""Shrek"", from the feature film of the same name, taking a bath in mud.<br/>"
"0107428","High Resolution Finite Difference and Spectral Algorithms for Piecewise Smooth Data","DMS","COMPUTATIONAL MATHEMATICS","09/01/2001","06/18/2003","Eitan Tadmor","CA","University of California-Los Angeles","Continuing Grant","Junping Wang","08/31/2004","$206,200.00","Anne Gelb","tadmor@cscamm.umd.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1271","0000, 9263, OTHR","$0.00","We plan to develop and implement new high resolution finite difference and spectral algorithms which reduce the spurious Gibbs effects near the internal edges of piecewise regular data, and recover with high resolution the underlying information in between those edges. These are precisely the issues which defy classical methods and  are of great research interest in various applications. Professors E. Tadmor (UCLA) and A. Gelb (ASU), will continue their ongoing cooperative research on the following. (i) Accurate realization of piecewise smooth data in one- and several space dimensions using edge detection and high-resolution reconstruction techniques. In this context we will develop, analyze and implement a spectrally accurate recovery procedures by combining localization based on appropriate concentration kernels which identify finitely many edges, followed by a novel two-parameter family of spectral mollifiers which recover the data between the edges with exponential accuracy. These techniques are at the heart of the modern high-resolution algorithms described below. (ii) Over the last decade, central schemes proved to be an extremely robust, all-purpose tool for solving general nonlinear convective-diffusive problems. We will integrate new recovery procedures and introduce further non-Cartesian enhancement procedures to the resolution of multidimensional central schemes. (iii) Further developments and applications of stable and spurious-free spectral viscosity algorithms. In particular, we plan to apply the new enhanced SV procedure to problems where piecewise smoothness forms due to mixing and instability (Richtmyer-Meshkov, Taylor, ...), simulations of the shallow water equations, and study of the critical threshold phenomena in mixed-type Euler-Poisson equations. <br/><br/>Look around you: edges are everywhere. Much of the data we encounter -- from images to signals is piecewise smooth, that is, it consists of smooth pieces separated by sharp internal edges. This project proposes to continue the development of novel methods that combat the typical difficulties associated with problems containing piecewise smooth data. Specifically, we propose to extend our ongoing study of dealing with the reconstruction of piecewise smooth data from its spectral information. Here, the large scales represented by the smooth 'pieces' are resolved by a variety of non-oscillatory reconstructions. The difficulty arises with the spurious oscillations due to unresolved small scales which are localized in the neighborhood of the edges. The problem is often realized in terms of the so called 'ringing phenomena'. To this end, the location of the edges is detected and information is treated in the 'direction of smoothness', i.e., away from the detected edges. Application include the high resolution recovery of piecewise smooth data obtained in such inverse applications as magnetic resonance imaging (MRI), positron emission tomography (PET), climatology data and more. Piecewise smooth data also arise in various time dependent problems, due to spontaneous breakup in waves patterns. In this case, the difficulties become even more intricate, as one has to trace  moving edges. We also plan to implement the new recovery procedures for tracing the evolution of breakdown of waves, and integrate these recovery procedures with central schemes and spectral viscosity methods -- two modern high resolution methods developed by us earlier."
"0196439","New High-Resolution Semi-Discrete Central Schemes:  Derivation, Applications and Local Error Analysis","DMS","COMPUTATIONAL MATHEMATICS","08/01/2001","08/06/2001","Alexander Kurganov","LA","Tulane University","Standard Grant","Junping Wang","07/31/2003","$33,280.00","","kurganov@math.tulane.edu","6823 ST CHARLES AVENUE","NEW ORLEANS","LA","701185698","5048654000","MPS","1271","0000, 9263, OTHR","$0.00",""
"0105653","Collaborative Research:  Software for Decomposing Solution Sets of Polynomial Systems","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS","08/01/2001","05/14/2003","Andrew Sommese","IN","University of Notre Dame","Continuing Grant","Junping Wang","07/31/2005","$179,965.00","","sommese@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","1264, 1271","0000, 9263, OTHR","$0.00","The project is on the development of efficient numerical algorithms and high quality software to solve systems of polynomials.  A basic problem is to decompose the positive dimensional subsets of solutions into irreducible components.  The numerical approach of Sommese, Verschelde, and Wampler is based on generic slicing with linear spaces, generic projections into lower dimensional linear spaces, and use of classical interpolation techniques to numerically do what elimination theory does in symbolic programs.  Given a polynomial system with parameters, a goal of the project is to find equations on the parameters that need to be satisfied for the system to have a positive dimensional component of solutions.  Two applications targeted by this project are factoring multivariate polynomials and finding overconstrained mechanisms.<br/><br/>A major outcome of this work will be publicly available software to solve polynomial systems that arise in science and engineering.  This work is carried out in the research fields of numerical analysis and computer algebra whose mission is to provide the scientific community with software to solve mathematical problems.  Since polynomial systems are used as models in application areas as far apart as chemical reaction systems, the design of mechanisms, or economic equilibria to name but a few areas, the focus of the project on such basic models as polynomial systems is appropriate.  Besides the technology transfer of advanced mathematical tools into science and engineering, an important aspect of this project is to introduce students in the design and use of the developed software.<br/>"
"0107492","Elliptic Inverse Problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2001","08/21/2002","Ian Knowles","AL","University of Alabama at Birmingham","Standard Grant","Junping Wang","08/31/2004","$95,158.00","","iknowles@uab.edu","1720 2nd Avenue South","Birmingham","AL","352940001","2059345266","MPS","1271","0000, 9263, OTHR","$0.00","This is a continuation of work on a new approach to parameter estimation inverse problems.  The coefficients in question are computed as the unique global minimum of certain non-negative functionals that also tend to have unique critical points, the latter property being of crucial importance if one seeks truly effective numerical algorithms.  The core of the idea involves the observation that the Dirichlet principle for self-adjoint elliptic equations can be reformulated to produce the coefficients in the equation, rather than the solution.  Recent work indicates that the techniques extend readily to parabolic and hyperbolic systems as well, which extends the range of applicability considerably.  As these methods confront the nonlinear inverse problems directly, they are computationally more expensive than algorithms wherein the problem is initially linearized.  On the other hand, when successful, the direct methods tend to give better images, free from the various artifact problems that surround the linearization methods.  The methods also exhibit remarkable stability and accuracy in the face of significant ill-posedness.  The proposal concentrates mainly on two generic cases, the (as yet unsolved) problem of the reconstruction of all the coefficient functions in the equations for groundwater flow and transport and the electrical impedance tomography problem.  These examples have been chosen in part to indicate the broad applicability of this circle of ideas.  The first is chosen as a representative of the class of problems in which measurements of the solution are available from inside the region, while the second is an example representative of the situation in which only boundary information on the solution is available.  An indication is also given on an extension to imaging undersea regions from reflection seismological data, and landmine detection using microwave impulse radar.  These methods may also have profound theoretical implications as well, in the direction of proving associated inverse problem uniqueness theorems.<br/><br/>Complex physical processes are often represented mathematically by systems of linear ordinary or partial differential equations.  A crucial part of this modeling process involves the determination of all of the coefficient functions in the equations modeling certain processes.  In many situations of practical interest it is, for various reasons, impractical to measure these functions directly.  In groundwater modeling, for example, one cannot easily measure most subsurface parameters, and in medical imaging, one is always trying to infer internal properties ``non-invasively.""  On the other hand, it is often true that one can make useful measurements of the effects of a particular physical process.  For example, in groundwater flow, one can measure the height (head) of water, over time, in a grid of wells, and in electrical impedance tomography, one can apply currents at the surface of a body and measure the resulting surface voltages.  Mathematically, in each of these examples one is given data on the solution of a underlying equation with the intent of using this data to estimate some or all of the parameter functions.  This is the essence of an inverse problem.  This project continues work on computational algorithms for inverse problems involving medical imaging, landmine detection, undersea seismic exploration, and groundwater modeling.  Expected benefits would include greatly improved image quality in low energy electrical tomography and the possibility of producing complete flow and contaminant models for use in the management and remediation of underground aquifers."
"0100403","Symbolic Computation and Combinatorics","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS","08/15/2001","04/19/2002","Doron Zeilberger","PA","Temple University","Continuing Grant","B. Brent Gordon","04/30/2002","$120,000.00","","zeilberg@math.rutgers.edu","1801 N. Broad Street","Philadelphia","PA","191226003","2157077547","MPS","1264, 1271","0000, 9263, OTHR","$0.00","Doron Zeilberger proposes to continue to develop methodologies for harnessing the great potential of Symbolic Computation to do research in Combinatorics and related areas.  In particular he hopes to introduce new computational and conceptual frameworks that would extend the so-called Wilf-Zeilberger proof theory to much wider classes of identities and theorems.  He also proposes to continue his efforts in `Artificial Combinatorics', and develop algorithms for the discovery and {\it rigorous} proof of theorems in combinatorics whose complexity make them unfeasible for human proofs.  This research should be symbiotic, as it is expected that both the concrete results and  the underlying methodologies, would help computer algebra developers to improve and enhance their systems.<br/><br/>This research is in the general area of Combinatorics.  One of the goals of  Combinatorics is to find efficient methods of studying how  discrete collections  of objects can be arranged.  The behavior of discrete systems is extremely  important to modern communications.  For example, the design of large networks, such as those occurring in  telephone systems, and the design of algorithms in  computer science deal with discrete sets of objects, and this makes use of combinatorial research.  This research is also in the general area of  Symbolic Computation, that attempts to teach computers to perform research that previously required extensive human resources.  Progress in this area promises to have important ramifications to science  and technology.<br/>"
"0113735","ITR/AP:  Collaborative Research: Sampling Methods for Optimization and Control of Subsurface Contamination","DMS","COMPUTATIONAL MATHEMATICS, ITR SMALL GRANTS","10/01/2001","08/30/2001","John Dennis","TX","William Marsh Rice University","Standard Grant","Junping Wang","09/30/2004","$166,666.00","","dennis@caam.rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271, 1686","0000, 9263, OTHR","$0.00","The objectives of this project are (1) to simultaneously design optimization methods and subsurface flow and transport simulators for a variety of subsurface contaminant remediation problems, (2) to implement the optimization-simulation combination in a way that exploits parallelism in both the optimization and simulation, including some ideas for grid-based implementations of search-poll type methods, (3) to apply the new algorithms to two demonstration problems, (4) to train a generation of students in this multidisciplinary setting.  Two of the PIs, Dennis and Kelley, have expertise in a wide range of optimization technology, both gradient-based methods for smooth problems and sampling methods for problems for which gradients are not available.  Kelley and Miller have a long-standing collaboration in numerical methods for simulation of multi-phase flow.  The PIs will exploit this expertise and design the simulators and optimization algorithms simultaneously.  There will be IT-centered activity in software.  Two codes, FOCUS and IFFCO, are under development by the principal investigators.  The most mature of these, IFFCO an implicit filtering optimization code, will be ported to MPI from PVM, packaged for ease of installation and testing, documented as a book, and released in final form.  The PIs will also continue to develop FOCUS, an implementation of the surrogate management framework ideas.<br/><br/>Remediation of subsurface contamination is an important issue world-wide. The goal is to clean up, or render immobile, underground contaminants such as fuel spills from underground tanks or industrial waste from leaking drums, and do this at minimum cost.  Management decisions must be made, and the potential financial penalty of bad decisions is great.  Mathematical models of subsurface systems are often used to assist in making such decisions and these models are increasingly linked with optimization approaches to aid design and management decision making.  Subsurface remediation strategies involve decisions such as the restoration approach to use, the selection of locations to install devices, such as wells, and the rates of extraction from or injection into these wells.  There are open scientific problems in simulation related to fundamental modeling and choice of numerical methods that need to be addressed as part of the design of software to optimize the remediation methods.  The three PIs have expertise in simulation, modeling, and experimental work in multi-phase flows as well as broad expertise in optimization methods.  The objectives of this project are to design models, simulators, and optimization methods simultaneously with the goal of more efficient optimal design based on more accurate models."
"0107539","Mesoscale Numerical Methods for Certain Types of Implicit Partial Differential Equations","DMS","COMPUTATIONAL MATHEMATICS","08/15/2001","08/20/2001","Petr Kloucek","TX","William Marsh Rice University","Standard Grant","Junping Wang","07/31/2004","$100,000.00","","kloucek@caam.rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271","0000, 9263, OTHR","$0.00","The investigator develops a comprehensive computational tool for modeling of the kinetics of microstructural evolution under thermo-mechanical loading in compound structures.  He approximates the microstructural kinetics using first order implicit partial differential equations with a certain specific type of Dirichlet boundary conditions, and using a novel subgrid projection method. He applies this computational technique to provide an active control of vibrations and noise reduction based on the martensitic phase transformation.  Implicit partial differential equations represent a large class of ordinary and partial differential equations and systems that are nonlinear in the highest derivatives, such as the Eikonal or Hamilton-Jacobi equations.  They are closely related to partial differential inclusions that play a crucial role in the study of phase transitions.  Typically, the solutions of imlicit partial differential equations are not smooth, they are not unique, and often they incorporate enormous amount of competing scales.  These three distinctive features present a definitive challenge to the design of suitable numerical methods applicable to finding solutions of implicit partial differential equations.  Recent work based on the Baire category argument shows that there exist solutions of such equations that cannot be obtained by standard analytical approaches.  The existence theory itself is not constructive, does not yield any hint as to how to construct selection principles, and it does not provide a notion of a generalized solution.<br/><br/>In regular use, machinery is subjected to periodic stresses.  This results in acoustic waves travelling through the material. Since these waves are small in comparison to the potential energy of the overall machine, conversion to heat is an effective method of noise reduction.  Thus highly damping materials may be used either in part or in full to accomplish this task.  Shape memory alloys exhibit such significant damping properties.  These are special alloys that change their microstructure from that of a stiff, rotationally symmetric phase to a ductile, less symmetric phase when cooled or put under stress.  These desirable damping properties are a result of movement within the twin boundaries in the martensite phase, as well as the motion of the incoherent austenite-martensite interface, and are significantly temperature dependent.  The investigator undertakes computational modeling to understand and actively control the phase transitions in shape memory alloys, based on implicit partial differential equations.  The applications include such possibilities as controlled vibration of a cutting edge in non-invasive surgery, ultrasonic wave detectors, stabilization of platforms on various spacecraft as well as acoustic suppression in cockpits. <br/>"
"0112759","ITR/AP(DMS): Numerical Studies of the Nonlinear Interaction Between Turbulent Air Flow and Sea Surface Waves, with Application to Ocean Surface Wave Turbulence","DMS","COMPUTATIONAL MATHEMATICS, ITR SMALL GRANTS","09/01/2001","08/14/2001","Gregory Baker","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","Leland Jameson","08/31/2007","$499,682.00","Joel Johnson, Kurt Berger","baker@math.ohio-state.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1271, 1686","0000, 1686, 9263, OTHR","$0.00","The proposed project develops a numerical model for turbulent airflow over nonlinearly evolving water surfaces that overcomes the limitations of previous studies. Efficient computational algorithms for modeling airflow, efficient hydrodynamic methods to evolve the sea surface boundary, and parallel computing techniques will be combined to make statistical analyses of realistic problems in both two and three dimensions possible. Scientific visualization methods will be applied to analyze results and to improve understanding of the nonlinear interactions between wind and waves that are important for sea surface growth and dissipation. Studies will be performed to determine the conditions under which ""decoupling"" the interacting complex systems into airflow-only or hydrodynamic-only simulations is possible by including appropriate forcing terms; the form of these forcing terms, when applicable, will be compared with existing empirical and analytical models to clarify the important physical processes. Decoupled hydrodynamic-only simulations will be pursued to investigate wave-wave energy transfer effects in larger scale problems than those possible in the coupled air-water model, and resulting forms of the equilibrium sea surface spectrum will be obtained. Commonly applied approximate hydrodynamic models will also be considered to assess their performance, and wave spectra obtained from numerical studies will be extrapolated using applicable approximate models to include breaking-wave effects not captured in the full numerical simulations. Results of the project will be significant for studies of the global climate, sea wave forecasting, and interpretation of direct and remotely sensed oceanographic data. More generally, project results will clarify important physical effects that can occur in a system of coupled nonlinear turbulent systems. <br/><br/>The generation of sea surface waves by winds is one of the fundamental air-sea interaction processes that affect the global climate. Although this topic has been studied extensively through both physical modeling and experimental measurements, understanding of the physics involved remains limited due to the nonlinear phenomena implicit in both airflow and sea surface wave evolution. Numerical simulations offer a means to improve understanding of the complex interactions of this problem, but only recently have computing resources improved to make sufficiently large scale simulations possible. The proposed project represents an interdisciplinary collaboration between applied mathematics and engineering researchers. Educational efforts also comprise a principal objective of the project, including graduate and undergraduate education and research. All participants in the project will gain information technology (IT) experience through algorithm and code development. Project results will be communicated to the external community through conference and journal publications and through use of the world-wide-web; project web resources will also be used in classes taught by the research team to introduce students to scientific applications of IT research. <br/>"
"0104573","Ultrasonic Imaging: A New Tool for Early Detection of Breast Cancer","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/15/2001","08/16/2004","Clifford Nolan","NY","Rensselaer Polytechnic Institute","Standard Grant","Michael Steuerwalt","07/31/2005","$61,000.00","","noland@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","MPS","1266, 1271","0000, 9263, OTHR","$0.00","Nolan<br/>0104573<br/>     The investigator seeks to improve ultrasound imaging in<br/>order for it to be a more effective tool in breast cancer<br/>diagnosis and treatment.  He uses modern methods of partial<br/>differential equations and microlocal analysis to provide images<br/>of the breast that are artifact-free and of a higher resolution<br/>than currently available.  He models propagation and scattering of<br/>ultrasonic waves using microlocal analysis and geometrical<br/>optics.  In particular, the approach uses the complete waveform of<br/>the ultrasound instead of just its phase.  Microlocal analysis<br/>provides a clear explanation of artifacts that appear in acoustic<br/>images.  More importantly, it provides a means of avoiding and<br/>removing such artifacts.  To help in obtaining clear images, the<br/>investigator looks for an arrangement of transducers providing<br/>the strongest measurable scattered signal at the surface of the<br/>breast.  The optimal configuration is based on a power method that<br/>starts with a non-zero arbitrary source and measures the<br/>resulting scattered field (caused by a tumor for example) at the<br/>surface.  He models the scattering process with a linear<br/>scattering operator based on the wave equation.  The scattering<br/>operator maps sources to scattered wave fields, whereas the<br/>adjoint scattering operator maps scattered wave fields to<br/>sources.  By composing the scattering operator with its adjoint,<br/>he uses the power method to find the eigenfunction corresponding<br/>to the largest scattered signal.  Novel in the approach is the<br/>ability to couple the power method with geometrical optics.  This<br/>provides an analytical tool for computing the optimal sources<br/>that can then be implemented computationally in a very efficient<br/>way for both modeling and imaging the inversion.<br/>     This work aims to provide a new method of imaging the female<br/>breast for use in diagnosing and treating of breast cancer.  The<br/>technique is based on ultrasound waves, howver it uses much more<br/>information from these waves than is currently used.  In<br/>particular, the shape of the waves that bounce off of a lesion or<br/>tumor in the breast is analyzed.  The method is tailored to avoid<br/>artifacts that can appear in current imaging techniques.  In<br/>addition, the method will be fast and feasible to implement.<br/>Because of the reliability and clarity of the images, this new<br/>imaging technique should significantly cut down on the number of<br/>unnecessary biopsies that are currently carried out in the United<br/>States each year.  Because of the speed of the method, it can help<br/>to provide a quick diagnosis.  The method also takes advantage of<br/>recently developed three dimensional imaging technology.<br/>Ultrasound is a tried and safe method of imaging; it involves no<br/>harmful radiation.  The method is general enought that it could<br/>concievably have spin-offs into other modes of imaging (X-Ray,<br/>MRI, etc.).  Finally, this new method of imaging should create<br/>opportunities for new and established companies to manufacture<br/>devices that take advantage of it.<br/>"
"0101458","FRG: Solutions for Inverse Problems","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2001","08/01/2002","Joyce McLaughlin","NY","Rensselaer Polytechnic Institute","Standard Grant","Henry Warchall","08/31/2005","$1,022,553.00","Margaret Cheney, Antoinette Maniatty, Clifford Nolan","mclauj@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","MPS","1266, 1271","0000, 1616, 9263, OTHR","$0.00","In the work proposed here, four researchers at Rensselaer Polytechnic Institute work in teams with postdocs and graduate students to solve inverse problems.  The goal is to identify material properties or surface features from indirectly related data sets.  Each problem is modeled as an elastic, electromagnetic, or acoustic medium and the mathematical model is strongly taken into account to develop solution techniques. Two of the four problems to be considered are: (1) find variations in stiffness in biological tissue so that regions that are abnormal (usually with 7 to 17 times stiffer than normal tissue) can be identified.  The model is the equations of elasticity and measurement of low frequency propagating shear waves using Doppler ultrasound provide the data.  Noise reduction, use of models with random media, determination of the minimum size of abnormal regions that can be identified, reconstruction algorithms and images are all targets of this investigation; and (2) create radar images from satellite or airborne radar equipment by developing solutions that correct for deviations from ideal flight paths, that correctly identify object positions (break left-right symmetry problems) and that can image objects that are moving.  The problems involve establishing mathematical results, utilizing engineering expertise and development and implementation of numerical algorithms.<br/><br/>In this work, four principal investigators, together with postdocs and graduate students, solve problems where noninvasive sensing is followed by the creation of images.  To solve these problems, the researchers work in teams to combine mathematical analysis, engineering and numerical computation to achieve results.  Two of the problems that will be addressed are: <br/>(a) elastography with ultrasound measurements of tissue movement created by a second low level propagated signal yields data that can distinguish the stiffer tissue of cancerous tumors from normal tissue.  The goal, then, is to determine algorithms for computing the location of the stiff and normal regions, to find the minimum size of regions that can be identified, and to create images in 'real time';<br/>(b) airborne and satellite topographic and object sensing where signals reflected from the earth's surface are used to locate positions of objects and topographic features.  The goal is to improve resolution and accuracy, to reduce the size of objects and features that can be identified and to correctly represent features that are partially hidden from view.  <br/>"
"0107420","Problems in Population Genetics","DMS","ADVANCES IN BIO INFORMATICS, POPULATION DYNAMICS, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2001","03/27/2007","Stanley Sawyer","MO","Washington University","Continuing grant","Mary Ann Horn","06/30/2007","$243,959.00","","sawyer@math.wustl.edu","CAMPUS BOX 1054","Saint Louis","MO","631304862","3147474134","MPS","1165, 1174, 1266, 1271","0000, OTHR","$0.00","Sawyer<br/>0107420<br/>     The investigator studies ways to estimate the average amount<br/>of selection for or against new mutations at a particular genetic<br/>locus using information contained in aligned DNA or protein<br/>sequences.  If the estimated amount of selection is significantly<br/>negative, this is an indication that the region is under<br/>stabilizing selection and may have an important biological<br/>function.  If the estimated average amount of selection is<br/>positive, then new variants in this region are favored, as is<br/>common for some immune system genes.  Models that allow random<br/>variation of the amounts of selection over sites are also<br/>considered.  The methods studied are computationally easy to carry<br/>out and have easily obtained significance levels, but assume<br/>statistical independence between polymorphic sites instead of the<br/>more usual assumption of tight linkage.  Studies have shown in<br/>many cases that short-segment gene conversion and recombination<br/>act at a greater rate than point mutation, so that tight linkage<br/>is not always a reasonable assumption.  Simulations are carried<br/>out to test the assumption that small to moderate amounts of gene<br/>conversion and recombination are sufficient to guarantee enough<br/>independence between sites so that the proposed estimators will<br/>have desirable statistical properties.  The models are applied to<br/>aligned sequences from public databases.  They are also applied to<br/>human SNP datasets, for which the assumption of sitewise<br/>independence is clearer.<br/>     Recent advances in biology have made a vast amount of data<br/>available about the human genome and about the genomes of other<br/>species.  Tools for understanding the significance of this data,<br/>and in particular the significance of variation in this data<br/>between individuals, have lagged behind.  One question is whether<br/>new genetic variants at a particular genetic locus (or segment of<br/>DNA) have been favored on the average in the past, have rendered<br/>their owners less fit, or have had essentially the same effect on<br/>the fate of individuals with those DNA variants.  Tests are<br/>proposed that give a computationally easy way of determining<br/>which of these three cases applies at a particular genetic locus.<br/>These tests are based on the configurations of variant sites in<br/>DNA from several individuals at that locus.  The results of this<br/>test can give information about the function of this stretch of<br/>DNA.  They can also give information about how a particular gene<br/>is affected by or is currently being affected by evolution.<br/>Information from related biological species can give further<br/>information.  The difficulty of the proposed methods is that they<br/>assume that different portions of the genetic locus are evolving<br/>approximately independently.  For some genetic data (for example,<br/>SNP for ""single-nucleotide polymorphism"" data) this can generally<br/>be assumed.  The sensitivity of the proposed methods to this<br/>independence assumption for non-SNP data is tested.<br/><br/>"
"0103588","FETI Algorithms for Mortar Methods","DMS","COMPUTATIONAL MATHEMATICS","08/15/2001","08/20/2001","Alan Edelman","MA","Massachusetts Institute of Technology","Standard Grant","Michael H. Steuerwalt","07/31/2003","$64,350.00","Dan Stefanica","EDELMAN@MATH.MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1271","0000, 9263, OTHR","$0.00","The investigator, Dan Stefanica, extends the Finite Element Tearing and Interconnecting (FETI) algorithms to mortar methods, thus taking advantage of the inherent flexibility of the mortar elements.  He establishes that the resulting algorithms preserve the convergence properties of the FETI methods for conforming elements.  This agrees with extensive numerical experiments for FETI algorithms for low order mortar finite elements.  He checks whether the condition number estimates obtained for the FETI algorithms for mortars are sharp.  He extends the FETI method to spectral methods and mortar spectral elements, by providing numerical and theoretical convergence analysis for these algorithms.  Finally, he designs Dual--Primal FETI algorithms for both geometrically conforming and geometrically nonconforming mortar methods. <br/><br/>The algorithms analyzed in this project are part of the larger family of domain decomposition methods, which are powerful, fast, and easily parallelizable methods for the numerical solution of partial differential equations arising from a large spectrum of practical applications.  The investigator couples one of these methods, the FETI method, with a versatile discretization of the equations, called the mortar method.  Thus, he is able to solve problems with very complicated geometry while concentrating most of the computational effort on resolving the critical parts of the problem.  The FETI method has already been implemented in huge parallel codes for a large spectrum of applications.  Improvements in it developed by this project will be important for such applications as aerospace design, computational mechanics and fluid flow problems.<br/>"
"0103822","Neuronal Dynamics of The Basal Ganglia and Related Systems","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2001","06/18/2003","David Terman","OH","Ohio State University Research Foundation -DO NOT USE","Continuing grant","Michael H. Steuerwalt","08/31/2004","$252,000.00","Alice Yew","terman@math.ohio-state.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1266, 1271","0000, 9263, OTHR","$0.00","Terman<br/>0103822<br/>     The investigator and his colleague develop geometric,<br/>dynamical systems tools for analyzing biophysical,<br/>conductance-based models for a broad class of neuronal networks.<br/>These systems arise in numerous applications including motor<br/>activity, sensory processing and learning.  They also develop<br/>computational models for the generation of sleep rhythms,<br/>epilepsy and parkinsonian tremor.  The investigators also<br/>construct and refine a mathematical and computational model for<br/>electrical activity in the subthalamic nucleus and external<br/>segment of the globus pallidus.  These are two nuclei in the<br/>basal ganglia, a part of the brain involved in motor activity.<br/>Dysfunction of the basal ganglia is associated with movement<br/>disorders such as Parkinson's disease and Huntington's disease.<br/>The model is used to test hypotheses on the role of the basal<br/>ganglia in both normal and pathological movement.  Numerous<br/>experiments have demonstrated that neurons within the basal<br/>ganglia display a rich variety of dynamic behavior; moreover,<br/>patterns of neuronal activity, both spatial and temporal, are<br/>different between a normal and a pathological state.  The<br/>investigators characterize the possible patterns of neuronal<br/>activity that arise in the model and determine how these patterns<br/>change with respect to modulations of network parameters and<br/>structure.  A long term goal is to develop a model realistic<br/>enough so that it can shed light upon the key parameters and<br/>mechanisms responsible for the generation and modulation of<br/>observed activity patterns.<br/>     A mathematical theory for the analysis of neuronal dynamics<br/>helps illuminate the role played by various components of a model<br/>in generating a particular population rhythm.  These components<br/>may correspond to some intrinsic property of individual cells, or<br/>to some network property such as the strength and type of<br/>synaptic coupling or the probability that two cells communicate<br/>with each other.  Clarification of the mechanisms underlying<br/>different activity patterns may lead to a classification of all<br/>possible rhythms that can emerge from a given network, and enable<br/>us to determine how complicated a model should be in order to<br/>display some observed behavior.  It also helps predict<br/>transitions of the network behavior as parameters in the model<br/>are varied.  The investigators develop models for neuronal<br/>dynamics, and in particular of electrical activity in the<br/>subthalamic nucleus and external segment of the globus pallidus.<br/>These are two nuclei in the basal ganglia, a part of the brain<br/>involved in motor activity.  Dysfunction of the basal ganglia is<br/>associated with movement disorders such as Parkinson's disease<br/>and Huntington's disease.  The model is used to test hypotheses<br/>on the role of the basal ganglia in both normal and pathological<br/>movement.  Such models may help illuminate both fundamental<br/>neuroscience questions and clinical issues about how the brain<br/>and central nervous system work.<br/>"
"0101208","Bifurcation in Dynamical Systems with Multiple Time Scales","DMS","INFRASTRUCTURE PROGRAM, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","09/01/2001","11/10/2005","John Guckenheimer","NY","Cornell University","Continuing grant","Michael H. Steuerwalt","08/31/2007","$377,347.00","","jmg16@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1260, 1266, 1271, 7334","0000, 9263, OTHR","$0.00","Guckenheimer<br/>0101208<br/>     The investigator studies slow-fast decompositions and<br/>bifurcations of trajectories in dynamical systems with multiple<br/>time scales.  This extends the theory of bifurcation in generic<br/>families of dynamical systems to those with two time scales.<br/>Emphasis is placed upon relaxation oscillations, periodic orbits<br/>that have both slow and fast segments.  The initial stages of the<br/>work seek a classification of degenerate decompositions appearing<br/>in periodic orbits of one parameter families with relaxation<br/>oscillations.  Geometric methods are used to determine<br/>bifurcations associated with each degenerate decomposition.<br/>Numerical investigation of examples is used to motivate the work<br/>and to ensure that the results are directly applicable to<br/>biological models.  Together with collaborators Kathleen Hoffman<br/>and Warren Weckesser, the investigator is reexamining a classical<br/>example, the forced van der Pol system that gave birth to the<br/>discovery of chaos for dissipative dynamical systems, and expects<br/>to give a full description of the bifurcations that occur within<br/>this system.  He also develops algorithms for the computation of<br/>structures that are difficult to compute with existing methods.<br/>     Rhythmic phenomena are ubiquitous in biological systems.<br/>Examples include the heartbeat, the cell cycle, circadian<br/>rhythms, legged locomotion, and electrical signals in the nervous<br/>system.  Most of these involve multiple time scales.  The<br/>investigator pursues new mathematical theory and computational<br/>methods that apply to dynamical systems with multiple time<br/>scales.  Emphasis is given to models of neural systems, an area<br/>in which the presence and importance of complex dynamics are<br/>manifest.  The investigations draw upon decades of research in<br/>characterizing generic phenomena observed in dynamical systems<br/>with a single time scale.  The resulting body of mathematics,<br/>sometimes called chaos theory, needs extension and modification<br/>to fully explain the behavior of systems with multiple time<br/>scales.  Those extensions are the goal of this project.  On a<br/>longer time frame, the project lays foundations for coming<br/>generations of biological models for cellular processes such as<br/>gene expression and signal transduction.  New biotechnology leads<br/>to ever more complicated reaction networks that are simulated as<br/>dynamical systems.  This project produces results that aid the<br/>implementation and interpretation of such simulations of complex,<br/>multiple time scale dynamical systems.<br/><br/>"
"0109957","Mathematical Models of Moving Boundary Problems in Developmental Biology and Cell Motility","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/15/2001","08/10/2001","Robert Dillon","WA","Washington State University","Standard Grant","Michael H. Steuerwalt","07/31/2005","$82,171.00","","dillon@math.wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","MPS","1266, 1271","0000, 9263, OTHR","$0.00","Dillon<br/>0109957<br/> The principal investigator and his colleagues develop<br/>mathematical models and computational methods for analyzing the<br/>motion of eucaryotic cilia and flagella, the growth and<br/>development of the vertebrate limb bud and tumor growth. The<br/>objective of the cilia modeling is to develop three-dimensional<br/>models for an individual cilium or flagellum and for<br/>three-dimensional arrays of cilia. These models incorporate<br/>discrete representations of the axoneme's ultrastructure<br/>including the individual dyneins and microtubules, as well as a<br/>continuous description of the fluid mechanics using the immersed<br/>boundary method. Vertebrate limb development is a widely-used<br/>model system for the study of development. The investigator and<br/>his collaborators have developed a 2D model for vertebrate limb<br/>development that describes limb bud outgrowth and predicts the<br/>spatio-temporal distribution of growth factors and morphogens.<br/>The major goals of the limb development modeling are to further<br/>validate the 2D model, to extend it to three dimensions in order<br/>to determine what signals cells see as a function of their<br/>position in the three-dimensional limb bud and developmental<br/>history and, from this, to suggest how the signals can be<br/>integrated by cells to produce the observed spatial patterning.<br/>Limb development and tumor growth are moving boundary problems<br/>that involve tissue growth, the transport of growth factors and<br/>control of gene activity. The investigators are developing an<br/>immersed boundary model for tumor growth along with the limb<br/>development model.<br/> These mathematical models focus on the complex biological<br/>systems involved in cell motility, vertebrate development, and<br/>tumor growth. An important aspect is the development of<br/>computational tools for the study of these processes. Thus, the<br/>ciliary model is used for the computational study of dynein<br/>activation mechanisms in cilia and flagella and the multiciliary<br/>model for the study of mucociliary interaction. The vertebrate<br/>limb development model may be used as an aid in interpreting<br/>experimental results, to help identify new experiments, and to<br/>test hypotheses that may be difficult to test experimentally.<br/>Because each of the models described in this proposal includes a<br/>fluid-mechanical representation, there is a unifying element in<br/>their mathematical description. The computational methods are<br/>based in part on the immersed boundary method, and the computer<br/>codes share many of the major subroutines. An important aspect of<br/>this proposal is the development of parallel numerical methods<br/>for the solution of these problems; advances in this area can be<br/>used in a variety of problems in the field of computational<br/>biofluids."

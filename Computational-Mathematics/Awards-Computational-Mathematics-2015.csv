"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1521009","Collaborative Research: Numerical Methods for Partial Differential Equations Arising in Shallow Water Modeling","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","09/17/2019","Alexander Kurganov","LA","Tulane University","Continuing Grant","Yuliya Gorb","06/30/2019","$131,311.00","","kurganov@math.tulane.edu","6823 SAINT CHARLES AVE","NEW ORLEANS","LA","701185665","5048654000","MPS","1271","8060, 8396, 8609, 9150, 9263","$0.00","This research project will contribute significantly toward development of computational methods for shallow water and related models. Special attention will be paid to applications arising in oceanography, in atmospheric sciences, and in hydraulic, coastal, civil, and enhanced oil recovery engineering, in which rapid changes in the bottom topography, Coriolis forces, friction, nonconservative terms, and uncertain phenomena have to be taken into account. The problems under study include rainwater drainage and flooding in urban areas, shallow water models of turbidity currents, multilayer flows, and shallow water models with uncertain data. The new tools under development promise to have great potential in designing coastal protection systems and investigating the effects of sediment transport on shelf drilling platforms as well as contributing toward the development of flood mitigation systems and planning of new urban areas. <br/><br/>The project is aimed at developing accurate, efficient, and robust numerical methods for shallow water equations and related models, with particular reference to problems that admit nonsmooth (discontinuous) solutions and involve complicated nonlinear waves, moving interfaces, and uncertain data. Shallow water models are systems of time-dependent partial differential equations (PDEs) that are derived using physical properties such as conservation of mass and momentum, and hydrostatic or barotropic approximations. Naturally these models, especially in the cases of high space dimensions, require development and implementation of special numerical techniques such as numerical balancing between the terms that are balanced in the original system of PDEs (development of well-balanced schemes), ensuring positivity of all fluid layers (this is absolutely necessary for both accurate description of dry and near dry states and enforcement of nonlinear stability), operator splitting methods, interface tracking approaches, and others that will be in the focus of the research project. The development of new techniques will be based on high-order shock-capturing finite-volume schemes, accurate and efficient ODE solvers, and stochastic Galerkin methods, utilizing major advantages of each one of these methods in the context of the problems under study."
"1503562","ACSB 2015:  A Conference on Algebraic and Combinatorial Approaches in Systems Biology","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","06/01/2015","05/15/2015","Martha Paola Vera-Licona","CT","University of Connecticut Health Center","Standard Grant","Mary Ann Horn","05/31/2016","$15,000.00","Elena Dimitrova, Luis Garcia Puente, Brandilyn Stigler, Jason Cory Brunson","veralicona@uchc.edu","263 FARMINGTON AVE","FARMINGTON","CT","060300001","8606794040","MPS","1264, 1271, 7334","7556, 9263","$0.00","The conference ""Algebraic and Combinatorial Approaches in Systems Biology"" (ACSB 2015) will be held at the University of Connecticut Health Center during May 22-24, 2015.  The primary goal of the conference is to provide a venue for researchers to share recent advances in applications of computational algebra and combinatorics to biology.  The conference will consist of talks by invited speakers, contributed talk,s and a poster session.  This grant helps to defray the expenses of the invited speakers and also to provide transportation and lodging for graduate students, postdoctoral researchers, and other junior researchers attending the conference.<br/><br/>The previous two decades have witnessed the emergence and acceleration of a new paradigm in mathematical biology, which employs advanced methods in computational algebra and discrete mathematics toward modeling biological systems. In molecular biology in particular, rich libraries of gene regulation, protein-protein interactions and high-throughput gene expression profiling enable the construction of increasingly granular discrete models, which require the development and analysis of new algebraic and combinatorial techniques.  Whereas traditional methods in mathematical biology benefit from a robust infrastructure of journals, organizations, and conferences, such infrastructure for algebraic and combinatorial biology is still taking shape.  A core objective of this conference is to consolidate the ACSB as a national forum for the presentation, discussion, and integration of current work in the field.  ACSB 2015 will focus on diverse aspects of modeling of molecular networks, in particular gene regulatory networks, with an emphasis on discrete modeling approaches, including stochastic aspects of networks.  In addition to models of specific molecular networks, it will explore questions such as the relationship between network structure and their dynamics and design principles of molecular networks.  Both mathematical and biological aspects of molecular network modeling will be discussed, and the conference will open with talks on both.<br/><br/>Conference web site:  http://acsb2015.cqm.uchc.edu/"
"1522249","Fast Huygens Sweeping Methods for Large-Scale High Frequency Wave Propagation and Wave-Related Imaging Problems","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/11/2015","Jianliang Qian","MI","Michigan State University","Standard Grant","Leland Jameson","08/31/2018","$323,644.00","","qian@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","9263","$0.00","Most information is communicated in the form of waves, and it is critical to carry out wave simulations to advance science and engineering disciplines. In fact, computational wave propagation has become a fundamental, vigorously growing technology in diverse disciplines ranging from radar, sonar, seismic imaging, medical imaging, submarine detection, stealth technology, remote sensing, and electronics to microscopy and nanotechnology. These fields and applications are of great strategic value in the petroleum industry, in medical imaging, and in materials science. One of the most challenging problems in computational wave propagation is how to carry out large-scale high frequency wave propagation efficiently and accurately.  This research project develops novel, fast numerical methods for high frequency wave propagation to tackle this long-standing challenge. <br/><br/>The investigator will develop and implement new data-enabled fast Huygens sweeping methods for large-scale high-frequency wave modeling and imaging with big data sets motivated by industrial and military applications. The goal is to develop efficient and accurate sweeping methods for the Helmholtz and Maxwell equations in inhomogeneous media in the high frequency regime and in the presence of caustics. This project will foster breakthrough innovations in at least four theoretical and computational aspects: first, fast higher-order sweeping methods for computing Eulerian geometrical-optics ingredients, such as eikonals and amplitudes; second, butterfly-algorithm-based fast Huygens sweeping methods for large-scale high-frequency wave modeling and simulation; third, fast Huygens sweeping-imaging methods for big seismic data sets; and fourth, implementations of these new algorithms on a range of novel parallel-computing architectures. Data-enabled fast Huygens sweeping algorithms will be developed for the first time for these large-scale imaging applications."
"1453661","CAREER:  Development of Discontinuous Galerkin Methods for Kinetic Equations in High Dimensions","DMS","COMPUTATIONAL MATHEMATICS, Division Co-Funding: CAREER","09/15/2015","07/02/2019","Yingda Cheng","MI","Michigan State University","Continuing Grant","Leland Jameson","08/31/2021","$400,000.00","","ycheng@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271, 8048","1045, 9263","$0.00","Kinetic equations arise as fundamental models in many applications such as rarefied gas dynamics, plasma physics, nuclear engineering, semiconductor device design, traffic networking, and swarming.  Due to the high-dimensionality of such models, conventional numerical partial differential equation (PDE) solvers will incur prohibitive computational cost, limiting their applications to real-world problems. This research project aims at addressing this issue by developing discontinuous Galerkin (DG) methods by the sparse grid approach. The resulting schemes enjoy the excellent properties that the traditional DG methods offer, while still being able to achieve high-order accuracy with a significant reduction in the required degrees of freedom. The research will have direct impact on the efficient and robust computations of kinetic equations and on other application areas involving high-dimensional PDEs. The research plan is complemented by educational and outreach activities involving the training of undergraduates, graduate students, and postdoctoral associates, and fostering collaborations among female researchers in computational mathematics.  <br/><br/>The research plan consists of several coherent projects, ranging from algorithm design, analysis, and implementation to application. In particular, the investigator plans to perform detailed studies of sparse tensor product polynomial spaces and to construct DG methods on sparse grids for model equations. The methods will be further developed for kinetic equations with attention to numerical challenges specific to Vlasov and Boltzmann equations. The PI will also develop similar numerical schemes for PDEs arising from areas such as optimal control and mathematical finance. Theoretical issues including stability, conservation, and error estimates, and computational issues including efficient linear solvers, adaptivity, and parallel implementations will be explored. The educational goals of this project are to develop a pipeline for the recruitment, retention, and early research exposure of undergraduate students, to continue integrated training and mentoring of graduate students and postdocs, and to promote collaborations and build a network of support for early career female researchers. Undergraduates, graduate students, and postdocs will be directly involved in every aspect of the proposed research. Summer research and career development workshops at Michigan State University will be established to promote research collaborations and build a community of support for early career female researchers."
"1522615","Integrated Geometric and Algebraic Multigrid Methods","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","08/13/2017","Jinchao Xu","PA","Pennsylvania State Univ University Park","Continuing Grant","Leland Jameson","08/31/2019","$385,000.00","Ludmil Zikatanov","xu@math.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1271","9263","$0.00","A primary goal of this project is to create framework for developing more robust and user-friendly solvers for linear systems of equations, which are ubiquitous in science, engineering, and industrial applications. The investigators will carry out integrated analysis and development of geometric multigrid and algebraic multigrid methodologies. Geometric multigrid (GMG) methods form a class of multilevel solvers designed to solve linear systems of equations arising from certain classes of discretized partial differential equations (PDEs). Algebraic multigrid (AMG) methods are also multilevel solvers, though these techniques avoid any dependence on information regarding an underlying grid geometry or PDE. As a result, GMG methods are effective tools for solving a more restricted class of linear systems with a strong theoretical backing for their performance, whereas AMG solvers apply to more general linear systems, though do not share the same mathematical rigor in justifying their performance. This research project will employ functional analysis as a natural framework for studying GMG and AMG methods in a unified setting. The resulting theory will establish strong guiding principles for analyzing and developing robust, efficient, and scalable multilevel solvers. The iterative solvers under development will be implemented in open source parallel codes, made available to a broader scientific computing community, providing powerful tools for simulation and a foundation for future algorithm research and development. Moreover, this project provides Ph.D. students with opportunities to participate in a variety of education and research activities, in which they will receive advanced training, participate in conferences, and collaborate with researchers from industry and Department of Energy laboratories.<br/><br/>This research project investigates a unifying framework for analyzing GMG and AMG methods in a functional analysis setting. This framework provides a strong foundation for understanding the operators, relationships between spaces, and other core ingredients involved in these multilevel solvers, such as the construction of coarse spaces and their respective bases, and a convergence analysis that applies to a broad variety of existing AMG methods. Furthermore, for problems originating from discretized PDEs, the integration of geometric information for constructing auxiliary grid-based preconditioners and highly effective smoothers on each level can be seamlessly integrated, allowing for more flexible and aggressive algebraic coarsening. More benefits of incorporating geometric information are realized in nearly optimal load balancing and predictable communication patterns for parallel implementations. For more general linear systems, the project studies the use of (relaxed) compressed sensing techniques to preserve sparsity for coarse space operators, which can be supplemented with information from an underlying geometric grid or adjacency graphs of the matrix to gain control over the computational complexity. It is expected that these techniques will be useful in solving problems with non-quasiuniform underlying grids or problems with matrices that are not symmetric and positive-definite (SPD). To verify the efficacy of the developed methodologies, the resulting solvers will be applied to fluid-structure interaction problems and nearly singular SPD problems."
"1521537","Construction of Finite Elements Using Generalized Barycentric Coordinates and Its Application for Numerical  Solution of Partial Differential Equations","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/22/2015","Ming-Jun Lai","GA","University of Georgia Research Foundation Inc","Standard Grant","Leland Jameson","07/31/2018","$150,336.00","","mjlai@uga.edu","310 E CAMPUS RD RM 409","ATHENS","GA","306021589","7065425939","MPS","1271","9263","$0.00","Continuous and smooth functions piecewisely defined over a domain which is a union of polygons or polyhedrons will be studied. These functions are called polygonal splines. Traditionally, functions defined on a domain consisting of triangles or tetrahedra were studied and used for many applications such as scattered data interpolation or fitting and numerical solution of partial differential equations. These traditional functions called finite elements or multivariate splines have been playing a significant and successful role in many applied problems in physics, chemistry, engineering, and biological studies. Polygonal splines defined on polygons instead of triangles can extend our ability to handle applied problems over complicated domains more effectively.  For example, polygonal splines over a union of irregular cubes will be more efficient than trivariate splines or finite elements since each cube has to be split into at least 5 tetrahedra in order to define finite element or trivariate spline functions over the cube. There are several fundamental properties one must understand in order to use them well for applications. For example, how many basis functions of polygonal splines over a collection of polygons are there? Another problem is how well polygonal splines can approximate arbitrary functions. <br/><br/>The finite element method is the fundamental tool for numerical solution of partial differential equations (PDE) which finds extensive applications in all applied sciences. The PI will develop a new approach to construct serendipity finite elements using generalized barycentric coordinates over polygons or polyhedrons. One of the advantages of serendipity elements is to reduce the number of elements used for the numerical solution of PDE so that the computation will be more efficient. Polygonal splines developed in this project can be constructed over any irregular cubes. The PI will generalize some of the existing ideas to construct serendipity finite elements over a 3D convex polyhedron, e.g. irregular cube with six 2D quadrilaterals as its boundary faces as well as other more general convex polyhedra in the 3D setting. The PI will implement these serendipity finite elements for some linear and nonlinear PDEs in the 3D setting. Furthermore, the PI will construct differentiable high order serendipity elements over convex polygons in 2D and 3D which will be useful for higher order PDEs."
"1522734","Phase transitions in porous media across multiple scales","DMS","COMPUTATIONAL MATHEMATICS, WORKFORCE IN THE MATHEMAT SCI","07/15/2015","05/12/2020","Malgorzata Peszynska","OR","Oregon State University","Continuing Grant","Yuliya Gorb","06/30/2021","$395,794.00","","mpesz@math.oregonstate.edu","1500 SW JEFFERSON ST","CORVALLIS","OR","973318655","5417374933","MPS","1271, 7335","019Z, 8549, 9263","$0.00","In this project the PI will develop and analyze computational models describing the evolution of methane hydrate. Methane hydrate is an ice-like substance of great interest in geophysics, climate studies, and energy engineering, because it can release methane, a powerful greenhouse gas and drilling hazard; hydrates can also serve as a potential unconventional energy source. This work will involve several spatial scales from the kilometer scale of hydrate bearing subsea sediments and Arctic permafrost regions, to the micro-scale at which one looks at the pores of the sediments. The PI's mathematical analysis of hydrate models, under some simplifying assumptions, has so far revealed very unusual features absent, e.g., in ice-water phase transitions. Based on these analyses the PI will formulate more accurate algorithms for the simplified and for more complex realistic models, which in turn can further our understanding of hydrate formation and dissociation in nature. The software the PI will develop will be shared with the geophysics community.<br/><br/>In PI's prior work, she has framed a simplified model of methane hydrate as a parameter-dependent free boundary problem, and her analyses show that its solutions can develop multiple singularities beyond those known, e.g., for Stefan problem of ice-water phase transitions. In this project the PI will develop new techniques for the simplified model extending substantially the classical results known for the non-parametrized case, and studying fundamental research questions concerning, e.g., nonlinear degenerate diffusion and conservation laws with the particular type of parameter-dependent monotone operators. The will extend the delicate time-stepping analyses in the abstract setting, develop a priori and a posteriori error analyses, and robust algorithms for the couplings across multiple time and spatial scales. In particular, the will consider the porescale at which she formulates reduced and dynamic models for many of the constitutive relationships needed for macroscale. In additional to computational mathematics, the project will impact the geophysics community involved in hydrate modeling, and more broadly other applications in porous media."
"1500254","Collaborative Research: Algebra and Algorithms, Structure and Complexity Theory","DMS","OFFICE OF MULTIDISCIPLINARY AC, FOUNDATIONS, COMPUTATIONAL MATHEMATICS, Special Projects - CCF","09/15/2015","05/03/2019","Keith Kearnes","CO","University of Colorado at Boulder","Standard Grant","Tomek Bartoszynski","08/31/2019","$144,199.00","Peter Mayr, Agnes Szendrei","kearnes@colorado.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1253, 1268, 1271, 2878","7433, 7933, 9263","$0.00","This project is a collaboration between mathematical researchers at five universities, including young mathematicians at the early stages of their careers, who are joining forces to tackle fundamental problems at the confluence of mathematical logic, algebra, and computer science. The overall goal is to deepen understanding about how to recognize the complexity of certain types of computational problems. The project focuses on a suite of mathematical problems whose solutions will yield new information about the complexity of Constraint Satisfaction Problems. These problems (CSP's) include scheduling problems, resource allocation problems, and problems reducible to solving systems of linear equations. CSP's are theoretically solvable, but some are not solvable efficiently. The research will be aimed at identifying a clear boundary between the tractable and intractable cases, and at providing efficient algorithms for solutions in the tractable cases. Many fundamental problems in mathematics and computer science can be formulated as CSP's, and progress here would have both practical and theoretical significance. A second component of the project investigates classical computational problems in algebra in order to determine whether they are algorithmically solvable. A third component of the project is the further development of the software UACalc, which is a proof assistant developed to handle computations involving algebraic structures.<br/><br/>The researchers shall work to decide the truth of the CSP Dichotomy Conjecture of Feder and Vardi, which states that every Constraint Satisfaction Problem with a finite template is solvable in polynomial time or is NP complete. They will further develop the algebraic approach to CSP's by refining knowledge about relations compatible with weak idempotent Maltsev conditions and about algebras with finitely related clones. A second goal of the project concerns the computable recognition of properties of finite algebras connected with the varieties they generate, such as whether a finite algebra with a finite residual bound is finitely axiomatizable, or whether a finite algebra can serve as the algebra of character values for a natural duality. One of the more tangible accomplishments of this project will be a broadening and strengthening of the applicability of the UACalc software. The agenda for this part of the project includes parallelizing the important subroutines, building in conjecture-testing and search features, adding further algorithms, and further developing the community of users and contributors."
"1502640","RTG: Data-Oriented Mathematical and Statistical Sciences","DMS","STATISTICS, COMPUTATIONAL MATHEMATICS, GOALI-Grnt Opp Acad Lia wIndus, WORKFORCE IN THE MATHEMAT SCI","08/01/2015","05/19/2021","Anne Gelb","AZ","Arizona State University","Continuing Grant","Junping Wang","07/31/2021","$1,143,368.00","Ming-Hung Kao, Douglas Cochran, Rodrigo Platte, Anne Gelb, John Stufken","annegelb@math.dartmouth.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1269, 1271, 1504, 7335","019Z, 1504, 7301, 9102, 9263","$0.00","The need and desire to analyze copious volumes of disparate data result in significant challenges. This RTG (Research Training Group) program will create a research environment and associated curricular elements that will engage U.S. citizen and permanent resident trainees in activities that will foster an understanding of the roles that statistics, computational mathematics, and applied harmonic analysis play in addressing data-oriented problems and appreciation of the synergies that can manifest when ideas from these areas, which are often studied by separate groups of students with little crossover, are brought to bear simultaneously. Durable impact is sought at ASU through cultivation of crosscutting faculty collaborations and curricular innovations intended to stimulate long-term strength in rigorous, integrated data-oriented mathematical and statistical research. Aggressive dissemination of innovative elements of the program will seek to provide a model for modern integrated data-oriented mathematics training for other institutions, and we will launch the careers of young researchers who will carry this vision.<br/><br/>The research will focus on the following three problem areas. (i) Closed-loop design of experiment for efficient data acquisition: Traditional approaches to collection and analysis of data are essentially ""feed-forward"" in nature, for example, data are collected, numerical algorithms are used to process it (e.g., for compression or to transform it in some other way), and statistical methods are ultimately employed for inference. Statisticians have long recognized the appeal of sequential design of experiments in which the nature of a sample is not fixed in advance, rather depends on previously observed samples. Recent and ongoing technological advances have led to measurement devices possessing many degrees of freedom that enable manipulating the nature of the measurement, often electronically and in real time. In this context, sequential design of experiments takes on a fundamental importance in throttling back otherwise unmanageable data torrents. Instead of collecting all the data all the time, a feedback strategy can be used to acquire only the most important new data for the task at hand based on what has already been observed; (ii) Data driven non-classical numerical approximation tools: A central problem in sensing and model simulation is recovery of characteristic features of a function, signal, image, or operator from a set (frequently these days a very large set) of collected data. In almost all such situations, the data set constitutes an incomplete and noisy description of the system. Classical numerical methods mostly use a limited system model as well data interpolation or approximation to extract pertinent model information and features. Deducing crucial features in large volumes of data calls out for new methods that are readily adaptable to model improvements and inclusion of appropriate prior or statistical information on provided data; (iii) Approximation for statistical inference: Traditional methods in approximation theory and their numerical realizations seek to reconstruct functions from measurements with fidelity described by a metric on a function space. Our RTG has particular expertise with problems where the data are not direct samples of the underlying function (e.g., they may be measurements of some transformed version of the function) and also where the objective is to reconstruct specific features of the function rather than the function itself. Earlier collaborations among the RTG PIs has led to the use of both overcomplete representations (e.g., frames) and statistical ideas in this vein of research."
"1522760","Algorithms for Inverse Problems that Exploit Kronecker Product and Tensor Structures","DMS","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL MATHEMATICS, Algorithmic Foundations","07/15/2015","07/15/2015","James Nagy","GA","Emory University","Standard Grant","Leland Jameson","06/30/2018","$299,933.00","","jnagy@emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1253, 1271, 7796","7933, 9263","$0.00","In many scientific and engineering applications, it is necessary to solve an inverse problem; that is, to determine quantities defining an object or a system through indirect measurements. The mathematics used to produce images in devices such as X-Ray Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are excellent examples. The mathematical models describing an inverse problem are often so complicated that it is not possible to find an exact analytical solution, and it is necessary to use an approximation based on a set of simpler equations. However, many equations may be needed; in medical imaging, for example, it is not unusual to compute approximations by solving millions of equations. Additional complications arise because an inverse problem may have infinitely many solutions, or the solutions may change dramatically if there are small errors in the indirect measurements. Thus, additional constraints and mathematical tools (often referred to as regularization) are needed to stabilize the numerical methods. The type and amount of regularization is problem dependent, requiring the algorithms to be able to easily adapt to user and/or problem specifications. The aim of this project is to develop mathematical tools and algorithms that exploit particular mathematical structures (called Kronecker product and tensors) in the matrices associated with equations that make up the inverse problem. Exploiting these structures will allow for better approximations, more efficient algorithms, and better facilitate the incorporation of regularization methods. One targeted application is for inverse problems that arise in image reconstruction for breast cancer detection; advancements in this area would have a clear benefit to society.<br/><br/>Efficient singular value decomposition (SVD) approximation methods for large-scale matrices that arise in discrete ill-posed inverse problems will be developed. The approach will exploit inherent Kronecker product and tensor structures, and will be the basis for a computational platform for the efficient solution of large-scale ill-posed problems. Efficient approaches to solve Kronecker product and tensor structured SVD updating problems will be developed. Iterative methods that can incorporate regularization, sparse and low-rank constraints on the solution will also be considered. The SVD approximations and updating methods developed in this project can be used as tools to obtain approximate solutions of ill-posed inverse problems, as preconditioners to accelerate iterative solvers, or as tools to build solution methods for nonlinear problems. The computational platform, based on SVD approximations, developed will have a broad scientific impact for applications where it is necessary to compute solutions of large-scale ill-posed inverse problems, including astronomy, cosmology, geophysics, microscopy, and medical imaging."
"1522599","Fast Algorithms for Solving Big Data PDE Parameter Estimation Problems on Cloud Computing Platforms","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/16/2015","Lars Ruthotto","GA","Emory University","Standard Grant","Leland Jameson","08/31/2019","$179,999.00","","lruthotto@emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1271","8037, 8396, 8609, 9263","$0.00","Parameter estimation problems arise in many scientific and economic disciplines, for example, in medical imaging, geophysical explorations, nondestructive testing, and economic structural estimation. Despite enormous effort put into designing efficient methods, solving parameter estimation problems is still very challenging, since the parametrized equations have to be solved repeatedly until the parameters are estimated with satisfactory accuracy. This research project aims to develop and implement efficient numerical methods for solving parameter estimation problems that involve a large number of measurements and partial differential equations.  Reusable, open source software will be developed and made available to the scientific community. The techniques under development in the project will be applicable in geophysics to reduce the computational costs of large surveys that are of high economic impact, for example, in oil and gas exploration and groundwater surveys. The results from this project will also be applicable in medical imaging to reduce health care screening costs and improve diagnosis of certain diseases.<br/><br/>Parameter estimation can be formulated as an optimization problem with constraints that are given by the parametrized partial differential equations (PDEs). The unknowns are parameters of the PDEs, which correspond to physical properties of the object to be measured. The objective is to minimize the misfit between PDE simulations and measured data plus some regularization term. Cloud computing platforms provide access to immense computational resources at moderate costs and are thus highly attractive for solving PDE parameter estimation problems. This holds particularly for big data problems since the computational costs of the estimation are dominated by the computational costs for PDE simulations. The latter, in many cases, grows linearly with the number of data. Straightforward extensions of the currently most reliable parameter estimation algorithms to massively parallel platforms, however, lead to huge communication overhead and memory requirement. This project seeks to design alternative tailored algorithms that make efficient use of cloud platforms and are able to solve parameter estimation problem with massive amounts of data in reasonable time. The approach undertaken in this project is based on three cornerstones. First, two reduced-order modeling techniques and their combination will be investigated. The PDEs will be discretized on rather coarse rectangular meshes that are aligned to the problem domain. On these meshes, reduced order models with adaptive multiscale bases will be used. Both techniques will dramatically reduce the computational cost associated with the PDE simulations. Second, stochastic optimization methods will be designed to exploit redundancy typically present in big data sets. The goal is to reduce the required number of PDE simulations, derive parameter selection rules, and quantify uncertainty of the solution. Third, the above steps will be combined and implemented on massively parallel cloud computing platforms."
"1532843","2015 Fifth Annual Mid-Atlantic Regional Mathematics Student Conference","DMS","COMPUTATIONAL MATHEMATICS","04/01/2015","08/11/2015","Padmanabhan Seshaiyer","VA","George Mason University","Standard Grant","Leland Jameson","03/31/2016","$19,602.00","Luis Melara, Lizette Zietsman","pseshaiy@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1271","7556, 9263","$0.00","This award supports participation in the Fifth Annual Mid-Atlantic Regional Joint Mathematics Conference, held at George Mason University, March 20-21, 2015. The goals of the conference are to (a) bring together students and experts in industry and academia; (b) provide undergraduate and graduate students as well as post-doctoral associates and new doctoral degree holders an opportunity to present their research in a professional conference setting; (c) foster interdisciplinary collaboration among research groups from both academia and industry; (d) encourage networking between students and industry professionals and; (e) increase student awareness of, and interest in, U.S. government and private industry careers.  This conference will strengthen the connection between mathematical advances in industry, government agencies, and academia. Professionals from industry and government agencies, academicians, and students will benefit from the diverse collection of talks, focusing on real world problems as well as recent theoretical advances. The core participating universities for this conference are George Mason University, Virginia Polytechnic Institute, and Shippensburg University.<br/><br/>The conference will focus on recent advances in applied mathematics, including optimal control theory, partial differential equations and the finite element method, computational topology, probability theory and stochastic processes, fluid dynamics, the modeling of materials, and machine learning.  Additionally, the conference will feature talks concerning traditionally pure and applied areas of mathematics with emphasis on computational aspects of problem solving as well as education. The conference will help to prepare mathematical sciences students for careers in business, industry, and government.  Industry participants will benefit from learning about cutting-edge academic advances and from access to highly-trained problem solvers, and students will benefit from exposure to real world problems, thus giving context to their areas of study.  The broader impact of the conference is the increased collaboration between industry, government and academia. By bringing these diverse groups together, the meeting will foster the exchange of ideas on how to tackle new, complex problems. Additionally, the conference provides young academic researchers a chance to network with industry professionals.  <br/><br/>Conference web page:  sites.google.com/site/siammidatlantic"
"1541647","Algebraic Vision Conference 2015","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","09/03/2015","Rekha Thomas","WA","University of Washington","Standard Grant","Rosemary Renaut","08/31/2016","$19,200.00","","thomas@math.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1271","7556, 9263","$0.00","This travel grant will support for US-based participants at the first meeting on Algebraic Vision to be held at the Technical University in Berlin from October 8-9, 2015. Algebraic Vision is an emerging research area at the intersection of computer vision and mathematics that seeks to develop the algebro-geometric foundations of fundamental geometric problems that arise in the reconstruction of 3-dimensional scenes and cameras from noisy 2-dimensional images. This structural understanding will allow the possibility of designing specialized algorithms for the many practical problems in vision on the one hand, while also exposing pure mathematicians to rich and structured algebraic problems that arise in applications. This exchange has the potential to benefit and enrich computer vision, algebraic geometry and polynomial optimization.<br/><br/>The primary goal of the meeting is to initiate connections between researchers in computer vision, algebraic geometry and polynomial optimization so as to identify a core set of problems that can benefit from established tools in the three fields. For this purpose, the meeting agenda will revolve around short talks by experts from both sides with plenty of time in between for discussions. Young researchers will have the opportunity to be exposed to this new exchange of problems and ideas which will create the possibility of training a new generation with a common set of sophisticated and useful skills from these three closely related fields."
"1500218","Collaborative Research: Algebra and Algorithms, Structure and Complexity Theory","DMS","OFFICE OF MULTIDISCIPLINARY AC, FOUNDATIONS, COMPUTATIONAL MATHEMATICS, Special Projects - CCF","09/15/2015","05/02/2016","Clifford Bergman","IA","Iowa State University","Standard Grant","Tomek Bartoszynski","08/31/2019","$106,881.00","","cbergman@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1253, 1268, 1271, 2878","7433, 7933, 9150, 9251, 9263","$0.00","This project is a collaboration between mathematical researchers at five universities, including young mathematicians at the early stages of their careers, who are joining forces to tackle fundamental problems at the confluence of mathematical logic, algebra, and computer science. The overall goal is to deepen understanding about how to recognize the complexity of certain types of computational problems. The project focuses on a suite of mathematical problems whose solutions will yield new information about the complexity of Constraint Satisfaction Problems. These problems (CSP's) include scheduling problems, resource allocation problems, and problems reducible to solving systems of linear equations. CSP's are theoretically solvable, but some are not solvable efficiently. The research will be aimed at identifying a clear boundary between the tractable and intractable cases, and at providing efficient algorithms for solutions in the tractable cases. Many fundamental problems in mathematics and computer science can be formulated as CSP's, and progress here would have both practical and theoretical significance. A second component of the project investigates classical computational problems in algebra in order to determine whether they are algorithmically solvable. A third component of the project is the further development of the software UACalc, which is a proof assistant developed to handle computations involving algebraic structures.<br/><br/>The researchers shall work to decide the truth of the CSP Dichotomy Conjecture of Feder and Vardi, which states that every Constraint Satisfaction Problem with a finite template is solvable in polynomial time or is NP complete. They will further develop the algebraic approach to CSP's by refining knowledge about relations compatible with weak idempotent Maltsev conditions and about algebras with finitely related clones. A second goal of the project concerns the computable recognition of properties of finite algebras connected with the varieties they generate, such as whether a finite algebra with a finite residual bound is finitely axiomatizable, or whether a finite algebra can serve as the algebra of character values for a natural duality. One of the more tangible accomplishments of this project will be a broadening and strengthening of the applicability of the UACalc software. The agenda for this part of the project includes parallelizing the important subroutines, building in conjecture-testing and search features, adding further algorithms, and further developing the community of users and contributors."
"1522677","Collaborative Research: Riemann-Hilbert Problems and Riemann Surfaces: Computations and Applications","DMS","COMPUTATIONAL MATHEMATICS","07/15/2015","06/27/2017","Bernard Deconinck","WA","University of Washington","Continuing Grant","Leland Jameson","06/30/2019","$199,707.00","","bernard@amath.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1271","9263","$0.00","Riemann-Hilbert problems (RHPs) arise in a plethora of applications, varying from equations describing tsunamis to the understanding of nuclear energy. In its most basic form, a RHP determines a function that jumps in a prescribed way along a curve in the plane and has specified behavior far away from where the jump occurs. Such problems were first posed by Riemann and later by Hilbert at the end of the 19th century. Their study has been at the forefront of pure and applied mathematics. Until recently, little effort had been devoted to the actual computation of solutions of such problems. This research project extends recent work in carrying out numerical investigations. It is anticipated that major advances will be made in the solution of RHPs, allowing for the increased understanding of tsunamis, fast optical communication, and other physical phenomena.<br/><br/>The goal of the project is to develop new computational tools for the solution of RHPs and their extensions. Traditionally, RHPs arise in the context of singular integral equations and the Wiener-Hopf technique. More recently, RHPs have been connected to random matrix theory, nonlinear special functions, and nonlinear wave equations. RHPs may be posed on Riemann surfaces, and nonlinear jump conditions may be specified. Recent developments involving the investigators and collaborators have led to the development of accurate and efficient numerical algorithms for the solution of RHPs, for problems posed on Riemann surfaces, and for the computation of special functions such as the Schottky-Klein prime function. However, many open problems remain, particularly concerning new applications. This project aims to develop new computational methods to solve these problems, with an emphasis on the development of fast and efficient algorithms that can deal with complicated geometries, and to deploy them in applications. The investigators, postdoctoral scholar, and collaborators bring together a unique combination of expertise in the different areas needed to successfully carry out the collaborative project."
"1522775","Collaborative Research: Modeling and Simulation of the Growth of Graphene Multilayers and Heterostructures","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/11/2015","John Lowengrub","CA","University of California-Irvine","Standard Grant","Leland Jameson","08/31/2018","$214,847.00","","lowengrb@math.uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1271","7237, 8037, 9263","$0.00","This award supports research and educational activities in mathematical and computational modeling of two-dimensional materials. Motivated by recent technological advancements in the isolation and transfer of different two-dimensional materials, such as grapheme and hexagonal boron nitride, the investigators will focus on two types of problems: heterostructures, where two materials are brought together in the same plane, and stacked two-dimensional layers of graphene. Compared to homogeneous monolayers, heterostructures and stacked layers contain many more degrees of freedom that can be exploited to fabricate materials with specifically designed electronic, micromechanical and optical properties. These novel materials have applications in many areas identified as critical to US strategic interests such as nanotechnology, information technology, and energy technology. Thus far, efforts in this area have been mainly experimental, using trial-and-error approaches. The multiscale mathematical and computational models developed here will make a substantial contribution to the field by providing a rational framework to optimize the production process. Further, this framework can be extended to examine other materials such as arrays of semiconductor quantum dots, and magnetic clusters, or metal-organic surface networks that are promising candidates for energy conversion, thermal transport, and other device applications. Two graduate students will receive interdisciplinary training and will present their findings at conferences, which will enhance their professional training. Outreach efforts include teaching high school students as part of the California State Summer School for Mathematics and Science (COSMOS) at UC Irvine. These efforts will help develop future generations of scientists. <br/><br/>This project will investigate the nonlinear dynamics of the mechanisms that govern the growth and morphology of graphene multilayers and heterostructures and to develop strategies to control its growth by (1) developing and applying state-of-the-art adaptive numerical methods to large-scale computation and (2) performing analytical, numerical, and modeling studies of important constituent processes. The research will provide a novel framework for the rational design of such materials. A significant challenge is that the structure and morphology of heterostructures and stacked layers is determined both by atomic-scale phenomena and by the diffusion of multiple species and elastic interactions over length scales of hundreds of nanometers. Consequently, no single model is able to describe all the processes involved in the formation of graphene heterostructures. The investigators will adopt a multiple-scale approach in which atomistic and mesoscale algorithms will be developed to determine material properties and to predict the role of strain in heterostructures and vertically-stacked sheets as well as the atomic structures and defects. The atomistic simulations will provide material parameters and forces to new continuum phase field models that describe the growth of multicomponent in-plane and vertically-stacked sheets at larger scales. The highly nonlinear nature of these problems makes fast, accurate, and robust numerical methods essential to their study."
"1522383","Theory and Algorithms of Transformed L1 Minimization with Applications in Data Science","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/16/2015","Jack Xin","CA","University of California-Irvine","Standard Grant","Leland Jameson","08/31/2018","$299,890.00","","jxin@math.uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1271","9263","$0.00","This research project studies computational methods to recover signals (images) from partial observations, a technique known as compressed sensing.  The project addresses an outstanding issue in compressed sensing, in which redundancies or restrictions preclude the successful use of established techniques.  The research investigates a class of functions that promote sparsity under robust conditions and can be minimized with efficient or fast algorithms. The computational tools studied in the project will enhance sensing capabilities in applications such as imaging in astronomy and radar, medical imaging, threat detection, and recommender systems.  The project provides systematic training of graduate students towards advanced degrees in computational mathematics. The computational methods developed in the project will serve as a valuable tool for information technology and data sciences, benefitting the country and the general public in the digital age.<br/> <br/>The widely used convex function for sparse signal recovery is L1, which is known to introduce bias. This project studies a family of unbiased non-convex sparsity promoting functions called the transformed L1 (TL1). The TL1 minimization under a linear constraint can be solved by iterative thresholding methods with closed-form thresholding functions. The goal is to improve on L1 under robust sensing conditions when the constraint is ill-conditioned or the theoretical guarantees for successful application of the L1 method are not satisfied."
"1445371","The Midwest Mathematics and Climate Conference","DMS","COMPUTATIONAL MATHEMATICS, Physical & Dynamic Meteorology, Climate & Large-Scale Dynamics","05/15/2015","05/05/2015","Erik Van Vleck","KS","University of Kansas Center for Research Inc","Standard Grant","Junping Wang","04/30/2016","$24,000.00","Xuemin Tu, David Mechem","erikvv@ku.edu","2385 IRVING HILL RD","LAWRENCE","KS","660457552","7858643441","MPS","1271, 1525, 5740","4444, 7556, 9150, 9263","$0.00","The Midwest Mathematics and Climate Conference will take place on the campus of the University of Kansas from April 30 to May 2, 2015. The conference shall bring together atmospheric scientists and applied mathematicians to address different aspects of our changing climate, one of the most urgent topics of our time. Bringing together these different groups will facilitate a transfer of knowledge between climate science and mathematics. The conference will provide an excellent opportunity for researchers, especially early-career graduate students, postdocs, and faculty members, to disseminate their results to a broad, interdisciplinary mathematical and scientific community, gaining experience and improving their skills in communication and presentation. The conference will play an active role in motivating and strengthening the relationship between research groups in applied mathematics and atmospheric sciences across universities in the region.<br/> <br/> <br/>Although the scientific basis for the anthropogenic influence on Earth's climate system is well established, substantial uncertainty surrounds important details of climate behavior. Many components of the climate system are substantially nonlinear, suggesting the possibility of ""tipping points"" or threshold behavior, which may lead to abrupt, irreversible climate change. Although the concepts of nonlinearity, tipping points, and threshold behavior are appreciated by the atmospheric science community, they are typically approached somewhat anecdotally. On the other hand, the applied mathematics community has been exploring these topics in nonlinear systems for many years, though often in highly idealized mathematical frameworks. The goal of this conference is to bring together atmospheric scientists and applied mathematicians with the goal being to foster collaboration between the two groups, to expose atmospheric scientists to cutting-edge techniques in nonlinear mathematics, and to engage mathematicians with a hierarchy of models that exhibit greater degree of physical realism than the idealized models they typically employ. With this goal in mind, the PIs will host a conference on atmospheric science and mathematics with the title ""Midwest Mathematics and Climate Conference"" from April 30 to May 2, 2015 on the campus of the University of Kansas. Conference topics of emphasis will include nonlinear climate dynamics, high performance computing, numerical analysis, cloud systems behavior, data assimilation, dimension reduction, uncertainty quantification, model hierarchies, and advanced statistical methods. Bringing together researchers with expertise in these different areas will facilitate a transfer of knowledge between mathematics and areas of atmospheric science. Furthermore, the conference will provide strong motivation for interdisciplinary collaboration between mathematicians in dynamical systems, PDEs, numerical analysis, probability theory, and statistics, with researchers in climate and the broader atmospheric sciences."
"1522471","Cluster-Robust Estimates for Galerkin and Petrov-Galerkin Discretizations of Elliptic Eigenvalue Problems","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","07/24/2015","Jeffrey Ovall","OR","Portland State University","Standard Grant","Leland Jameson","08/31/2018","$149,936.00","","jovall@pdx.edu","1600 SW 4TH AVE","PORTLAND","OR","972015522","5037259900","MPS","1271","9263","$0.00","Eigenvalue problems for differential operators naturally arise in the study of vibrations in membranes and solids, fluid-solid interactions, and photonic crystals, and they also often play an important role in the practical analysis of many other time-dependent phenomena, such as acoustic or electromagnetic scattering. For many problems of interest, it is necessary to have provably efficient and robust means of estimating the error in computed approximations, as well as algorithms that can use this information to intelligently improve the approximations. This project concerns theoretical and algorithmic development of eigenvalue error estimates and self-adaptive methods for three computational approaches that promise to broaden the scope of available tools for addressing these challenging problems.<br/><br/>The PI considers eigenvalue problems arising from second-order, linear, differential operators that are not necessarily self-adjoint, and which may have continuous components in their spectrum. The proposed work includes the development of a posteriori eigenvalue/eigenspace error estimates that are robust in the presence of repeated or tightly-clustered discrete eigenvalues---even if they are near components of the essential spectrum. Three broad classes of discretizations will be considered: penalty-based Discontinuous Galerkin (DG) methods, Discontinuous Petrov-Galerkin (DPG) methods, and so-called Implicit Element methods, which include variations on Virtual Element (VEM) methods and Boundary-Element-Based Finite Element (BEM-FEM) methods. In each case, the project will provide a posteriori error estimates that are cluster-robust in the sense that are insensitive to distances between true eigenvalues within the cluster that one is approximating, but instead depend on the relative distance between this cluster and the rest of the spectrum. In the case of DG methods, the PI expects to produce at least one provably-convergent, high-order adaptive method. In the case of implicit elements, he will first produce a high-order source-problem solver that is competitive with VEM and BEM-FEM, develop corresponding a posteriori error estimates, and then extend the approach to eigenvalue problems. In the case of DPG methods, he plans to exploit the fact that, with these techniques, indefinite source problems can be treated in a computational way that only involves self-adjoint and positive definite systems, and use this to derive a FEAST-like algorithm for computing large spectral clusters and/or clusters higher in the spectrum."
"1454010","CAREER:  Fast Algorithms for Particulate Flows","DMS","COMPUTATIONAL MATHEMATICS, Division Co-Funding: CAREER","09/15/2015","07/01/2019","Shravan Veerapaneni","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Leland Jameson","08/31/2021","$420,721.00","","shravan@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1271, 8048","1045, 9251, 9263","$0.00","The objective of this Faculty Early Career Development (CAREER) project is to build stable, high-accuracy, and optimal algorithms for direct numerical simulations of particulate flows. Dense suspensions of deformable particles in viscous fluids are ubiquitous in natural and engineering systems. Examples include drop, bubble, vesicle, swimmer, and blood cell suspensions. Unlike simple Newtonian fluids, the laws describing their flow behavior are not well established, owing to the complex interplay between the deformable micro-structure and the macro-scale flow. For instance, interactions between soft particles modify their trajectories and cause shear-induced diffusion, and interaction with confining walls controls the spatial organization of the suspensions. Besides experiments, direct numerical simulations are often the only means for gaining insights into the non-equilibrium behavior of such complex fluids. Hence, there is a need for robust and optimal algorithms that are scalable. Integrated with the research effort, this project will undertake educational, mentoring, and outreach activities including a new interdisciplinary graduate-level course on computational methods for complex fluids and an interactive education module illustrating the non-intuitive phenomena observed in complex fluid systems that is accessible to high school students.  <br/> <br/>The specific aims of this project include (i) highly accurate algorithms to compute the nearly singular integrals that arise within the context of boundary integral methods when particles approach very close to each other when subjected to flow, (ii) fast, high-order, and adaptive algorithms to simulate multiphase flows through arbitrary periodic geometries, and (iii) stable time-marching and reparameterization schemes for the coupled systems of stiff, nonlinear, time-dependent differential and integro-differential equations governing the evolution of particles and some material concentration on their surfaces (e.g., surfactants or multi-phase lipids). The proposed computational infrastructure will lead to better predictive capabilities for blood flow through complex geometries, margination of platelets and targeted carriers, and new design tools for microfluidic devices. They can be applied to a large class of particulate flow problems. Besides fluid mechanics, the proposed numerical methods and the computational infrastructure can be applied to solve partial differential equations that arise in various other disciplines. The research outcomes will have an impact on a broad spectrum of disciplines in sciences and engineering."
"1521158","Gaussian-Localized Polynomial Approximation: A Well-Conditioned Spectral Method  for Solving Partial Differential Equations in Complicated Domains","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","08/10/2017","John Boyd","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Leland Jameson","08/31/2019","$159,087.00","","jpboyd@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1271","9263","$0.00","Radial basis functions (RBF) are a numerical technology that has proven to be of great value in many fields. For example, three-dimensional laser scanners convert objects, such as a human face, into a ""point cloud,"" that is, measurements of the position of points on the face. RBF interpolation connects the dots into a smooth surface so that face appears as a recognizable face instead of a cloud of unconnected markers. RBFs have been applied to solve the partial differential equations of fluid flow so as to track hurricanes and predict weather, tidal flows in harbors, combusting flows in an automobile engine, and so on. Unfortunately, RBFs also have flaws. Calculations scale poorly to a large number of degrees of freedom, round-off errors can turn a forecasting model into a useless random number generator, and poor accuracy is sometimes present in problem classes where RBFs have hitherto been a great success. One goal of this research project is to understand RBFs at a deeper level. Why do they work so well (much of the time)? Why do they triumph when similar polynomial-based methods fail? What is the relationship between RBFs and polynomials? This project will explore the foundations of RBFs to better delineate their domain of application, improve performance where feasible, and potentially mark some application domains as unsuitable for RBFs. <br/><br/>To cope with their shortcomings and to also understand RBFs at a more fundamental level, the PI will intensively study RBF-substitutes: these are products of polynomials with Gaussians, equivalent to extending the infinite interval basis of Hermite functions to interpolation and PDE-solving on a finite interval. Earlier work of the PI established a rigorous convergence-and-error theorem and also numerical comparisons showing the superiority of Hermite functions to RBFs in some applications. Conventional single-domain pseudospectral methods fail unless the domain is a rectangle or ellipse, a so-called tensor product domain. The PI plans to extend these Hermite pseudo-RBFs to solve multidimensional PDEs in geometrically-complicated domains using irregular grids, problems where RBFs are sometimes good and sometimes failures. Such complicated domains include a telescope with a hexagonal lens or an ocean ringed with bays and pierced with islands. Hermite functions and RBFs are easy to program and therefore ideal for preliminary design, classroom modeling, and complementing and enriching theory. An applied goal of the project is to advance numerical rapid prototyping, that is, to devise algorithms that, despite complicated domain boundaries, combine brevity of code with spectral accuracy."
"1542183","Polytopal Element Methods in Mathematics and Engineering; October 26 - 28, 2015; Atlanta, GA","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","07/30/2015","Chunmei Wang","GA","Georgia Tech Research Corporation","Standard Grant","Leland Jameson","08/31/2016","$25,000.00","Andrew Gillette","chunmei.wang@ufl.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1271","7556, 9263","$0.00","This award supports participation in the conference ""Polytopal Element Methods in Mathematics and Engineering,"" held October 26-28, 2015, at the Georgia Institute of Technology, Atlanta, GA. This conference will promote communication among the many mathematical and engineering communities currently researching polytopal discretization methods for the numerical approximation of solutions of partial differential equations.  A variety of distinct polytopal element methods have been designed to approximate solutions of the same types of modern engineering problems, but a workshop-type environment is required to foster a community-wide understanding of the comparative advantages of each technique and to develop a set of best practices regarding implementation.  The grant funds will be used to support the attendance of Ph.D. researchers and graduate students, with emphasis on supporting recent Ph.D. recipients and researchers who are members of under-represented groups in this rapidly developing research area.  More information on the conference is available at http://www.poems15.gatech.edu.<br/><br/>Robust and efficient methodologies for the numerical approximation of the solutions of partial differential equations are essential for the characterization and quantification of many physical phenomena. Discretization of solutions with respect to simplicial and cubical meshes has been studied for decades, resulting in a clear understanding of both the relevant mathematics and computational engineering challenges.  Recently, there has been both a desire and need for an equivalent body of research regarding discretization with respect to generic polytopal meshes, typically a mesh of convex polygons in 2D or a mesh of convex polyhedra in 3D.  Methodologies accommodating polytopal meshes include virtual element, weak Galerkin, mimetic finite difference, generalized barycentric coordinate, and compatible discrete operator methods.  These methods have been applied to diffusion modeling, Stokes flow, elasticity,  Maxwell's equations,  eigenvalue problems, and other modeling problems.  Many of the approaches and implementations have only been developed in the past few years, generating a number of open questions in the field.  This conference will help the research community identify the most important results and most pressing needs in this area from both theoretical and practical standpoints."
"1522586","Innovative Weak Galerkin Finite Element Methods with Application in Fluorescence  Tomography","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/22/2015","Chunmei Wang","GA","Georgia Tech Research Corporation","Standard Grant","Leland Jameson","08/31/2016","$98,621.00","","chunmei.wang@ufl.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1271","8990, 9263","$0.00","Fluorescence tomography (FT) is an emerging three-dimensional optical imaging modality that uses in vivo noninvasive depth-resolved localization and quantification of fluorescent-tagged inclusions. FT techniques have been extensively employed in early cancer detection, guidance of tumor resection, and drug monitoring and discovery. This research project aims to develop new numerical methods for computational problems arising in FT imaging. Besides fluorescence tomography, the numerical methods can be applied to solve partial differential equations that arise in various other disciplines. The models, theory, and computational methods under development in this project will be of great value in the fields of digital image processing, medical imaging, and numerical analysis, with direct applications in broad areas such as digital image and even construction industries. Students will be trained in this project.<br/><br/>The goal of this research project is to develop and analyze efficient numerical methods -- weak Galerkin (WG) finite element methods -- to address challenges posed by fourth-order problems arising in fluorescence tomography theory. The research will explore algorithmic advancements, new convergence theory, and imaging technology improvement, by: (1) developing robust finite element methods for a fourth order partial differential equation in the primal variable formulation for which no existing method works; (2) establishing a stability and convergence, including superconvergence, theory for the newly developed finite element methods; (3) validating and verifying the methods through collaboration with domain-specific researchers; (4) analyzing a new weak Galerkin mixed finite element method for a fourth order partial differential equation; and (5) developing application-oriented software packages, tested and validated with collaborators in the area of FT."
"1522617","Modeling complex properties of material interfaces: from quantum and atomic to macroscopic scales","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","09/01/2015","Xiantao Li","PA","Pennsylvania State Univ University Park","Standard Grant","Leland Jameson","08/31/2018","$215,000.00","","xli@math.psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1271","9263","$0.00","This research will advance a computational method that starts at atomic and quantum-mechanical level, and bridges the significant gap between atomistic models and macroscopic properties by deriving a reduced model that only involves the degrees of freedom near the material interfaces. This ultimately permits the study of collective properties of materials on much larger scales based on first principle. Results of the research will be incorporated into graduate courses and undergraduate summer research programs.<br/><br/>Material properties are mostly determined by the underlying micro-structures. Geometric interfaces, such as grain boundaries and precipitates, represent some of the most interesting and important material structures. The goal of this project is to develop highly accurate computational tools for simulating and understanding the roles of interfaces in material properties. The advent of modern computing capability has changed the qualitative nature of much of the material modeling effort. In particular, computer models that rely on atomic scale interactions have emerged as a popular approach. In most cases, however, direct atomistic simulations are still limited to small systems with simple geometry, and they are unable to deal with the realities of the systems that they are supposed to describe. With an appropriate mathematical reduction method, this research will enable large-scale material simulations while still retaining the accuracy of atomic and electronic level descriptions."
"1522537","Collaborative Research: A New Three-Dimensional Parallel Immersed Boundary Method with Application to Hemodialysis","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/11/2015","Suchuan Dong","IN","Purdue University","Standard Grant","Leland Jameson","08/31/2019","$90,000.00","","sdong@purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1271","9263","$0.00","Fluid-structure interaction problems involving thin-walled structures are ubiquitous in biological and engineering applications. However, to date an efficient and effective technique, and a computational capability, for modeling and simulating the interactions between fluids and thin-walled structures are still sorely lacking. The investigators aim to design a new three-dimensional parallel immersed boundary method for computational simulation of fluid-thin-walled-structure interactions in a generic setting and apply it to blood flow past patient-specific distal anastomosis of arteriovenous grafts (AVG), which are essential to blood access of hemodialysis for numerous patients with end-stage renal disease. The new method, which will significantly broaden the applicability of immersed boundary methods, will be particularly valuable to the mathematical biology community for computational studies of vascular diseases such as vascular intimal hyperplasia, aneurysm, and atherosclerosis. Compared to existing models, the proposed computational model is more physiologically realistic: the simulation accommodates deformation of the vein/graft with the pulsatile blood flow, and it incorporates the small yet finite thickness of the vein/graft walls into the model. New computational results will clarify existing contradictory results in the literature regarding the force/flow characteristics near the distal AVG anastomosis and thus lead to a greater understanding of AVG-associated vascular intimal hyperplasia. The new method under development in this project will be generic and applicable to numerous significant problems in engineering, including parachute opening and novel design for street/highway signs. The studies will also enhance the understanding of vascular intimal hyperplasia due to dialysis, which may inspire the creation and development of novel vascular devices to prolong the patency rate of AVGs. This will not only improve quality of life for patients, but also offer savings in dialysis-related healthcare costs. The associated research and education activities will provide multidisciplinary training and research opportunities in mathematics, biology, scientific computing, fluid/solid mechanics, blood flows, and vascular disease for graduate students and undergraduates. The open source implementation of the new method will enable the fluid-structure-interaction community to dramatically increase their research productivity. <br/><br/>The investigators will develop numerical methods to improve computational capability for fluid-thin-walled-structure interaction in three dimensions. They approach this type of problem by integrating several components: a structural component based on the high-order spectral/hp element technique, a fluid component based on the lattice Boltzmann method, and the coupling of the fluid and structure through the framework of the immersed boundary method. The goal of this project is three-fold: 1) Develop a three-dimensional IB-based method for fluid and thin-walled structure interactions in a general setting. The method will account for Newtonian and non-Newtonian fluids, material nonlinearity, and geometric nonlinearity. 2)Design, develop, and implement novel parallel algorithms for the new 3D method on hybrid CPU-GPU linux clusters. 3) Apply the new parallel method to model and simulate blood flow past the distal anastomosis of arteriovenous graft for hemodialysis using patient-specific data.  The investigators' outreach activities will inspire high school students to consider careers in mathematical and computational sciences and raise public awareness for the dire consequences of end-stage renal disease, its associated healthcare costs, and the important roles mathematics and scientific computing play in studying disease and promoting health."
"1547743","Workshop on Software and Applications of Numerical Algebraic Geometry","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","09/02/2015","Jonathan Hauenstein","IN","University of Notre Dame","Standard Grant","Leland Jameson","08/31/2017","$19,020.00","Andrew Sommese, Charles Wampler, Silviana Amethyst","hauenstein@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","1271","7556, 9263","$0.00","Recent developments in software for solving nonlinear polynomial systems may provide many scientists and engineers the tools needed to solve key problems in their research. To facilitate the training necessary to make best use of these powerful tools, the PIs will run a workshop May 23-25, 2016 at the University of Notre Dame. In addition to training, the workshop will also provide cross-fertilization between the mathematical sciences and other areas of science and engineering by bringing together a diverse group of researchers and software developers to exchange both knowledge and challenges. These interactions will spur practical research in applications while highlighting needs for future algorithm development. <br/> <br/>Since they arise in many areas of science and engineering, several approaches have been developed for solving polynomial systems. The software package Bertini, which is being redeveloped in C++ and Python for modularity and scriptability, implements key numerical algebraic geometric algorithms for solving and manipulating polynomial systems.  This workshop will bring together a wide range of computational scientists and practitioners to discuss recent progress and current challenges in software, algorithms, and applications of numerical algebraic geometry. This interdisciplinary workshop will include small group discussions, hands-on demonstrations of Bertini, and sessions focused on developing Bertini modules related to participants' particular interests."
"1522645","Geometric PDEs Based Methods for Analyzing Point Clouds in 3D and Higher","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/28/2015","Rongjie Lai","NY","Rensselaer Polytechnic Institute","Standard Grant","Leland Jameson","07/31/2019","$157,910.00","","lai249@purdue.edu","110 8TH ST","Troy","NY","121803522","5182766000","MPS","1271","9263","$0.00","Recent advances of sensor and data collection techniques have led to ""data deluge"" from wide applications in science and engineering. In order for data scientists to make sense of this massive amount of data, it becomes increasingly important to develop new tools in both theoretical and computational point of views for data representation, analysis and mining. In practice, it is natural to represent data as a collection of points sampled from a low dimensional manifold in a high dimension space. In this scenario, intrinsic geometric information extraction is a crucial step to conduct high level structure understanding of data.<br/><br/>The PI will investigate geometry and partial differential equation (PDE) based methods for analyzing point clouds data in 3D and higher dimension. One objective is to develop and improve numerical methods for solving PDEs on point clouds sampled from Riemannian manifolds. Another one is to utilize solutions of specially designed geometric PDEs on point clouds to conduct intrinsic and global understanding of data sets, such as topological information extraction and point clouds intrinsic comparisons. This investigation will lead to different viewpoints for tackling problems in intrinsic data analysis. The research also contributes to related areas in numerical methods for solving PDEs, computational differential geometry and non-convex optimization. By collaborating with computer scientists and biomedical engineers, applications to real problems will also be explored by the PI."
"1521573","Advances in Robust Multilevel Preconditioning Methods for Sparse Linear Systems","DMS","COMPUTATIONAL MATHEMATICS, Algorithmic Foundations","08/01/2015","07/14/2015","Yousef Saad","MN","University of Minnesota-Twin Cities","Standard Grant","Leland Jameson","07/31/2019","$265,548.00","","saad@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1271, 7796","7933, 9263","$0.00","Scientists and engineers in many disciplines, ranging from mechanical or aerospace engineering to chemistry and economics, need to solve large linear systems  of equations. These systems are typically 'sparse' in that most of their entries are zeros. Linear systems that arise from three-dimensional  physical systems are often exceedingly costly to solve by standard direct elimination, also called direct methods. In such cases, iterative methods, which produce a sequence of approximations to the solution, become mandatory. These methods have made important advances in recent years but their lack of robustness when dealing with a variety of real-life problems remains an issue. Recent research on so-called Preconditioned Krylov Subspace Methods has aimed at achieving a good compromise between generality and efficiency by incorporating techniques from different horizons, including multilevel concepts to improve scalability and adopting ideas from direct solution methods to improve robustness. At the same time that these improvements are  being deployed, new demands from challenging applications as well as from the  new computational environments are making obsolete algorithms and computational codes that often took several decades to mature. The aim of this project is to address these new demands and the challenges that have emerged for iterative methods in recent years, as well as to explore other research issues that are of great practical importance.<br/><br/>This project will explore a class of iterative methods for solving linear systems of equations, emphasizing robustness and scalability issues. The starting point of the proposed research is to investigate a new set of Multi-Level Low-Rank (MLR) approximation techniques within Domain-Decomposition  (DD) type methods. MLR preconditioners, especially within the DD framework have a great potential for a number of reasons. First, because they rely on approximate inverses, these methods tend to be far more robust than their Incomplete LU (ILU) counterparts. As such they can be much more effective than existing methods when dealing with highly indefinite linear systems, e.g., those arising from wave scattering simulations. Second, MLRs do not require factorizations and are excellent candidates for high-performance computers, e.g., ones equipped with Graphical Processing Units (GPUs). Finally, they are easy to update in that it is inexpensive to augment or refine them in order to improve their accuracy in the situation when their observed performance is not satisfactory. Different ways to define low-rank approximations will be explored that are all rooted in the Domain-Decomposition framework and Schur complement techniques. This project will also continue to explore standard multi-level preconditioners, placing a high emphasis on robustness issues. Finally, other important topics related to the impact of high-performance computing on the one hand and to the development  of effective software on the other will be considered. Among the broader impacts of this research the project highlights the dissemination of computational software and the training of students in an area that is of vital and growing importance. In addition, the PI will continue the practice of freely disseminating articles, books, lecture notes, and MATLAB scripts for educational purposes."
"1522657","Superconvergent Hybridizable Discontinuous Galerkin and Mixed Methods for Partial Differential Equations","DMS","COMPUTATIONAL MATHEMATICS","10/01/2015","07/19/2017","Bernardo Cockburn","MN","University of Minnesota-Twin Cities","Continuing Grant","Leland Jameson","09/30/2019","$374,999.00","","cockburn@math.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1271","9263","$0.00","Computer simulation of physical phenomena is a highly valued tool of practical importance in a wide variety of applications in science and engineering. This project studies two new, promising techniques of carrying out these simulations with highly accurate and more efficient algorithms for a wide range of problems of practical interest. They include many applications to aerospace and mechanics (incompressible fluid flow, subsonic and supersonic flow) as well as to civil engineering (solid structures). <br/><br/>This research project aims to introduce a systematic way of obtaining new, competitive discontinuous Galerkin and mixed methods that superconverge on unstructured meshes made of elements of arbitrary shape. This will be done for a wide variety of partial differential equations arising in fluid dynamics (including the incompressible Navier-Stokes equations) and continuum mechanics (including the equations of large deformation elasticity), both linear and nonlinear. The project will also consider adjoint-recovery methods that will result in a very efficient way of obtaining more accuracy than previously thought possible from general finite element approximations. By only doubling the computational effort, the order of accuracy of the approximation will be doubled. In particular, the application of this technique to methods satisfying a Galerkin orthogonality property will result in the quadrupling of the order of accuracy."
"1540135","28th Annual Pacific Northwest Numerical Analysis Seminar 2015 at Western Washington University","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/16/2015","Jianying Zhang","WA","Western Washington University","Standard Grant","Leland Jameson","08/31/2016","$8,000.00","Tjalling Ypma, Yun-Qiu Shen, Tilmann Glimm","jianying.zhang@wwu.edu","516 HIGH ST","BELLINGHAM","WA","982255946","3606502884","MPS","1271","7556, 9263","$0.00","This award supports participation in the 28th Pacific Northwest Numerical Analysis Seminar (PNWNAS), held at Western Washington University, October 17, 2015. As an annual event for the past twenty-seven years, PNWNAS has become one of the primary means for local research groups in numerical computation to engage in scientific exchanges. With invited speakers from industry, government, and academia, PNWNAS 2015 will help keep its participants abreast of recent developments in the field of computational mathematics and numerical analysis, hence fostering research interactions and further promoting mathematical innovation regionally as well as globally. One of the invited lectures will be a designated public presentation accessible to a broader audience on a subject of public interest, such as modeling in the social sciences. This will benefit both the scientific community and the general public in Bellingham. This award will encourage and support the involvement of traditionally underrepresented groups by providing some exclusive financial support for them. <br/>  <br/>A broad range of interdisciplinary research topics will be covered, including mathematical modeling and numerical simulation of large-scale complex systems in applied fields, such as biology, chemical engineering, fluid mechanics, medical science, and materials science.  Topics will also include numerical linear algebra, multi-scale modeling and simulation, stochastic processes, inverse problems, and image processing. Poster sessions presented mainly by graduate students and postdoctoral fellows will provide early-stage researchers with an opportunity to showcase their academic achievements. The goals of PNWNAS 2015 include: to foster the scientific interaction of computational mathematics groups from academia, industry, and government agencies working in a broad range of science and technology; to promote mathematical innovation regionally as well as globally; to facilitate interactions that potentially lead to regional employment and internship opportunities for postdoctoral fellows, graduate students, and undergraduates; and to attract prospective students into the Pacific Northwest's higher education programs in computational and applied mathematics."
"1522526","Computational Methods in Arithmetic Geometry","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/11/2015","Andrew Sutherland","MA","Massachusetts Institute of Technology","Standard Grant","Leland Jameson","08/31/2019","$180,000.00","","andrewvsutherland@gmail.com","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","MPS","1271","7433, 9263","$0.00","This project will develop new computational tools that will allow mathematicians to investigate some major open questions in number theory. The same tools can also be used to help build faster and more secure cryptographic systems, and to construct error-correcting codes that allow data to be reliably transmitted over unreliable networks. Preliminary results indicate that it should be possible to improve the performance and precision of the algorithms involved by several orders of magnitude, which would make it feasible to address a number of questions that are currently out of reach.<br/><br/>The bulk of the project will be aimed at the practical realization of average polynomial-time algorithms for computing zeta functions of arithmetic schemes, with a particular focus on algebraic curves.  These algorithms allow one to efficiently compute the zeta function of the reduction of a fixed arithmetic scheme modulo all primes up to specified bound. This is precisely the computation needed to investigate various questions in arithmetic statistics, and to approximate the associated L-function to high precision. These tools will be used to study Sato-Tate distributions and explicit aspects of the Langlands program. Computational results of the project will be published in open access electronic databases such as the LMFDB (L-functions and Modular Forms Database)."
"1522587","Intrinsically Parallel Spectrum Decomposition Algorithm for Large Eigenvalue Problems","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","08/06/2015","Yunkai Zhou","TX","Southern Methodist University","Standard Grant","Yuliya Gorb","07/31/2019","$150,000.00","","yzhou@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1271","9263","$0.00","Eigenvalue problems are of fundamental importance in a wide range of science and engineering disciplines, including electronic structure calculations in materials science, and clustering, ranking, and matching in data science. Large-scale eigenvalue problems arise naturally from significant modern applications in these materials science and data science computations. However, the associated rapid increase in the dimension of the eigenvalue problems can easily overwhelm existing algorithms. This generates pressing demand for more efficient algorithms, especially those that can scale well on modern supercomputers with many thousands of cores.  This research project centers on designing highly efficient algorithms for solving large-scale eigenvalue problems and implementing them in robust software that can be effectively utilized by other researchers.<br/><br/>The project focuses on the essence of algorithm acceleration for eigenvalue problems, represented by spectrum filtering (both by polynomial filtering and by rational function ""preconditioning""). The investigator will study both standard and generalized eigenvalue problems, which often arise from first-principles calculations.  The project will use a tailored filtering method as the first step for spectrum estimation and develop a practical spectrum decomposition algorithm that is intrinsically parallel. The spectrum decomposition method is designed to overcome the main difficulties encountered in state-of-the-art spectrum slicing algorithms. The investigator will develop novel approaches such as adaptive Ritz iteration and adaptive selection of (locally) optimal shifts; both techniques are important for achieving efficiency as well as robustness for intrinsically scalable methods. The research will significantly extend the forefront of practical methods for solving large-scale eigenvalue problems. The project will provide useful algorithms and software to facilitate cutting-edge research that requires solving increasingly larger eigenvalue problems."
"1550666","Risk and Resiliency of the Electric Power Grid: Mathematical and Statistical Challenges","DMS","COMPUTATIONAL MATHEMATICS","07/15/2015","07/20/2015","Barry Lee","TX","Southern Methodist University","Standard Grant","Leland Jameson","06/30/2017","$13,860.00","","barryl@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1271","7556, 8396, 8399, 8611, 9263","$0.00","The electric power grid is dramatically changing as new technologies and policies are being implemented. These changes will affect the quality of delivered electric power to the consumer, and will pose new risks and alter the resiliency of the power grid system. To understand and mitigate these risks and to strengthen the resiliency, mathematical and statistical techniques will be invaluable. The goal of this workshop, held July 14, 2015 in Arlington, Virginia, is to bring together the representatives from the power grid engineering, mathematics, and statistics communities to discuss some of the challenges in the power grid.<br/><br/>The workshop involves participants from the academic-based mathematical and statistical sciences and the U.S. Department of Energy (DOE). Power grid experts from DOE will share their knowledge, experience, and visions of the current and future power grid, particularly for large-scale, real power grid networks. This will ensure that realistic power grid challenges are addressed by the mathematics and statistics communities. The workshop will include several talks from power grid experts and mathematicians who have investigated topics related to the power grid. Topics to be included are stability analysis, uncertainty quantification, model reduction, graph networks, and algorithmic development for the grid. Floor discussions will be encouraged, both for elucidating the problems to math/stats researchers new to this field and for discussing math/stats directions in power grid research. At the end of the workshop, the organizers will produce a report for the agencies and participants, summarizing the talks, discussions, and possible research directions."
"1419023","Collaborative Research: Data selection for unique model identification","DMS","COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","01/01/2015","12/16/2014","Brandilyn Stigler","TX","Southern Methodist University","Standard Grant","Leland Jameson","12/31/2017","$100,000.00","Elena Dimitrova","bstigler@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1271, 7334","9263","$0.00","While this is the age of big data, there is still a question of whether more data translates to more knowledge. Particularly when generating data is expensive or time consuming, as it is often the case with clinical trials and biomolecular experiments, the problem of identifying information-rich data becomes crucial for creating models that can reliably predict the outcome of future experiments. Few results have been published on the amount of necessary data, and currently there are no methods for generating specific data sets which would unambiguously identify a predictive model. This research project addresses fundamental mathematical and computational questions in data selection.  The theoretical results will advance the fields of design of experiments and network inference through the determination of criteria for selecting data sets to uniquely identify models. The algorithms under development will serve as a guide for experimentalists in determining the data that are needed to identify the structure of a network of interest. Such knowledge has the potential to drastically reduce wasted resources that arise from too much data with too little information. Graduate students will participate at the appropriate level in each component of the project. Such an experience will provide possible topics for M.S. or Ph.D. dissertations and will very likely inspire career-long involvement of the participants in the STEM disciplines.<br/><br/>As a first step towards developing a complete theory, the PIs will focus on models described by finite-valued nonlinear polynomial functions. Finite-state multivariate polynomial functions have successfully been used to model complex networks from discretized data; however, few results have been published on the amount of data necessary for such models, with the majority applying to Boolean models only. The PIs will address the issue of the minimality and specificity of data to uniquely identify discrete polynomial models by developing the appropriate theory, implementing the theoretical results as algorithms, and applying the algorithms to important physical systems. The proposed work will also increase the utility of polynomial dynamical systems as models of complex networks by establishing the minimal amount of the data for unique model identification."
"1438451","Advanced Discretization Techniques and Applications (ADTA)","DMS","COMPUTATIONAL MATHEMATICS","01/01/2015","01/07/2016","Yalchin Efendiev","TX","Texas A&M University","Standard Grant","Junping Wang","12/31/2015","$30,000.00","Raytcho Lazarov, Ke Shi","efendiev@math.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1271","7556, 9263","$0.00","The Advanced Discretization Techniques and Applications (ADTA) workshop will be held at Texas A&M University, May 11-15, 2015. During the last few decades, finite element methods (FEM) have come to be among the most important tools for numerically approximating solutions of partial differential equations. FEM are a powerful research tool in subsurface engineering, structural engineering, failure analysis, earthquake engineering, progressive collapse, biophysics, electromagnetics, acoustics, solar and fuel cell, batteries, and material sciences, to mention but a few. Despite considerable work on their analysis, many aspects of the method continue to be improved. The novel techniques allow us to pursue comprehensive understanding and reliability of the numerical results, to improve the accuracy, reduce computational costs, and guarantee product credibility. The objective of this workshop is to bring researchers working on advanced discretization techniques and multiscale algorithms and promote interaction among these different groups. By bringing these researchers together, we expect to have an impact in many areas and promote progress in designing efficient multiscale discretization techniques for a wide range of problems. By holding this conference at Texas A&M University, a university traditionally strong in engineering and natural sciences, we expect to disseminate the know-how to researchers not only in mathematics, but also in other disciplines. We will actively pursue participation by non-mathematicians. This conference will therefore help researchers and practitioners improve their theoretical tools and deepen their knowledge of modern trends in numerical analysis.<br/><br/>The workshop will cover a range of topics in finite element methods (FEM) and applications. Recent advances in finite element methodologies, in particular, the discontinuous Galerkin method (DG-FEM), weak Galerkin finite element methods (WG-FEM), and multiscale methods, will be some of the central workshop topics. The workshop will bring together researchers working on various finite element discretization techniques and multiscale methods and promote interaction among different groups."
"1459887","Copper Mountain Conference on Multigrid Methods","DMS","COMPUTATIONAL MATHEMATICS","06/15/2015","06/03/2015","Steve McCormick","AZ","Front Range Scientific Computations, Inc.","Standard Grant","Junping Wang","05/31/2016","$10,000.00","Thomas Manteuffel","stevem@colorado.edu","8865 E CALLE BUENA VIS","SCOTTSDALE","AZ","852558364","3035541232","MPS","1271","7556, 9263","$0.00","Copper Mountain Conference On Multigrid Methods, Copper Mountain, Colorado, March 22-27, 2015.This grant is to support participation in the 2015 Copper Mountain Conference on Multigrid Methods.  The funding from this award is specifically for students, women, and scientists who are members of underrepresented groups.  The Copper Mountain Conferences have graduate students forming a very large fraction of the attendees (typically 30%-40%), many of them full participants who co-author papers and give presentations.  Women and minorities are growing fractions of the attendees, but further increases are desired, so this award also targets their participation. Support for the students, women, and minority scientists from this award is in the form of reduced registration fees and local and travel expenses. A hallmark of this conference is a ""Student Paper Competition,"" which draws entries from a significant fraction of the student attendees and results in extraordinarily high-quality papers on scientific discovery from the students, working in tandem with and under the guidance of their faculty advisors.  These results, like the conferences as a whole, span wide-ranging and important theoretical and applications areas, such as techniques of convergence analysis, implementation and development of mathematical software, and use of such ideas in novel settings, including advanced computer architectures and new applications such as uncertainty quantification. The aim here is to continue the increase in participation by members of these underrepresented groups so that the multigrid discipline and science in general are improved by the professional development of these talented researchers.<br/><br/>The Copper Mountain Conferences form arguably the premier conferences in two closely related mathematical fields: iterative and multigrid methods. These two fields provide computational support for numerical simulation of a very wide host of human endeavors, including environmental and energy research, medical and biological applications, and many other areas critical to the U.S. and international science and engineering community. This award supports the participation of students, women, and scientists who are members of underrepresented groups at the 2015 Conference. The Conferences traditionally work to ensure the future vitality of the fields of iterative and multigrid methods by facilitating development and nurturing of a community of capable graduate students and entry-level scientists. Through their egalitarian structure, with no invited speakers and all talks of equal length, the Conferences provide mechanisms for young people to meet each other and all participants in a relaxed yet scientifically rigorous setting; these mechanisms include topical tutorials, themed evening workshops, and access to the broad representation of participants from academia, national laboratories, and industry. The Conferences have a tradition of a very high level of student participation (typically 30-40% of attendees), and will cultivate this through supporting students' local and travel expenses.  Emphasis is also placed on engendering diversity through support of women and minority scientists. These conferences bring together the world's leading practitioners in these critical fields and result in high-level publications, effective applications codes, and the establishment of long-term collaborative research partnerships."
"1522777","A High Order Discontinuous Galerkin Multi-Scale Approach for Kinetic-Hydrodynamic Simulations","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/22/2015","Jing-Mei Qiu","TX","University of Houston","Standard Grant","Leland Jameson","05/31/2018","$235,826.00","","jingqiu@udel.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271","7237, 9263","$0.00","This research project will develop novel numerical methods for simulation of the dynamics of rarefied gas.  Compared with the widely used Monte Carlo approach, the algorithm under development will be able to more accurately capture complicated solution structures in long-time simulations. Moreover, physically conserved quantities such as mass, momentum, and energy can be exactly preserved at the discrete level. The new algorithm also has the potential to be extended to a broader class of applications such as plasma physics, astrophysics, and semi-conductor device simulation.  Students will be trained through involvement in the research project.<br/><br/>This project aims to develop a very high order mesh-based multi-scale numerical approach to modeling rarified gas dynamics between the kinetic and hydrodynamic regimes. The approach is based on the so-called micro-macro formulation of the kinetic equation, which involves a natural decomposition of the problem into equilibrium and non-equilibrium parts. The high order spatial accuracy is achieved by a nodal discontinuous Galerkin (DG) finite element approach, and the high order temporal accuracy is achieved by globally stiffly accurate implicit-explicit Runge-Kutta methods.  Due to deliberate design and considerations of the hydrodynamic asymptotics, the scheme under development becomes a DG method with explicit RK time discretizations for the Euler system in the zero limit of the Knudsen number, and a local DG discretization of the Navier-Stokes equations for a simplified BGK collision operator in a formal asymptotic analysis. Such a local DG method is similar in spirit to classical approaches based on a mixed formulation of the equations. The new scheme will be tested on problems at kinetic-hydrodynamic scales and compared with the results from the simplified BGK model, its ellipsoidal statistical (ES-BGK) extension, as well as with results from the macroscopic hydrodynamic models. The project will also study numerically the boundary layer for kinetic simulations when a diffusive hydrodynamic limit is considered."
"1521600","Collaborative Research: An Integrated Approach to Convex Optimization Algorithms","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/16/2015","Anne Gelb","AZ","Arizona State University","Standard Grant","Leland Jameson","02/28/2017","$27,735.00","","annegelb@math.dartmouth.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1271","9251, 9263","$0.00","Image reconstruction and feature extraction have been important aspects in various applications such as medical resonance imaging (MRI) and synthetic aperture radar (SAR). However, these procedures involve challenges. Different applications may vary in data acquisition (sampling) domains, levels of detail required, and processing domains for the features of interest. The data acquisition is usually under-prescribed and noisy. The sampling domains and/or processing domains may not be well suited for the underlying question. All of these make the problems ill-posed, and various regularization techniques are necessary to study the problems by formulating them as convex optimization models. This project will develop an integrated framework of investigating such convex optimization models. The project will provide graduate students with opportunities for training through research involvement and will prepare them for careers in science and engineering. <br/><br/>The PIs aim to propose a systematic way of evaluating various regularization techniques in such models, conduct a rigorous numerical analysis of the models, and develop efficient numerical algorithms of solving the models. Specifically, the PIs will address the following technical questions: (1) What constraints must be placed on the collected data in order to construct a numerically robust approximation to the underlying function? (2) How quickly and in what sense does the approximation converge? (3) Are the corresponding numerical algorithms developed for the fidelity and regularization terms viable? (4) How well are perturbations from the original data tolerated? The project aims to provide answers to all of these questions."
"1522252","Collaborative Research: Variational Structure Preserving Methods for Incompressible Flows: Discretization, Analysis, and Parallel Solvers","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/11/2015","Maxim Olshanskiy","TX","University of Houston","Standard Grant","Leland Jameson","08/31/2018","$89,980.00","","molshan@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271","8396, 8609, 9263","$0.00","The goal of this project is to develop improved models and numerical methods that advance the state of the art for incompressible fluid flow simulation. The key ideas are the better enforcement of geometric and physical laws in the computer algorithms, building a solid mathematical framework for the models and methods developed, and the devising of algorithms that will allow for efficient implementation on supercomputers. Although the simulation of fluid flow is a critical subtask in a wide spectrum of engineering applications, current tools and techniques are often unreliable and it is not uncommon for state of the art methods to take weeks or months (or possibly never finish with an accurate solution), even with thousands of processors, to perform simulations of flows around a car, through a nuclear reactor, or around part of an airplane. The project aims to develop mathematical prediction models and numerical methods that will provide more accurate solutions in a more efficient manner than state-of-the-art methods.<br/><br/>The PIs will construct efficient methods for incompressible flow simulation by constructing models and methods that better adhere to geometric structure and physical conservation laws than modern methods. The key components are to i) construct novel methods that are efficient and can correctly account for vorticity dynamics, and energy, helicity, and mass conservation -- this will require development of efficient boundary conditions and significant analysis to build a solid mathematical framework; ii) develop more efficient algebraic solvers for these methods that can be used on thousands of processors; iii) large scale testing on benchmark problems as well as on application problems with collaborators. Broader impacts include i) developing efficient methods for simulating high speed incompressible flows, which will improve the design process for a wide spectrum of applications in environmental engineering, in cardiovascular simulations, and in atmosphere and ocean sciences; ii) training graduate and undergraduate students through research involvement; and iii) developing large scale, parallel codes to be made publicly available as part of the deal.II library."
"1520886","Collaborative Research:  Numerical Simulation of the Morphosynthesis of Polycrystalline Biominerals","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","08/08/2019","Ronald Hoppe","TX","University of Houston","Standard Grant","Leland Jameson","08/31/2020","$150,000.00","","rohop@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271","7237, 9263","$0.00","Polycrystalline biominerals are thermodynamically stable crystal polymorphs of biogenic minerals featuring stacked layers of crystals with mineral bridges between adjacent layers. The unique crystal texture gives rise to specific material properties such as toughness, corrosion resistance, and temperature resistance, which makes these crystals highly attractive for optical nanostructures (photonic band gaps, diffraction gratings) and for special coatings (e.g., in semiconductor device technology). Therefore, many material scientists are currently trying to realize the synthesis of such biominerals. The aim of this project is to provide both a mathematical model for the crystallization process and algorithmic tools for numerical simulations in order to understand the mechanisms of the process, to enable the experimentalists to optimize their laboratory settings, and thus to pave the way for an industrially relevant production line.<br/><br/>The morphosynthesis of polycrystalline biominerals follows a multistage crystallization process including a polymer-induced liquid-precursor (PILP) phase, the occurrence of spherulites due to nucleation, and the recrystallization of mosaic mesocrystal thin structures. The PILP phase consists of an aqueous solution of the biomineral and an anionic polymer mixed with ethanol and features a liquid-liquid phase separation in terms of polymer-rich PILP droplets in the liquid mixture. The mixing is taken care of by a surface acoustic waves (SAWs) manipulated fluid flow where the SAWs are generated by two tapered interdigital transducers operating in dual mode. The polycrystallization sets in with the formation of spherulites that spread across the substrate to form a uniform spherulitic thin film. Continuous cooling leads to a recrystallization of the spherulitic thin film into a mosaic polycrystalline thin structure. The liquid-liquid phase separation characterizing the PILP phase can be described by a coupled system consisting of the incompressible Navier-Stokes equations and a Cahn-Hilliard equation. For the numerical simulation, the project will use a splitting scheme based on an implicit discretization in time and C0 Interior Penalty Discontinuous Galerkin (C0-IPDG) methods for discretization in space with respect to simplicial triangulations of the computational domain. The research will study the convergence of the splitting method and realize space-time adaptivity by the goal oriented dual weighted approach. As a mathematical model for the polycrystallization the project investigates a phase field model consisting of the dynamic equations for the measure of local crystallinity, the concentration field for the biomineral, the orientation field, and a heat equation for the evolution of the temperature during the cooling process. The equation for the concentration field is a fourth order Cahn-Hilliard type equation. Again, discretizing implicitly in time and by C0-IPDG methods in space, the project will use a splitting method and dual weighted residuals for space-time adaptivity featuring a desired crystallinity at final time as the objective functional. A model validation will be based on experimental data provided by cooperating laboratories and a systematic parameter study will be performed to investigate the influence of various process parameters."
"1522639","Windowed Fourier Methods for Overlapping Domain Approximations","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/16/2015","Rodrigo Platte","AZ","Arizona State University","Standard Grant","Leland Jameson","07/31/2019","$150,001.00","","rbp@asu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1271","9251, 9263","$0.00","Partial differential equations on surfaces arise in many applications, including atmospheric research, evolution of coat patterns in animals, finger print formation, and deformation of elastic solids in fluids, to mention just a few. This research project aims to develop computationally efficient and accurate methods for numerical solution of such equations. The project is driven in particular by problems stemming from atmospheric models and will advance research on novel numerical schemes that are based on windowed Fourier approximations. Objectives include the design of algorithms for solving time-dependent differential equations on spherical geometries and other surfaces, the development of a rigorous mathematical analysis of convergence and stability for such algorithms, the implementation of high performance and scalable solvers, and to make efficient, accurate software available for use by the scientific community.<br/><br/>Windowed Fourier methods are suitable for adaptive and parallel implementations and large-scale computations. The proposed schemes rely on domain decomposition, such as in the cubed sphere, with computations being carried out using fast Fourier transforms. Approximations are obtained on overlapping domains and a global solution is obtained by weighted average. Current high-order and spectral methods used in atmospheric research, such as spectral elements, are constrained by the well-known CFL condition; that is, node clustering in the space domain restricts the discretization in the time domain. Windowed Fourier methods use nearly uniform nodes on each subdomain, allowing for larger time steps when explicit schemes are used for time integration. For large-scale simulations, such as those required in climate prediction for example, each time step incurs significant communication cost at scale. By leveraging better node distributions, larger time steps and spectral accuracy, windowed Fourier schemes are expected to be more efficient than methods currently used in critical applications such as climate and geodynamics."
"1522548","OP: High Order Perturbation of Surfaces Methods for Crossed Surface Plasmon Resonance Sensors: Simulation, Validation, and Design","DMS","COMPUTATIONAL MATHEMATICS","07/15/2015","07/13/2015","David Nicholls","IL","University of Illinois at Chicago","Standard Grant","Leland Jameson","06/30/2019","$179,979.00","","davidn@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1271","7237, 8990, 9263","$0.00","Surface Plasmon Resonance (SPR) sensors are a relatively new yet pervasive technology for measuring biological and chemical interactions. These devices respond to the change in the refractive index of a structure due to the occurrence of a biological or chemical reaction. Such a change modifies the behavior of surface waves at metal-dielectric interfaces within the structure, which are then measured to form the basis for thousands of highly sensitive and effective tests for not only chemical and biological species detection, but also medical diagnosis and environmental measuring.  Indeed, a 2008 survey article (J. Homola, Surface plasmon resonance sensors for detection of chemical and biological species. Chemical Reviews, 108(2):462493, 2008) lists SPR biosensors for Food Quality and Safety Analysis (62 listed), Medical Diagnostics (33 listed), and Environmental Monitoring (21 listed). In the decade since the appearance of this publication the list has grown substantially. This project shall advance the state of the art in the numerical simulation of these SPR sensors. The numerical simulation will be validated and verified through direct collaboration with experimentalists.<br/><br/>The algorithm of ""High Order Perturbation of Surfaces"" (HOPS) is optimal among the wide class of potential schemes for simulating solutions of the relevant model equations. Despite their advantageous properties for the problem at hand, the HOPS methods require further enhancements for their use by engineers. This research project aims to develop several improvements.  First, the project will study extensions to the three dimensional, vectorial, multiply layered configurations of current interest. Second, the project will investigate the possibility of a joint expansion of the scattered fields in both boundary deformation and wavenumber. These latter developments will mandate new rigorous analysis (which this project will deliver) to determine their domain of applicability. Once these additions have been implemented and tested, these algorithms will be vaildated against the laboratory experimental results. After this, the project aims to create an optimization framework to design SPR sensors with enhanced sensing capabilities. This will involve the design of appropriate objective functions, followed by the efficient interfacing of the new forward solvers to state of the art minimization libraries."
"1522289","Enhancing the Periodic Table of Finite Elements","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/12/2017","Andrew Gillette","AZ","University of Arizona","Continuing Grant","Leland Jameson","07/31/2018","$224,998.00","","agillette@math.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1271","9263","$0.00","The frontiers of science are explored today by elaborate computer-based simulations of real-world phenomena.  An essential tool in this endeavor is the finite element method, which has been used over an incredible range of temporal and spatial scales to deepen our understanding of physical behavior. Modern methods rely on a wide variety of finite element ""types,"" each with its own set of challenges and potential benefits.  Many of these element types have been classified by the recently developed and widely distributed Periodic Table of Finite Elements.  While the table characterizes the most well-known finite elements, some of the included types have never been implemented in practice, due in part to a missing piece of mathematical theory.  Further, the mesh element geometries presented in the table do not include a number of key shapes that are used throughout national laboratories and academia, implying that a need exists for a broader characterization of finite element types.  This project will address these mathematical gaps in knowledge, thereby promoting best practices in finite elements for computation in a wide range of contexts.<br/><br/>In conjunction with collaborators and student researchers, the PI will carry various tasks aimed at enhancing the usefulness and scope of the framework defined by the Periodic Table of Finite Elements. These tasks include the construction of canonical sets of local basis functions for the serendipity family of finite elements, the description of finite element families for key geometries employed in practice but not appearing in the table, and proofs that certain finite element families have minimal dimension in a mathematically precise sense. Proof techniques and basis constructions will involve the use of generalized barycentric functions, Whitney differential form spaces, and gradient bounds for error estimation.  The research activities will be accompanied by an outreach program through the Math Teachers' Circle Network and the American Institute of Mathematics."
"1552238","Computation of crowded geodesics on the universal Teichmueller space for planar shape matching in computer vision","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","06/30/2015","09/15/2015","Akil Narayan","UT","University of Utah","Continuing Grant","Leland Jameson","07/31/2017","$195,562.00","","akil@sci.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","MPS","1266, 1271","9150, 9263","$0.00","Quantifying the (dis)similarity between two shapes is a central problem in computer vision. One distance metric on the space of planar shapes is realized by identifying this space as a subset of the Universal Teichmueller Space, and equipping it with the Weil-Petersson metric. This results in a metric that is scale- and translation-invariant on shapes, and has unique geodesic flow between two shape endpoints. The work of this proposal develops robust computational methods for the computation of metric distances and geodesics between shapes on this space. The major difficulty lies in computations involving ""crowded"" shapes, i.e., those with elongated, winding, or extended protrusions. Such shapes stymie finite-precision computations because direct algorithms suffer from severe roundoff error. The major thrusts of this proposal develop algorithmic methodologies to address roundoff error and related issues: The Zipper conformal mapping algorithm will be augmented to produce accurate conformal maps for crowded shapes. The velocity field representation on a geodesic will be rewritten into a form that is resistant to roundoff error. The geodesic equation will be transformed into a expression that takes advantage of the aforementioned velocity field transformation, and can effectively flow between crowded shapes. The final phase of this project will demonstrate accurate geodesic flow and distance computations between crowded shapes. The methods developed under this project can be applied to several related problems in scientific computing: solutions to differential equations on irregular geometries through conformal mapping, conservative integration methods with ill-conditioned particle systems, and moving-mesh kernel approximations.<br/><br/>The work of this project can contribute to far-reaching applications in scientific and computer vision problems: automated object recognition (e.g. projectile identification), outline classification (determination of an animal's species), medical imaging (usage of MRI to diagnose dementia and related diseases), and artificial intelligence (visual recognition and interpretation) to name a few. All computational deliverables (computer code, example simulations, documentation) will be made publicly available. Through the engagement of students in related research tasks, this project will contribute to the educational development of future engineers, mathematicians, and computer scientists."
"1521930","Computational Biofluids in Physiology","DMS","COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","05/15/2015","05/05/2015","James Keener","UT","University of Utah","Standard Grant","Mary Ann Horn","04/30/2016","$22,500.00","","keener@math.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","MPS","1271, 7334","7556, 9150, 9263","$0.00","A conference entitled, ""Computational Biofluids in Physiology 2015 (CBP 2015)"", will be held at the University of Utah on May 14-15, 2015.  Computational biofluids is an interdisciplinary field at the intersection of mathematics, physics, computing, engineering, and biology. Although classical biofluids problems such as low Reynolds number locomotion have been studied theoretically for decades, the studies were limited to problems that were analytically tractable.  These problems can now be probed with greater depth and biological relevance using computational methods.  The power of personal computing combined with development of new algorithms has led to a revival of research in biofluids. Problems in physiology relating to the function of heart valves, the mechanics of blood clotting, and the mechanics of motile cells are also now being investigated using novel applied mathematical techniques and computational models. Integrative research on these biological and physiological processes has the potential to lead to medical advances such as the prevention of heart attacks, strokes, and cancer metastasis.<br/><br/>The applied mathematics community has played a central role in the resurgence of biofluids by developing theories, algorithms, and models aimed at answering fundamental biological and physical questions.  While some in this community focus primarily on computation and numerics, others stay closer to the biology or physics of the problem at hand.  This meeting on ""Computational Biofluids in Physiology 2015"" will bring together mix of senior and junior researchers from mathematics, computational science, and the life sciences in a small, exceptionally interactive setting.  Invited speakers and poster contributors will present work on computational methods for fluid/structure interaction, cellular mechanics, locomotion, and systems physiology."
"1522555","Point and state constrained optimal control parabolic problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","09/01/2015","Dmitriy Leykekhman","CT","University of Connecticut","Standard Grant","Leland Jameson","08/31/2018","$99,999.00","","dmitriy.leykekhman@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1271","9263","$0.00","The PI will investigate a class of optimal control problems with pointwise controls. These problems are classical and have a wide range of applications, for instance in water waste treatment, river pollution, calcium waves in a heart cell, and noise control. The finite element method is the most popular method to solve such problems numerically, but there are very few results in this area on a priori error estimates. The difficulty lies in the roughness of the solutions of the underlying equations.  The aim is to understand how well such rough solutions can be approximated numerically and to obtain sharp a priori error estimates. This area of research requires many tools from classical and modern analysis, partial differential equations, finite element methods, and optimization, and offers a wide variety of exciting problems well suited for research and educational purposes.<br/><br/>The PI will study problems with pointwise controls and/or state constraints. These problems are usually modeled by Dirac delta functions in the source term, and control and state variables are in some nontrivial admissible sets. Analysis of such problems is challenging due to low regularity of solutions of the state equations. In the presence of state constraints, the Lagrange multipliers are merely measures and solutions of the adjoint equation have very low regularity as well. To show optimal error estimates one has to establish sharp best approximation properties of the finite element solution in non-standard norms, such as pointwise in space and global in time. Such error estimates are not available in the finite element literature and need to be developed. The key idea in obtaining these sharp error estimates in such non-standard norms is first to show discrete maximum regularity results for a class of fully discrete discontinuous Galerkin methods. These new results will provide a deeper insight into numerical methods commonly used to solve such problems and may also be useful for other problems where anisotropic spaces are used. Presently, there are very few results on finite element error estimates on anisotropic spaces and those sharp, best approximation type results will advance the current finite element knowledge."
"1558744","Algorithms and Computation for Rare Events in Complex Systems","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, DYNAMICAL SYSTEMS","08/01/2015","08/25/2015","Qiang Du","NY","Columbia University","Standard Grant","Rosemary Renaut","05/31/2017","$147,196.00","","qd2125@columbia.edu","202 LOW LIBRARY 535 W 116 ST MC","NEW YORK","NY","10027","2128546851","MPS","1266, 1271, 7478","9263","$0.00","The project is concerned with mathematical and computational issues related to the simulation and analysis of equilibria, metastable and transition states and  minimum energy paths for complex energy landscapes of practical interests, and associated stochastic dynamics. The research to be carried out is closely motivated by applications in a number of areas of federal strategic interests: the development of effective algorithms and codes is a crucial part of high-performance computing, and numerical methods and software tools to be developed may be potentially useful for effective computational  materials and drug design.<br/> <br/>The principal investigator will carry out interdisciplinary research that encompassing subjects like computational mathematics, physics, information, materials and biological sciences. He will focus on new algorithmic development and analysis which have the potential to significantly improve the usual practice on the modeling and simulation of rare events. He will consider some specific and important applications, including systems of interacting particles and interfaces in geometrically confined and frustrated configurations or deformable geometry which arise in many areas of physics, chemistry and biology (such as formation of nano-clusters, bimolecular conformation, vesicle mediated interactions, and critical nucleation in solid state transformations). He will attempt to draw strong connections with some of the algorithms developed by practitioners implemented in existing software codes such as those for first principle calculation and computational chemistry.  Most of the problems involved in the research project are associated with either infinite dimensional spaces such as deterministic or stochastic partial differential equations or finite dimensional spaces with high dimensions (discretization of differential equations or particle systems involving a large number of particles), which lead to many computational challenges. Various mathematical and numerical issues will be studied, ranging from efficient local saddle point search and its robust numerical implementation to rigorous analysis and effective multiscale simulations of relevant dynamics and rare events. It is expected that the progress made during the project will have a broad impact on the community interested in the study of rare events. The project will also contribute to education and training as it will provide students valuable training ground and research experience in an interdisciplinary environment. Much effort will be devoted to promoting active engagement of student participation at all levels and integrating research findings into teaching and training."
"1522799","Collaborative Research:  Random Dynamics on Networks","DMS","COMPUTATIONAL MATHEMATICS","08/15/2015","07/31/2017","Daniel Tartakovsky","CA","University of California-San Diego","Continuing Grant","Leland Jameson","11/30/2017","$250,000.00","","tartakovsky@stanford.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271","9263","$0.00","Transport and distribution networks come in a number of forms, from animal cardiovascular and respiratory systems to communication and industrial infrastructures. Practical issues abound: prediction of local spikes, estimation of perfusion, and impact of structural changes such as vessel occlusion. The complexity of such phenomena can be illustrated by the well-known Braess' paradox: adding links to a transportation network might not improve the operation of the system! In spite of recent successes, our understanding of network flows is usually limited to small deterministic problems, while most applications correspond to large uncertain ones. The goal of this project is to enable improved predictions in biological and technological transport and diffusion networks. For instance, can one predict how cerebral blood flow will be affected if one of the carotids becomes narrow or blocked? Will the vasculature allow for re-routing? If so, with what probability and how fast? <br/> <br/>The main challenge in this research project is the presence of uncertainties. For many applications, only partial information about the systems is available. For instance the size or even the presence of a specific vessel might be uncertain or the status of a router unknown. The analysis of such problems requires the creation of novel mathematical tools and numerical methods to describe how uncertainties propagate through vast and complex networks. The computational tools to be constructed will provide information, usually probabilistic in nature, regarding phenomena that are difficult, expensive, or impossible to measure."
"1522675","Collaborative Research: Riemann-Hilbert Problems and Riemann Surfaces: Computations and Applications","DMS","COMPUTATIONAL MATHEMATICS","07/15/2015","06/27/2017","Stefan Llewellyn Smith","CA","University of California-San Diego","Continuing Grant","Leland Jameson","06/30/2019","$274,995.00","","sgls@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271","9263","$0.00","Riemann-Hilbert problems (RHPs) arise in a plethora of applications, varying from equations describing tsunamis to the understanding of nuclear energy. In its most basic form, a RHP determines a function that jumps in a prescribed way along a curve in the plane and has specified behavior far away from where the jump occurs. Such problems were first posed by Riemann and later by Hilbert at the end of the 19th century. Their study has been at the forefront of pure and applied mathematics. Until recently, little effort had been devoted to the actual computation of solutions of such problems. This research project extends recent work in carrying out numerical investigations. It is anticipated that major advances will be made in the solution of RHPs, allowing for the increased understanding of tsunamis, fast optical communication, and other physical phenomena.<br/><br/>The goal of the project is to develop new computational tools for the solution of RHPs and their extensions. Traditionally, RHPs arise in the context of singular integral equations and the Wiener-Hopf technique. More recently, RHPs have been connected to random matrix theory, nonlinear special functions, and nonlinear wave equations. RHPs may be posed on Riemann surfaces, and nonlinear jump conditions may be specified. Recent developments involving the investigators and collaborators have led to the development of accurate and efficient numerical algorithms for the solution of RHPs, for problems posed on Riemann surfaces, and for the computation of special functions such as the Schottky-Klein prime function. However, many open problems remain, particularly concerning new applications. This project aims to develop new computational methods to solve these problems, with an emphasis on the development of fast and efficient algorithms that can deal with complicated geometries, and to deploy them in applications. The investigators, postdoctoral scholar, and collaborators bring together a unique combination of expertise in the different areas needed to successfully carry out the collaborative project."
"1522792","Multiscale Computations of Time Dependent Highly Oscillatory Systems","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/11/2015","Bjorn Engquist","TX","University of Texas at Austin","Standard Grant","Rosemary Renaut","08/31/2016","$190,919.00","Yen-Hsi Tsai","engquist@math.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1271","9263","$0.00","Many important systems in science, engineering, and industry are modeled by dynamical systems with solutions that are highly oscillatory relative to the overall time scale.  Molecular dynamics computations for materials science and biology are of this form and so is simulation of neural processes. Such systems pose severe challenges to numerical simulations. In direct simulations each oscillation must be resolved over the full time interval, which is highly computationally costly. This research project aims to develop new computational methods that are well suited to simulation of such systems. An example of the infinite-dimensional systems that will be considered is high frequency wave propagation. One goal is improving the quality of seismic imaging. Earlier contributions have successfully handled cases with strong separation of large and small scales in the solutions by only applying microscale simulations locally. The current work aims at reducing this requirement by adaptively monitoring the solution and exploiting parallel-in-time computations. <br/><br/>This research will go one step further than the typical analytical or numerical averaging and homogenization approaches to deal with more challenging multiscale problems where there is no clear scale separation. The research will start to establish a new multiscale framework, which will support adaptive application of microscale models in small parts of the computational domain. The computational efficiency will result partially from parallelization in time. The theoretical goal is to establish a mathematical link between averaging theories to existing parallel-in-time computational frameworks and filtering techniques. The practical goal is to prepare the study of seismic wave propagation where the microscale model describes detailed diffraction and the effective macroscales are represented by geometrical optics type models."
"1522747","Algorithms for Nonlinear Nonconvex Optimization under Uncertainty","DMS","OE Operations Engineering, COMPUTATIONAL MATHEMATICS","09/15/2015","09/15/2015","Andreas Waechter","IL","Northwestern University","Standard Grant","Leland Jameson","08/31/2019","$210,000.00","","waechter@iems.northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","006Y, 1271","072E, 073E, 077E, 9263","$0.00","The investigator will develop computational methods that seek to find optimal decisions when not all information is known exactly.  This situation arises when future events cannot be predicted with high accuracy or when quantities can only be estimated from limited data.  A similar setting occurs when computer programs are employed to simulate processes. In that case, the computer output can be noisy, due to limited precision of the underlying numerical algorithms or due to randomness inherent in the simulation. An illustrative example is the deployment of solar or wind energy: Electricity demand estimates based on past observations are uncertain, and weather forecast models are started from random perturbations of the initial conditions. Computational optimization algorithms exist that address data uncertainty, but current methods are not able to handle difficult nonlinear relationships in the mathematical optimization model. This research will overcome this limitation on two fronts. The first research project will result in optimization algorithms for problems in which constraints need to be satisfied only with a given probability. These methods will permit a much wider range of these constraints than the present state-of-the-art. The second project will produce methods that optimize computer simulations by explicitly addressing the nature of the output noise. In contrast to existing approaches, these algorithms will not stagnate at spurious solutions induced by the noise.  All new methods will be implemented in software and evaluated on real-life problems, and new mathematical theory will be developed that proves the convergence of these methods.<br/><br/>In this project, new algorithms for continuous chance-constrained optimization will be developed. In current approaches, the objective and constraint functions are required to be linear or convex, and the nonconvexity induced by the chance-constraints is handled either by conservative convex approximations or by the global solution of discrete formulations via combinatorial branch-and-bound enumeration.  The new methods will permit problem statements that involve nonlinear and nonconvex objective functions and include joint chance constraints with nonconvex probabilistic constraints. This is made possible by seeking only local optima, which can be found more easily than global minima but are still highly valuable in practice. As a result, established techniques from nonconvex nonlinear optimization can be built upon and extended. The new sequential quadratic chance-constrained programming framework requires the introduction of new chance-constrained trust-region subproblem solvers and convergence theory for chance-constrained penalty functions which will be developed in this project. The PI will also develop a derivative-free optimization method for objective functions with deterministic noise caused by numerical error in computer simulations. The approach is based on a smoothed objective function obtained via convolution with a Gaussian kernel.  The integral in the new objective is approximated by Monte-Carlo sample average approximation. Adaptive multiple importance sampling permits the reuse of the expensive function evaluations computed in all previous iterations."
"1522332","Collaborative Research: Multiscale Proximity Algorithms for Optimization Problems Arising from Image/Signal Processing","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/28/2015","Lixin Shen","NY","Syracuse University","Standard Grant","Leland Jameson","07/31/2018","$183,406.00","","lshen03@syr.edu","900 S CROUSE AVE","SYRACUSE","NY","132440001","3154432807","MPS","1271","9263","$0.00","Restoring images or signals from limited available data is required in variety of applications, including parallel magnetic resonance imaging in medical applications and fingerprint and face recognition in security identification. Such image or signal reconstruction problems are often modeled as large-scale optimization problems.  This research project aims to develop more efficient computational algorithms for solving these optimization problems. Results of this project are anticipated to have an impact in practical applications. In particular, the numerical schemes under development are expected to support medical imaging research and assist in improving the accuracy of clinical decisions. Senior undergraduate and graduate students are trained in the course of this project. <br/><br/>This research project aims to develop multiscale proximity algorithms for optimization problems arising in image or signal processing. Signal processing problems of practical importance, such as incomplete data recovery, compressive sensing, and matrix completion, are modeled as optimization problems that have non-differentiable objective functions. A signal of interest naturally has a hierarchical structure or allows itself to be sparsely represented in a multiscale analysis. Multiscale analysis, as a convenient tool for computation, however, is mainly used to sparsify the underlying signal in formulating optimization problems; it is not fully exploited in development of efficient algorithms for optimization problems. In this project, to make systematic use of the hierarchical structure that exists in optimization problems of interest, the investigators will synthesize and combine multiscale analysis and proximity algorithms to solve the problems in an accurate and computationally efficient way."
"1521830","Effective Preconditioners for High Frequency Wave Equations","DMS","COMPUTATIONAL MATHEMATICS","07/15/2015","06/09/2017","Lexing Ying","CA","Stanford University","Continuing Grant","Leland Jameson","06/30/2019","$299,999.00","","lexing@math.stanford.edu","450 JANE STANFORD WAY","STANFORD","CA","943052004","6507232300","MPS","1271","9263","$0.00","Many essential phenomena in physical science and engineering are modeled by high frequency waves. Modern computational methods have become essential tools for understanding these phenomena.  In this project, the PI will continue his research in this area, with an emphasis on time-harmonic equations. The computational problems of high frequency waves are challenging. The PI will develop effective preconditioners for time-harmonic high frequency wave equations for several key areas. The research results can allow for highly efficient solution methods and play important roles in applications. The educational component of the project involves graduate and undergraduate student training and curriculum development for modern computational mathematics. <br/> <br/>The PI will develop effective preconditioners for time-harmonic high frequency wave equations by combining the following ideas: (1) Waves often propagate in well-defined directions. This often allows one to decouple the complicated wave interaction into simple components, each of which is in a preferred direction and can be compressed with low-rank and randomized techniques.  (2) Accurate discretization schemes should be utilized to capture the correct dispersion relationship. (3) The sparsity of the time-harmonic wave operator should be exploited even when the linear system from the numerical discretization is not sparse. Using these ideas, the PI will address the following technical problems: (1) Developing more efficient sweeping preconditioners by investigating more efficient designs for perfectly matched layers, alternative factorization forms, and recursive sweeping strategies; (2) Developing sparsifying preconditioners for the Lippmann-Schwinger equation by effectively reducing the dense integral system to a sparse form; (3) Developing sparsifying preconditioners for the pseudospectral approximations of problems on periodic structures from computational photonics and electron structure calculation; and (4) Constructing directional preconditioners for the obstacle scattering problem via developing a sparse representation of the boundary integral operator and introducing a new kernel-independent directional fast summation methods."
"1521684","Hemivariational Inequalities: Numerical Methods and Applications","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/24/2015","Weimin Han","IA","University of Iowa","Standard Grant","Leland Jameson","07/31/2018","$167,806.00","","weimin-han@uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1271","8396, 8609, 9150, 9263","$0.00","Certain systems in physical sciences and engineering cannot be described by mathematical equations but are instead described by more complicated relations known as inequalities (stating that one quantity is greater than or smaller than another).  Solving a system of inequalities numerically can be very challenging. This research project aims to develop a rigorous and comprehensive mathematical theory, as well as reliable and efficient numerical methods, for the numerical solution of a class known as hemivariational inequalities. In addition to the resultant advances in computational mathematics, the research will have impact in the petroleum industry through application to analysis of borehole pumping systems. Numerical simulations are used by the petroleum industry to infer the downhole oil pump conditions based on measured surface conditions. For deviated (non-vertical) oil wells, methods for reliable simulation have yet to be developed, mainly due to the complicated friction and dynamics in deviated wells. Appropriate models can be cast in the form of hemivariational inequalities; this research project is expected to facilitate practically useful numerical simulations for diagnostics of oil pump conditions in deviated wells. Collaboration with researchers in the petroleum industry will ensure the transfer of the new mathematical results to applications. The results from the project are expected to help correctly control downhole oil pumps to save energy and avoid pump damage.  Graduate students will actively participate in all aspects of the research and will thus be trained in high level mathematics and numerical methods on challenging and important problems from applications.<br/> <br/>Inequality problems in mechanics can be divided into two main classes: that of variational inequalities, which is concerned with convex energy functionals (potentials), and that of hemivariational inequalities, which is concerned with nonsmooth and nonconvex energy functionals (superpotentials). Through the formulation of hemivariational inequalities, problems involving nonmonotone, nonsmooth, and  multivalued constitutive laws, forces, and boundary conditions can be treated successfully. During the last three decades, hemivariational inequalities were shown to be very useful across a wide variety of disciplines, ranging from nonsmooth mechanics, physics, and engineering, to economics. However, relatively little work on the numerical analysis of hemivariational inequalities has been done. In this research project, a comprehensive theory will be developed for the numerical solution of various hemivariational inequalities, including several families of elliptic, parabolic, and hyperbolic hemivariational inequalities. For each family of hemivariational inequalities, numerical schemes will be introduced based on the finite element method for spatial discretization and finite differences for temporal discretization. Convergence of the numerical solutions will be shown under the basic solution regularity, and error estimates will be derived under appropriate solution regularity assumptions. The error estimates will be of optimal order when linear elements are used. Numerical experiments will be performed to illustrate convergence orders predicted by the theory. Results from this project (such as a posteriori error analysis, adaptive algorithms, and discontinuous Galerkin methods) will form a solid foundation for further developing numerical methods to solve hemivariational inequalities."
"1522663","Higher-Order Methods for Interface Problems with Non-Aligned Meshes","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/11/2015","Marcus Sarkis","MA","Worcester Polytechnic Institute","Standard Grant","Leland Jameson","08/31/2018","$189,344.00","","msarkis@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","MPS","1271","9263","$0.00","Interface problems arise in several applications including heart models, cochlea models, aquatic animal locomotion,  blood cell motion, front-tracking in porous media flows and material science, to name a few. One of the difficulties in these problems is that solutions are normally not smooth across interfaces, and therefore standard numerical methods will lose accuracy near the interface unless the meshes align to it. However, it is advantageous to have meshes that do not align with the interface, especially for time dependent problems where the interface moves with time. Re-meshing at every time step can be prohibitively costly, can destroy the structure of the grid, can deteriorate the well-conditioning of the stiffness matrix, and affect the stability of the problem. The first problem studied will involve new stable and higher-order accurate Finite Element - Immersed Boundary Methods (FE-IBM) for evolution problems where the interface moves with time. The second problem studied is the design and analysis of robust higher-order discretizations for interface problems with high-contrast discontinuous diffusion coefficients. Benefits of the project include the strengthening of connections between numerical analysis and other areas of science and engineering, particularly bioengineering, porous media flows, material sciences and parallel computing. This project will impact the development of numerical algorithms used in the fluid-structure interaction communities. A broader impact will be the training of graduate and undergraduate students of mathematics and related disciplines by exposing them to interdisciplinary problems and collaborations addressing questions of great technological importance.<br/><br/>One of the drawbacks of the finite element and finite difference immersed boundary methods is that they are only first-order accurate due to the non-smoothness of the solution across the interface. Furthermore, very few mathematical analyses of these methods exist for time dependent problems and for fluid-structure interaction problems. The first part of the project involves the construction of higher-order FE-IBM algorithms and establishing a corresponding mathematical foundation to obtain rigorous time stability and a priori and a posteriori error estimates. The second part of the project deals with new finite element methods which are able to accurately capture solutions of elliptic interface problems with high-contrast coefficients in the case that the finite element mesh is not necessarily aligned with the interface. The goal here is to develop finite element methods with optimal convergence rates, where the constants hidden in these estimates are independent of the contrast and on how the mesh crosses the interface."
"1522651","Statistical Learning for High-Dimensional Stochastic Dynamical Systems","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/15/2015","09/16/2015","Mauro Maggioni","NC","Duke University","Continuing Grant","Leland Jameson","05/31/2017","$196,236.00","","mauro.maggioni@jhu.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1266, 1271","9263","$0.00","High-dimensional stochastic dynamical systems arise in a wide variety of scientific fields and applications, including models for the dynamics of molecules, of multi-agent systems (such as cars or animals), of activity of neurons, etc. The high-dimensionality of these systems corresponds to the large number of variables (atoms, agents, neurons, respectively, in the preceding examples) and typically makes these systems very expensive to simulate and hard to understand even when simulations are possible. This research project aims to develop novel ideas and algorithms for the model reduction of such systems. The investigator will develop automatic learning algorithms that, by collecting a number of short simulations in parallel, output a much lower-dimensional model of the original system that yields faster simulations, with provable accuracy, in a much reduced number of variables. This will make the simulations less expensive, allowing one to perform more and longer simulations, and make the extraction of useful information from simulated data easier. He will apply these constructions to molecular dynamics simulations, expecting to significantly reduce the computational time needed to simulate small biomolecules.<br/><br/>Large data sets in high dimensional spaces appear in a wide variety of scientific fields and applications. The PI focuses here on data sets that originate from the simulation of high-dimensional stochastic systems that arise in a wide variety of applications (e.g. molecular dynamics), with the goal of producing a much lower dimensional stochastic system with similar statistical properties as the full system, at least at a certain time scale. This is possible for a wide variety of dynamical systems with separation of time scales when the structure of the forces acting on the system and the stochastic perturbations are such that the trajectories of the system accumulate, in state or phase space, around low-dimensional sets (at the appropriate time scale and accuracy). The approach only requires access to a simulator S for the original system that, given initial conditions and the shortest time scale of interest as inputs, produces as output a (stochastic) path of the system starting at that initial condition and stops at the specified time. A call to S is typically expensive, but after a small number of carefully designed calls that yield a relatively small number of short paths, the algorithm learns and outputs a low-dimensional representation of the system, that is, a low-dimensional stochastic system whose trajectories are (in a suitable statistical sense), at the requested time scale, close to those of the original system. This construction may be performed in an online setting, as new regions of state space are explored, and in a multiscale fashion, where the time scale at which the system is reduced varies. These techniques will be adaptive to the assumed low intrinsic dimension of the simulated data, the timescale of interest, and the accuracy, leading to a new generation of results and algorithms for learning and approximating high-dimensional stochastic systems. While the techniques to be developed are applicable to a large family of stochastic systems, the main application considered in this project is Molecular Dynamics (MD). These techniques are expected to dramatically speed up the exploration of the state space of these molecules and of MD simulations as a whole. At the same time, they are general enough that they are applicable to a wide variety of stochastic systems, and the framework sets the stage for a novel approach to automatic learning of dynamical systems that is amenable to further generalizations."
"1522554","Collaborative Research: A New Three-Dimensional Parallel Immersed Boundary Method with Application to Hemodialysis","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/11/2015","Luoding Zhu","IN","Indiana University","Standard Grant","Leland Jameson","08/31/2019","$209,314.00","Fengguang Song","luozhu@iupui.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","MPS","1271","9263","$0.00","Fluid-structure interaction problems involving thin-walled structures are ubiquitous in biological and engineering applications. However, to date an efficient and effective technique, and a computational capability, for modeling and simulating the interactions between fluids and thin-walled structures are still sorely lacking. The investigators aim to design a new three-dimensional parallel immersed boundary method for computational simulation of fluid-thin-walled-structure interactions in a generic setting and apply it to blood flow past patient-specific distal anastomosis of arteriovenous grafts (AVG), which are essential to blood access of hemodialysis for numerous patients with end-stage renal disease. The new method, which will significantly broaden the applicability of immersed boundary methods, will be particularly valuable to the mathematical biology community for computational studies of vascular diseases such as vascular intimal hyperplasia, aneurysm, and atherosclerosis. Compared to existing models, the proposed computational model is more physiologically realistic: the simulation accommodates deformation of the vein/graft with the pulsatile blood flow, and it incorporates the small yet finite thickness of the vein/graft walls into the model. New computational results will clarify existing contradictory results in the literature regarding the force/flow characteristics near the distal AVG anastomosis and thus lead to a greater understanding of AVG-associated vascular intimal hyperplasia. The new method under development in this project will be generic and applicable to numerous significant problems in engineering, including parachute opening and novel design for street/highway signs. The studies will also enhance the understanding of vascular intimal hyperplasia due to dialysis, which may inspire the creation and development of novel vascular devices to prolong the patency rate of AVGs. This will not only improve quality of life for patients, but also offer savings in dialysis-related healthcare costs. The associated research and education activities will provide multidisciplinary training and research opportunities in mathematics, biology, scientific computing, fluid/solid mechanics, blood flows, and vascular disease for graduate students and undergraduates. The open source implementation of the new method will enable the fluid-structure-interaction community to dramatically increase their research productivity. <br/><br/>The investigators will develop numerical methods to improve computational capability for fluid-thin-walled-structure interaction in three dimensions. They approach this type of problem by integrating several components: a structural component based on the high-order spectral/hp element technique, a fluid component based on the lattice Boltzmann method, and the coupling of the fluid and structure through the framework of the immersed boundary method. The goal of this project is three-fold: 1) Develop a three-dimensional IB-based method for fluid and thin-walled structure interactions in a general setting. The method will account for Newtonian and non-Newtonian fluids, material nonlinearity, and geometric nonlinearity. 2)Design, develop, and implement novel parallel algorithms for the new 3D method on hybrid CPU-GPU linux clusters. 3) Apply the new parallel method to model and simulate blood flow past the distal anastomosis of arteriovenous graft for hemodialysis using patient-specific data.  The investigators' outreach activities will inspire high school students to consider careers in mathematical and computational sciences and raise public awareness for the dire consequences of end-stage renal disease, its associated healthcare costs, and the important roles mathematics and scientific computing play in studying disease and promoting health."
"1522629","Fast Sparse Nonlinear Optimization and Its Application to Optimal Control","DMS","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL MATHEMATICS","07/15/2015","07/31/2017","William Hager","FL","University of Florida","Standard Grant","Leland Jameson","06/30/2019","$299,793.00","Anil Rao","hager@ufl.edu","1523 UNION RD RM 207","GAINESVILLE","FL","326111941","3523923516","MPS","1253, 1271","019Z, 1515, 9263","$0.00","New computational algorithms will be developed in this project for solving sparse optimization problems. These are large, complex problems that arise in science, engineering, and industry where the goal is to operate a large interconnected system in an efficient way. Applications range from power grids to air traffic control systems to the fabrication of computer chips. A specific application developed in the project is to optimal control which has a wide range of uses including space flight maneuvers, the optimal design of aerodynamic shapes, and the optimal design of manufacturing processes. The algorithms developed in the project for the sparse optimization problem provide solutions much faster and with much greater accuracy than was previously possible. <br/> <br/>The computational techniques developed in the project will be built around a new error estimator, which yields a tight bound for the error in a solution to an optimization problem in terms of the violation in the first-order optimality conditions, and a new dual-based approach to polyhedral projection.  At the same time that the error in the solution is estimated, approximations to the dual multipliers at a stationary point are generated. The new polyhedral projection algorithm will be incorporated into a new algorithm for polyhedral constrained nonlinear optimization. With linearization and globalization techniques, the new algorithms will be used to achieve a fast and accurate solver for the general sparse constrained nonlinear optimization problem. The new optimization framework will be used to develop hp-orthogonal collocation techniques for solving optimal control problems.  The research will focus on mesh refinement techniques for which the fast and accurate optimization solver will be instrumental in advancing the technology."
"1454939","CAREER: Research and training in advanced computational methods for quantum and statistical mechanics","DMS","COMPUTATIONAL MATHEMATICS, Division Co-Funding: CAREER","09/01/2015","07/22/2019","Jianfeng Lu","NC","Duke University","Continuing Grant","Leland Jameson","08/31/2020","$420,000.00","","jianfeng@math.duke.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1271, 8048","1045, 7237, 9263","$0.00","Ab initio simulations have become widely used scientific workhorses with applications in physics, chemistry, materials science, and many related fields, but the reach of simulations is limited by computational complexity. To go beyond current capabilities in realistic and predictive ab initio simulations, it is necessary to address the current bottlenecks. This project aims to greatly enlarge the scope of ab initio simulations and open up major new areas for electronic-structure-theory-based predictive methodologies that are not possible today. This research lies at the intersection of multiple disciplines. The project connects advanced state-of-the-art techniques in computational mathematics to challenging problems arising from chemistry and materials science. The integrated research program will bring together ideas and techniques, which will not only help advance the application areas, but also has the potential to open up new research areas in mathematics.<br/><br/>The research goal of this project is to innovate and analyze efficient algorithms based on advanced computational mathematics for electronic structure theory and computational statistical mechanics, which will greatly advance the scope of ab initio simulations with applications in chemistry, materials science, and many related fields. More specifically, topics considered include: (1) reduced scaling methods for electronic structure theory, which will extend the system size of the simulation; (2) efficient sampling algorithms for metastable systems, which will bridge the temporal scales; and (3) algorithms that go beyond Born-Oppenheimer approximation, which will give a more accurate account of quantum effects in classical dynamics. The educational objectives of this proposal are to prepare and train students for interdisciplinary research and to disseminate knowledge from graduate and undergraduate students to high school students and the general public in the U.S. and abroad."
"1513633","RAPID: Early Warning Algorithms for Predicting Ebola Infection Outcomes","DMS","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL MATHEMATICS","01/01/2015","12/31/2014","Michael Kirby","CO","Colorado State University","Standard Grant","Leland Jameson","12/31/2016","$137,119.00","Richard Slayden","kirby@math.colostate.edu","601 S HOWES ST","FORT COLLINS","CO","805212807","9704916355","MPS","1253, 1271","001Z, 7914, 9263","$0.00","This investigation concerns the development of mathematical algorithms to provide rapid diagnostic tools for the early detection of infection by the Ebola virus. Unlike current methods, the proposed approach will not require that the subject be symptomatic for detection of the virus. The proposed methodology exploits the observation that the immune system behaves like a canary in a coal mine, providing an early warning system that, if quantitatively understood, could be used to identify infection, accelerate treatment and improve outcomes. The detection technique consists of building an array of mathematical models for, e.g., gene expression data, that characterize the nominal state of the healthy immune system. These models are then applied to detect novel, or anomalous behavior of the immune system in infected subjects. The initial model building phase will employ non-human primate and mouse data to establish viability of the approach.<br/><br/>Transcriptional analysis has been widely applied to identify markers for disease classification, diagnosis, and prognosis. Many methods have been developed to identify the signaling pathways that respond to the changes between varying biological states, i.e., healthy and disease states, from a static viewpoint. However, the transition between biological states is a complex dynamic process that is information rich. In preliminary work on influenza, a nonlinear model of gene expression was built for over 400 pathways using data from healthy individuals that were experimentally infected with influenza virus. Of these pathways, the cytosolic DNA/RNA sensing pathway (a system for detecting pathogen-associated nucleic acids) was the first to exhibit changes in gene expression in the majority of subjects who became symptomatic, reflecting the immune system's initial response to an invading pathogen. Moreover, as the immune system response progressed, there was a cascade of anomalous pathway signaling, reflected by changes in gene expression, which could provide an early warning signature for detection of a pathogen well before externally observable symptoms of the disease appear. In this project, the pathway cascade of the mammalian cell response to Ebola virus will be investigated with the goal of characterizing the features of its disease-specific evolution, which will be used to identify molecular signatures for diagnosis prior to observable symptoms. The sensitivity and robustness of the modeling procedure will also be explored."
"1519021","Montana Workshop on Uncertainty Quantification, June 2015; The University of Montana, Missoula, MT","DMS","COMPUTATIONAL MATHEMATICS","06/15/2015","06/18/2015","Johnathan Bardsley","MT","University of Montana","Standard Grant","Leland Jameson","12/31/2017","$19,465.00","","bardsleyj@mso.umt.edu","32 CAMPUS DR","MISSOULA","MT","598120003","4062436670","MPS","1271","7556, 9150, 9263","$0.00","Montana Workshop on Uncertainty Quantification, University of Montana, June 24-26, 2015<br/><br/>Developments in high-performance computing in the last decade have enabled tremendous advances in applied mathematics and physics, in large part because today's computers allow scientists to solve problems that couldn't before even be attempted. Many of the fields that are advancing the most quickly impact Americans' daily lives, including radiation detection interrogation and analysis techniques for national security science, medical advances through new imaging techniques, and weather modeling and prediction. In the science of the 21st-century, solving difficult problems is not enough; scientists must also make statistically justifiable statements about the uncertainties in their solutions. Uncertainty Quantification (UQ) is a burgeoning field that sits at the interface of computation, applied mathematics, physics, and statistics, and its goal is to extend and develop statistical methods to assign meaningful error bars and uncertainty estimates to the solutions of computationally challenging applied mathematics and physics problems. The Montana Workshop on Uncertainty Quantification will gather top experts and talented young researchers in the field to the University of Montana to develop and disseminate the latest theory, techniques, and applications of uncertainty quantification.<br/><br/>Uncertainty quantification (UQ) denotes the science of incorporating statistical methods into the problem of fitting physics-based mathematical models to physical systems, with the ultimate goal of obtaining theoretically justifiable error bars for model predictions and for estimates of model parameters. A few of the physical systems that will be discussed in the Montana Workshop on Uncertainty Quantification (MUQ) are high energy radiography in the security sciences, interferometry, data assimilation in weather modeling, astronomical imaging, medical imaging, process tomography, and control theory. Most of these applications are so-called inverse problems, which involve the estimation of parameters in a mathematical model of a physical system from finite dimensional measurements of output from that system. The unknown parameters are typically high-dimensional, resulting in an over-parameterized statistical model and hence high uncertainties in model predictions and in estimates of model parameters. Since such problems pose a particular challenge for traditional statistical techniques, the practice of UQ for inverse problems provides a rich source of new and important research problems in UQ and, as such, is one of the primary focuses of MUQ. The class of applications and research problems covered by the workshop will be broad, from the very applied (e.g., estimating object densities from real data in high-energy radiography experiments) to the very theoretical (e.g., an analysis of a specific Bayesian approach for solving an inverse problem in the function space setting). In this workshop, researchers specializing in theory, computation, and applications will come together to share the latest techniques and advances in UQ. More details about the workshop can be found at http://www.math.umt.edu/bardsley/MUQ/MUQ.html."
"1522697","Advanced Eigensolvers for Science and Engineering Applications","DMS","COMPUTATIONAL MATHEMATICS, Algorithmic Foundations","09/01/2015","07/14/2015","Zhaojun Bai","CA","University of California-Davis","Standard Grant","Leland Jameson","08/31/2019","$249,590.00","","bai@cs.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1271, 7796","7933, 9263","$0.00","Large-scale eigenvalue computation is a long-standing problem in computational mathematics and computational science and engineering. It is frequently encountered as a critical kernel in simulations and data analysis. Significant progress has been made both in general-purpose eigensolvers and also in specialized eigensolvers that exploit underlying particular mathematical properties and data structure. However, new needs and challenges continue to emerge from science and engineering applications. This project involves the development of advanced mathematical analysis and robust, efficient algorithms for two emerging classes of eigenvalue problems: sparse plus low rank linear eigenvalue problems and eigenvalue problems with eigenvalue nonlinearity. In addition, this project has broader impacts in training graduate students in interdisciplinary research. While much of the work involves significant technical expertise, other areas can be successfully understood and tackled by advanced undergraduates.<br/><br/>The computational stability, efficiency and reliability of the new solvers for the two classes of eigenvalue problems will be greatly enhanced by skillful exploitation of underlying mathematical properties and matrix structure. In particular, for the eigenvalue problems with eigenvalue nonlinearity, new solver will combine rational approximations of nonlinearity for high accuracy, trimmed linearizations for low dimensionality, and compact representations of the projection subspace bases for memory-saving and communication efficiency. The outcomes of this project will be the publication of new theory and algorithms and open-source software."
"1521582","Collaborative Research: An Integrated Approach to Convex Optimization Algorithms","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/16/2015","Weihong Guo","OH","Case Western Reserve University","Standard Grant","Leland Jameson","08/31/2020","$127,267.00","","wxg49@case.edu","10900 EUCLID AVE","CLEVELAND","OH","441061712","2163684510","MPS","1271","9251, 9263","$0.00","Image reconstruction and feature extraction have been important aspects in various applications such as medical resonance imaging (MRI) and synthetic aperture radar (SAR). However, these procedures involve challenges. Different applications may vary in data acquisition (sampling) domains, levels of detail required, and processing domains for the features of interest. The data acquisition is usually under-prescribed and noisy. The sampling domains and/or processing domains may not be well suited for the underlying question. All of these make the problems ill-posed, and various regularization techniques are necessary to study the problems by formulating them as convex optimization models. This project will develop an integrated framework of investigating such convex optimization models. The project will provide graduate students with opportunities for training through research involvement and will prepare them for careers in science and engineering. <br/><br/>The PIs aim to propose a systematic way of evaluating various regularization techniques in such models, conduct a rigorous numerical analysis of the models, and develop efficient numerical algorithms of solving the models. Specifically, the PIs will address the following technical questions: (1) What constraints must be placed on the collected data in order to construct a numerically robust approximation to the underlying function? (2) How quickly and in what sense does the approximation converge? (3) Are the corresponding numerical algorithms developed for the fidelity and regularization terms viable? (4) How well are perturbations from the original data tolerated? The project aims to provide answers to all of these questions."
"1522267","Numerical Analysis of Non-Equilibrium Turbulence","DMS","COMPUTATIONAL MATHEMATICS","07/15/2015","07/15/2015","William Layton","PA","University of Pittsburgh","Standard Grant","Leland Jameson","06/30/2019","$298,613.00","","wjl+@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1271","8396, 8609, 9263","$0.00","This research project treats a central problem in many industrial and environmental flow simulations. Complex turbulence not at statistical equilibrium occurs in almost all engineering fluid flows. Its accurate simulation is essential for many critical applications, including global change estimation and energy efficiency optimization.  For example, 85% of the energy in the U.S. is generated by combustion; energy efficiency optimization requires accurate simulation of turbulent mixing. However, there is little mathematical theory, few effective models, and no systematic computational practice for simulation of turbulent flows that are not at statistical equilibrium.  This research project aims to develop mathematical models and computational algorithms to simulate such flows and to conduct a rigorous numerical analysis of the new models and algorithms.  Several graduate students will participate in the project.<br/><br/>This project is to conduct research in numerical analysis on turbulence not at statistical equilibrium.  The goal of the project is to extend current models and computational algorithms to turbulence not at statistical equilibrium, allowing for backscatter (energy flow from fluctuations back to means) without negative viscosities, and to develop mathematical support for the new models and algorithms.  The research will develop, analyze, and test new models of turbulence adapted from models at statistical equilibrium and will develop new unconditionally nonlinearly stable, linearly implicit numerical methods for their approximation.  The project represents a systematic attack on modeling and numerical analysis, broadly understood, of turbulence not at statistical equilibrium, investigating fundamental mathematical, modeling, and computational issues in its predictive simulation."
"1522574","Large Eddy Simulations in Magnetohydrodynamics Flows","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/24/2015","Catalin Trenchea","PA","University of Pittsburgh","Standard Grant","Leland Jameson","07/31/2018","$182,977.00","","trenchea@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1271","8396, 8609, 9263","$0.00","Magnetohydrodynamics describes the behavior of an electrically conducting fluid flow in the presence of magnetic fields. Electrically conducting fluids arise in applications including astrophysics, geophysics, plasma confinement, controlled thermonuclear fusion, liquid-metal cooling of nuclear reactors, electromagnetic casting of metals, ion thrusters for low orbiting satellites, magnetohydrodynamic drive for ships and submarines, microfluidic devices, and molecular biology. The fluid motion of the Earth's core maintains the terrestrial magnetic field, the solar magnetic field generates sunspots and solar flares, and the galactic magnetic field influences the formation of stars from interstellar clouds. These applications require substantially better modeling and simulation capabilities than presently exist. Problems without a clear scale separation, such as turbulence, are still at the frontier of multiscale modeling and simulation. When the fluid is electrically conducting, the turbulent fluid motions are accompanied by magnetic fluctuations. For Magnetohydrodynamic (MHD) turbulence, numerical simulations play a greater role than they play for hydrodynamic turbulence, since laboratory experiments are practically impossible and astrophysical systems (solar-wind turbulence, the most important system of high-Reynolds-number MHD accessible to in situ measurements) are too complex to be comparable with theoretical results.  This research project will develop improved computational methods for these important problems.<br/><br/>This research project studies mathematically rigorous and computationally efficient methods to analyze direct and inverse problems constrained by MHD models. This includes the numerical analysis of computational algorithms, implicit explicit time-stepping schemes using the Elsasser variables, post processing via time-filters, spatial linear and nonlinear filters, spectral filtering, development of spatial filters specific to MHD turbulence, optimal control, and parameter estimation. Another objective of this project is to investigate the mathematical properties of several models for the simulation of the large eddies in turbulent viscous, incompressible, electrically conducting flows and new numerical models that permit long-time simulations, by time-splitting. The project has a broad impact for training undergraduate and graduate students in analytical and numerical aspects of magnetohydrodynamics, turbulence, and inverse problems."
"1519934","High-Order Added-Mass Partitioned Algorithms for Fluid-Structure Interaction Problems","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","06/27/2017","William Henshaw","NY","Rensselaer Polytechnic Institute","Continuing Grant","Leland Jameson","07/31/2019","$270,000.00","Donald Schwendeman","henshw@rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","MPS","1271","9263","$0.00","This research project will dramatically improve computer algorithms used for the simulation of important problems in applied science, engineering, and bio-medicine, such as the modeling of blood flow in human veins, which provides critical data to evaluate surgical procedures and to design medical devices and implants.  The area of fluid-structure interaction (i.e. coupling fluids with deforming solids), which this project addresses, is of current and growing interest due to its importance in many areas of engineering and applied science, including not only bio-medicine but also the modeling of flow-induced vibrations of structures (aircraft, undersea cables, wind turbines, buildings), wave energy devices, parachutes and airbags, acoustic lenses, thermal expansion in nuclear reactor cores, and shock-structure interactions (blast effects).  Computing accurate solutions of fluid-structure interaction problems efficiently is very challenging due to their complex physical and geometrical properties, but also due to challenging mathematical issues.  The new techniques developed by this project will address both these challenges.<br/><br/>A wide class of important fluid-structure interaction (FSI) problems are difficult to simulate due to the so-called added mass instability.  The focus of the research is on FSI problems coupling incompressible flow with elastic or rigid structures.  Of particular interest are problems in which the mass of the fluid and that of the structure are similar, such as in applications of hemodynamics, where added-mass effects are strong.  The overarching goal of this project is to develop stable and high-order accurate partitioned algorithms for FSI problems that overcome the numerical difficulties associated with complex fluid-solid couplings and strong added-mass effects. These algorithms will be based on newly devised Added-Mass Partitioned (AMP) schemes that represent a conceptual breakthrough in the field by providing a path forward to develop accurate schemes for fluid-structure interaction problems.  An important property of AMP schemes is that they remain stable even for light solids when added-mass effects are significant. The new algorithms will be incorporated into a flexible computational framework that will allow efficient simulations for wide class of complex fluid-structure interaction problems."
"1541585","Nonlinear PDE's, Numerical Analysis, and Applications; October 2-3, 2015; Pittsburgh, PA","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","07/29/2015","Michael Neilan","PA","University of Pittsburgh","Standard Grant","Rosemary Renaut","08/31/2016","$10,000.00","","neilan@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1271","7556, 9263","$0.00","Conference: Nonlinear PDEs, Numerical Analysis, and Applications, October 2-3, 2015, University of Pittsburgh. Partial differential equations (PDEs) play a key role in the understanding, simulation and prediction of various phenomena occurring in the sciences and engineering.  Yet even for simple problems, exact mathematical solutions are unattainable, and computational methods are necessary to construct approximate solutions.  Therefore there is a critical need to develop numerical methods and to theoretically justify the quality and reliability of the resulting approximations.  In addition to providing justification of the numerical methods, the theoretical analysis often gives insight for the development of new methods with improved efficiency, accuracy and capabilities.  This two-day conference will create a forum for junior and senior researchers to discuss cutting-edge results and applications of numerical methods for nonlinear PDEs.  The aim of the conference is to provide an informal setting in which young researchers and leading experts in numerical analysis can meet, collaborate, and develop new ideas.  In addition, the conference will expose graduate students and postdocs on this active field of numerical PDEs.<br/><br/>The construction, implementation, and analysis of computational methods for fully nonlinear second order partial differential equations are relatively new, yet critical research areas in numerical analysis and scientific computing. Such problems arise in many application areas including meteorology, cosmology, geometric optics, differential geometry, optimal transport, economics, imagine processing and mesh generation. These problems constitute one of the most difficult classes of PDEs to approximate numerically, and breakthroughs in their discretization have only appeared within the last 15 years. While there have been several advances in numerical fully nonlinear second order PDEs, there still remain fundamental challenges that need to be properly addressed. Examples include the construction of nonlinear Galerkin methods with monotonicity properties under realistic mesh conditions, robust numerical methods for general families of nonlinear problems, imposition of non-standard boundary conditions, and fast solvers of the resulting non-linear algebraic systems. This conference will gather leaders in the field to discuss state-of-the-art research trends and directions of future research."
"1522337","New Developments in Geometric and Multiscale Numerical Methods","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/24/2015","Thomas Yu","PA","Drexel University","Standard Grant","Leland Jameson","07/31/2019","$230,000.00","","yut@drexel.edu","3141 CHESTNUT ST","PHILADELPHIA","PA","191042816","2158956342","MPS","1271","9263","$0.00","This research project will study representation and approximation of complex data structures involving geometrical configurations and shapes. The methods developed in this project will shed light on questions such as: Why do human red blood cells have a bi-concave donut shape instead of, say, a peanut shape? It is known that the lipid bilayer is the most elementary and indispensable structural component of biological membranes that form the boundary of all cells. To understand the amazing variety of different shapes they exhibit, biophysicists strive to find an accurate mathematical model for such bio-membranes. With the mathematical and computational techniques under development in this project, the solution of such models can be obtained accurately and efficiently. Similar mathematical techniques can also be applied to unravel white matter structure from diffusion tensor magnetic resonance images (DT-MRI) of the brain and to predict, in real-time, a life-threatening phenomenon called aerodynamic flutter in air transportation. These will help diagnose psychiatric disorders and save lives of pilots and passengers. <br/><br/>The project studies multi-scale representation and approximation of manifold-valued data, reduced order modeling, as well as design of numerical algorithms that respect the geometric or topological characteristics of the underlying problem. In particular, this research addresses the following: I. Approximation of manifold-valued data (scattered manifold-valued data and reduced order modeling, and theory of nonlinear subdivision algorithms); II. New multi-scale geometric modeling tools (numerical simulation of lipid bilayers, subdivision differential forms for genus 0 topology, and 3-D subdivision methods). The research will combine techniques from mathematical analysis, geometry, numerical analysis, optimization theory, and computing to advance the understanding of applied geometry problems."
"1522786","A Non-Convex Approach for Signal and Image Processing","DMS","COMPUTATIONAL MATHEMATICS, GOALI-Grnt Opp Acad Lia wIndus","09/01/2015","12/14/2018","Yifei Lou","TX","University of Texas at Dallas","Standard Grant","Leland Jameson","08/31/2019","$208,382.00","","yifei.lou@utdallas.edu","800 WEST CAMPBELL RD.","RICHARDSON","TX","750803021","9728832313","MPS","1271, 1504","019Z, 9263","$0.00","As the digital revolution increases the amount of data generated by sensing methodology such as magnetic resonance imaging and radar, the need to process the data better, faster, and cheaper has been the focus of much research, most notably through work with compressive sensing (CS). However, CS is not without its problems, most of which have emerged as CS has moved from the theoretical to the practical. The theory was developed with convex problems, but many practical applications require the ability to process nonconvex problems that are not easy to solve as quickly as digital sensing systems require. This research project focuses on a particular nonconvex model along with associated numerical algorithms, which when completed will advance the field of nonconvex optimization. Theoretical investigations will be performed to establish conditions for guaranteed performance, which will help engineers and scientists devise experiments to acquire data and recover useful information in a more effective manner. The tools developed will have broad applicability due to the profound impacts of CS, specifically in the fields of medical imaging and geospatial information that are addressed in this project. Furthermore, the investigator will incorporate results of the research into undergraduate and graduate courses and will develop new interdisciplinary courses with focus on both the theory and application, including machine learning and medical imaging, which will serve as a springboard for student recruitment. <br/><br/>Compressive sensing (CS) can exactly recover a sparse signal (most elements being zero) from incoherent linear systems, in which any two measurements have as little correlation as possible. Sparsity and incoherence are two important assumptions in CS, but many practical problems are coherent, and conventional methods do not work well. To overcome the coherency barrier, the investigator and collaborators investigate a novel nonconvex model that has advantages over the state-of-the-art methods in CS. The goal of this project is to address key challenges regarding both computational and theoretical aspects of the algorithms, to establish new criteria for exact recovery, and to demonstrate its applicability in prototypical problems. As such, this research is organized with three objectives: (1) Developing efficient algorithms to solve the nonconvex minimization problem, using techniques in convex optimization and dynamical systems to design algorithms and analyze convergence; (2) Searching for conditions that can quantify the success of both convex and nonconvex methods, for example, coherence and minimum separation; (3) Conducting numerical experiments in two types of real problems, medical image reconstruction and hyperspectral image classification, to demonstrate the advantages of the method in terms of accuracy and efficiency. Overall, this project will advance theoretical understanding and algorithmic developments in computational mathematics and provide a new perspective to enable CS-based data recovery in a wide spectrum of applications."
"1621111","Development of high-order accurate numerical methods for the shallow-water equations and other hyperbolic conversation laws with source terms","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","04/26/2016","Yulong Xing","CA","University of California-Riverside","Standard Grant","Rosemary Renaut","07/31/2016","$63,981.00","","xing.205@osu.edu","200 UNIVERSTY OFC BUILDING","RIVERSIDE","CA","925210001","9518275535","MPS","1271","9150, 9263","$0.00","The objective of the proposed project is to provide a class of novel high-order accurate and efficient well-balanced discontinuous Galerkin (DG) and Weighted Essentially Non-Oscillatory (WENO) schemes for the shallow-water equations and other hyperbolic conservation laws with source terms. The proposed activity includes a comprehensive coverage of new algorithm development, theoretical numerical analysis, numerical implementation issues and practical applications. The investigator proposes to provide a detailed study of highly efficient high-order well-balanced methods in the following directions: 1. Development of well-balanced methods: Very accurate well-balanced numerical methods will be developed for several equations arising in different areas; 2.  Shallow-water equations: Positivity-preserving well-balanced methods for the shallow-water equations will be developed. Then, the investigator will investigate their performance, including efficiency, scalability, etc., and study their potential application in the coastal ocean modeling; 3.  Euler equations under a gravitational field: Hydrodynamical evolution in a gravitational field arises in most astrophysical problems. The investigator will develop well-balanced methods for such system; 4. Nonlinear water wave equations: Conservative DG methods will be developed for nonlinear dispersive wave equations.<br/><br/>The proposed project will provide more efficient and accurate numerical approaches to solve the shallow-water equations, and other conservation laws with source term. It will have a direct impact in many application problems arising from hydraulic engineering and atmospheric modeling, and is suitable for other source-term problems in chemistry, biology, fluid dynamics, astrophysics, and meteorology. Due to its multi-disciplinary nature, the proposed research will initiate and serve as a solid foundation for collaborative research work with applied mathematicians, hydraulic engineers and astrophysicists, and promote interdisciplinary research between Oak Ridge National Laboratory and the University of Tennessee. The proposed project will also provide training and education opportunities for both graduate and undergraduate students interested in computational mathematics.<br/>"
"1500174","Collaborative Research: Algebra and Algorithms, Structure and Complexity Theory","DMS","OFFICE OF MULTIDISCIPLINARY AC, FOUNDATIONS, COMPUTATIONAL MATHEMATICS, Special Projects - CCF","09/15/2015","09/11/2015","Ralph McKenzie","TN","Vanderbilt University","Standard Grant","Tomek Bartoszynski","07/31/2019","$103,519.00","","rn.mckenzie@vanderbilt.edu","110 21ST AVE S","NASHVILLE","TN","372032416","6153222631","MPS","1253, 1268, 1271, 2878","7433, 7933, 9150, 9263","$0.00","This project is a collaboration between mathematical researchers at five universities, including young mathematicians at the early stages of their careers, who are joining forces to tackle fundamental problems at the confluence of mathematical logic, algebra, and computer science. The overall goal is to deepen understanding about how to recognize the complexity of certain types of computational problems. The project focuses on a suite of mathematical problems whose solutions will yield new information about the complexity of Constraint Satisfaction Problems. These problems (CSP's) include scheduling problems, resource allocation problems, and problems reducible to solving systems of linear equations. CSP's are theoretically solvable, but some are not solvable efficiently. The research will be aimed at identifying a clear boundary between the tractable and intractable cases, and at providing efficient algorithms for solutions in the tractable cases. Many fundamental problems in mathematics and computer science can be formulated as CSP's, and progress here would have both practical and theoretical significance. A second component of the project investigates classical computational problems in algebra in order to determine whether they are algorithmically solvable. A third component of the project is the further development of the software UACalc, which is a proof assistant developed to handle computations involving algebraic structures.<br/><br/>The researchers shall work to decide the truth of the CSP Dichotomy Conjecture of Feder and Vardi, which states that every Constraint Satisfaction Problem with a finite template is solvable in polynomial time or is NP complete. They will further develop the algebraic approach to CSP's by refining knowledge about relations compatible with weak idempotent Maltsev conditions and about algebras with finitely related clones. A second goal of the project concerns the computable recognition of properties of finite algebras connected with the varieties they generate, such as whether a finite algebra with a finite residual bound is finitely axiomatizable, or whether a finite algebra can serve as the algebra of character values for a natural duality. One of the more tangible accomplishments of this project will be a broadening and strengthening of the applicability of the UACalc software. The agenda for this part of the project includes parallelizing the important subroutines, building in conjecture-testing and search features, adding further algorithms, and further developing the community of users and contributors."
"1410144","Collaborative Research: Mathematical Foundations of Topological Quantum Computation","DMS","ALGEBRA,NUMBER THEORY,AND COM, APPLIED MATHEMATICS, TOPOLOGY, COMPUTATIONAL MATHEMATICS, ANALYSIS PROGRAM","12/15/2015","12/17/2015","Eric Rowell","TX","Texas A&M University","Standard Grant","Leland Jameson","11/30/2017","$89,999.00","","rowell@math.tamu.edu","400 HARVEY MITCHELL PKY S STE 30","COLLEGE STATION","TX","778454375","9798626777","MPS","1264, 1266, 1267, 1271, 1281","9263","$0.00","Recently discovered topological phases in materials such as topological insulators have potential use in (quantum) computational devices that can out-perform standard microchip based computers. The most commonly encountered model for quantum computation, the quantum circuit model, requires challenging, if not impossible, accuracy on the hardware to be of practical value, due to local interactions of the system with the surrounding environment. The topological model based on exotic states of matter, while mathematically more complicated, has a built-in tolerance for such interactions. This research project studies mathematically the application of topological phases of matter to new computational paradigms with potentially significant benefit in quantum computation.<br/><br/>In this project the investigators study mathematical models for topological phases, focusing on their applications to topological quantum computation.  Topological phases of matter in two spatial dimensions are well-described in the framework of modular categories, but relatively little is known in three spatial dimensions.  A large part of this project is devoted to developing appropriate mathematical models in three spatial dimensions and analyzing the corresponding computational paradigms.  Specifically, the project will study (3+1)-dimensional topological quantum field theories and representations of the loop braid group, and symmetry enriched topological order and gauging symmetry.  In addition, because locality and universality are two desirable properties for quantum computation that are manifested in the representations of the braid group, this project also aims to formulate conjectures characterizing when these properties hold and to verify and adapt these conjectures where appropriate to better characterize physical and computational aspects.  To compare the computational power of topological quantum computers to that of classical computers, the project investigates the complexity of the most natural computation in this setting: topological invariants."
"1522604","Hybrid Numerical Methods for Three-Phase Flows with Moving Contact Lines","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","08/11/2017","Pengtao Yue","VA","Virginia Polytechnic Institute and State University","Continuing Grant","Leland Jameson","08/31/2019","$167,234.00","","ptyue@math.vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1271","8396, 8609, 9263","$0.00","Multiphase flows that involve both solid bodies and fluid interfaces are ubiquitous in many applications such as particle-stabilized emulsions, flotation technology, and animal locomotion at fluid interfaces. These applications play important roles in the mining industry, micro- and nanotechnologies, biotechnology, and environmental science. For example, nanoparticles may self-assemble on a fluid interface due to capillary forces; this effect can be used in the fabrication of nanostructured materials. Dry water, a water-in-air emulsion stabilized by silica nanoparticles, has demonstrated extraordinary capability to absorb carbon dioxide and is potentially useful in the war against global warming. Flotation techniques, where gas bubbles are used to remove suspended particles, have been widely used in wastewater treatment. This project aims to develop numerical tools for detailed studies of these complex flows and to resolve fundamental questions that arise in applications. The project will also provide hands-on training opportunities for graduate students to work in interdisciplinary research. <br/> <br/>The goal of this project is to develop hybrid numerical methods for the direct numerical simulation of flow problems where deformable fluid interfaces, moving rigid particles, and moving contact lines coexist. An arbitrary Lagrangian-Eulerian (ALE) algorithm on an unstructured adaptive moving mesh will be adopted to track the moving boundaries of rigid particles. Two interface-capturing methods based on the same moving mesh, namely the phase-field and the level set methods, will be developed to capture the deformation of fluid interfaces as well as the moving contact lines. These hybrid ALE-interface-capturing methods combine the advantages of both components: the ALE method provides an accurate representation of the particle surface, which is critical for delicate contact-line conditions; the interface-capturing methods easily handle the topological transitions of fluid interfaces. In particular, the ALE-phase-field approach satisfies an energy law and is essentially free of parasitic currents. Finite-element software packages will be developed to implement the proposed methods. This work will advance the development of numerical algorithms for different types of moving boundary problems and also enrich the understanding of contact-line dynamics in complex systems."
"1522454","Advances in Multilevel Methods for Saddle Point Problems","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/28/2015","Constantin Bacuta","DE","University of Delaware","Standard Grant","Leland Jameson","07/31/2018","$150,000.00","","bacuta@udel.edu","220 HULLIHEN HALL","NEWARK","DE","197160099","3028312136","MPS","1271","9150, 9263","$0.00","In spite of the fast increase in computational power of today's computers, the development of faster algorithms for equational models is paramount for the simulation of phenomena investigated nowadays in practical applications in science and engineering. For a large spectrum of situations in the scientific world, a computer simulation has to replace experimental simulation and data collection. For a computer simulation, in order to obtain accurate approximation of the physical quantities of interest, a powerful computational system has to be combined with a fast and reliable algorithm to test and to confirm the designed models. The project will contribute to the construction and implementation of new and efficient algorithms for solving numerically challenging problems. In particular, the proposed research will focus on constructing fast and robust algorithms for solving computational fluid dynamics and electromagnetic problems. The breakthrough approach of the project is based on recent results in  numerical analysis, on new  algorithm designing and implementation, and on scientific  testing and validation of the new computational tools.  The methodology from this project for the  time-harmonic Maxwell's equations has a broad range of applications in nano-optics and analog signal packages. The work for solving variational problems has scientific and technical applications in optimization of electrical networks and image restoration. <br/><br/>The research will study and develop reliable and efficient numerical algorithms for solving partial differential equations that can be described as variational saddle point systems or mixed variational formulations. The goal of the project is to build robust algorithms  for solving such equations in the presence of low regularity of solutions due to discontinuities in data or coefficients. The research will produce a rigorous and systematic analysis of a large class of saddle point problems, focusing on efficient algorithm development  and testing. The methods will be based on finite element discretization algorithms in the context of a multilevel and adaptive choice of approximation spaces. The PI's approach uses a new saddle point least-squares type of discretization for systems of PDEs, that takes full advantage of the regularity of the solution and involves an efficient level change criterion that minimizes the running time of the global iterative process. This study will lead to more reliable methods for a variety of applications of the  finite element method to science and engineering communities, such as those interested in elasticity, electromagnetism, friction, and computational fluid dynamics. The research findings will be shared within the field and with a more general audience including mathematics and science high-school teachers."
"1522191","Collaborative Research: Variational Structure Preserving Methods for Incompressible Flows: Discretization, Analysis, and Parallel Solvers","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","03/07/2019","Leo Rebholz","SC","Clemson University","Standard Grant","Leland Jameson","08/31/2019","$234,780.00","Timo Heister","rebholz@clemson.edu","201 SIKES HALL","CLEMSON","SC","296340001","8646562424","MPS","1271","8396, 8609, 9150, 9263","$0.00","The goal of this project is to develop improved models and numerical methods that advance the state of the art for incompressible fluid flow simulation. The key ideas are the better enforcement of geometric and physical laws in the computer algorithms, building a solid mathematical framework for the models and methods developed, and the devising of algorithms that will allow for efficient implementation on supercomputers. Although the simulation of fluid flow is a critical subtask in a wide spectrum of engineering applications, current tools and techniques are often unreliable and it is not uncommon for state of the art methods to take weeks or months (or possibly never finish with an accurate solution), even with thousands of processors, to perform simulations of flows around a car, through a nuclear reactor, or around part of an airplane. The project aims to develop mathematical prediction models and numerical methods that will provide more accurate solutions in a more efficient manner than state-of-the-art methods.<br/><br/>The PIs will construct efficient methods for incompressible flow simulation by constructing models and methods that better adhere to geometric structure and physical conservation laws than modern methods. The key components are to i) construct novel methods that are efficient and can correctly account for vorticity dynamics, and energy, helicity, and mass conservation -- this will require development of efficient boundary conditions and significant analysis to build a solid mathematical framework; ii) develop more efficient algebraic solvers for these methods that can be used on thousands of processors; iii) large scale testing on benchmark problems as well as on application problems with collaborators. Broader impacts include i) developing efficient methods for simulating high speed incompressible flows, which will improve the design process for a wide spectrum of applications in environmental engineering, in cardiovascular simulations, and in atmosphere and ocean sciences; ii) training graduate and undergraduate students through research involvement; and iii) developing large scale, parallel codes to be made publicly available as part of the deal.II library."
"1521563","Multiscale multilevel iterative substructuring","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","09/01/2015","Bedrich Sousedik","MD","University of Maryland Baltimore County","Standard Grant","Leland Jameson","08/31/2019","$199,920.00","","bedrich.sousedik@gmail.com","1000 HILLTOP CIR","BALTIMORE","MD","212500001","4104553140","MPS","1271","9263","$0.00","The PI will develop novel algorithms that can be used for the simulation of flow through porous media in real-world reservoir models. The simulation of flow in porous media finds applications in a number of areas, such as water management, oil and gas recovery, carbon dioxide (CO2) sequestration, and nuclear waste disposal, to name a few. The underlying mathematical models and efficient numerical simulation is challenging due to several aspects. The reservoirs are typically very large, so the discretized mathematical model leads to systems of equations with hundreds of millions of unknowns, they have irregular structure, which complicates the model geometry, and they consist of materials that significantly differ in geological properties, which translates in the model to jumps in coefficients over several orders of magnitude. Moreover, the geological formations quite often also contain fractures that alter the effective permeabilities, and therefore need to be be accurately incorporated into the numerical model. For example, the flow of water in granite rock, which represents one of the suitable sites for nuclear waste deposit, is conducted by the complex system of vugs, cavities and fractures with various topology and sizes. Alternatively, the fractures might result from the engineering activities, for example hydraulic fracturing (also known as fracking) used for the extraction of natural gas.<br/><br/>The PI will develop novel algorithms for solving saddle-point linear systems combining numerical upscaling techniques with parallel, domain decomposition iterative solvers. There are many aspects of multiscale and domain decomposition methods that are quite well understood, but the major drawback of current methodologies is that they do not take full advantage of their potential by using the multiscale phenomena in the design of the solvers, which results in their inefficiency. Multiscale methods also frequently consist in fact only of two scales, whereas in a porous medium there are typically many scales. At the same time, advances in multicore architectures, networking, high end computers, and large data stores, are ushering in a new era of high performance parallel and distributed simulations. Naturally, with these new capabilities come new challenges in computing and system modeling. The goal of this project is to open new avenues to tackle these issues. In particular, the PI will develop multiscale methods that allows for a multiple of scales, and uses the upscaling algorithm to build a multilevel preconditioner for the iterative solver. The components of the method are thus recycled, which significantly decreases the computational cost. Moreover, this approach can be applied recursively and thus naturally offers a multilevel multiscale potential, unlike many traditional multiscale approaches that consist in fact of only two scales. It is expected that understanding of the issues related to design of multiscale and multilevel methods for extremely large problems will ultimately contribute to development of the next generation of parallel iterative solvers suitable for implementation on future exascale supercomputers."
"1555033","Collaborative Research: Sparse spectral-tau methods for binary neutron star initial data","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","09/23/2015","Richard Price","MA","University of Massachusetts, Dartmouth","Standard Grant","Rosemary Renaut","08/31/2016","$11,464.00","","rprice.physics@gmail.com","285 OLD WESTPORT RD","NORTH DARTMOUTH","MA","027472356","5089998953","MPS","1271","9263","$0.00","Binary neutron star inspiral is the most certain source of gravitational waves detectable by Earth-based observatories like the US LIGO project, and simulations of such binaries should facilitate eventual detections. These simulations require initial conditions: solutions to the initial value problem of general relativity for the coupled gravity-matter system. The conformal thin sandwich method is an excellent approach for solving the initial value problem; however, although not an intrinsic assumption of the method, in practice the approach has assumed conformal flatness (as have other valuable approaches). Conformal flatness yields unphysical junk radiation. By numerically constructing helically symmetric solutions to the Einstein equations, the PI will extract initial data (or conformal thin sandwich trial data) which does not rely on conformal flatness, and therefore contains the correct initial gravitational wave content. The mixed PDEs arising from the helical reduction of the Einstein equations (or their approximation in the post-Minkowski formalism) will be solved with innovative techniques: sparse modal spectral-tau methods with new preconditioning strategies. In part, these strategies may rely on randomized algorithms for the interpolative decomposition. Spectral methods deliver superb accuracy for smooth problems(neutron star spacetimes are smooth almost everywhere), and sparsity affords a fast matrix-vector multiply when using a Krylov-subspace method to iteratively solve a linear system. Whereas the preconditioning of nodal (collocation) spectral methods is well studied, less is known about modal preconditioning. Our techniques have been successfully applied to models of the binary neutron star problem. Moreover, the problem's physical structure has already been explored with different, but limited, techniques. <br/><br/>This project is to combine two sets of techniques (each already developed) and further develop the first set (spectral-tau methods), in order to obtain new results for a leading problem in gravitational wave physics. The PI will develop these mathematical methods by applying them to the specific neutron star problem described above. This strategy of specificity is often used in the development of techniques, which then prove to be more general. Because the scientific problem is of great interest, much is known about it, and results therefore exist withwhich comparisons can be made. These comparisons facilitate the development of mathematical algorithms. Conversely, new mathematical methods deliver more and/or better solutions which enhances scientific understanding."
"1522798","Numerical Solution of Constrained Optimization Problems Governed by Partial Differential Equations with Uncertain Parameters","DMS","COMPUTATIONAL MATHEMATICS","07/15/2015","07/10/2017","Matthias Heinkenschloss","TX","William Marsh Rice University","Continuing Grant","Leland Jameson","06/30/2019","$209,999.00","","heinken@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271","9263","$0.00","This project will provide new mathematical algorithms and theoretical analyses for the solution of optimization problems governed by partial differential equations (PDEs) with uncertain parameters. These problems arise in many science and engineering decision making applications, where a decision has to be made before the realization of uncertain inputs can be observed that will impact the outcome of the decision. The uncertainty can be incorporated into the optimization formulation using so-called risk measures, which typically involve the expected value of the quantity of interest and a measure of its deviation from the expected value. In principle, these formulations allow one to compute decisions that balance maximization of their expected outcome and minimization of the risk due to uncertainty. However, the numerical solution of these problems presents many theoretical and algorithmic challenges.  For example, the numerical solution requires some sort of sampling of the random inputs, which can make these PDE constrained optimization problems extremely expensive to solve. <br/><br/>To address several of the above mentioned challenges, this research will provide theoretical analyses of the well-posedness and of optimality conditions for a class of semilinear elliptic PDE constrained optimization problems, and it will derive discretization error bounds for sparse grid and quasi Monte Carlo discretizations applied to PDE constrained optimization. Furthermore, it will develop and analyze adaptive methods which reduce the total number of samples needed, or incorporate reduced order models. This research is at the interface between stochastic programming, deterministic PDE constrained optimization, and solution of PDEs with random inputs, and it will make algorithmic and theoretical contributions to these areas. The application of theories and numerical methods to example problems will serve as a model for other researchers and decision makers, and will lead to more efficient algorithms for important classes of decision making under uncertainty. Results will be disseminated through publication of algorithms and results. In addition, the results of the project will be used in regularly offered courses on the theory and applications of optimization as well as in special courses on PDE constrained optimization under uncertainty aiming at students in both mathematics and engineering."
"1521661","Collaborative Research: An Integrated Approach to Convex Optimization Algorithms","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/16/2015","Guohui Song","NY","Clarkson University","Standard Grant","Leland Jameson","08/31/2019","$140,501.00","","gsong@odu.edu","8 CLARKSON AVE","POTSDAM","NY","136761401","3152686475","MPS","1271","9251, 9263","$0.00","Image reconstruction and feature extraction have been important aspects in various applications such as medical resonance imaging (MRI) and synthetic aperture radar (SAR). However, these procedures involve challenges. Different applications may vary in data acquisition (sampling) domains, levels of detail required, and processing domains for the features of interest. The data acquisition is usually under-prescribed and noisy. The sampling domains and/or processing domains may not be well suited for the underlying question. All of these make the problems ill-posed, and various regularization techniques are necessary to study the problems by formulating them as convex optimization models. This project will develop an integrated framework of investigating such convex optimization models. The project will provide graduate students with opportunities for training through research involvement and will prepare them for careers in science and engineering. <br/><br/>The PIs aim to propose a systematic way of evaluating various regularization techniques in such models, conduct a rigorous numerical analysis of the models, and develop efficient numerical algorithms of solving the models. Specifically, the PIs will address the following technical questions: (1) What constraints must be placed on the collected data in order to construct a numerically robust approximation to the underlying function? (2) How quickly and in what sense does the approximation converge? (3) Are the corresponding numerical algorithms developed for the fidelity and regularization terms viable? (4) How well are perturbations from the original data tolerated? The project aims to provide answers to all of these questions."
"1522654","Acceleration Techniques for Lower-Order Algorithms in Nonlinear Optimization","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/28/2015","Hongchao Zhang","LA","Louisiana State University","Standard Grant","Leland Jameson","07/31/2019","$177,771.00","","hozhang@math.lsu.edu","202 HIMES HALL","BATON ROUGE","LA","708030001","2255782760","MPS","1271","9150, 9263","$0.00","This project focuses on developing efficient innovative acceleration techniques and their underlying theories for the algorithms in nonlinear optimization. The acceleration techniques and algorithms developed in this project will have broad impact in many areas of computational science, including imaging/signal processing, optimal control, computer vision, petroleum engineering, topology optimization, and electronic structure computations. The algorithms developed in this research will be made publicly available on the web and will be applied in solving various computational problems. In addition, the student involved in this project will have excellent opportunities to participate in interdisciplinary research.<br/><br/>The research will include developing subspace techniques for nonlinear conjugate gradient method and accelerated nonlinear conjugate gradient methods with theoretically guaranteed optimal global complexity. A framework of inexact alternating direction method of multipliers (ADMM) will also be developed, in which multiple steps are allowed to solve the subproblem to an adaptive accuracy, while still maintaining global convergence even when the problem has more than two blocks. The project will also study acceleration strategies for gradient based stochastic optimization. In particular, adaptive strategies for choosing sample points and extracting quasi-Newton information based on the obtained stochastic information will be explored. In addition, a novel dual active set approach will be developed for solving smooth large-scale nonlinear optimization. For example, in projection on polyhedra, an algorithm can be developed to approximately identify the active linear constraints, while an asymptotically faster algorithm can be used to compute a high accuracy solution."
"1522593","Robust and Efficient High Order Methods for Time Dependent Problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","07/21/2017","Xiangxiong Zhang","IN","Purdue University","Continuing Grant","Leland Jameson","08/31/2019","$196,912.00","","zhan1966@purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1271","8396, 8609, 9263","$0.00","Robust and efficient high order accurate methods for computation have gained more and more popularity in the numerical modeling of real world problems for their ability to produce high fidelity simulations. However, such methods are not available or not well understood for hydrodynamics equations modeling high speed flows in spacecraft design, combustion, detonation, astrophysical jets, hurricanes, tsunamis, plasma dynamics, and inertial confinement fusion. This research project aims to develop improved numerical methods for simulation of these systems.  Progress in designing robust and more efficient high order methods will significantly impact on simulation technology for such applications.  <br/><br/>The state of high order accurate numerical methods is still far from being practically satisfactory for time-dependent nonlinear problems. Compared to their low order counterparts, high order methods are much harder to stabilize and might be less efficient in practice due to much larger computer memory cost. Thus it remains challenging to utilize a high order accurate method to solve nonlinear hydrodynamics equations in real world problems. The objective of this proposal is to address these real-world problem challenges from specific perspectives. First of all, one would like to ensure the robustness of Eulerian schemes by preserving certain invariances of physical quantities such as positivity. Second, one would like to design more efficient implementations of very high order schemes on curved elements for complex geometries."
"1522616","Interpolatory Model Reduction for the Control of Fluids","DMS","COMPUTATIONAL MATHEMATICS","07/15/2015","07/15/2015","Serkan Gugercin","VA","Virginia Polytechnic Institute and State University","Standard Grant","Leland Jameson","06/30/2018","$319,933.00","Jeffrey Borggaard","gugercin@math.vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1271","8396, 8609, 9263","$0.00","Fluid flow control problems are ubiquitous.  They arise in important applications such as drag reduction (with the benefits of saving fuel or improving range/speed), enhancing mixing (for more efficient combustion), reducing structural fatigue, improved solidification and die casting, and efficient cooling in large indoor-air environments. However, the ever-increasing need for improved accuracy and complexity of the underlying flow problems lead to very large-scale dynamical systems whose simulations and control make overwhelming and unmanageable demands on computational resources. This research project aims to develop novel computational techniques and a new rigorous mathematical framework to solve large-scale flow control problems very efficiently.  In addition, the project will develop a year-long graduate course on Model Reduction and Flow Control and will provide students with valuable interdisciplinary education.<br/><br/>The current state-of-the-art is to solve flow control problems by using reduced models constructed using the proper orthogonal decomposition.  However, these models are limited in that they are only guaranteed to be accurate for a pre-selected range of inputs.  This project will provide significantly improved tools for the efficient analysis and approximation of large-scale dynamical systems. It will also have direct application to model reduction for problems with bilinear and quadratic nonlinearities. Bilinear models arise in control problems for heat exchangers, and nonlinear partial differential equations with quadratic nonlinearities include the Korteweg-de Vries (shallow waves), the Kuramoto-Sivashinsky (turbulent flames), and the Landau-Lifshitz (magnetic fields in solid state physics) equations.  Using rational interpolation, this research will lead to new algorithms to systematically perform high-fidelity, in most cases optimal, model reduction for linear and nonlinear systems associated with (discretized) flow equations. These reduced models will be used to design optimal feedback laws. The new framework will offer major advantages: First, unlike current approaches, the proposed control design will not require expensive full-order, time-accurate simulations for specific input trajectories or solutions of large dense matrix equations; the computational efforts lie in computing a steady-state solution and solving a modest number of sparse linear systems. Second, the reduced models will be uniformly accurate for a wide range of input profiles and will not depend on specific input trajectories. Third, the methodology will naturally create reduced models that respect the stability properties of the original flow."
"1522656","Collaborative Research: Reduced Order Modeling of Realistic Noisy Flows","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/28/2015","Traian Iliescu","VA","Virginia Polytechnic Institute and State University","Standard Grant","Leland Jameson","07/31/2018","$143,196.00","","iliescu@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1271","1303, 9263","$0.00","Many flows in engineering, geophysics, and medicine pose two significant challenges for computations.  First, the computational resources that are available for the numerical simulations can accommodate only low spatial and temporal resolutions.  Therefore, standard numerical methods usually yield extremely inaccurate results.  To alleviate this, state-of-the-art numerical methods generally use spatial filtering to eliminate the noise (i.e., numerical artifacts).  The second challenge posed by these realistic flows is that they require numerous repeated runs (e.g., to determine optimal parameters in automobile design or cardiovascular flow simulation, or to find appropriate initial conditions in weather forecasting and climate modeling).  These repeated runs can tremendously increase the computational cost of the numerical simulations.  Thus, low cost surrogate models (called reduced-order models) that target only the dominant flow structures are generally used.  Combining state-of-the-art data generation methods and reduced-order modeling is required for an accurate and efficient numerical simulation of realistic flows.  A simplistic attempt to combine these two approaches is, however, doomed to fail due to numerical instability, noisy data, and modeling inconsistency.  This project aims to develop a framework that will transform reduced-order modeling into a robust tool that can tackle the challenges raised by realistic noisy flows in engineering, geophysics, and medicine. <br/> <br/>The numerical simulation of many realistic flows is fraught with difficulties (insufficient numerical resolution; numerical instability; need for repeated runs).  To address these challenges, state-of-the-art numerical approaches are needed: large eddy simulation (LES) and regularized models tackle the lack of numerical resolution and the instability, whereas reduced-order models (ROMs) based on proper orthogonal decomposition (POD) balance the computational cost and accuracy when repeated runs are needed.  A simplistic attempt to combine LES and regularized models with standard ROMs is, however, doomed to fail due to the following reasons: (i) standard ROMs are plagued by numerical instability; (ii) although LES and regularized models stabilize the numerical simulations, the data that they generate for ROMs is inherently noisy; and (iii) the modeling inconsistency between data generation (i.e., regularized and LES models) and ROMs can yield inaccurate results.  This project will develop a modeling, theoretical, and computational framework that will transform reduced-order modeling into a robust tool that can tackle the challenges raised by realistic noisy flows.  The main innovation is the explicit POD spatial filter, which bridges the inconsistency gap between the data generation (i.e., regularized and LES models) and ROMs.  This breakthrough paves the way for the development of novel regularized ROMs and the introduction in a ROM setting of genuine LES models that use approximate deconvolution to recover subfilter-scale information.  Over the last decades, a wealth of regularized and LES models have been highly developed in the engineering and geophysics communities.  The explicit POD spatial filter represents the missing link that finally allows the leverage of these successful approaches in reduced-order modeling."
"1522334","Priorconditioned Krylov Subspace Methods for Inverse Problems","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","08/14/2019","Daniela Calvetti","OH","Case Western Reserve University","Standard Grant","Leland Jameson","07/31/2020","$220,002.00","","dxc57@case.edu","10900 EUCLID AVE","CLEVELAND","OH","441061712","2163684510","MPS","1271","9263","$0.00","Inverse problems are gaining importance in a wide variety of applications; they play an important role in medical imaging because of the push towards non-invasive diagnostic techniques. In some applications, e.g., in the investigation of the brain activity from the measurement of the induced magnetic field in the space outside the skull, the relation between the unknown causes and the observed effects can be expressed as a linear function. In other cases, when the relationship is more complicated, the solution of linear inverse problems may have to be addressed as part of a more general solution scheme. While in principle easy to state, the solution of a linear system of equations arising from inverse problems can be extremely challenging, in particular when there is a mismatch between the number of observations and the degrees of freedom and when the dimensions of the problems are very large. When data collection is problematic because of the associated costs, technical difficulties, or health risks, the number of unknowns in the resulting linear system exceeds the number of equations. In order to produce a meaningful solution for such systems it is necessary to augment standard techniques with qualitative knowledge about the problem. This project concerns the design and analysis of computational methods for the solution of linear ill-posed problems that naturally translate qualitative information or belief about the data and the solution in quantitative terms. In particular, by formulating the problem within the framework of Bayesian inference, the project will develop mathematically sound and computationally efficient schemes for large scale problems where the disturbance in the data may be rather substantial and may have a statistics rather different from white noise. The Bayesian framework is the natural setting for expressing the a priori beliefs about the solution. The prior beliefs may vary widely from one time instance to another, or from one point in space to another, and it may be necessary to express them in hierarchical layers. Since this approach very closely resembles the way in which people formulate what they know and how knowledge is updated as new evidence arrives, it is expected that the methodology will be widely utilized. <br/><br/>The increasing popularity of complex models in inverse problems comes with an increase in associated computational costs. The methodology developed as part of this project addresses the need for computational efficiency by combining Bayesian inference with the Krylov subspace iterative methods, the natural choice for the solution of large scale linear systems. In this manner the philosophical appeal of the Bayesian framework is transformed in a very powerful Bayes-meets-Krylov computational scheme of wide applicability. The project provides an important connection between numerical linear algebra and Bayesian inference and will shed some light on how to link spectral properties of linear operators with statistical features of the unknown solution. Krylov subspace methods for inverse and ill-posed problems and the Bayesian solution of inverse problems are two very rich research areas which have received much interest, individually and jointly, in the last decade. There is experimental evidence that their symbiotic cooperation can be very advantageous in a variety of applications, but a solid understanding of the changes in the subspaces where the approximate solutions are sought and in approximation of the relevant eigenvalues in the associated Lanczos processes is still largely missing. The combination of theoretical and computational tools will fill this intellectual gap and open the way for the use of state-of-the-art iterative numerical solvers for very large ill-posed systems in the context of sequential Monte Carlo methods. This will reduce the gap between statistical uncertainty quantification and numerical linear algebra, to great advantage for both fields. In fact, the success of the Krylov-meets-Bayes approach, confirmed in a number of different settings and particularly in the solution of underdetermined problems, relies on left and right preconditioners to augment the quantitative data with additional qualitative information. Understanding the changes in the Krylov subspaces and in the associated Lanczos process induced by the statistically inspired preconditioners in discrete linear inverse problems is one of the aims of this project. In particular, the powerful tools of numerical linear algebra and the connection between Krylov subspace iterative solvers, the Lanczos process, and the associated orthogonal polynomials will be utilized to enlighten the connections and differences with classical schemes, including Tikhonov regularization. In the first part of the project, the analysis will be first carried out in the case of Gaussian prior and noise, and will be subsequently extended to the case of conditionally Gaussian prior, whose covariance matrix depends on unknown parameters, which are estimated via a nonlinear step as we learn more about the unknown of primary interest. In the latter case, the ensuing prior conditioners will be a parametrized family of matrices. Understanding how the spectral properties of the preconditioned systems change as functions of the parameters of the prior covariance will be part of the project; here, the connections with Gauss-type quadrature rules and moments may turn out be crucial."
"1522631","Fast Direct Solvers for Boundary Value Problems on Evolving Geometries","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/28/2015","Adrianna Gillman","TX","William Marsh Rice University","Standard Grant","Leland Jameson","07/31/2019","$149,980.00","","adrianna.gillman@colorado.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271","9263","$0.00","With the ability to reduce the cost of testing theories and ideas, numerical simulations will continue to play a growing role in scientific discovery and device development. Frequently, these simulations involve the solution of problems that are prescribed by physics.  In many cases, the speed and accuracy with which such problems can be solved is a key limiting factor to what can and cannot be modeled numerically.  Fast direct solvers have recently shown great promise for solving a large number of problems involving the same geometry by reusing the most expensive part of the solution technique.  This situation happens often in settings such as product development where each problem involving the geometry corresponds to a different physical situation.  When a large number of problems are under consideration, the fast direct solvers can show hundreds of times speed up over other techniques.  While this speed up is great, many engineering situations involve a large number of problems with slightly different geometries. Each different geometry requires the most expensive part of the solver to be recomputed.  This project will focus on the development and application of fast direct solvers to problems with evolving geometries.  The new technique will recycle information obtained in the construction of the fast direct solver for one geometry to build solvers for the evolved geometries.  As a result, the cost of the most expensive step in the fast direct solution technique will be substantially reduced while retaining the benefit of being able to solve multiple problems for each geometry quickly.  This work should accelerate many numerical simulations and will have a technological impact on society through applications such as solar cell design, meta-material design, sonar, radar and simulations of blood flow.<br/><br/>The numerical simulations under consideration in the proposed work will involve the solution of linear boundary value problems.  Many linear boundary value problems can be recast as integral equations.  Solution techniques based on integral equations come with the cost of having to solve a dense linear system upon discretization.  Fast direct solvers invert a dense system by exploiting structure in the matrix with a cost that grows linearly (or nearly linearly) with the problem size. The proposed work will adapt the fast linear algebra framework to create efficient direct solvers for a family of problems with similar geometries. The new technique will be the first to reuse the structural information obtained in the construction of a fast direct solver for a single geometry to build new direct solvers for evolved geometries.  This recycling of structural information reduces the cost of the most expensive step in constructing fast direct solvers. The new technique should accelerate time-stepping for evolution equations, Stokes' flow, optimal design algorithms, model reduction methods and periodic boundary value problem simulations.  The solution technique will be applied to microfluids, inverse scattering, and periodic scattering."
"1522736","Domain Decomposition Methods: Algorithms and Theory","DMS","COMPUTATIONAL MATHEMATICS, Algorithmic Foundations","08/01/2015","07/28/2015","Olof Widlund","NY","New York University","Standard Grant","Leland Jameson","07/31/2019","$200,000.00","","widlund@cs.nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1271, 7796","7933, 8396, 8609, 9263","$0.00","This research project aims primarily to further develop fast and reliable methods for large scale computations to support the solution of complicated engineering problems. Examples are provided by oil platforms, complicated antenna systems, and flow of oil, gas, and contaminants in porous media. Successful computer simulations of such problems require careful modeling as well as efficient methods to obtain timely solutions of the often very large systems of equations that will arise in any attempt to provide reliable support for the design and optimization of complicated engineering structures. The need for such work can be illustrated by the very costly failures that have happened to oil platforms already installed or in the process of being built. This work involves the development of accurate mathematical models as well as the design of fast solvers; the current project will focus on developing such solvers. Large scale computational models require access to modern computer technology, in particular to computer systems with many processors. The principal investigator will continue to work actively with software engineers to develop improved solvers for a variety of problem classes. <br/><br/>The algorithms developed in this project will all be based on domain decomposition. Domain decomposition algorithms respect the memory hierarchies of modern parallel computing systems, and experiments clearly illustrate that they scale very well up to the full set of processors and billions of degrees of freedom. Domain decomposition methods provide iterative solvers based on a conjugate gradient algorithm combined with a preconditioner. A preconditioner provides an approximate inverse of the stiffness matrix of the partial differential equation formulated variationally and approximated using a Galerkin method. Any successful domain decomposition algorithm works with solvers on often very many subdomains into which the domain of the given partial differential equation has been subdivided. In addition, to obtain a scalable algorithm, i.e., an algorithm with a convergence rate that does not deteriorate when the number of subdomains and processors are increased, a coarse global part of the preconditioner must be introduced; for large problems a third even coarser level is also introduced. Firmly rooted in mathematical theory, these algorithms are now developing rapidly in a way not foreseen just a few years ago. Powerful ideas are now developing that provide much improved design of the coarse components at the expense of solving relatively small generalized eigenvalue problems in the set-up phase of the computation. Recent experiments show that these devices greatly improve the robustness of the algorithms even of problems with greatly varying material properties.  This project aims to contribute to these developments."
"1522585","Study of Limiting Methods for Computation of Conservation Laws and Other Hyperbolic Problems","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","06/05/2020","Yingjie Liu","GA","Georgia Tech Research Corporation","Standard Grant","Leland Jameson","08/31/2021","$236,581.00","","yingjie@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1271","9263","$0.00","Many natural systems are modeled by differential equations of so-called hyperbolic type. For example, models used for weather forecasting, aircraft design, and study of ocean currents and biofluids are all hyperbolic differential equations. This research project concerns the development of techniques to approximate the solutions of hyperbolic differential equations. More precisely, the project continues the development of numerical methods for hyperbolic equations with non-smooth solutions (such as is the case when the terms appearing in the model itself are subject to the influence of non-smoothness; say, because of non-smooth boundaries, or interfaces). In such cases, effective methods must be able to remove approximation artifacts introduced by the non-smoothness, while maintaining as much as possible a behavior close to that of the underlying expected true solution. The impact of the project on applied domains in the sciences and engineering will be in introducing techniques to guarantee more accurate and efficient solutions based on numerical methods. Additional broader impacts include mentoring and collaborating with a graduate student and disseminating the research findings to the larger scientific community through seminars and public lectures. <br/><br/>The project studies numerical methods for hyperbolic differential equations, in particular for problems that do not have smooth solutions, where accurate computations largely depend on nonlinear limiting methods. The PI will develop several techniques to improve existing numerical methods and also to develop new methods. One goal of the project is the development of new limiting methods for the ""back and forth error compensation and correction method."" A second goal of the project is to improve on the Courant-Friedrichs-Lewy (CFL) numbers of Runge-Kutta discontinuous Galerkin methods for hyperbolic conservation laws. Here, the main idea of the PI and collaborators is to use extra conservation constraints as penalty for the variational energy functional, and thereby achieve CFL numbers several times larger without increasing the complexity or reducing order of accuracy."
"1522597","AG15: SIAM Conference on Applied Algebraic Geometry","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS","07/01/2015","03/25/2015","Grigoriy Blekherman","GA","Georgia Tech Research Corporation","Standard Grant","Andrew Pollington","06/30/2016","$25,000.00","","greg@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1264, 1271","7556, 9263","$0.00","The award will be used to support travel of students and early career researchers to the 2015 Society for Industrial and Applied Mathematics (SIAM) Conference on Applied Algebraic Geometry, to be held at the National Institute for Mathematical Sciences (NIMS) and at the Korean Advanced Institute of Science and Technology (KAIST) in Daejeon, South Korea on August 3-7, 2015. This is the biennial conference of the SIAM Activity Group on Algebraic Geometry.  The SIAM activity group was founded in 2009 and the SIAM Conference on Applied Algebraic Geometry has quickly grown to be the premier conference on applications of algebraic geometry. This conference presents a significant opportunity to establish new connections and collaborations with the algebraic geometry community in Asia. The meeting promises to have a significant impact on the growth of Applied Algebraic Geometry, and the conference offers a unique opportunity for students and early career researchers.<br/><br/>The 2015 SIAM Conference on Applied Algebraic Geometry will continue the tradition of covering a very wide range of topics in applied algebraic geometry. ""Algebraic Geometry"" is interpreted broadly to include at least: algebraic geometry, commutative algebra, noncommutative algebra, symbolic and numeric computation, algebraic and geometric combinatorics, representation theory, and algebraic topology. The conference will include more than 60 minisymposium sessions with approximately 250 invited talks, and 9 plenary addresses given by internationally renowned experts.  The conference is a satellite of the International Congress on Industrial and Applied Mathematics (ICIAM 2015) to be held in Beijing on the following week, so the participants will have an opportunity to attend both conferences. The conference website is  http://camp.nims.re.kr/activities/eventpages/?id=200&action=overview"
"1520862","Collaborative Research:  Numerical Simulation of the Morphosynthesis of Polycrystalline Biominerals","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","08/04/2017","Natasha Sharma","TX","University of Texas at El Paso","Continuing Grant","Leland Jameson","08/31/2019","$159,999.00","","nssharma@utep.edu","500 W UNIVERSITY AVE","EL PASO","TX","799680001","9157475680","MPS","1271","7237, 9263","$0.00","Polycrystalline biominerals are thermodynamically stable crystal polymorphs of biogenic minerals featuring stacked layers of crystals with mineral bridges between adjacent layers. The unique crystal texture gives rise to specific material properties such as toughness, corrosion resistance, and temperature resistance, which makes these crystals highly attractive for optical nanostructures (photonic band gaps, diffraction gratings) and for special coatings (e.g., in semiconductor device technology). Therefore, many material scientists are currently trying to realize the synthesis of such biominerals. The aim of this project is to provide both a mathematical model for the crystallization process and algorithmic tools for numerical simulations in order to understand the mechanisms of the process, to enable the experimentalists to optimize their laboratory settings, and thus to pave the way for an industrially relevant production line.<br/><br/>The morphosynthesis of polycrystalline biominerals follows a multistage crystallization process including a polymer-induced liquid-precursor (PILP) phase, the occurrence of spherulites due to nucleation, and the recrystallization of mosaic mesocrystal thin structures. The PILP phase consists of an aqueous solution of the biomineral and an anionic polymer mixed with ethanol and features a liquid-liquid phase separation in terms of polymer-rich PILP droplets in the liquid mixture. The mixing is taken care of by a surface acoustic waves (SAWs) manipulated fluid flow where the SAWs are generated by two tapered interdigital transducers operating in dual mode. The polycrystallization sets in with the formation of spherulites that spread across the substrate to form a uniform spherulitic thin film. Continuous cooling leads to a recrystallization of the spherulitic thin film into a mosaic polycrystalline thin structure. The liquid-liquid phase separation characterizing the PILP phase can be described by a coupled system consisting of the incompressible Navier-Stokes equations and a Cahn-Hilliard equation. For the numerical simulation, the project will use a splitting scheme based on an implicit discretization in time and C0 Interior Penalty Discontinuous Galerkin (C0-IPDG) methods for discretization in space with respect to simplicial triangulations of the computational domain. The research will study the convergence of the splitting method and realize space-time adaptivity by the goal oriented dual weighted approach. As a mathematical model for the polycrystallization the project investigates a phase field model consisting of the dynamic equations for the measure of local crystallinity, the concentration field for the biomineral, the orientation field, and a heat equation for the evolution of the temperature during the cooling process. The equation for the concentration field is a fourth order Cahn-Hilliard type equation. Again, discretizing implicitly in time and by C0-IPDG methods in space, the project will use a splitting method and dual weighted residuals for space-time adaptivity featuring a desired crystallinity at final time as the objective functional. A model validation will be based on experimental data provided by cooperating laboratories and a systematic parameter study will be performed to investigate the influence of various process parameters."
"1522339","Collaborative Research: Multiscale Proximity Algorithms for Optimization Problems Arising from Image/Signal Processing","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/28/2015","Charles Micchelli","NY","SUNY at Albany","Standard Grant","Leland Jameson","07/31/2018","$136,593.00","","charles_micchelli@hotmail.com","1400 WASHINGTON AVE","ALBANY","NY","122220100","5184374974","MPS","1271","9263","$0.00","Restoring images or signals from limited available data is required in variety of applications, including parallel magnetic resonance imaging in medical applications and fingerprint and face recognition in security identification. Such image or signal reconstruction problems are often modeled as large-scale optimization problems.  This research project aims to develop more efficient computational algorithms for solving these optimization problems. Results of this project are anticipated to have an impact in practical applications. In particular, the numerical schemes under development are expected to support medical imaging research and assist in improving the accuracy of clinical decisions. Senior undergraduate and graduate students are trained in the course of this project. <br/><br/>This research project aims to develop multiscale proximity algorithms for optimization problems arising in image or signal processing. Signal processing problems of practical importance, such as incomplete data recovery, compressive sensing, and matrix completion, are modeled as optimization problems that have non-differentiable objective functions. A signal of interest naturally has a hierarchical structure or allows itself to be sparsely represented in a multiscale analysis. Multiscale analysis, as a convenient tool for computation, however, is mainly used to sparsify the underlying signal in formulating optimization problems; it is not fully exploited in development of efficient algorithms for optimization problems. In this project, to make systematic use of the hierarchical structure that exists in optimization problems of interest, the investigators will synthesize and combine multiscale analysis and proximity algorithms to solve the problems in an accurate and computationally efficient way."
"1500067","Conference on Multiscale Modeling with Partial Differential Equations in Computational Science and Engineering","DMS","COMPUTATIONAL MATHEMATICS","06/01/2015","05/16/2017","Salim Haidar","MI","Grand Valley State University","Standard Grant","Leland Jameson","05/31/2018","$10,590.00","Paul Yu","haidars@gvsu.edu","1 CAMPUS DR","ALLENDALE","MI","494019401","6163316840","MPS","1271","7237, 7556, 8396, 8609, 9263","$0.00","This award supports participation in a regional conference on the topic of multiscale modeling. The conference is organized by the Great Lakes Section of the Society for Industrial and Applied Mathematics (SIAM) and will take place at Grand Valley State University in Grand Rapids, MI on May 2, 2015. Multiscale modeling from atomistic to macro scales, which is at the core of computational science and engineering (CSE), has affected almost every modern industry and all disciplines of basic science, engineering, as well as social sciences. To bridge gaps in communication among mathematics, statistics, computer science, environmental science, basic science, engineering, and industry, the 2015 Annual Conference of the Great Lakes Section of SIAM (GLS-SIAM) brings together researchers in CSE to foster advanced discovery and understanding of multiscale modeling, while promoting training in CSE. The conference features a series of keynote talks by world-leading researchers in CSE that address current challenges and opportunities in advanced multiscale simulations. The conference has significant impact for the establishment of CSE-related research activities and training in the Great Lakes area.<br/><br/>In recent years, modeling and simulation of multiscale phenomena have attracted enormous attention from the mathematical and scientific research communities, with profound positive impact on transcendent technologies and future product development in both academia and industry. These modeling problems are characterized by having multiple scales of length and time, interactions among multiple phenomena or components, generation of truly big data, and unknown levels of uncertainty. They require innovative approaches to be able to formulate accurate, reliable models to bridge the macro-nano-molecular-atomic scales by capturing the complex dynamic interactions that are vital to the simulation of any real device and to the optimization of design-cycle times. However, this systematic multiscale framework presents central mathematical and scientific challenges both in theory and computation required for successfully combining models or coupling algorithms across various scales. This conference shall provide a forum for scientists to communicate their new findings in multiscale modeling, and furthermore, to develop new collaboration in research and student training. The conference website is  http://www.gvsu.edu/siamgls2015/"
"1515849","Dynamics of Dispersive PDE","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/15/2015","08/09/2017","David Ambrose","PA","Drexel University","Continuing Grant","Victor Roytburd","07/31/2019","$269,987.00","","ambrose@math.drexel.edu","3141 CHESTNUT ST","PHILADELPHIA","PA","191042816","2158956342","MPS","1266, 1271","9263","$0.00","This project studies dispersive partial differential equations, which are equations describing wave-like behavior in physical systems.  As such, dispersive partial differential equations can describe phenomena in optics, fluids, or plasmas, to name just a few areas of application.  To determine the validity of a particular equation as a model for a given phenomenon, it can be helpful to determine whether the equation has a solution, and if it does, how this solution depends upon the physical parameters.  For certain nonlinear Schrodinger equations, and for certain equations which can be used as models for waves in water, the principal investigator will study when these equations either lack solutions, or have solutions which depend discontinuously upon the data.  This will indicate that these particular models have only limited validity for applications.  As another part of the project, for some other families of dispersive partial differential equations, the principal investigator will determine when the equations either do or do not possess solutions with certain special forms.  Understanding such solutions, such as waves of permanent form or time-periodic waves, can lead to a deeper understanding of the behavior of the corresponding physical phenomena over long intervals of time.<br/><br/>The principal investigator will study well-posedness and ill-posedness for quasilinear Schrodinger equations and for truncated series water wave models.  For quasilinear Schrodinger equations, well-posedness theorems have been proved under certain non-degeneracy hypotheses; in this work, ill-posedness will be studied when the non-degeneracy condition is violated.  For truncated series models of water waves, the relationship between well-posedness/ill-posedness and the strength of dispersion will be investigated using complex variable methods and tools from paradifferential calculus.  For nonlinear Schrodinger equations and for general families of nonlinear dispersive equations, the principal investigator will use small divisor estimates, normal forms, and dispersive smoothing estimates to demonstrate the non-existence of small-amplitude spatially periodic, time-periodic waves in certain parameter regimes.  Also, existence of time-periodic, spatially-periodic waves for dispersive equations with high-derivative nonlinearities (especially for systems related to capillary waves in fluids) will be studied, using small divisor methods and techniques of Fourier analysis."
"1419038","Collaborative research: Data selection for unique model identification","DMS","COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","01/01/2015","12/16/2014","Elena Dimitrova","SC","Clemson University","Standard Grant","Leland Jameson","12/31/2018","$100,002.00","","edimitro@calpoly.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","MPS","1271, 7334","9150, 9263","$0.00","While this is the age of big data, there is still a question of whether more data translates to more knowledge. Particularly when generating data is expensive or time consuming, as it is often the case with clinical trials and biomolecular experiments, the problem of identifying information-rich data becomes crucial for creating models that can reliably predict the outcome of future experiments. Few results have been published on the amount of necessary data, and currently there are no methods for generating specific data sets which would unambiguously identify a predictive model. This research project addresses fundamental mathematical and computational questions in data selection.  The theoretical results will advance the fields of design of experiments and network inference through the determination of criteria for selecting data sets to uniquely identify models. The algorithms under development will serve as a guide for experimentalists in determining the data that are needed to identify the structure of a network of interest. Such knowledge has the potential to drastically reduce wasted resources that arise from too much data with too little information. Graduate students will participate at the appropriate level in each component of the project. Such an experience will provide possible topics for M.S. or Ph.D. dissertations and will very likely inspire career-long involvement of the participants in the STEM disciplines.<br/><br/>As a first step towards developing a complete theory, the PIs will focus on models described by finite-valued nonlinear polynomial functions. Finite-state multivariate polynomial functions have successfully been used to model complex networks from discretized data; however, few results have been published on the amount of data necessary for such models, with the majority applying to Boolean models only. The PIs will address the issue of the minimality and specificity of data to uniquely identify discrete polynomial models by developing the appropriate theory, implementing the theoretical results as algorithms, and applying the algorithms to important physical systems. The proposed work will also increase the utility of polynomial dynamical systems as models of complex networks by establishing the minimal amount of the data for unique model identification."
"1411212","Collaborative Research: Mathematical Foundations of Topological Quantum Computation","DMS","ALGEBRA,NUMBER THEORY,AND COM, APPLIED MATHEMATICS, TOPOLOGY, COMPUTATIONAL MATHEMATICS, ANALYSIS PROGRAM","12/15/2015","12/17/2015","Zhenghan Wang","CA","University of California-Santa Barbara","Standard Grant","Leland Jameson","11/30/2019","$60,001.00","","zhenghwa@math.ucsb.edu","Office of Research","Santa Barbara","CA","931062050","8058934188","MPS","1264, 1266, 1267, 1271, 1281","9263","$0.00","Recently discovered topological phases in materials such as topological insulators have potential use in (quantum) computational devices that can out-perform standard microchip based computers. The most commonly encountered model for quantum computation, the quantum circuit model, requires challenging, if not impossible, accuracy on the hardware to be of practical value, due to local interactions of the system with the surrounding environment. The topological model based on exotic states of matter, while mathematically more complicated, has a built-in tolerance for such interactions. This research project studies mathematically the application of topological phases of matter to new computational paradigms with potentially significant benefit in quantum computation.<br/><br/>In this project the investigators study mathematical models for topological phases, focusing on their applications to topological quantum computation.  Topological phases of matter in two spatial dimensions are well-described in the framework of modular categories, but relatively little is known in three spatial dimensions.  A large part of this project is devoted to developing appropriate mathematical models in three spatial dimensions and analyzing the corresponding computational paradigms.  Specifically, the project will study (3+1)-dimensional topological quantum field theories and representations of the loop braid group, and symmetry enriched topological order and gauging symmetry.  In addition, because locality and universality are two desirable properties for quantum computation that are manifested in the representations of the braid group, this project also aims to formulate conjectures characterizing when these properties hold and to verify and adapt these conjectures where appropriate to better characterize physical and computational aspects.  To compare the computational power of topological quantum computers to that of classical computers, the project investigates the complexity of the most natural computation in this setting: topological invariants."
"1405348","Geometry and Complexity Theory","DMS","GEOMETRIC ANALYSIS, COMPUTATIONAL MATHEMATICS, Special Projects - CCF","09/01/2015","08/26/2015","Joseph Landsberg","TX","Texas A&M University","Standard Grant","Christopher Stark","08/31/2019","$218,782.00","","jml@math.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1265, 1271, 2878","9263","$0.00","Complexity theory addresses both practical and theoretical questions. Practical issues include developing new, efficient algorithms for computations that need to be done frequently  (such as multiplying matrices), as well as determining whether more efficient algorithms than the ones already known may exist. The theoretical aspects include  questions almost philosophical in nature, such as: Is there really a difference between  intuition and systematic problem solving? (This question was asked by Godel and contributed to the development of the famous P versus NP conjecture.) The project will use modern mathematical tools to address these questions, more specifically, algebraic geometry and representation theory. Although the research for this project is driven by questions from computer science, these questions are of interest to mathematics in their own right  and have the potential to guide future mathematical research in the way physics has done in recent years.<br/><br/>This project focuses on two central questions in complexity theory: the complexity of matrix multiplication and the Geometric Complexity Theory approach to Valiant's conjectures. Regarding matrix multiplication, linear algebra is central to all applications of mathematics,  and matrix multiplication is the essential operation of linear algebra.  In 1969 Strassen discovered  a new algorithm to multiply matrices significantly faster than the standard algorithm. This and subsequent work has led to the astounding  conjecture that asymptotically, it is essentially almost as easy to multiply matrices as it is to add them. It is a central question  to determine just how efficiently one can multiply matrices, both practically and asymptotically.  This project will prove bounds for how efficiently  matrices can be multiplied. Regarding Geometric Complexity Theory, a central question in theoretical computer science is whether brute force calculations can be avoided in problems such as the traveling salesman problem. This is the essence of the P versus NP  conjecture. The   Geometric Complexity Theory  (GCT) program  addresses such  questions  using geometric methods. GCT touches on central questions in algebraic geometry, differential geometry, representation theory and combinatorics. The goals  of this project are (i)  to better establish the mathematical foundations of GCT by solving open problems in combinatorics and classical algebraic   geometry and (ii) to solve complexity problems considered more tractable, such as determining whether or not the determinant polynomial admits a small formula."
"1522603","Collaborative Research: Modeling and Simulation of the Growth of Graphene Multilayers and Heterostructures","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/11/2015","Vivek Shenoy","PA","University of Pennsylvania","Standard Grant","Leland Jameson","08/31/2019","$150,000.00","","vshenoy@seas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","MPS","1271","7237, 8037, 9263","$0.00","This award supports research and educational activities in mathematical and computational modeling of two-dimensional materials. Motivated by recent technological advancements in the isolation and transfer of different two-dimensional materials, such as grapheme and hexagonal boron nitride, the investigators will focus on two types of problems: heterostructures, where two materials are brought together in the same plane, and stacked two-dimensional layers of graphene. Compared to homogeneous monolayers, heterostructures and stacked layers contain many more degrees of freedom that can be exploited to fabricate materials with specifically designed electronic, micromechanical and optical properties. These novel materials have applications in many areas identified as critical to US strategic interests such as nanotechnology, information technology, and energy technology. Thus far, efforts in this area have been mainly experimental, using trial-and-error approaches. The multiscale mathematical and computational models developed here will make a substantial contribution to the field by providing a rational framework to optimize the production process. Further, this framework can be extended to examine other materials such as arrays of semiconductor quantum dots, and magnetic clusters, or metal-organic surface networks that are promising candidates for energy conversion, thermal transport, and other device applications. Two graduate students will receive interdisciplinary training and will present their findings at conferences, which will enhance their professional training. Outreach efforts include teaching high school students as part of the California State Summer School for Mathematics and Science (COSMOS) at UC Irvine. These efforts will help develop future generations of scientists. <br/><br/>This project will investigate the nonlinear dynamics of the mechanisms that govern the growth and morphology of graphene multilayers and heterostructures and to develop strategies to control its growth by (1) developing and applying state-of-the-art adaptive numerical methods to large-scale computation and (2) performing analytical, numerical, and modeling studies of important constituent processes. The research will provide a novel framework for the rational design of such materials. A significant challenge is that the structure and morphology of heterostructures and stacked layers is determined both by atomic-scale phenomena and by the diffusion of multiple species and elastic interactions over length scales of hundreds of nanometers. Consequently, no single model is able to describe all the processes involved in the formation of graphene heterostructures. The investigators will adopt a multiple-scale approach in which atomistic and mesoscale algorithms will be developed to determine material properties and to predict the role of strain in heterostructures and vertically-stacked sheets as well as the atomic structures and defects. The atomistic simulations will provide material parameters and forces to new continuum phase field models that describe the growth of multicomponent in-plane and vertically-stacked sheets at larger scales. The highly nonlinear nature of these problems makes fast, accurate, and robust numerical methods essential to their study."
"1543876","International Workshop on Domain Decomposition Methods for PDEs","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/09/2015","Hansong Tang","NY","CUNY City College","Standard Grant","Leland Jameson","08/31/2017","$8,000.00","","htang@ccny.cuny.edu","Convent Ave at 138th St","New York","NY","100319101","2126505418","MPS","1271","7556, 9263","$0.00","Domain decomposition (DD) is an important and powerful methodology to solve partial differential equations arising from science and engineering. An international workshop on ""Domain Decomposition Methods for Partial Differential Equations"" will be held at Dalhousie University at Halifax, Nova Scotia, Canada during August 3-8, 2015. Driven by the urgent need to develop robust and efficient numerical techniques to simulate challenging problems, this workshop strives to bring together researchers in theory and application. It will provide a platform of interaction and collaboration between computational mathematicians and application researchers. The workshop will disseminate recent progress in DD research and promote its further development. This award provides support to defray expenses of US-based participants, including students and junior researchers, particularly those from under-represented groups. <br/> <br/>The workshop consists of three distinct but uniquely connected components. The first component is a two-day short course on DD methods for partial differential equations. The second one features research level talks given by mathematicians specialized in theory and researchers from application fields. The third component presents breakout interactive sessions for the short-course participants, DD experts, and applied researchers. The US-based participants from mathematics and engineering will attend the workshop and give presentations. The activities of the workshop, including abstracts, short-course notes, and talk slides will be made available on the workshop website: www.math.mun.ca/anasc/ddworkshop.html."
"1522751","Fast and Scalable Multigrid Methods for Hypergraph Partitioning Problems","DMS","COMPUTATIONAL MATHEMATICS","07/15/2015","07/14/2015","Ilya Safro","SC","Clemson University","Standard Grant","Leland Jameson","06/30/2019","$180,000.00","","isafro@udel.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","MPS","1271","9263","$0.00","The advancement of science requires the development of new mathematical methods that can rapidly and reliably solve large-scale scientific computing problems. Any modern scientific computing tool and supercomputer must have the ability to effectively (and one hopes optimally) manipulate both the data and parallel computational processes to manage: (a) the load-balancing in parallel computation; (b) data migration between components in a supercomputer; (c) performance optimization; (d) task scheduling; and (e) storage/memory reduction and data compression. These and many other problems (such as electronic chip design and community detection in social networks) can be tackled using a family of mathematical optimization problems called partitioning that are formulated on mathematical models called hypergraphs.  However, partitioning of hypergraphs is extremely hard in theory and practice. To tackle it, this research project will develop and investigate efficient and effective methods that are inspired by multigrid, which is one of the most successful classes of numerical methods for solving large-scale scientific computing problems.  <br/> <br/>The technical goal of this project is to carry out computational and theoretical investigations in algebraic and nonlinear multigrid methods for hypergraph partitioning motivated by various problems in the areas of computational mathematics and scientific computing. These investigations aim to provide breakthroughs in practical computational capabilities, modeling matrix-matrix (vector) multiplication, matrix partitioning, and general load-balancing for scientific computing applications. The results of the project will also deepen understanding of theory of multigrid methods applied to computational discrete optimization. In recent decades, multigrid-inspired methods for graphs (also known as multilevel) led to important breakthroughs in a variety of computational problems. However, in contrast to the multigrid-inspired methods for graph cut-based problems (such as graph partitioning and linear arrangement), multigrid-inspired methods for hypergraphs are relatively unexplored. This project aims to develop theory related to multigrid-inspired methods for discrete optimization problems on graphs and hypergraphs, of great practical importance in computational mathematics."
"1517152","Conferences on Frontiers in Applied and Computational Mathematics: 2015-2017","DMS","APPLIED MATHEMATICS, STATISTICS, COMPUTATIONAL MATHEMATICS, FD-Fluid Dynamics, MATHEMATICAL BIOLOGY","06/01/2015","05/16/2015","Michael Siegel","NJ","New Jersey Institute of Technology","Standard Grant","Lora Billings","05/31/2016","$20,000.00","Amitabha Bose, Cyrill Muratov","misieg@njit.edu","University Heights","Newark","NJ","071021982","9735965275","MPS","1266, 1269, 1271, 1443, 7334","058E, 059E, 7556, 9263","$0.00","This grant supports the participation of undergraduate and graduate students, postdoctoral fellows, junior faculty, and other researchers in a conference on ""Frontiers in Applied and Computational Mathematics"" (FACM) at the New Jersey Institute of Technology (NJIT) on June 5-6, 2015.  FACM 2015 is focused on mathematical fluid dynamics and applications in biomedicine and climate science.  There will be dedicated minisymposia on biofluid dynamics, the connection between mathematical modeling and experiment, geophysical fluid dynamics, wave propagation in fluids, and computational methods, including high-performance computing and multiscale methods. In addition, there will be five minisymposia on the application of statistics and data analysis to biomedicine and to atmosphere/ocean science, which are areas of great current interest.  This conference series brings together mathematicians, statisticians, scientists and engineers in an environment where significant interaction and cross-fertilization takes place. More information can be found on the conference website http://m.njit.edu/Events/FACM15/.<br/> <br/>The FACM conference series has been organized over the past eleven years by the Department of Mathematical Sciences and Center for Applied Mathematics and Statistics at NJIT. The annual meeting is a forum for the dissemination of research in applied and computational mathematics and statistics.  FACM conferences are more intimate and student centered than large society meetings, and a goal of the organizers is to introduce future leaders of applied mathematics to established investigators and emerging research areas. Participation among graduate students and postdocs is greatly enhanced with contributed talks through which they give presentations in minisymposia alongside leading scientists. For students and postdocs, this will be a learning and networking experience that will help them with their research and career paths. Special efforts will be made to continue the participation underrepresented groups in the conference."
"1522184","Multiscale Computational Methods for Semiclassical Schroedinger Equations with Non-Adiabatic Effects","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","07/15/2015","11/02/2018","Shi Jin","WI","University of Wisconsin-Madison","Standard Grant","Leland Jameson","06/30/2019","$270,000.00","","shijin-m@sjtu.edu.cn","21 North Park Street","MADISON","WI","537151218","6082623822","MPS","1266, 1271","7237, 9263","$0.00","The project addresses some fundamental issues in scientific computation in the modern age: multiscale modeling and simulation, which play essential roles in nanotechnology, communications, material sciences, and other areas of science and technology.   The research will develop state-of-the-art computational methods for systems involving multiscale quantum-classical coupling.  The methods under development have a wide range of applications to problems arising in solid-state physics, semiconductor device modeling, quantum chemistry, and materials science.  Several graduate students will be trained through these research activities.  Some of the research results also will be incorporated in the graduate curriculum to better train the next generation of researchers in modern applied mathematics.<br/><br/>The investigator will develop efficient semiclassical and multiscale computational methods for some problems in quantum dynamics with non-adiabatic effects. The non-adiabatic effects are important since they correspond to quantum transitions between different potential energy surfaces that are important to describe quantum dynamic behavior in chemical reactions and semiconductors. Specifically, the project will investigate surface hopping and quantum dynamics in periodic lattices. In the adiabatic cases only the diagonal entries of the Wigner matrix need to be considered in the semiclassical limit, which often results in classical Liouville equations. To account for the non-adiabatic effects, one needs to also follow the dynamics of the off-diagonal entries of the Wigner matrix in order to adequately describe the quantum transition between different energy surfaces. These yield coupled, inhomogeneous, oscillatory partial differential equations that will be numerically solved by Gaussian beam and multiscale methods."
"1521590","Numerical Analysis of Partial Differential Equation Constrained Optimization Problems","DMS","COMPUTATIONAL MATHEMATICS","07/15/2015","07/13/2015","Harbir Antil","VA","George Mason University","Standard Grant","Leland Jameson","05/31/2019","$139,985.00","","hantil@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1271","7237, 9263","$0.00","Every process in nature tries to optimize certain quantities under the available resources. The limited available resources act as constraints and, for this reason, constrained optimization problems are ubiquitous in science and engineering. Such optimization problems are inherently nonlinear due to the presence of constraints. This makes the development and analysis of algorithms and computational techniques to simulate the relevant phenomena extremely challenging and of practical relevance. A systematic approach is needed to allocate resources, without violating constraints, and remain effective in minimizing cost. This project will lead to development of advanced, efficient algorithms to design micro- and nano-scale lab-on-chip devices, as well as novel materials with desired configuration and user specified properties. As an outcome of shape optimization tools, robust algorithms for image processing and structural design will be developed. A significant amount of effort will be devoted to develop new models for cardiac electrical response and diffusion of water molecules in brain tissue. <br/><br/>Many of the partial differential equations (PDEs) that describe the constraints are geometric, nonlinear, and multiscale, with an unknown domain, i.e. free boundary problems (FBPs). This makes the development and analysis of numerical techniques for the solution of these problems challenging. This project focuses on constrained optimization problems with PDE constraints -- the so-called PDE constrained optimization problem. In fact, it aims to develop and study numerical techniques for a new collection of such problems for which classical ideas and methods do not work. They are: optimization with Stokes FBP (including surface tension) as constraint, problems where the Hessian appears in the cost functional, multilevel techniques for shape optimization and image segmentation problems, optimization with nonlocal (fractional) operators, and degenerate parabolic equations as constraints. The applications range from micro- to nano-scales, where surface effects dominate bulk effects. This will have impact in designing next generation lab-on-chip devices. By manipulating curvature, one can drive particles towards a desired assembly or a specific configuration. This will lead to highly sophisticated materials with user specified properties.  The multilevel techniques developed to solve shape optimization problems will provide a general tool to study inverse problems (acoustic scattering, for example), image processing, structural design, and fluid dynamics (for example, in aircraft design)."
"1521965","Fast and Stable Compact Exponential Time Difference Based Methods for Some Parabolic Equations","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/28/2015","Lili Ju","SC","University of South Carolina at Columbia","Standard Grant","Leland Jameson","07/31/2018","$200,954.00","","ju@math.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","MPS","1271","7237, 9150, 9263","$0.00","The goal of this project is to develop and analyze fast, stable, and accurate methods for numerical solutions of a family of parabolic equations that appear in diverse applications in science and engineering. The research will lead to production of very efficient and effective computational tools for problems typified by phase transition modeling, chemical reactions, population dynamics, cell membrane modeling, molecular beam epitaxy, fluid dynamics, and light propagation. The well-designed robust high-order algorithms would allow researchers to accurately catch the dynamics of these systems without high computational costs. This project also offers new insights into the understanding of the kinetic processes of microstructure coarsening, shape transformation of membrane lipid vesicles, and epitaxial growth of thin films through extensive numerical simulations. Graduate students will be directly involved in and benefit from their participation in the frontier research.<br/> <br/>Although exponential time integrator based techniques have been widely researched in the literature for solving semilinear or nonlinear parabolic equations of different orders, there still lack careful numerical and theoretical studies on accurate and stable treatments of stiff nonlinearities, direct and explicit incorporation of various inhomogeneous boundary conditions, and corresponding fast implementation algorithms. The methods in this project are explicit in nature, and they will utilize compact representations of high-order finite differences or spectral approximations for spatial operators in a rectangular domain, exponential multistep or Runge-Kutta approximations for accurate time integrations of boundary and stiff nonlinear terms, linear splitting schemes for effectively enhancing numerical stabilities, and FFT-based fast calculations for greatly reducing computational costs. The research will systematically study several techniques for improving accuracy and efficiency of the compact exponential time differencing methods in both space and time, and develop energy stability and error analyses for these schemes.  The project will also generalize and apply these methods to some important problems arising from the study of some biological and physical phenomena, such as phase field bending energy models for cell membrane shape transformation and molecular beam epitaxy models for thin film growth."
"1522768","Novel Ideas and Analysis for Interface and Fluid-Structure Interaction Problems and Applications","DMS","COMPUTATIONAL MATHEMATICS","09/15/2015","09/11/2015","Zhilin Li","NC","North Carolina State University","Standard Grant","Leland Jameson","08/31/2019","$250,000.00","","zhilin@math.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1271","9263","$0.00","Studying interface and fluid-structure interaction problems, such as those involving oil and water mixtures, gas bubbles, ice and water interfaces, tumor growth, or cell deformation, has many practical applications. It is often costly to carry out experiments on such systems, and computational simulation provides an alternative for study of these challenging problems. The objectives of this research project are to model such systems and to design efficient and practical computational algorithms to simulate, solve, and control those problems. Newly developed augmented immersed interface methods will be applied to several important applications in multi-phase flows and fluid-structure interactions. The investigator will develop and disseminate software packages implementing these methods. Graduate students will be involved in the research. <br/><br/>This project concerns the development and analysis of some new ideas for interface and fluid-structure interaction problems based on structured meshes. The methods under development will be supported by rigorous mathematical analysis and numerical experiments. Applications include optimal control of interface problems, fluid-structure interactions in modeling tissue mechanics coupled with cell biology, and tip propagation of a crack. The methods are based on structured meshes such as Cartesian meshes that are not necessarily aligned with the interface in two and three dimensions. The projects include: (1) new augmented methods for a fluid structure interaction between a fluid flow modeled by Stokes or Navier-Stokes equations and a porous medium modeled by the Darcy's law; (2) a new computational framework for accurate gradient computation on a boundary or interface with accurate solution globally; (3) a new second-order symmetric, consistent, and parameter-free immersed finite element method; (4) a new SVD-free augmented IFE method for elliptic interface problems with non-homogeneous jump conditions; (5) applications of the new methods for cell deformation in different layers (porous media, Stokes flow, Navier-Stokes flow) and scales; and (6)numerical simulations of crack propagation based on the Mumford-Shah minimizer. This project will have positive effect on education by attracting graduate students and postdoctoral researchers to conduct research in this area. Some components of the project will be designed as undergraduate projects. The project will produce software useful to computational science involving discontinuities and singularities, free-boundary/moving interfaces, multi-phase and multi-physics, and irregular domain problems."
"1418953","Efficient high order methods for two multiscale problems","DMS","COMPUTATIONAL MATHEMATICS","01/01/2015","12/19/2014","Wei Wang","FL","Florida International University","Standard Grant","Leland Jameson","12/31/2018","$126,125.00","","weiwang1@fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","MPS","1271","8396, 8609, 9263","$0.00","Multiscale problems are ubiquitous in engineering and physics. This kind of problem involves phenomena that occur across a variety of time and length scales, which may vary in orders of magnitude. To prevent inaccurate solutions, traditional approximation methods need extremely refined meshes to resolve all the scales, which places huge demands on memory and computation time and thus limits the applications. In this project, we construct new multiscale methods to efficiently and accurately solve two model equations that are broadly used in studies of detonation, combustion and turbulence involving reactions, and nanoscale semiconductor devices. The proposed research will develop new multiscale methods to meet with the increasing demand for computational resources in multiscale problems. Reliable and efficient multiscale methods will further help predict physical phenomena in realistic applications. <br/><br/>Specifically, this project focuses on multiscale methods for reactive flow equations and the Schrodinger equation. In high-speed reacting flows with multispecies and multireactions, incorrect propagation of discontinuities may occur in underresolved mesh regions. Our approach is to combine a high order shock-capturing scheme such as WENO for the convection part with Harten's subcell resolution for the reaction part. The subcell treatment utilizes the flow information and is able to control the dissipation of shock-capturing schemes to avoid the spurious solutions due to the underresolved mesh. The goal is to capture the correct locations of shocks and discontinuities in high-speed reacting flows with coarse meshes in both time and space. In simulations of electron transport modeled by the Schrodinger-Poisson system, the computational cost is huge due to the high frequency oscillations of the solution. The idea is to incorporate some known structures of the solution into the base functions of Discontinuous Galerkin methods. This can be accomplished by building local solution spaces based on the semiclassical approximation WKB asymptotic, which has certain multiscale structures of the solution. We aim to construct an inexpensive and reliable solver for Schrodinger-Poisson system to simulate quantum transport of electrons in nanoscale semiconductors."
"1522662","Collaborative Research: Randomized and Structure-Based Algorithms in Commutative Algebra","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","08/25/2015","Sonja Petrovic","IL","Illinois Institute of Technology","Standard Grant","Leland Jameson","08/31/2019","$229,577.00","Despina Stasi","spetrov1@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","MPS","1271","9263","$0.00","Systems of multivariate polynomial equations are ubiquitous in optimization, statistics, biology, and other fields of science and engineering. Solving such systems is a cornerstone of computational algebra today and the main focus of this project. The project addresses fundamental problems in symbolic computation with multivariate polynomials, with particular interest in very large systems that appear, for example, in biological data modeling and data mining. Such systems are so large that they cannot be completely read into the computer's memory. This project proposes the use of probabilistic and statistical analysis to cleverly select and sample smaller subsystems that lead to the desired solution. The problems attacked are fundamental questions of practical relevance. The project will also have educational and training activities in the development of human resources. In addition to many students working with the investigators, the project includes a Summer School on the foundational mathematical concepts from the areas relevant to this interdisciplinary research project. The target audience is graduate students; the School will foster a sense of community among the students and enhance further interdisciplinary collaboration. <br/><br/>This research project approaches the problem of solving systems of polynomial equations, and of finding generators for polynomial ideals using a probabilistic method -- focusing on providing low expected runtime and the use of random choices -- for computational algebra problems that have high worst case complexity. The project uses the underlying combinatorial structure of certain families of problems (e.g. polynomial system feasibility) for significant speed-up. The resulting algorithms and software will be of use in commutative algebra, statistics, optimization, graph theory, and other fields where large-scale systems of polynomial equations arise naturally. The project adapts to the problem under study a sampling technique that has been used in computational geometry and optimization. The theoretically expected running time will be linear in the number of input polynomials. There are several applications including statistics and optimization, where key applied methods rely on algorithms to compute such ideal generators. Furthermore, the research will increase the use of combinatorial structures in polynomial computational problems, in particular, for the calculation of Nullstellensatz infeasibility certificates and syzygies. Applications have been found in graph theory, and further applications are expected in combinatorics, coding theory, and systems biology."
"1522672","Collaborative Research:  Reduced Order Modeling of Realistic Noisy Flows","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/28/2015","Zhu Wang","SC","University of South Carolina at Columbia","Standard Grant","Leland Jameson","07/31/2018","$111,258.00","","wangzhu@math.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","MPS","1271","1303, 9150, 9263","$0.00","Many flows in engineering, geophysics, and medicine pose two significant challenges for computations.  First, the computational resources that are available for the numerical simulations can accommodate only low spatial and temporal resolutions.  Therefore, standard numerical methods usually yield extremely inaccurate results.  To alleviate this, state-of-the-art numerical methods generally use spatial filtering to eliminate the noise (i.e., numerical artifacts).  The second challenge posed by these realistic flows is that they require numerous repeated runs (e.g., to determine optimal parameters in automobile design or cardiovascular flow simulation, or to find appropriate initial conditions in weather forecasting and climate modeling).  These repeated runs can tremendously increase the computational cost of the numerical simulations.  Thus, low cost surrogate models (called reduced-order models) that target only the dominant flow structures are generally used.  Combining state-of-the-art data generation methods and reduced-order modeling is required for an accurate and efficient numerical simulation of realistic flows.  A simplistic attempt to combine these two approaches is, however, doomed to fail due to numerical instability, noisy data, and modeling inconsistency.  This project aims to develop a framework that will transform reduced-order modeling into a robust tool that can tackle the challenges raised by realistic noisy flows in engineering, geophysics, and medicine. <br/> <br/>The numerical simulation of many realistic flows is fraught with difficulties (insufficient numerical resolution; numerical instability; need for repeated runs).  To address these challenges, state-of-the-art numerical approaches are needed: large eddy simulation (LES) and regularized models tackle the lack of numerical resolution and the instability, whereas reduced-order models (ROMs) based on proper orthogonal decomposition (POD) balance the computational cost and accuracy when repeated runs are needed.  A simplistic attempt to combine LES and regularized models with standard ROMs is, however, doomed to fail due to the following reasons: (i) standard ROMs are plagued by numerical instability; (ii) although LES and regularized models stabilize the numerical simulations, the data that they generate for ROMs is inherently noisy; and (iii) the modeling inconsistency between data generation (i.e., regularized and LES models) and ROMs can yield inaccurate results.  This project will develop a modeling, theoretical, and computational framework that will transform reduced-order modeling into a robust tool that can tackle the challenges raised by realistic noisy flows.  The main innovation is the explicit POD spatial filter, which bridges the inconsistency gap between the data generation (i.e., regularized and LES models) and ROMs.  This breakthrough paves the way for the development of novel regularized ROMs and the introduction in a ROM setting of genuine LES models that use approximate deconvolution to recover subfilter-scale information.  Over the last decades, a wealth of regularized and LES models have been highly developed in the engineering and geophysics communities.  The explicit POD spatial filter represents the missing link that finally allows the leverage of these successful approaches in reduced-order modeling."
"1522158","Collaborative Research: Randomized and Structure-Based Algorithms in Commutative Algebra","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","07/11/2017","Jesus De Loera","CA","University of California-Davis","Continuing Grant","Leland Jameson","08/31/2019","$160,001.00","","deloera@math.ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1271","9251, 9263","$0.00","Systems of multivariate polynomial equations are ubiquitous in optimization, statistics, biology, and other fields of science and engineering. Solving such systems is a cornerstone of computational algebra today and the main focus of this project. The project addresses fundamental problems in symbolic computation with multivariate polynomials, with particular interest in very large systems that appear, for example, in biological data modeling and data mining. Such systems are so large that they cannot be completely read into the computer's memory. This project proposes the use of probabilistic and statistical analysis to cleverly select and sample smaller subsystems that lead to the desired solution. The problems attacked are fundamental questions of practical relevance. The project will also have educational and training activities in the development of human resources. In addition to many students working with the investigators, the project includes a Summer School on the foundational mathematical concepts from the areas relevant to this interdisciplinary research project. The target audience is graduate students; the School will foster a sense of community among the students and enhance further interdisciplinary collaboration. <br/><br/>This research project approaches the problem of solving systems of polynomial equations, and of finding generators for polynomial ideals using a probabilistic method -- focusing on providing low expected runtime and the use of random choices -- for computational algebra problems that have high worst case complexity. The project uses the underlying combinatorial structure of certain families of problems (e.g. polynomial system feasibility) for significant speed-up. The resulting algorithms and software will be of use in commutative algebra, statistics, optimization, graph theory, and other fields where large-scale systems of polynomial equations arise naturally. The project adapts to the problem under study a sampling technique that has been used in computational geometry and optimization. The theoretically expected running time will be linear in the number of input polynomials. There are several applications including statistics and optimization, where key applied methods rely on algorithms to compute such ideal generators. Furthermore, the research will increase the use of combinatorial structures in polynomial computational problems, in particular, for the calculation of Nullstellensatz infeasibility certificates and syzygies. Applications have been found in graph theory, and further applications are expected in combinatorics, coding theory, and systems biology."
"1521051","Collaborative Research:  Numerical Methods for Partial Differential Equations Arising in Shallow Water Modeling","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","08/21/2017","Alina Chertock","NC","North Carolina State University","Continuing Grant","Leland Jameson","08/31/2019","$250,000.00","","chertock@math.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1271","8060, 8396, 8609, 9263","$0.00","This research project will contribute significantly toward development of computational methods for shallow water and related models. Special attention will be paid to applications arising in oceanography, in atmospheric sciences, and in hydraulic, coastal, civil, and enhanced oil recovery engineering, in which rapid changes in the bottom topography, Coriolis forces, friction, nonconservative terms, and uncertain phenomena have to be taken into account. The problems under study include rainwater drainage and flooding in urban areas, shallow water models of turbidity currents, multilayer flows, and shallow water models with uncertain data. The new tools under development promise to have great potential in designing coastal protection systems and investigating the effects of sediment transport on shelf drilling platforms as well as contributing toward the development of flood mitigation systems and planning of new urban areas. <br/><br/>The project is aimed at developing accurate, efficient, and robust numerical methods for shallow water equations and related models, with particular reference to problems that admit nonsmooth (discontinuous) solutions and involve complicated nonlinear waves, moving interfaces, and uncertain data. Shallow water models are systems of time-dependent partial differential equations (PDEs) that are derived using physical properties such as conservation of mass and momentum, and hydrostatic or barotropic approximations. Naturally these models, especially in the cases of high space dimensions, require development and implementation of special numerical techniques such as numerical balancing between the terms that are balanced in the original system of PDEs (development of well-balanced schemes), ensuring positivity of all fluid layers (this is absolutely necessary for both accurate description of dry and near dry states and enforcement of nonlinear stability), operator splitting methods, interface tracking approaches, and others that will be in the focus of the research project. The development of new techniques will be based on high-order shock-capturing finite-volume schemes, accurate and efficient ODE solvers, and stochastic Galerkin methods, utilizing major advantages of each one of these methods in the context of the problems under study."
"1522687","Stable, Efficient, Adaptive Algorithms for Approximation and Integration","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/12/2017","Fred Hickernell","IL","Illinois Institute of Technology","Continuing Grant","Leland Jameson","07/31/2018","$270,000.00","Gregory Fasshauer","hickernell@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","MPS","1271","9263","$0.00","Computational methods allow the simulation of experiments that are too costly, too dangerous, or otherwise infeasible to perform in physical situations.  Examples include evaluating the safety and efficiency of designs for nuclear reactors, predicting the frequency of breakdowns in power grids, and assessing the risks and rewards of financial investments.  This research project will develop new ways of computing surrogate models for time-consuming simulations.  These new surrogate models will be quicker to compute, avoid catastrophic computer error, and more faithfully represent the processes that they are designed to model.  More clever ways of exploring future scenarios will be developed to decrease the computational time required to find the average, or the worst possible, behavior of complex systems such as those mentioned.  The new algorithms arising from this research will be made publicly available for other researchers and practitioners.  Undergraduate and graduate students involved in developing these algorithms will be better prepared for scientific careers.<br/><br/>Function approximation and integration are two fundamental problems in computational mathematics. This research project will construct algorithms for function approximation and integration that are computationally stable, avoiding catastrophic round-off error.  These algorithms will be adaptive, determining the algorithm parameters based on function data to meet the user-specified error tolerance with rigorous justification. The algorithms will also be asymptotically efficient, requiring essentially the same computational effort as the best possible algorithms.  Function approximation algorithms will be based on the Hilbert-Schmidt SVD decomposition that the investigators have developed for kernel methods.  Integration algorithms will be developed that adaptively determine the sample size required.  The investigators will continue to mentor students as well as post-doctoral scholars and will establish the usefulness of the algorithms under development through collaborations with domain specialists."
"1522577","Advancements in the Ultraspherical Spectral Method","DMS","COMPUTATIONAL MATHEMATICS","08/01/2015","07/28/2015","Alex Townsend","MA","Massachusetts Institute of Technology","Standard Grant","Junping Wang","10/31/2016","$144,797.00","","townsend@cornell.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1271","9263","$0.00","The numerical solution of real-world fluid flow and airfoil problems needs an accurate, flexible, and fully-adaptive spectral element method. The so-called ultraspherical spectral method, with its sparsity and regularity preserving discretizations, is promising to overcome many of the traditional computational barriers. This research project will exploit and investigate the remarkable properties of the ultraspherical spectral method with the aim of producing a high quality and industrial-strength spectral element solver for partial differential equations. One key feature will be its robustness to pinching boundary features, typical with airfoils, that will alleviate the current tremendous burden on mesh generation algorithms. The project will radically alter the perception of spectral methods in the computational mathematics and engineering communities by extensively demonstrating that, when done carefully, they can be a flexible, general, and powerful numerical tool.<br/><br/>Today's pseudospectral methods deliver both convenience and spectrally accurate discretizations for the solution of differential equations. However, they lead to dense discretizations, numerical instability, and a severe limitation to simple geometries. The novel ultraspherical spectral method is an alternative that retains the same accuracy and convenience, but leads to almost banded well-conditioned discretizations that faithfully preserves the regularity of the underlying differential operator while also being amenable to specialized fast linear algebra routines. Based on this new spectral method, the PI will derive a new mathematically-grounded fully-adaptive spectral element method for meshed geometries. Key novel computational features will include: (1) A high accuracy on mesh elements that is independent of the aspect ratio; (2) True hp-adaptivity that allows for essentially arbitrarily large element degree p and small average mesh element size h (without concern of ill-conditioning); and (3) The flexibility to solve a wide range of differential equations with general boundary constraints; and (4) Local refinement and mesh coarsening for the resolution of corner singularities. This new spectral element method will be applied to challenging partial differential equations for the state-of-the-art numerical simulation of advection-dominated fluid flow problems."
"1522707","A Posteriori Error Estimation through Duality and Some Other Topics","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","07/04/2017","Zhiqiang Cai","IN","Purdue University","Continuing Grant","Leland Jameson","08/31/2019","$260,000.00","","zcai@math.purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1271","9263","$0.00","Self-adaptive numerical methods provide a powerful and automatic approach in scientific computing. In particular, Adaptive Mesh Refinement (AMR) algorithms have been widely used in computational science and engineering and have become a common tool in computer simulations of complex natural science and engineering problems. As identified by the US National Research Council, AMR is one of two necessary tools (AMR and Parallel Computing) for computationally tackling Grand Challenge problems. The key ingredient for success of AMR algorithms is a posteriori error estimates that are able to accurately locate sources of global and local error in the current approximation. Another challenge in computer simulations of complex systems is the reliability of computer predictions. These considerations (efficiency in AMR algorithms and error control) demonstrate the need for an error estimator that can a posteriori be extracted from the computed numerical solution and the given data of the underlying problem. Such an a posteriori error estimate ideally should provide an underlying rigorous mathematical theory for estimating and quantifying discretization error in terms of the error's magnitude and its spatial distribution. Success in this project will allow AMR algorithms to automatically locate physical interfaces, detect layers and discontinuities, and resolve oscillations of various scales. The dual estimators to be developed in this project will resolve the most natural but extremely difficult question of discretization error control on coarse meshes for a class of problems and hence partially guarantee reliability of computer simulations.<br/><br/>This research project focuses on the development, analysis, and test of a posteriori error estimators through the methodology of duality. The dual estimators to be developed in this project will have a guaranteed reliability bound with reliability constant being one. Hence, these estimators are perfect for discretization error control and may be used as an accurate stopping criterion for iterative solvers. The methodology of duality may be applied to a large class of problems arising from continuum mechanics including linear and nonlinear problems. Since these estimators will not use a priori knowledge on the locations and characteristics of interface singularities, discontinuities (in the form of shock-like fronts, and of interior and boundary layers), and/or oscillations of various scales (multi-scale phenomena), they may then be applied more readily to highly nonlinear problems and have the potential of being applied to complex systems arising in applications. The emphases and the difficulties of the proposed research are (1) explicit or local construction of an approximation to the dual variable such that the resulting indicator is efficient and robust, and (2) theoretical and numerical confirmation of the efficiency and robustness. Finally, a small portion of the proposed research addresses an open theoretical question on the robustness of estimators for interface problems."
"1522398","Algorithms for Complex Systems","DMS","COMPUTATIONAL MATHEMATICS","08/15/2015","08/13/2015","David Aristoff","CO","Colorado State University","Standard Grant","Leland Jameson","07/31/2019","$179,802.00","","aristoff@rams.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1271","9263","$0.00","Complex systems arise in many scientific problems. They are high dimensional structures, comprised of many locally interacting agents, with emergent phenomena often occurring at multiple length and time scales. Such systems have global structural and dynamical properties that are usually impossible to determine exactly. Instead, these properties must be estimated by computer experiment and simulation. Unfortunately, due to the size and multiscale effects in complex systems, straightforward algorithms are often too slow. Thus, fast algorithm design, along with a rigorous study of accuracy, is crucial. This research project focuses on designing, improving, and quantifying the error of state-of-the-art algorithms for complex systems. Potential applications include a wide range of problems arising in materials science, computational chemistry, and solid-state physics. In particular, the methods studied could help pave the way for cheap and efficient in silico drug design. <br/><br/>The principal investigator will analyze state-of-the-art algorithms using a mixture of rigorous mathematical analysis and computer experiment. An important application will be the efficient simulation of metastable systems, in which the system dynamics tend to remain for very long times in certain subsets of state space. Such systems are widespread in molecular dynamics, an increasingly important tool in computational chemistry. In molecular dynamics, metastability arises from the well-known time scale problem: atomic vibrations occur on a time scale much smaller than that of thermally activated reactions and other interesting dynamical events. For this reason, it is usually impossible to observe the most interesting aspects of the dynamics by direct atomistic simulations. For metastable dynamics, many approximate simulation methods serve as alternatives to direct atomistic simulation, but they are limited by applicability and accuracy. The project will focus on several methods for overcoming metastability, including the parallel replica method, milestoning, and kinetic Monte Carlo. The PI will introduce a mathematical framework for generalizing these algorithms, based largely on the quasi-stationary distribution, a mathematical object that encodes metastability. The PI will show how the more general framework leads to new applications, including more efficient simulation of Markov State Models and glasses. Moreover, the PI will use this framework to pursue rigorous error estimates, which are crucial for extracting quantitative information from simulations."
"1547107","Eighth Annual Graduate Student Mini-conference in Computational Mathematics; Clemson, SC; February 5-6, 2016","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","07/30/2015","Leo Rebholz","SC","Clemson University","Standard Grant","Leland Jameson","08/31/2017","$9,808.00","Qingshan Chen, Hyesuk Lee","rebholz@clemson.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","MPS","1271","7556, 9150, 9263","$0.00","This grant supports the eighth annual ""Graduate Student mini-Conference in Computational Mathematics"" which will be held at Clemson University on February 5 and 6, 2016. Topics covered in the conference consist of recent advances in the theory and implementation of numerical methods for partial differential equations, including multiphase flow, porous media, numerical methods for stochastic PDEs, turbulence, fluid dynamics, climate simulation and PDE constrained optimization. Aside from one plenary speaker, all talks will be given by graduate students. <br/> <br/>Approximately twenty graduate students from Clemson, Virginia Tech, the University of Pittsburgh, Auburn University, the University of South Carolina, Florida State University, the University of Tennessee, and Emory University will present scientific research talks involving mathematical modeling in science and engineering, and rigorous analysis and efficient implementation of numerical methods for the modeling equations. The conference organizers strongly encourage participation of under-represented groups, and it is expected that nearly half of the graduate student speakers will be from an under-represented group. The goals of the conference are to give graduate students an opportunity to give a professional scientific talk, to bring graduate students and researchers (their advisors) together to discuss collaborative research projects, and to exchange new ideas in many challenging research areas."
"1522782","Theory, Modeling and Computation of Chemical Enhanced Oil Recovery","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","09/09/2016","Prabir Daripa","TX","Texas A&M University","Standard Grant","Leland Jameson","08/31/2019","$144,005.00","","daripa@math.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1271","8396, 8609, 9263","$0.00","Chemical Enhanced Oil Recovery (EOR) is an important energy technology. It involves injection of complex fluids containing chemicals that help improve oil recovery, in particular, net oil recovered, sweeping efficiency, net present value, and so on. Therefore, quantitative evaluation of these performance measures of any EOR flooding scheme is very important for decision making. Current practice does not account accurately for some important physical effects such as elasticity of the complex displacing fluids which can have a significant bearing on the net oil recovered. This project will enable accurate modeling of the underlying physical processes as well as contribute to the development of efficient numerical methods and software to solve the complex mathematical equations describing these EOR processes. One of the fundamental phenomena, known as elastic turbulence, a subject of intense current study in the turbulence area, also has a significant role to play in the EOR processes in porous media. This is due to the fact that an increase in drag due to elastic turbulence is undesirable from an economic point of view for EOR. Complex systems of mathematical equations to be used in our study will advance the field of computational mathematics, computational fluid dynamics and theoretical fluid mechanics, in particular, complex fluids and elastic turbulence. Progress in these areas has been very slow and the proposed work will advance they state-of-the-art. Research findings will advance the area of fluid mechanics and will have an impact in many engineering disciplines, applied and computational mathematics, and computational science, to name a few. The project also has many educational components.<br/><br/>The PI will study chemical enhanced oil recovery processes theoretically and numerically by developing appropriate numerical methods and software for this purpose. In particular, the PI will develop a novel multiphase, multi-component, viscoelastic, porous media flow model, efficient and high order accurate novel multi-scale finite element- and method of characteristics-based numerical methods for highly heterogeneous media, and user-friendly software encapsulating the numerical methods. The work combines theory, modeling and computation, and blends knowledge from fluid dynamics, porous media flows, numerical analysis, partial differential equations and scientific computing to gain insight into several fundamental problems: How does the elasticity modify development of linear and fingering instabilities? What is the mechanism behind transition from elastic instability to elastic turbulence in porous media as the Weissenberg number is gradually increased? How do different rheological properties affect the sweeping efficiency of displacement processes and net oil recovered? Numerical simulations with the software developed will play a significant role in this direction. The potential benefit of the proposed research lies in its open-ended nature which can further stimulate basic research and applications related to numerical methods, complex fluids and their use in EOR. Our research will contribute to rapidly developing fields of numerical methods and fast algorithms for EOR using viscoelastic fluid models in porous media, transition to elastic turbulence in porous media and scientific computing. This project is part of a broader research program in the PI's group on fluid mechanics, enhanced oil recovery, interfacial dynamics, multiphase multi-component continuous and heterogeneous porous media flows, fast high-order algorithms, and scientific computing."
"1522765","Collaborative Research:  Random Dynamics on Networks","DMS","COMPUTATIONAL MATHEMATICS","08/15/2015","08/17/2017","Pierre Gremaud","NC","North Carolina State University","Continuing Grant","Leland Jameson","07/31/2019","$250,000.00","","gremaud@math.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1271","9263","$0.00","Transport and distribution networks come in a number of forms, from animal cardiovascular and respiratory systems to communication and industrial infrastructures. Practical issues abound: prediction of local spikes, estimation of perfusion, and impact of structural changes such as vessel occlusion. The complexity of such phenomena can be illustrated by the well-known Braess' paradox: adding links to a transportation network might not improve the operation of the system! In spite of recent successes, our understanding of network flows is usually limited to small deterministic problems, while most applications correspond to large uncertain ones. The goal of this project is to enable improved predictions in biological and technological transport and diffusion networks. For instance, can one predict how cerebral blood flow will be affected if one of the carotids becomes narrow or blocked? Will the vasculature allow for re-routing? If so, with what probability and how fast? <br/> <br/>The main challenge in this research project is the presence of uncertainties. For many applications, only partial information about the systems is available. For instance the size or even the presence of a specific vessel might be uncertain or the status of a router unknown. The analysis of such problems requires the creation of novel mathematical tools and numerical methods to describe how uncertainties propagate through vast and complex networks. The computational tools to be constructed will provide information, usually probabilistic in nature, regarding phenomena that are difficult, expensive, or impossible to measure."
"1522231","2015 Gene Golub SIAM Summer School (G2S3): Randomization in Numerical Linear Algebra (RandNLA)","DMS","COMPUTATIONAL MATHEMATICS","06/15/2015","06/15/2015","Ilse C.F. Ipsen","NC","North Carolina State University","Standard Grant","Junping Wang","05/31/2016","$25,000.00","","ipsen@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1271","7556, 9263","$0.00","This project supports the participation of about 25 US based Ph.D. students at the Gene Golub SIAM Summer School. The Summer School will take place 15-26 June 2015, at the European Cultural Centre of Delphi, in Greece, and the topic is ""Randomization in Numerical Linear Algebra (RandNLA)"". Randomization, when it comes down to it, amounts to playing dice and taking chances. At first sight it appears to be an unconscionable approach for a scientific investigation. However, randomization goes back to Los Alamos National Laboratory in 1946, where John von Neumann, Stanislaw Ulam and Nicholas Metropolis designed Monte Carlo methods to model the behavior of neutrons for the purpose of radiation shielding. When applied judiciously and with great statistical care, randomized sampling can reduce a data deluge to a manageable volume.<br/><br/>RandNLA, specifically, is a new interdisciplinary area that targets big data applications in data mining, information retrieval, and internet modeling,  as well as more traditional areas like genetics and astronomy. The difficulty is guaranteeing reliability and predictable behaviour in the face of randomization. The development and analysis of RandNLA methods requires innovative tools from applied mathematics, statistics, computer science, and optimization. The summer school will give the students an interdisciplinary education in an area that is too fresh to have produced textbooks. They will learn the relevant theoretical foundations in linear algebra, probability theory, numerical methods, theory of algorithms, and convex optimization; and they will be exposed to applications in data analysis and machine learning. The students will also interactively participate in projects that combine algorithm design, numerical implementations, and empirical evaluations on real data. More information about this summer school can be found at http://scgroup19.ceid.upatras.gr/g2s32015/."
"1521555","Finite Element Methods for High Order Eigenvalue Problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2015","08/10/2015","Jiguang Sun","MI","Michigan Technological University","Standard Grant","Leland Jameson","08/31/2019","$144,996.00","","jiguangs@mtu.edu","1400 Townsend Drive","Houghton","MI","499311295","9064871885","MPS","1271","9263","$0.00","Numerical computation of eigenvalue problems is of fundamental importance in many scientific and engineering applications such as structural dynamics, quantum chemistry, electrical networks, magnetohydrodynamics, control theory, and inverse problems. This project focuses on developing effective and efficient finite element methods for two high order eigenvalue problems, the quad-curl eigenvalue problem (QCE) and the Maxwell's transmission eigenvalue problem (MTE), arising from the electromagnetic inverse scattering theory. The MTE has received significant attention recently due to several facts: 1) transmission eigenvalues are closely related to non-scattering waves; 2) transmission eigenvalues can be determined from scattering data and thus play an important role in a variety of inverse problems in target identification and nondestructive testing; 3) the MTE is non-selfadjoint and does not seem to be treatable by standard techniques for partial differential equations. The proposed research will provide mathematicians and engineers reliable new tools for the QCE and MTE. Furthermore, the physics and theory of the MTE are not yet fully understood. Numerical results may lead physicists and mathematicians in the correct direction.<br/><br/>There are two major difficulties for developing finite element methods for the QCE and MTE. Computation of eigenvalue problems usually starts with the corresponding source problems. It is challenging to develop finite element methods for high order partial differential equations. The second difficulty is that a successful method for the corresponding source problem might not be a good choice for an eigenvalue problem. To overcome the above difficulties, the PI aims to develop spectrally correct finite element methods including 1) discontinuous Galerkin method for the QCE; 2) iterative discontinuous Galerkin method for the MTE; 3) finite element methods for a quadratic reformulation of the MTE. The proposed research will be an important advance of finite element methods for high order eigenvalue problems. Successful results will enrich finite element theories, in particular, for non-selfadjoint and nonlinear eigenvalue problems."

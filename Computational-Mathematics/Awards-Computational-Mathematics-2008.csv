"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"0811062","Collaborative Research: Second-order methods for large-scale optimization in compressed sensing","DMS","COMPUTATIONAL MATHEMATICS","07/15/2008","04/04/2009","Roummel Marcia","NC","Duke University","Continuing Grant","Leland Jameson","09/30/2010","$132,529.00","Rebecca Willett","rmarcia@ucmerced.edu","2200 W MAIN ST","DURHAM","NC","277054640","9196843030","MPS","1271","0000, 9263, OTHR","$0.00","Nonlinear image reconstruction based upon sparse representations of signals and images has received widespread attention recently with the advent of compressed sensing. This emerging theory indicates that, when feasible, judicious selection of the type of distortion induced by measurement systems may lead to dramatically improved inverse problem solutions. In particular, when the signal or image of interest is very sparse (i.e., zero-valued at most locations) or highly compressible in some basis, relatively few indirect observations are necessary to reconstruct the most significant non-zero signal components. Compressed sensing theory states that sparse signals can be recovered very accurately with high probability from indirect measurements by solving an appropriate optimization problem. This research aims at the development of significantly more efficient methods for solving compressed sensing minimization problems. A crucial property of the proposed methods is that the algorithms are designed to be ""matrix-free"", i.e., they do not require the storage of (potentially very large) second-derivative matrices. Instead, these methods use matrices only as operators for matrix-vector products. <br/>This research also includes the application of the developed solvers to real large-scale problems from image processing (e.g., coded aperture superresolution, hyperspectral image reconstruction, and compressive video reconstruction), as well as theoretical proofs of convergence and numerical stability of the algorithms. <br/><br/>""Compressed sensing"" is an emerging field in computational mathematics that aims to improve signal (and image) reconstruction with less data. <br/>More efficient methods for compressed sensing can benefit such fields as medical imaging, astrophysics, biosensing, and geophysical data analysis. The basic theory exploits the fact that many natural signals in science and engineering are ""sparse""--that is, they can be represented as a weighted combination of a small subset of commonly occurring signals. When a signal is sparse, scientists can accurately reconstruct the original signal using a relatively small number of measurements of the original signal. However, in practice finding the right weighted combination of signals can create staggering numerical and computational complexity. This research aims to develop novel optimization methods that can quickly find accurate solutions to these large-scale problems. For example, consider an astronomer wishing to image the night sky, which consists of small, bright stars against a dark background. A conventional digital camera or imaging system would need a very high resolution and sensitive photodetector to effectively localize the different stars, but collecting this large number of pixels can be very costly and energy inefficient. This work allows the astronomer to collect a relatively small number of random projection measurements of the scene and use these to reconstruct the image with a high probability of accuracy. Fast and accurate optimization algorithms for sparse signal reconstruction can impact many other areas of image and signal processing as well, from reducing the dose of CT scans in biomedical imaging and improving image resolution in video surveillance systems for airport security, to more efficiently transmitting communication signals from distant satellites and NASA spacecraft and more carefully monitoring the health of a forest ecosystem using hyperspectral imaging."
"0813502","Midwest Conference on Mathematical Methods for Biomedical Images and Biological Surfaces; September 2008","DMS","COMPUTATIONAL MATHEMATICS","07/15/2008","07/11/2008","Yang Wang","MI","Michigan State University","Standard Grant","Junping Wang","06/30/2009","$25,000.00","Guowei Wei","ywang@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","0000, 7556, 9263, OTHR","$0.00","The investigators propose a three-day meeting, the Midwest Conference on Mathematical Methods for Biomedical Images and Biological Surfaces, to be held in the fall semester of 2008 in Michigan State University. This meeting will bring together researchers at different career stages and graduate students, mainly from the Midwestern area who work or are interested in mathematical methods for biomedical images and biological surfaces. With the explosion of new ideas and exciting new challenges in this area, this meeting aims to provide a forum to exchange new ideas and results related to research and equally importantly, to foster the interaction and collaboration between faculties and students in mathematics, engineering and life sciences. <br/><br/>The requested funding will be used principally to support participants. Additional funding will come from Michigan State University. A large portion of the NSF funding will be used to support the participation of graduate students and junior faculty members, particularly those in the under-represented minority groups.<br/>"
"0810104","New numerical methods for Hamilton-Jacobi equations, Gaussian beams, and kinetic inverse problems","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/30/2008","Jianliang Qian","MI","Michigan State University","Standard Grant","Leland Jameson","06/30/2012","$170,576.00","","qian@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","0000, OTHR","$0.00","The investigator, with his students and collaborators, develops novel and efficient numerical methods for Hamilton-Jacobi equations, Gaussian beams, and kinetic inverse problems. Hamilton-Jacobi equations arise from seismic wave propagation, geometrical optics, optimal control, traveltime tomography, medical imaging, computer vision, and material sciences. His previous works range from fast sweeping methods for Hamilton-Jacobi equations on triangulated meshes, level-set based Eulerian geometrical optics, to fast numerical methods for traveltime tomography. These successful works lead him to develop more powerful numerical methods for these equations and incorporate these new numerical methods into seismic modeling and inversion, as well as other possible applications.<br/>Problems under consideration include developing Legendre-transform based fast sweeping methods for stationary Hamilton-Jacobi equations on triangulated meshes, developing fast algorithms for kinetic inverse problems based on discretizing eikonal equations on triangulated meshes, developing fast algorithms for geodesic X-ray transforms in kinetic inverse problems, developing Eulerian Gaussian beams for high frequency waves, and developing Eulerian Gaussian beam methods for semi-classical quantum mechanics.<br/>This investigation advances the state-of-the-art in numerical methods for Hamilton-Jacobi equations, high frequency wave propagation and kinetic inverse problems.<br/><br/>These fields and applications are of great strategic value in the US petroleum industry, in the medical imaging, and in material sciences and nanotechnology. The current surge in price for crude oil and other earth resources increasingly demands better imaging techniques in exploration seismology. The increasing amount of data in global and exploration seismology requires more sophisticated mathematical models. The techniques developed as part of this project will provide crucial tools for the development of the next-generation seismic imaging tools that enable substantial cost savings in seismic explorations and expedite routine data processing.<br/>"
"0811041","L1-based Approximation Techniques for PDEs","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/30/2008","Bojan Popov","TX","Texas A&M Research Foundation","Standard Grant","Junping Wang","08/31/2012","$329,997.00","Jean-Luc Guermond","popov@math.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1271","0000, 9263, OTHR","$0.00","L1-approximation methods have recently gained momentum due to profound theoretical results relating L1 to sparse representation of data.<br/>This property is at the origin of the compressed sensing technique.<br/>In parallel to the development of compressed sensing movement, the investigators started in 2005 a NSF-sponsored research program to develop a new nonlinear approximation technique based on L1 minimization for solving first-order nonlinear differential equations. In this research program the investigators proved that L1-based methods are very efficient tools for approximating first-order nonlinear differential equations whose solutions are discontinuous or have sharp interfaces.  In the forthcoming research program the investigators will do the following: <br/>(i) develop fast algorithms for computing L1-minimizers; (ii) extend their methodology to time-dependent nonlinear first-order differential equations; (iii) develop L1-based techniques for surface reconstruction, data enhancing, image recovery/de-blurring.<br/><br/><br/>The outcome of this project will be a computational framework radically different from the existing mainstream techniques.  The key is to rely on sparsity in the spirit of compressed sensing.  Both the computational and theoretical aspects of the project are very challenging because of the strong nonlinearity and lack of smoothness introduced by L1. This research project will have a broad impact in engineering (mechanical, aerospace, ocean, etc.), environmental sciences, geophysics, petroleum engineering.  Proposing a novel robust approximation technique for solving nonlinear problems developing shock or sharp interfaces will eventually benefit every areas of science and engineering where controlling or dealing with this type of phenomenon is still an enormous challenge. The algorithms developed by the investigators will also be useful for solving problems like surface reconstruction from scattered data, face recognition, data recovery and enhancing. This last aspect of the program could be useful for national security purposes.<br/>"
"0810387","Collaborative Research: Tuning-free adaptive multilevel Discontinuous Galerkin methods for Maxwell's equations","DMS","COMPUTATIONAL MATHEMATICS","07/15/2008","05/06/2010","Guido Kanschat","TX","Texas A&M Research Foundation","Continuing Grant","Leland Jameson","06/30/2012","$184,228.00","","kanschat@tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1271","0000, 9263, OTHR","$0.00","The investigator and colleagues are formulating, analyzing, and implementing adaptive multilevel Discontinuous Galerkin methods for coupled interior/exterior domain problems associated with the time-harmonic Maxwell equations. These advanced finite element methods are being realized as multilevel techniques on the basis of an adaptively generated hierarchy of triangulations of the computational domain. The research team is focusing on three central issues related to the basic steps `SOLVE', `ESTIMATE', `MARK', and `REFINE' of the adaptive loop. First, the smoothing process within the multilevel solver is performed only on the newly refined part of the triangulation obtained by a residual type a posteriori error estimator. Second, the a posteriori error analysis, which additionally has to take into account the effect of such local smoothing, aims to provide conditions guaranteeing a reduction of the global discretization error at each refinement step. <br/>Third, the selection of elements, faces and edges of the triangulation for refinement are based on a bulk criterion with an automatic (`tuning free') choice of the parameters controlling the amount of refinement in order to achieve optimal performance of the overall algorithm. Finally, the team is developing criteria to choose the parameters of artificial radiation boundary conditions automatically, such that no tuning on behalf of the user is required there as well. <br/><br/>Simulation of electromagnetic phenomena <br/>is a particularly challenging problem in computational mathematics. <br/>The investigator and colleagues are <br/>establishing a profound theoretical foundation for adaptive multilevel discontinuous Galerkin methods in electromagnetic field computations. They are developing a reliable algorithmic tool, of optimal computational complexity, that can be used for the numerical solution of challenging real-life problems in electrical engineering applications. The methods developed in this project have numerous technical and scientific applications, for instance semiconductor simulation or particle accelerator design. The results will be disseminated through publication of algorithms and results and reference computer codes being developed during this project will be made available to practitioners."
"0811172","Solving Polynomial Systems by the Polyhedral Homotopy","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/30/2008","Tien-Yien Li","MI","Michigan State University","Standard Grant","Leland Jameson","06/30/2012","$235,900.00","","li@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","0000, 9263, OTHR","$0.00","Polynomial systems arise very frequently in various fields of science and engineering, such as formula construction, geometric intersection, inverse kinematics, robotics, vision, and the computation of equilibrium states of chemical reaction equations.  Algorithms for solving such systems numerically are therefore highly in demand.<br/><br/>Over the years, practical evidence has been given that homotopy continuation methods are efficient, reliable, and powerful in solving polynomial systems numerically.  Recently, a major computational breakthrough called the polyhedral homotopy method emerged, making the method considerably more efficient and powerful.  Based on this new method, the software package HOM4PS-2.0 developed by the PI has produced marvelous performance on solving a large collection of polynomial systems and has led all the other existing codes in efficiency and storage requirement by a huge margin.<br/><br/>The essence of the proposed project is the further development in all aspects of the solver HOM4PS-2.0 as well as its parallelized version HOM4PS-2.0para, based on the conduction of further research to greatly enlarge the scope of the applications, especially applications to very large systems.  The ultimate goal of the project is more-complete high-quality black-box software which will provide the general scientific community a reliable source for solving polynomial systems on a wide variety of advanced architectures."
"0811180","Iterative upscaling of fluid flows in nonlinear deformable porous media","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","09/11/2012","Yalchin Efendiev","TX","Texas A&M Research Foundation","Standard Grant","Junping Wang","06/30/2012","$154,505.00","Dimitris Lagoudas, Peter Popov, Yuliya Gorb","efendiev@math.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1271","0000, 9263, OTHR","$0.00","The objective of this project is to develop numerical upscaling models for fluid flow through shape changing, inelastic, porous media, capable of shape recovery under pressure and temperature variations. Currently, the well-established models for poro-elastic media can only be applied to linear, elastic, porous solids. Moreover, macroscopic parameters such as average fluid pressure, and solid displacements are subject to various limitations. A key scientific contribution of the proposed research is modeling of the nonlinear coupling at the microscale between the fluid flow and solid deformation, due to both inelastic behavior of the solid and large pore-level displacements. <br/>The project will focus on fluid flow in various types of 3D pore geometries and different macroscopic parameters such as temperature, pressure and displacements. The homogenization method will be used to identify macroscopic equations and upscaled parameters which describe the effective media. Due to the complexity of the coupled fluid-structure interaction problem at the fine scale and the complex nonlinear response shape memory solids we will not attempt do derive closed form macroscopic equations. Instead, an efficient, easily parallelizable, Hybrid Multiscale Finite Element Model (HMFEM) which bypasses the explicit homogenization step by building fine-scale information directly into a coarse-scale computational grid will be developed. This numerical upscaling method will be applied to the analysis of a variable permeability filter with an SMA (Shape Memory Alloy) matrix, as a demonstration of the proposed methodology. Experimental verification of the numerical simulations will also be carried out.<br/><br/>A porous SMA (Shape Memory Alloy) matrix makes possible devices with changing, temperature and/or stress dependent, porosity without the need for moving parts and active control mechanisms. The project will expand our understanding of tightly coupled multiphyics phenomena in such media. Design of novel temperature and pressure-controlled flow regulators with applications to filters, catalytic converters, separators and microfluidic sensors can only become possible with accurate mathematical modeling and numerical simulations of fluid flow in such deformable porous media. The project will also provide a sound theoretical understanding of upscaling strongly coupled fluid-structure interaction problems, extending current methods for engineering analysis and design of complex devices. While we focus on SMAs, SMAs encompass standard plastic materials and are representative of a broader class of shape changing materials such as Magnetic SMAs, Shape Memory Polymers and Ferroelectric materials. As a result, this work will be directly applicable to a more general class of inelastic, temperature-dependent materials.<br/>"
"0810869","Computational Challenges in Fluid Transport and Imaging","DMS","COMPUTATIONAL MATHEMATICS","08/15/2008","08/07/2008","Guergana Petrova","TX","Texas A&M Research Foundation","Standard Grant","Junping Wang","07/31/2012","$156,500.00","","gpetrova@math.tamu.edu","400 HARVEY MITCHELL PKWY S STE 3","COLLEGE STATION","TX","778454375","9798626777","MPS","1271","0000, 9263, OTHR","$0.00","The proposed research emphasizes that dramatic advances in computation must be driven by a fundamental understanding of the theory behind numerical methods.  One of the main innovations in this proposal is the development of central schemes. While central schemes are considered to be among the most powerful numerical methods for time dependent PDEs, in particular for solving transport equations, there are still several fundamental issues to be resolved if they are to reach their full potential. This proposal also concentrates on the investigation of the power of anisotropy of level set methods, the implementation of implicit methods, and the power of multi-scale techniques in data assimilation.<br/><br/>Most real world problems are solved by computers. Many such problems are so complicated that the existing computational methods are insufficient to provide the accuracy needed.  This situation cannot be solved by simply building faster computers.  Indeed, the quest for finer resolution and more accurate models far outstrips gains in computational power.  Moreover, as has been demonstrated often in the past, innovations in computer algorithms and software pay larger dividends than advances in computational power.  The research in this project seeks to make innovative advances in the mathematics behind computer programs (so-called numerical algorithms) which will have the effect of significantly speeding up computation and thereby solving many scientific and engineering problems facing this country such as tracking pollutants in coastal areas or developing fast and accurate sensors for medical imaging. <br/>"
"0811175","Systematic Lagrangian Methods for Transport Problems","DMS","COMPUTATIONAL MATHEMATICS","08/01/2008","08/01/2008","Andrew Christlieb","MI","Michigan State University","Standard Grant","Junping Wang","07/31/2011","$167,000.00","","christli@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","0000, OTHR","$0.00","This work focuses on the development of higher-order Lagrangian particle methods for problems which have long-range self forces exhibiting algebraic decay. The advantage of a Lagrangian particle framework is that regions that require characterization (i.e., mesh points) are precisely where there are test particles.   Most particle methods are first order at best; in particular, the ad hoc fashion in which they are obtained makes it extremely challenging to systematically derive higher-order boundary conditions.  An important sub-class of problems with long range forces are those arising in plasma physics. The fundamental governing set of equations is the Boltzmann-Maxwell (BM) system, which may be reduced to the Vlasov-Poisson (VP) system under key assumptions. These problems are 6D plus time and exhibit a range of complex dynamics. Typically, Particle-In-Cell (PIC) is used to address computational problems governed by these systems. The proposer has been engaged in addressing issues with PIC (and other schemes) by developing a systematic particle formulation. The initial framework is based on the evolution of the Lagrangian flow map, where long range forces are evaluated using fast summation algorithms. The proposed work seeks to develop high-order Lagrangian methods with four or higher dimensions.  Critical issues to be addressed include:  I) a rigorous analysis of numerical heating in mesh-based particle methods, II) increasing the spatial accuracy of Lagrangian particle methods, III) an analysis of the use of regularization and its impact on the accuracy of boundary integral methods, and IV) accelerating explicit/implicit Spectral Deferred Correction (SDC) using high-order correction.<br/><br/>The project will create new simulation tools that are more accurate, more reliable, and of broader applicability than existing methods for a range of problems of interest in applied physics, chemistry and materials science.   Higher accuracy and better predictive capability have a real impact in the applied sciences on reducing the huge expense of experimental design procedure by providing a refined design prior to construction. Collaboration with the Air Force Research Laboratory is under way, aimed at real world problems such as modeling spacecraft plume interactions. <br/>"
"0813750","Analog-to-Digital Conversion:  Mathematics and Algorithms","DMS","COMPUTATIONAL MATHEMATICS","09/15/2008","09/14/2009","Yang Wang","MI","Michigan State University","Standard Grant","Leland Jameson","08/31/2011","$190,000.00","","ywang@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","0000, 9263, OTHR","$0.00","The advent of computer and digital information technologies and there development have greatly changed the world we live in. Today digital technologies are everywhere in our lives. A key step that makes all those technologies possible is to convert analog data into digital ones, a process known analog-to-digital conversion, or A/D conversion. With demand for higher precision and more cutting-edge technologies, the mathematics of A/D conversion algorithms plays a key role in this quest. The research proposed in this proposal focuses mainly on the mathematics and algorithms of A/D conversion. Given that analog devices are to different degrees imprecise by nature, robustness is extremely important if high precision is desired. The proposal will focus on robust A/D conversion algorithms.<br/><br/> <br/><br/>In this proposal the PI will address two of the main sources of imprecisions: imprecise quantizations and imprecise multiplications. The proposed encoders are the first encoders to be completely robust against quantizer and multiplier imprecisions. The PI proposes to study both the algorithms and the mathematical questions that arise from the study. The PI also studies ways the encoders can be extended and refined, along with many related and often challenging mathematical problems. The core of this project is a novel class of analog-to-digital conversion algorithms. These algorithms have the advantage that they are robust against quantization errors (imprecisions) and multiplication imprecisions. The novelty of these A/D algorithms comes from the use of Golden Ratio based expansions of real numbers related to the Fibonacci numbers. More general algorithms involve a class of algebraic integers and matrix encoders. The PI and his collaborators have already built a circuit of an A/D converter based on one of our algorithms, which has yielded outstanding result. A main objective of this proposal is to study these algorithms in greater depth, particularly concerning the stability and robustness of these algorithms. Equally important is that the study of these A/D algorithms has raised a number of interesting and challenging mathematical questions. Graduate students will be involved in the project. In addition, the proposed research lends naturally to collaborations between mathematicians and the engineering community."
"0811197","Fast and Accurate Integral Equation Solvers for Mixed-scale Electromagnetic Simulation","DMS","COMPUTATIONAL MATHEMATICS, THEORETICAL FOUNDATIONS (TF)","07/15/2008","07/09/2008","Shanker Balasubramaniam","MI","Michigan State University","Standard Grant","Junping Wang","06/30/2012","$231,022.00","","shanker.32@osu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271, 7351","0000, 9263, OTHR","$0.00","In the electromagnetic simulation of realistic structures, the spatial <br/>representation of the domain being analyzed depends not only on the <br/>frequency of interest but also on the need to capture possible fine <br/>geometric features. Such mixed scales cause havoc in standard integral <br/>equation based solvers on three fronts; (i) discretized integral <br/>equations become poorly conditioned as the size of the element becomes <br/>smaller, (ii) the function spaces used do not optimally represent the <br/>underlying physics, and (iii) the overall computational burden is <br/>exceedingly large. This largely limits the applicability of the <br/>existing methods. The proposed project seeks to develop a demonstrably <br/>unified, robust and accurate solution methodology that is well conditioned <br/>over a wide range of frequencies and, at the same time, has the flexibility <br/>to handle complicated (and possibly near singular) geometries. This is <br/>achieved by (i) developing a well conditioned integral equation scheme <br/>(that are Fredholm equations of the second kind) with provable bounds on <br/>convergence rates and accuracy to solve for electromagnetic quantities <br/>over a large range of spatial frequencies; (ii) enlarging the approximation <br/>space used for representing the unknown quantity so as to include the local <br/>physics; (iii) designing a scheme that permits seamless interplay between <br/>a variety of basis functions to model the unknown quantities to be used <br/>with the above integral equation scheme; (iv) deriving error bounds and <br/>convergence estimates on these schemes to demonstrate clear and easy user<br/>control over the error, and (iv) developing a domain decomposition framework <br/>so that these schemes can be integrated seamlessly with classical integral <br/>equation and finite element methods to solve electrically large problems. <br/>The educational objective is to develop a publicly available set of <br/>tutorials/teaching modules based on this research. <br/><br/>The rapid progress in simulation methods in concert with the Moore's law has made the analysis of electrically large problems possible on simple desktop machines in reasonable computational times. So much so that fullwave or rigorous simulation of realistic devices are within the realm of possibility. However, as one tends towards this goal, new and more challenging problems arise. In modeling mixed scale physics, it is necessary to correctly represent local physics, develop methods to overcome conditioning issues, and develop means to accelerate computation over multiple scales. This project addresses the resolution of these problems. The methods developed herein will have a wide footprint ranging from national security (design of conformal antennas) to sensor technology (surface enhanced raman and plasmonics) to metamaterials to nanotechnology (nano-structure crystal growth dynamics) to molecular dynamics.  In addition to training graduate students in engineering and mathematics, existing channels are utilized to recruit women and minorities and undergraduate students are involved through senior design projects and potential REU<br/>supplements. <br/>"
"0811096","Computational Methods in Applied Nonlinear Dynamical Systems","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","08/24/2011","Evelyn Sander","VA","George Mason University","Standard Grant","Leland Jameson","06/30/2012","$89,402.00","","esander@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","MPS","1271","0000, 9263, OTHR","$0.00","Investigations are proposed at the overlap of computational mathematics and applied dynamical systems, including improving our understanding of the role of chaotic dynamics in computer simulations, and new computational methods for inferring information about dynamical structure from time series data.  The work on simulation is an ongoing study of large statistical errors occurring in deterministic modeling, including computational models of complex systems. The second major area is work by the investigator on fundamental questions in the interpretation of experimental multivariate time series, including data from physical and biological experiments.  Particular attention will be paid to complex deterministic time series from physical, chemical, engineering and biological/medical settings that are produced by network dynamics. We propose new methods to solve discrimination problems, infer network structure and dynamics, and to track and reduce noise from trajectories using data assimilation methods in cases where the structure is known.<br/><br/> The project focuses on the development of new approaches to study computer simulation validity, and to the interpretation of experimental data. Computer simulation is a critical ingredient of modern science. Simulations that depend on the solution of differential equations are subject to small modeling and truncation errors. The proposal builds a foundation for analyzing the possible effect of the errors on long-term simulation results, for physically relevant models. The second major focus of this project is the interpretation of data collected from experimental systems and nature when only a partial mathematical model is available. We will investigate a range of techniques which, depending on the completeness of the available model, can be used to attempt to reconstruct the model and dynamical behavior of the process, with potential to predict or control the process. Both parts of the project have implications in many areas of sciences and engineering, and special relevance to biological systems.<br/>"
"0811275","Collaborative Research:  Enhanced Least-Squares Methods for PIV Analysis","DMS","COMPUTATIONAL MATHEMATICS, OPPORTUNITIES FOR RESEARCH CMG, MATHEMATICAL GEOSCIENCES","09/15/2008","07/12/2012","Steve McCormick","CO","University of Colorado at Boulder","Continuing Grant","Junping Wang","12/31/2011","$156,122.00","Thomas Manteuffel","stevem@colorado.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1271, 7215, 7232","0000, 9263, OTHR","$0.00","Particle Image Velocimetry (PIV) is a method for obtaining a fluid velocity field based on the translation of particles between images with a known time span between them.  A potential limitation of PIV is that only two-dimensional velocity field along a single plane can be obtained from a two-dimensional image.  This limitation is normally overcome by designing the experimental flow system so that the third velocity component is either zero or unimportant.  In many applications, however, it is not possible to simplify the fully three-dimensional velocity field (e.g., in the left ventricle of the heart), and the two-dimensional limitations associated with PIV analysis are a significant problem that must be overcome.  Would it be possible, however, to combine the two-dimensional PIV data together with a fully three-dimensional numerical approximation to the Navier-Stokes equations and obtain a sufficiently accurate three-dimensional velocity field in a domain such as the left ventricle of the heart?  The use of least-squares finite element methods (LSFEMs) is proposed here to approximately solve the Navier-Stokes equations, and, significantly, to weakly constrain the solution along the PIV plane to match the experimental data.  The PIV data would basically act as an internal boundary, and the numerical solution would weakly match the data with a variable weighting that determines the strength of the coupling between the data and numerical solution.  LSFEMs are uniquely well suited for solving an over-constrained problem like this in a computationally efficient manner.<br/><br/>Echocardiologists have developed methods for introducing microbubbles into circulating blood that can be resolved using ultrasound.  The location of the microbubbles combined with the high temporal resolution of ultrasound allows the local blood velocity to be determined using Particle Image Velocimetry, but the high temporal resolution requirement also limits the ultrasound scans (and, hence, the velocity field data) to two dimensions.  One goal of using the FDA approved microbubbles is to use the blood velocity data to calculate physiologically important information such as pressure gradients and energy loss for the blood flow, but these calculations require a full three-dimensional velocity field.  The goal of the proposed research is to develop mathematical techniques that combine computational fluid dynamics with experimental velocity data, such as that from microbubbles, to obtain a full, three-dimensional velocity field, thus providing greater insight into the dynamics of the flow.<br/>"
"0748488","CAREER: Fast Direct Solvers for Differential and Integral Equations","DMS","COMPUTATIONAL MATHEMATICS","09/01/2008","07/31/2012","Per-Gunnar Martinsson","CO","University of Colorado at Boulder","Continuing Grant","Junping Wang","08/31/2014","$400,000.00","","pgm@ices.utexas.edu","3100 MARINE ST","BOULDER","CO","803090001","3034926221","MPS","1271","0000, 1045, 1187, OTHR","$0.00","Over the last several decades, the development of powerful computers and fast algorithms has dramatically increased our capability to computationally model a broad range of phenomena in science and engineering. Our newfound ability to design complex systems (cars, new materials, city infrastructures, etc.) via computer simulations rather than physical experiments has in many fields led to both cost savings and profound improvements in performance. Intense efforts are currently being made to extend these advances to biochemistry, physiology, and several other areas in the biological and medical sciences.<br/>     The goal of the proposed research is to develop faster and more accurate algorithms for computing approximate solutions to a class of mathematical equations called ""partial differential equations"" (PDEs) that lie at the core of models of physical phenomena such as heat transport, deformation of elastic bodies, scattering of electro-magnetic waves, and many others. The task of solving such equations is frequently the most time-consuming part of computational simulations, and is the part that determines which problems can be modeled computationally, and which cannot.<br/>     Technically speaking, most existing numerical algorithms for solving large PDE problems use ""iterative methods,"" which construct a sequence of approximate solutions that gradually approach the exact solution. The proposed research seeks to develop ""direct methods"" for solving many PDEs. Loosely speaking, a direct method computes the unknown data from the given data in one shot. Direct methods are generally preferred to iterative ones since they are more robust, are more suitable for incorporation in general-purpose software, and work for important problems that cannot be solved with known iterative methods. The reason that they are typically not used today is that they are often prohibitively expensive. However, recent results by the PI and other researchers indicate that it is possible to construct direct methods that are competitive in terms of speed with the very fastest known iterative solvers. In fact, in several important applications, the direct methods appear to be one or two orders of magnitude faster than existing iterative methods.<br/>     In addition to constructing faster algorithms, a core goal of the proposed research is to demonstrate the capabilities of the new methods by applying them to a number of technologically important problems that are not amenable to existing techniques. These problems include scattering problems at frequencies close to a resonance frequency of the scatterer, modeling of crack propagation in composite materials, and the modeling of large bio-molecules in ionic solutions.<br/>     An integral part of the proposed work is the development of new educational material on computational techniques. Specific goals include: (1) The development of a textbook on so called ""Fast Multipole Methods."" (2) The development of a new graduate-level class on fast algorithms for solving PDEs. (3) The updating of the standard curriculum of undergraduate classes in numerical analysis to reflect new modes of thinking about numerical algorithms (specifically the development of methods that are not ""convergent"" in the classical sense, but that can solve specified tasks to any preset accuracy). This work is to be undertaken in close collaboration with Leslie Greengard at NYU and Vladimir Rokhlin at Yale University."
"0810862","Approximation of Matrix Functions: Theory, Algorithms, and Software","DMS","COMPUTATIONAL MATHEMATICS","08/01/2008","06/30/2008","Michele Benzi","GA","Emory University","Standard Grant","Leland Jameson","07/31/2011","$229,481.00","","mbenzi@emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1271","0000, 9263, OTHR","$0.00","This project has two main goals.  One goal is to provide rigorous mathematical foundations for a wide array of techniques developed by physicists, chemists, and other scientists over the last 10-15 years to perform computer simulations in fields ranging from theoretical chemistry and molecular physics to data mining and statistics.  A common theme in these areas is the need to quickly compute approximations of functions of large and sparse matrices (such as the exponential, the square root, the logarithm, and combinations thereof).  Scientists have had some success using a combination of physical intuition and heuristics, but rigorous justifications and analysis are still lacking and are sorely needed.  On the other hand, computational mathematicians have until now devoted scant attention to these types of problems.  The present project addresses this need.  The main conceptual tool is a theory of localization, in the form of decay bounds, that the PI has been developing in recent years.<br/><br/>Another goal is to construct better (i.e., faster and more accurate) algorithms to compute the desired approximations.  The PI will devote considerable effort to this objective, in particular using various types of polynomial approximations (interpolation, Chebyshev, Faber, etc).  The resulting software will be distributed to interested parties.<br/><br/>The ultimate goal of research in computational mathematics is to provide scientists and engineers the algorithmic and software tools needed for the solution of challenging scientific and technical problems of increasing size and complexity.  The competitiveness of American science and technology greatly benefits from (and to a large extent depends on) the creation of innovative computational methods and software and from the continuous improvement of existing techniques.  Progress in the solution of the problems targeted by the PI will have a positive impact on science and engineering by enabling faster and more detailed computer simulations.  In addition, several graduate students (and possibly a few undergraduates) will be impacted by this research either through direct involvement, or through the positive effects this research will have on the PI's teaching of computational and applied mathematics courses at Emory University."
"0811031","Structured Nonlinear Least Squares Problems in Biomedical and Biomolecular Imaging","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/20/2008","James Nagy","GA","Emory University","Standard Grant","Leland Jameson","06/30/2012","$297,373.00","","jnagy@emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1271","0000, 9263, OTHR","$0.00","This project has significant mathematical and computational challenges, and at the same time is focused on specific applications in biomedical (tomosynthesis) and biomolecular (microscopy) imaging.<br/>The mathematical models addressed in this project are difficult ill-posed inverse problems. Computed solutions of these problems are very sensitive to errors in the data, and implementation for large scale 3-dimensional images is nontrivial.  New image post processing algorithms developed in this project will be based on computing solutions of large scale structured nonlinear least squares problems.<br/>Efficiency will be obtained by exploiting algorithmic structure of the nonlinear least squares problem, as well as structure that arises in the applications.  Collaborations with researchers in the School of Medicine and Winship Cancer Research Center at Emory University will be used to test and verify developed methods, and will facilitate efforts to transition new software to clinical use.<br/><br/>Improved image reconstruction and post processing algorithms for tomosynthesis can have a profound impact on breast cancer screening. In addition to providing better screening capabilities than mammography, tomosynthesis requires less compression of the breast (reducing physical pain to the patient), and it requires a smaller radiation dose than computed tomography (CT).  In addition to its application to breast cancer screening, tomosynthesis can be used for many other medical imaging applications where standard x-ray and CT are used. Thus, advances in computational methods for this application can have a very broad impact in the medical field.  In the case of biomolecular imaging, improved computational approaches can help provide inexpensive, yet accurate, point-of-care diagnostic imaging systems. This can significantly impact the monitoring of infections that have serious consequences to society; for example, management of HIV infected patients in poor regions of the world. Moreover, the application considered in this project (deconvolution microscopy), can be used to examine many other microscopic quantities, and thus development of new algorithms that provide better and faster reconstructions, can have a broad impact in many scientific fields, including biology, chemistry, neuroscience, and physics.<br/>"
"0753009","Linear Algebra and Applications Graduate Summer School","DMS","ALGEBRA,NUMBER THEORY,AND COM, APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","06/01/2008","12/10/2007","Leslie Hogben","IA","Iowa State University","Standard Grant","Bruce E. Sagan","05/31/2009","$25,223.00","Yiu Poon, Wolfgang Kliemann","hogben@aimath.org","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1264, 1266, 1271","0000, 7556, 9263, OTHR","$0.00","ABSTRACT<br/><br/><br/>Principal Investigator: Hogben, Leslie    <br/>Proposal Number: DMS - 0753009 <br/>Institution: Iowa State University <br/>Title: Linear Algebra and Applications Graduate Summer School: Additional Funding<br/><br/><br/><br/>Linear algebra is a subject of central importance in both mathematics and a variety of other disciplines. It is used by virtually all mathematicians and by statisticians, physicists, biologists, computer scientists, engineers, and social scientists. Just as the basic idea of first semester differential calculus (approximating the graph of a function by its tangent line) provides information about the function, the process of linearization often allows difficult problems to be approximated by more manageable linear ones. This can provide insight into, and, thanks to ever-more-powerful computers, approximate solutions of the original problem.  The fundamental nature of linear algebra and its many applications make this an ideal theme for a summer program for graduate students.<br/><br/>The 2008 Institute for Mathematics and its Applications (IMA) Participating Institutions  Summer Graduate Program on ``Linear Algebra and Applications'' will be held Iowa State University June 29 - July 26, 2008.   The IMA will pay all costs for the principal speakers  and all expenses for up to two students from each of the 40 IMA participating institutions to this Summer Graduate Program.  This project will provide support for an 12 additional graduate students from non-IMA US institutions, and salary for a post-doctoral associate who will serve as a mentor to the graduate students.   Applications will be taken through the Linear Algebra and Applications Graduate Summer School website http://www.ima.umn.edu/2007-2008/PISG6.30-7.25.08/<br/><br/>The program organizers are Leslie Hogben, Wolfgang Kliemann and <br/>Yiu-Tung Poon.   The the principal speakers and their topics are:<br/><br/>Bryan Shader, University of Wyoming, Linear algebra and applications to combinatorics.<br/><br/>David S. Watkins, Washington State University, Numerical Linear Algebra.<br/><br/>Chi-Kwong Li, College of William and Mary, Matrix inequalities and equations in science and engineering.<br/><br/>Fritz Colonius, University of Augsburg, Germany, Applications of linear algebra to dynamical systems.<br/><br/>Lectures will be presented in the mornings and students will work in groups in the afternoons, supervised by post-doctoral associate Jason Grout.  Each week the students will learn the necessary background to attack current research problems presented at the end of the week.<br/><br/>The Linear Algebra and Applications Graduate Summer School will help to educate future US mathematical scientists.  This NSF funding will broaden participation in the summer school and will hence magnify its impact and allow a more diversified group of students to participate, since the organizers will now advertise the summer school broadly also to underrepresented groups of students outside of the IMA participating institutions.<br/><br/>"
"0739261","EMSW21-RTG: Complex Biological Systems Across Multiple Space and Time Scales","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY, WORKFORCE IN THE MATHEMAT SCI","06/01/2008","06/13/2011","Beatrice Riviere","PA","University of Pittsburgh","Continuing Grant","Mary Ann Horn","05/31/2013","$1,863,866.00","Ivan Yotov, David Swigon, Bard Ermentrout, Jonathan Rubin","Beatrice.Riviere@rice.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1266, 1271, 7334, 7335","0000, 7301, OTHR","$0.00","This Research Training Group (RTG) will focus on training a new generation of scientists in (i) the development of analytical and computational algorithms for solving complex spatio-temporal problems that arise in biology and (ii) applications of these and other methods to problems arising in neuroscience and inflammation. In particular, on the computational side, this project will develop effective discretizations of coupled biological processes, iterative solvers and coupling/decoupling strategies.  Within the neuroscience application, the investigators will derive new results on propagating waves and sustained activity patterns, synchrony and rhythmicity, noise and synaptic plasticity.  Within the inflammation application, the research projects deal with the immune response to influenza infection, inflammation and sepsis, and models of necrotizing enterocolitis and wound healing.<br/><br/>This project builds on the research and training experience of the principal investigators who have ongoing successful collaborations with the departments of Neuroscience and of Computational Biology, the Center for Inflammation and Regeneration Modeling and the Center for the Neural Basis of Cognition. The trainees are postdoctoral fellows and graduate and undergraduate students. They will participate in a variety of integrated research and training activities, including regular trainee presentations, journal clubs, working groups and research seminars, semiannual RTG Theme Days, an annual retreat, and an online forum. The biomedical applications targeted in this project are of enormous importance to society, as symbolized by the inclusion of inflammation as an NIH roadmap emphasis area for 2008 and the NIH Blueprint for Neuroscience Research."
"0811314","Analysis and Applications of the Discontinuous Galerkin Method","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/25/2008","Ohannes Karakashian","TN","University of Tennessee Knoxville","Standard Grant","Junping Wang","06/30/2012","$169,629.00","","okarakas@utk.edu","201 ANDY HOLT TOWER","KNOXVILLE","TN","379960001","8659743466","MPS","1271","0000, 9263, OTHR","$0.00","In its broad outlines, the research program of the P.I. aims at the<br/>development, analysis and computer implementation of numerical methods designed<br/>to approximate the solutions of some partial differential equations (pde's)<br/>that have important applications in the fields of engineering and physics. The<br/>Discontinuous Galerkin method constitutes the core methodology of this<br/>effort. The ultimate goal of the research is to make full use of this method<br/>to develop convergent and efficient adaptive methods designed to reduce the<br/>run time of the algorithms by finding optimal or quasi-optimal meshes. Other<br/>efforts will be directed towards the development of domain decomposition and<br/>multigrid algorithm for the fast solution of the resulting systems of equations<br/>on single as well as multiprocessor computers.<br/>The P.I. will develop methods and adaptive algorithms for second and fourth<br/>order elliptic problems, the incompressible Navier-Stokes equations and the<br/>Cahn-Hilliard equations and use  them to simulate phenomena<br/>modeled by these equations. Adaptive methods will also be used to simulate<br/>finite-time blowup of nonlinear evolution equations. Some areas of<br/>applications are chemical reactions where the geometry of the domain plays an<br/>important role and the simulation of tumor growth.<br/><br/>Scientific computing is playing an increasingly important role in the progress<br/>of Science as a cost effective alternative to ""real life"" experiments which<br/>could be very costly, say wind tunnel experiments in aircraft design, or even<br/>impossible to duplicate, such as supernova explosions and other astrophysical<br/>phenomena. It is worth mention that numerical simulations are playing a crucial<br/>role in the identification of the potential effects of global warming. The U.S.<br/>government, through its various funding agencies, has made important investments<br/>by the creation of supercomputing centers equipped with the latest<br/>generation of massively parallel computers. To extract the full power of these<br/>machines, with some having tens of thousands of individual processors, efficient<br/>and ""scalable"" methods and algorithms must be developed to keep pace with the<br/>advances in hardware. Indeed, most current algorithms fall short of harnessing<br/>the full power of these computers especially when the number of processors<br/>exceeds a few thousand. The Discontinuous Galerkin method is a recently<br/>introduced methodology with great potential and wide applicability. Its many<br/>attributes include flexibility, ability to handle complex geometries and<br/>scalability. Further understanding of this approach and the development of<br/>efficient and parallel computer codes will have a positive impact on the<br/>ever increasing areas of Science that make essential use of numerical<br/>simulations. Finally, two graduate students are actively participating in this<br/>project as partial fulfillment of their Ph.D. degree requirements. This will<br/>achieve another goal of this project which is to contribute to the training of<br/>the next generation of researchers."
"0813901","Multiscale numerical modeling of multiphysics systems","DMS","COMPUTATIONAL MATHEMATICS, OPPORTUNITIES FOR RESEARCH CMG, MATHEMATICAL GEOSCIENCES","09/01/2008","08/22/2008","Ivan Yotov","PA","University of Pittsburgh","Standard Grant","Junping Wang","08/31/2011","$264,546.00","","yotov@math.pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1271, 7215, 7232","0000, 9263, OTHR","$0.00","This project focuses on multiscale and stochastic numerical methods<br/>for complex multiphysics problems. A parallel computational framework<br/>for modeling various multiscale applications will be developed.  The<br/>models of interest will be based on coupling the Stokes or the<br/>Navier-Stokes equations with single phase or multiphase Darcy flow.<br/>The flow system will be coupled with reactive transport.  The research<br/>approach is based on a multiblock domain decomposition methodology.<br/>The simulation domain is a union of subdomains (blocks).  Each block<br/>is associated with a physical, mathematical, and numerical<br/>model. Physically meaningful interface conditions are imposed on the<br/>discrete level via mortar finite elements. The formulation provides<br/>great flexibility for multiphysics and multinumerics<br/>couplings. Furthermore, this domain decomposition approach, combined<br/>with coarse scale mortar elements, provides a multiscale approximation<br/>and an efficient way to solve the coarse grid problem in parallel.<br/>Uncertainty in the physical characteristics is modeled via stochastic<br/>partial differential equations. Approximation in probability space is<br/>achieved using stochastic finite element and collocation methods.  The<br/>main components of the research will be 1) development of multiscale<br/>mathematical models, identifying appropriate interface conditions, and<br/>analysis of the well-posedness of the models; 2) novel discretization<br/>techniques for stochastic partial differential equations; 3) a<br/>posteriori error estimates and adaptive mesh refinement algorithms in<br/>physical and stochastic space; and (4) efficient parallel domain<br/>decomposition solvers and preconditioners.<br/><br/>The research methodology can be applied for the design, analysis, and<br/>implementation of computational methods for a large class of problems<br/>arising in science and engineering applications. The project will<br/>emphasize accurate, robust, and efficient numerical methods for<br/>problems exhibiting behavior on a wide range of scales in space and<br/>time. The research also aims to quantify uncertainty in the prediction<br/>due to to uncertainty in the input parameters through stochastic<br/>modeling.  The research results will be applied to two main<br/>application areas: couplings of surface water with groundwater flows<br/>and modeling complex multiscale processes occurring in the<br/>inflammatory response in the human body. Computer modeling of<br/>subsurface and surface flow and transport has a major economic impact<br/>on environmental and energy industries. Due to the proximity of<br/>groundwater and surface water systems, contamination of rivers, lakes,<br/>and wetlands can lead to contamination of aquifers and<br/>vice-versa. Simulating these complex interactions is a major<br/>computational challenge. Acute inflammation, which occurs in response<br/>to trauma or infection, may lead to a systemic inflammation or<br/>sepsis. Sepsis frequently leads to organ failure and death (with more<br/>that 500,000 fatal cases per year in the U.S.). Effective treatments<br/>are lacking, due to the complexity of the process. Mathematical and<br/>computer modeling is becoming a more common tool in the effort to gain<br/>insight of the dynamics and spatial characteristics of the<br/>inflammatory process.<br/>"
"0757371","FRG: Collaborative Research: Semidefinite optimization and convex algebraic geometry","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS","09/01/2008","08/06/2008","Rekha Thomas","WA","University of Washington","Standard Grant","Junping Wang","08/31/2012","$239,998.00","","thomas@math.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1264, 1271","0000, 1616, 9263, OTHR","$0.00","The goal in this proposal is to develop the mathematical foundations and<br/>associated computational methods for the study of convex sets in real<br/>algebraic geometry. This work requires a combination of ideas and<br/>mathematical tools from optimization, analysis, algebra and combinatorics.<br/>The proposed program will lead not only to theoretical insights, but also to<br/>new algorithms and software that will enable novel applications in<br/>mathematics, engineering, and beyond. The work is organized in five main<br/>thrusts: semidefinite programming and sums of squares, convex semi-algebraic<br/>sets, sparsity and graphical structure, numerical polynomial optimization<br/>and applications, and deformations and variation of parameters. The PIs will<br/>focus on the development of a comprehensive theory and practical new<br/>algorithms for convex sets defined by polynomial inequalities. Specific<br/>problems and techniques include the formulation of semidefinite descriptions<br/>of convex hulls of real algebraic varieties, determinantal representations<br/>of hyperbolic polynomials, sparse polynomials and their symmetries, tropical<br/>geometry and homotopy techniques, and geometric programming.<br/><br/>Many areas in mathematics, as well as applications in engineering, finance<br/>and the sciences, require a thorough understanding of convex sets. This is a<br/>class of geometric shapes, with several different but complementary<br/>interpretations. The goal in this project is to achieve a better<br/>understanding of how these geometric properties emerge from their algebraic<br/>descriptions in terms of polynomial equations, and the corresponding<br/>computational implications. One of the main motivations is the possibility<br/>of applying these results in the context of optimization. The proposed<br/>research will contribute to existing knowledge, both in algebraic-geometric<br/>techniques as well as in mathematical optimization. It will create synergies<br/>between different branches of applied mathematics, and their engineering and<br/>scientific applications (e.g., in computational biology and statistical<br/>modeling). Successful completion of this project should contribute to the<br/>availability of efficient and reliable computational tools for solving<br/>polynomial systems, which have clear technological and economic interest.<br/>Other key features of this proposal include its integration with curriculum<br/>development, undergraduate research projects, training of graduate students<br/>and postdocs, and the development of new software tools for computational<br/>optimization.<br/>"
"0811272","Theory and Algorithm of Adaptive Methods for Numerical Methods","DMS","COMPUTATIONAL MATHEMATICS","09/01/2008","08/25/2008","Long Chen","CA","University of California-Irvine","Standard Grant","Junping Wang","08/31/2011","$149,999.00","","chenlong@math.uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1271","0000, 9263, OTHR","$0.00","This proposal is on the study of advanced numerical methods for partial differential equations (PDEs) that arise from scientific and engineering applications. The theme of research is on the development, application and analysis of multilevel adaptive finite element methods. Comparing with the uniform refinement of the computational grid, adaptive finite element methods through mesh adaptation are more preferred to locally increase mesh densities in the regions of interest, thus saving the computer resources. The strategies of mesh adaptation can fall into two categories: h-method and r-method. The PI proposes to study several novel ideas in both methods and combine them to develop a more efficient, integrated, and flexible method for a large class of PDEs. More precisely, for r-method, the PI proposes a new energy using the concept of Optimal Delaunay Triangulation (ODT) and will develop related fast optimization methods and apply to the numerical solution of PDEs. For h-method, the PI will design and analyze multigrid methods, gradient recovery schemes, and refinement and coarsening algorithms based on a novel decomposition of bisection grids. Furthermore, these two methods will be naturally incorporated to result a more multilevel mesh adaptation strategy, in which h-method will be mainly used as a local smoother while the coarse mesh will be moved using the information from fine grids to severs as a coarse grid correction. The PI hopes to develop a more complete theoretical foundation and modern techniques for the combined use of adaptivity and multilevel solvers.<br/><br/><br/>The multilevel adaptive methods developed and studied in this work are expected to have a broader impact on the numerical solutions of a large class of practical problems. Special target applications for this work are the convection-dominated problems and numerical simulation of pattern formation. The convection-dominated convection diffusion problems are particularly important to several flow problems in the real applications, for example, automotive industry (flow in combustion engines), plating industry (electro-chemically reacting flows with mass transfer at the electrode boundaries), and aerospace (high Reynolds number flow) among many others. Pattern formation occurs in diverse physical, chemical, and biological systems, from Drosophila embryo to the large-scale structure of the universe. By developing improved multilevel numerical techniques to reduce the computer time required to solve the underlying equations, and at the same time producing more accurate solutions through the use of adaptive finite element methods, this project will provide powerful tools for the exploration of models in physics and biology.  In addition, a fully integrated involvement in undergraduate and graduate computational mathematics education is an integral part of the project. By developing a MATLAB package (iFEM), the PI will be able to design a new project-oriented course on multilevel adaptive finite element methods.<br/>"
"0811254","The Fast Sweeping Method and Its Applications","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/30/2008","Hong-Kai Zhao","CA","University of California-Irvine","Standard Grant","Junping Wang","06/30/2011","$153,261.00","","zhao@math.duke.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1271","0000, 9263, OTHR","$0.00","The recently developed fast sweeping method has been proved to be an efficient iterative method for a class of Hamilton-Jacobi equations. In this project, a continuing study and analysis of the fast sweeping method will be carried out.<br/>Applications of the method to other problems will be pursued.<br/>In particular the following work will be done:<br/>(1) Understand the connection to control/game theory.<br/>(2) Study the contraction property of the fast sweeping algorithm in the general framework of iterative method.<br/>(3) Error analysis for the numerical solution and its derivative.<br/>(4) Develop fast sweeping method for other type of hyperbolic partial differential equations, such as hyperbolic conservation laws and radiative transport equation.<br/><br/>Hamilton-Jacobi equations are nonlinear partial differential equations that have many applications in classical mechanics, optimal control, geophysics, geometric optics and image processing. The nonlinearity of this type of problem poses great challenges for mathematical analysis and for developing efficient numerical algorithms. The fast sweeping method is a simple iterative method. It can work efficiently for a class of difficult nonlinear problem, which is quite remarkable and worth further understanding and development.<br/>The broader impact of this project is not only to provide efficient numerical methods for real applications in science and engineering but also to shed insight for constructing iterative methods for other nonlinear problems.<br/>In addition integration with education at different levels will also be designed.<br/>"
"0812800","Numerical Spectral Analysis and Approximation of Functional Traveling Waves","DMS","COMPUTATIONAL MATHEMATICS","09/01/2008","08/28/2008","Erik Van Vleck","KS","University of Kansas Center for Research Inc","Standard Grant","Leland Jameson","08/31/2012","$150,808.00","","erikvv@ku.edu","2385 IRVING HILL RD","LAWRENCE","KS","660457552","7858643441","MPS","1271","0000, 9150, 9263, OTHR","$0.00","Differential equations are used as models of physical and biological phenomena in many areas of science and engineering.  The focus of the investigator and his colleagues in this proposal is on the approximation of solutions of differential equations. The investigator is interested in the analysis and computation of stability spectra (point spectrum of differential and difference operators, Sacker-Sell spectrum, and Lyapunov exponents) and techniques for analysis and approximation of discrete models similar to time dependent partial differential equations but with a difference operator instead of a spatial differential operator.  Discrete models play a prominent role in the modeling of physical and biological systems.  Of particular interest are traveling wave solutions  of lattice differential equations. <br/>The approach taken is to combine dynamical systems and numerical analysis ideas with the modeling and analysis of ordinary, partial, and lattice differential equations.  This project is concerned with the development and analysis of efficient, accurate numerical techniques that are useful for the computation and analysis of dynamical systems.  Sacker-Sell and Lyapunov spectral intervals are natural analogues of the real parts of the eigenvalues that provide stability information for time varying differential equations.  The investigator develops, analyzes, and justifies the use of numerical techniques for the approximation of these spectral intervals.  A suite of computational modules for the computation of stability information and for functional traveling waves is being developed. It is backed by analysis of the numerical techniques in a form that should prove useful to working scientists and engineers.<br/><br/>The investigator and his colleagues consider issues in the approximation and computation of solutions of differential equations. Differential equations are commonly used to model physical and biological phenomena in many areas of science and engineering.  A differential equation is a rule, a relationship between the solution and the rate of change of the solution, that determines how an initial configuration evolves into future configurations.  The focus of this project is on the approximation of Lyapunov exponents and related quantities that provide information on stability, the tendency for nearby configurations to evolve and stay nearby, and instability, the tendency for nearby configurations to move apart. This type of analysis is useful in understanding complex biological phenomena that occur in the environment and in identifying instabilities in, for example, models of weather prediction. The analysis and computation of lattice differential equations, i.e., differential equations that are discrete in space and continuous in time, are important in the modeling of physical and biological systems in which the spatial component is naturally discrete, in particular for microscopic models in materials and physiology.<br/>"
"0810113","Computational Methods for Heteroepitaxial Growth, Grain Boundary Motion, and High Frequency Wave Propagation","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","04/16/2010","Peter Smereka","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Junping Wang","06/30/2013","$256,429.00","","psmereka@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1271","0000, 9263, OTHR","$0.00","This proposal involves three projects. The first concerns modeling and<br/>efficient simulation of heteroepitaxial growth using kinetic Monte Carlo <br/>and will build from prior NSF support which resulted in the<br/>development of a Fourier multigrid method for the fast solution of<br/>discrete elastic equations for complex geometries.  This work will be <br/>extended to develop methods for obtaining inexpensive upper bonds on<br/>rates, the use of local computations for elastic equations, and the inclusion of<br/>intermixing of multiple species.  The second project involves the <br/>simulation of grain boundary motion in two and three dimensions using<br/>a recently developed multiphase variational level set framework which<br/>allows one to systematically deduce level set equations for a network of<br/>grains moving under curvature flow. We plan to extend this formulation to <br/>allow the simulation of thousands of seeds by using only a few level set<br/>functions.  The efficient computation of high frequency wave propagation <br/>and the semi-classical limit of the Schrodinger equation is the third<br/>project. The proposed algorithm is based on the observation that most of <br/>the time, in these limiting regimes, the solutions are very localized <br/>in the wavenumber domain. This can be exploited by solving the equations in this<br/>domain using a fast local convolution. It is planned to update the solutions<br/>by  the computation of the matrix exponential using a Krylov subspace<br/>approach.<br/><br/>Each of the proposed projects has the potential to have a significant impact <br/>on problems that are both fundamental and technologically important. <br/>Heteroepitaxial growth is scientifically interesting since it has effects<br/>on both nanoscales and mesoscales.  It is technologically relevant since<br/>quantum dot materials are made in this way.  Our proposed techniques will<br/>greatly increase the simulation speed  thereby facilitating  model development.<br/>The study of grain boundary motion using curvature flow is a classic problem <br/>in applied and computational mathematics which has importance in material s<br/>cience. Since there are no robust simulations of a large number grains in <br/>three dimensions the proposed project should have significant impact. <br/>The efficient computation of high frequency wave propagation has important <br/>facets ranging from antenna design to seismic sensing.  On the other hand,<br/>fast simulation of the semi-classical limit of the Schrodinger equation <br/>could  provide deeper insight into chemical reaction dynamics, <br/>molecular-surface scattering, and photodissociation, for example.<br/>"
"0748333","CAREER: Analysis and Modeling for Image Processing Problems","DMS","COMPUTATIONAL MATHEMATICS","02/01/2008","01/17/2008","Selim Esedoglu","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Leland Jameson","09/30/2014","$400,000.00","","esedoglu@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1271","0000, 1045, OTHR","$0.00","The investigator, together with his students and collaborators, will tackle important analytical and computational problems from image processing and computer vision that arise also in a variety of other fields, such as computer graphics, population dynamics, and materials science. A significant analytical component of the research program will focus on better understanding the coarsening phenomena observed in ill-posed diffusion equations that are encountered in such disparate fields as image processing (where they appear in models of image segmentation), granular flow (where they describe the formation of shear bands in granular materials), and population dynamics (where they model aggregation of certain types of bacteria). A second component of the research program will develop novel numerical algorithms for efficient computation of the motion of multiple phases under geometric motions arising as steepest descent for energies that contain bulk and surface tension terms. These interfacial motions are frequently encountered not only in fundamental image processing and vision procedures such as image segmentation, but also in materials science where they can describe grain boundary motion in polycrystalline materials. A number of numerical ideas will be utilized in this direction, including the level set method and some novel alternatives to it. The algorithms will be tested on large scale computations of high scientific interest, such as three dimensional simulations of grain growth in materials, using many grains. In addition, the investigator will continue his research program in extending successful image processing models to the context of surface processing in computer graphics.<br/><br/>The image processing and computer vision tasks that will be impacted by results from this research program include image denoising and segmentation, which are fundamental preliminary operations that are needed whenever useful information is to be extracted from images or video automatically. The goal of image denoising is to remove artifacts and noise from images so that subsequent operations can be performed more reliably. The goal of segmentation is to identify parts of an image occupied by distinct objects. <br/>Examples of practical applications where these procedures play a primary role include medical imaging, face recognition, and target identification and tracking. Insights gained from the analytical component of this research program will elucidate the nature of some of the most popular but incompletely understood techniques developed for these vision tasks, and consequently allow more effective use of these techniques. The new computational methods that will be developed as a major part of the project will allow efficient and accurate solution of the numerical problems encountered in these computer vision applications, and thus reduce the computer time needed for knowledge extraction from images and video. The numerical issues from computer vision that will be targeted by this research program have a lot in common with numerical issues seen in some central problems in materials science. This connection will in particular make it possible to apply the new computational methods to large scale simulations of the motion of grain boundaries in materials; this computational capability is highly desirable and would be invaluable in predicting the performance and reliability of materials.<br/>"
"0810982","Collaborative Research:  Algebraic Multigrid Methods:  Multilevel Theory and Practice","DMS","COMPUTATIONAL MATHEMATICS","10/01/2008","09/18/2008","","PA","Pennsylvania State Univ University Park","Standard Grant","Junping Wang","09/30/2012","$198,592.00","","","201 Old Main","University Park","PA","168021503","8148651372","MPS","1271","0000, 9263, OTHR","$0.00","The primary goal of this collaborative proposal is to develop<br/>theoretically based algebraic multigrid (AMG) solvers for Hermitian<br/>(and, where possible, non-Hermitian) positive-definite problems.  The<br/>team aims to improve understanding of the performance of the family of<br/>AMG algorithms and, with this improved knowledge, to develop AMG<br/>methods that offer provable, computable, a priori information on the<br/>algorithm's performance.  The project team represents a close<br/>collaboration of experts in this area, each of whom has made<br/>contributions in the field.  Over the past several years, the team has<br/>begun to work collectively on developing new multilevel solvers and<br/>rigorous theoretical results for the convergence and complexity<br/>analysis thereof.  Together, the team will have the capability to take<br/>a step toward answering some of the fundamental research questions<br/>associated with these two essential aspects of the analysis and design<br/>of efficient algorithms.<br/><br/>We expect the work proposed here to: (1) directly impact computational<br/>simulation codes currently employing multi-level solvers, by providing<br/>faster and more reliable computational tools for the numerical<br/>computations at the core of physical simulations; and (2) allow for<br/>simulation of phenomena for which suitable solvers are currently<br/>unavailable.  The results from the proposed research will, thus, have<br/>a direct impact on scientific and engineering problems, including<br/>those from energy, through both the simulation of particle physics and<br/>processing of data from oil reservoir models, biophysics, in surgical<br/>simulation, and the environment, in climate prediction and contaminant<br/>remediation models.  The algorithms to be investigated here are<br/>already in use in many of these fields, but are often considered to be<br/>""expert-only"" tools.  The goal of this proposal is to develop more<br/>reliable and robust versions of these tools.  The proposed research<br/>will have a strong educational impact as well, as it provides for a<br/>solid base for training of graduate students in the modern theoretical<br/>and practical aspects of numerical methods for modeling of<br/>applications arising in science and engineering."
"0810918","Collaborative Research: A Software System for Algebraic Geometry Research","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS","09/15/2008","06/25/2009","David Eisenbud","CA","University of California-Berkeley","Continuing Grant","Junping Wang","08/31/2010","$18,000.00","","de@math.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1264, 1271","0000, 9263, OTHR","$0.00","Macaulay2 is a free computer algebra system dedicated to the qualitative <br/>investigation of systems of polynomial equations in many variables. It <br/>was developed by Daniel Grayson and Michael Stillman with NSF funding. <br/>Grayson and Stillman will continue the development of Macaulay2. They <br/>will upgrade existing algorithms, develop and publish new algorithms, and <br/>implement new algorithms. In particular, they will develop the interaction <br/>of the symbolic computations that are Macaulay2's strength with the new <br/>floating point algorithms in algebraic geometry that are now being <br/>developed by Andrew Sommese, Jan Verschelde, Anton Leykin, Frank Schreyer <br/>and others. It is anticipated that these will make a whole new class of <br/>problems accessible to experimentation and, in many cases, solution. <br/>Eisenbud will organize contacts for the extended integration with other <br/>systems and will engage other mathematicians in the development work that <br/>needs to be done. Central to the project are the continued expansion of <br/>the collaborations that have been the hallmark of Macaulay2 development. <br/>For this purpose two Macaulay2 Workgroup Meetings will be held in the <br/>course of the two-year grant. One particular research problem to be <br/>attacked is: the use of computational systems to (probabilistically) <br/>disprove, or suggest a proof of, the Jacobian Conjecture on polynomial <br/>automorphisms of affine spaces (this will require the use the new floating <br/>point algorithms). Other areas where new algorithms can make an impact <br/>include the study of numerical systems, fractions with specified types of <br/>denominators, ideal factorization, systems where the multiplication of the <br/>variables doesn't satisfy the commutative law, geometric optimization, the <br/>analysis of observations of gene expression levels over time, and <br/>bioinformatics. <br/><br/><br/>Macaulay2 is part of the infrastructure that supports mathematical <br/>research involving systems of polynomial equations in many variables. The <br/>study of such systems of polynomial equations is central in pure and <br/>applied mathematics and in physics, with recent new impacts in such fields <br/>as cryptography, robotics and string theory. Increasing computer power <br/>and the availability of programs like Macaulay2 are making a new level of <br/>experimentation possible. The experimental results found with Macaulay2 <br/>are helping in the formulation and development of tractable conjectures in <br/>mathematics as well as in physics. A measure of Macaulay2's impact is that <br/>at least 270 research papers have cited Macaulay2, several mathematicians <br/>have contributed code, and books and course materials are now using it. <br/>The PI's will develop the software further and will recruit developers <br/>from the research community. They will introduce graduate students and <br/>mathematicians to the use of computers in research mathematics and the <br/>requisite skills in programming and development of algorithms, through <br/>workgroups at Berkeley and through the appointments of graduate students <br/>as graduate assistants."
"0818030","Collaborative Research: Multiscale Modeling of Solid Tumor Growth","DMS","COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","09/15/2008","07/17/2009","Steven Wise","TN","University of Tennessee Knoxville","Continuing Grant","Mary Ann Horn","08/31/2012","$70,000.00","","swise1@utk.edu","201 ANDY HOLT TOWER","KNOXVILLE","TN","379960001","8659743466","MPS","1271, 7334","0000, 7722, 7752, 9150, OTHR","$0.00","The goal of this project, which lies at the interface between mathematics and biology, is to develop mathematical models of tumor growth that connect intra-tumor molecular and cellular properties with critical tumor behaviors, such as invasiveness, and experimentally observable properties such as morphology. The research group will (1) perform novel analytical and computational studies of important constituent processes, (2) incorporate experimental data into these studies, and (3) develop and apply state-of-the-art numerical algorithms to large-scale computations over multiple time and space scales. By integrating experimental data with sophisticated, multi-scale mathematical and computational models, the potential for breakthroughs that will significantly further the understanding of tumor biology are great, thereby addressing a pressing national and global need. <br/><br/>Solid tumors are complex micro-structured materials, where the three-dimensional tissue architecture (morphology) and dynamics are coupled in complex, nonlinear ways to cellular characteristics and to molecular composition and structure of the growth environment. This close connection between the tumor morphology and the underlying cellular/molecular dynamics has fundamental scientific importance in that the cellular dynamics that give rise to various tumor morphologies also control its ability to invade the host tissue. This allows observable properties of the tumor, such as its morphology, to be used to both understand the underlying cellular physiology and predict the tumors invasion potential. In particular, the conjecture that diverse morphologic patterns of invasion observed during tumor growth are the quantitatively predictable result of molecular inhomogeneity (of both composition and structure) in the tumors growth environment will be tested. Because tumor cells use similar or identical migration and proliferation mechanisms as normal cells, and because of the multi-scale nature of these processes, the mathematical modeling, analysis and simulation that will be conducted in this project will also have application in understanding normal functional processes during development, wound-healing, stem cell differentiation and tissue regeneration. This project will also establish a new collaboration among five institutions and broadens the participation of women and minorities in research as trainees in the investigators? groups, thereby addressing national needs. It will provide interdisciplinary training with theoreticians and experimentalists at the interface between mathematics and tumor cell biology. Finally, a month-long summer COSMOS (California State Summer School for Mathematics and Science) course at UC Irvine will be developed for high school students on these topics. This course enhances the participation of gifted high school students in research, and helps to recruit new math and science undergraduates, which addresses another national need."
"0811039","Numerical Analysis of Quasicontinuum Methods","DMS","COMPUTATIONAL MATHEMATICS","08/01/2008","06/30/2008","Mitchell Luskin","MN","University of Minnesota-Twin Cities","Standard Grant","Junping Wang","07/31/2012","$282,034.00","","luskin@math.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1271","0000, 7237, 9263, OTHR","$0.00","Many physical processes of great importance to science and technology require numerical methods that couple modeling of the interactions of individual atoms in small regions with modeling of the interactions among larger sets of atoms (continuum models) in the remaining regions.  An important example is crack growth, where practical predictive models require the accuracy of the detailed interaction of the atoms in a small region near the crack tip, along with the efficiency of continuum modeling in the larger surrounding material.<br/><br/>Although the greatest accuracy could be achieved by a computer simulation of the interactions of all of the atoms in the material, the large number of atoms in the material makes this approach infeasible for even the fastest computers.  The quasicontinuum method can make possible such simulations without sacrificing the accuracy needed for reliable prediction by coupling an atomistic model in the neighborhood of the crack tip with a continuum model in the surrounding material.  The continuum model achieves the desired accuracy with a major reduction in computational work by replacing the large numbers of atoms in selected regions with representative atoms.<br/><br/>The principal investigator proposes to develop analysis and adaptive algorithms for quasicontinuum methods that will ensure their reliability and improve their efficiency.  Theory and rigorous numerical experiments will be developed to determine the most accurate and efficient atomistic-continuum coupling.  The development of the quasicontinuum method has the potential to facilitate the design of new materials better able to resist failure and having other properties important for science and technology."
"0753106","SM: Student Travel Grant for SIAM Data Mining Conference 2008","DMS","COMPUTATIONAL MATHEMATICS, Info Integration & Informatics","05/15/2008","07/30/2009","Jaideep Srivastava","MN","University of Minnesota-Twin Cities","Standard Grant","Leland Jameson","10/31/2009","$20,000.00","","srivasta@cs.umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1271, 7364","0000, 7556, 9263, OTHR","$0.00","The investigator will use the funds from this grant to support the travel of students to the 2008 SIAM Data Mining (SDM 2008) Conference on Data Mining. This conference will be held in Atlanta, USA from April 24th to April 26th, 2008 (http://www.siam.org/meetings/sdm08/).<br/>Jointly organized by computer scientists and statisticians, this technical conference is a premiere venue for presenting new research results in the area of data mining, and is widely attended by researchers and practitioners in the field. Selection of award recipients will be done on the basis of quality of their research progress, as well as the potential to benefit from attending this meeting. Special emphasis will be given to graduate students, and in particular women and under-represented minority students, since attending conferences is an important part of their educational experience, and they often have limited travel funds.<br/><br/>Attending conferences such as SDM is of paramount importance for the development of graduate students. Participants have the opportunity to present their work, attend panel and keynote sessions, and interact with hundreds of others performing leading-edge research in the field.<br/>Such experiences help students in a number of ways ways. First, it enables them to get exposure to a wide range of cutting edge ideas in their field, beyond what they are likely to be exposed to in their home institutions and research groups. Second, it provides them an opportunity to meet established researchers, which can potentially lead to long term interactions and mentoring relationships. Third, in case they are presenting a paper, it provides an opportunity for early feedback on their thesis research and presentation skills from a broad audience. All of these benefits are invaluable in ensuring the development of research excellence, leading to a productive career in innovative research. Given that cutting edge innovation is critical to the United States' economic preeminence in the world, and a future workforce trained to be innovative is critical for it, the proposed project is well aligned with the national agenda.<br/><br/>"
"0810938","Numerical Linear Algebra and Approximation Theory Methods for Efficient Data Exploration","DMS","COMPUTATIONAL MATHEMATICS, THEORETICAL FOUNDATIONS (TF)","07/15/2008","07/09/2008","Yousef Saad","MN","University of Minnesota-Twin Cities","Standard Grant","Junping Wang","06/30/2012","$275,505.00","","saad@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1271, 7351","0000, 9263, OTHR","$0.00","The rapidly increasing sizes of the data sets being treated in various applications is starting to  render inadequate many of the traditional methods used in data exploration.  These methods break down not only because of  the increase in the  number of observations  (size of datasets themselves)  but also because the underlying  phenomena observed are intrinsically of high dimension, i.e., they involve a large number of variables  or parameters.  High-dimensional  datasets present great mathematical challenges but in  practice, the related difficulties are mitigated  by  the fact  that  in most  cases,  not  all the  measured variables  are  important  for  an  understanding  of  the  underlying phenomenon.   One  of  the  difficulties faced  by  current  dimension reduction techniques is that  existing algorithms are often too costly when dealing  with very  large data  sets.  To tackle  a few  of these challenges  the research team  of  this  project  will focus  on  the development of computationally efficient methods which blend classical techniques such  as PCA or  LLE, with other strategies  from numerical linear  algebra and  approximation  theory, to  reduce problem  sizes.  Thus, multilevel or divide and  conquer techniques are quite common in other  areas of  scientific computing  but received  relatively little attention in data  mining. The proposed work will  put methods of this type at  the forefront.   The research team  will also  consider tools borrowed   from  graph  theory,   specifically  techniques   based  on hypergraphs, graph  partitioning, and kNN graph  construction, to help with  dimension reduction.   Finally,  this project  will address  the complex issue of  dimension reduction by means of  tensors and the use of multilinear algebra.<br/><br/>Society  is  currently facing  an  unparalleled  surge of  exploitable information in  scientific, engineering, and  economical applications.  Typical examples of such applications include face-recognition which has uses in  security and commerce for example,  and the processing of queries  on the  world-wide web.   The  data sets  generated in  these applications are not only gaining in size (more data samples) but also in  their dimension (number  of parameters  or variables  to represent each data sample).  For example,  in face recognition, where one deals with sets of pictures the size would be the number of pictures and the dimension  would  be the  number  of  pixels  used to  represent  each picture.   Reducing the  dimension of  data is  a vital  tool  used in applications dealing  with large data  sets.  It is therefore  not too surprising that  this line of research has  gained enormous importance in the last few years.  The investigators of this project will explore methods to  solve this problem,  putting an emphasis on  those methods characterized  by low computational  cost. Among  these methods  are a class  of divide  and  conquer  techniques which  divide  the sets in smaller ones on which the classical methods are applied independently."
"0809270","Local Scales and Multi-Scale Representations of Images","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/23/2008","Triet Le","CT","Yale University","Standard Grant","Leland Jameson","10/31/2011","$138,249.00","","trietle@math.upenn.edu","105 WALL ST","NEW HAVEN","CT","065118917","2037854689","MPS","1271","0000, 9263, OTHR","$0.00","The investigator and his colleagues study the problem of extracting features of images and their properties via scales and local scales using the classical theory of function spaces and partial differential equations. They use the knowledge of scales and local scales in images to obtain an efficient method for image decompositions and multiscale representations, which are image dependent. Finally, they extend the theory of scales and local scales to higher dimensional data (graphs and manifolds), and to find efficient computational methods and algorithms to solve these problems.<br/><br/>Reducing a complex data to simpler representations that can be more easily analyzed is an important task in mathematics and information science. The study of scales and local scales provides a tool for doing this task. In particular, it provides a tool for organizing and clustering the internal multiscale structures of the data.<br/>Applications include medical imaging, hyperspectral imaging, satellite imaging, material science, terrain data analysis, and dimension reduction in higher dimensional data.<br/>"
"0810896","Mathematical and Numerical Study of Electromagnetic Waves Interacting with Metamaterials","DMS","COMPUTATIONAL MATHEMATICS","07/15/2008","04/05/2010","Jichun Li","NV","University of Nevada Las Vegas","Continuing Grant","Leland Jameson","06/30/2011","$121,726.00","","jichun.li@unlv.edu","4505 S MARYLAND PKWY","LAS VEGAS","NV","891549900","7028951357","MPS","1271","0000, 9150, 9263, OTHR","$0.00","This project is concerned with  the mathematical analysis and design of robust and efficient computational algorithms for  modeling wave interactions with negative index metamaterials (NIMs). These negative index metamaterials have some exotic properties (such as near field<br/>refocusing) never seen before in those natural electromagnetic materials.<br/>The numerical NIM analysis plays an important role for the design of the nano-structures with complicated geometries that establish a NIM.<br/>these NIM models are far more complicated than the well-studied  Maxwell's equations in free space due to its dispersiveness,  and the introduced electric and magnetic polarization currents.<br/>Solving them accurately and efficiently is quite challenging and very few work has been done in regards of solid mathematical analysis and modeling. The ultimate goal of this project is to develop an efficient set of time domain finite element methods that are mathematically sound, accurate and fast convergent for simulating wave interactions with metamaterials.<br/><br/>Study of metamaterials is one of the hottest topics in many disciplinaries since 2000, with potential revolution in design of antenna, waveguides and radar, nanolithography and imaging at subwavelength resolution (used for better understanding of the images obtained from noninvasive geophysical probing and tumor detection), near field control and manipulation (used for detecting low levels of chemical and biological agents, manipulation of molecules), and invisibility cloak (used for stealth technology). Developing robust and efficient algorithms for negative index metamaterials will benefit broader areas such as electrical engineering, materials, optics, physics, nano-technology, and biomedical technology. Furthermore, the proposed project will help the PI recruit and train graduate students (this project will support a female Ph.D student currently supervised by the PI) to pursue careers in computational mathematics.<br/>"
"0807811","Adaptive Finite Element Methods for Multiscale Problems Governed by Geometric PDE","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2008","08/28/2008","Ricardo Nochetto","MD","University of Maryland, College Park","Standard Grant","Michael Steuerwalt","08/31/2012","$510,119.00","","rhn@math.umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1266, 1271","0000, 9263, OTHR","$0.00","Nochetto<br/>DMS-0807811<br/><br/>     The adequate numerical treatment of continuum phenomena<br/>exhibiting disparate space-time scales is a formidable<br/>mathematical and computational challenge.  Modern algorithms<br/>should be able to resolve fine scales for certain physical<br/>quantities without overresolving others, thereby optimizing the<br/>computational effort and making realistic 3d simulations<br/>feasible.  Mathematical models in biophysics (such as<br/>biomembranes in both fluid and gel state), in materials science<br/>(such as crystal surface morphologies and bilayer actuators), and<br/>in shape optimization (including both fluid and solid mechanics)<br/>are typical yet quite distinct examples addressed in this<br/>project.  Their multiscale structure manifests in rapid<br/>transients and slow motion regimes in time, as well as point and<br/>line singularities (interfaces) and thin layers in space.  Large<br/>domain deformations, perhaps leading to topology changes, is<br/>another intriguing feature.  The goal of this project is to<br/>design, test, and analyze reliable and efficient adaptive finite<br/>element methods for these multiscale problems, most governed by<br/>geometric partial differential equations, with space-time error<br/>control based on a posteriori error estimation.  This project<br/>builds upon, and in fact extends and enhances, the work of the<br/>prior NSF Grant DMS-0505454.  It is organized in a number of<br/>small but interrelated topics permeated by the roles of geometry<br/>and adaptivity throughout, from issues in numerical analysis and<br/>computation to applications. <br/><br/>     This project blends quite delicate analytical,<br/>computational, and modeling issues in several areas of research<br/>of strategic importance such as nanotechnology, materials and<br/>manufacturing, biotechnology, and high performance computing. <br/>Designing smart electro-mechanical devices as well as<br/>understanding key functions of lipid membranes in living<br/>organisms, both at the nano and microscales, require powerful and<br/>flexible computational algorithms capable of dealing with large<br/>shape variations.  This project develops such tools.  It is a<br/>collaborative and interdisciplinary project involving a number of<br/>scientists in the US (four of them engineers and physicists) and<br/>abroad, as well as several students and postdocs.  Because a<br/>substantial effort is devoted to education and human resource<br/>development, most resources are used to partially support<br/>students and postdocs. <br/><br/>"
"0811177","Efficient Numerical Methods for Viscous Incompressible Flows","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","05/11/2009","Jian-Guo Liu","MD","University of Maryland, College Park","Continuing Grant","Junping Wang","03/31/2010","$150,920.00","","jliu@phy.duke.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1271","0000, 9263, OTHR","$0.00","This project will address the development of a class of novel and very efficient numerical methods for incompressible flows based on a reformulation of the Navier-Stokes equations that has been proved well-posed and provides boundary conditions for the viscous contribution to pressure in terms of vorticity circulation.  The main insight is that the total creation of boundary vorticity can be computed exactly by a commutator between the Laplacian and Helmholtzprojection operators. Moreover, this term is dominated by the viscosity term, hence we can treat it explicitly to gain efficiency and <br/>stability. The major advance is that the method can be formulated in standard continuous finite element space, and there is no compatibility condition needed for the velocity and pressure approximation spaces. Hence the standard fast solver can be applied to Navier-Stokes equation directly.<br/><br/>Accurate, efficient simulations of 3D flows in complicated domain are still  major challeges for many scientific and engineering problems. The resolution of the flows near physical boundaries is essential to the accurate prediction of the body force such as the lift and drag, shedding of vortex and boundary layer separation. The success of this project will have an important impact on many branches of science and engineering. It will give more accurate predictions of body force which will result in better energy efficiency for transportation related applications. It will provide fast and reliable simulations for scientific research and engineering application.<br/>"
"0929241","Numerical Methods for Wave Propagation Problems: Efficient Resolution of Multiple Scales","DMS","COMPUTATIONAL MATHEMATICS","10/02/2008","05/14/2009","Thomas Hagstrom","TX","Southern Methodist University","Standard Grant","Junping Wang","07/31/2009","$50,858.00","","thagstrom@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1271","0000, 9150, 9263, OTHR","$0.00","The focus of our research will be the detailed study of questions we deem crucial to the development of reliable, efficient, and general computational tools for wave propagation problems. These, in turn, can have long-term impacts on numerous fields in science and engineering. Precisely we will:<br/>(i) Further develop accurate methods for truncating the computational domain near regions where full approximations are required, extending the range of application of the successful methods we have previously constructed to inhomogeneous and anisotropic media as well as to multiscale computations; (ii) Construct and analyze novel high-resolution approximation schemes enabling accurate simulations with near-optimal degrees-of-freedom per wavelength, mild time-step stability restrictions, and easy coupling with grid grid generation software to efficiently treat problems in complex geometry; (iii) Apply our methods to difficult problems in aeroacoustics; (iv) Collaborate with other computational scientists who are building and<br/>maintaining high-quality software for simulating waves. <br/><br/>Wave propagation phenomena are ubiquitous in nature. Although waves may be produced by physical processes ranging from electric currents to turbulent  flows to massive earthquakes, their basic features allow a unified mathematical description. From the perspective of simulations on modern computers, it is <br/>reasonable to hope that generally applicable tools can be constructed which will be useful in answering important questions throughout the basic and applied sciences. The challenge in the computational analysis of waves is that almost all problems of interest exhibit widely varying spatial scales. This is a consequence of the fundamental fact that waves propagate long distances relative to their characteristic dimension, the wavelength. We thus will work to develop methods which allow us to avoid the direct computation of the wave field everywhere along its path, concentrating computational resources only where they are needed. In addition to our work on basic techniques with broad applications, we plan focused studies on problems related to the generation of sound by jets and its propagation into the environment. We believe that the fundamental studies we will carry out can motivate the development of better sound suppression technologies for commercial and military aircraft.  <br/>"
"0810213","Nonlinear Optimization: Algorithms, Theory and Software","DMS","COMPUTATIONAL MATHEMATICS","08/01/2008","07/09/2008","Jorge Nocedal","IL","Northwestern University","Standard Grant","Leland Jameson","07/31/2012","$291,172.00","","j-nocedal@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1271","0000, 9263, OTHR","$0.00","Numerical optimization plays an essential role in a wide variety of scientific and engineering applications.  Medical imaging, electrical power network simulations, computational finance, and atmospheric sciences make extensive use of optimization models to simulate real-life phenomena.  As these models become increasingly more complex and incorporate a larger amount of data, the demands placed on optimization techniques have surpassed their capabilities.  This proposal presents three projects designed to address this challenge.<br/><br/>The first project proposes matrix-free methods for very large constrained optimization problems; we have in mind applications where the number of variables and constraints range in the millions.  The new algorithms compute inexact Newton steps by applying iterative methods to the inner linear systems of equations.  Questions to be addressed in this research include the appropriate management of inexactness, nonconvexity, and Jacobian singularity. The second project concerns the development of open-source software for problems in which the constraints are defined by the discretization of partial differential equations.  The new optimization solvers will be created in collaboration with Argonne National Laboratory and will operate in a matrix-free environment.  The third project investigates procedures for detecting if a nonlinear optimization problem is feasible.  This question has not received sufficient attention in spite of its importance in mixed integer nonlinear programming and in parametric studies of optimization models.  The goal is to develop new optimization techniques that transition smoothly between optimization and feasibility, and vice versa. <br/><br/>The intellectual merits of the proposed activity lie in the complexity of designing optimization methods that are capable of dealing with nonconvexities and nonlinearities.  The broader impacts resulting from this work will be seen in the successful application of the new algorithms and software in areas such as circuit simulation, computational chemistry, medical imaging, atmospheric sciences, and machine learning.  The principles and ideas developed in this project will stimulate future research in new areas of application."
"0811138","Collaborative Research: Efficient algorithms for free boundary flows of complex fluids","DMS","COMPUTATIONAL MATHEMATICS","08/15/2008","08/11/2008","Giovanna Guidoboni","TX","University of Houston","Standard Grant","Junping Wang","07/31/2011","$122,140.00","Roland Glowinski","giovanna.guidoboni@maine.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271","0000, 9263, OTHR","$0.00","In engineered and biological systems, flows often involve complexity because of the shape of the flow domain which can be unknown, and because the fluid can consist of or contain macromolecules and nanoparticles which impart unusual behavior.  This research aims at developing novel numerical algorithms for computing flows with such complexity. Because of their robustness and reliability, fully-coupled (monolithic/implicit) schemes are commonly used to simulate flows with free boundaries; however, these schemes may become impractical, especially in three-dimensional geometries and when the fluid has complex behavior, e.g., viscoelasticity. On the other hand, partitioned schemes have simpler implementation and potential lower computational cost, yet have shown severe stability issues when applied to highly nonlinear free boundary flows.  This research will develop novel partitioned algorithms for free boundary flows of complex fluids which will combine good stability properties with low computational costs and ease of implementation.<br/><br/>The novel algorithms developed in the project will be used to study and understand complex flows arising in biology, medicine, and engineering. Major applications include blood flow in human arteries, the optimization of coating processes, and the flow of emulsions which naturally occur in oil extraction. Multidisciplinary training will be incorporated in the research by joint advising of two PhD students in Applied Mathematics and Chemical Engineering.  The outcomes of the proposed research will impact cyberinfrastructures through scientific computing, as well as other areas of complex fluid mechanics where computing plays a central and essential role.<br/>"
"0810176","Collaborative Research: Tuning-Free Adaptive Multilevel Discontinuous Galerkin Methods for Maxwell's Equations","DMS","COMPUTATIONAL MATHEMATICS","07/15/2008","05/06/2010","Ronald Hoppe","TX","University of Houston","Continuing Grant","Leland Jameson","06/30/2012","$177,188.00","","rohop@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271","0000, 9263, OTHR","$0.00","The investigator and colleagues are formulating, analyzing, and implementing adaptive multilevel Discontinuous Galerkin methods for coupled interior/exterior domain problems associated with the time-harmonic Maxwell equations. These advanced finite element methods are being realized as multilevel techniques on the basis of an adaptively generated hierarchy of triangulations of the computational domain. The research team is focusing on three central issues related to the basic steps `SOLVE', `ESTIMATE', `MARK', and `REFINE' of the adaptive loop. First, the smoothing process within the multilevel solver is performed only on the newly refined part of the triangulation obtained by a residual type a posteriori error estimator. Second, the a posteriori error analysis, which additionally has to take into account the effect of such local smoothing, aims to provide conditions guaranteeing a reduction of the global discretization error at each refinement step. <br/>Third, the selection of elements, faces and edges of the triangulation for refinement are based on a bulk criterion with an automatic (`tuning free') choice of the parameters controlling the amount of refinement in order to achieve optimal performance of the overall algorithm. Finally, the team is developing criteria to choose the parameters of artificial radiation boundary conditions automatically, such that no tuning on behalf of the user is required there as well. <br/><br/>Simulation of electromagnetic phenomena <br/>is a particularly challenging problem in computational mathematics. <br/>The investigator and colleagues are <br/>establishing a profound theoretical foundation for adaptive multilevel discontinuous Galerkin methods in electromagnetic field computations. They are developing a reliable algorithmic tool, of optimal computational complexity, that can be used for the numerical solution of challenging real-life problems in electrical engineering applications. The methods developed in this project have numerous technical and scientific applications, for instance semiconductor simulation or particle accelerator design. The results will be disseminated through publication of algorithms and results and reference computer codes being developed during this project will be made available to practitioners."
"0811153","Optimal Matching of 3D Movies by ""Time/Space"" Deformations","DMS","COMPUTATIONAL MATHEMATICS","08/15/2008","08/07/2008","Robert Azencott","TX","University of Houston","Standard Grant","Leland Jameson","07/31/2012","$583,922.00","Roland Glowinski, Jiwen He, Ronald Hoppe","razencot@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271","0000, 9263, OTHR","$0.00","This project focuses on the optimal diffeomorphic matching for pairs of 3D movies of dynamic deformable shapes. This includes the development and analysis of adequate mathematical classes of 4D-deformations in time-space, as well as the implementation of efficient algorithmic tools to determine an approximately optimal diffeomorphic matching. Such problems are of significant relevance in medical imaging, for instance to compare echographic movies of soft organs such as beating hearts, and the investigators will collaborate with cardiologists at The Methodist Hospital (Houston) to test their numerical and mathematical approaches on real patients data. The results of this project will thus contribute to a very active field in medical research.<br/>Extending remarkable results of Trouve, Younes, Miller, Glaunes for optimal diffeomorphic matching of static 3D- shapes, the investigators consider the unknown time-space diffeomorphism as the endpoint of the flow of time-space diffeomorphisms generated by an unknown flow of vector field V . Their strategy is then to minimize a suitable cost functional over admissible vector field flows V in a suitable  Hilbert space. The cost functional combines a disparity functional, measuring the distance between a deformed movie and the target movie, and an energy functional , namely the kinetic energy of V. The existence of solutions will be studied as well as approximate cost functionals whose minimization is computationally feasible by gradient-descent methods. These techniques require the solution of high dimensional systems of ODEs whose efficient  integration will be one of the numerical challenges. An optimal control approach featuring operator-valued controls will be  intensively explored. The models and numerical tools will be validated by selected medical case studies on echocardiographic movies, in collaboration with cardiology specialists.<br/><br/>Echographic movies of patients hearts are now part of numerous clinical protocols in cardiology. Visual comparison of echocardiographic movies by medical doctors is frequent, to evaluate the effect of treatment on a patient, or to compare the cases of different patients. In these comparisons, biological heart cycles are not identical in time and can only  be matched by a mental time warping. The hearts of distinct patients are dissimilar in detailed shape and volume , and are in constant elastic deformation. To compare them visually requires a geometric distortion of their shapes which is implicitly realized by the vision system of expert cardiologists.<br/>In this project, mathematicians seek to emulate these comparison tasks, in a very generic context, by computing the time warping and the frame by frame geometric distortions which achieve the best matching of two movies. The size of these distortions is then quantified by the imaginary energy which would be needed to physically deform one beating heart to match the other.<br/>The solution of this problem requires sophisticated mathematical theory and presents a serious challenges to reach an efficient numerical computation on standard computers.<br/>The results of this project impact comparative medical diagnosis, for instance in cardiology and foetus development, with new tools to specify and visualize key differences between the time dynamics of soft deformable organs. They will also provide generic tools to technically compare deformation dynamics, with  a wide range of applications to performance evaluation and optimization in high tech manufacturing of sophisticated deformable materials and soft objects.<br/>"
"0803403","International Conference on Modeling and Simulation","DMS","COMPUTATIONAL MATHEMATICS","05/01/2008","03/13/2008","Jiwen He","TX","University of Houston","Standard Grant","Leland Jameson","04/30/2009","$20,000.00","Tsorng-whay Pan","jiwenhe@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271","0000, 7556, 9263, OTHR","$0.00","The investigators and their colleagues propose to organize an<br/>International Conference on Modeling and Simulation, which will be<br/>held from July 9-12, 2008 in Xi'an Jiaotong University, Xi'an, Shaanxi<br/>Province, China. In the last two decades, the rapid development of<br/>computer power and sophisticated computational methodology has<br/>permitted the application of high performance computing to simulation<br/>and modeling of unprecedented accuracy and scope applied to a wide<br/>range of important problems.  These applications have had profound<br/>implications and applications in mathematics, science, engineering and<br/>industry. Scientific computing is in fact the only feasible tool for<br/>analyzing many different types of critical important phenomena such as<br/>flow and transport, weather prediction, wave propagation, novel<br/>material design, computational chemistry, genome sequencing and<br/>analysis.  The proposed conference will bring prominent researchers<br/>from U.S. and Europe working in these active areas to Asia, as to<br/>enhance and facilitate contacts and dialogs between specialists of<br/>different fields dealing with modeling and simulation so that new<br/>concepts can be discovered and new methodologies can express their<br/>power.<br/><br/>In the past quarter of a century, along with rapid economical<br/>developments and advancements in the Far East Asian countries,<br/>particularly the phenomenal growth in China, there has been ever<br/>increasing collaborations between U.S.- and Asia-based (also between<br/>Europe- and Asia-based) researchers in all fields of scientific<br/>research. Such a phenomenon and trend is clearly exemplified by what<br/>has taken place in mathematical sciences latterly (thanks to the<br/>effort and initiatives of the NSF). These cross-continental research<br/>collaborations have already made a large impact on many fields of<br/>scientific research, most noticeably, on the human genome sequencing<br/>project in biological sciences and on the proof of the Poincare<br/>Conjecture in mathematical sciences.  The proposed conference will be<br/>a contribution to promoting, enhancing, and stimulating such<br/>cross-continental research interactions and collaborations in<br/>mathematical sciences, more specifically, in applied and computational<br/>mathematics,"
"0810506","Marching Over Poles: Innovative Ways to Solve Matrix Differential Riccati Equations","DMS","COMPUTATIONAL MATHEMATICS","09/01/2008","06/30/2008","Ren-Cang Li","TX","University of Texas at Arlington","Standard Grant","Junping Wang","08/31/2012","$262,256.00","","rcli@uta.edu","701 S. NEDDERMAN DR","ARLINGTON","TX","760199800","8172722105","MPS","1271","0000, 9263, OTHR","$0.00","Matrix Differential Riccati Equations (MDREs) arise frequently throughout applied mathematics, science, and engineering.  In particular, they play major roles in optimal control, filtering and estimation, elementary particle physics, and two-point boundary value problems of differential equations.  A number of algorithms have been proposed over the past 25 years for solving MDREs numerically.  These include general-purpose numerical methods retooled to take advantage of today's computing technology for better performance.  Nevertheless, these methods, because of their generality, inherently do not and cannot fully exploit many valuable structural properties of MDREs, some of which remain to be discovered.  The objectives of this project are to build a better mathematical understanding of the long-term solution behavior of MDREs, especially those that have singularities in the solutions, and to derive new numerical methods that are able to gracefully handle singularities, unlike the commonly used abort-and-restart procedures.  A new MDRE solver package as the result of a better mathematical understanding, new algorithms, and the exploitation of IEEE floating-point exception handling capabilities in today's microprocessors is expected to be released publicly for the use of scientific communities upon the successful completion of the project.  Graduate students with emerging expertise in numerical differential equations will be involved."
"0810958","Numerical Algorithms for the Detection and Simulation of Surface Water Waves","DMS","COMPUTATIONAL MATHEMATICS, FD-Fluid Dynamics, OPPORTUNITIES FOR RESEARCH CMG","07/01/2008","08/02/2009","David Nicholls","IL","University of Illinois at Chicago","Standard Grant","Junping Wang","06/30/2012","$155,140.00","","davidn@uic.edu","809 S MARSHFIELD AVE M/C 551","CHICAGO","IL","606124305","3129962862","MPS","1271, 1443, 7215","0000, 7303, 9263, OTHR","$0.00","The generation, propagation, and energetic transport of ocean waves<br/>play a crucial role in many technologies including near-shore<br/>watercraft navigation, the design of deep-sea oil-drilling rigs, and<br/>the generation and propagation of tsunamis.  However, the fundamental<br/>technologies for the remote detection and measurement of these<br/>nonlinear waves are based upon the scattering of linear acoustic or<br/>electromagnetic radiation from the surface or bulk of the fluid.  Not<br/>surprisingly, the state-of-the-art in theory, approximation, and<br/>numerical simulation of these two disparate wave phenomena is quite<br/>different.  In this proposal the Principal Investigator (PI) advocates<br/>the development of a unified computational approach to these two<br/>problems with the goal of producing a set of highly accurate, stable,<br/>and error-controllable numerical algorithms for the detection,<br/>simulation, and evaluation of waves on the surface of large bodies of<br/>water.  Over the past ten years the PI has developed a set of stable,<br/>high-accuracy Boundary Perturbation (BP) algorithms for the study of<br/>both traveling ocean waves and scattering returns from irregularly<br/>shaped obstacles.  These BP algorithms each involve an independent<br/>perturbation parameter and the PI proposes to investigate a strategy<br/>of coupling these two disparate algorithms by linking these<br/>parameters, thus realizing huge computational savings.  Additionally,<br/>the PI proposes to significantly extend and improve existing BP<br/>algorithms for the simulation of linear waves in the high-frequency<br/>regime.  In short, the Geometric Optics solution points the way to a<br/>solution Ansatz which can be represented with a frequency-independent<br/>number of unknowns.  The PI, in collaboration with F. Reitich, has<br/>demonstrated how this Ansatz coupled to classical BP methods can be<br/>used to compute high-frequency scattering returns from shallow<br/>surfaces with enormous computational savings.  In this proposal the PI<br/>advocates the further development of these algorithms to incorporate<br/>the scenario of multiple reflections.<br/><br/>The generation and propagation properties of ocean waves are crucial<br/>in many technologies including watercraft navigation, design of<br/>deep-sea oil-drilling rigs, and the study of tsunamis.  However, the<br/>methods for the measurement of these nonlinear ocean waves are based<br/>upon the scattering of linear acoustic or electromagnetic radiation.<br/>The state-of-the-art in both the theory and practice of these two<br/>disparate wave phenomena is quite different.  In this proposal the<br/>Principal Investigator (PI) advocates the development of a unified<br/>approach to the computation of these two problems with the goal of<br/>producing a set of reliable and accurate numerical algorithms for the<br/>detection and simulation of ocean waves.  This will involve the PI<br/>bringing together two separate threads of his research program to<br/>implement this unified approach.  Additionally, he will need to expand<br/>the capabilities of his algorithms to account for the highly<br/>oscillatory character of the linear waves which arise in these<br/>applications.  The latter task will be particularly challenging and is<br/>the current focus of a great deal of research by both civilian and<br/>military agencies.<br/>"
"0809189","Wave Turbulence: Computational and Theoretical Investigations of a Story Far From Over","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2008","08/20/2008","Alan Newell","AZ","University of Arizona","Standard Grant","Junping Wang","08/31/2011","$150,000.00","Vladimir Zakharov","anewell@math.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1266, 1271","0000, 9263, OTHR","$0.00","Wave Turbulence is about understanding the long time statistical behavior of solutions of weakly nonlinear field equations describing a sea of random waves. The weak nonlinearity allows one to obtain a natural closure of the BBGKY hierarchy of the Fourier space moment equations which includes a kinetic equation which describes how the wavenumber density is redistributed throughout the spectrum by resonances. These equations have especially relevant finite flux solutions (Kolmogorov-Zakharov or KZ) which capture how conserved densities such as energy flow from sources to sinks. What makes Wave Turbulence an open problem is that these KZ solutions are almost never uniformly valid at all scales and so one is left with the challenge of finding out what happens in the regions of wavenumber space where they break down. The work in this proposal continues the authors' efforts to fill in these missing gaps and to make the theory complete. This is not an easy task because once breakdown occurs, the new states may contain strongly nonlinear coherent structures. In particular, we are working on the problem of condensation and whitecap formation. <br/><br/>Wave turbulence is about understanding the statistics of systems of waves. Imagine the sea surface after a storm. There are waves of all wavelengths, travelling in all directions. It is absolutely remarkable that one can, over a large range of scales, say what the longtime energy spectrum looks like. The energy spectrum is a graph showing how much energy there is in waves of different lengths and different directions. Even though the initial graph may reflect how the sea is first excited, the sea, because of resonances between waves of different wavelengths and directions, relaxes to a statistically steady universal state called the Kolmogorov-Zakharov or KZ spectrum. But not quite. If the storm is strong enough, at the smaller scales, the statistics of the sea surface is not described by the KZ spectrum. In fact, an observer will see that the more stormy the sea is, the higher the density of whitecaps (locally breaking waves which combine air and water into a frothy emulsion). The challenge is to find out what replaces the KZ spectrum at these small scales. We are developing a new theory of this behavior. In connection with this work, we are also interested in a phenomenon which has been known by mariners for centuries but which only now is being studied, the sudden emergence of freak waves in an otherwise relatively placid sea<br/>"
"0810875","Collaborative Research: Efficient High Order Methods for Deterministic and Stochastic Problems in Flow Analysis and Control","DMS","COMPUTATIONAL MATHEMATICS","08/01/2008","07/09/2008","Ana-Maria Croicu","GA","Kennesaw State University Research and Service Foundation","Standard Grant","Junping Wang","07/31/2012","$54,941.00","","acroicu@kennesaw.edu","1000 CHASTAIN RD MAILSTOP 0111","KENNESAW","GA","301445588","4705786381","MPS","1271","0000, OTHR","$0.00","Efficient, high order, spectral element algorithms will be developed to   analyze and control large-scale deterministic and stochastic problems in unsteady compressible fluid flow. The focus will be on spatial, temporal, polynomial chaos, and adjoint based optimal control algorithms that can be used for active control of complex time-dependent flows, particularly those that include the generation and propagation of aerodynamic noise. For the time and space approximations, we will develop discontinuous Galerkin spatial  <br/>approximations and high order, optimized implicit/explicit (IMEX) methods that minimize phase and dissipation errors.  Adjoint based methods for flow control will be used, and control strategies developed for appropriate use with the discontinuous Galerkin approximation. Since uncertainty is present in real flows, we will include efficient stochastic collocation approximations. Finally, we will allow controls to be deterministic or stochastic, and develop strategies to control flows in the presence of uncertainty.<br/><br/>The class of problems addressed by these methods are of great practical importance and require very large, computationally intensive simulations with efficient and high order methods. The ability to control acoustic noise generation is important to a wide variety applications, from windmill farms to jet engines.  Problems of optimality under uncertainty occur frequently in a wide variety of problems in science, engineering and technology that have  <br/>probabilistic parameters, nondeterministic initial conditions, uncertain input situations, and models based on incomplete knowledge.  A large number of problems such as engineering design, supply allocation, production planning and scheduling, transportation, inventory networks, finance, energy systems, environmental protection, pattern recognition, and military logistics require that decisions be made in the presence of uncertainty. Uncertainty governs the prices of fuels, the availability of electricity, and the demand for chemicals. Much of life requires us to make optimal choices under uncertainty, i.e., to choose the optimum from some set of optional courses of action in uncertain situations. Clearly, the development of the mathematics and computational methods for optimal control with and without uncertainty has broad impacts both inside and outside mathematics."
"0811167","Discontinuous Galerkin Methods for Optimal Control Problems Governed by Advection-Diffusion Equations","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/30/2008","Dmitriy Leykekhman","CT","University of Connecticut","Standard Grant","Dalin Tang","06/30/2012","$106,836.00","","dmitriy.leykekhman@uconn.edu","438 WHITNEY RD EXTENSION UNIT 11","STORRS","CT","062691133","8604863622","MPS","1271","0000, 9263, OTHR","$0.00","There are several methods to approach real-life problems.  An attractive and relatively cheap approach is to design a mathematical model that describes some physical phenomena and to use this model to make predictions.  To validate the model it is important that the predictions agree with real observations.  Usually mathematical models have several parameters that are unknown or uncertain.  Given some observations, these parameters can be tuned in order to reduce the discrepancy between the predicted and the observed values.  In other words, by refining a model we want to minimize certain objective quantities produced by the model.  Mathematically these types of problems can be classified as optimal control problems.   <br/><br/>In this proposal we are interested in optimal control problems with constraints given by systems of partial differential equations (PDEs).  In particular, we are interested in models that describe an evolution of a flow.  The corresponding PDEs for such problems are called advection-diffusion equations.  These equations are fundamental and solutions to such equations often exhibit ""nonsmooth behavior"", like shocks, boundary and interior layers, and interface discontinuities.  Such phenomena are often observed in reality and possess serious computational and analytical challenges in order to solve such problems numerically.  In designing a numerical method it is very important to know how the method behaves in the neighborhood of such discontinuities, and whether or not the resulting effects are global or local. <br/><br/>Over the years many competitive methods have been designed.  In this proposal we intend to incorporate a family of Discontinuous Galerkin (DG) methods.  These methods have received a lot of attention lately.  The main attractive feature is that DG methods use discontinuous functions to approximate the unknown solutions and in principle are well suited for problems with sharp changes in function values.<br/><br/>In this proposal we intend to develop new analytical tools that will enable us to analyze such methods in the context of optimal control problems.  We also plan to demonstrate computationally the advantages of the DG methods for estimating important physical quantities, such as bottom drag coefficient and eddy viscosity, over other commonly used methods for real-life geophysical flow problems."
"0811370","Dynamics at a fixed resolution","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, DYNAMICAL SYSTEMS","07/01/2008","06/30/2008","Sarah Day","VA","College of William and Mary","Standard Grant","Leland Jameson","06/30/2013","$128,281.00","","sday@math.wm.edu","1314 S MOUNT VERNON AVE","WILLIAMSBURG","VA","231852817","7572213965","MPS","1266, 1271, 7478","0000, 9263, OTHR","$0.00","This project focuses on extending and optimizing existing techniques for computational studies of dynamical systems.  Recognizing the need for discretizations and truncations of dynamical systems as a first step towards making a numerical study, the investigator and her collaborators focus on techniques that can overcome the corresponding loss of information.  In particular, in constructing the finite representation of the system used for numerical computations, they incorporate explicit bounds for discretization, truncation, and other errors accumulated during this process.  The finite representation is the dynamical system viewed at a fixed resolution -- information about the precise location of individual points and their trajectories is lost, but coarse, topological structures remain.  The investigator will incorporate topological tools, such as algebraic topology and Conley index theory, in computations on the finite representation to detect and prove the existence of dynamics for the original system.<br/><br/><br/>Dynamical systems models are being used throughout society.  Some examples include weather models used for hurricane prediction and population models used to study environmental effects on population size and persistence.  Currently, many researchers study dynamical systems like these using high powered computer simulations and statistical techniques.  On the other end of the spectrum, mathematicians have been able to decipher highly complicated dynamics in more abstract mathematical models.  The work described in this proposal aims to serve as a bridge between these two approaches.  More specifically, the investigator and her collaborators focus on the development of computational techniques that use sophisticated mathematical tools and yield mathematically rigorous results.  The mathematical tools come from the fields of algebraic topology, analysis, numerical analysis, and dynamical systems theory and may be used to decipher some of the phenomena of interest in the studied systems.  Prior progress in studying complicated dynamics in models from population ecology and heat convection motivates these continued studies.<br/>"
"0757212","FRG: Collaborative Research: Semidefinite optimization and convex algebraic geometry","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS","09/01/2008","08/06/2008","J. William Helton","CA","University of California-San Diego","Standard Grant","Junping Wang","08/31/2012","$478,980.00","Jiawang Nie","helton@math.ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1264, 1271","0000, 1616, 9263, OTHR","$0.00","The goal in this proposal is to develop the mathematical foundations and<br/>associated computational methods for the study of convex sets in real<br/>algebraic geometry. This work requires a combination of ideas and<br/>mathematical tools from optimization, analysis, algebra and combinatorics.<br/>The proposed program will lead not only to theoretical insights, but also to<br/>new algorithms and software that will enable novel applications in<br/>mathematics, engineering, and beyond. The work is organized in five main<br/>thrusts: semidefinite programming and sums of squares, convex semi-algebraic<br/>sets, sparsity and graphical structure, numerical polynomial optimization<br/>and applications, and deformations and variation of parameters. The PIs will<br/>focus on the development of a comprehensive theory and practical new<br/>algorithms for convex sets defined by polynomial inequalities. Specific<br/>problems and techniques include the formulation of semidefinite descriptions<br/>of convex hulls of real algebraic varieties, determinantal representations<br/>of hyperbolic polynomials, sparse polynomials and their symmetries, tropical<br/>geometry and homotopy techniques, and geometric programming.<br/><br/>Many areas in mathematics, as well as applications in engineering, finance<br/>and the sciences, require a thorough understanding of convex sets. This is a<br/>class of geometric shapes, with several different but complementary<br/>interpretations. The goal in this project is to achieve a better<br/>understanding of how these geometric properties emerge from their algebraic<br/>descriptions in terms of polynomial equations, and the corresponding<br/>computational implications. One of the main motivations is the possibility<br/>of applying these results in the context of optimization. The proposed<br/>research will contribute to existing knowledge, both in algebraic-geometric<br/>techniques as well as in mathematical optimization. It will create synergies<br/>between different branches of applied mathematics, and their engineering and<br/>scientific applications (e.g., in computational biology and statistical<br/>modeling). Successful completion of this project should contribute to the<br/>availability of efficient and reliable computational tools for solving<br/>polynomial systems, which have clear technological and economic interest.<br/>Other key features of this proposal include its integration with curriculum<br/>development, undergraduate research projects, training of graduate students<br/>and postdocs, and the development of new software tools for computational<br/>optimization.<br/>"
"0811259","Computational Modeling and Numerical Analysis of Solvation of Molecules","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/30/2008","Bo Li","CA","University of California-San Diego","Standard Grant","Junping Wang","06/30/2012","$160,000.00","","bli@math.ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271","0000, 7237, 9263, OTHR","$0.00","This project concerns computer studies of the solvation of molecules---the attraction and association of the solvent (such as water or salted water) molecules with solute (such as alcohol, protein, etc.) molecules.  The PI develops a new continuum-solvent approach in which an equilibrium solute-solvent interface is defined to minimize a solvation free-energy functional.<br/>There are three essential components of this new approach:  (1) a diffuse-interface description of solute-solvent interfaces that incorporates different kinds of molecular interactions;  (2) the molecular mechanics of solute atoms;  and (3) a multiscale method for the solute-solvent molecular interaction and the motion of solute atoms.  All these are closely connected to other studies in applied and computational mathematics such as geometrical motions of surfaces and multiscale computations.  The PI implements several new computational techniques, including the construction of initial solutions, an efficient local method, and stable calculations of free energy.  He also carries out the related numerical analysis on convergence, stability, and errors.  The proposed theory and methods are validated by the comparison with molecular dynamics simulations and known experiments.<br/><br/>Molecular interactions determine structures and functions of biological systems.  For example, drug cure diseases because the correct molecular interactions occur.  While much effort has been made to the study of drug designs, it is now widely accepted that new theories must be developed.<br/>In this regard, the proposed research is of great interest, since it brings rigorous mathematics into a challenging study of life sciences.  It is hoped that results of this project can be used to better understand why some drugs cure diseases better than others, to make new biomaterial that can better protect our homeland, and to develop sophisticated nanotechnologies that allow us to manipulate molecules for our daily lives.  In addition to potential applications to many areas of national interest, this project creates an education and training program that prepares mathematics students to do research in biologcal science.  This is clearly a needed and challenging task.<br/>"
"0811052","Mixed finite elements and smooth approximations for partial differential equations","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/25/2008","Gerard Awanou","IL","Northern Illinois University","Standard Grant","Junping Wang","06/30/2012","$137,940.00","","awanou@uic.edu","1425 W LINCOLN HWY","DEKALB","IL","601152828","8157531581","MPS","1271","0000, 9263, OTHR","$0.00","The project is directed towards the development, analysis and improvement of numerical methods for the elasticity equations and Monge-Ampere type equations.<br/>The research in methods for the linear elasticity equations will focus on improvement of mixed finite element methods. Very simple elements with weakly imposed symmetry have been recently developed on triangular and tetrahedral meshes but these elements have yet to be extended to quadrilateral, 2D and 3D rectangular and hexahedral meshes which are often favored by practitioners. This proposal will use the technique of constructing piecewise polynomial exact sequences for the development of stable mixed finite elements on the above mentioned meshes. The second area of study is the construction and analysis of smooth approximations to Monge-Ampere type equations and the application of the methods developed to the solution of problems from science and engineering involving Monge-Ampere type equations. Several approaches will be followed, including global optimization ones. As it is well known the Monge-Ampere type equations, like other fully nonlinear partial differential equations do not possess in general smooth solutions but several of the approximation schemes, e.g. the vanishing moment methodology, require to work in spaces of smooth functions. The focus will be on the implementation and improvement of the spline element method, developed by the investigator and others, which uses multivariate splines for the solution of higher order partial differential equations. It leads to flexible, robust, efficient and accurate approximations allowing easy implementation, the flexibility of using polynomials of different degrees on different elements and the simplicity of a posteriori error estimates since the method is conforming.<br/><br/><br/>Mathematical modeling of physical phenomena have become the standard tool for the investigation of numerous problems in science and engineering. But often the resulting equations do not have solutions that can be represented by simple mathematical formulas. Hence the development of numerical methods and their analysis is essential to this process. This project adresses two types of equations which appear in fundamental problems but the impact of the methods developed here goes well beyond the particular applications being considered.  The elasticity equations appear in many industrial, biological and engineering applications. The Monge-Ampere type equations appear in various geometric and variational problems, e.g.<br/>the Monge-Kantorovich problem. They also appear in applied fields such as meteorology, fluid mechanics, nonlinear elasticity, material sciences and mathematical finance. The development of the new methods from this project have the potential to put more  competitive tools in the hands of the nation's scientists and engineers. The educational component of the project is that it will introduce a new generation of students to computational mathematics involving practical problems. Therefore this also contributes to national security and helps maintain the global scientific leadership position of the nation.<br/><br/>"
"0810385","Numerical Analysis, Analysis and Modeling of Fluid Motion","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","09/20/2009","William Layton","PA","University of Pittsburgh","Continuing Grant","Junping Wang","06/30/2012","$278,549.00","","wjl+@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1271","0000, 9263, OTHR","$0.00","The accurate, efficient and reliable prediction of quantities in turbulent flows (the focus of the proposed research) forces systematic and integrated confrontation of basic issues in the modeling, analysis, numerical analysis and computation of turbulent flows, many of which are linked to basic issues in other flow problems and other areas of mathematics. Interconnected research sub-projects include: (i) Eduction of large coherent vortices and detection of false vortices created by models or numerics, (ii) Development and numerical analysis of high accuracy and efficiently solvable regularizations of fluid flow equations, (iii) New algorithms for ill-posed problems and applications, (iv) Development of LES models for compressible turbulence and analysis of their acoustic noise predictions, (v) Mathematical foundation for time-averaged large eddy simulation: continuous transitioning between direct numerical simulation, large eddy simulation and Reynolds averaged turbulence models, (vi) Uncoupling multi-physics fluid flow problems in fluid-structure interaction, groundwater-surface water models and fluid-fluid problems motivated by atmosphere-ocean coupling, and (v) Precise numerical analysis of time relaxation regularizations. <br/>    The accurate, efficient and reliable prediction of quantities in turbulent flows is a problem of fundamental importance in many applications ranging from global climate change, homeland security (dispersion of biological or chemical agents), pollution dispersal, energy efficiency and biomedical device design. Large eddy simulation is an approach to turbulent flows in which the large features are separated from the fine details of a flow for a numerical simulation. Hard-won experience and understanding has grown in large eddy simulation so that the methods can now successfully simulate benchmark turbulent flows efficiency and reliably. Scientific and industrial applications are systems of fluid flow equations coupled to other physical effects. These require even greater efficiency and accuracy than benchmark turbulent flow problems. To make progress into these industrial and scientific applications, large eddy simulation now requires systematic methods for assessing sensitivity and uncertainty as well as methods for interrogating the large amounts of data coming out of the finer meshes on faster computers with larger memories. These issues are the focus of the proposed research. Training PhD students to contribute to this important, difficult, interdisciplinary, fascinating and beautiful area is a large part of the proposed effort.<br/>"
"0810946","Analysis and implementation of accurate numerical boundary conditions for Large Eddy Simulations and Boltzmann equation","DMS","COMPUTATIONAL MATHEMATICS","09/01/2008","08/20/2008","Fang Hu","VA","Old Dominion University Research Foundation","Standard Grant","Junping Wang","08/31/2012","$146,000.00","","fhu@odu.edu","4111 MONARCH WAY","NORFOLK","VA","235082561","7576834293","MPS","1271","0000, 9263, OTHR","$0.00","Proposed research will improve the accuracy and efficiency of numerical simulations in unbounded domains. Investigations in two important areas of fluid dynamics are pursued. The first is the development of absorbing boundary conditions based on the Perfectly Matched Layer (PML) technique for Large Eddy Simulation (LES) of turbulent flows. Following recent successes of extending the Perfectly Matched Layer methodology to the nonlinear Euler and Navier-Stokes equations, further development of the PML technique for Large Eddy Simulation of turbulent flows is proposed. Formulations of absorbing equations for LES, as well as other turbulent flow simulation, such as the time-dependent Reynolds Averaged Navier-Stokes (RANS) simulations, are proposed. The second area of research is the development of non-reflecting boundary conditions for numerical schemes for the Boltzmann-BGK equation in gas kinetic theory. Proposed work will develop, analyze and implement the absorbing boundary condition based on the Perfectly Matched Layer methodology. Implementation and analysis of PML absorbing boundary condition in the Lattice Boltzmann Method will also be carried out in proposed research.<br/><br/>Due to the ubiquity of non-reflecting boundaries and the importance of Large Eddy Simulation in the computational studies of complex turbulent flows, proposed work will have a direct impact on the quality and efficiency of a broad class of numerical simulations in computational fluid dynamics and computational acoustics, such as in the reduction of airframe and jet noises, in studies of turbulent combustion in reactive flows, and in numerical models for weather predictions. The PML for the Boltzmann-BGK equation developed in proposed research is applicable to a diverse field of scientific investigations that employ the kinetic theory, such as multiphase and multi-component flows, microfluidics in nanotechnologies, particle suspensions and microflows in micro-electro-mechanical systems (MEMS).<br/><br/><br/><br/><br/>"
"0811086","Accurate digital representation and recovery for redundant frames","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/30/2008","Alexander Powell","TN","Vanderbilt University","Standard Grant","Leland Jameson","06/30/2012","$123,108.00","","alexander.m.powell@vanderbilt.edu","110 21ST AVE S","NASHVILLE","TN","372032416","6153222631","MPS","1271","0000, 9150, OTHR","$0.00","Redundant frame representations play a central role in signal processing applications.  By using overcomplete systems to represent signals, frames offer increased design flexibility and are thereby able to provide robustness against noise and data loss in many settings.  The investigator studies the mathematics of digitally representing redundant finite frame expansions, with an emphasis on two key steps:  coefficient quantization and signal reconstruction.  The work on quantization (the encoding step) focuses on the class of Sigma-Delta algorithms and studies rigorous approximation error bounds, dual frame methods for boosting performance, and the design of new algorithms for multiple description coding and orthogonal frequency division multiplexing.  Sigma-Delta algorithms are well suited for utilizing the correlations inherent in redundant collections of frame coefficients, and are desirable in practice since they can be robustly implemented using very coarse, for example one bit, scalar quantizers.  In the work on signal recovery (the decoding step), the investigator studies new nonlinear consistent reconstruction algorithms for analog-to-digital conversion in the settings of Pulse Code Modulation and Sigma-Delta quantization.  The investigator also studies how to extend noise-shaping and consistent reconstruction methods to distributed processing applications based on fusion frames.<br/><br/>Digital data is ubiquitous in modern technology.  The project addresses the general problem of how to accurately process, transmit, and recover digital signals in noisy environments.  The massive size of many digital data sets drives a need for new mathematical techniques that are capable of providing efficient and robust data processing algorithms.  The work on quantization of fusion frames applies to scenarios where a dense collection of low resolution sensors is deployed in a remote environment; the project studies methods for optimally communicating and extracting digital information through the sensor network.  The work on quantization of finite frame expansions is applicable to communication over erasure channels where physical constraints or other interference result in a loss of transmitted information; the project studies procedures for mitigating the effect of data erasures.<br/>"
"0810925","Collaborative Research: Efficient High Order Methods for Deterministic and Stochastic Problems in Flow Analysis and Control","DMS","COMPUTATIONAL MATHEMATICS, COFFES","08/01/2008","07/09/2008","David Kopriva","FL","Florida State University","Standard Grant","Junping Wang","07/31/2012","$251,767.00","Mohammed Hussaini","kopriva@math.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1271, 7552","0000, OTHR","$0.00","Efficient, high order, spectral element algorithms will be developed to   analyze and control large-scale deterministic and stochastic problems in unsteady compressible fluid flow. The focus will be on spatial, temporal, polynomial chaos, and adjoint based optimal control algorithms that can be used for active control of complex time-dependent flows, particularly those that include the generation and propagation of aerodynamic noise. For the time and space approximations, we will develop discontinuous Galerkin spatial  <br/>approximations and high order, optimized implicit/explicit (IMEX) methods that minimize phase and dissipation errors.  Adjoint based methods for flow control will be used, and control strategies developed for appropriate use with the discontinuous Galerkin approximation. Since uncertainty is present in real flows, we will include efficient stochastic collocation approximations. Finally, we will allow controls to be deterministic or stochastic, and develop strategies to control flows in the presence of uncertainty.<br/><br/>The class of problems addressed by these methods are of great practical importance and require very large, computationally intensive simulations with efficient and high order methods. The ability to control acoustic noise generation is important to a wide variety applications, from windmill farms to jet engines.  Problems of optimality under uncertainty occur frequently in a wide variety of problems in science, engineering and technology that have  <br/>probabilistic parameters, nondeterministic initial conditions, uncertain input situations, and models based on incomplete knowledge.  A large number of problems such as engineering design, supply allocation, production planning and scheduling, transportation, inventory networks, finance, energy systems, environmental protection, pattern recognition, and military logistics require that decisions be made in the presence of uncertainty. Uncertainty governs the prices of fuels, the availability of electricity, and the demand for chemicals. Much of life requires us to make optimal choices under uncertainty, i.e., to choose the optimum from some set of optional courses of action in uncertain situations. Clearly, the development of the mathematics and computational methods for optimal control with and without uncertainty has broad impacts both inside and outside mathematics."
"0811002","Experimental and Theoretical Approaches for Efficient Tree Distance Algorithms","DMS","COMPUTATIONAL MATHEMATICS, THEORETICAL FOUNDATIONS (TF)","07/01/2008","06/30/2008","Sean Cleary","NY","CUNY City College","Standard Grant","Junping Wang","06/30/2012","$120,989.00","","cleary@sci.ccny.cuny.edu","160 CONVENT AVE","NEW YORK","NY","100319101","2126505418","MPS","1271, 7351","0000, 9263, OTHR","$0.00","This project has involves research on several fronts in binary trees and algorithms for their efficient use. Rotations are small changes to binary tree structures used in balancing and optimizing search trees for efficient searching. The rotation distance between two trees is the minimal number of such small changes required to transform one tree into another. There are no known algorithms for computing rotation distances effectively, and even precise bounds on rotation distance are difficult to obtain.  This project develops methods for improving understanding of rotation distances and algorithms for computing or estimating them. This research involves experiments to understand the general properties of rotation distances, as well as developing abstract methods for describing rotation distances based upon connections between rotation distances and geometric methods in group theory.<br/><br/>This project seeks to improve  methods and understanding of binary trees and relevant algorithms. Binary trees are a fundamental structure, underlying efficient storage of data sets for quick retrieval of items. The amount of data routinely used in modern scientific and engineering settings is often staggeringly large. Biological data sets, for example, are often gigabytes of data.  When working with large data sets, the efficiency of methods used to analyze the data is of crucial importance- many questions which are immediate for small data sets are far beyond the capability of even today's most powerful computers for even moderate-sized data sets. Careful organization of large data sets is essential for productive investigation<br/><br/>"
"0813571","A  complete numerical analysis for finite volume methods to The Navier-Stokes equations","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","05/19/2010","Xiu Ye","AR","University of Arkansas Little Rock","Continuing Grant","Leland Jameson","06/30/2011","$204,622.00","","xxye@ualr.edu","2801 S UNIVERSITY AVE","LITTLE ROCK","AR","722041000","5015698474","MPS","1271","0000, 9150, 9263, OTHR","$0.00","Like the finite element method, the finite volume method is a <br/>discretization technique for solving partial differential equations. Due to the local conservation property and other attractive properties such as robustness with unstructured meshes, the finite volume method is widely used in computational fluid dynamics. Unlike finite element methods, numerical analysis for the finite volume methods is very limited. Only handful of papers analyzing the finite volume approximation for the Navier-Stokes equations can be found. The goal of the proposed research is to establish a complete numerical analysis for a class of the finite volume methods to the Navier-Stokes equations, including establishing stability of the methods; deriving optimal priori error estimates, posteriori error estimates and superconvergence of the solutions; and proving convergence of adaptive procedures. <br/><br/>Computational fluid dynamics covers wide range of problems from air flow around airplane to blood flow of human body. The mathematical foundations of any computational fluid dynamics problem are the Navier-Stokes equations.  The finite volume method is the classical numerical method for the problem used most often in commercial software and research codes. The objective of the project is to provide mathematical theory for the finite volume method."
"0811029","Analysis of Algorithms for Simulating Complex Materials","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","07/01/2008","05/18/2010","Noel Walkington","PA","Carnegie-Mellon University","Continuing Grant","Junping Wang","06/30/2011","$400,556.00","","noelw@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1266, 1271","0000, 9263, OTHR","$0.00","The focus of this project is the development and analysis of numerical<br/>algorithms to simulate materials which exhibit intricate rheological<br/>behavior or mechanical response due to their microstructural<br/>makeup. For example, elastic properties of particulates, molecules, or<br/>cells of a material frequently influence the macroscopic properties.<br/>These materials are modeled by formidable systems of partial<br/>differential equations, and it is important to develop numerical<br/>schemes to faithfully represent the mathematical structure of these<br/>models.  Tools from partial differential equations, continuum<br/>mechanics, and numerical analysis, will be used to analyze numerical<br/>schemes to simulate these systems. Past experience has shown that new<br/>mathematical tools can lead to numerical schemes which inherit<br/>important structural properties of these models, and that a deeper<br/>understanding of the current schemes frequently leads to improved and<br/>simpler algorithms.<br/><br/><br/>Essentially all biological and manufactured materials exhibit complex<br/>macroscopic behavior due to their fine scale makeup. Examples include<br/>micro electromechanical systems (MEMS); biological fluids; ink for ink<br/>jet devices; semiconductors; liquid crystals; and metals undergoing<br/>plastic deformation.  Predicting material response is an essential<br/>technology needed to determine biological or physiological function;<br/>or to design and manufacture these materials; or for the design of the<br/>multitudes of devices which use their special properties. This<br/>research will improve our understanding of the mathematical models and<br/>the computational tools used to accomplish these tasks.  The equations<br/>used to model such phenomena are complex and poorly understood, and<br/>much of the research is directed to revealing the theoretical<br/>(mathematical) properties of these models. This work complements the<br/>more practical approaches undertaken in the engineering community and<br/>at the national laboratories."
"0757207","FRG: Collaborative Research: Semidefinite optimization and convex algebraic geometry","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS","09/01/2008","08/06/2008","Pablo Parrilo","MA","Massachusetts Institute of Technology","Standard Grant","Junping Wang","08/31/2012","$240,000.00","","Parrilo@mit.edu","77 MASSACHUSETTS AVE","CAMBRIDGE","MA","021394301","6172531000","MPS","1264, 1271","0000, 1616, 9263, OTHR","$0.00","The goal in this proposal is to develop the mathematical foundations and<br/>associated computational methods for the study of convex sets in real<br/>algebraic geometry. This work requires a combination of ideas and<br/>mathematical tools from optimization, analysis, algebra and combinatorics.<br/>The proposed program will lead not only to theoretical insights, but also to<br/>new algorithms and software that will enable novel applications in<br/>mathematics, engineering, and beyond. The work is organized in five main<br/>thrusts: semidefinite programming and sums of squares, convex semi-algebraic<br/>sets, sparsity and graphical structure, numerical polynomial optimization<br/>and applications, and deformations and variation of parameters. The PIs will<br/>focus on the development of a comprehensive theory and practical new<br/>algorithms for convex sets defined by polynomial inequalities. Specific<br/>problems and techniques include the formulation of semidefinite descriptions<br/>of convex hulls of real algebraic varieties, determinantal representations<br/>of hyperbolic polynomials, sparse polynomials and their symmetries, tropical<br/>geometry and homotopy techniques, and geometric programming.<br/><br/>Many areas in mathematics, as well as applications in engineering, finance<br/>and the sciences, require a thorough understanding of convex sets. This is a<br/>class of geometric shapes, with several different but complementary<br/>interpretations. The goal in this project is to achieve a better<br/>understanding of how these geometric properties emerge from their algebraic<br/>descriptions in terms of polynomial equations, and the corresponding<br/>computational implications. One of the main motivations is the possibility<br/>of applying these results in the context of optimization. The proposed<br/>research will contribute to existing knowledge, both in algebraic-geometric<br/>techniques as well as in mathematical optimization. It will create synergies<br/>between different branches of applied mathematics, and their engineering and<br/>scientific applications (e.g., in computational biology and statistical<br/>modeling). Successful completion of this project should contribute to the<br/>availability of efficient and reliable computational tools for solving<br/>polynomial systems, which have clear technological and economic interest.<br/>Other key features of this proposal include its integration with curriculum<br/>development, undergraduate research projects, training of graduate students<br/>and postdocs, and the development of new software tools for computational<br/>optimization.<br/>"
"0811104","Absolutely Stable Time Domain Integral Equation Methods in Computational Electromagnetism","DMS","COMPUTATIONAL MATHEMATICS","09/01/2008","07/16/2010","Daniel Weile","DE","University of Delaware","Continuing Grant","Junping Wang","08/31/2011","$200,000.00","Peter Monk","weile@eecis.udel.edu","220 HULLIHEN HALL","NEWARK","DE","197160099","3028312136","MPS","1271","0000, 9150, 9263, OTHR","$0.00","Despite decades of advances in computational electromagnetics (CEM), some problems still elude efficient and accurate solutions.  Problems occurring in the analysis of microelectromechanial systems, electromagnetic compatibility, and ultra wide band antenna design involve complex geometry, small structures, and large propagation distances across homogeneous regions. Solutions to problems with these characteristics are most efficiently computed (in principle) by time-domain integral equation (TDIE) techniques. Unfortunately, while TDIEs have been researched for more than thirty years, there is still no TDIE scheme for CEM that  can efficiently model curvilinear geometry while ensuring stability. Thus, current techniques are either of low order of convergence or unreliable stability.  Mathematical error analysis, work estimates and computer programs created under this proposal will demonstrate a new method, based on convolution quadrature, that promises to deliver both superlinear accuracy and unconditional stability.<br/><br/>Electromagnetic theory governs the behavior of light, radio waves, and microwaves.    Understanding the behavior of electromagnetic waves is of the utmost concern to national security (in the design of radar and surveillance systems and protecting them from interference), energy systems (in the collection and distribution of electric power), and the economy (in cell phones and television broadcasts).  Unfortunately, as electromagnetic systems become more complicated and powerful, they become harder to simulate and therefore design.  For instance, modern problems in shielding devices from electromagnetic interference involve broad bandwidths, long propagation distances, and small features.  While it has been known for decades that this combination of difficulties can be mitigated using time domain integral equation formulations, no such formulation has been produced that can accurately model arbitrary geometry without producing a solution with exponentially increasing error.  This proposal produces such a scheme by introducing a new method for modeling the time dependence of the solution."
"0844497","SGER: Collaborative Research: Non-negative Matrix Factorizations for Data Mining: Algorithms and Applications","DMS","COMPUTATIONAL MATHEMATICS","09/15/2008","09/16/2008","Chris Ding","TX","University of Texas at Arlington","Standard Grant","Junping Wang","08/31/2009","$56,000.00","","chqding@uta.edu","701 S. NEDDERMAN DR","ARLINGTON","TX","760199800","8172722105","MPS","1271","0000, 9237, 9263, OTHR","$0.00","Nonnegative matrix factorization (NMF) factorizes an input nonnegative matrix into two nonnegative matrices of lower rank.  It is recently discovered that NMF in the most basic form is equivalent to a relaxed K-means clustering, the most widely used pattern discovery algorithm in data mining.  This direct link between mathematics and data mining sets in motion a large number of developments on using matrix factorizations for pattern discovery.  It turns out that NMF provides more consistent and mathematically well-defined optimization formulations for many fundamental and emerging data-mining problems.  NMF algorithms have well-understood properties; they are simple and easy-to-implement, well suited for distributed parallel architectures.  This research aims to formally establish a comprehensive NMF-based framework for data mining.  In particular, we will (1) extend matrix factorization data-mining methodology from current focus on clustering (pattern discovery) to newer problems: semi-supervised clustering (extending partial knowledge to whole data) and classifications (pattern prediction, such as predicting a cancer tumor tissue from a normal one); (2) develop fast numerical algorithms and incorporate state-of-the-art numerical optimization techniques; and (3) apply and evaluate the NMF algorithms in different real-world applications including text mining and bioinformatics."
"0811130","An Optimal Time Stepping Method for Computational Science Applications","DMS","COMPUTATIONAL MATHEMATICS, OPPORTUNITIES FOR RESEARCH CMG, MATHEMATICAL GEOSCIENCES","09/01/2008","08/19/2008","Jingfang Huang","NC","University of North Carolina at Chapel Hill","Standard Grant","Junping Wang","08/31/2012","$277,789.00","","huang@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1271, 7215, 7232","0000, 7303, 9263, OTHR","$0.00","The focus of this proposal is on the mathematical analysis and efficient <br/>implementation of a new class of Krylov deferred correction accelerated <br/>""method of lines transpose"" for time dependent PDE's. The method first <br/>discretizes the temporal direction using Gaussian type nodes and spectral <br/>integration, and the resulting coupled elliptic equations are preconditioned <br/>using deferred corrections, in which each correction procedure only <br/>requires the solution of a decoupled system using available fast elliptic <br/>equation solvers. The preconditioned nonlinear system is then solved <br/>efficiently using iterative Newton-Krylov techniques. Preliminary numerical <br/>experiments show that this method is unconditionally stable, very efficient, <br/>and can achieve arbitrary order of accuracy in both time and space. In <br/>particular, no CFL constraints have been observed and the time step size <br/>only depends on the smoothness of the solution and hence is ""optimal"".  <br/>Highlights of the PI's preliminary results include (a) a time domain <br/>Maxwell equation solver which provides accurate results for a long-time <br/>electromagnetic wave simulation problem for which most existing time <br/>integration schemes fail; and (b) a symplectic Schrodinger equation solver <br/>which preserves the structure of a Hamiltonian system with singular <br/>potential while most existing numerical techniques quickly blow up. <br/><br/>It is well known that inaccurate numerical algorithms have caused many <br/>costly project failures, examples include the sinking of the Sleipner A <br/>offshore platform in Gandsfjorden near Stavanger, Norway, on August <br/>23, 1991, which was due to inaccurate finite element analysis and resulted <br/>in a loss of nearly one billion dollars. The purpose of this proposal is to <br/>use advanced mathematical analysis and develop numerical techniques that<br/>can efficiently provide accurate and stable numerical simulation results <br/>to important science and engineering problems. In particular, the PI <br/>will study and implement a novel class of numerical algorithms for <br/>time dependent problems modeled by partial differential equations.<br/>The success of this project will bring new tools and techniques to a <br/>wide class of applications in science and engineering that are impossible<br/>to solve efficiently and accurately using existing techniques, examples <br/>including the design of optimal drug structures in biochemistry, the study<br/>of cosmos structure in astrophysics, and improved understanding of the <br/>physics that govern hydrologic processes in Earth system science. <br/>This project also focuses on the training of a new generation of scientists <br/>capable of developing advanced numerical tools using sophisticated <br/>mathematical theory.<br/>"
"0758242","Eighteenth International Symposium on the Mathematical Theory of Networks and Systems - MTNS 2008, July 28 - August 1, 2008, Blacksburg, VA","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, ANALYSIS PROGRAM","07/01/2008","06/20/2008","Christopher Beattie","VA","Virginia Polytechnic Institute and State University","Standard Grant","Henry Warchall","06/30/2009","$12,600.00","Joseph Ball, Serkan Gugercin, Craig Woolsey, Reinhard Laubenbacher","beattie@math.vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1266, 1271, 1281","0000, 7556, OTHR","$0.00","The International Symposium on the Mathematical Theory of Networks and <br/>Systems (MTNS) has met biennially since 1973 in ten countries spread across <br/>North America, Europe and Asia as a forum for a wide range of topics in mathematical system theory, networks and control theory.  The primary objective of MTNS is to bring together established and new researchers, including advanced graduate students, for the exchange of ideas and the discussion of recent developments in the general area at the border of engineering and mathematics encompassed within the title ?mathematical system theory?.  These areas of focus are of critical importance in the analysis and control of the behavior of complex systems as manifested in nature, science, and industry.  The MTNS meetings provide an opportunity for the global <br/>community of established and beginning researchers in this area to meet with and learn from one another through both formal presentations and informal interactions. <br/><br/>The next MTNS Symposium will be held July 28?August 1, 2008 at Virginia Tech <br/>in Blacksburg, Virginia.  This occasion provides a unique opportunity for US graduate students, young PhDs, and others that may have restricted resources to make contact with a vibrant international community of researchers active in research areas related to systems theory.  The objective of this proposal is to secure funding for the financial support of advanced graduate students, recent PhDs,  and members of traditionally under-represented groups (including in particular, women and members of minority groups) so that they might have the advantage of participating in a major international conference which comes to the United States only once or twice a decade. For many of the prospective beneficiaries, this will be a first opportunity to engage in professional <br/>activity beyond their home institutions. For all of them it will be a unique opportunity to present their work to an expert audience and to further their knowledge of the field by interacting with established researchers in both engineering and mathematics - interactions that are crucial to their development as young researchers and educators. <br/>"
"0809262","Discontinuous Galerkin Methods for Partial Differential Equations:","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/25/2008","Slimane Adjerid","VA","Virginia Polytechnic Institute and State University","Standard Grant","Junping Wang","06/30/2012","$179,999.00","","adjerids@math.vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1271","0000, 9263, OTHR","$0.00","Discontinuous Galerkin (DG) methods are becoming important techniques for the computational solution of partial differential equations.  With discontinuous finite element bases, they capture discontinuities in, {\it e.g.}, hyperbolic systems with high accuracy and efficiency; simplify adaptive mesh refinement, order-variation and lead to efficient parallel solution procedures.<br/>The PI proposes to study the superconvergence properties of DG solutions of hyperbolic systems and local discontinuous Galerkin<br/>(LDG) solutions of convection-diffusion and higher-order problems in one and multiple space dimensions. The PI will investigate several aspects of the superconvergence phenomena, including the effects of numerical fluxes, stabilization schemes, mesh structure, and order variation on superconvergence properties.  The PI will use superconvergence properties to construct simple and asymptotically exact {\it a posteriori} estimates of discretization errors and very accurate functions of interest. Both of these provide valuable accuracy appraisals and guidance for an adaptive solution strategy.<br/>Stabilization or limiting is necessary with high-order DG and LDG methods to remove spurious oscillations near discontinuities and sharp transition layers.  The PI will investigate several stabilization strategies based on residual dissipation, solution moments, and ENO schemes with a goal of discovering those that provide optimal performance in specified circumstances. Furthermore, superconvergence properties can be used to locate discontinuities and sharp transitions and, as such, provide the possibility of developing adaptive stabilization techniques that need only be applied where necessary to avoid oscillations and loss of accuracy caused by unnecessary stabilization in smooth solution regions.<br/><br/>Computer simulations of complex and realistic problems from a variety of disciplines still require a very long time on the fastest available computers. Efficient, reliable and accurate discontinuous Galerkin methods for partial differential equations will be developed by the investigator and implemented in numerical software that can be used to address large-scale problems arising in many critical areas such as energy and environment. The reliability provided through {\it a posteriori} error estimation, will additionally enable advanced adaptive software to be used in educational settings to help students understand delicate and intricate phenomena and prepare the next generation of scientists.<br/>"
"0811169","Computational Harmonic Analysis in Information Theory, Signal Processing, and Data Analysis","DMS","COMPUTATIONAL MATHEMATICS","09/15/2008","07/27/2010","Thomas Strohmer","CA","University of California-Davis","Continuing Grant","Leland Jameson","08/31/2012","$419,984.00","","strohmer@math.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1271","0000, 7721, 7752, 9263, OTHR","$0.00","In this research effort the investigator creates mathematical concepts and numerical methods for information technology, communications engineering, and data analysis. The investigator uses tools from pseudodifferential operator theory, time-frequency analysis, random matrix theory, and Banach algebra theory, yielding efficient numerical algorithms with rigorously-established properties under carefully stated conditions.<br/>Some concrete topics of this research effort are: (i) Development of a theoretical framework for key problems in classical and quantum information theory. Specifically, the investigator considers the channel capacity problem in time-varying communications and quantum communications;<br/>(ii) Sparse representations and compressed sensing in X-ray crystallography, communications, and radar. Initial steps toward building a framework for nonlinear compressed sensing; (iii) Noncommutative harmonic analysis and pseudodifferential operators from the point of view of computational efficiency and the development of fast algorithms.<br/>Particular attention is paid to spectral factorization for operators in a noncommutative setting, and their application in signal processing and wireless communications.<br/>Strong expectation for success of this project can be based on existing solid achievements by the investigator in each of the described areas.<br/><br/><br/>The research proposed in this effort is a marriage of several areas of cutting edge mathematics with state-of-the-art engineering, seeking to bring advanced techniques from abstract and applied harmonic analysis to communications engineering, signal processing, and data analysis in form of fast and efficient computational methods.<br/>By taking the modern harmonic analysis methodology into the engineering community this research activity will enable further advances and breakthroughs in important applications.<br/>At the same time it will stimulate new research areas in applied mathematics and pave the road for further interactions between applied mathematicians and engineers.<br/>The payoffs of this research effort for society at large are many, ranging from new information technology capabilities and sophisticated tools to deal with today's massive volumes of data.<br/>There are financial efficiencies to be gained by communications providers which will be accompanied by better and increased communications services for the public. Other potential benefits include improved methods for medical imaging and biomedical engineering.<br/>Beyond the project's broad technological impact, it serves as a model for the kind of cross-disciplinary activity critical for research and education at the mathematics/engineering frontier. Hence this research effort helps to train graduate students in mathematics to develop and enhance skills that are crucial and urgently needed in a high-tech oriented society.<br/>"
"0810891","COLLABORATIVE PROPOSAL: Enhanced Least-Squares Methods for PIV Analysis","DMS","COMPUTATIONAL MATHEMATICS","09/15/2008","09/16/2008","Jeffrey Heys","MT","Montana State University","Standard Grant","Junping Wang","08/31/2012","$122,094.00","","jheys@asu.edu","216 MONTANA HALL","BOZEMAN","MT","59717","4069942381","MPS","1271","0000, 9150, 9263, OTHR","$0.00","Particle Image Velocimetry (PIV) is a method for obtaining a fluid velocity field based on the translation of particles between images with a known time span between them.  A potential limitation of PIV is that only two-dimensional velocity field along a single plane can be obtained from a two-dimensional image.  This limitation is normally overcome by designing the experimental flow system so that the third velocity component is either zero or unimportant.  In many applications, however, it is not possible to simplify the fully three-dimensional velocity field (e.g., in the left ventricle of the heart), and the two-dimensional limitations associated with PIV analysis are a significant problem that must be overcome.  Would it be possible, however, to combine the two-dimensional PIV data together with a fully three-dimensional numerical approximation to the Navier-Stokes equations and obtain a sufficiently accurate three-dimensional velocity field in a domain such as the left ventricle of the heart?  The use of least-squares finite element methods (LSFEMs) is proposed here to approximately solve the Navier-Stokes equations, and, significantly, to weakly constrain the solution along the PIV plane to match the experimental data.  The PIV data would basically act as an internal boundary, and the numerical solution would weakly match the data with a variable weighting that determines the strength of the coupling between the data and numerical solution.  LSFEMs are uniquely well suited for solving an over-constrained problem like this in a computationally efficient manner.<br/><br/>Echocardiologists have developed methods for introducing microbubbles into circulating blood that can be resolved using ultrasound.  The location of the microbubbles combined with the high temporal resolution of ultrasound allows the local blood velocity to be determined using Particle Image Velocimetry, but the high temporal resolution requirement also limits the ultrasound scans (and, hence, the velocity field data) to two dimensions.  One goal of using the FDA approved microbubbles is to use the blood velocity data to calculate physiologically important information such as pressure gradients and energy loss for the blood flow, but these calculations require a full three-dimensional velocity field.  The goal of the proposed research is to develop mathematical techniques that combine computational fluid dynamics with experimental velocity data, such as that from microbubbles, to obtain a full, three-dimensional velocity field, thus providing greater insight into the dynamics of the flow.<br/>"
"0802444","Student and early career support for ISSNLA, July 20-25, 2008, Castro Urdiales, Spain","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/20/2008","Daniel Szyld","PA","Temple University","Standard Grant","Junping Wang","06/30/2009","$13,000.00","Ilse C.F. Ipsen","szyld@temple.edu","1801 N BROAD ST","PHILADELPHIA","PA","191226003","2157077547","MPS","1271","0000, 7556, 9263, OTHR","$0.00","This grant will support the participation of US-based Ph.D.  students at the International Summer School on Numerical Linear Algebra (ISSNLA). The ISSNLA will take place at the International Center of Mathematical Meetings (CIEM) in Castro Urdiales, Spain on 20-25 July 2008. The ISSNLA is organized by the SIAM Activity Group on Linear Algebra (SIAG/LA).  The local organization is assumed by SIMUMAT (the Spanish initiative from the Comunidad de Madrid on ``Mathematical Modeling and Numerical Simulation in Science and Technology""). The target audience of the ISSNLA are doctoral students in any field that requires results and methods from Numerical Linear Algebra.  The courses will be self contained, and taught at a level that is accessible to a wide audience. They will be of interest to graduate students and recent Ph.D.'s in science and engineering. The lecturers are leading experts who are also well known for their expository skills.  They were chosen by an Advisory Committee composed by leading international scientists.<br/><br/>The purpose of the ""International Summer School on Numerical Linear Algebra"" is to acquaint doctoral students with recent developments and novel applications in a subfield of Applied Mathematics called ""Numerical Linear Algebra"". By design, the summer school is forward-looking: the courses cover topics that are too recent to have been included in textbooks or doctoral courses; these topics include applications to information retrieval and mechanics. The target audience are doctoral students in science and engineering. NSF support for US-based participants will have a positive impact on the continued strong competitiveness of the US in this crucial discipline, with its connections to virtually every area of scientific computing.  <br/>"
"0746676","CAREER:  Multilevel Discontinuous Least-Squares Finite Element Methods","DMS","COMPUTATIONAL MATHEMATICS","05/15/2008","06/21/2012","Luke Olson","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Junping Wang","10/31/2014","$400,000.00","","lukeo@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1271","0000, 1045, 1187, 9263, OTHR","$0.00","For the numerical approximation of partial differential equations (PDEs), a balance is sought between the approximation properties (accuracy, consistency, stability, etc.), the solution time (solver speed, implementation efficiency), and robustness (scalability and applicability).  To this end, the principle goal in this proposal is to develop a high-order discretization framework amenable to fast solution techniques in a multilevel setting.  The application focus of the proposal is motivated by two core problems: neutrophil chemotaxis in the blood stream and cellular mechanics in microcirculation.  Principally, these models are governed by coupled anisotropic diffusion-convection-reaction equations and coupled Stokes equations.  The efficient and effective numerical solutions of these complex equations is central to the proposed work.  This includes the development of an effective discontinuous least-squares spectral element method, a comparison with popular strategies such as discontinuous Galerkin, and the development of an integrated algebraic multigrid preconditioner for use with high-order spectral elements in this situation.  Ultimately, this work establishes a theoretical and computation base for further research in discontinuous least-squares methods and high-order preconditioning.  Moreover, an intrinsic element of this project is  the integration of new methods in numerical PDEs and iterative methods into the existing scientific computing curriculum to help train future computational scientists.<br/><br/>As physical models grow in complexity and high-performance computing environments grow in speed, so do the demands on the underlying mathematical algorithms.  The goal of this project is to make progress toward a more generalized mathematical framework that encompasses more layers of the entire simulation tool chain.  Large-scale computational analysis is a critical experimental component in many areas of the physical sciences and yet, computational scientists are limited in their tool set.  Olson proposes to develop a multilevel approximation method for core applications, such as cellular behavior in the blood stream, and to expand wider adaptation of new methods in the field through outreach and education. The proposed research methodology promotes conformity with the physics of the problem, allows for a natural and extensible computational implementation, and yields an accurate and efficient solution.  This project will develop new steps for the multilevel methodology, disseminate the computational tools to the broader scientific and computing community, and train students and scientists on using these emerging computational technologies.<br/>"
"0810602","The optimization and control of flexible propulsors in inviscid fluids","DMS","COMPUTATIONAL MATHEMATICS","08/15/2008","08/11/2008","Silas Alben","GA","Georgia Tech Research Corporation","Standard Grant","Junping Wang","07/31/2012","$152,000.00","","alben@umich.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1271","0000, 9263, OTHR","$0.00","The investigator will study five fundamental and interrelated problems dealing with the motions of flapping appendages in inviscid fluids. The first project is a study of the large-amplitude flapping of a rigid plate which sheds vortex sheets into an oncoming fluid. This study will build on the method developed by the proposer for computing the coupled motions of vortex sheets and a flexible flag to consider leading-edge vortices and their interactions with a body and with trailing-edge vortices.  The second problem is an improved understanding of how to approximate the spatial complexity of vortex sheets computationally, in settings of vortex-vortex and vortex-body interactions. These approximations will consist of truncations of the multipole expansion of the velocity field induced by vortex sheets. The third project is a study of the optimal flexibility for propulsion, in simple situations modelling two-dimensional motions of fish fins. The work ranges from analysis of linearized solutions to computational solutions of the full system of nonlinear singular integrodifferential equations. The fourth problem considers the mechanics of flexible fins in more detail, through a constrained optimization of a fin ray model. The constraints will be chosen with both the relevant scientific questions and the well-posedness of the optimization problem in mind.  The fifth project considers how to control and stabilize the fin ray using our understanding of the mechanics underlying actual fish fins. This effort requires finding optimal target trajectories when the fin stiffness is varied periodically in time, and stabilizing the target trajectories against undesired excitations from body-vortex coupling.<br/><br/>The project will develop methods to answer questions of broad biological interest about swimming with fish fins and other flexible locomotory structures. The project will also improve the quantitative understanding of strategies for propulsion which can be readily assimilated into theoretical engineering fields concerning how solids and fluids interact. Applications to propulsion technology include improving the efficiency and maneuverability of submersible vehicles.<br/>"
"0810948","Collaborative Research: A Software System for Algebraic Geometry Research","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS, NUMERIC, SYMBOLIC & GEO COMPUT","09/15/2008","06/04/2009","Daniel Grayson","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Junping Wang","01/31/2011","$139,999.00","","danielrichardgrayson@gmail.com","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1264, 1271, 2865","0000, 9263, OTHR","$0.00","Macaulay2 is a free computer algebra system dedicated to the qualitative <br/>investigation of systems of polynomial equations in many variables. It <br/>was developed by Daniel Grayson and Michael Stillman with NSF funding. <br/>Grayson and Stillman will continue the development of Macaulay2. They <br/>will upgrade existing algorithms, develop and publish new algorithms, and <br/>implement new algorithms. In particular, they will develop the interaction <br/>of the symbolic computations that are Macaulay2's strength with the new <br/>floating point algorithms in algebraic geometry that are now being <br/>developed by Andrew Sommese, Jan Verschelde, Anton Leykin, Frank Schreyer <br/>and others. It is anticipated that these will make a whole new class of <br/>problems accessible to experimentation and, in many cases, solution. <br/>Eisenbud will organize contacts for the extended integration with other <br/>systems and will engage other mathematicians in the development work that <br/>needs to be done. Central to the project are the continued expansion of <br/>the collaborations that have been the hallmark of Macaulay2 development. <br/>For this purpose two Macaulay2 Workgroup Meetings will be held in the <br/>course of the two-year grant. One particular research problem to be <br/>attacked is: the use of computational systems to (probabilistically) <br/>disprove, or suggest a proof of, the Jacobian Conjecture on polynomial <br/>automorphisms of affine spaces (this will require the use the new floating <br/>point algorithms). Other areas where new algorithms can make an impact <br/>include the study of numerical systems, fractions with specified types of <br/>denominators, ideal factorization, systems where the multiplication of the <br/>variables doesn't satisfy the commutative law, geometric optimization, the <br/>analysis of observations of gene expression levels over time, and <br/>bioinformatics. <br/><br/><br/>Macaulay2 is part of the infrastructure that supports mathematical <br/>research involving systems of polynomial equations in many variables. The <br/>study of such systems of polynomial equations is central in pure and <br/>applied mathematics and in physics, with recent new impacts in such fields <br/>as cryptography, robotics and string theory. Increasing computer power <br/>and the availability of programs like Macaulay2 are making a new level of <br/>experimentation possible. The experimental results found with Macaulay2 <br/>are helping in the formulation and development of tractable conjectures in <br/>mathematics as well as in physics. A measure of Macaulay2's impact is that <br/>at least 270 research papers have cited Macaulay2, several mathematicians <br/>have contributed code, and books and course materials are now using it. <br/>The PI's will develop the software further and will recruit developers <br/>from the research community. They will introduce graduate students and <br/>mathematicians to the use of computers in research mathematics and the <br/>requisite skills in programming and development of algorithms, through <br/>workgroups at Berkeley and through the appointments of graduate students <br/>as graduate assistants."
"0810913","Further Study of Hierarchical Reconstruction Algorithms","DMS","COMPUTATIONAL MATHEMATICS","08/15/2008","08/14/2008","Yingjie Liu","GA","Georgia Tech Research Corporation","Standard Grant","Leland Jameson","07/31/2012","$167,780.00","","yingjie@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1271","0000, 9263, OTHR","$0.00","High resolution capturing schemes for solving conservation laws, e.g., the ENO scheme, smear discontinuities within a few mesh cells and achieve high accuracy where the solution is smooth. A series of works by Cockburn and Shu et al. on discontinuous Galerkin (DG) methods and local DG introduce many new techniques to the DG family and enable it to solve a broader class of equations including conservation laws. Still, the limiting technique for DG is not very mature and is considered to be one of the major open problems in scientific computing. The investigator and his colleagues propose the further study of a new limiting technique, the hierarchical reconstruction (HR). It is a general reconstruction procedure used as a limiter to remove spurious oscillations in the presence of shocks.  The HR algorithm, motivated by the moment limiter of Biswas, Devine and Flaherty (1994), involves only a MUSCL, a second order ENO or other piecewise linear reconstructions in each stage of a multi-layer reconstruction process without characteristic decomposition. Therefore it is compact and easy to implement for arbitrary meshes. It does not truncate higher degree terms of a polynomial and actually uses the information from all degree terms. It has been proved that HR does not reduce the approximation order of a polynomial. Moreover, HR can be used for finite volume and central schemes as well without characteristic decomposition, which leads to a new finite volume approach. The investigator and his colleagues also propose the study of the local constant velocity version of the back and forth error compensation and correction method (BFECC) for velocity advections in multi-phase fluid simulation, BFECC for moving meshes and for interpolation between grids. <br/>Adapting HR to BFECC wherever necessary could significantly improve the robustness of BFECC for non-smooth solution.<br/><br/>This project is on the study and development of new methods for using computers to simulate certain natural phenomena such as airflow passing a wing, shock waves propagating in a body, smokes etc. Computer simulations help scientists and engineers testing various experimental configurations and product designs without conducting costly experiments.<br/>Nowadays a lot of special effects in Hollywood movies are made by computer simulation. The BFECC method co-developed by the principal investigator has been used by NVIDIA for smoke simulation, http://developer.download.nvidia.com/SDK/10/direct3d/Source/Smoke/doc/Smoke.wmv.<br/>However, computer simulation is a noisy process. Noises constantly come from machine errors, and from the non-smoothness of simulated objects, such as shocks, corners of boundaries, interfaces separating different fluids or tissues in a body etc. Without special techniques, simulation noises caused by shocks can easily destroy a simulation result. In fact, two of the fundamental challenges for developing computational methods are to reduce simulation time and noises from the non-smoothness of simulated objects. The investigator and his colleagues study a new method for removing noise, which is easier to use for complex geometry and less dependent on simulated objects. Preliminary results for simulations of shocks are encouraging. The new idea could be adapted to many other areas and motivate the development of improved computational methods. For example, it could allow a complicated aircraft shape to be simulated more easily, motivate more robust techniques to stabilize simulations of multi-phase fluids, fuel cells etc and provide a black-box de-noising tool for simulations whose underlying physics are more empirical.<br/>"
"0807574","Geometry of state space in plane Couette flow","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS, OPPORTUNITIES FOR RESEARCH CMG","09/01/2008","08/22/2008","Predrag Cvitanovic","GA","Georgia Tech Research Corporation","Standard Grant","Henry Warchall","08/31/2012","$250,000.00","","predrag.cvitanovic@physics.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1266, 1271, 7215","0000, 7303, 9263, OTHR","$0.00","A large conceptual gap separates the theory of low-dimensional chaotic dynamics from the infinite-dimensional nonlinear dynamics of turbulence. Recent advances in experimental imaging, computational methods, and dynamical systems theory suggest a way to bridge this gap and make a fundamental breakthrough in our understanding of turbulence. It has recently been discovered that recurrent coherent structures observed in wall-bounded shear flows (such as pipes and boundary layers) result from close passes to weakly unstable invariant solutions of the Navier-Stokes equations. These 3D, fully nonlinear solutions (equilibria, traveling waves, and periodic orbits) structure the state space of turbulent flows and provide a skeleton for analyzing their dynamics. We propose to calculate a hierarchy of invariant solutions for a canonical wall-bounded shear flow and to use these solutions (1) to develop a quantitative description of the flow's turbulent dynamics, and (2) to predict, directly from the fundamental equations, physical quantities such as bulk flow rate and mean wall drag. We will use a combination of novel and proven numerical and analytical techniques, such as periodic orbit theory, group representation theory, nonlinear search methods and variational solvers, and computational fluid dynamics. The proposed research will be conducted with collaborators in Japan, Germany, the UK, and the US, and all results and numerical software will be disseminated through through our group's collaborative e-book www.ChaosBook.org and open-source CFD software and invariant solution database www.Channelflow.org.<br/><br/>Turbulence is not only the great unsolved fundamental problem of classical physics, but a problem of great practical importance. Any progress in our fundamental understanding of turbulence impacts technology and engineering. Applications range from the suppression of plasma instabilities in magnetic confinement fusion reactors to the key challenge of reducing turbulent drag. Drag is responsible for a significant part of the fuel consumed in automotive, aviation and shipping industry as well as in transport of fluids. Even an incremental reduction of drag by improving flow control methodology would have a significant economic impact across a broad range of industries.<br/>"
"0811005","Efficient spectrally accurate global basis methods for high frequency wave scattering, chaotic eigenmodes, and photonics","DMS","COMPUTATIONAL MATHEMATICS","09/15/2008","07/27/2010","Alexander Barnett","NH","Dartmouth College","Continuing Grant","Leland Jameson","08/31/2011","$310,517.00","","ahb@math.dartmouth.edu","7 LEBANON ST","HANOVER","NH","037552170","6036463007","MPS","1271","0000, 9150, 9263, OTHR","$0.00","Accurate and rapid numerical solution of the Helmholtz and related partial differential equations in complex geometries is key to future progress in device design, in imaging, and in basic science.  However, at high frequencies (many wavelengths across the system) this becomes prohibitively challenging using direct discretization, due to the multiscale nature of the problem.  The investigator seeks to build upon boundary-based methods which have been uniquely successful (up to a thousand times faster than the competition) in solving eigenmode problems hundreds of wavelength in size with spectral accuracy in two dimensions, and to extend them to the scattering problem, to more general media and periodic boundary conditions, and to three dimensions.  These methods are global approximation by particular solution basis sets, and the scaling method for Dirichlet eigenmodes.<br/>Proposed extensions include: 1) use of fundamental solutions basis sets, and their analysis via the role of singularities in the analytic continuation of the wave field, 2) exploiting a little-known analytic formula for the fundamental solution in linear graded-index materials, enabling non-piecewise-constant media to be solved on the boundary, 3) error analysis of a reformulation of the scaling method via the Dirichlet-to-Neumann map for the domain, 4) application of such methods to the spectrally accurate solution of dielectric photonic crystal band structure, and to `quantum chaos' (the wave and spectral properties of cavities with ergodic ray dynamics).<br/><br/>The impact of our technology such as radar, microwave communication (eg cellphones), optics and lasers, acoustics, medical ultrasound imaging, and miniaturized quantum devices has been, and will continue to be, profound and far-reaching.  To design all such devices, one must calculate how they will reflect, guide and trap waves, and this is a time-intensive, difficult and sometimes unreliable computation.<br/>The computer algorithms proposed by the investigator will make such calculations faster and more accurate, particularly when the objects are large or complicated in shape.  This is expected to lead to improvements in the design of, for example, optical signal-processing devices (which rely on microscopic periodic structures the size of the wavelength of light), promising candidates for the next generation of fast (post-silicon) computers.  A deeper grasp of quantum chaos (the behavior of waves trapped in cavities which cause chaotic bouncing of<br/>rays) would impact nanoscale quantum wave devices such as quantum dots, super-fast quantum computers, as well as areas of pure mathematics and physics theory.  The proposal also provides training in applied and computational mathematics at both graduate and undergraduate levels, and a course on the ``Mathematics of Music and Sound'' introducing non-majors to waves, modes, and resonance.<br/><br/>"
"0748839","CAREER: Optimizations for Sparse Solutions and Applications","DMS","COMPUTATIONAL MATHEMATICS","05/15/2008","04/30/2012","Wotao Yin","TX","William Marsh Rice University","Continuing Grant","Junping Wang","12/31/2013","$405,754.00","","wotaoyin@math.ucla.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271","0000, 1045, 9263, OTHR","$0.00","In many areas such as signal processing, control, statistics, learning, inverse problems, and management, ""large"" data sets are often processed to find ""small"" solutions, those depending ultimately upon a small number of factors. Since these solutions tend to be sparse in a way, it is possible for methods that pick out the sparse solutions to find them from a reduced number of indirect measurements compared to what are usually considered necessary. This is the emerging technology of compressed sensing (CS). In this research, the PI proposes to study a broad range of issues and techniques to advance CS. His proposed reseach includes the introduction of new methodolgies for exploiting solution sparsity to accelerate CS computation, the development of algorithms that utilize operations requiring low storage and maintain robustness to noise and errors in data, and the discovery of efficient methods for minimizing the l1-norms of wide classes of functions such as first and higher-order differences. This project will include an integrated educational program involving a new course, one Ph.D. student, and the participation in the Rice-Houston AGEP program in producing competitive women and minority graduate students.<br/><br/><br/>The new emerging technology of ""compressed sensing"" is a complement to traditional data compression. While the traditional technology encodes digital data using fewer bits in order to save storage and transmission time, the new technology can significantly reduce the time, energy, and cost associated with the acquisition of digital data. This is achieved by acquiring digit information of an object of interest from a reduced number of obervations than what is usually necessary. For example, the life of an aeriel such as a space telescope can be greatly extended due to a lower sampling rate (and thus a lower power demand). Hyperspectral and infrared imaging devices can produce the same images with smaller sensors, or if with the same sensors, images at higher resolution. As such, the new technology can lead to breakthroughs for applications where the bottleneck lies in the high cost of data acquisition.<br/>"
"0809068","Reconfiguration and Rigidity","DMS","COMPUTATIONAL MATHEMATICS","09/01/2008","07/24/2010","Robert Connelly","NY","Cornell University","Continuing Grant","Junping Wang","08/31/2012","$312,000.00","","connelly@math.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1271","0000, 9263, OTHR","$0.00","Geometric and computational problems involving points and objects of various shapes are basic to the understanding of geometric modeling, computer graphics, the analysis and synthesis of mechanisms, robot manipulation, structural analysis, protein folding and granular materials, for example.  What are the geometric principles that are relevant to understanding how configurations of linkages in the plane can be reconfigured?  What are the basic geometric tools that can be applied to understand and analyze the area/volume of various arrangements and formulas for the union and intersections of circular disks in the plane or space?  Recent advances have shown that there are some subtle and insightful ideas that can be brought to bear on these very basic problems that include packing and covering problems as extreme special cases.  <br/><br/>Suppose a configuration of points in the plane or space is given.  The distance between some (but not all) of the pairs of these points is fixed.  Is there another corresponding configuration, other than congruent copies of the original configuration, of the points with corresponding distances the same?  When this occurs, the configuration is called globally rigid.  When the configuration is sufficiently generic, there is a reasonable algorithm to determine if it is globally rigid.  This is of interest in protein folding, point location problems and structural stability, for example.  It is proposed that this theory and corresponding algorithms can be used to provide structural information about the shapes of molecules as well as the stability of a wide variety of novel structures.<br/>"
"0811106","Second-order methods for large-scale optimization in compressed sensing","DMS","COMPUTATIONAL MATHEMATICS","07/15/2008","04/05/2010","Jennifer Erway","NC","Wake Forest University","Continuing Grant","Leland Jameson","06/30/2012","$144,949.00","","erwayjb@wfu.edu","1834 Wake Forest Road","Winston Salem","NC","271098758","3367585888","MPS","1271","0000, 9263, OTHR","$0.00","Nonlinear image reconstruction based upon sparse representations of signals and images has received widespread attention recently with the advent of compressed sensing. This emerging theory indicates that, when feasible, judicious selection of the type of distortion induced by measurement systems may lead to dramatically improved inverse problem solutions. In particular, when the signal or image of interest is very sparse (i.e., zero-valued at most locations) or highly compressible in some basis, relatively few indirect observations are necessary to reconstruct the most significant non-zero signal components. Compressed sensing theory states that sparse signals can be recovered very accurately with high probability from indirect measurements by solving an appropriate optimization problem. This research aims at the development of significantly more efficient methods for solving compressed sensing minimization problems. A crucial property of the proposed methods is that the algorithms are designed to be ""matrix-free"", i.e., they do not require the storage of (potentially very large) second-derivative matrices. Instead, these methods use matrices only as operators for matrix-vector products.<br/>This research also includes the application of the developed solvers to real large-scale problems from image processing (e.g., coded aperture superresolution, hyperspectral image reconstruction, and compressive video reconstruction), as well as theoretical proofs of convergence and numerical stability of the algorithms.<br/><br/>""Compressed sensing"" is an emerging field in computational mathematics that aims to improve signal (and image) reconstruction with less data.<br/> More efficient methods for compressed sensing can benefit such fields as medical imaging, astrophysics, biosensing, and geophysical data analysis.  The basic theory exploits the fact that many natural signals in science and engineering are ""sparse""--that is, they can be represented as a weighted combination of a small subset of commonly occurring signals.  When a signal is sparse, scientists can accurately reconstruct the original signal using a relatively small number of measurements of the original signal.  However, in practice finding the right weighted combination of signals can create staggering numerical and computational complexity.  This research aims to develop novel optimization methods that can quickly find accurate solutions to these large-scale problems.  For example, consider an astronomer wishing to image the night sky, which consists of small, bright stars against a dark background.  A conventional digital camera or imaging system would need a very high resolution and sensitive photodetector to effectively localize the different stars, but collecting this large number of pixels can be very costly and energy inefficient.  This work allows the astronomer to collect a relatively small number of random projection measurements of the scene and use these to reconstruct the image with a high probability of accuracy.  Fast and accurate optimization algorithms for sparse signal reconstruction can impact many other areas of image and signal processing as well, from reducing the dose of CT scans in biomedical imaging and improving image resolution in video surveillance systems for airport security, to more efficiently transmitting communication signals from distant satellites and NASA spacecraft and more carefully monitoring the health of a forest ecosystem using hyperspectral imaging.<br/><br/>"
"0810909","Collaborative Research: A Software System for Algebraic Geometry Research","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS, NUMERIC, SYMBOLIC & GEO COMPUT","09/15/2008","06/25/2009","Michael Stillman","NY","Cornell University","Continuing Grant","Junping Wang","08/31/2010","$142,000.00","","mike@math.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1264, 1271, 2865","0000, 9263, OTHR","$0.00","Macaulay2 is a free computer algebra system dedicated to the qualitative<br/>     investigation of systems of polynomial equations in many variables.  It<br/>     was developed by Daniel Grayson and Michael Stillman with NSF funding.<br/>     Grayson and Stillman will continue the development of Macaulay2.  They<br/>     will upgrade existing algorithms, develop and publish new algorithms, and<br/>     implement new algorithms. In particular, they will develop the interaction<br/>     of the symbolic computations that are Macaulay2's strength with the new<br/>     floating point algorithms in algebraic geometry that are now being<br/>     developed by Andrew Sommese, Jan Verschelde, Anton Leykin, Frank Schreyer<br/>     and others.  It is anticipated that these will make a whole new class of<br/>     problems accessible to experimentation and, in many cases, solution.<br/>     Eisenbud will organize contacts for the extended integration with other<br/>     systems and will engage other mathematicians in the development work that<br/>     needs to be done.  Central to the project are the continued expansion of<br/>     the collaborations that have been the hallmark of Macaulay2 development.<br/>     For this purpose two Macaulay2 Workgroup Meetings will be held in the<br/>     course of the two-year grant.  One particular research problem to be<br/>     attacked is: the use of computational systems to (probabilistically)<br/>     disprove, or suggest a proof of, the Jacobian Conjecture on polynomial<br/>     automorphisms of affine spaces (this will require the use the new floating<br/>     point algorithms).  Other areas where new algorithms can make an impact<br/>     include the study of numerical systems, fractions with specified types of<br/>     denominators, ideal factorization, systems where the multiplication of the<br/>     variables doesn't satisfy the commutative law, geometric optimization, the<br/>     analysis of observations of gene expression levels over time, and<br/>     bioinformatics.<br/><br/><br/>     Macaulay2 is part of the infrastructure that supports mathematical<br/>     research involving systems of polynomial equations in many variables.  The<br/>     study of such systems of polynomial equations is central in pure and<br/>     applied mathematics and in physics, with recent new impacts in such fields<br/>     as cryptography, robotics and string theory.  Increasing computer power<br/>     and the availability of programs like Macaulay2 are making a new level of<br/>     experimentation possible.  The experimental results found with Macaulay2<br/>     are helping in the formulation and development of tractable conjectures in<br/>     mathematics as well as in physics. A measure of Macaulay2's impact is that<br/>     at least 270 research papers have cited Macaulay2, several mathematicians<br/>     have contributed code, and books and course materials are now using it.<br/>     The PI's will develop the software further and will recruit developers<br/>     from the research community.  They will introduce graduate students and<br/>     mathematicians to the use of computers in research mathematics and the<br/>     requisite skills in programming and development of algorithms, through<br/>     workgroups at Berkeley and through the appointments of graduate students<br/>     as graduate assistants.<br/>"
"0811232","Recovery Type a Posteriori Estimation and Mesh Refinement for the Finite Element Method on Anisotropic Meshes","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/25/2008","Weiming Cao","TX","University of Texas at San Antonio","Standard Grant","Junping Wang","06/30/2012","$121,793.00","","wcao@math.utsa.edu","One UTSA Circle","San Antonio","TX","782491644","2104584340","MPS","1271","0000, 9263, OTHR","$0.00","For problems exhibiting strong anisotropic features the finite element method <br/>(FEM) based on anisotropic meshes can be much more efficient than the one <br/>based on isotropic meshes, provided the meshes are properly aligned and the <br/>right aspect ratios are maintained. However, there has been little rigorous <br/>analysis about it, and most computational work has been restricted to linear <br/>elements, based on heuristic justification and ad-hoc treatments.<br/>The aim of this project is to provide the numerical analysis for the FEM <br/>on anisotropic meshes and develop effective controls for the adaptive<br/>mesh refinement. The main objectives of this project include:<br/>(i) Analyze various recovery type error estimators for the FEM <br/>on anisotropic meshes; (ii) Develop reliable mesh metrics and mesh <br/>quality measures to control the anisotropic mesh refinement process; <br/>(iii) Develop a software tool box for the post-processing of the<br/>finite element solutions, including error estimation, mesh metrics <br/>construction, and mesh quality evaluation. (iv) Provide research training <br/>to graduate students on adaptive finite element computation in science <br/>and engineering.<br/><br/>Error estimation and adaptive mesh refinement are two major components <br/>in practical finite element simulations. The former assesses the accuracy <br/>of the solution and provides the quality assurance when it is delivered. <br/>The latter is an indispensable tool for improving the solution when the <br/>desired accuracy has not been reached. For anisotropic FEM, the elements <br/>are allowed to be long and narrow to fit the underlining problems more<br/>effectively. They may vary in sizes, orientations, and aspect ratios. <br/>This project is aimed at providing better understanding of the behavior <br/>of anisotropic FEM and developing more efficient techniques for the FE <br/>simulations. Since the recovery type error estimation techniques <br/>are simple, extraordinary robust, and most commonly used in engineering, <br/>it will be the main focus of this research. Successful completion of <br/>this project will provide rigorous mathematical theory and more efficient <br/>algorithms for practical FE modeling and simulations in aerospace <br/>engineering, material processing, and biomedical researches."
"0813648","Capturing subgrid structures with level set methods","DMS","COMPUTATIONAL MATHEMATICS","07/15/2008","11/28/2011","Rodolfo Rosales","MA","Massachusetts Institute of Technology","Continuing Grant","Leland Jameson","06/30/2013","$491,981.00","Benjamin Seibold, Jean-Christophe Nave","rrr@math.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1271","0000, 9263, OTHR","$0.00","This project tackles important problems arising from the need to find, represent and track small structures using level set methods. A particular focus are fluid dynamics applications of the new approaches developed.  Level set methods encode surfaces using level set functions defined on Eulerian grids, and evolve them by evolving the function.  Commonly used implementations suffer from mass loss, and<br/>small structures can vanish over the course of a computation.    To remedy these<br/>problems,  local mesh refinements  and  Lagrangian features have been reasonably successful, but at the expense of the method's basic simplicity and transparency.<br/>This research introduces a new solution to the difficulty:  incorporate gradient information into the process.  Current approaches do not carry,  nor update this information. Instead (when/if needed) it is approximated from the grid function.<br/>Knowledge of  gradient information is  not  enough to allow actual simulation of subgrid scale processes, but it enables the capture and tracking of subgrid size objects. It is also expected to improve accuracy in calculating quantities (e.g.<br/>stresses) where gradients play a role. The gradient data must be updated in time, maintaining coherence between function values and derivatives,  while exploiting the extra information carried by derivatives.  This is done using characteristic<br/>properties of the exact solutions to the underlying equation(s).   The advantage<br/>of the proposed approach is that it captures small structures,  while preserving the simplicity of a purely Eulerian approach on a regular grid.  This new method uses gradient information with a computational effort which is of the same order of magnitude as that of the current techniques that ignore gradients.<br/><br/>Identifying and accurately tracking small or thin structures, and the boundaries separating regions with different properties,  is fundamental in simulating many physical and biological processes, and in many other computational applications.<br/>Examples arise in:  medical imaging; image processing;  evolution of thin liquid and solid films, wafers, and fibers;  bubbly flows; droplet formation; colloids;<br/>etc.    The research in this project should contribute to a better simulation of<br/>such processes. A very useful technology for surface tracking is provided by the<br/>level set method:   the key idea is to model the surface as the locus where some<br/>property/function changes sign,   and to move the surface advecting the function<br/>--- rather than the surface itself.  This has many advantages; e.g. it allows an easy interface with other associated calculations where the surface plays a role<br/>--- in which it is usually preferable to have the data on a regular grid,  where the surface is hard to represent directly (e.g.: the pixels used to represent an image). However, one standard difficulty with this approach is that parts of the interface may be lost when below some level of resolution.  In this research the authors investigate a new approach to ameliorating this difficulty,  by carrying in the calculation gradient information,  in addition to the level set function.<br/>Unlike prior remedies, this approach  does not  tamper with the basic simplicity of the level set method.  In many practical applications gradient information is available, but currently not fully used.  Example 1: Data structures in computer graphics store surface normals,  which are  not fully used in simulations of the object.  Equipping the data with gradients should improve the quality of further processing steps,  such as in  visualization techniques for realistic rendering.<br/>Example 2:  The dynamic range of 2-D and 3-D MRI or CT-SCAN images is high,  but current technology does not make use this gradient information. Incorporating it into the calculations should  increase the effective resolution,  thus improving the detection of tumors in infants and the identification of small anomalies.<br/>"
"0757236","FRG: Collaborative Research: Semidefinite optimization and convex algebraic geometry","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS","09/01/2008","08/06/2008","Bernd Sturmfels","CA","University of California-Berkeley","Standard Grant","Junping Wang","08/31/2012","$245,734.00","","bernd@math.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1264, 1271","0000, 1616, 9263, OTHR","$0.00","The goal in this proposal is to develop the mathematical foundations and<br/>associated computational methods for the study of convex sets in real<br/>algebraic geometry. This work requires a combination of ideas and<br/>mathematical tools from optimization, analysis, algebra and combinatorics.<br/>The proposed program will lead not only to theoretical insights, but also to<br/>new algorithms and software that will enable novel applications in<br/>mathematics, engineering, and beyond. The work is organized in five main<br/>thrusts: semidefinite programming and sums of squares, convex semi-algebraic<br/>sets, sparsity and graphical structure, numerical polynomial optimization<br/>and applications, and deformations and variation of parameters. The PIs will<br/>focus on the development of a comprehensive theory and practical new<br/>algorithms for convex sets defined by polynomial inequalities. Specific<br/>problems and techniques include the formulation of semidefinite descriptions<br/>of convex hulls of real algebraic varieties, determinantal representations<br/>of hyperbolic polynomials, sparse polynomials and their symmetries, tropical<br/>geometry and homotopy techniques, and geometric programming.<br/><br/>Many areas in mathematics, as well as applications in engineering, finance<br/>and the sciences, require a thorough understanding of convex sets. This is a<br/>class of geometric shapes, with several different but complementary<br/>interpretations. The goal in this project is to achieve a better<br/>understanding of how these geometric properties emerge from their algebraic<br/>descriptions in terms of polynomial equations, and the corresponding<br/>computational implications. One of the main motivations is the possibility<br/>of applying these results in the context of optimization. The proposed<br/>research will contribute to existing knowledge, both in algebraic-geometric<br/>techniques as well as in mathematical optimization. It will create synergies<br/>between different branches of applied mathematics, and their engineering and<br/>scientific applications (e.g., in computational biology and statistical<br/>modeling). Successful completion of this project should contribute to the<br/>availability of efficient and reliable computational tools for solving<br/>polynomial systems, which have clear technological and economic interest.<br/>Other key features of this proposal include its integration with curriculum<br/>development, undergraduate research projects, training of graduate students<br/>and postdocs, and the development of new software tools for computational<br/>optimization.<br/>"
"0752905","Early Career Support for the Householder XVII Symposium","DMS","COMPUTATIONAL MATHEMATICS","06/01/2008","05/28/2008","Charles Van Loan","NY","Cornell University","Standard Grant","Junping Wang","05/31/2009","$20,000.00","Ilse C.F. Ipsen","cv@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1271","0000, 7556, 9263, OTHR","$0.00","Financial support is provided for the participation of USA-based early career scientists and PhD students at the Householder Symposium XVII on Numerical Linear Algebra. The Symposium is to be held in June 1-6, 2008 in Zeuthen, Germany. Matrix computations, a central topic in numerical linear algebra, plays a central role in scientific computing and is at the heart of many numerical procedures for partial differential equations and optimization. More recently, matrix methods have found their way in computational biology and a host of information science applications. The Householder symposia contribute to the vibrancy of this field by bringing together scholars from all over the world, both young and old, for five days of informal research exchanges. Multicore technologies have placed a premium on methods that scale and so it is no surprise that model reduction, low rank tensor approximation, and data compression, will be recurring themes at the Symposium. Matrix dimensions are so large in certain applications that radically new approaches are required,<br/>e.g., matrix factorizations that are based on sampling and data structures<br/>that match multilevel memory systems. Attendees of Householder XVII will exit the meeting with a sharpened sense of ``large n'' matrix computations.<br/> <br/>There are several reasons why the  Symposium is a particularly effective vehicle for ``jump starting'' the careers of its young attendees. There is an emphasis on ongoing research and an invitation process that proactively favors budding researchers. There is a deliberate attempt to build a communal sense of where the field is headed through specific ``forward thinking'', open microphone, panel discussions. Because it is the premier conference in the area, the Symposium attracts the top senior researchers from around the world; it is a ``must attend'' event. The intermingling of established researchers with the fresh PhDs is guaranteed because the conference is modest in size (about 150) and everyone who attends gives a talk. Enabling  USA-based early career scientists to attend the Householder Symposium on Numerical Linear Algebra ensures the well being of a field that is critical to American Science and Engineering. Consistent with the goal of broadening participation in computational science.<br/>"
"0811003","Shape and Topology Optimization on Elliptic Eigenvalue Problems in Inhomogeneous Media","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/30/2008","Chiu-Yen Kao","OH","Ohio State University Research Foundation -DO NOT USE","Standard Grant","Junping Wang","06/30/2011","$152,125.00","","ckao@cmc.edu","1960 KENNY RD","Columbus","OH","432101016","6146888734","MPS","1271","0000, 9263, OTHR","$0.00","The frequencies at which a drumhead can vibrate depend on its shape and topology.  We can then ask what kind of shape and topology of a drumhead with a specific weight provide the smallest bass tone (fundamental frequency).  If the drumhead is made of a single material, the answer for the shape is a disk.  However, if the drumhead is made of composite material, it is difficult to find the optimal shape and topology.  This is one of the questions that can be formulated as shape and topology optimization on elliptic eigenvalue problems in inhomogeneous media. <br/><br/>The goal of this work is to develop efficient numerical approaches to find the optimal shape and topology by using gradient calculation and a thresholding technique.  The common numerical approach for these problems is to start with an initial guess for the shape and then gradually evolve it, until it morphs into the optimal shape.  One of the difficulties is that the topology of the optimal shape is unknown.  Developing numerical techniques that can automatically handle topology changes becomes essential for shape and topology optimization problems.  The level-set approach based on both shape derivatives and topological derivatives has been well-known for its ability to handle topology changes.  Instead of using shape derivatives and topological derivatives, we develop a new binary approach, which is based on the projection gradient method combined with a thresholding process that can potentially change the topology.<br/><br/>The proposed research will result in a new binary approach to find the optimal geometry for elliptic eigenvalue problems.  These problems have many applications including resonant frequency control, photonic devices design, and population biology.  Specifically, the proposed numerical approach will be applied to four different types of problems: (1) Design a vibrating composite membrane with extremal resonant frequency; (2) Find the composite material with maximal or desired spectrum gap; (3) Design optical and electromagnetic resonators that have high quality factor (low loss of energy); (4) Find the best spatial environment for the maintenance of alleles in population genetics.<br/><br/>This research will also provide dissertation topics and research projects for some undergraduate students, graduate students, and postdocs.  The PI plans to release the code for public usage.  It will enhance general research study on shape optimization for elliptic eigenvalue problems.  More numerical optimal configurations will be found, and this will produce new insight into theoretical discovery."
"0844513","SGER: Collaborative Research: Non-negative Matrix Factorizations for Data Mining: Algorithms and Applications","DMS","COMPUTATIONAL MATHEMATICS","09/15/2008","09/16/2008","Tao Li","FL","Florida International University","Standard Grant","Junping Wang","08/31/2009","$44,000.00","","taoli@cs.fiu.edu","11200 SW 8TH ST","Miami","FL","331990001","3053482494","MPS","1271","0000, 9237, 9263, OTHR","$0.00","Nonnegative matrix factorization (NMF) factorizes an input nonnegative matrix into two nonnegative matrices of lower rank.  It is recently discovered that NMF in the most basic form is equivalent to a relaxed K-means clustering, the most widely used pattern discovery algorithm in data mining.  This direct link between mathematics and data mining sets in motion a large number of developments on using matrix factorizations for pattern discovery.  It turns out that NMF provides more consistent and mathematically well-defined optimization formulations for many fundamental and emerging data-mining problems.  NMF algorithms have well-understood properties; they are simple and easy-to-implement, well suited for distributed parallel architectures.  This research aims to formally establish a comprehensive NMF-based framework for data mining.  In particular, we will (1) extend matrix factorization data-mining methodology from current focus on clustering (pattern discovery) to newer problems: semi-supervised clustering (extending partial knowledge to whole data) and classifications (pattern prediction, such as predicting a cancer tumor tissue from a normal one); (2) develop fast numerical algorithms and incorporate state-of-the-art numerical optimization techniques; and (3) apply and evaluate the NMF algorithms in different real-world applications including text mining and bioinformatics."
"0829830","Workshop on Mathematics for Petascale Data, June 3-5, 2008, Rockville, MD","DMS","COMPUTATIONAL MATHEMATICS","05/15/2008","05/15/2008","Nagiza Samatova","NC","North Carolina State University","Standard Grant","Leland Jameson","04/30/2009","$20,000.00","","samatova@csc.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1271","0000, 7556, OTHR","$0.00","The investigator and her colleagues propose to organize a workshop to identify opportunities for future mathematical advances required for enabling scientific discoveries from petascale scientific data. The workshop will embrace scientists from academia, national laboratories, and industry. Application scientists, mathematicians, statisticians, high performance data miners will  define a research agenda for developing the next-generation mathematical techniques needed to meet the challenges posed by petascale data sets. They will produce a workshop report that will:<br/>* Articulate the requirements of various scientific domains such<br/>as global carbon cycle modeling, fusion energy production, nationally security, etc.<br/>* Delineate appropriate mathematical approaches and techniques,<br/>* Determine the current state-of-the-art in these approaches and<br/>techniques, and<br/>* Identify the gaps that must be addressed to enable the effective<br/>analysis of large, complex data sets in the next five to ten years.<br/><br/>Renewable energy production, carbon sequestration, national security, and human health protection are the urgent issues in today's society.<br/>Major incentives-economic, geopolitical, and environmental-drive a scientific mandate to research and develop cost-effective and beneficial solutions. It is imperative to truly synthesize the three pillars of scientific discovery-experimentation, theory, and ultra-scale computation-to address these challenges effectively and comprehensively.<br/>Advances in each have been already revolutionizing the way science is conducted. With this promise, however, comes a problem - the massive quantities of data so produced by national high-throughput experimental faculties, observatories and ultrascale computing facilities. Those data hold the answers to fundamental questions about the nature of the universe.  However, the answers will be subtly hidden in the raw data.<br/>Those data need to be analyzed to extract knowledge - to understand the science. Discovering such new knowledge will require the next-generation mathematical techniques from several fields, including but not restricted to statistics, machine learning, image analysis, and pattern recognition. Advances in these areas will also enable iteratively validating ultra-scale simulations with experimental and observational data.  As a result, these technologies will bring revolutionary and unconventional solutions to some of our most pressing and expensive challenges in health, energy, environment, and national security.<br/>"
"0811022","Collaborative Research: Algebraic Multigrid Methods: Multilevel Theory and Practice","DMS","COMPUTATIONAL MATHEMATICS","10/01/2008","09/18/2008","Scott MacLachlan","MA","Tufts University","Standard Grant","Junping Wang","09/30/2012","$201,042.00","","scott.maclachlan@tufts.edu","136 Harrison Ave","Boston","MA","021111817","6176273696","MPS","1271","0000, 9263, OTHR","$0.00","The primary goal of this collaborative proposal is to develop<br/>theoretically based algebraic multigrid (AMG) solvers for Hermitian<br/>(and, where possible, non-Hermitian) positive-definite problems.  The<br/>team aims to improve understanding of the performance of the family of<br/>AMG algorithms and, with this improved knowledge, to develop AMG<br/>methods that offer provable, computable, a priori information on the<br/>algorithm's performance.  The project team represents a close<br/>collaboration of experts in this area, each of whom has made<br/>contributions in the field.  Over the past several years, the team has<br/>begun to work collectively on developing new multilevel solvers and<br/>rigorous theoretical results for the convergence and complexity<br/>analysis thereof.  Together, the team will have the capability to take<br/>a step toward answering some of the fundamental research questions<br/>associated with these two essential aspects of the analysis and design<br/>of efficient algorithms.<br/><br/>We expect the work proposed here to: (1) directly impact computational<br/>simulation codes currently employing multi-level solvers, by providing<br/>faster and more reliable computational tools for the numerical<br/>computations at the core of physical simulations; and (2) allow for<br/>simulation of phenomena for which suitable solvers are currently<br/>unavailable.  The results from the proposed research will, thus, have<br/>a direct impact on scientific and engineering problems, including<br/>those from energy, through both the simulation of particle physics and<br/>processing of data from oil reservoir models, biophysics, in surgical<br/>simulation, and the environment, in climate prediction and contaminant<br/>remediation models.  The algorithms to be investigated here are<br/>already in use in many of these fields, but are often considered to be<br/>""expert-only"" tools.  The goal of this proposal is to develop more<br/>reliable and robust versions of these tools.  The proposed research<br/>will have a strong educational impact as well, as it provides for a<br/>solid base for training of graduate students in the modern theoretical<br/>and practical aspects of numerical methods for modeling of<br/>applications arising in science and engineering."
"0811025","A New Real-Space Finite Element Method to Solve the Kohn-Sham Equations of Density Functional Theory","DMS","COMPUTATIONAL MATHEMATICS","10/01/2008","09/16/2008","Natarajan Sukumar","CA","University of California-Davis","Standard Grant","Junping Wang","09/30/2011","$48,000.00","","nsukumar@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1271","0000, 9263, OTHR","$0.00","First principles (ab initio) quantum mechanical simulations based on Kohn-Sham density functional theory (DFT) are a vital component of modern materials research.  The parameter free, quantum mechanical nature of the theory facilitates both fundamental understanding and robust predictions across the gamut of materials systems, from metallic actinides to insulating organics.<br/>However, the solution of the equations of DFT (coupled Schrodinger and Poisson<br/>equations) is a formidable task and this has severely limited the range of materials systems that can be investigated by such rigorous, quantum mechanical means. Current state-of-the-art approaches for DFT calculations extend to more complex problems by adding more grid points (finite-difference methods) or basis functions (planewave and finite-element methods) without regard to the nature of the complexity, leading to substantial inefficiencies in the treatment of highly inhomogeneous systems such as those involving first-row, transition-metal or actinide atoms. This project will overcome this basic limitation of current approaches by employing partition-of-unity techniques in finite-element analysis to build the known atomic physics into the solution process, thus substantially reducing the degrees of freedom required and increasing the size of problems that can be addressed.<br/><br/>The electronic structure and fundamental properties (mechanical, electrical, magnetic, and optical) of materials are obtained via efficient and accurate solutions of the equations of density functional theory. By virtue of its generality, the proposed partition-of-unity finite element method for electronic-structure calculations has the potential to change the way the largest, most complex quantum mechanical calculations are done, thereby paving the way for new applications in metallic, biological, and nanostructural materials that were not possible before. The external collaboration with Dr. John Pask at LLNL will lead to the development of an optimized, fully self-consistent implementation, well-suited to large-scale parallel computational platforms. This collaboration greatly strengthens the likelihood of maximum impact with the realization of faster computational times on complex simulations, such as melting of d- and f-electron systems, equation of state studies, and structure and energetics of defects in new materials.<br/>"
"0810963","High-Order Numerical Solution of  Wave-Type Equations with Discontinuous Coefficients","DMS","COMPUTATIONAL MATHEMATICS","09/15/2008","07/20/2010","Semyon Tsynkov","NC","North Carolina State University","Continuing Grant","Leland Jameson","11/30/2011","$199,847.00","","tsynkov@math.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1271","0000, 9263, OTHR","$0.00","The key objective of the project is to build an efficient numerical method for computing the propagation of waves in the media with material discontinuities. Potential applications include a broad range of problems in both electromagnetism and acoustics. Mathematically, the propagation is governed by wave-like equations (either in the frequency domain or in the time domain) with discontinuous coefficients. Discontinuities in the coefficients are typically of the first kind. They present a major challenge when constructing a high-order numerical approximation, especially when they are not aligned with the discretization grid. Having a high-order discretization, on the other hand, is crucial for obtaining a robust predictive capability, because it alleviates the points-per-wavelength constraint and is also far better suited for multiscale problems, such as computing a small scale phenomenon (e.g., nonlinear backscattering in optics) at a large background (such as the forward propagating laser beam). In the course of the project, we will address the foregoing problem with the help of Calderon?s pseudodifferential boundary projections. These operators allow one to obtain equivalent surface parameterizations of solutions. The latter are subsequently combined with the appropriate interface conditions, which yields a self-consistent formulation. A key advantage of Calderon?s operators is that their discrete counterparts can be efficiently computed using the method of difference potentials. In doing so, one can use regular grids with no adaptation, and obtain high-order approximations for the domains of irregular shape. The anticipated results will make an important contribution to the theory of numerical methods for partial differential equations. On the practical side, the outcome will be an efficient and robust numerical methodology for solving a variety of applied problems.<br/><br/>It is very common that the propagating light, or sound, or radio waves have to pass through the interfaces between the materials with different properties. Examples are abundant and range from simple everyday setups, such as the interface between air and glass for light, to various applications of radars and sonars, to satellite communications, to plasma fusion devices, and others. The presence of interfaces, across which the material characteristics vary sharply, makes it more difficult to solve these problems on the computer. However, from the standpoint of mathematics, the corresponding formulations share a number of important components, and in the course of the project we are going to develop and test a universal numerical methodology for solving a variety of such problems. The methodology will exploit the advanced mathematical apparatus known as Calderon?s projections. The results of the project will contribute to both the theory and practice of solving scientific problems on the computer, and will be important for applications in acoustics, electromagnetism, and optics.<br/>"
"0810105","RUI:   Computations in Ehrhart Theory","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/25/2008","Matthias Beck","CA","San Francisco State University","Standard Grant","Junping Wang","06/30/2012","$140,414.00","","mattbeck@sfsu.edu","1600 Holloway Ave","San Francisco","CA","941321722","4153387090","MPS","1271","0000, 9229, 9263, OTHR","$0.00","This research proposal, continuing various strands of the investigator's work, is related to discrete and continuous volume computation for rational polytopes. Here continuous volume refers to the usual (relative) volume of the polytope, while discrete volume refers to the number of integral points in the polytope, often as a function of additional parameters. Examples of the latter are the Ehrhart (quasi-)polynomials and vector partition functions.<br/>Continuous and discrete volume computation for polytopes has been of great interest in recent years, partly because of applications to many mathematical fields, some of which seem distant from discrete and computational geometry: number theory, commutative algebra, algebraic geometry, optimization, representation theory, statistics, and computer science. <br/><br/>The goal of this project is to develop useful theoretical and computational methods for volume computation for polytopes. The PI proposes four concrete lines of problems to work on, including variations of the Birkhoff polytope, vector partition functions, growth series of lattices, and inside-out polytopes and their application to classical enumerative combinatorial problems. All of the proposed work has a computational focus. The investigator has a track record of sustained and serious effort both in outreach to students at all levels (secondary, undergraduate, and graduate), and in building institutions in which discrete and computational geometry can grow. The investigator will continue to attract students into these fields and nurture the careers of students and young researchers.<br/>"
"0809086","Efficient High Order Numerical Methods for Convection Dominated Partial Differential","DMS","INFRASTRUCTURE PROGRAM, COMPUTATIONAL MATHEMATICS","08/01/2008","05/18/2010","Chi-Wang Shu","RI","Brown University","Continuing Grant","Junping Wang","07/31/2012","$551,953.00","David Gottlieb, Jan Hesthaven","chi-wang_shu@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1260, 1271","0000, 7556, 9150, 9263, OTHR","$0.00","In this project, research in the algorithm design and analysis<br/>of high order numerical methods, including the finite difference<br/>and finite volume weighted essentially non-oscillatory (WENO) <br/>schemes, discontinuous Galerkin finite element methods, and <br/>particle methods, for hyperbolic and other convection dominated <br/>partial differential equations, especially in adaptive, multiscale <br/>and uncertain environments, will be carried out.  Parallel <br/>implementation and applications of these methods will also be <br/>addressed.  The intellectual merit of the proposed activity lies <br/>in its comprehensive coverage of algorithm development, analysis,<br/>implementation and applications.  Problems in applications<br/>motivate the design of new algorithms or new features in<br/>existing algorithms; mathematics tools are used to analyze<br/>these algorithms to give guidelines for their applicability<br/>and limitations; practical considerations including parallel<br/>implementation issues are addressed to make the algorithms<br/>competitive in large scale calculations; and collaborations<br/>with engineers and other applied scientists enable the<br/>efficient application of these new algorithms or new features<br/>in existing algorithms.<br/><br/>The proposed research aims at the design of efficient algorithms,<br/>which, when used on today's powerful computers, will help to <br/>solve many problems from diversified applications such as<br/>aerodynamics and aeroacoustics for aircraft design, <br/>electromagnetism wave simulation for communications, <br/>and semiconductor device simulation for the computer industry.<br/>The thrust of this proposal is to use powerful mathematical<br/>tools to guide the design of algorithms, so that they are more<br/>efficient, more reliable, and more robust in applications.<br/>"
"0809062","Data-driven stochastic analysis of flow in random heterogeneous media","DMS","COMPUTATIONAL MATHEMATICS, COFFES","08/15/2008","07/09/2008","Nicholas Zabaras","NY","Cornell University","Standard Grant","Junping Wang","07/31/2012","$252,000.00","","nzabaras@nd.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","MPS","1271, 7552","0000, 9263, OTHR","$0.00","This proposal concerns the analysis of transport phenomena in heterogeneous random media with emphasis on the multi-length scale variations in properties that these phenomena exhibit, and the inherently limited information available to quantify these property variations that necessitates posing these phenomena as stochastic processes.  The non-intrusive stochastic multiscale framework being developed has three key components: (a) A computational framework that encodes the limited information available about the variability of the (multiscale) material properties (permeability) into a reduced-order stochastic input model, (b) An adaptive sparse grid collocation framework for solving the stochastic PDEs involved and (c) A mathematically consistent strategy to exchange information across length scales for the solution of stochastic multiscale problems.  The key concept explored in the data-driven reduced-order stochastic input model construction is the low-dimensional parametrization of manifolds embedded in high-dimensional spaces. The sparse grid collocation approach constructs the stochastic solution solely based on function calls to the corresponding deterministic physical simulator. The framework is based on hierarchical basis functions in multiple dimensions. Adaptivity and convergence are ensured by utilizing a local support while scalability is guaranteed by the careful choice of appropriate data structures. The information transfer strategies are based on the decoupled structure of the stochastic and multiscale algorithms.  <br/><br/>The results of this research will impact the understanding of flow processes in random media. Thermal and hydrodynamic transport in random heterogeneous media are ubiquitous processes occurring in various scales ranging from the large scale (e.g. geothermal energy systems, oil recovery, geological heating of the earth?s crust) to smaller scales (e.g. heat transfer through composites, polycrystals, flow through pores, inter-dendritic flow in solidification, heat transfer through fluidized beds). There has been increasing scientific, technological and economic interests in predictive modeling of the thermal and hydrodynamic behavior of such media. In addition, this work can be valuable in understanding other systems that are poorly understood and/or controlled due to the gappy and inaccurate data available for their description. The problems addressed provide a unique and valuable training opportunity for students to learn, develop and apply cutting edge computational mathematics techniques to a variety of complex systems.<br/>"
"0810413","High order numerical methods for PDEs on complex domains and their applications in computational biology","DMS","COMPUTATIONAL MATHEMATICS","09/01/2008","08/28/2008","Yongtao Zhang","IN","University of Notre Dame","Standard Grant","Junping Wang","08/31/2012","$80,575.00","","yzhang10@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","1271","0000, 9263, OTHR","$0.00","Efficient numerical methods have become more and more important for studying PDE models in computational biology. These time dependent PDE models can be parabolic or hyperbolic, and are defined on complex spatial domains. The PI proposes to perform research in the design, analysis of high order weighted essentially non-oscillatory (WENO) schemes and discontinuous Galerkin (DG) methods on two-dimensional (2D) and three-dimensional (3D) unstructured meshes, for hyperbolic systems of conservation laws and stiff reaction-diffusion equations. And the PI proposes to apply these methods to high dimensional models for morphogen systems in developmental biology. Understanding morphogen gradient formation during embryo development is a fundamental problem in developmental biology. Mathematical models the PI will study are built on three-dimensional complex geometrical domains. Computational analysis of these models will reveal the formation mechanisms of BMP morphogen gradients, and the importance of incorporating 3D realistic shape of the embryo. In the spatial direction, the morphogen gradient systems usually develop sharp gradients on the embryo with a 3D complex shape. Hence the high order robust numerical methods on 2D and 3D unstructured meshes will be especially useful in the numerical simulations to deal with the 3D complex geometrical domains and the sharp gradients of the solutions. Four specific tasks are: 1) Develop high order WENO schemes and codes on 3D tetrahedral meshes; 2) Develop high order implicit integrating factor discontinuous Galerkin (IIF-DG) schemes and codes on 2D and 3D unstructured meshes; 3) Develop 3D reaction-diffusion models for BMP gradient formation during dorsal-ventral patterning of the zebrafish embryo. Starting from a simple model based on simplified bio-chemical gene network, the PI will create a more complete model motivated by new biological experiments; 4) Via computational analysis of the models on 3D complex geometrical domains, study the formation mechanisms of morphogen gradients, address important biological questions. Then, apply the model to other organisms such as Xenopus. The proposed numerical methods will be applied in the simulations. <br/><br/>The proposed research will result in a suite of nice modeling and computational techniques, suitable for quantitative study of various morphogen systems, during the embryo development of different organisms. These techniques are expected to make positive contributions to computer simulations of complicated phenomena in morphogen gradient pattern formation. The proposed activity will also provide excellent training and education opportunities for both graduate and undergraduate students interested in research at the interface of mathematics, computation, and biology. The students will be well prepared to work in interdisciplinary research through developing intellectual and technical tools for the advancement of computational mathematics and applying these tools to important questions in biology.<br/>"
"0810939","Development of high-order numerical methods for the dynamics of suspended lipid membranes with internal structure","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","09/01/2008","09/03/2008","Gregory Miller","CA","University of California-Davis","Standard Grant","Junping Wang","09/30/2013","$203,232.00","","grgmiller@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","MPS","1266, 1271","0000, 7237, 9263, OTHR","$0.00","The shape and rigidity of cell membranes are dynamic features related to cell function.  An idealized model of the cell membrane that includes these features is a lipid bilayer vesicle, comprised of two or more chemical components capable of phase separation and endowed with different inherent rigidities. Examples include several lipid-cholesterol systems.  In general, compositional fluxes and elastic moduli may be derived from a common energy potential, resulting in a tightly coupled system.  These vesicles have characteristic thicknesses that are much less than characteristic diameters, suggesting their treatment as co-dimension 1 surfaces.  This proposal links three research topics to develop a continuum model of such vesicles in fluid media.  First is the high-order interface tracking.  Embedded boundary methods will be developed to couple the motion of a membrane suspended in, and containing, viscous fluid.  Second is the dynamics -- chemical and mechanical -- of a membrane with time dependent shape.  An approach to this problem is to employ cartesian grid finite volume methods with a phase field approach to compute dynamics (Cahn-Hilliard for chemistry, and solid mechanics for motion) in an annulus, with the desired behavior obtained in the limit of zero thickness.  The problems of front tracking and interface dynamics must be tightly coupled, e.g., through predictor-corrector and relaxation strategies.  The third problem concerns constitutive modeling - the development of energy potentials, constrained by experimental phase diagrams, elasticity measurements, and the symmetry requirements of frame invariance, to couple the effects of chemistry with shape.<br/><br/>This proposed work will advance the understanding of numerical partial differential equations of mixed hyperbolic-elliptic type on time-dependent domains.  A numerical model of vesicle dynamics, linking the dynamics of shape and chemistry, will be developed for the purpose of better understanding experiments on heterogeneous vesicles, both natural and biomimetic, and ultimately to aid in understanding the connections between cell membrane chemistry, shape, and function.<br/>"
"0813893","Algorithms and Numerical Analysis for Nested Approximations of Stochastic Particle Dynamics","DMS","COMPUTATIONAL MATHEMATICS, COFFES","09/01/2008","07/11/2008","Petr Plechac","TN","University of Tennessee Knoxville","Standard Grant","Junping Wang","12/31/2011","$250,179.00","","plechac@math.udel.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","MPS","1271, 7552","0000, 9150, 9263, OTHR","$0.00","The ever-increasing computational power has instigated rapid development of computational techniques for simulating large stochastic interacting particle systems.  Growing computing capabilities have helped to obtain unprecedented insight into numerous problems ranging from physical phenomena in materials, chemical reactions, and biological processes to image processing.  However, as is common in the initial development of simulation methodologies the rapid emergence of new computational techniques far outstrips theoretical understanding of the algorithms.  From the computational point of view, balancing the competing aims of numerical accuracy and computational efficiency still remains one of the central problems in simulations of large multi-scale systems. <br/>     The proposed research outlines a framework for the numerical analysis and implementation of simulation algorithms based on coarse-graining of the microscopic system.  In the proposed work we view coarse-graining as numerical approximation of coarse observables which would have to be estimated from computationally expensive microscopic simulations.  The goal of the proposed work is to develop numerical tools for assessing the quality of the approximation and to use error indicators in order to implement reliable simulation algorithms.  Understanding approximations and their limitations is particularly important in the context of stochastic simulations, since even a convergent simulation may not provide any reliable insight into simulated phenomena (e.g., phase transitions, critical phenomena).  Furthermore, the computational complexity can be substantially decreased if a trade-off between accuracy and efficiency is properly adjusted.  In simulations of systems with a large number of interacting entities we often end up estimating average values of certain observables that depend on randomly distributed microscopic configurations of the system.  In the proposed approach we apply hierarchical microscopic-macroscopic computational paradigms and explore their potential for improving efficiency, reliability, and algorithmic complexity of methods used for sampling probability measures on high-dimensional spaces.  We aim at developing tools suitable also for approximating transient behavior which may not be properly captured, on experimentally relevant time-scales, by sampling the equilibrium distribution.  The proposed framework will derive nested approximations of the underlying multi-scale system with optimal interactions between different scales.  This scale decomposition strategy will be applied to the development of parallel algorithms for simulating stochastic systems composed of a large number of degrees of freedom.<br/>     The proposed work is in the intersection of numerical analysis, stochastic processes, and statistical mechanics.  The potential impact of the proposed mathematical topics encompasses a wide range of applications in physics, chemistry, materials science, and other fields where large multi-scale simulations are used.  Therefore, one of the objectives is to implement flexible tools that can be tailored to specific problems and existing packages within a short learning curve."
"0811046","Galerkin Lattice Boltzmann Methods for Direct Simulation of Liquid Slip on Superhydrophobic Surfaces","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/30/2008","Taehun Lee","NY","CUNY City College","Standard Grant","Junping Wang","06/30/2012","$181,200.00","","thlee@ccny.cuny.edu","Convent Ave at 138th St","New York","NY","100319101","2126505418","MPS","1271","0000, 9263, OTHR","$0.00","The objective of the proposed research is to develop an unstructured lattice Boltzmann method (LBM) based on the Galerkin formulation (GLBM) for the direct simulation of liquid slip on superhydrophobic surfaces and identify important design factors that maximize the effective slip under practical conditions. The large effective slip on superhydrophobic surfaces is expected due to the sizable difference in viscosity between liquid and gas that is trapped in the nanostructures. A successful numerical model should be able to deal with complex shape of superhydrophobic surfaces and large viscosity difference between fluids. The proposed two-phase GLBM on the unstructured mesh will overcome several undesirable properties inherent to the LBM on the structured mesh as a modeling tool for superhydrophobic surfaces; namely, instability at large density/viscosity difference and geometrical restriction imposed by the mesh. It will enable investigation of detailed flow physics on superhydrophobic surfaces covered with complex nanostructures. The proposed research is to: (1) Develop GLBM for immiscible two phase flows having a large density and viscosity ratio, using implicit time marching on unstructured mesh; (2) Establish appropriate boundary conditions at the liquid-solid-gas boundary based on the minimization of the free energy and incorporate them into GLBM framework; (3) Examine the physics of continuous and dispersed (droplets) liquid flows on superhydrophobic surfaces with complex nanostructures; and (4) Find the optimum profile and distribution of nanostructures for the maximum superhydrophobicity and effective slip.<br/><br/>Superhydrophobic surfaces are of great interest in many industrial and biological applications, because properties such as anti-sticking, anti-contamination, and self-cleaning are expected. When a droplet rolls over a contamination, it collects the particles from the surface and the contaminant particles are removed from the surface. In microfluidic and biomedical applications, superhydrophobic surfaces reduce the hydrodynamic drag at the wall, and prevent cross-contamination of one drop by another one moving on the same surface. The proposed research explores a new modeling capability that could significantly change existing approaches to designing microfluidic devices and help prescreen design alternatives reducing the design cost. The research experience acquired from the proposed project will also enhance the course materials for the advanced computational fluid dynamics courses.<br/>"
"0811150","Numerical methods for transport problems on networks","DMS","COMPUTATIONAL MATHEMATICS","07/15/2008","07/09/2008","Pierre Gremaud","NC","North Carolina State University","Standard Grant","Junping Wang","06/30/2013","$207,225.00","","gremaud@math.ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1271","0000, 9263, OTHR","$0.00","The objective of this research is  to develop efficient numerical methods for the simulation of networked systems of hyperbolic balance laws. In such networks, each edge is a quasi one-dimensional domain interacting with the rest of the system through junctions at each of its ends. The character of those interactions depends on the applications at hand; ideally, they are<br/>modeled to mitigate the effects of dimension reduction. Mathematically, the presence of junctions complicates the selection process of proper solutions. The naive use of existing numerical methods in the present context may be inefficient, unstable and lead to nonphysical solutions. Numerical methods specifically optimized for network problems will be designed, analyzed and implemented.  This involves not only discretization issues but also and more importantly the construction of new solvers. Those solvers will be designed by building on recent progress in both numerical methods for differential algebraic equations and in domain decomposition methods. Some phenomena are <br/>essentially one-dimensional in most of the computational domain and only ""locally multidimensional"".  Being able to reliably switch to one-dimensional approximations represents significant savings; how to do this efficiently will be investigated. Transport phenomena in trees, which play an essential role in many organisms (breathing, blood circulation,etc...), lead to other types of couplings for which new numerical approaches are also proposed. Two applications are considered as test beds for various aspects of the research. They respectively involve blood flows in arteries and gas flows.<br/><br/>Networks of roads, pipelines or arteries play a fundamental role in many aspects of our lives. They allow the efficient transport and distribution of, for instance,  cars, raw sewage, gas or blood in respectively cities, countries, organisms, etc... Related practical problems range from business <br/>(optimization of natural gas pipeline networks)  and  public safety (emergency evacuation schedules in specific geographic areas) to health (likelihood of stroke based on patients' vasculature).  While the tools of scientific computing have been applied very successfully to many  types of transport <br/>phenomena such as problems in aerodynamics, the numerical simulation of transport on networks faces several specific challenges that have yet to resolved. Three main issues will be  studied. (i) Efficiency: the methods have to be nimble enough to allow the simulation of entire networks as opposed to only some of their parts. (ii) Accuracy: flows are more involved near junctions or crossroads than they are away from them. Different models may have to be used at different locations of a same network. The project will study efficient implementation of such multi-physics models for network flows. (iii) Finally, the various theoretical and numerical aspects of the project will be testedon two specific applications, arterial blood flow and gas flow in rigid pipes."
"0810187","Collaborative Research: Tuning-Free Adaptive Multilevel Discontinuous Galerkin Methods for Maxwell's Equations","DMS","COMPUTATIONAL MATHEMATICS","07/15/2008","05/06/2010","Timothy Warburton","TX","William Marsh Rice University","Continuing Grant","Leland Jameson","06/30/2012","$188,167.00","","tcew@vt.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271","0000, 9263, OTHR","$0.00","The investigator and colleagues are formulating, analyzing, and implementing adaptive multilevel Discontinuous Galerkin methods for coupled interior/exterior domain problems associated with the time-harmonic Maxwell equations. These advanced finite element  methods are being realized as multilevel techniques on the basis of an adaptively generated hierarchy of triangulations of the computational domain. The research team is focusing on three central issues related to the basic steps `SOLVE', `ESTIMATE', `MARK', and `REFINE' of the adaptive loop. First, the smoothing process within the multilevel solver is performed only on the newly refined part of the triangulation obtained by a residual type a posteriori error estimator. Second, the a posteriori error analysis, which additionally has to take into account the effect of such local smoothing, aims to provide conditions guaranteeing a reduction of the global discretization error at each refinement step.<br/>Third, the selection of elements, faces and edges of the triangulation for refinement are based on a bulk criterion with an automatic (`tuning free') choice of the parameters controlling the amount of refinement in order to achieve optimal performance of the overall algorithm. Finally, the team is developing criteria to choose the parameters of artificial radiation boundary conditions automatically, such that no tuning on behalf of the user is required there as well.<br/><br/>Simulation of electromagnetic phenomena<br/>is a particularly challenging problem in computational mathematics.<br/>The investigator and colleagues are<br/>establishing a profound theoretical foundation for adaptive multilevel discontinuous Galerkin methods in electromagnetic field computations. They are developing a reliable algorithmic tool, of optimal computational complexity, that can be used for the numerical solution of challenging real-life problems in electrical engineering applications. The methods developed in this project have numerous technical and scientific applications, for instance semiconductor simulation or particle accelerator design. The results will be disseminated through publication of algorithms and results and reference computer codes being developed during this project will be made available to practitioners.<br/>"
"0811223","Time Domain Numerical Methods for Electromagnetic Wave Propagation Problems in Complex Dispersive Dielectrics","DMS","COMPUTATIONAL MATHEMATICS","09/15/2008","08/26/2010","Vrushali Bokil","OR","Oregon State University","Standard Grant","Junping Wang","08/31/2013","$132,800.00","","bokilv@math.oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","MPS","1271","0000, 9251, 9263, OTHR","$0.00","This project aims to develop and analyze efficient numerical methods for forward problems in the time domain involving the propagation and scattering of electromagnetic waves in complex dielectric media that display physical dispersion due to multiple polarization mechanisms.  The forward solution of dispersive, time-dependent electromagnetic wave systems is often used to obtain safety standards regarding exposure of humans to high-energy electromagnetic fields.  These numerical simulations can also be utilized in an inverse-problem formulation, for example, in cancer detection.  In imaging for medical applications, one seeks to investigate the internal structure of an object (the human body) by means of electromagnetic fields at sub-infrared frequencies to detect and characterize cancer or other anomalies by studying changes in the dielectric properties (such as permittivity, conductivity, relaxation times, etc.) of tissues.  Electromagnetic wave propagation in complex dispersive media is governed by the time-dependent Maxwell's equations coupled to equations that describe the evolution of the induced macroscopic polarization.  The behavior of the media's macroscopic polarization may include first-order relaxation mechanisms of Debye type, used in modeling polar materials such as water and living tissue, as well as second-order mechanisms of Lorentz type.  These two mechanisms will be used as model problems.  The investigator and a graduate student will develop operator-splitting techniques along with mixed finite element methods for electromagnetic wave propagation in dispersive dielectrics.  Additionally, fictitious domain methods will be applied to wave-scattering problems in these materials.  Efficient absorbing layers for terminating computational boundaries involving dispersive dielectrics will be constructed and analyzed.  Analysis of well-posedness, stability, and convergence of the new methods will be conducted.  The graduate student will study existing simulation methods and make comparisons with the newly developed computational techniques."
"0839866","International Conference on Spectral and High-Order Methods 2009 - ICOSAHOM'09; June 2009, Trondheim, Norway","DMS","COMPUTATIONAL MATHEMATICS","09/01/2008","08/27/2008","George Karniadakis","RI","Brown University","Standard Grant","Junping Wang","08/31/2009","$15,000.00","David Gottlieb, Jan Hesthaven","George_Karniadakis@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1271","0000, 7556, 9150, 9263, OTHR","$0.00","The investigators propose to participate in organizing the next ICOSAHOM (International Conference on Spectral And High-Order Methods) on June 22-26 2009 at the Norwegian University of Science and Technology (NTNU) in Trondheim, Norway. They have been working in this field for several decades and have been active participants in all past ICOSAHOM meetings. Selected papers from the proceedings will be published as a special issue in a archival journal. They also plan to produce a report for the funding agency that will provide feedback from a round-table discussion we plan to organize. The theme of this round-table discussion will be the future algorithmic trends and applications of numerical methods and what is the role of high-order methods. Also, their transition to computer codes that will be used for the simulation needs of the national labs and in industry will be discussed, especially in the context of petaflop applications. To this end, they plan to invite researchers with advanced computing expertise and also have interest on high-order accuracy to participate in this panel. The invited and contributed talks will contain new theoretical and applications results of spectral and high-order methods which had not been published yet. Spectral and high-order methods have become increasingly important in diverse applications as aeroacoustics, electromagnetics, ocean modeling, seismology, energy, non-Newtonian flows, nonlinear optics, plasma dynamics, and uncertainty quantification. New developments in the last few years have addressed issues of complex geometry, high-speed flows, fast solvers for large-scale simulations, reduced order modeling, and stochastic modeling.<br/><br/>NSF has invested heavily in computer hardware in their last three decades in par with the rapid growth of computer industry. However, simultaneous advances are required in the development of algorithms and software in order to make effective use of the new computer resources. This conference will advance the state-of-the-art in algorithms of high accuracy that are required in order to produce credible simulation results. In addition to the senior researchers invited, we plan to sponsor young researchers, postdocs and PhD students, especially from under-represented minorities, in order to educate a new cadre of simulation scientists in these methods. Today, high-order accuracy (at least second-order) is a requirement for publishing archival work in engineering journals, and the planned panel discussion is expected to have great impact in the engineering community. In addition, the important issue of numerical versus modeling accuracy and issues related to Verification and Validation will be discussed.<br/><br/>"
"0810929","An Efficient High-Order Method for Fluid-Structure Interactions","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/30/2008","Suchuan Dong","IN","Purdue University","Standard Grant","Junping Wang","06/30/2012","$154,996.00","","sdong@purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1271","0000, 9263, OTHR","$0.00","This project addresses mathematical, algorithmic, and computational issues for coupling the three-dimensional Navier-Stokes equations and the three-dimensional nonlinear elastodynamic equations to achieve high-order accuracy. The transmission conditions between the fluid and structure will be enforced exactly with the proposed formulation, which allows for the fluid and structure sub-problems to be computed independently and in parallel, greatly facilitating the solution of such systems on massively parallel computers. The elastodynamic equations and the Navier-Stokes equations are solved employing identical set of high-order basis functions, substantially simplifying the implementation of transmission conditions between the fluid and the structure. Scalable algorithms for efficient computations of large-scale fluid-structure problems are proposed and investigated.<br/><br/>Fluid-structure interaction is omnipresent in natural and man-made environments, and underlies many engineering, physical and biological applications.  Wind rustling leaves and ringing chimes, hurricanes breaching levees and destroying a community or an entire city, circulating blood in normal or diseased arteries inducing favorable conditions for the formation of aneurysms or atherosclerotic plaques, are several immediate examples. The proposed research will enable accurate computations of fluid-structure interactions. This will bring about unprecedented predictive capabilities and profoundly impact many scientific disciplines.<br/>"
"0810855","Flux Recovery, A Posteriori Error Estimation, and Adaptive Finite Element Method","DMS","COMPUTATIONAL MATHEMATICS","08/15/2008","08/11/2008","Zhiqiang Cai","IN","Purdue University","Standard Grant","Leland Jameson","07/31/2012","$270,980.00","","zcai@math.purdue.edu","2550 Northwestern Ave.","West Lafayette","IN","479061332","7654941055","MPS","1271","0000, 9263, OTHR","$0.00","The main purpose of this project is to develop, analyze, and test novel,<br/>accurate a posteriori error estimators of the recovery type for various<br/>finite element discretizations of a variety of elliptic equations and<br/>systems arising from solid and fluid mechanics, including nonlinear<br/>problems. The investigator and his colleagues plan to study two types of<br/>recovery procedures: one is accurate only for the constitutive equation<br/>and the other is accurate for both the constitutive and equilibrium<br/>equations. Based on these recovered fluxes (or the stresses for solid<br/>and fluid mechanics), they will study three kinds of estimators. In<br/>particular, they will study an exact estimator on any given mesh,<br/>including an arbitrary initial mesh, with no regularity assumptions.<br/>Exactness on any given mesh implies that the estimator is ideally<br/>perfect for error control (or the so-called solution verification) on<br/>coarse (pre-asymptotic) meshes. No regularity assumptions in this<br/>project mean that the only assumptions on the existence of the<br/>underlying problem are required.<br/>This is weaker than those required for approximation theory and much<br/>weaker than those required by the current theory of the recovery-based<br/>estimators. Therefore, the estimators can be applied to problems of<br/>practical interests such as interface singularities, discontinuities in<br/>the form of shock-like fronts and of interior or boundary layers. The<br/>second part of the project is to establish convergence of adaptive<br/>finite element methods based on the recovery-based estimators and the<br/>newly developed estimators of this project.<br/><br/><br/>A major problem with computer simulations of physical phenomena is that<br/>all computational results obtained involve numerical error.<br/>Discretization error can be large, pervasive, unpredictable by classical<br/>heuristic means, and can invalidate numerical predictions.<br/>A posteriori error estimation is a rigorous mathematical theory for<br/>estimating and quantifying discretization error in terms of the error's<br/>magnitude and distribution based on the current simulation and given<br/>data of the underlying problem. This information provides bases for<br/>solution verification and for adaptive control of simulation process:<br/>adaptive mesh refinement, adaptive control of mathematical models and<br/>numerical algorithms. Success in this project will provide accurate and<br/>reliable a posteriori error estimators for a large class of elliptic<br/>equations/systems arising from engineering, physics, aerodynamics,<br/>atmospheric sciences, geology, biomechanics, material sciences,<br/>nano-technology, and industrial applications. The development of the<br/>exact estimator will enable error control on pre-asymptotic meshes and<br/>predictable computation analysis. Error control on pre-asymptotic meshes<br/>is of paramount importance for simulating physical phenomena in<br/>engineering applications and scientific predictions with limited<br/>computer resources.<br/><br/>"
"0811160","Collaborative research: Efficient algorithms for free boundary flows of complex fluids","DMS","COMPUTATIONAL MATHEMATICS","08/15/2008","08/11/2008","Matteo Pasquali","TX","William Marsh Rice University","Standard Grant","Junping Wang","07/31/2012","$163,703.00","","mp@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271","0000, 9263, OTHR","$0.00","In engineered and biological systems, flows often involve complexity because of the shape of the flow domain which can be unknown, and because the fluid can consist of or contain macromolecules and nanoparticles which impart unusual behavior.  This research aims at developing novel numerical algorithms for computing flows with such complexity. Because of their robustness and reliability, fully-coupled (monolithic/implicit) schemes are commonly used to simulate flows with free boundaries; however, these schemes may become impractical, especially in three-dimensional geometries and when the fluid has complex behavior, e.g., viscoelasticity. On the other hand, partitioned schemes have simpler implementation and potential lower computational cost, yet have shown severe stability issues when applied to highly nonlinear free boundary flows.  This research will develop novel partitioned algorithms for free boundary flows of complex fluids which will combine good stability properties with low computational costs and ease of implementation.<br/><br/>The novel algorithms developed in the project will be used to study and understand complex flows arising in biology, medicine, and engineering. Major applications include blood flow in human arteries, the optimization of coating processes, and the flow of emulsions which naturally occur in oil extraction. Multidisciplinary training will be incorporated in the research by joint advising of two PhD students in Applied Mathematics and Chemical Engineering.  The outcomes of the proposed research will impact cyberinfrastructures through scientific computing, as well as other areas of complex fluid mechanics where computing plays a central and essential role.<br/>"
"0810415","RUI: Structure of Entanglement in Macromolecules","DMS","COMPUTATIONAL MATHEMATICS","09/01/2008","09/03/2008","Eric Rawdon","MN","University of St. Thomas","Standard Grant","Leland Jameson","08/31/2012","$150,000.00","","ejrawdon@stthomas.edu","2115 Summit Avenue","St. Paul","MN","551051096","6519626038","MPS","1271","0000, 9229, OTHR","$0.00","The structures of tightly entangled tubes have been used to predict properties of entangled macromolecules, e.g. the pitch of alpha helices in proteins and the pitch of the DNA double helix.  In addition, physicists have proposed that subatomic particles known as glueballs are tightly entangled QCD flux tubes.  While there is a great deal of mathematical and scientific interest in these tight entanglements, explicit descriptions are unknown for all but some simple examples.  In the first portion of this project, the PI, collaborators, and students determine the structure of tight knots and links by performing careful numerical simulations and using the data to provide explicit descriptions of tight configurations and their sets of tube-to-tube contacts.  These configurations provide insights into how long tubes pack into small spaces, behavior seen throughout nature (e.g. in the packing of DNA in viral capsids).  Entanglement can also be seen in natural materials that form open chains.  The existence (or lack thereof) of entangled regions in polymeric chains, for example, raises many interesting questions regarding the complex interplay between structure and function.  Recently, the discovery of knotted regions in proteins has received much attention.  However, the algorithms for detecting the regions (and, thus, the definition of exactly what a knotted open chain is) vary by research group and is in much need of rigorous analysis.  In the second portion of this project, the PI, collaborators, and students explore new notions of knotting in open chains that can be used to detect entangled regions in polymers and then measure the size and shape of these knotted regions.  In particular, they establish a firm theoretical understanding of entanglement in open chains, use new and established spatial measurements to better understand the shapes of linked catenanes and knotted polymer chains and loops, and find connections between the structure of polymer models and those of tight knots.<br/><br/>Knotting and tangling occur frequently in nature at every scale.  For example, DNA forms knots during biological reactions and physicists conjecture that magnetic fields formed during storms on the sun can be knotted.  The early proposal of Kelvin that elementary particles form knots found recent support in studies of subatomic particles known as glueballs which are hypothesized to take the shape of tightly entangled tubes.  Other chain-like natural materials pack tightly as well, e.g. a large amount of DNA is packed into the head of viruses and the release of the DNA into the cell is what spearheads the infection process.  The structure and function of these chains are inherently intertwined.  In particular, understanding the native structure of these chains is a key step in manipulating processes, such as in creating designer plastics or identifying and killing rogue cells.  The PI, collaborators, and students study two models: tight realizations of knots and knots occurring in open and closed chains.<br/>Examples of applications include predicting unidentified glueballs, describing how materials pack tightly in small spaces, and characterizing the structure of knots in open chains of protein.<br/><br/><br/>"
"0811188","Practical Optimization Algorithms for Large-Scale Image and Data Processing","DMS","COMPUTATIONAL MATHEMATICS","09/15/2008","09/08/2008","Yin Zhang","TX","William Marsh Rice University","Standard Grant","Leland Jameson","02/29/2012","$242,015.00","","yzhang@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271","0000, 9263, OTHR","$0.00","Optimization algorithms are at the core of solving many problems in image and data processing, and dedicated algorithms are often critical in real-world applications.  The principal investigator (PI) will conduct algorithmic research in two important areas: image deconvolution and compressive sensing, to develop enabling algorithms that make relevant methodologies practical for large-scale, real applications.  For image deconvolution, the PI aims to develop optimization algorithms for total-variation-based models that are faster than existing algorithms by at least one or more order of magnitude.  Preliminary studies have shown that this ambitious goal is well within grasp. The new compressive sensing<br/>(CS) methodologies make it possible to significantly reduce the number of measurements needed for reconstructing compressible data.  The PI proposes to develop algorithms for important real-world applications of CS, and study random Kronecker-product measurement matrices that can drastically reduce data reconstruction complexity.<br/><br/>MRI (magnetic resonance imaging) is a widely used medical imaging modality that creates an image from scanned data.  A typical abdominal scan may take around 90 minutes.  Recent progress in a new methodology called compressive sensing (CS) makes it possible to reduce this time to 30 minutes by scanning only one third of data, while maintaining good image quality.  However, such a possibility can be realized only when fast algorithms are available to do real-time processing on incomplete data. <br/>This project is to develop and analyze such fast algorithms.  Another class of fast algorithms to be investigated is for improving the clarity of fuzzy images.  With such fast algorithms, for example, satellite or medical images can be better analyzed in a more timely fashion.  The results of this project will impact applications ranging from information technology to biotechnology.<br/>"
"0811203","Image Processing Using PDE on Image Features and Image Databases","DMS","COMPUTATIONAL MATHEMATICS","07/01/2008","06/23/2008","Arthur Szlam","CA","University of California-Los Angeles","Standard Grant","Leland Jameson","11/30/2011","$96,214.00","","aszlam@courant.nyu.edu","10889 Wilshire Boulevard","LOS ANGELES","CA","900951406","3107940102","MPS","1271","0000, 9263, OTHR","$0.00","In recent years the heat equation on a weighted graph  has been used to attack problems in image processing, including denoising, segmentation, and inpainting.<br/>In many cases the data used to build the graph is the set of patches or feature responses from a single image; and even then, patches or filter responses are usually only compared with their spatial<br/>neighborhoods.   On the other hand, it has become practical to<br/>manipulate large collections of images, and using the statistics of large collections of images has become an important image processing<br/>tool.   The PI will investigate how to use larger databases of<br/>images and image features in a PDE framework. For some of the proposed work, it will be necessary to invent new theory to lift notions of the smoothness of a surface embedded in Euclidean space to maps from the discrete grid into a weighted graph; and perhaps further to maps from a more general weighted graph into a weighted graph.  Other proposed work will try to make use of (and improve) recent methods for sparse feature extraction, and solidify the theory underlying the NL-means method of Buades, Coll, and Morel.<br/><br/><br/>A large class of popular image processing techniques making use of the theory of partial differential equations operate locally; that is, the behavior of each step of these algorithms at a given pixel in an image is determined solely by the neighboring pixel values.<br/>In recent years, it has become possible to manipulate large databases of images; and such databases of images have become available from many sources, including the world wide web. The PI will work towards extending the local techniques to make use of these large databases. The long term goal is image processing techniques which understand and utilize the content and context of images; such techniques would have many important applications, for example in medical imaging, hperspectral imaging, and computer vision in general.<br/>"

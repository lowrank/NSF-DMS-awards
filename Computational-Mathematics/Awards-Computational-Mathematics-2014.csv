"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1439979","Conference on mathematical and computational challenges of wave propagation and inverse problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","09/28/2015","Jianliang Qian","MI","Michigan State University","Standard Grant","Junping Wang","08/31/2015","$40,000.00","Fadil Santosa, Gang Bao","qian@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","7556, 9263","$0.00","This project provides participant support for the ``Conference on mathematical and computational challenges of wave propagation and inverse problems'' that will be held in April 9-11, 2015 on the campus of Michigan State University, East Lansing, MI. The theme of the conference is ""wave propagation in inverse problems"". Wave propogation and inverse problems have tremendous applications in medical imaging and exploration of Earth natural resources. This conference brings together leading researchers in the world to discuss recent advances on such applications. The conference also provides ample opportunity for graduate students, post-docs, junior faculty, especially women, under-represented minorities and people with disabilities to present their work. By locating the conference central to the Michigan State University campus, participation of local, non-mathematical students and scientists will be maximized.<br/><br/>The conference contributes significantly to the advancement of research and education in applied and computational mathematics. The conference focuses on three related topics. The first is the development of wave-related multi-scale, multi-physics modeling, analysis, and computation techniques. The second is the integration of inverse problem techniques with imaging analysis methods for important physical processes that arise in radar and sonar, geophysical exploration, non-destructive testing, and medical imaging. The third is the stochastic modeling and uncertainty quantification of the large scale wave propagation focusing on imaging and inversion in random media. The conference provides a forum to update recent progress on these research areas. By interspersing theoretical, applied, and computational talks in the conference program, this focused conference will encourage collaboration between scientists with various disciplinary backgrounds. By outlining the potential impact that theoretical tools can have on the most salient issues of our day, the conference will influence and invigorate the educational efforts at the institutions of the conference speakers."
"1418752","Numerical algorithms for nonlinear subsurface flow and transport","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","05/24/2016","Todd Arbogast","TX","University of Texas at Austin","Continuing Grant","Leland Jameson","06/30/2018","$365,000.00","","arbogast@ices.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1271","8396, 8609, 9263","$0.00","Assessment, design, and monitoring of human activities involving reservoirs and aquifers in the Earth's subsurface require large-scale computer simulation of flow and transport processes over long time periods. Such simulations are used to guide scientists and engineers regarding, e.g., petroleum production, groundwater management, and secure storage of wastes such as carbon dioxide and nuclear contaminants. Computer simulation of the subsurface environment is especially important because it is largely inaccessible to direct observation and therefore also difficult or impossible to rectify human failures. The project involves fundamental research and education on immiscible, two-phase subsurface flow.  This project has the potential for significantly greater fidelity simulations by properly accounting for the nonlinearly coupled behavior of the physical processes embodied in the governing equations, and the numerical algorithms should work well on modern supercomputers.  Moreover, many models of scientific and engineering interest consist of similar nonlinearly coupled flow and transport systems, and so general progress here is likely to support efforts more broadly.  The project will fund the research of two STEM Ph.D. graduate students and involve at least two undergraduate students.  They will work in the Center for Subsurface Modeling of the Institute for Computational Engineering and Sciences, which provides an interdisciplinary environment mixing expertise in mathematics, computational science, petroleum engineering, and geological science.  The students will be prepared for employment opportunities in academia, government laboratories, and industry. External collaborations will enhance the impact of the project.<br/><br/><br/>The project involves fundamental research on and development of new algorithms for nonlinear, coupled systems of partial differential equations with algebraic constraints. The target system to be addressed is immiscible, two-phase subsurface flow, which is used for numerical simulation of the movement of underground fluids.  The project emphasizes the highly nonlinear physical processes of flow and transport, and how they influence each other. The objectives of the project are: (1) Improved numerical algorithms of mixed type for approximation of nonlinear flow within a multi-scale, heterogeneous porous medium when coupled to a transport model; (2) Development of Eulerian-Lagrangian Weighted Essentially Non-Oscillatory numerical algorithms for multi-dimensional, nonlinear transport when coupled to a nonlinear flow model; (3) Demonstration of the effectiveness of the algorithms in specific applications such as petroleum production, groundwater management, and carbon sequestration; and (4) Education and training of two graduate and two undergraduate students in an interdisciplinary setting. The project is expected to result in significantly better numerical approximations of subsurface flow and transport, even over very long time periods.  Project success will be seen by the development of numerical algorithms that preserve mass locally, produce little to no overshoots or undershoots, exhibit low levels of numerical diffusion, and are of high order and accurate on coarse computational meshes.  They will require a significantly relaxed CFL time-step restriction for stability.  Most importantly, the methods will be shown to work well for the nonlinearly coupled system, in that the two systems will complement each other in terms of accuracy and efficiency."
"1418973","Weak Galerkin Modeling of Wave Scattering and Propagation in Dispersive Media","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","12/04/2015","Lin Mu","MI","Michigan State University","Continuing Grant","Leland Jameson","08/31/2015","$26,049.00","","linmu@uga.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","9263","$0.00","The propagation and scattering of electromagnetic waves in dispersive and inhomogeneous media are of essential importance in microwave, THz (tera-hertz) and light-wave technologies. The effects of dispersive dielectrics, such as biological tissues, rocks, soil, snow, ice, plasma, optical fibers, and radar-absorbing materials, must be taken into account in the design of electromagnetic devices. The understanding of these effects leads to new devices with unique characteristics.  Due to geometric complexity, interface discontinuities, solution singularity and time-varying nature of the solution, the development of efficient and accurate numerical methods for solving dispersive media problems remains a challenging issue in computational mathematics. <br/>   <br/>The goal of this proposal is to develop a new efficient and accurate numerical method for modeling, analysis, and simulation of electromagnetic wave problems in dispersive and inhomogeneous media.  In this project, the investigator will construct stable and reliable formulations based on the weak-Galerkin finite element method for handling the complex geometry, interface discontinuity and low regularity solution, and then extend the analysis to time dependent problems.  New mathematical schemes will be developed based on dispersive media models and complicated coupling of field components at the dielectric interface. The proposed approaches will be applied to a number of realistic problems, including microwave breast imaging, ground penetrating radar and double negative metamaterial.  Advanced computational tools developed in this project, including those for constructing efficient numerical formulation, handling complex geometries and interface discontinuities will lead to a major advance in mathematical modeling and computation of wave scattering and propagation in dispersive media."
"1418843","Upscaling and multilevel methods for three dimensional elasticity via element agglomeration","DMS","COMPUTATIONAL MATHEMATICS","09/15/2014","07/15/2016","Ludmil Zikatanov","PA","Pennsylvania State Univ University Park","Continuing Grant","Leland Jameson","08/31/2017","$180,000.00","Ilya Lashuk, Panayot Vassilevski","ltz1@psu.edu","201 OLD MAIN","UNIVERSITY PARK","PA","168021503","8148651372","MPS","1271","8396, 8609, 9263","$0.00","This project focuses on the integration of recent theoretical and algorithmic advances in numerical models, which describe the behavior of elastic materials on multiple scales, exhibiting stochastic behavior. The project will include algorithmic design, convergence and complexity analysis, as well as issues that arise in the performance of the upscaling and multilevel algorithms in realistic simulations. The proposed research: (1) aids the development of new and robust methods for upscaling that provide reliable calculations and predictions in structural mechanics; (2) supports the migration of such methods into real-life scientific and engineering simulations; and (3) engages the broader scientific community through research and educational activities, highlighting the integrated approach in numerical modeling of elastic materials from adaptive discretizations to robust solvers and back.<br/><br/>The proposed research aims to improve understanding of the interplay between the techniques from differential geometry and topology, which lead to discretizations compatible with the geometric and topological structures inherited from the physical/mathematical model. Based on this, the PIs plan to develop agglomeration methods that offer provable optimal algorithm performance.  The novel efficient and accurate upscaling techniques for elasticity problems have potential applications in material sciences and geosciences. In addition, accurate coarse discretizations yield efficient multilevel solvers for the linear systems coming from corresponding discretizations of linear elasticity. Such solvers enable simulations with finer spatial resolution and/or reduce the necessary computational resources for such simulations.  Finally, the design of upscaling techniques has many similarities with the design of discretizations in general. The success of the project will facilitate accurate discretization schemes and robust solvers for linear elasticity equations based on element agglomeration."
"1359889","Third University of Florida SIAM Gators Conference, March 27-29, 2014","DMS","COMPUTATIONAL MATHEMATICS, MATHEMATICAL BIOLOGY","03/15/2014","03/12/2014","William Hager","FL","University of Florida","Standard Grant","Leland Jameson","02/29/2016","$15,300.00","Yunmei Chen, Maia Martcheva, Scott McKinley","hager@ufl.edu","1523 UNION RD RM 207","GAINESVILLE","FL","326111941","3523923516","MPS","1271, 7334","7556, 9263","$0.00","Third University of Florida SIAM Gators Conference, March 27-29, 2014<br/><br/>A three day student conference will be hosted by the student chapter of the Society for Industrial and Applied Mathematics (SIAM) in the Mathematics Department of the University of Florida.  The conference will be organized by students with faculty acting as advisors. There will be 6 keynote talks given by invited professors, while 21 talks will be given by students. The conference will have 3 focus areas: numerical optimization and applications, mathematical modeling and algorithms in imaging, and mathematical biology and modeling. The conference will showcase the latest computational techniques in the fields of optimization and imaging, as well as state-of-the art models in biomathematics. Students will have the opportunity to meet experts in their own and related fields, and will be able to network with other students in the focus areas of the conference. The student organizers of the conference will develop organizational skills, and skills in interacting and communicating with other researchers.<br/><br/>The conference will help students from the three interrelated fields of numerical optimization, imaging sciences, and mathematical biology achieve a broader view of their research and their field as they learn from and interact with researchers who work in application fields such as medical imaging technology and disease modeling and treatment. The conference will plant the seeds for future research projects. By networking with other students, a foundation for future collaborations will be established. The abstracts of the presented papers will be publicly available on the conference web site.<br/><br/>Conference web site:  siam.math.ufl.edu/conference/"
"1418959","Numerical Methods for Multiscale Modeling of Nano-Optics","DMS","COMPUTATIONAL MATHEMATICS","07/15/2014","08/25/2016","Di Liu","MI","Michigan State University","Continuing Grant","Leland Jameson","06/30/2018","$280,000.00","","richardl@math.msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","9263","$0.00","The main objective of the proposed research is to study robust, efficient and accurate numerical methods for multiscale and multiphysical models of nanoscale optical devices. Nanoscale experimental and manufacturing technologies have proven very successful with huge potentials in biomedical engineering and engineering of new materials. Quantitative modeling and simulation will not only help to understand the mechanism of nano devices, but will also greatly reduce the costs by minimizing trial errors and optimizing performance. The PI will collaborate with chemists and material scientists on photon driven nano devices and attosciences. Methods developed for nano-optics will shed light on a wide variety of nanoscale problems. The mathematical analysis of the self-consistent multiscale methods will bring new insights into the field of multiscale modeling and computation. Junior collaborators and graduate students will receive cross disciplinary training on new frontiers of applied mathematics, and will be better prepared to solve realistic problems using a combination of modeling, analysis and computations.<br/><br/>To fully understand photon driven nano and micro devices, the PI will derive multiscale models for interactions between the electromagnetic field, electronic excitations and molecular motions by combining classical electrodynamics, time dependent current density functional theory and Ehrenfest molecular dynamics in the linear response regime. The self-consistent multiscale scheme will be adopted to solve the system efficiently. Metal enhancement will also be investigated. To accurately simulate long time propagations of strong field processes involving nonlinear higher order interactions, the PI will adopt adaptive numerical strategies. Multiscale schemes will be designed for the concurrent simulation of electrodynamics and quantum mechanical processes in the time domain. The PI will also conduct a thorough analysis of the algorithms in terms of convergence and stability."
"1418804","A Practical Approach to Rothe's Method:  Method of Lines Transpose","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","09/09/2015","Andrew Christlieb","MI","Michigan State University","Continuing Grant","Leland Jameson","07/31/2018","$205,000.00","","christli@msu.edu","426 AUDITORIUM RD RM 2","EAST LANSING","MI","488242600","5173555040","MPS","1271","9263","$0.00","Many important equations in Physics, Chemistry and Materials Science exhibit a range of critical time scales. Generally, behaviors break up into a range of fast, medium, and slow time scales. Take for example the process of casting a polymer membrane that undergoes phase separation during the casting process. Polymers of this type include fuel cell membranes, separators in modern batteries, and polymer based solar cells. The fastest time scale is the spinodal region (an initial coursing process during casting) which might be pico to micro seconds. Then comes a longer transient phase, which is on the order of tens of minutes to hours. This transient is when the system settles down and the membrane starts to take its final form. The overall casting process can take days to complete, dictating how well the membrane will work. The state of the art for modeling these processes is to use direct simulation of casting by modeling the individual atoms in the system using molecular dynamics. However, even on the biggest super computers, the best methods can only simulate hundreds of pico seconds. To obtain accurate models of these systems over these time scales, a new class of models, functionalized Cahn Hilliard, was proposed. However, this type of model is very challenging to solve, requiring both temporal and spatial accuracy over a wide range of scales. To accommodate this model, we are developing a new class of numerical methods which take advantage of the multi-core computing revolution.  If successful, this new class of numerical methods will facilitate rapid simulations of problems we could only experimentally interrogate in the past. The overall goal in developing this new class of numerical methods for challenging models of this nature is to move the process of design away from an Edisonian approach to one of thoughtful design process. With reliable numerical tools, the process of design can be greatly enhanced. A key example of this is the materials and wing design of the new Boeing 777 aircraft, which was designed primarily through computer simulation.<br/><br/>This proposal centers on the development of O(N), semi-analytic, high order, implicit solvers based on the method of lines transpose, otherwise known as Rothe's method, for a large class of PDEs. The methods are motivated by the PIs work on developing an A-Stable to all orders in time implicit method for acoustic problems with a variable wave speed. The method starts by discretizing the PDE in time, then solving the resulting non-oscillatory Helmholtz equation using a fast summation methodology, i.e., we use the free space Green's function to invert the operators followed by applying a boundary integral to correct the free space solution. To generate high order solutions, a new approach based on successive convolution is introduced. The proposal centers on the extension of the core algorithm, based on successive convolution, to a wide class of linear and non-linear PDEs.  A novel method for multi-level domain decomposition (DD) is presented. The DD method offers a possible  path for developing scalable versions of the algorithm for distributed multi-core platforms."
"1438811","The Tenth Mississippi State Conference on Differential Equations and Computational Simulations","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","04/15/2014","04/11/2014","Hyeona Lim","MS","Mississippi State University","Standard Grant","Michael Steuerwalt","03/31/2015","$35,000.00","Jerome Goddard, Suzanne Shontz","hlim@math.msstate.edu","245 BARR AVE","MISSISSIPPI STATE","MS","39762","6623257404","MPS","1266, 1271","7556, 9150","$0.00","This award supports participation in the ""Tenth Mississippi State Conference on Differential Equations and Computational Simulations,"" which provides a forum where mathematicians, engineers, and scientists from academia and industry exchange research and education ideas involving theoretical, applied, and computational developments in differential equations and associated simulations.  The conference will be held October 23-25, 2014 on the campus of the Mississippi State University. The website for this conference is http://www.ccs.msstate.edu/deconf/de2014.<br/> <br/>The conference is a collaborative endeavor between the Department of Mathematics and Statistics, and the Center for Computational Sciences (CCS), a High Performance Computing Collaboratory (HPCC) member center, at Mississippi State University. The majority of the grant funds will be used to provide travel support to graduate students and recent doctoral recipients. The conference will provide these future research leaders an opportunity to present their work, to meet and collaborate with other researchers, educators, and practitioners, and to learn about recent developments in this interdisciplinary field."
"1419028","Numerical Approximation of Joint Spectral Radius by Lower Rank Matrix Sets","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","08/22/2014","MingQing Xiao","IL","Southern Illinois University at Carbondale","Standard Grant","Leland Jameson","08/31/2018","$90,000.00","Jianhong Xu","mxiao@math.siu.edu","900 S NORMAL AVE","CARBONDALE","IL","629014302","6184534540","MPS","1271","9263","$0.00","This research program originates from a broad interdisciplinary study, combining mathematical and computational machineries to address a long-standing problem in the field of computational science, i.e., to approximate the joint spectral radius in an efficient way. Different from current approaches, in this project, the PIs propose an innovative approach via matrix rank decomposition. The main idea is to reduce the computational complexity through rank decomposition methodology so that desirable precision can be achieved. The proposed work will provide much needed common ground between theoretical study and computational approximation of joint spectral radius, and the research findings would effectively improve the existing approaches in literature, which in turn will lead to a valuable impact on applied sciences, such as system design and control in engineering and other related fields, as well as to better understandings of some long-standing open problems in mathematics. This project will provide mentorship and training for graduate and undergraduate students. Professional development of the students will be enhanced through mentoring activities which include broad training in mathematics and numerical analysis, guidance in oral and written communication, advice on career development, and dissemination of project results at national/international conferences.<br/><br/>The goal of this research project is to establish error bounds and to develop efficient algorithms for the computation and approximation of joint spectral radius by using lower rank matrix sets. The PIs will pay particular attention to the singular value decomposition, although other types of decompositions will also be explored, such as the rank decomposition from row echelon form obtained from Gaussian elimination. By using the concept of symmetric gauge function and its subdifferential, the joint spectral radius will be approximated through rank decompositions and the error bounds will be estimated by using generic vector norms. The proposed procedure for seeking a desirable gauge function is based on the rank approximation, allowing the minimization of its value at each step. Numerical algorithms will be established and are expected to provide effective computational tools to be applied to engineering and related fields."
"1413859","Computational Differential and Difference Algebra, a special session at the Applications of Computer Algebra 2014 Conference, July 9 - 12, 2014.","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS","06/01/2014","04/11/2014","Alexey Ovchinnikov","NY","CUNY Queens College","Standard Grant","Andrew Pollington","05/31/2015","$19,000.00","","aovchinnikov@qc.cuny.edu","6530 KISSENA BLVD","FLUSHING","NY","113671575","7189975400","MPS","1264, 1271","7556","$0.00","The special session ""Computational Differential and Difference Algebra"" will be part of the conference Applications of Computer Algebra (ACA) 2014 to be held at Fordham University, July 9-12, 2014.  Th subject of differential and difference algebra concerns the study of differential and difference equations from an algebraic point of view.  These types of equations have many applications to real life problems, including biology (population dynamics and cellular biology), chemical reactions, and physics.   The aim of the special session is to bring together researchers who develop theory for these types of equations and those who develop algorithms for working with them.<br/><br/>The annual ACA conference series started in 1995. The aim of this special session is to bring together researchers who develop theory and algebraic algorithms that solve or simplify systems of linear and non-linear differential and difference equations. This group includes researchers in differential and difference Galois theories, differential and difference elimination, symbolic computation in commutative algebra and algebraic geometry, and model theory. This special session will foster the interaction of researchers in differential and difference algebra with researchers in model theory, algebraic geometry, and commutative algebra at a new level. The schedule will be designed to make this interaction particularly convenient and will include time for informal discussion. Moreover, special attention will be paid to mentoring the participants in the special session who are young investigators. In addition to the talks in the conference format, it is planned to have several longer lectures to foster in-depth discussions.  Additional information can be found at the website of the special session: http://qcpages.qc.cuny.edu/~aovchinnikov/ACA2014/index.html"
"1418889","Numerical Methods for Graph and Network Analysis","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","07/15/2014","07/08/2014","Michele Benzi","GA","Emory University","Standard Grant","Leland Jameson","06/30/2017","$179,999.00","","mbenzi@emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1271, 8069","7433, 8084, 9263","$0.00","Network science, the study of large, interconnected complex systems, has been the focus of much effort on the part of many scientists in recent years. The modeling and analysis of natural, engineered, and social networks, ranging from molecular and biological networks to the Internet, the World Wide Web, and also to online social media like Facebook and Twitter, has emerged as an important multidisciplinary field of study. Progress in this area is rapidly affecting many traditional fields of science and engineering as well as areas like public health, business, marketing, finance, and national security. The project concerns the development and analysis of computational methods for studying the structure of networks and of certain dynamical processes taking place on them. In particular, the project will develop mathematically sound approaches for maximally increasing the connectivity and robustness of networks when only a limited number of links can be added or modified. Network robustness is of paramount importance when designing infrastructure networks, since real world networks must be resilient to damage resulting from natural disasters or by man-made, malicious attacks. In addition, the project will develop efficient techniques for simulating the spread of information, viruses, rumors, etc. on networks, as well as the propagation of shocks. The techniques will be implemented on computers and tested on existing available data sets, and the software will be made available to other researchers.  <br/><br/>In the first part of the project we will design efficient algorithms for increasing network robustness and connectivity using as an objective function the total communicability of  the network, which is a scalar quantity associated with the adjacency matrix of the graph. In particular, we will investigate different criteria for the addition of links in such a way as to (approximately) maximize the increase in total communicability. Edge rewiring and deletion will also be studied. The second part of the project deals with the numerical analysis of so-called Quantum Graphs. We will study numerical methods, including finite element discretizations and fast linear algebra, for solving PDEs posed on metric graphs (1D simplicial complexes), such as the diffusion and Schroedinger equations. In particular, we will study domain decomposition and iterative substructuring methods for solving the large linear systems arising when solving elliptic and parabolic problems on graphs. Exponential integrators based on Krylov subspace methods will also be investigated."
"1419060","Hierarchical model reduction techniques for incompressible fluid dynamics and fluid-structure interaction problems","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","06/29/2014","Alessandro Veneziani","GA","Emory University","Standard Grant","Leland Jameson","06/30/2018","$248,384.00","","ale@mathcs.emory.edu","201 DOWMAN DR","ATLANTA","GA","303221061","4047272503","MPS","1271","8396, 8609, 9263","$0.00","Networks perfused by fluids are found in several engineering applications, ranging from hydrogeology, oil distribution, and internal combustion engines to hemodynamics. A quantitative analysis of these problems is of utmost interest for understanding fluid dynamics in the network, for predicting effects of local changes on the network (for instance, the effects of a surgical operation over the fluid dynamics in the arterial tree), and for optimizing flow distribution. Mathematical description and numerical approximation of these problems are challenging when coupling the accurate description of local dynamics with the large scale of the network. This proposal investigates a novel numerical method to undertake the quantitative analysis of fluid dynamics in complex networks called HiMod (Hierarchical Model Reduction). The primary (but not exclusive) application is the physiopathology of the arterial system, including in the mathematical model up to almost 2000 segments of the network. Several specific properties of this method need to be investigated for its development and engineering.  The research provides a graduate student the opportunity of working on advanced mathematical and numerical techniques - including theoretical as well as practical aspects - in a truly interdisciplinary framework with frequent contacts with engineers and doctors expected to be the end users of these methodologies.<br/><br/>Network of pipes are often modeled by assembling simplified equations describing each segment, like the well known Euler equations. Originally proposed for blood flow (incompressible fluid in compliant pipes) they have been extensively used in gas dynamics - for instance - in internal combustion engines (compressible fluid in rigid pipes). These equations are the result of several approximations to reduce the fully 3D mathematical model to a 1D set of hyperbolic equations. Unfortunately, this model reduction prevents proper  capture the local features of the network that affects the global dynamics. The  HiMod approach moves from a different  perspective. We couple different numerical approximation techniques along the mainstream and the transversal directions. We use a finite element approximation for the mainstream and a spectral or modal approximation transversally. The number of modes can be locally and adaptively tuned to get the best possible trade-off between accuracy and computational efficiency. The rationale is that a relatively small number of modes is enough to guarantee good accuracy for the transversal dynamics, leading to a system of 1D problems (called a ""psychologically 1D"" model). Moving from preliminary promising studies for advection-diffusion problems, in this proposal we aim at developing the method for the 3D incompressible Navier-Stokes equations and fluid-structure interaction problems. Inf-sup stability and accuracy of the HiMod discretization as well as its role as preconditioner of the full problem will be investigated, together with adaptive techniques for the appropriate (automatic) selection of the transversal modes."
"1418947","Multiscale domain decomposition methods for flow and mechanics problems","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","07/14/2016","Ivan Yotov","PA","University of Pittsburgh","Continuing Grant","Leland Jameson","06/30/2018","$359,999.00","","yotov@math.pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1271","8396, 8609, 9263","$0.00","A computational framework will be developed for modeling interactions of different physical phenomena. It will be applied to geoscience and biomedical problems of societal importance.  Coupling subsurface and surface flow and transport will be investigated to model interactions between contaminated aquifers, rivers, lakes, and wetlands. Flows in fractured and deformable reservoirs will be modeled to provide improved understanding and predictive simulations of important processes occurring in hydraulic fracturing and carbon sequestration, including surface subsidence, pore collapse,  cavity generation, and wellbore collapse.  Another application of interest is flow in arteries, accounting for flow within the arterial wall. This has an effect on the blood velocity in the lumen and the speed of the pressure wave, as well as low density lipoproteins (LDL) transport and drugs filtered into the tissue during coronary artery flow. We expect the research on modeling arterial flows to lead to the development of optimized simulation tools which will advance drug delivery as well prevention, detection, and therapy of cardiovascular diseases.  Educational activities will be integrated with and enhanced by research activities. Graduate students and postdocs will participate actively in research projects through research working groups or dissertation work. State-of-the-art research results will be incorporated into the curriculum.  <br/><br/>The primary objective of this work is to develop a computational framework for modeling multiphysics systems of coupled flow and mechanics problems with multiscale input parameters. The research approach is based on a multiblock domain decomposition methodology. The simulation domain is decomposed into a union of subdomains, each one associated with a physical, mathematical, and numerical model. Physically meaningful interface conditions are imposed on the discrete level via mortar finite elements. The formulation provides great flexibility for multiphysics and multinumerics couplings. Furthermore, this domain decomposition approach, combined with coarse scale mortar elements, provides a multiscale approximation and an efficient way to solve the coarse grid  problem in parallel. The project will develop 1) Mathematically rigorous and physically meaningful multiphysics models; 2) Robust, accurate and efficient multiscale discretization techniques; 3) Efficient multiscale parallel domain decomposition solvers and preconditioners. The computational framework will be applied to geoscience and biomedical problems.  We will develop variational formulations of systems of partial differential equations coupling free and porous media fluid flows with deformations of the porous solids. These formulations will couple through physically meaningful interface conditions free fluid models such as Stokes, Brinkman, or Navier-Stokes  equations with single phase or multiphase Darcy flow.  In regions involving deformable porous media  the Darcy flow will be coupled with elasticity and modeled by the Biot system of poroelasticity. We will  study well posedness of the variational formulations. We will develop stable and accurate multiscale  mortar discretization methods for these multiphysics variational formulations. We will employ suitable  mixed finite element, finite volume, and discontinuous Galerkin methods for the discretization of the  subdomain equations on a fine scale. A mortar finite element space will be utilized to impose interface  conditions on a coarse scale. We will carry out a priori multiscale error analysis for these methods. We  will also develop efficient parallel non-overlapping domain decomposition algorithms for the solution of  the resulting algebraic systems by reducing the coupled global multiscale problem to a coarse scale  interface problem. We will analyze the condition number of the interface operator and will develop  efficient preconditioners for speeding up the interface iteration."
"1419020","Discontinuous Galerkin Schemes for Fluid, Kinetic, and Multiscale Fluid/Kinetic Models in Plasma Physics Applications","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","08/23/2014","James Rossmanith","IA","Iowa State University","Standard Grant","Leland Jameson","08/31/2016","$71,364.00","","rossmani@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1271","8396, 9150, 9263","$0.00","Plasma physics is the study of the dynamics of ionized gases. Because plasma, often referred to as the fourth state of matter, is the most abundant form of ordinary matter in the universe, there exist many important application problems for which an understanding would be advantageous. The motivating application for this research is high-energy particle acceleration using laser-plasma interactions. The production of high-energy particle beams is important in the process of creating synchrotron radiation, which is used for small scale imaging in many application areas including materials science, biology, and medicine. Laser-plasma acceleration is a mechanism for generating high-energy electrons by hitting a plasma with a specifically-tuned ultra-short laser beam. This approach provides the possibility to create high-energy particles with a device much smaller than standard particle accelerators. The big challenge with this approach is to be able to generate a relativistic electron beam with a small energy spread. The objective of this research is to develop highly accurate and efficient computational methods for simulating the acceleration of high-energy electrons. This research aims to develop novel computational techniques for so-called 'kinetic models' of plasma that achieve accuracy and robustness by exactly conserving certain important properties (e.g., mass and energy conservation and positivity of the solution). These methods will be implemented in computer code that will take advantage of modern computer architectures. The resulting methods will be used to simulate various scenarios of laser-plasma interactions with the aim to elucidate the mechanisms of electron acceleration and the features of the produced electron bream.<br/><br/>The primary objective of this research is to develop accurate and efficient computational methods for solving nonlinear partial differential equations used to model plasma dynamics.  The mathematical models considered in this work arise from an important application problem:  laser plasma accelerators, which provide a promising mechanism for generating high-energy electrons by hitting a plasma with a specifically-tuned ultra-short laser beam. The research will focus on developing efficient high-order discontinuous Galerkin schemes for kinetic Vlasov systems. The two main challenges that will be addressed are the development of (1) efficient time-stepping procedures that allow for time-steps that are dictated by physically important time scales and (2) high-order discretizations that numerically conserve important physical properties such as mass, momentum, and energy, as well as properties such as the positivity of the solution. One approach that will be explored in this research in order to improve the efficiency of the kinetic Vlasov numerical methods is the coupling of kinetic Vlasov solvers to fluid solvers to obtain a multiscale fluid/kinetic method. Part of the work will be done in collaboration with an experimental physicists, Prof. Dr. Malte Kaluza (Jena), who will help guide the mathematical modeling in order to keep the models relevant to the important physics of  laser-plasma accelerators. Correspondingly, the resulting computational results will be used to influence laser-plasma experiments. The software that will result from this research project, along with proper<br/>documentation, will be made freely available on the web as part of the open-source package DoGPack."
"1418728","Algebraic Structures in Optimization","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","06/29/2014","Rekha Thomas","WA","University of Washington","Standard Grant","Leland Jameson","06/30/2018","$210,001.00","","thomas@math.washington.edu","4333 BROOKLYN AVE NE","SEATTLE","WA","981950001","2065434043","MPS","1271","9263","$0.00","The reconstruction of 3D-scenes from noisy camera images is a fundamental problem in computer vision. In this project, these reconstructions are approached as problems in polynomial optimization, which is not the usual practice in computer vision. By understanding the mathematical structure of these problems, we aim to advance theoretical understanding of these problems and to develop practical and efficient algorithms for 3D reconstruction.  This project creates an unusual opportunity to apply methods from optimization, algebraic geometry and computational algebra to applied problems that are both mathematically rich and practically important. A second component of this project seeks to develop the theory of cone factorizations of nonnegative matrices with a special emphasis on positive semi-definite factorizations. This new concept has many potential applications to diverse areas such as optimization, data compression, machine learning and statistics. Both projects involve graduate students and their training in a number of inter-disciplinary mathematical areas.  <br/><br/>The main project outlined in this proposal is a multi-year effort to understand the algebraic geometric foundations of 3D-reconstruction problems in computer vision.  This process involves two steps -- the first is to identify the constraints and the second to solve these problems efficiently using methods from convex optimization. The first step requires methods from computational algebra and classical algebraic geometry and the second step relies on techniques from optimization, real algebraic geometry and convex geometry. These problems also provide numerous fascinating applications of (computational) representation theory, multi-linear algebra, and geometry, and the two-way exchange will enrich both mathematics and computer vision. Students will be trained in this cross-disciplinary work and several researchers from mathematics, optimization, and computer vision will be involved in this project. The project on cone factorizations builds on a currently booming area at the interface of mathematics and computer science. Developed originally by the PI and collaborators for the sake of understanding extended formulations of convex sets, these factorizations have much further potential and applications. The plan is to develop the structural and mathematical theory of cone factorizations with the aim of better understanding extended formulations and other potential applications."
"1418908","Simulation of the Wave-Matter Interactions: Geometrical Optics and Nano Optics","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","07/22/2014","Songting Luo","IA","Iowa State University","Standard Grant","Leland Jameson","07/31/2017","$109,234.00","","luos@iastate.edu","1350 BEARDSHEAR HALL","AMES","IA","500112103","5152945225","MPS","1271","9150, 9263","$0.00","Broader Significance and Importance:  The proposed project is devoted to providing reliable models and the state-of-the-art numerical methods for studying the wave-matter interactions as in geometrical optics and nano optics with practical applications arising from seismic imaging, medical imaging, nanotechnology, nanosciences, and so on. The proposed models and numerical methods are based on knowledge of geometrical optics, physical optics, nano optics, computational chemistry and scientific computing, aiming to provide numerical tools and guidance for applications.  Interdisciplinary collaborations will be pursued to extend the practical applications of the proposed methods.  The project also involves the integration of research and education in computational mathematics. Graduate and undergraduate students, and members from underrepresented  groups will be encouraged to participate in the project  to enhance their knowledge and research. The proposed project will further the training and education of students and encourage them to pursue  future career in science, technology, engineering and mathematics (STEM).  <br/>  <br/>The PI will develop reliable models and efficient numerical methods to simulate the wave-matter interactions as in geometrical optics and nano optics.  When the size of the matter is much larger than the wavelength, the interactions are equivalent to simulating high frequency waves in nonhomogeneous media with the  effect of matter averaged and embedded continuously as medium constants.  The PI will combine geometrical optics  and physical optics along with scientific computing to design  and analyze efficient numerical methods.  When the  size of the matter reaches nanoscale, the interactions  require to simulate the motion of the matter and the  evolution of  the waves simultaneously as in the study of the optical  responses of nanostructures in nano optics,  where the quantum  effects of the matter must be considered. The PI will use the  semi-classical theories as the  guideline to derive simple models  to characterize the interactions  and propose multiscale methods to  simulate  the interactions.   The proposed models and methods  consist of  the following core ideas: (1) for simulating high  frequency waves, the  Huygens-Kirchhoff integral with  the Green functions serves as the formulation and mechanism for  advancing the  wave propagation, where geometrical optics provides  asymptotic approximations of the Green functions. The phase  and  amplitude to approximate the Green functions can be computed  efficiently with accurate methods for  Hamilton-Jacobi equations. The oscillatory Huygens-Kirchhoff integral can be evaluated  efficiently with  multilevel algorithms based on low-rank matrix decompositions.  And data compression techniques to compress  the phase and  amplitude ensure the feasibility of building the waves in  both two and three-dimensional spaces.  And  (2) for studying the optical responses of nanostructures,  the semi-classical theories that treat the waves  classically  with the Maxwell equations and retain the quantum mechanical description of the matter provide  characterizations  of the light-matter interactions. The Born-Oppenheimer  approximation and the ab initio molecular  dynamics  can be utilized  to resolve the difficulty of  dealing with the many-body system quantum mechanically,  which  results in simple semi-classical models that are numerically trackable with proposed multiscale schemes."
"1418422","Shape and data analysis using computational differential geometry","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","07/01/2014","06/23/2014","Hong-Kai Zhao","CA","University of California-Irvine","Standard Grant","Christopher Stark","12/31/2018","$328,860.00","","zhao@math.duke.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1271, 8069","7433, 8084, 9263","$0.00","Due to rapid and vast data acquisition by modern technology, experiments and computer simulations, computational and data analytical approaches have become increasingly important for many important applications in science and engineering. An important but challenging task in data representation, analysis and understanding is to extract information of interest, which includes intrinsic geometric features and global structures from large data set. The aim of this project is to introduce advanced mathematical theories and analytical tools in differential geometry and translate them into efficient computation algorithms for data analysis.<br/><br/>The main effort of this project is to develop new mathematical models and computational tools for a few important but challenging tasks in 3D modeling and data analysis in higher dimensions. These models and tools will be utilized to extract and characterize geometric structures for data analysis in various applications. In particular, quasi-conformal map and conformal structure will be used for representation and analysis for surface maps and shape modeling. Intrinsic geometric differential operators, such as Laplace-Beltrami operator and its eigen-system, will be explored for point cloud analysis and manifold learning in high dimensions. Models, methods and computational tools developed in this project will be tested on benchmark data sets as well as real applications. Interdisciplinary applications and collaborations with computer scientists and statisticians will also be pursued."
"1418812","Geometric Methods for Graph Partitioning","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","08/06/2014","Braxton Osting","CA","University of California-Los Angeles","Standard Grant","Leland Jameson","10/31/2014","$79,000.00","Dominique Zosso","osting@math.utah.edu","10889 WILSHIRE BLVD STE 700","LOS ANGELES","CA","900244201","3107940102","MPS","1271","9263","$0.00","The proposed activity is to develop and analyze new computational methods for a graph partitioning problem based on the Beltrami energy. This problem has diverse applications in machine learning and image analysis (medical, satellite, and material). For example, a clear need for such methods in imaging has been identified by collaborators at the California NanoSystems Institute, where the proposed work will directly impact fundamental research in nano science and foster the understanding of amyloid beta sheets. Such amyloids are associated with the pathology of more than 20 serious human diseases including Alzheimer's and other neurodegenerative diseases. Thanks to the multidisciplinary nature of the activity, awareness and literacy outside one's field will also be mutually fostered for all involved parties.<br/><br/>The PIs, together with their students and collaborators, will seek new methods that combine variational arguments with ideas from geometry and partial differential equations in order to extend and overcome the limitations of existing methods. The research has three goals. The first goal concerns fundamental theoretical questions raised by the proposed model: analyze the existence, uniqueness, and properties of the minimizers of the variational problems; establish a generalized isoperimetric inequality related to the Beltrami functional in the continuum; and explore relations to existing theorems and conjectures about optimal partitions. A graph analogue of the Beltrami energy is formulated and is the foundation for a graph partitioning objective. The PIs have identified a relaxation of this objective and propose an in-depth study of a provably convergent rearrangement method for its solution. The second goal addresses the important computational and numerical aspects of the proposed graph partitioning model. An efficient optimization strategy is key for the framework to be usable in practical applications. Here, promising primal-dual methods from convex optimization will be utilized, as well as other competitive state-of-the-art methods. To develop the most efficient solution method, it will be crucial to explore the similarities to other models, including those for non-negative matrix factorization and those related to motion by mean curvature. The third goal is to address concrete, real-world problems and to engage the developed algorithms in practical applications of societal importance."
"1418934","Multigrid Methods for a Class of Saddle Point Problems","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","07/06/2016","Long Chen","CA","University of California-Irvine","Continuing Grant","Leland Jameson","07/31/2018","$204,996.00","","chenlong@math.uci.edu","160 ALDRICH HALL","IRVINE","CA","926970001","9498247295","MPS","1271","8396, 8609, 9263","$0.00","The fast multigrid methods developed and studied in this work are expected to have a broader impact on the numerical solutions of a large class of practical problems. Important applications include: vector (Hodge) Laplacian, Maxwell equations, Stokes equations, Oseen and Navier-Stokes equations, and Magnetohydrodynamics (MHD) etc. MHD, in particular, has important applications in the development of fusion technology and casting processes. In these applications, since no experimentation is nowadays possible, the numerical simulation of the corresponding partial differential equations is indispensable. These simulations are very challenging, requiring large computational resources. The multigrid solvers developed in this project offer the potential for increasingly accurate models to be solved. In addition, our improvements in algorithm developments will have impact on many other areas, such as image processing, and computer graphics.<br/><br/>This project is divided into two parts: algorithmic development and theoretical analysis. For the algorithmic development, multigrid solvers will be developed for mixed finite element discretization based on Finite Element Exterior Calculus (FEEC). In our study, effective smoothers, which are the key of multigrid methods, will be developed by using existing effective preconditioners or splitting schemes. One such example is a distributive smoother proposed in this project which is highly related to the well-known projection methods used in computational fluid dynamic. In addition to the algorithmic development, a more completed convergence theory of multigrid methods for saddle point problems will be developed. This theory aims to relax the strong regularity assumption in existing work. Consequently our theory can be applied to more realistic problems especially for solutions with singularities. Our theoretical investigation will also provide insight for the algorithmic development, e.g., the construction of approximated distributive smoothers and Schwarz smoothers, and optimal choice of relaxation parameters used in several smoothers."
"1419069","Implicit sampling methods and their applications","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","08/24/2016","Xuemin Tu","KS","University of Kansas Center for Research Inc","Continuing Grant","Leland Jameson","08/31/2018","$287,063.00","","xtu@math.ku.edu","2385 IRVING HILL RD","LAWRENCE","KS","660457552","7858643441","MPS","1271","8396, 8609, 9150, 9263","$0.00","Data assimilation incorporates the observations, which can be real-time data, into a  computational model of a real system.  The output of this process is the adjusted states of the system based on both computational model and the observations. These adjusted states are better than those that could be obtained using just the data or model alone. Data assimilation is required in many fields such as  statistical signal processing, oceanography, meteorology, hydrology, geosciences, econometrics, and finance. Due to the large-scale and nonlinear properties of the models in those  applications, commonly used methods rely on unrealistic assumptions. The PI and her collaborators develop an efficient data assimilation method without those unrealistic assumptions. As one example, successful application of this method to reservoir history matching will greatly benefit the reservoir management.  This project involves undergraduate and graduate students. The PI  has outreach for successful participation of underrepresented group in STEM-related disciplines.<br/><br/>Data-driven computations, such as data assimilation, need to identify the state of a system and/or unknown parameters in the system from an uncertain model supplemented by a stream of noisy and incomplete data. The Bayesian framework is a standard approach for such problems and it involves characterizing the posterior distribution of the state and/or parameters in terms of given prior distribution and data. Commonly used methods, like ensemble Kalman filter-type and variational methods, rely on assumptions of Gaussianity or near Gaussianity. By contrast, the implicit sampling methods obtain high qualify samples of the posterior density by using importance sampling with good proposal density and can be applied to more general non-Gaussian situations. These samples are independent and focus on the high probability regions. The first step in the implicit sampling methods usually requires solving an optimization problem, which is the most time-consuming part of the methods. The proposed research is to develop and analyze preconditioners using domain decomposition methods, a widely-used paradigm for parallel computation, combined with efficient nonlinear solvers to accelerate this procedure and make it suitable for high performance computation. The PI and her collaborators apply these newly developed implicit sampling methods to data assimilation and uncertainty quantification in subsurface flow applications including reservoir history matching."
"1438161","International Workshop on Recent Developments in the Adaptive Solution of PDEs, August 17-22, 2014","DMS","COMPUTATIONAL MATHEMATICS","07/15/2014","07/12/2014","Weizhang Huang","KS","University of Kansas Center for Research Inc","Standard Grant","Leland Jameson","06/30/2015","$6,400.00","","whuang@ku.edu","2385 IRVING HILL RD","LAWRENCE","KS","660457552","7858643441","MPS","1271","7556, 8396, 8609, 9150, 9263","$0.00","This award  provides support to defray expenses of US-based participants of the international workshop ""Recent Developments in Adaptive Methods for Partial Differential Equations"" to be held on August 17-22 in 2014, on the campus of Memorial University of Newfoundland, in St. John's, Newfoundland, Canada.  Funds are directed towards students, junior investigators, and researchers from groups under-represented in the sciences to attend the workshop.  The workshop is motivated by the increased interaction and collaboration between computational mathematicians and applied researchers in engineering and science.  Driven by the need for robust and efficient numerical simulations, this workshop strives to bring together researchers currently involved in numerical simulations for real problems with experts in adaptivity.  The workshop consists of three distinct but uniquely connected components.  The first is a two day short course on adaptive methods for partial differential equations.  The second is research level talks given by experts in adaptivity and applied researchers with a need for adaptive simulation techniques.  The last is breakout interactive sessions coupling short course participants, adaptivity experts and applied researchers. <br/><br/>The workshop will facilitate greater interaction between researchers in adaptive methods for partial differential equations and applied researchers in engineering and science. Moreover, the workshop will provide great opportunities for graduate students, postdoctoral fellows, and young researchers to acquaint themselves with research frontiers, present their own work, and interact with experts in the field. Furthermore, the workshop will provide strong motivation for interdisciplinary collaboration between computational mathematicians and applied researchers in dynamical systems, Earth sciences, and medical imaging. Finally, the workshop will play an active role in motivating mathematics and strengthening applied mathematics groups at universities in the region.<br/><br/>Workshop web site:  www.math.mun.ca/anasc/workshop.html"
"1419164","Pacific Northwest Numerical Analysis Seminar 2014, October 18, 2014","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","07/29/2014","Bin Jiang","OR","Portland State University","Standard Grant","Junping Wang","07/31/2015","$8,000.00","Dacian Daescu, Jeffrey Ovall, Jay Gopalakrishnan","bjiang@pdx.edu","1600 SW 4TH AVE","PORTLAND","OR","972015522","5037259900","MPS","1271","7556, 8396, 8609, 9263","$0.00","The Pacific Northwest Numerical Analysis Seminar (PNWNAS) will be held at Portland State University in Portland, Oregon on October 18, 2014. PNWNAS has been an annual event since 1987, held on a Saturday each fall in the northwestern states of USA and nearby Canadian provinces. The goal of this meeting is to bring together researchers from universities, government research labs, and industry from the Pacific Northwest region to share expertise in the field of numerical analysis and computational mathematics. This conference will enable computational scientists in the region to hear, consult, and possibly collaborate with the outside speakers and members of other institutions in disciplinary or interdisciplinary research areas. Meanwhile, it will also facilitate interactions that lead to employment and internship opportunities for postdocs, undergraduates, graduate students and members of underrepresented groups attending the conference. <br/><br/>There will be about 70-80 participants for the 2014 seminar, with 8 invited speakers from industry, government, and academia. The talks will be mainly concerned with modeling large-scale complex systems, construction of accurate and efficient numerical schemes, energy sustainability, and software and hardware development, including emerging computer architectures. The last two topics are popular research fields in local government and industry. Poster sessions will be arranged for undergraduates, graduate students, and postdoctoral fellows to present their research work so as to maximize their visibility to all participants. Meanwhile, the inclusion of participants from national labs and industrial partners will bring new potential internship and employment opportunities for the students and postdocs attending the meeting. The seminar will be publicized through various communication channels, including Portland State University student chapter of the Society of Women Engineers, the Society of Hispanic Professional Engineers, and Pacific Northwest Louis Stokes Alliance for Minority Participation Program to broaden participation from members of those underrepresented groups. The exposure to topics and techniques not typically available at the home universities and the opportunity for face-to-face collaboration will enhance the research programs of faculty, student, and postdoc participants. <br/><br/>Seminar web site:  http://faculty.washington.edu/rjl/pnwnas/"
"1419047","Topics in Computational Dynamics","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","08/01/2014","07/15/2016","Erik Van Vleck","KS","University of Kansas Center for Research Inc","Continuing Grant","Leland Jameson","07/31/2018","$225,000.00","","erikvv@ku.edu","2385 IRVING HILL RD","LAWRENCE","KS","660457552","7858643441","MPS","1266, 1271","9150, 9263","$0.00","Models of complex systems often involve nonlinear differential equations that must be approximated numerically to gain detailed information. To understand the behavior of such approximations, this project explores similarities and differences in the dynamic behavior of the original model and what is obtained through numerical simulations. Understanding when the original model and numerical simulations yield similar dynamic behavior leads to greater confidence when making inferences from simulation results. In addition, stability analysis is useful in understanding the robustness of complex biological and physical phenomena. Of particular emphasis in this project is the application of these results to the dynamic behavior of models of climate dynamics. The techniques to be developed include approaches to dimension reduction and uncertainty quantification based upon Lyapunov exponent theory. The understanding of biological and physical processes will increase due to the computational techniques developed to address the dynamic behavior of detailed, microscopic models. <br/><br/>In many areas of science and engineering dynamical systems are employed as models of complex phenomena. The focus of this project is on approximation of solutions of nonlinear dynamical systems. In particular, the investigator and colleagues are interested in understanding the dynamics of the finite dimensional approximations and how they relate to the dynamics of the original dynamical system. The main research topics to be investigated are related to the application of time dependent orthogonal change of variables and in the dynamics of traveling waves under discretization. Specific interests include the use of Lyapunov vectors (analogues of eigenvectors in time dependent stability analysis) for dimensional reduction of dissipative nonlinear differential equations, robust stability for time dependent linear Hamiltonian systems, and techniques for analyzing time dependent stability of numerical time stepping techniques, stability of plane wave solutions to bistable reaction-diffusion equations in periodic media, the impact of time and space discretization on traveling waves, and non-planar traveling waves in discrete media. Techniques to investigate these systems combine numerical analysis and dynamical systems ideas to gain a better understanding of computational dynamics both qualitatively and quantitatively."
"1461416","Computational Intersection Theory for Infinite Dimensional Dynamical Systems","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","01/27/2015","Jason Mireles James","FL","Florida Atlantic University","Standard Grant","Rosemary Renaut","06/30/2016","$45,564.00","","jmireles@math.rutgers.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","MPS","1271","9263","$0.00","The purpose of this research is to develop mathematically rigorous computational methods for studying intersections of stable and unstable manifolds of infinite dimensional dynamical systems.  The problem splits naturally into two distinct technical challenges.  First it is necessary to extend existing methods of computational intersection theory to higher dimensions than currently accessible.  This problem will be addressed via reductions to lower dimensional slow stable invariant manifolds.  In order to study connecting dynamics it is also important to compute the linear bundles of these reduced manifolds. This requires an extension of classical Floquet theory into the slow manifold setting. The second major challenge is to develop a-posteriori techniques for proving the existence of connecting orbits in infinite dimensions. The question is: can we conclude the existence of connecting orbits in the infinite dimensional system once the existence of corresponding connections have been established in a projection of high enough finite dimension? Answering this question requires extending existing methods for studying infinite dimensional equilibria and periodic orbits to the setting of the boundary value problems which describe connecting orbits. The project will also consider the plausibility of computer assisted techniques for studying continuation with respect to parameter, as well as bifurcations of connecting orbits.<br/><br/>This research will yield new methods for insuring the correctness of scientific computations.  The focus of the project is on infinite dimensional models of applied mathematics such as partial differential equations, delay equations, and renormalization operators.  In addition to providing mathematically rigorous error bounds for approximate numerical solutions of these problems, the techniques of computer assisted proof resulting from this work are able to provide answers to theoretical questions about the global dynamics of nonlinear systems.  For example by establishing the existence of some transverse connecting orbits it is possible to prove the existence of turbulence, spatiotemporal chaos, or positive topological entropy in the phase space of a partial differential equation. Other theoretical problems which might be approached computationally once the techniques of this project become available include studying the combinatorial dynamics of renormalization operators, as well as some problems in nonlinear analysis involving the application of Floer's Homology theory. A central theme of this project is that at each stage of advancement the theoretical and computational tools developed will be applied to established problems of applied mathematics and dynamical systems theory."
"1418966","Collaborative Research:  Boundary Integral Simulations for Solvent Effects in Protein Structure and Dynamics","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","08/10/2016","Robert Krasny","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Leland Jameson","08/31/2018","$164,869.00","","krasny@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1271","9263","$0.00","Proteins are large biomolecules each consisting of a unique sequence of amino acids with a complex three-dimensional structure. Proteins perform many essential functions in living organisms, and some diseases are associated with improper protein structure. Hence there is great interest among biomedical researchers in understanding the structure, dynamics, and function of proteins. In their natural environment proteins are surrounded by water with dissolved salt; the protein/solvent interactions are critical to proper function in the organism. Laboratory experiments are used to study these protein/solvent interactions, but computer simulations are also increasingly employed to complement the experiments. The investigators will use their expertise in computational mathematics to develop improved numerical algorithms and software for computing protein/solvent interactions, with potential impact on areas such as protein folding and synthetic drug design. Several applications will be studied in collaboration with bioscientists. The software developed will be posted in open source format on a public website and will be installed in a widely distributed molecular simulation software package for use by bio-computational researchers. The project will train a postdoc and a graduate student in this important branch of scientific research. <br/><br/>The project will develop improved numerical algorithms and software for computing electrostatic solvent effects which play a key role in determining protein structure, dynamics, and function. Computing these effects is challenging, and implicit solvent models based on the Poisson-Boltzmann (PB) equation for the electrostatic potential are a popular approach to reducing the cost. However, grid-based PB simulations encounter difficulties due to the singular point charges representing the protein, the complex geometry and discontinuous dielectric constant across the molecular surface, and the unbounded computational domain. In previous NSF-supported research, the investigators developed a new treecode-accelerated boundary integral (TABI) potential solver with improved accuracy and efficiency, low memory usage, and straightforward parallelization. The current project has the following components. 1. (algorithm development) The investigators will extend the current TABI potential solver to compute the electrostatic solvation forces needed for molecular dynamics simulations. This requires careful discretization of singular integrals representing the induced charge on the molecular surface separating the low-dielectric protein domain from the high-dielectric solvent domain. 2. (parallel computing) The investigators will develop a new parallel TABI solver for graphics processing units (GPUs), taking advantage of the treecode's low memory and communication requirements. 3. (biological applications) The investigators will apply the new TABI potential solver and force driver to study solvent effects in proteins. Applications to be studied in collaboration with bioscientists include: (a) pH-dependent properties of a bactericidal lectin protein; (b) structural changes of a neurotransmitter receptor in an ionic environment which is relevant to an autoimmune disease; (c) extension of TABI to incorporate polarizable atomic multipole solutes."
"1418964","Fast high-order methods for electrohydrodynamics of vesicle suspensions","DMS","COMPUTATIONAL MATHEMATICS, PMP-Particul&MultiphaseProcess, MSPA-INTERDISCIPLINARY","07/15/2014","07/03/2014","Shravan Veerapaneni","MI","Regents of the University of Michigan - Ann Arbor","Standard Grant","Leland Jameson","06/30/2017","$216,637.00","","shravan@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1271, 1415, 7454","055E, 8007, 9263","$0.00","The objective of this project is to develop novel computational tools for simulating the electrohydrodynamics (EHD) of vesicle suspensions.  Vesicles enclose a viscous fluid and they share the same structural component of a biological cell, the bilipid membrane.  Hence, their EHD has been a paradigm for understanding how general biological cells behave under an electric field. The dynamics of this system is characterized by a competition between viscous, elastic, and electric stresses on the individual membranes and the nonlocal hydrodynamic interactions. A number of technological applications call for better understanding of EHD including targeted drug delivery, gene transfection and lab-on-a-chip design. The proposed work will enable researchers to learn how the physics at micro-scale (e.g., single particle deformation) alters the macro-scale behavior of dense suspensions (e.g., electrorheology). In addition, this project will undertake educational, mentoring, and outreach activities that are expected to have a broad impact.   <br/><br/>The mathematical model for multiple-vesicle EHD consists of the incompressible Stokes equation for the fluid surrounding the membranes combined with a far-field boundary condition and a kinematic condition at the fluid-vesicle interfaces. The classical Taylor- Melcher leaky-dielectric framework models the electric response of individual vesicles and the Helfrich energy combined with local inextensibility models their elastic response.  The proposed numerical methods will be based on boundary integral equation formulations, which avoid volume discretizations. Fast, high-order methods will be developed for solving the coupled boundary integral equations for the vesicle positions and their trans-membrane electric potentials. Computational challenges faced by the vesicle EHD are shared by other particulate flows such as drop, bubble and capsule flows. Therefore, the methods proposed here could more generally be applied to several other problems of practical importance. This award by the Computational Mathematics Program of the Division of Mathematical Sciences is co-funded by the Particulate and Multiphase Processes program in the CBET Division of the ENG Directorate."
"1417053","Computational methods for materials science, high frequency wave propagation, and quantum mechanics","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","09/09/2016","Peter Smereka","MI","Regents of the University of Michigan - Ann Arbor","Continuing Grant","Leland Jameson","06/30/2019","$353,035.00","","psmereka@umich.edu","1109 GEDDES AVE, SUITE 3300","ANN ARBOR","MI","481091079","7347636438","MPS","1271","8396, 8609, 9263","$0.00","This project involves modeling, simulation, and efficient computation of technologically important and intellectually interesting problems. In the area of material science the research investigates two new off-lattice methods for the computational modeling of the evolution of crystalline materials in which the lattice structure may change. This is needed when defect formation is important, for example. The project will also address the efficient computation of high frequency wave propagation in nonhomogeneous materials. This classic problem in computational science and applied mathematics has attracted much attention because it is useful for problems in geophysics and optics and it is a mathematically rich problem requiring insight into the behavior of wave equation. The PI also plans to develop numerical methods for studying quantum dynamics of molecules. The importance of quantum mechanics in understanding chemical reactions cannot be overstated, and this research offers the potential of providing detailed insight and predictive power for a variety of photo-induced chemical reactions. The project also involves training of graduate students.<br/><br/>The first project involves developement of off-lattice computation models for the evolution of crystalline materials.  In one case, the PI plans to formulate an off-lattice kinetic Monte Carlo method for the simulation of epitaxial growth and in other an alternative to the phase field crystal method is proposed.  In both cases the underlying energy of the system is a suitable intermolecular potential.  In addition, in both cases one will be computing on time scales orders of magnitude larger than molecular dynamics. Another project involves the computation of high frequency solutions of the wave equation. The goal is to exploit the asymptotic structure of high frequency solutions to develop efficient numerical methods that do not have the errors that result from asymptotic approximations that occur when one uses methods that rely on geometrical optics or Gaussian beams. The PI also plans to develop numerical methods for the studying quantum dynamics of molecules when conical intersections play an important role. A full quantum mechanical treatment would mean solving the Schroedinger equation in 20 to 30 dimensions even for small molecules. Our plan is to identify the important modes by using surface hopping methods as a probe of the energy landscape. A full quantum treatment can then be accurately obtained just using this smaller set of modes."
"1419105","Collaborative Research:  Construction, Analysis, Implementation and Application of New Efficient Exponential Integrators","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","09/01/2014","08/24/2017","Mayya Tokman","CA","University of California - Merced","Continuing Grant","Leland Jameson","08/31/2018","$249,977.00","","mtokman@ucmerced.edu","5200 N LAKE RD","MERCED","CA","953435001","2092012039","MPS","1271, 8069","7433, 8084, 9263","$0.00","As the scale and complexity of scientific and engineering problems grow, computer simulations become a necessary and integral part of the vast majority of research endeavors. An ability to create a computer model of a process under investigation, whether it comes from physics, economics, biology or some other field, provides not only significant cost savings for a study, but also brings insights inaccessible through experimental procedures alone. The growing complexity with which we describe phenomena of interest requires increasingly more sophisticated computer models. In particular, it is important to be able to simulate many complex processes over very long times, which is a computationally intensive and challenging task. This project is focused on developing new computational methods that allow simulating and studying time evolving phenomena from a wide range of scientific and engineering disciplines over long time intervals of interest.  The mathematical and computer tools created during this project will enable researchers to study problems at a scale and complexity not possible with currently available computational tools.<br/><br/>This project will advance the state-of-the-art in both the theory and practice of time discretization methods. In the course of the project a complete theoretical framework and high performance implementations of the new generation of exponential time integrators will be developed. The new methods will significantly improve computational efficiency of numerical models in many important areas of science and engineering, and will enable simulations at a scale and complexity that are not currently possible. The research will advance core numerical analysis through the development and study of new classes of exponential propagation iterative (EPI) methods such as split, hybrid, partitioned, and Krylov-based techniques. In addition, specialized efficient schemes will be designed and optimized for a wide range of problems. The theoretical work will be complemented by the creation of a mathematical software package that will provide high quality implementations of the most efficient exponential integrators to the broad scientific community."
"1418692","Efficient, Adaptive, and Convergent Numerical Methods for Phase Field and Phase Field Crystal Equations with Applications","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","08/29/2016","Steven Wise","TN","University of Tennessee Knoxville","Continuing Grant","Leland Jameson","08/31/2018","$105,000.00","","swise1@utk.edu","201 ANDY HOLT TOWER","KNOXVILLE","TN","379960001","8659743466","MPS","1271","8396, 8609, 9150, 9263","$0.00","This research focuses on the design and practical application of efficient computational methods for two classes of mathematical models. The first class is comprised of the phase field crystal equation, the dynamic density functional theory equation, and several related models. Broadly, these equations describe crystal growth and dynamics in various materials science applications, modeling  solidification; grain boundary dynamics; crack propagation; and vacancy transport to name a few.  The second class is comprised of models that describe two-phase fluid flows.  A prototypical two-phase fluid system is oil and vinegar; the constituent fluids do not mix and can form complex multi-phase structures that are impossible to predict without numerical simulation. Specifically, the principal investigator shall develop efficient computational methods for Cahn-Hillard-Navier-Stokes and related equations that model two-phase flows in various technologically important contexts. Such models can be used to describe micro-fluids; tumor growth; two-phase polymer flows used to create organic photovoltaic devices; and flows in porous media such as encountered in oil and natural gas recovery. The physical phenomena describe technologically important phenomena in several scientific areas. Gaining a more thorough physical understanding of the modeled processes through scientific computing is fundamental to designing better cancer treatments; faster, more reliable semiconducting and photovoltaic devices; and more durable polymer components, to mention a few applications. Through this research project, graduate students will be exposed to cutting-edge scientific computing technologies, to traditional techniques of rigorous mathematical analysis, and to applications in real-world problems of materials fabrication. <br/><br/>The principal investigator shall conduct some computational and modeling work for Phase Field Crystal-type equations in materials science and for the Cahn-Hillard-Navier-Stokes-type equations for two-phase flows.  The equations under study are coupled systems of highly nonlinear, high-order partial differential and integro-partial differential equations.  Because of this, designing efficient and reliable numerical methods that give rise to convergent approximations is a non-trivial task. This project will design unconditionally energy stable, 1st and 2nd-order-in-time approximations based on the convex splitting framework. PI will rigorously prove that some of the schemes are optimally convergent. PI will implement optimally or nearly-optimally efficient solvers that take advantage of the variational structure of the proposed schemes. With his materials science collaborators, the PI will develop new PFC-type models to describe complex phase transformations in novel materials for energy applications. The work will present some new ideas for implementing and rigorously analyzing efficient and stable approximation schemes for the equations. The overarching objectives of the project are to design stable and convergent schemes for the proposed models, to implement efficient solvers for the schemes, and to apply these to real-world phenomena in materials and fluids science. The PI will leverage his experience designing fast adaptive multigrid solvers, using finite element and finite difference methods in space, and energy stable convex-splitting methods to discretize time. The codes that the PI develops, including improved versions of the PI's BSAM software package and new planned adaptive finite element packages, will be made publicly available through the PI's website, to the benefit of the wider scientific community."
"1418805","Applications and development of finite element exterior calculus","DMS","COMPUTATIONAL MATHEMATICS","07/15/2014","05/24/2016","Douglas Arnold","MN","University of Minnesota-Twin Cities","Continuing Grant","Leland Jameson","06/30/2017","$387,387.00","","arnold@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1271","8396, 8609, 9263","$0.00","Computer simulation of physical systems involving the deformation of solid materials, the motion of fluids, electromagnetism, and other phenomena is applied in countless ways every day in areas as varied as geophysics, medicine, and civil engineering.  Once a physical system has been modeled by a system of mathematical equations, successful simulation depends not only on powerful computer hardware but also on mathematical algorithms that can harness the computer's high speed to obtain accurate solutions of the model's equations.  While such algorithms exist for many important physical systems -- and moreover have been certified by mathematical analysis so that we can have confidence in the results -- there remains substantial room for improvement, with large potential payoffs.  Even more important is the need to develop accurate, fast, and certifiable algorithms for important applications for which they do not yet exist.  This project focuses on a new approach to the development and analysis of computational algorithms for simulation that has in recent years achieved great success for simulations involving the deformation of solid materials ranging from auto bodies to bones.  A primary goal of the project is to increase the range of systems that can be simulated accurately and confidently.  One emphasis will be on complex materials that combine solid and fluid aspects together, such as the tissue in the human brain, or the saturated subsurface soil and sand in which groundwater flows.  A second emphasis will be on the simulation of gravity on an astrophysical scale, which is at the heart of a new class of astronomical observatories.<br/><br/>The Principal Investigator will devise, improve, and validate algorithms for the computer simulation of complex physical phenomena modeled by partial differential equations. The algorithms to be developed and studied are finite element methods, which are an indispensable tool for simulation of a wide variety of phenomena in science and engineering, with the tremendous asset that they not only provide a methodology to develop numerical algorithms for simulation, but also a theoretical framework in which to assess the accuracy of computed solutions, and thus the possibility to develop validated methods. The present work will be based on a theory called finite element exterior calculus, initiated by the Principal Investigator and developed over the past decade, which has greatly enhanced our understanding of finite element methods and extended the range of problems for which validated finite element methods can be used.  A major direction of the research will be the extension of newly discovered methods for linearly elastic materials to more complex materials such as nonlinear, viscoelastic, and poroelastic materials. A second major direction will be the development of finite element methods suited to the Einstein equations of numerical relativity. In a third direction the Principal Investigator will participate as the computational scientist in a mathematics/physics theoretical/computational team seeking to elucidate the important but poorly understood phenomenon of localization of eigenfunctions in disordered media. Although this phenomenon, known as Anderson localization, was discovered 50 years ago in Nobel prize winning research, we still lack the understanding needed to accurately predict and control it."
"1418386","Novel Paradigms in Geometric Modeling of Large and High-Dimensional Data Sets","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","07/15/2014","07/11/2014","Gilad Lerman","MN","University of Minnesota-Twin Cities","Standard Grant","Christopher Stark","06/30/2018","$250,000.00","","lerman@umn.edu","200 OAK ST SE","MINNEAPOLIS","MN","554552009","6126245599","MPS","1271, 8069","7433, 8084, 8396, 8609, 9263","$0.00","The principal investigator and his collaborators aim to develop effective data modeling paradigms that are sufficiently simple for statistical inference. Current scientific investigations, as well as industrial applications, produce and rely on massive, high-dimensional and possibly corrupted data sets. A major focus of applied mathematicians and statisticians in this area has been on quantitative geometric data modeling. In order to effectively analyze large data and obtain meaningful statistical inference, the underlying geometric models need to be sufficiently simple. The proposal suggests mathematical paradigms for such effective geometric models. It plans to develop rigorous mathematical theory for these paradigms combined with carefully designed numerical strategies addressing specific and important applications. Despite the recent progress in this area, there are many open directions, several of which this research project addresses.<br/><br/>More specifically, the proposal focuses on several important directions of geometric data modeling. One direction aims to address modern issues in single robust subspace modeling with respect to new paradigms of learning and computation that have hardly been addressed so far in this setting. Another direction will explore important issues in modeling data by multiple subspaces or manifolds with new paradigms and perspectives. The proposal will also emphasize specific paradigms of low-rank and sparse modeling, which are induced by important applications, such as approximate nearest subspace for object recognition, improved feature tracking, structure from motion in computer vision, and sparse modeling in the atmospheric sciences."
"1418853","Theory and Implementation of Novel Numerical Methods for Equations with Singularities","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","07/29/2014","Hengguang Li","MI","Wayne State University","Standard Grant","Leland Jameson","08/31/2018","$152,805.00","","li@wayne.edu","5700 CASS AVE STE 4900","DETROIT","MI","482023692","3135772424","MPS","1271","9263","$0.00","Elliptic partial differential equations are essential mathematical models in various scientific disciplines, and the development of methods for approximating solutions of these equations has been a central focus of computational mathematics. The performance of numerical methods in general depends on the smoothness of the solutions under study. Quite common in practical applications, singularities in the solution can severely deteriorate the efficacy of the numerical approximation. Addressing major concerns in scientific computations, the study of finite element methods for singular solutions has led to many effective algorithms, but most of them are for two-dimensional singular problems. The area of finite element approximations for three-dimensional singular solutions is much less explored and much more challenging. Due to the anisotropic multiscale character of the singularity and the complexity of three-dimensional geometry, the existing methods are complicated and difficult to implement and are still missing some critical pieces of theoretical analysis. This project will significantly improve the effectiveness of existing numerical simulations in many areas where multi-dimensional computations are essential. These areas include aircraft design in aerospace engineering, crack propagation in mechanical engineering, elastography in medical imaging, Black-Scholes models in finance, modeling of fluids and of electromagnetic fields, and computation for the Schrdinger equation in quantum mechanics. <br/><br/>In this project, the PI proposes a systematic research on finite element methods (FEMs) for singular solutions of elliptic PDEs, especially in 3D. Targeting fundamental theoretical and numerical issues, this research has two main components. (I) Innovative numerical advancements: the development of new 3D meshing algorithms. Simple, explicit, and well structured, these meshes can effectively capture the local behavior of the singular solution and lead to optimal FEMs. (II) Rigorous theoretical investigations: (1) sharp regularity estimates in new function spaces; (2) sharp error analysis in energy and non-energy norms on both 2D graded meshes and the proposed 3D meshes; (3) fast multigrid-based numerical solvers on these meshes; (4) a-posteriori estimates on the proposed 3D meshes and extensions to other 3D PDEs with singularities. Pushing forward the frontier of the FEMs for 3D singular solutions, this proposed research will bring new ideas to the development of novel numerical algorithms for 3D PDEs with broader applications. In addition, the sharp non-energy error estimates and a-posteriori analysis can provide theoretical justifications for nonlinear models and optimization control problems."
"1416742","Mathematical study and finite element simulation of wave propagation in metamaterials","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","07/15/2016","Jichun Li","NV","University of Nevada Las Vegas","Continuing Grant","Leland Jameson","08/31/2018","$149,880.00","","jichun.li@unlv.edu","4505 S MARYLAND PKWY","LAS VEGAS","NV","891549900","7028951357","MPS","1271","9150, 9263","$0.00","Since 2000, the study of metamaterials has attracted the attention of many scientists and engineers. Wave propagation in metamaterials is of interest in various disciplines and could potentially revolutionize design of efficient antennas, waveguides and radars, nanolithography and subwavelength imaging, near field control and manipulation (useful for detecting low levels of chemical and biological agents, and manipulation of molecules), particle detection, and invisibility cloaking (useful for stealth technology). Developing robust and efficient algorithms for modeling metamaterials will benefit diverse areas such as electrical engineering, materials science, optics, physics, nano-technology, and biomedical technology. <br/><br/>The goal of this project is to study problems of wave interaction with metamaterials through mathematical modeling and computer simulations. Mathematical modeling plays an important role in the design and application of metamaterials. In October 2006 using computer simulations, a group of researchers at Duke University found a way to fabricate the metamaterials to build a 2-D ""invisibility cloak"" that makes an object invisible to certain frequencies. However, most metamaterial models proposed by engineers are not well studied and there is a lack of solid mathematical analysis and modeling. Metamaterials are lossy and dispersive, which leads to governing equations which are much more complicated than the well-studied Maxwell?s equations in free space. Approximating solutions of the metamaterial equations accurately and efficiently is challenging (note that all unknowns are objects in three dimensional spaces and vary in time) and requires additional research. This project will develop mathematically robust, accurate, and efficient, finite element methods that can be be used for simulating wave interactions with metamaterials. During the proposed project the PI will train graduate students interested in pursuing careers in computational sciences and engineering."
"1418806","Advanced Modeling, Numerical Studies and Analysis of Fluid-Structure Interaction Problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","06/16/2016","Pengtao Sun","NV","University of Nevada Las Vegas","Continuing Grant","Leland Jameson","08/31/2018","$132,219.00","","pengtao.sun@unlv.edu","4505 S MARYLAND PKWY","LAS VEGAS","NV","891549900","7028951357","MPS","1271","8396, 8609, 9150, 9263","$0.00","The purpose of this project is to develop advanced modeling and novel numerical techniques in order to effectively perform stable, precise, and state of the art simulations for a type of dynamic fluid-structure interaction (FSI) problem with a possibly large rotational and deformable elastic structure. Coupled fluid-structure problems, which are characterized by the interaction of fluid forces and structural deformations/rotations, play prominent roles in many scientific and engineering fields such as aerodynamics, bio-engineering and hydrodynamics. Yet, a comprehensive study of such problems remains a challenge due to their strongly nonlinear coupling and multidisciplinary nature, so a correct mathematical model equation to precisely demonstrate the basic characteristics of FSI becomes more important. For most FSI problems, analytical solutions to the model equations are impossible to obtain, whereas laboratory experiments are also limited in scope; thus to investigate the fundamental physics involved in the complex interaction between fluids and solids, mathematical modeling and numerical simulations become more necessary and promising. Because of the complexity of the underlying mathematical model of FSI problems, current solution techniques are still far from being satisfactory, and therefore more efficient and robust numerical techniques are urgently needed.<br/><br/>While there is still a long way before such multiphysics FSI problems can be completely solved in an efficient and precise manner, this proposal will be devoted to the development of advanced modeling and novel numerical techniques for the following three methods: Arbitrary Lagrangian-Eulerian (ALE) method, fictitious domain method, and full Eulerian-phase field method. These methods still possess the problems of well-posedness, stability and/or convergence analysis, as well as a number of critical numerical difficulties caused by the mismatched grid on the interface, nonlinear coupling, and discontinuous/degenerate coefficients. The goal of this project is to address these difficulties, develop and analyze proper discretization schemes, robust iterative methods, and high performance computing techniques to solve the discretized system of FSI model in the sense of stable, fast and accurate convergence. The advanced and novel modeling and numerical techniques to be developed include: (1). A newly developed FSI model for a rotational and deformable elastic structure that is immersed in fluid, and its efficient numerical method with an ALE approach and method of characteristics (MOC). (2). A new fictitious domain method for FSI problem with incompressible fluid and compressible structure, and its well-posedness analysis. (3). A new stable implicit scheme for a full Eulerian FSI model with phase field formulation to deal with the degenerate structural momentum equation in Eulerian description.  Newly developed numerical techniques will be immediately implemented to enrich our in-house codes and to validate our FSI model with the obtained numerical solutions. It is hoped that the effective numerical techniques developed in this proposed project will result in one to two orders of magnitude of improvement over current existing FSI solvers. Industrial applications are natural outcomes of this research because of its extensive applications in industry, and the close ties of my collaborators with engineers in the fields of hydrodynamics and bio-engineering."
"1418754","Computational Methods for Stochastic Eigenvalue Problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","07/15/2016","Howard Elman","MD","University of Maryland, College Park","Continuing Grant","Leland Jameson","08/31/2017","$150,000.00","","elman@cs.umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1271","9263","$0.00","This project concerns the development of numerical algorithms for efficiently computing eigenvalues of algebraic systems of equations derived from mathematical models of physical processes. The study and understanding of eigenvalues is of fundamental importance in numerous engineering applications. Examples include the design of buildings and bridges, for which eigenvalues correspond to resonant frequencies at which structures vibrate, and identification of qualities of flowing liquids or gases that establish whether or not the flows are turbulent. The focus of the project is on solving eigenvalue problems in cases where the underlying model is stochastic.  This scenario arises when components of the model, such as elastic properties of the materials used in structures or permeabilities of the media in which flows take place, are not known with certainty but instead are treated as random variables. The resulting eigenvalue solutions are themselves also random variables. Having such solutions will enable scientists and engineers to incorporate new probabilistic methods into design, for example, by using new mathematical techniques to analyze the likelihood that a structure will buckle and devise methods to prevent it.<br/><br/>The technical approaches the PI will use in the project include the study of solutions of stochastic eigenvalue problems to develop an enhanced understanding of stability of dynamical systems and the impact of uncertainty on mathematical models.  For example, it is known that pseudospectral analysis reveals aspects of stability not shown by traditional linear stability analysis, but it is difficult to use pseudospectral  methods to make quantitative statements.  The PI expects to be able to assess the probability of a linearly stable process being unstable when pseudospectra suggest it is, and how such an assessment depends on the statistical properties of the random components of the model. He will also develop new and efficient computational algorithms for solving the eigenvalue problems, including algorithms designed for stochastic Galerkin finite element discretizations and stochastic collocation methods, and strategies for reduced-order models."
"1419040","Superconvergent post-processing of some newly developed numerical methods with weak derivatives","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","08/06/2014","Zhimin Zhang","MI","Wayne State University","Standard Grant","Leland Jameson","07/31/2018","$169,999.00","","zzhang@math.wayne.edu","5700 CASS AVE STE 4900","DETROIT","MI","482023692","3135772424","MPS","1271","9263","$0.00","There has recently been rapid development in computational mathematics due to the demands from science and engineering. Many new methods and algorithms for approximating solutions of partial differential equations have been proposed and analyzed. Compared to traditional methods such as finite element, finite difference, and finite volume methods, these newly developed methods are still in their infancy, especially with respect to post-processing techniques. Encouraged by the success of a special post-processing technique called Polynomial Preserving Recovery (PPR has been adopted by the commercial software COMSOL Multiphysics since 2008) designed by the PI and his students for finite element methods, this project is intended to develop post-processing techniques for some other newly developed numerical methods. The proposed project will not only design algorithms, but also establish a mathematical foundation for post-processing techniques under a unified framework. The proposed research has direct application to other scientific disciplines such as classical mechanics, molecular dynamics, hydrodynamics, electrodynamics, plasma physics, relativity, and astronomy. The success of the project will impact science and engineering practice as well as theoretical mathematical development.<br/> <br/>The goal of this project is to develop some robust and high accuracy post-processing algorithms and related mathematical theory for some newly developed numerical methods such as Weak Galerkin methods, Virtual Element methods, Hybridizable Discontinuous Galerkin methods, etc. Research efforts will be devoted to developing problem and method independent gradient recovery techniques for the aforementioned numerical methods. The implementation and theory of the Polynomial Preserving Recovery (PPR) for continuous finite element methods will be utilized and further developed and combined with recently developed algorithms and theory for the aforementioned numerical methods."
"1418871","Robust High-Order Methods for Wave Equations in the Time Domain","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","06/29/2014","Thomas Hagstrom","TX","Southern Methodist University","Standard Grant","Leland Jameson","06/30/2018","$399,036.00","","thagstrom@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1271","8396, 8609, 9263","$0.00","The goal of this research is to address basic issues in the development of robust and efficient computational methods for simulating waves. Problems governed by wave propagation span much of the physical phenomena we experience and play a fundamental role both in engineered systems for communication and imaging as well as naturally-occurring aspects of earth's environment. Specific examples include: wave phenomena involved with natural disasters such as earthquakes and tsunamis; environmental irritants such as acoustic pollution near airports and in cities; electromagnetic phenomena of importance to defense and civilian applications, such as radar imaging;  and applications in medicine such as the interaction of high-frequency ultrasound and tissue. This project will develop improved tools for simulating waves and will design associated general-purpose open-source software with the potential for significant impact in a range of important application areas.<br/><br/>With the staggering increases in computational power that have been and continue to be achieved, we expect to simulate more difficult and comprehensive models of physical phenomena. For wave propagation problems posed in the time domain, this means problems with many wavelengths within the computational domain involving interactions with complex geometrical features. To treat such problems efficiently requires the use of high-order discretization methods to minimize the effects of dispersion and dissipation. This work will be focused on fundamental mathematical issues required for the further development of robust, high-order wave solvers. These include: i. Development and analysis of energy-stable high-order/high-resolution discretization methods on hybrid structured-unstructured grids. Specifically we will investigate coupling high-order upwind discontinuous Galerkin methods on unstructured grids near complex boundaries and material interfaces with more efficient structured grid methods such as novel spectral element methods based on Hermite-Birkhoff interpolation (also known as jet schemes) or upwind difference methods constructed from piecewise polynomial or band-limited interpolation functions. Both first-order and second-order hyperbolic systems will be considered. ii. Development, analysis, and implementation of hp-adaptive strategies for these methods. iii. Coupling with an open-source radiation boundary condition library (expected release late 2014) containing various  implementations of complete radiation boundary conditions (CRBC). These allow a priori determination of the boundary condition parameters to guarantee any desired accuracy for isotropic, homogeneous models in the far field.  iv. Leveraging the fact that CRBCs are stable for any Friedrichs system, extend their applicability to more complex physical models including anisotropy."
"1411808","Nonlinear Multiscale Phenomena: Analysis, Control, and Computation","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","07/01/2014","08/22/2018","Ricardo Nochetto","MD","University of Maryland, College Park","Continuing Grant","Pedro Embid","06/30/2020","$994,079.00","","rhn@math.umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1266, 1271","8396, 8609, 9263","$0.00","Capturing the essential behavior of nonlinear phenomena with the simplest possible models is of paramount importance in science and engineering. This allows for understanding of basic mechanisms, the design and implementation of efficient numerical methods for simulation and control of devices, and the analysis of both models and algorithms. These crucial aspects of modern research are blended together in this research project, which deals with modeling, formulation, and numerical analysis of physical and biological phenomena at a scale where surface tension competes with bulk effects and could in principle be manipulated (or controlled) to produce scientifically interesting and practically useful dynamical behavior.  Applications of the work include nano and microtechnology (such as the design and control of micro electro-mechanical systems (MEMS)), biotechnology (such as the study of biomembranes), and high performance computing (such as the design of novel efficient numerical methods). Results of the work will enhance modeling and prediction capabilities and help educate students and postdocs in exciting, mathematically and computationally challenging, and practically relevant areas of research. <br/><br/>This project investigates models, such as biomembranes, ferrofluids, liquid crystals, and bilayer actuators, that are governed by nonlinear geometric partial differential equations defined on deformable domains that are unknown beforehand. Numerical approximation is carried out via adaptive finite element methods, with a posteriori error estimation and multilevel solvers, which allow for the resolution of problems with very disparate space-time scales with relatively modest computational resources. The project will advance understanding of adaptive approximation methods and the role of geometry in key questions concerning:<br/>1. Convergence and complexity of adaptive finite element methods (FEM) for elliptic PDE; study of fractional diffusion, hybridizable discontinuous Galerkin methods, hp-FEM and isogeometric methods, and the Laplace-Beltrami operator on parametric surfaces. <br/>2. Design of high order arbitrary Lagrangian-Eulerian methods for parabolic PDE on deformable domains and surfaces. <br/>3. Control of problems involving surface tension and magnetic effects, with or without free boundaries, relevant for device design in technology and biomedicine. <br/>4. Computational modeling and analysis of ferrofluids and liquid crystals; these are technologically useful and mathematically intriguing complex fluids which can be actuated by magnetic and electric fields, and thus manipulated and controlled for specific purposes. <br/>5. Novel FEM for geometric PDE: handling of large deformations with isometry constraints, typical of bilayer actuators, and dealing with fully nonlinear PDE."
"1356939","Modern Perspectives in Applied Mathematics: Theory and Numerics of PDEs","DMS","APPLIED MATHEMATICS, COMPUTATIONAL MATHEMATICS","01/15/2014","01/14/2014","Doron Levy","MD","University of Maryland, College Park","Standard Grant","Eugene Gartland","12/31/2015","$25,000.00","","dlevy@math.umd.edu","3112 LEE BLDG 7809 REGENTS DR","College Park","MD","207420001","3014056269","MPS","1266, 1271","7556, 9263","$0.00","The International Conference on ""Modern Perspectives in Applied Mathematics: Theory and Numerics of PDEs"" will be taking place April 28-May 2, 2014.  This conference is proposed in order to highlight the recent advances and new perspectives in applied analysis and computational mathematics, focusing on theoretical, computational and applied aspects of partial differential equations. The meeting will bring together researchers from different disciplines and provide a unique opportunity for in-depth technical discussions and exchange of ideas in all areas involving mathematical and computational sciences, modeling and simulations, as well as their applications. The conference topics are: the study of time-dependent partial differential equations including hyperbolic conservation laws and their numerical approximations, nonlinear wave patterns, image processing, kinetic PDE models in biological processes, social network dynamics, traffic and pedestrian flows, theory and numerics of interfaces.<br/><br/>This meeting will serve as a forum for researchers at different stages of their career to share experiences, exchange ideas, and explore new collaborations.  The conference will provide opportunities to promote scientific advances at the interface between mathematics and the physical sciences, life sciences, and engineering. The meeting will be used to identify new large-scale problems that require innovative mathematical approaches. The organizers will take several measures to attract young scientists. Graduate students, postdocs, and early-career professors will be encouraged to attend."
"1418957","Collaborative Research: Boundary Integral Simulations for Solvent Effects in Protein Structure and Dynamics","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","06/17/2016","Weihua Geng","TX","Southern Methodist University","Continuing Grant","Leland Jameson","08/31/2018","$114,000.00","","wgeng@smu.edu","6425 BOAZ LANE","DALLAS","TX","75205","2147684708","MPS","1271","9263","$0.00","Proteins are large biomolecules each consisting of a unique sequence of amino acids with a complex three-dimensional structure. Proteins perform many essential functions in living organisms, and some diseases are associated with improper protein structure. Hence there is great interest among biomedical researchers in understanding the structure, dynamics, and function of proteins. In their natural environment proteins are surrounded by water with dissolved salt; the protein/solvent interactions are critical to proper function in the organism. Laboratory experiments are used to study these protein/solvent interactions, but computer simulations are also increasingly employed to complement the experiments. The investigators will use their expertise in computational mathematics to develop improved numerical algorithms and software for computing protein/solvent interactions, with potential impact on areas such as protein folding and synthetic drug design. Several applications will be studied in collaboration with bioscientists. The software developed will be posted in open source format on a public website and will be installed in a widely distributed molecular simulation software package for use by bio-computational researchers. The project will train a postdoc and a graduate student in this important branch of scientific research. <br/><br/>The project will develop improved numerical algorithms and software for computing electrostatic solvent effects which play a key role in determining protein structure, dynamics, and function. Computing these effects is challenging, and implicit solvent models based on the Poisson-Boltzmann (PB) equation for the electrostatic potential are a popular approach to reducing the cost. However, grid-based PB simulations encounter difficulties due to the singular point charges representing the protein, the complex geometry and discontinuous dielectric constant across the molecular surface, and the unbounded computational domain. In previous NSF-supported research, the investigators developed a new treecode-accelerated boundary integral (TABI) potential solver with improved accuracy and efficiency, low memory usage, and straightforward parallelization. The current project has the following components. 1. (algorithm development) The investigators will extend the current TABI potential solver to compute the electrostatic solvation forces needed for molecular dynamics simulations. This requires careful discretization of singular integrals representing the induced charge on the molecular surface separating the low-dielectric protein domain from the high-dielectric solvent domain. 2. (parallel computing) The investigators will develop a new parallel TABI solver for graphics processing units (GPUs), taking advantage of the treecode's low memory and communication requirements. 3. (biological applications) The investigators will apply the new TABI potential solver and force driver to study solvent effects in proteins. Applications to be studied in collaboration with bioscientists include: (a) pH-dependent properties of a bactericidal lectin protein; (b) structural changes of a neurotransmitter receptor in an ionic environment which is relevant to an autoimmune disease; (c) extension of TABI to incorporate polarizable atomic multipole solutes."
"1341263","International Conference on Spectral and High Order Methods 2014","DMS","COMPUTATIONAL MATHEMATICS","03/01/2014","08/13/2014","Janis Morey Naugle","AZ","Arizona State University","Standard Grant","Junping Wang","02/28/2015","$35,000.00","Anne Gelb","janismn@asu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1271","7556, 9263","$0.00","This NSF grant provides funds in support of the International Conference on Spectral and High Order Methods which will be held in June 2014, in Salt Lake City, Utah. This conference series brings together researchers and practitioners with an interest in the theoretical, computational and applied aspects of high-order and spectral methods for the solution of differential equations impacting increasingly diverse and important applications. Subjects of the meeting include, but are not limited to spectral methods, high-order finite difference methods, p- and h-p type finite element methods, discontinuous Galerkin methods, ENO/WENO methods, high order methods for integral equations, wavelet-based methods, stochastic methods, efficient solvers and preconditioners for high order methods, efficient time-stepping methods, as well as computational aspects for modern hardware environments. The format of the meeting includes both invited papers as well as submitted contributions in order to encourage wider participation especially from young scientists and under-represented minorities. Target of funding to women and minorities facilitates their engagement in this important field of computational mathematics. <br/><br/>This NSF grant provides funds in support of the International Conference on Spectral and High Order Methods which will be held in June 2014, in Salt Lake City, Utah. The conference engages a broad group of researchers at all career stages representing the mathematical sciences and engineering, as well as practitioners from multiple industrial and government communities, with research interests impacting a number of important scientific domains. These include topics such as aeroacoustics, bioengineering, electromagnetics, ocean and climate modeling, seismology, turbulent flows, non-Newtonian flows, nonlinear optics, plasma dynamics, image processing and uncertainty quantification.  The format of this moderately-sized conference provides an opportunity for networking and career advancement for junior academics. NSF funding is targeted to provide fellowships for minorities and women, particularly those who are at early career levels and facilitates their exposure to important application areas for research."
"1418377","Collaborative Research:  Computational techniques for nonlinear joint inversion","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","06/05/2018","Janis Morey Naugle","AZ","Arizona State University","Standard Grant","Leland Jameson","09/30/2018","$90,764.00","","janismn@asu.edu","660 S MILL AVENUE STE 204","TEMPE","AZ","852813670","4809655479","MPS","1271","8396, 8609, 9263","$0.00","An accurate representation of the Earth's subsurface is needed to manage natural resources such as groundwater and to monitor pollutants such as those from industrial landfills.  Geophysical exploration techniques are non-invasive strategies for imaging the subsurface.  In these approaches, electric fields are induced into the subsurface and the subsequent decay response is measured.  These measurements are converted into information about the subsurface by combining them with a physical model in an inversion methodology.  It is often the case that these problems are mathematically ill-posed because the measurements and mathematical model provide inconsistent or incomplete information.  This project will provide a new method of electromagnetic geophysical characterization that combines complex resistivity and ground-penetrating radar measurements, integrating material properties across a vast range of frequency bands: 102 - 109 Hz.  This range of information will be combined in a joint inversion that offers more observational information than is traditionally used to image the subsurface.  We will accommodate inconsistent information by appropriately weighting measurements and models with experimental statistics.  The algorithms developed under this project are computationally efficient and can be used with large data sets or complex mathematical models because they are grounded in modern numerical linear algebra techniques.  <br/><br/>Regularizing solutions for ill-posed linear inverse problems have been widely studied with respect to the impact of the choice and relevant weighting of applied regularizers. Yet, in the context of the solution of ill-posed nonlinear inverse problems the impact of stabilizing a Jacobian inversion within a Newton update, which effectively regularizes the solution, appears to be less well-appreciated.  In addition, Lagrange parameters that connect one or more models and data for joint or multiple inversion, and control the relationship between components of an inversion process, may be chosen in a somewhat ad-hoc manner.  The computational cost of generating a convergent sequence of solutions in the linear framework limits serious consideration of most linear approaches in the nonlinear framework. This project transforms the solution of relevant nonlinear problems by applying techniques that appropriately include physically based modeling constraints, and choosing regularization parameters based on underlying noise statistics in data.  This methodology opens efficient avenues for incorporating uncertainty in solutions of nonlinear problems by emphasizing solution techniques that permit analysis of the propagation of intrinsic measurement and numerical error through the solution process.  Thus the underlying computational algorithms have the potential for significant impact beyond the specifics of this project."
"1347684","Copper Mountain Conferences on Iterative Methods","DMS","COMPUTATIONAL MATHEMATICS","02/01/2014","02/07/2014","Steve McCormick","AZ","Front Range Scientific Computations, Inc.","Standard Grant","Amnon J Meir","01/31/2015","$34,998.00","Thomas Manteuffel","stevem@colorado.edu","8865 E CALLE BUENA VIS","SCOTTSDALE","AZ","852558364","3035541232","MPS","1271","7556, 9263","$0.00","Copper Mountain Conference on Iterative Methods, Copper Mountain, Colorado, April 6 -- April 11, 2014. The purpose of this proposal is to support the participation of 20 students and 10 women, post docs, minorities, and disabled in the 2014 Copper Mountain Conference on Iterative Methods. Begun in 1983 and alternating between Multigrid Methods (odd-numbered years) and Iterative Methods (even-numbered years), with substantial and growing overlap in both programs, this series is an important forum for exchange of ideas in these two closely related fields. The meeting will continue to promote the established tradition of a very high level of student participation and to increase efforts to promote participation by members of groups underrepresented in the mathematical sciences. The proposal is to support travel and local expenses for these groups, to foster an egalitarian structure with no invited speakers and all talks of equal length, to continue to organize tutorials and themed evening workshops, and to encourage broad representation of participants from academia, national labs, and industry.<br/><br/>Iterative methods and multilevel algorithms are of critical importance for the development of efficient simulations in all domains of science and technology. The conference series is concerned with all aspects of these methods, including traditional techniques of convergence analysis, implementation and development of mathematical software, and use of such ideas in new settings, including advanced computer architectures and new applications. Building on previous success, the plan is to take a proactive role in recruiting students and female and minority participants. This support will have significant impact on science and technology because, for many simulations performed in these important fields, iterative solvers dominate overall calculation time and constrain capabilities. As simulations continue to grow in complexity, the need for more effective solvers becomes increasingly acute. This conference will facilitate the development of efficient iterative solvers and their dissemination to users in industry and national labs. Moreover, the continued high level of participation of students and other young scientists will create and nurture a community of researchers and practitioners who are welcomed to the field.<br/><br/>Conference web page:<br/>http://grandmaster.colorado.edu/~copper/2014/"
"1361219","FRG: Collaborative Research: Developing Mathematical Algorithms for Adaptive, Geodesic Mesh MHD for use in Astrophysics and Space Physics","DMS","COMPUTATIONAL MATHEMATICS, COMPUTATIONAL PHYSICS","07/01/2014","07/15/2016","Vladimir Florinski","AL","University of Alabama in Huntsville","Continuing Grant","Leland Jameson","06/30/2018","$124,534.00","","vaf0001@uah.edu","301 SPARKMAN DR NW","HUNTSVILLE","AL","358051911","2568242657","MPS","1271, 7244","1616, 7433, 7569, 8084, 9150, 9263","$0.00","Simulation tools for astrophysical and space physics systems share a set of common requirements ? they need to robustly simulate magnetohydrodynamic (MHD) flows around spherical bodies with high accuracy. This multidisciplinary project will develop algorithms from applied mathematics for robust, highly accurate non-relativistic MHD on geodesic meshes. In the past few years new schemes for simulating conservation laws with truly multi-dimensional divergence free approximate Riemann solvers for applications have been developed. Currently, these Riemann solvers are only available for two-dimensional rectangular structured meshes for MHD. This project will employ a geodesic mesh to provide the best possible coverage for simulations of magnetohydrodynamic flows around spherical bodies and to incorporate Delaunay triangulation to achieve high accuracy. Divergence-free formulations of vector fields can be found on these triangular meshes. <br/><br/><br/>Simulation tools for astrophysical and space physics systems share a set of common requirements ? they need to robustly simulate magnetohydrodynamic (MHD) flows around spherical bodies with high accuracy. Building a computational framework, based on shared needs in space physics and astrophysics, will unleash important synergies between these two allied fields of study. The MHD equations are a combination of the Navier-Stokes equations for fluid dynamics and Maxwell?s equations for electromagnetism.  Thus, the MHD equations require numerical solvers that incorporate the hydrodynamic fluid motion and enforce the divergence free magnetic field, i.e. no magnetic monopoles, requirements on the geometric domain approximated by a polygonal mesh. The nature of the MHD equations closely couples solution methodologies to the underlying mesh, making it necessary to develop new algorithms for the divergence-free reconstruction of the magnetic field on novel mesh structures. Additionally, the MHD system is formulated as a system of conservation laws. With a traditional conservation law, the fluxes can be evolved on a dimension-by-dimension basis. The fact that different flux components are coupled in an involution-constrained system also makes a case for multidimensional upwinding based on multidimensional Riemann solvers. Such solver strategies are again intimately coupled to the mesh structure."
"1446139","Collaborative Research: New Formulation and Algorithms for Fluid-Structure Interaction, with Application to Tumor Growth","DMS","COMPUTATIONAL MATHEMATICS","05/21/2014","06/07/2014","Jianjun Paul Tian","NM","New Mexico State University","Standard Grant","Leland Jameson","08/31/2017","$111,511.00","","jtian@nmsu.edu","1050 STEWART ST STE E1200","LAS CRUCES","NM","880038001","5756461590","MPS","1271","9263","$0.00","Fluid-structure interaction (FSI) problems play prominent roles in many scientific and engineering fields, yet an accurate study of such problems remains highly challenging due to the strong nonlinearity and multi-physics involved. Current computational theory and methods for fluid-structure interactions lack sufficient accuracy and realistic material representations, and are limited to relatively simple structural configurations. The goal of this project is to overcome such limitations by establishing a new computational mathematical framework for accurate and efficient numerical investigation of complex FSI problems, particularly for tumor growth modeling and simulation. The PIs will derive new mathematical formulation that allows the study of general FSI problems with sophisticated structural settings, develop efficient numerical algorithms that ensure high accuracy in FSI computations, and apply the proposed mathematical formulation and computational methods to the individual-cell-based modeling and simulation of tumor growth. This proposal builds on the PIs? solid background in computational and applied mathematics and significant work on tumor modeling.<br/><br/>The proposed research will improve the understanding of the nonlinear dynamics in highly complex FSI problems through a combined mathematical and computational framework. Experimental measurements will also be used for validating the numerical results. The success of this project will not only provide a solid knowledge base for advancing the current state of computational FSI study, but also enable important discoveries in the fundamental mechanism of early tumor formation and development. The project experiences and findings will strengthen the research collaboration and curriculum development in computational mathematics and mathematical biology at both Old Dominion University and the College of William and Mary. Due to the close geographical proximity between the two schools, project impacts will be maximized through convenient communication between faculty and students and through joint programs between the two schools. Education and outreach activities will involve graduate and undergraduate students in the theory, methods and application of computational mathematics."
"1418308","Positive definiteness preserving approaches for viscoelastic flow of Oldroyd-B and FENE-CR types: Applications to particulate flow","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","07/27/2014","Tsorng-whay Pan","TX","University of Houston","Standard Grant","Leland Jameson","07/31/2018","$234,212.00","Roland Glowinski","pan@math.uh.edu","4300 MARTIN LUTHER KING BLVD","HOUSTON","TX","772043067","7137435773","MPS","1271","8396, 8609, 9263","$0.00","The motion of particles in fluids is not only of fundamental theoretical interest, but is also of importance in many applications to industrial processes involving particle-laden materials. While numerical methods for simulating particle motion in Newtonian fluids have been very successful, numerically simulating particle motion in viscoelastic fluids is quite complicated and challenging. Through the computational methodologies explored in this project, efficient simulations will be performed to investigate and understand the complex fluid-particle and particle-particle dynamics in viscoelastic fluids, especially the sedimentation, migration, lift-off and resuspension of particles in three dimensional channels. The simulation tools developed in this project will have significant engineering and biomedical applications, such as proppant transport in hydraulic fracturing operations used in oil and gas wells and the elasto-inertial particle focusing for developing cost-effective labs-on-a-chip such as cell counting devices.<br/><br/>Numerical methods for simulating particle motion in viscoelastic fluids is quite complicated and challenging. One of the difficulties for simulating viscoelastic flows is the breakdown of the numerical methods. It has been widely believed that the lack of positive definiteness preserving property of the conformation tensor at the discrete level during the entire time integration is one of the reasons for the breakdown. To preserve the positive definiteness property of the conformation tensor, the constitutive equation can be reformulated as equations for the matrix logarithm of the conformation tensor to preserve the property of the positive definiteness as Fattal and Kupferman did. Another approach is to factorize the conformation tensor and then to write down the new associated equations at the discrete level, and hence the positive definiteness of the conformation tensor is forced as Lozinski and Owens did. In this project, we aim to extend and combine fictitious domain based distributed Lagrange multiplier methods, which is for simulating particle motion in fluid, with either the Lozinski and Owens' factorization approach or the log-conformation tensor approach via an operator splitting technique to preserve the positive definiteness property of the conformation tensor for simulating particle motion in viscoelastic fluids of Oldroyd-B and FENE-P types. Through the computational methodologies proposed in this project, efficient simulations will be performed to investigate and understand the complex fluid-particle and particle-particle dynamics in viscoelastic fluids, especially the sedimentation, migration, lift-off and resuspension of particles in two and three dimensional channels."
"1434212","Conference Proposal: Thirteenth International Conference on Continuum Models and Discrete Systems, July 21-25, 2014","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","06/21/2014","Kenneth Golden","UT","University of Utah","Standard Grant","Junping Wang","06/30/2015","$35,000.00","Andrej Cherkaev, Graeme Milton, Yekaterina Epshteyn","golden@math.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","MPS","1271","7556, 9150, 9263","$0.00","This project is to partially support the Thirteenth International Conference on Continuum Models and Discrete Systems (CMDS13) which is being held at the University of Utah in Salt Lake City, Utah, between July 21st and 25th, 2014.  The CMDS meeting series has been running for about 40 years, and this is the first time it is being held in the U.S.A. Having become quite influential over the years, these meetings attract leading experts and introduce young researchers to cutting edge developments. The meetings provide a unique forum for mathematicians and scientists, experimentalists and theorists, to get together and exchange ideas and results on a wide range of materials-related problems. The main focus of the CMDS13 meeting is the interplay between discrete and continuum descriptions of physical phenomena. This interplay is of fundamental importance to a range of interdisciplinary problems throughout physics, biology, materials science and engineering. These methods are crucial for the study of complex media, including composites, polycrystals such as metals and ice,  biomaterials, nano-structures, and metamaterials, as well as smart  (shape memory) alloys, optimally designed composite microstructures,  and many other related systems. The conference website is www.math.utah.edu/~cherk/CMDS13/cmds13-home.html  <br/><br/>The conference topics encompass methods for the dual discrete-continuous description of complex materials as well as several active new areas such as composites (metamaterials) with extreme or unusual properties, the dynamics of failure of complex structures, the design and optimization of structured materials, the dynamics of metastable structures with multiple   equilibria, complex fluids, models for sea ice, and multiscale computational methods. The format of the conference will consist of 11 plenary talks and about 40 invited lectures together with some contributed presentations. The meeting will foster interdisciplinary and international contacts among scientists, engineers, and applied mathematicians who would not normally   interact, thus increasing the potential for future ground breaking collaborations. The organizers will try to maximize the participation of younger researchers,from postdoctoral associates to graduate and undergraduate students, which provides career development opportunities for them. Much of the NSF support will be allocated to students, postdocs, junior researchers and participants from  underrepresented groups, including women."
"1418775","Computational Nonlinear Dynamics: Variance Reduction Methods and Numerical Studies of Large, Chaotic, and Noisy Systems","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","07/19/2014","Kevin Lin","AZ","University of Arizona","Standard Grant","Leland Jameson","07/31/2018","$219,999.00","","klin@math.arizona.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1271","9263","$0.00","Scientists and engineers increasingly depend on computational analyses of mathematical models to understand, predict, design, and control dynamic processes in physical and biological systems.  Models often contain numerical parameters whose values may vary widely, or may be poorly constrained by data; the sensitivity of model predictions to parameter variations is thus an essential practical consideration in such computational analyses.  However, exhaustive, brute-force ""parameter sweeps,"" in which one tests all possible parameters, can be computationally expensive and is sometimes simply impractical.  The proposed research concerns efficient numerical algorithms for computing sensitivities of noisy, chaotic systems to parameter variations; these systems arise in a variety of different applications, ranging from statistical physics to neuroscience.  The proposed research can potentially help researchers in these fields perform computational analyses of mathematical models more efficiently.  The proposal, combining as it does the study of numerical algorithms and their applications, is also interdisciplinary in nature and provides ample opportunities for the training of future mathematical scientists who can collaborate effectively with scientists and engineers.<br/><br/><br/>This proposal concerns the design, analysis, and application of algorithms for computing the statistical properties of nonlinear dynamical systems that are chaotic, noisy, and potentially high-dimensional.  The proposed projects aim to (i) study novel variance reduction algorithms for estimating expectation values of observables and their sensitivities for noisy chaotic systems; (ii) investigate the utility of such sensitivity estimators in computational nonlinear dynamics; (iii) extend these general algorithms to special classes of dynamical systems where the general form of the proposed algorithms may not necessarily apply.  The proposal includes plans for implementing, testing, and analyzing novel numerical algorithms, as well as applying them to specific dynamical systems.  As large, chaotic, and noisy dynamical systems occur naturally in a variety of physical and biological contexts, the proposed research is expected to produce algorithmic tools useful to practitioners in these and other fields where these types of dynamical systems arise, and will be directly applicable to a range of problems of interest to the PI, his students, and collaborators."
"1418772","Collaborative Research: Wavelet Frames for Variational Models in Imaging: Bridging Discrete and Continuum","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","08/12/2016","Kenneth T-R McLaughlin","AZ","University of Arizona","Continuing Grant","Leland Jameson","07/31/2017","$170,832.00","Leonid Kunyansky","kmclaughlin@tulane.edu","845 N PARK AVE RM 538","TUCSON","AZ","85721","5206266000","MPS","1271","9263","$0.00","From the beginning of science, visual observations have been playing important roles. Advances in computer technology have made it possible to apply some of the most sophisticated developments in mathematics and the sciences to the design and implementation of fast algorithms running on a large number of processors to process image data. As a result, image processing and analysis techniques are now applied to virtually all natural sciences and technical disciplines ranging from computer sciences and electronic engineering to biology and medical sciences; and digital images have come into everyone's life. Mathematics has been playing an important role in image and signal processing from the very beginning. There are two major mathematical approaches for image restoration, namely, wavelet tight frame approaches and differential/variational approaches. The main research objective of this project is to investigate geometric aspects of the former approach by connecting it with the latter. It will give rise to new mathematical models and numerical algorithms that benefit researchers in academia, national research laboratories, as well as in industry. The understandings of the geometric aspects of the wavelet frames and the connections with differential operators will contribute to both the community of computational harmonic analysis and the community of variational techniques and numerical PDEs. The education plan will bring undergraduate and graduate students to the frontiers of research in computational mathematics, computer vision and medical imaging; and strengthen the collaborations among mathematicians, engineers, computer scientists and medical doctors.<br/><br/>Wavelet frames are systems of functions that provide linear representations of functions living in certain function spaces such as L2(Rn). In contrast to the classic (bi)orthogonal wavelet bases, such representations are generally redundant which is desirable in many applications. Although most theoretical aspects of wavelet frames have already been well understood in the literature, geometric meanings of the wavelet frame transform are still generally unknown. In fact, the lack of geometric interpretations is one of the major flaws of wavelet frames that prohibits the applications of wavelet frames in some important problems of data analysis that require geometric regularization of the objects-of-interest reside in the data. The main research objective of this proposal is to develop a generic geometric interpretation to the wavelet frame transform, by studying its relations with differential operators within various variational frameworks. Based on the geometric interpretation, we propose new models and algorithms for several important applications such image restoration (deblurring, inpainting, CT/MR imaging, etc.). Through both theoretical analysis and numerical experiments, we will explore the advantages of the proposed wavelet frame based models over the existing variational and differential models for different applications. The proposed research will focus on: (1) the approximation of the differential operators by the wavelet frame transform within general variational frameworks; (2) solving large-scaled ill-posed inverse problems (e.g., image restoration, blind deconvolution) through convex/nonconvex optimizations using wavelet frames; (3) designing and solving wavelet frame based models in real-world applications in imaging such as low-dose CT image reconstruction, removing blurs caused by camera shaking, etc. The study of the geometric meanings of the wavelet frame transform will interpret wavelet frames and their associated optimization models from a whole new angle. Such fundamental study enables us, for the very first time, to fully utilize the unique properties of wavelet frames in geometry-involved data analysis tasks and finding numerical solutions of PDEs. The practical advantages (such as the quality of restoration for inverse problems) of wavelet frame transform over standard finite difference approximations in various applications will become more evident after the proposed studies. Furthermore, this project will also bring new understandings to numerical methods solving variational models; and answers some fundamental and important questions of variational models that are unclear from the literature."
"1418903","Maximum Entropy Closure of Boltzmann-Equation Moment-Hierarchy","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","06/29/2014","Dale Pullin","CA","California Institute of Technology","Standard Grant","Leland Jameson","06/30/2017","$268,897.00","","dpullin@caltech.edu","1200 E CALIFORNIA BLVD","PASADENA","CA","911250001","6263956219","MPS","1271","8396, 8609, 9263","$0.00","The numerical  simulation, using advanced computational platforms, of both high and low-speed flows of gases where the distance that molecules travel between molecular collisions is comparable with a body dimension   remains a challenging discipline in both aerodynamics and gas-fluid mechanics in general. Important practical examples are the motion of low-earth orbit satellites, space-vehicle re-entry into both earth and extra-terrestrial atmospheres, gas-flows that occur in inertial-confinement fusion and the fluid-dynamical behavior of nano-devices, particularly where either or both molecule-surface interactions and fluid/gas mixing are active.  The present project develops a novel methodology for the computational simulation of gas-dynamic flows of engineering interest.  This is based on consideration of generalized, aggregated gas properties known as moments, combined with ideas from the theory of probability and statistics. It is expected that this research will provide a new and viable approach to the numerical simulation of real-gas flows with improved predictive power over presently available methods. <br/><br/>The research proposes a methodology for the numerical solution of the Boltzmann equation describing the kinetic theory of gases at the mean-free path level. This is based on a new approach to the moment-closure problem, cast in terms of the Grad 13+9N-moment expansion of the Boltzmann equation, where N is an arbitrary integer. For any specific N =1,2.., a set of time-space, differential-integral equations for moments can be formulated.  These are not closed because each equation contains both moments that lie outside the set of retained time derivatives, and also collision-integral terms contain the distribution function itself. The new approach is to close the system at each time step by constructing an analytic form of the local distribution function that maximizes a standard measure of the single-particle entropy, while satisfying the known moments as a given set of constraints.  This can be done numerically as a constrained optimization problem.  The result is a local analytic form for the local  distribution function that satisfies positivity, and which then allows numerical evaluation of unclosed moments and collision terms, implementation of boundary conditions followed by an update of the retained moment equations in time.  The proposed method has been tested by comparison with a known exact solution of the relaxation in time of a given initial distribution function towards an equilibrium state.  It is proposed to apply the method to a sequence of  flows with increasing space dimensionality, including rarefied and transition Couette flow, the internal structure of shock waves and free-molecule to continuum transition flows about bodies in two and three space dimensions."
"1418771","Efficient Algorithms for Uncertainty Quantification in High Dimensions","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","07/29/2014","Dongbin Xiu","UT","University of Utah","Standard Grant","Junping Wang","10/31/2016","$224,999.00","","xiu.16@osu.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","MPS","1271","8396, 8609, 9150, 9263","$0.00","Uncertainty quantification (UQ) has become an integral part of today's scientific computing, as it is essential to the understanding of the impacts of various uncertain inputs (boundary and initial data, parameter values, geometry, etc.) to numerical predictions. UQ is thus critical to many important practical problems such as climate modeling, weather prediction, ocean dynamics, bio-chemical reactions, etc. One of the biggest challenges in UQ computations is the simulation cost, as UQ makes the traditional computations in much higher dimensional parameter spaces. For large and complex systems, the standard baseline deterministic simulations can be very time consuming, and conducing UQ simulations will further increase the simulation cost and can be prohibitively expensive. This is precisely the core issue this project intends to address and study. A novel set of highly efficient UQ algorithms will be developed to make UQ simulations amenable for large and complex systems. The new algorithms will significantly advance the current state-of-the-art of UQ methods. One prominent feature of the new algorithms is that they are designed to produce mathematically optimal UQ simulation results based on given affordable simulation capacity. While the traditional UQ methods seek to provide the smallest cost at fixed accuracy, the new methods will provide the best results at fixed affordable cost. This new feature thus makes the new algorithms ideally suited for practical UQ simulations of large and complex systems, and will have a profound impacts in various multidisciplinary fields where UQ is critical.<br/><br/>The core group of the new algorithms will be based on stochastic collation (SC). In order to provide optimal prediction under given and limited simulation capacity, this project is to develop a set of novel and efficient stochastic collocation methods, with a focus on high dimensional problems using very few number of samples. An important and  fundamental assumption is made: the number of the sample runs and the location of the samples are arbitrary and given by practitioners.  The goal is then to seek the best approximation in the potentially very high dimensional parameter space, based on the given samples.  The proposed research consists of three major approaches: (1) arbitrary interpolation type SC; (2) sparse regression type SC; and (3) multi-fidelity SC. In all cases, the number of high-fidelity simulations is assumed to be limited and given by the available simulation capacity, and methods are constructed to produce the best possible UQ simulations. All methods will be mathematically rigorous, as their constructions rely heavily on approximation theories in high dimensions; and also easy to implement, as they are the non-intrusive SC methods."
"1345013","RTG: An Interdisciplinary Research Training Program in Applied Mathematics, Computational Science, and Mathematical Physics","DMS","COMPUTATIONAL MATHEMATICS, WORKFORCE IN THE MATHEMAT SCI","08/01/2014","11/01/2022","Randolph Bank","CA","University of California-San Diego","Continuing Grant","Yuliya Gorb","07/31/2023","$1,837,677.00","Melvin Leok, David Meyer, Michael Holst, Philip Gill","rbank@ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271, 7335","7301, 9263","$0.00","This project involves a coordinated training program for post-doctoral researchers, graduate students and undergraduate students that give equal emphasis to theory, computation and application---the three cornerstones of modern research in applied mathematics. A fundamental goal of the project is to provide the students and postdoctoral researchers with the intellectual and software tools that they need to engage in cutting-edge research in either a university or industry based environment. The 5-year program is designed to fund up to 6 postdoctoral researchers, 10 graduate students, and 3 undergraduate students during the academic year and summer session.   These postdocs and students will be organized into team-based research groups, with each team being coordinated and mentored by two or more of the Investigators. The research topic of each assembled team will reflect the diverse range of areas in applied mathematics, computational and data science, and mathematical physics that are focus areas of the Investigators.  The cross-disciplinary aspects of the training program will be enhanced substantially by the newly developed interdisciplinary Computational Science, Mathematics, and Engineering (CSME) Doctoral Program at UCSD.  An RTG outreach program includes elements devoted to the training of local-area high-school teachers on the important role of computational and applied mathematics in our society. Efforts will focus on local-area high schools that provide intensive college preparatory education for low- income students.  <br/><br/>The technology produced by this award will include mathematical algorithms, analyses, and numerical software that will provide powerful tools for the exploration of multiscale models in physics, chemistry, biology, engineering and medicine.  The results will potentially impact the formulation, analysis and development of methods for critical complex multiscale and multiphysics problems that have a fundamental impact on the US economy.  RTG-sponsored students and postdocs will participate in the application of mathematical sciences to a range of research projects involving such diverse areas as undersea oil exploration, optimal power scheduling, drug design and organ modeling. Our outreach program to local-area high-schools will encourage students in under-represented groups to choose careers in applied mathematics and computational science.  At the national level, the project will make a positive contribution towards achieving the long range goal of the RTG program of expanding the work force of well-prepared US citizens, nationals and permanent residents in the mathematical sciences. In addition, the project will make an important contribution in the direction of the NSF-wide, and more generally, national priority of nurturing the crucial role of the basic  mathematical sciences in interdisciplinary research."
"1461138","Geometric Methods for Graph Partitioning","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","09/04/2014","Braxton Osting","UT","University of Utah","Standard Grant","Leland Jameson","07/31/2016","$79,000.00","","osting@math.utah.edu","201 PRESIDENTS CIR","SALT LAKE CITY","UT","841129049","8015816903","MPS","1271","9150, 9263","$0.00","The proposed activity is to develop and analyze new computational methods for a graph partitioning problem based on the Beltrami energy. This problem has diverse applications in machine learning and image analysis (medical, satellite, and material). For example, a clear need for such methods in imaging has been identified by collaborators at the California NanoSystems Institute, where the proposed work will directly impact fundamental research in nano science and foster the understanding of amyloid beta sheets. Such amyloids are associated with the pathology of more than 20 serious human diseases including Alzheimer's and other neurodegenerative diseases. Thanks to the multidisciplinary nature of the activity, awareness and literacy outside one's field will also be mutually fostered for all involved parties.<br/><br/>The PIs, together with their students and collaborators, will seek new methods that combine variational arguments with ideas from geometry and partial differential equations in order to extend and overcome the limitations of existing methods. The research has three goals. The first goal concerns fundamental theoretical questions raised by the proposed model: analyze the existence, uniqueness, and properties of the minimizers of the variational problems; establish a generalized isoperimetric inequality related to the Beltrami functional in the continuum; and explore relations to existing theorems and conjectures about optimal partitions. A graph analogue of the Beltrami energy is formulated and is the foundation for a graph partitioning objective. The PIs have identified a relaxation of this objective and propose an in-depth study of a provably convergent rearrangement method for its solution. The second goal addresses the important computational and numerical aspects of the proposed graph partitioning model. An efficient optimization strategy is key for the framework to be usable in practical applications. Here, promising primal-dual methods from convex optimization will be utilized, as well as other competitive state-of-the-art methods. To develop the most efficient solution method, it will be crucial to explore the similarities to other models, including those for non-negative matrix factorization and those related to motion by mean curvature. The third goal is to address concrete, real-world problems and to engage the developed algorithms in practical applications of societal importance."
"1417985","Semidefinite Programming Methods for Moment and Optimization Problems","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","06/29/2014","Jiawang Nie","CA","University of California-San Diego","Standard Grant","Leland Jameson","06/30/2018","$209,999.00","","njw@math.ucsd.edu","9500 GILMAN DRIVE","LA JOLLA","CA","920930021","8585344896","MPS","1271","9263","$0.00","This project targets at solving moment and optimization problems. A moment is the integral of a polynomial over a set with densities. Optimization is about making a decision so that an objective is optimized. An exemplary question is how to find the lowest valley of an area full of uneven mountains. Semidefinite programming is an efficient tool for solving moment and optimization problems, because it provides mathematically easy descriptions for complicated sets. The research results made in the project will produce plenty of mathematical methods for solving various computational problems.    <br/><br/>This project works on moment and optimization problems. A moment is the integral of a monomial with respect to a measure. Moment problems are about existences and constructions of measures satisfying some given properties. Optimization problems are about minimizing functions, typically nonlinear and nonconvex, globally over given sets. We propose semidefinite programming methods for solving these two kinds of problems. For moment problems, semidefinite programming can be applied to describe the set of moments of desired measures. For optimization problems, global optimum can be computed by minimizing linear functions in moment variables, subject to semidefinite programming constraints.  These two kinds of problems are closely connected to each other by semidefinite programming.  The main task of moment problems is to determine whether a given sequence can be represented as moments of a measure supported in a prescribed set. In optimization, we are mostly interested in computing global minimizers of nonlinear nonconvex functions. An efficient tool for unifying moment and optimization problems is semidefinite programming. The underlying mathematics includes convex geometry, duality theory, complex and real algebraic geometry, matrix theory, optimization theory, and scientific computing. The PI has expertise on the proposed subjects. Novel methods and tools for overcoming research challenges are proposed with supporting evidences. The research results produced by the project could not only make significant advances in the PI's field, but also generate novel methods for many other areas in computational mathematics.  Moment and optimization problems have broad applications in science and engineering. Typical applications include: matrix theory, computational algebra, convex algebraic geometry, tensor computations. The moment and optimization problems in such applications have their own special features and properties. Education is an important part of the project. The students will get trained by taking advanced courses as well as conducting research activities. Achievements produced by the project will be disseminated to the scientific community timely in various outlets."
"1418822","Discontinuous Petrov Galerkin (DPG) Method with Optimal Test Functions. Space-Time Formulations and Elements of Irregular Shapes","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","06/29/2014","Leszek Demkowicz","TX","University of Texas at Austin","Standard Grant","matthias gobbert","06/30/2017","$235,014.00","","leszek@oden.utexas.edu","110 INNER CAMPUS DR","AUSTIN","TX","787121139","5124716424","MPS","1271","9263","$0.00","The project focuses on the development of a new technique for simulating complicated physical and engineering processes with computers - the Discontinuous Petrov Galerkin (DPG) method.  The DPG methodology has the potential to transform existing Finite Element software at US National Labs and industry.  Without undermining the logic of existing codes, the method asks for a complete overhaul of local operations (on the element level) utilizing emerging multicore architectures. The method will have a lasting impact on software development for difficult problems arising in applications which require high accuracy simulations. Application areas targeted in this work include aerospace engineering (transonic and supersonic flows), geomechanics and bioengineering (implants, imaging of soft tissues). On the application side, the project builds on collaboration with Sandia and Argonne National Laboratories, Ben Gurion University and Boeing.<br/><br/><br/><br/><br/>The project focuses on two research directions: (a) DPG space time elements, and (b) use of elements with irregular shapes. The DPG method minimizes residuals in the dual norm corresponding to a prespecified test norm. Computation of the residual requires inversion of the Riesz operator in the test space. With the use of broken test spaces and localizable test norms, the inversion can be done element-wise using standard Galerkin and ""enriched"" spaces. With the error of inverting the Riesz operator controlled locally, i.e. on the element level, the method automatically guarantees discrete stability for any well-posed problem in the sense of classic theory of closed operators. The methodology leads to uniform stability for singular perturbation problems and, being a minimization method, does not suffer from any preasymptotic instabilities. The residual is computed rather than estimated and provides a basis for automatic adaptivity. The first focus area deals with space-time elements for convection-dominated problems with applications to compressible and incompressible Navier-Stokes (NS) equations in two space dimensions. The second line of research investigates the sensitivity of the method to element shapes with a view to using elements of arbitrary polyhedral shapes. This part of the project will be done in the context of linear elasticity and hyperelasticity in three space dimensions with applications to geomechanics and bioengineering. The proposed work includes both analysis and semi-professional software development. The project builds on existing codes and joint computational effort with Argonne and Sandia National Laboratories."
"1361209","FRG: Collaborative Research: Developing Mathematical Algorithms for Adaptive, Geodesic Mesh MHD for use in Astrophysics and Space Physics","DMS","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL MATHEMATICS","07/01/2014","06/27/2014","Katharine Gurski","DC","Howard University","Standard Grant","Leland Jameson","06/30/2018","$273,146.00","","kgurski@howard.edu","2400 6TH ST NW","WASHINGTON","DC","20059","2028064759","MPS","1253, 1271","1616, 9263","$0.00","Simulation tools for astrophysical and space physics systems share a set of common requirements: they need to robustly simulate magnetohydrodynamic (MHD) flows around spherical bodies with high accuracy. Building a computational framework, based on shared needs in space physics and astrophysics, will unleash important synergies between these two allied fields of study. The MHD equations are a combination of the Navier-Stokes equations for fluid dynamics and Maxwell's equations for electromagnetism.  Thus, the MHD equations require numerical solvers that incorporate the hydrodynamic fluid motion and enforce the divergence free magnetic field, i.e. no magnetic monopoles, requirements on the geometric domain (which is approximated by a polygonal mesh). The nature of the MHD equations closely couples solution methodologies to the underlying mesh, making it necessary to develop new algorithms for the (divergence-free) reconstruction of the magnetic field on novel mesh structures.<br/><br/>Simulation tools for astrophysical and space physics systems share a set of common requirements: they need to robustly simulate magnetohydrodynamic (MHD) flows around spherical bodies with high accuracy. This multidisciplinary project will develop algorithms from applied mathematics for robust, highly accurate non-relativistic MHD on geodesic meshes. In the past few years new schemes for simulating conservation laws with truly multi-dimensional divergence free approximate Riemann solvers for applications have been developed. Currently, these Riemann solvers are only available for two-dimensional rectangular structured meshes for MHD. This project will employ a geodesic mesh to provide the best possible coverage for simulations of magnetohydrodynamic flows around spherical bodies and to incorporate Delaunay triangulation to achieve high accuracy. Divergence-free formulations of vector fields can be found on these triangular meshes.  The MHD system is formulated as a system of conservation laws. With a traditional conservation law, the fluxes can be evolved on a dimension-by-dimension basis. The fact that different flux components are coupled in an involution-constrained system also makes a case for multidimensional upwinding based on multidimensional Riemann solvers. Such solver strategies are again intimately coupled to the mesh structure."
"1418672","Collaborative Research: Fluctuating Hydrodynamics of Suspensions of Rigid Bodies","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","06/29/2014","Neelesh Patankar","IL","Northwestern University","Standard Grant","Leland Jameson","06/30/2018","$154,846.00","","n-patankar@northwestern.edu","633 CLARK STREET","EVANSTON","IL","602080001","3125037955","MPS","1271","8396, 8609, 9263","$0.00","Over the last decade there has been rapid progress in the manufacturing and design of materials and devices that employ small-scale active particles to produce novel physical behaviours such as self-organizing flows (e.g., active colloidal suspensions), or to perform specific tasks such as cargo transport (e.g., targeted drug delivery). While much progress has been made experimentally, theoretical and computational modelling lags behind, due to the difficulty in designing suitable numerical algorithms and the lack of public-domain codes capable of capturing the complex multi-physics of active propulsion. In this work we develop novel computational methods for simulating active-particles suspended in fluid, and implement the developed techniques in the public-domain code IBAMR, therefore making them available to applied researchers in physics and engineering. A specific distinguishing aspect of the work is the consistent inclusion of the random Brownian motion necessarily present when dealing with small-scale flows due to the small numbers of molecules involved in the process. Such stochastic effects are important in flows at micro and nano scales typical of nano- and micro-fluidic and microelectromechanical devices, novel materials such as nanofluids, and biological systems such as lipid membranes, Brownian molecular motors, and nanopores. We therefore expect the work to have a broad range of applications in science and engineering, beyond the specific research goals detailed below. The scientific component of this project will be supplemented by an educational and outreach component, including the development and enrichment of new graduate courses, such as Coarse Grained Modeling of Materials, which will include training in statistical mechanics, applied stochastic analysis, fluid dynamics, and high-performance computing.<br/><br/>This collaborative project focuses on computational methods for problems involving Brownian rigid and semi-rigid structures immersed in a fluid. Examples include colloidal particles, polymer chains, and macromolecules in a solvent. We aim to develop novel methods for fluid-structure coupling at small Reynolds numbers that consistently include the effects of thermal fluctuations. At small scales, the motion of immersed structures is driven by thermal fluctuations, giving rise to Brownian motion strongly affected by hydrodynamic effects. We plan to develop methods that couple an immersed-boundary Lagrangian representation of rigid bodies to a fluctuating finite-volume fluid solver. Unlike commonly-used methods based on Green's functions, we rely on an explicit-fluid fluctuating hydrodynamics formulation in which we add a stochastic stress tensor to the usual viscous stress tensor. We will handle complex rigid (e.g., synthetic nanorods) and semi-rigid (e.g., short DNA segments) bodies by composing each structure from a collection of spherical particles constrained to move (semi)rigidly. The underlying fluctuating hydrodynamics formulation automatically ensures the correct translational and rotational Brownian motion. The novel methods developed in this project will build upon prior work by the PIs and enable simulations of the long-time diffusive (Brownian) dynamics of the immersed structures. In particular, we will develop, implement, and apply computational methods that: (1) do not employ time splitting and are thus suitable for the steady Stokes (viscous-dominated or low Reynolds number) regime; (2) strictly enforce the rigidity constraint; and, (3) ensure fluctuation-dissipation balance in the overdamped limit even in the presence of nontrivial boundary conditions."
"1418956","RUI: Transport of inertial particles in time-dependent and stochastic flows","DMS","COMPUTATIONAL MATHEMATICS","07/15/2014","04/04/2018","Eric Forgoston","NJ","Montclair State University","Standard Grant","Leland Jameson","06/30/2019","$299,987.00","Philip Yecko, Lora Billings","eric.forgoston@montclair.edu","1 NORMAL AVE","MONTCLAIR","NJ","070431624","9736556923","MPS","1271","9229, 9263","$0.00","Objects in moving fluids rarely go with the flow. Instead they may sink, swim or steer in order to reach a destination, or they may respond to other influences, including their own sizes and shapes. Taking advantage of the ways that real objects interact with flows enables a wide range of important technologies.  On small scales, micro robots may be steered inside the human body to perform surgery. On the largest scales, ocean drifters may efficiently monitor currents, marine life or global weather patterns.  Each example presents challenges originating from the complex fluid flow patterns and from the difficulty in planning the most efficient navigation strategy.  This project concentrates on the challenges of positioning autonomous vehicles in the ocean, where unpredictable and variable currents, seasonal variability, weather events, and other random influences must also be accounted for.  Computer models of fluid flows and mathematical models of control will be combined to find optimal strategies to position autonomous ocean vehicles.  Laboratory experiments will use precisely tuned fluid flows and remotely controlled particles to capture the important effects of the vehicles' mass, size, and shape.  The project payoff is significant in that a better monitored ocean is advantageous to fishing and shipping, the military, and environmental monitoring. The project will involve and support undergraduate and graduate students in leading-edge  research. Significantly, the student population at Montclair State University, and in particular, the Department of Mathematical Sciences, includes a substantial proportion who are members of groups underrepresented in STEM disciplines (including women and minorities) and the research program will leverage existing programs directed to these students. The outcome of the research will be disseminated through seminars, presentations at meetings, and publications in peer-reviewed journals. <br/><br/>In this project computer models of fluid flows and mathematical models of transport and control will be combined to find optimal control strategies for autonomous ocean vehicles, which will be modeled both as inertial and non-inertial objects.  Laboratory experiments on precisely tuned flows and magnetically controlled particles will be used both to validate and guide the investigations.  The goal is to use experimental and computed flow fields to identify critical transport features and integrate these features into control algorithms that optimally position particles.  This project will improve transport control capabilities by developing models for transport and control of inertial objects in canonical flows subject to time-dependent and stochastic perturbations.  Flow data will be generated by the numerical simulation of gyre flows, jets, and boundary currents. Inertial particles will be modeled directly using a state-of-the-art interfacial multi-phase numerical code. Laboratory experiments have been designed so that similar flows can be generated by reconfiguring the geometric forcing devices.  Through high resolution particle imaging velocimetry (PIV) and particle tracking, experimental flow fields and their transport properties will be correlated with those of the associated model flows.  Additionally, control strategies will be implemented using ferromagnetic tracer particles and magnetic pulses. To gain insight into the flow transport properties, flow and tracer data will be analyzed using a variety of geometric and probabilistic methods including finite-time Lyapunov exponents, inertial particle models, almost invariant sets, and finite-time coherent sets. These techniques will directly result in the ability to identify loitering regions and their boundaries and to determine maximal transport rates. This information will be leveraged to develop simple predictive models of transport and trajectory control that can be efficiently adapted to emergent applications."
"1418737","Collaborative Research: Wavelet Frames for Variational Models in Imaging: Bridging Discrete and Continuum","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","08/02/2016","Weiyu Xu","IA","University of Iowa","Continuing Grant","Leland Jameson","07/31/2019","$129,167.00","","weiyu-xu@uiowa.edu","105 JESSUP HALL","IOWA CITY","IA","522421316","3193352123","MPS","1271","9150, 9263","$0.00","From the beginning of science, visual observations have been playing important roles. Advances in computer technology have made it possible to apply some of the most sophisticated developments in mathematics and the sciences to the design and implementation of fast algorithms running on a large number of processors to process image data. As a result, image processing and analysis techniques are now applied to virtually all natural sciences and technical disciplines ranging from computer sciences and electronic engineering to biology and medical sciences; and digital images have come into everyone's life. Mathematics has been playing an important role in image and signal processing from the very beginning. There are two major mathematical approaches for image restoration, namely, wavelet tight frame approaches and differential/variational approaches. The main research objective of this project is to investigate geometric aspects of the former approach by connecting it with the latter. It will give rise to new mathematical models and numerical algorithms that benefit researchers in academia, national research laboratories, as well as in industry. The understandings of the geometric aspects of the wavelet frames and the connections with differential operators will contribute to both the community of computational harmonic analysis and the community of variational techniques and numerical PDEs. The education plan will bring undergraduate and graduate students to the frontiers of research in computational mathematics, computer vision and medical imaging; and strengthen the collaborations among mathematicians, engineers, computer scientists and medical doctors.<br/><br/>Wavelet frames are systems of functions that provide linear representations of functions living in certain function spaces such as L2(Rn). In contrast to the classic (bi)orthogonal wavelet bases, such representations are generally redundant which is desirable in many applications. Although most theoretical aspects of wavelet frames have already been well understood in the literature, geometric meanings of the wavelet frame transform are still generally unknown. In fact, the lack of geometric interpretations is one of the major flaws of wavelet frames that prohibits the applications of wavelet frames in some important problems of data analysis that require geometric regularization of the objects-of-interest reside in the data. The main research objective of this proposal is to develop a generic geometric interpretation to the wavelet frame transform, by studying its relations with differential operators within various variational frameworks. Based on the geometric interpretation, we propose new models and algorithms for several important applications such image restoration (deblurring, inpainting, CT/MR imaging, etc.). Through both theoretical analysis and numerical experiments, we will explore the advantages of the proposed wavelet frame based models over the existing variational and differential models for different applications. The proposed research will focus on: (1) the approximation of the differential operators by the wavelet frame transform within general variational frameworks; (2) solving large-scaled ill-posed inverse problems (e.g., image restoration, blind deconvolution) through convex/nonconvex optimizations using wavelet frames; (3) designing and solving wavelet frame based models in real-world applications in imaging such as low-dose CT image reconstruction, removing blurs caused by camera shaking, etc. The study of the geometric meanings of the wavelet frame transform will interpret wavelet frames and their associated optimization models from a whole new angle. Such fundamental study enables us, for the very first time, to fully utilize the unique properties of wavelet frames in geometry-involved data analysis tasks and finding numerical solutions of PDEs. The practical advantages (such as the quality of restoration for inverse problems) of wavelet frame transform over standard finite difference approximations in various applications will become more evident after the proposed studies. Furthermore, this project will also bring new understandings to numerical methods solving variational models; and answers some fundamental and important questions of variational models that are unclear from the literature."
"1417980","Finite element methods for non-divergence form partial differential equations and the Hamilton-Jacobi-Bellman equation","DMS","COMPUTATIONAL MATHEMATICS","07/15/2014","05/24/2016","Michael Neilan","PA","University of Pittsburgh","Continuing Grant","Leland Jameson","06/30/2017","$200,686.00","","neilan@pitt.edu","4200 FIFTH AVENUE","PITTSBURGH","PA","152600001","4126247400","MPS","1271","9263","$0.00","Many models in the sciences and engineering are solved approximately using computational methods, and it is necessary to theoretically justify the reliability of the computed approximations. In addition to providing justification of the numerical methods, the theoretical analysis often provides insight for the development of new methods with improved efficiency, accuracy, and viability. In this project, the investigator and a graduate student will construct, analyze and implement numerical methods for classes of linear and nonlinear partial differential equations arising in stochastic financial models, stochastic differential games, and other applications in finance and engineering. The overall aim of the project is to develop methods that can be implemented using current computational software and to derive explicit estimates of the approximate solutions. <br/><br/>The main goal of this project is to propose robust finite element discretizations for a class of fully nonlinear Hamilton-Jacobi-Bellman (HJB) equations and to develop a comprehensive convergence theory.  The project consists of two integrated components: (1) The development of finite element methods for second order elliptic equation in non-divergence form with non-smooth coefficients; the building-blocks of the HJB problem, (2) The construction, implementation and convergence analysis of practical finite element discretizations for the HJB problem.  The work will broaden the mathematical theory of the finite element method to problems that have been relatively untouched in the numerical community. The success of this project will have broad impacts in the mathematical and computational applications of stochastic optimal control in finance and engineering.  In addition, the impacts of this project are felt though the training of the next generation of computational scientists, course development, and dissemination to the mathematical and scientific computing communities."
"1348777","International Conference on Computational Harmonic Analysis, May 19-23, 2014","DMS","COMPUTATIONAL MATHEMATICS, ANALYSIS PROGRAM","04/01/2014","04/11/2014","Akram Aldroubi","TN","Vanderbilt University","Standard Grant","Junping Wang","03/31/2015","$29,545.00","","akram.aldroubi@vanderbilt.edu","110 21ST AVE S","NASHVILLE","TN","372032416","6153222631","MPS","1271, 1281","7556, 9150, 9263","$0.00","The International Conference on Computational Harmonic Analysis will be held in conjunction with the 29th Shanks Lecture Series, from May 19 to May 23, 2014, at Vanderbilt University, in Nashville Tennessee. This conference will play a crucial role in the development of science and technology based on computational harmonic analysis, identifying new trends, and providing an important common link to other science and engineering disciplines at large. It is intended to highlight computational mathematics and its connections with interdisciplinary research areas, and to bring researchers and graduate students together for in-depth discussions on all aspects of computational harmonic analysis.<br/><br/>This award supports the participation of junior researchers and invited speakers in the meeting. The emphasis of the conference will be on state-of-the-art developments in topics such as compressed sensing, random matrix theory, multi-fractals, time-frequency analysis, sampling and sensing networks, high-dimensional data geometry, frame theory, learning theory, mathematics of imaging, applications to signal/image processing, information technology, natural and life sciences, and engineering.<br/><br/>Conference web site:  http://www.math.vanderbilt.edu/~iccha5/"
"1363146","Constructive Functions 2014 Conference and School","DMS","COMPUTATIONAL MATHEMATICS, ANALYSIS PROGRAM","02/01/2014","12/03/2013","Douglas Hardin","TN","Vanderbilt University","Standard Grant","Edward Taylor","01/31/2015","$32,200.00","Doron Lubinsky, Brian Simanek","doug.hardin@vanderbilt.edu","110 21ST AVE S","NASHVILLE","TN","372032416","6153222631","MPS","1271, 1281","7556, 9150","$0.00","The Constructive Functions 2014 Conference and School will be a five day event dedicated to constructive function theory and related fields; it will occur May 26-30, 2014, at Vanderbilt University in Nashville, Tennessee.  The conference will focus on orthogonal polynomials, potential theory, discrete and continuous energy problems, special functions, Pad approximation, polynomial inequalities, and various problems related to optimization and efficiency.  The plenary speakers will deliver research level talks on topics such as the connection between orthogonal polynomials, operator theory, and random matrices; polynomial approximation and applications to numerical analysis and quadrature; and Riemann Hilbert problems.  Parallel sessions of non-plenary speakers will include many more topics.  Furthermore, the event will include four lectures (the school) aimed at young researchers and designed to provide an introduction to important research topics in constructive function theory.  Topics to be covered are Riemann Hilbert problems for orthogonal and multiple orthogonal polynomials, discrete minimum energy problems, orthogonal polynomials on finite gap sets, and numerical computing with functions and approximation.<br/><br/>This conference and school presents an exceptional opportunity for many researchers in constructive function theory. While a special emphasis will be placed on young researchers during the school portion of this event, the new research presented at the conference will help progress the discipline and foster communication and collaboration between researchers from around the world. An active field of research for more than a hundred years, constructive function theory has found new applications in many areas of science including the modeling of physical processes with random matrices, the modeling of geological phenomena using potential theory, and machine assisted numerical analysis.  The applicability of results in this field is intimately linked to the wide variety of subjects that it includes.  This conference and school aims to be an inclusive event that brings together researchers from all aspects of constructive function theory to share the results of their past research and explore the possibilities for future discovery."
"1418742","Rational Geometric Splines for Isogeometric Analysis","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","07/29/2014","Marian Neamtu","TN","Vanderbilt University","Standard Grant","Leland Jameson","07/31/2018","$219,897.00","","mike.neamtu@gmail.com","110 21ST AVE S","NASHVILLE","TN","372032416","6153222631","MPS","1271","8396, 8609, 9150, 9263","$0.00","Modern engineering applications, such as computer-aided design and manufacturing of aircraft and auto parts, demand a surface representation methodology that can be used with so-called unstructured meshes, which are typically required for modeling of complex geometric shapes on a digital computer. This research project will address theoretical questions concerning recently introduced parametric surfaces for approximating solutions of partial differential equations. The research involves areas critical to many industries, such as the aircraft and car manufacturing industries and others, and it has the potential to lead to significant cost savings for these industries. Although the proposed research will be conducted mainly in the context of engineering applications, the fields of computer-aided design and finite element analysis span many other disciplines, including medicine, biology, art, architecture, scientific visualization, and crude oil and natural gas exploration. The results of this project will advance scientific discovery, which may contribute to increased competitiveness of U.S.-based industries. <br/><br/>The project will address theoretical questions concerning recently introduced parametric surfaces, called RAGS - Rational Geometric Splines, and their utility in Isogeometric Analysis for approximating solutions of partial differential equations. RAGS are piecewise rational functions that can be used to model surfaces of arbitrary topology from unstructured meshes. The project will investigate the applicability of RAGS in representing finite-element spaces and their effectiveness in approximating solutions of differential equations numerically. In particular, one goal of the proposed project will be to determine whether RAGS can successfully compete with and/or complement the current technology used in Isogeometric Analysis, namely NURBS (Non Uniform Rational B-splines) and their generalization, T-splines. The objective is to develop tools for Isogeometric Analysis that are more versatile and robust than NURBS. The PI and collaborators will develop and test new methods for spline-based surfaces and the associated finite-element spaces that are mathematically sound and computationally tractable. The project will also explore how a successful synergy of disciplines, ranging from the classical approximation theory to industrial computer-aided design and engineering, can be achieved to advance the understanding of geometric models and their analysis. The research will also have an important educational component: graduate students will be exposed to interesting and relevant problems of interdisciplinary nature."
"1418677","Krylov Multigrid Methods for Eigenvalues and Linear Equations","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","07/27/2014","Ronald Morgan","TX","Baylor University","Standard Grant","Leland Jameson","07/31/2018","$180,000.00","","Ronald_Morgan@baylor.edu","700 S UNIVERSITY PARKS DR","WACO","TX","767061003","2547103817","MPS","1271","9263","$0.00","The characteristic values (eigenvalues) associated with a matrix are important throughout science. For an airplane, eigenvalues give the natural frequencies of vibration, and thus knowledge of eigenvalues can be used to prevent destructive resonance.  Eigenvalues are computed for buildings in earthquake zones for similar purposes, and eigenvalues are employed in a wide variety of other contexts as well, for example in finding the energy levels of atoms and molecules.  As mathematical models become more accurate and sophisticated, it becomes necessary to compute eigenvalues of ever larger matrices, requiring increasing computational time. The plan for this project is to take advantage of different sizes of matrices that are developed by placing different size grids on the domain of the problem. By doing much of the work on a smaller matrix, there is potential to substantially reduce the computational expense. Another application of eigenvalues is to improve convergence of systems of linear equations. This project will look at how eigenvalues from a grid with fewer points can effectively speed up the convergence for an iterative linear equations solver on a grid with many points. This project has potential impact in many areas of science, since many scientific applications lead to partial differential equations that are solved with grid-based eigenvalue problems.<br/><br/>New methods are studied for large eigenvalue problems and systems of linear equations. The methods combine multigrid with Krylov iterations in order to solve difficult problems. Regular multigrid methods are for differential equations that are solved on a grid, and they cycle through different grid sizes. They struggle for some problems (such as indefinite or too nonsymmetric). The proposed methods are more robust in that they can be effective in situations where regular multigrid fails. For eigenvalue problems, a method is proposed that computes eigenvalues on a coarse grid and improves them on the fine grid. An Arnoldi-type method is used that, unlike standard Arnoldi methods, can accept initial approximate eigenvectors. It is planned to develop and test this approach including for multiple grid levels. Also needed is analysis to explain why fine grid convergence unexpectedly matches that of regular Arnoldi. Near-Krylov theory will be developed for this. Iterative linear equations solvers suffer slow convergence when there are small eigenvalues. The plan is to use approximate eigenvectors from the coarse grid to essentially remove eigenvalues and improve the convergence. This approach can be used on preconditioned systems including with multigrid preconditioning. There is potential to very significantly improve computations for difficult eigenvalue problems and linear equations."
"1418983","A spectrally accurate hybrid moment-of-fluid and level set method for multiphase flows","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","08/12/2016","Mark Sussman","FL","Florida State University","Continuing Grant","Leland Jameson","07/31/2018","$349,993.00","Mohammed Hussaini","sussman@math.fsu.edu","874 TRADITIONS WAY","TALLAHASSEE","FL","323060001","8506445260","MPS","1271","8396, 8609, 9263","$0.00","State-of-the-art, highly accurate and efficient  numerical algorithms will be developed for simulating multi physics and multi-phase flows of engineering and technological importance. These new numerical methods are also designed to be scalable with respect to an exceedingly increasing number of computer processors, and thus easily implemented on the current and immediately future high-performance computing platforms. They will constitute an enabling technology for use by practitioners in science, engineering, medicine, industry and military. The primary applications of this technology will be to the design of fuel injectors for diesel engines, the design of swirling flow injectors in modern aircraft gas turbine engines, the design of flat fan nozzles employed in combustion, painting, spray cooling, agriculture irrigating applications, the design of off-shore facilities for the conversion of natural gas into liquid state, patient specific drug delivery, and the design of aircraft wing anti-freezing devices. The high accuracy and efficiency of these methodologies will allow parameter studies and design optimization procedures, more sophisticated geometries and models, which have proved until now to be prohibitively expensive.<br/><br/>A hybrid methodology coupling an adaptive mesh refinement, easily parallelizable, spectral-element method with the moment-of-fluid method will be developed for numerically simulating multi-phase flows. The principal investigators have already developed an adaptive, parallel, moment-of-fluid algorithm for incompressible and compressible multiphase flows.  The primary objective of this proposal is to take the previous work of the principal investigators to a higher level, where  each material has its own spectral-element representation, and a grid cell containing multiple materials will contain independent solution expansions for each material. A robust cell integrated semi-Lagrangian method will be implemented so that each material in each rectangular grid cell has a separate mapping from the departure region to the target region in such a way that the combination of all mappings tessellate the computational domain.  A cut-cell spectral-element algorithm will be developed for numerically solving the pressure projection equation (a Helmholtz equation) and for numerically solving for the viscous forces in multi-material flows.  The benefits of the new algorithm will be the greatly improved accuracy (without loss of robustness) in predicting the shape of deforming material boundaries where there are thin shear layers and/or thin thermal boundary layers."
"1417676","Modeling and Computation in Elastography","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","07/27/2014","Junshan Lin","AL","Auburn University","Standard Grant","Leland Jameson","07/31/2017","$144,812.00","","jzl0097@auburn.edu","321-A INGRAM HALL","AUBURN","AL","368490001","3348444438","MPS","1271","9150, 9263","$0.00","Elastography is an emerging imaging modality that seeks to non-destructively determine the mechanical properties of elastic/viscoleastic media from their response to external forces. It has a wide range of applications in medical diagnostics and non-destructive testing. The relevance and high cost of physical experimentation in such instances has driven the need for novel and efficient numerical algorithms to accelerate the corresponding design and identification process. To date, however, existing models and numerical schemes for their resolution have proven inadequate for this purpose. This research project initiates an integrated program for the remediation of this situation through the development of fast methods of broadly applicable models that are based on rigorous mathematical analysis. The results of the work should have a direct impact in a number of these instances, enhanced by the PI's access to real data and experiments from leading practitioners. Improved reconstruction schemes should lead to more accurate identification; their fast implementation, in turn, will allow for true guidance in the design of improved imaging systems.<br/><br/>The objective of this project is to examine fundamental mathematical issues and develop efficient computational methods for solving the direct and inverse problems in elastography. For the forward modeling, fast and accurate integral equation based solvers shall be developed for solid and solid/fluid interaction problems, aimed at attaining fully efficient and reliable simulation infrastructures for the underlying physical models in elastography. The approach proposed will address a number of significant challenges associated with the design of high-order quadratures for hypersingular integrals, with the accelerated evaluation of long-range potentials, and with the development of suitable preconditioners. To tackle the inverse problems, novel procedures with a combination of the multi-frequency continuation with fast linear approximations at long wavelengths shall be developed to address classical difficulties related to the nonlinear and ill-posed character of the inverse problems. This, in turn, should enable the possibility of true virtual design leading to advances in the area of PDE-constrained optimization."
"1419030","Coupling Continuum and Density Functional Theories for Materials Modeling","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","08/11/2014","Weinan E","NJ","Princeton University","Standard Grant","Junping Wang","01/31/2016","$124,829.00","","weinan@princeton.edu","1 NASSAU HALL","PRINCETON","NJ","085442001","6092583090","MPS","1271","8396, 8609, 9263","$0.00","This research concerns improved methods for the modeling of material systems.  Many materials modeling problems rely heavily on quantum mechanical models. However, for quantum mechanics to be really useful, one has to couple it with either continuum models or molecular mechanics models. The success of such an approach has been well documented in chemistry applications.  This project explores approaches based on quantum mechanics (density functional theory) coupled with molecular mechanics or continuum mechanics in modeling materials.  Although the coupled quantum mechanics/molecular mechanics approach has enjoyed a considerable amount of success, winning the 2013 Nobel Prize in chemistry, its application to general material systems, particularly metallic systems, has been at issue for a long time. The problem comes from the non-local effect of the errors made at the interface between the molecular mechanics and quantum mechanics regions. This problem is particularly severe for metallic systems. This project tackles these important challenges in the modeling of material systems.<br/><br/>There are many technical hurdles that one needs to overcome. First of all, one needs to improve the efficiency of density functional theory (DFT) algorithms to be able to handle inhomogeneous systems, such as systems with point defects or a small portion of extended defects. The next hurdle is to derive classical models, based on either molecular mechanics or continuum theory, that are consistent with DFT. This means that the DFT models reduce to classical models in some limit. A third problem is to design coupling schemes that move smoothly from DFT to classical models. The current project will focus on the second and third problems, beginning with one-dimensional model problems. The investigation's starting point is the Fermi operator formulation of DFT. By making successive approximations on the Fermi operator formalism, the PI aim to arrive at a consistent DFT/molecular mechanics or DFT/continuum model coupling scheme."
"1418911","Collaborative Research: Determining Forms and Data Assimilation with Stochastic Data","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","09/22/2016","Michael Jolly","IN","Indiana University","Standard Grant","Leland Jameson","07/31/2018","$175,000.00","Aseel Farhat","msjolly@indiana.edu","107 S INDIANA AVE","BLOOMINGTON","IN","474057000","3172783473","MPS","1271","9263","$0.00","Among the numerous factors that make accurate weather forecasting a challenge is initialization. The finite number of data collecting instruments distributed about the globe and atmosphere give only an incomplete description of the state of the weather at any instant. Yet there is a wealth of such data over an extended time period in the past, which when combined with certain mathematical models can provide a more complete description. The general procedure for this is called data assimilation. It provides more accurate starting values for computer simulations of the weather going into the future.  This award funds research into a new method of data assimilation that is flexible enough to be combined with a variety of simulation techniques. The work will concern both the implementation of this method, and the effect of measurement errors in the data, which inevitably occur. The assimilation process leads to another mathematical model that can be used to cleanse the past data of such noise. In this second direction, the goal is not to obtain a higher resolution condition for starting a simulation for the future, but to reconstruct a highly resolved version over the time period of the original data. This has natural applications in voice and pattern recognition. <br/><br/>The research team will implement a recent data assimilation algorithm based on feedback control for several important physical systems. The approach can be applied with a variety of determining parameters, such as nodal values, and finite volume elements. The team will also carry out computations with a new determining form based on feedback control. The steady states of this ordinary differential equation are precisely the finite-dimensional projections of trajectories in the global attractor. The team will study both computationally and analytically the effects of stochastic perturbations in the data. In the case of the data assimilation algorithm, the plan is to quantify and control the effect of the noise on the highly resolved state to be used in a subsequent direct numerical simulation. In the case of the determining form, the idea is to use its evolutionary process to remove the noise from the data itself."
"1417820","Experimental and Theoretical Analyses of Tree Distance Distributions","DMS","PHYLOGENETIC SYSTEMATICS, COMPUTATIONAL MATHEMATICS, Cross-BIO Activities, MSPA-INTERDISCIPLINARY","07/01/2014","06/29/2014","Sean Cleary","NY","CUNY City College","Standard Grant","Leland Jameson","06/30/2019","$240,000.00","","cleary@sci.ccny.cuny.edu","160 CONVENT AVE","NEW YORK","NY","100319101","2126505418","MPS","1171, 1271, 7275, 7454","8007, 9169, 9263","$0.00","Large data sets arise in a wide range of settings across scientific and engineering disciplines. Organizing such large data sets into forms where they can be effectively searched or understood can be very difficult. A common method is based upon the ""divide and conquer"" approach, in which successive division of large data sets leads, after some steps, to the amount of data in each piece being much more tractable. For example, after successively halving a data set with a trillion entries 20 times, there would be about a thousand entries in each remaining piece. Such successive division processes can be represented naturally by a branching structure known as a binary tree. It is important for efficiency that each step divides the pieces roughly in half; otherwise not all of the pieces will shrink to more manageable sizes. Trees also are a fundamental structure showing hierarchical relationships in a broad range of settings, from the data storage and searching technique just described to modeling evolutionary processes in biology. Tree distances measure how different two trees are and are important in assessing the degree to which two possible trees agree or disagree. There a number of different tree distances, which vary in what aspects of tree commonality they measure, their importance in different applications, and in their difficulty of calculation. This project seeks to better understand the distribution of multiple measures of distance on trees.<br/><br/>This project studies both ordered trees, used in computational settings such as data and disk storage techniques, and unordered trees, arising in biological and other scientific studies. The statistical properties of the distribution, including averages, variance and asymptotic properties, present difficult challenges and will be approached both experimentally and via exact and asymptotic combinatorial methods. The project also works on new approaches to compute tree distances that are both important for phylogenetic applications and computationally feasible. Greater understanding of rotation operations used in tree balancing may lead to improved algorithms for trees used in broad classes of computation, underlying the efficiency of many large database structures. The project develops understanding of how to scale tree distances by understanding the distribution of distances, giving scientists studying biological questions better understanding for interpreting results of tree reconstruction efforts. The project involves undergraduate researchers in cutting-edge research projects, both in the development of large-scale computational experiments and in developing the theoretical foundations of the answers to these questions. This award by the Computational Mathematics Program of the Division of Mathematical Sciences is co-funded by the Biodiversity and Ecosystem Dynamics Program in the DEB Division of the BIO Directorate."
"1419428","Midwest Women in Math Symposium (Midwest WIMS)","DMS","PROBABILITY, ALGEBRA,NUMBER THEORY,AND COM, GEOMETRIC ANALYSIS, APPLIED MATHEMATICS, TOPOLOGY, FOUNDATIONS, STATISTICS, COMPUTATIONAL MATHEMATICS, ANALYSIS PROGRAM, MATHEMATICAL BIOLOGY, Combinatorics, CDS&E-MSS","02/01/2014","01/31/2014","Amy Buchmann","IN","University of Notre Dame","Standard Grant","Tomek Bartoszynski","01/31/2016","$23,329.00","Julia Knight, Kathleen Ansaldi, Sonja Szekelyhidi, Fang Liu","abuchmann@sandiego.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","1263, 1264, 1265, 1266, 1267, 1268, 1269, 1271, 1281, 7334, 7970, 8069","7556, 9263","$0.00","The Midwest Women in Mathematics Symposium (Midwest WIMS) will be held at Notre Dame on April 5, 2014.  The goals of this symposium are to highlight women's contributions to the mathematical community, strengthen the network of female mathematicians in the Midwest, encourage new collaborations, and facilitate mentoring among graduate students, postdocs, and professors to help women stay active in research. The symposium will include several mathematical talks, including a keynote address given by Lenore Blum of Carnegie Mellon University. There will be parallel sessions in algebra, dynamical systems, geometry and topology, logic, mathematical biology, partial differential equations, and statistics. In addition, there will be a problem session to encourage new collaborations; problems that cut across disciplines are particularly encouraged. The symposium encourages participation of women in various stages of their academic careers, ranging from graduate students to full faculty. <br/><br/>It is not uncommon for women with research talent to drop out of the research community. The talks and the problem session will lead women to new and exciting projects. An extended lunch and other breaks will give participants at various stages in their careers the opportunity discuss mathematical problems, or simply exchange stories. The PI expects that some participants will, in discussions that are not strictly mathematical, learn strategies for staying active in research.<br/><br/>The conference website is at http://www3.nd.edu/~wims/"
"1419027","Theory, Methods for Diffusive Optical Imaging, Graph Based Fokker-Planck Equations and Mass Transportations","DMS","COMPUTATIONAL MATHEMATICS","07/15/2014","07/03/2014","Hao-Min Zhou","GA","Georgia Tech Research Corporation","Standard Grant","Leland Jameson","06/30/2018","$200,001.00","","hmzhou@math.gatech.edu","926 DALNEY ST NW","ATLANTA","GA","30332","4048944819","MPS","1271","9263","$0.00","This research project concerns theoretical and computational aspects of inverse source problems and mass transport equations.  Inverse source problems have a wide range of applications in science, engineering, and medicine.  Among different types of inverse source problems, diffusive optical imaging, such as Fluorescence Molecular Tomography (FMT), is particularly important.  FMT uses harmless infrared light, instead of X-ray, to capture molecular specific inclusions in biological tissues. It offers great potential in early cancer detection and drug monitoring.  However, diffusive optical imaging demands large scale computation and careful treatment of the ill-posedness due to the diffusive nature of light propagation in tissues.  This project aims to develop a new efficient and robust computation strategy for inverse source problems to improve image resolution and speed up computation significantly.  Optimal mass transport theory plays a crucial role in many important applications, such as logistics, transportation, physics, and chemistry, and the theory also has potential application to study information propagation on social media. Despite remarkable development in the theory in continuous settings in recent years, much less is known concerning the problems on graphs or networks.  This project will conduct theoretical and numerical analysis of graph-based mass transport problems, to design efficient and accurate simulation methodologies and to apply them to data analytics. The research activities will be integrated with education and training of undergraduates, graduate students, and postdocs through seminars and courses.<br/><br/>This project includes research in two areas: numerical methods for inverse source problems, and theoretical and numerical analyses for graph-based Fokker-Planck equations and mass transport problems.  For inverse source problems, the research aims to develop a new 2-stage methodology, called orthogonal solution and kernel correction algorithm, to separate the competing requirements on regularity and boundary data fidelity in the common regularization approach, so that both requirements can be addressed more effectively. The method leverages an adaptive multiscale basis, the finite element method, and the spectral method to gain significant computation speed up and resolution improvement. The method can be integrated into FMT data acquisition equipment.  Understanding of the Fokker-Planck equation and optimal mass transport theory that play crucial roles in many important applications has undergone remarkable development in continuous settings in recent years, but much less is known when one considers the problems on graphs or networks.  This project will conduct theoretical and numerical analysis of graph based Fokker-Planck equations and mass transport problems, to design efficient and accurate simulation methodologies and to apply them to data analytics, which aim to understand and to optimize strategies to handle information hidden in large scale, high dimension data sets."
"1418991","Analysis of Algorithms for Simulating Macroscopic Material Response","DMS","COMPUTATIONAL MATHEMATICS","07/15/2014","08/24/2016","Noel Walkington","PA","Carnegie-Mellon University","Continuing Grant","Leland Jameson","06/30/2017","$329,999.00","","noelw@andrew.cmu.edu","5000 FORBES AVE","PITTSBURGH","PA","152133815","4122688746","MPS","1271","8396, 8609, 9263","$0.00","The very essence of science is to explain and understand natural phenomena in order to predict and forecast outcomes. The most successful predictions result when fundamental laws of nature are integrated into conceptual models of the phenomena of interest. Newton's development of mathematical tools to express many fundamental laws of nature has resulted in mathematical models with unparalleled predictive power. These models consist of complex systems of equations relating the physical quantities of interest and form the conceptual foundation of modern engineering and science.  Solution of these complex systems of equations is a key technology needed to realize the potential of these theories, and the computational tools under investigation in this project are indispensable in this step of the modeling process.  This project will enhance the computational tools used to simulate materials such as polymers, liquid crystals, and many biological components. Improved predictive capability of computational models will play an essential role in the development and manufacture of many next generation devices such as micro-mechanical devices, biological materials, and prosthetic organs. Predicting material response is essential to determine biological and/or physiological function, reliability, and durability of these devices. In addition to the technological developments, this project will also support the education and training of the next generation of scientists needed sustain the remarkable pace of discovery and our scientific leadership in these disciplines.   <br/><br/>The focus of this proposal is the development and analysis of numerical schemes to simulate materials whose macroscopic response depends upon the state of their fine scale structure. This scenario is typical when material particles exhibit elasticity, attraction and/or repulsion, entropic interactions which can result in phase formation, and internal dissipation.  At the macroscopic scale these effects are modeled with internal variables which couple to the dynamic equations of motion.  This multi-scale character gives rise to many modeling, mathematical, and numerical challenges.  Models of materials with microstructure involve formidable systems of partial differential equations which inherit the delicate balance between transport and inertial effects, configurational energy, and dissipation of the physical system. While the past two decades have witnessed the development of many algorithms and codes in the engineering and scientific computing communities to solve these equations, there are many gaps in the mathematical theory and very little analysis of their fundamental properties is available.  In this situation is important to develop numerical schemes which faithfully inherit the complex interactions of the physical system.  Experience has shown that this paradigm can lead to a deeper understanding of the current schemes and frequently leads to improved and simpler algorithms.  This project will bring together tools from partial differential equations, continuum mechanics, and numerical analysis, to develop and analyze numerical schemes which simulate these systems."
"1418898","Collaborative Research: Phase-field models, algorithms and simulations for multiphase complex fluids","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","08/26/2014","XIAOFENG YANG","SC","University of South Carolina at Columbia","Standard Grant","Leland Jameson","08/31/2018","$100,000.00","","xfyang@math.sc.edu","1600 HAMPTON ST # 414","COLUMBIA","SC","292083403","8037777093","MPS","1271","8396, 8609, 9150, 9263","$0.00","Mixtures of two or more immiscible viscous and/or complex fluid components are widely used in many science and engineering applications, in particular, in designing advanced materials involving polymers, composites, gels, liquid crystals, etc. It is expected that the proposed models and numerical methods/simulations will contribute to a better understanding of the complex physical and mathematical issues related to multiphase complex fluids, and provide valuable information for the design of advanced materials and on the rheological and hydrodynamic properties of complex fluids. The proposed research will also provide valuable opportunities for undergraduate and graduate students to engage in interdisciplinary research with strong ties to biological and engineering material systems, to learn critical skills of computational and applied mathematics, and to develop state-of-the-art numerical tools for science and engineering applications.<br/><br/>Flows of multiphase complex fluid mixtures usually involve the coupling of microstructures, interfacial morphology and macroscopic hydrodynamics. The complexity of these nonlinear couplings presents many mathematical challenges for modeling and algorithm development, numerical analysis and implementation. The proposed research aims at overcoming these challenges to design efficient and accurate numerical algorithms for nonlinear multiphase complex fluid systems that couple the microstructure, moving material interfaces and hydrodynamics. Very few efforts have been made to address these numerical challenges. This project will result in numerical schemes which satisfy discrete energy dissipation laws, and which allow large time steps and controllable error and capture the interfacial dynamics accurately. In addition, the developed predictive tools and numerical simulations will extend the applicability of mathematical analysis and numerical codes to physical problems of current interest, and contribute to a better understanding of pressing science and engineering applications."
"1418714","Collaborative Research:  Computational techniques for nonlinear joint inversion","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","06/29/2014","Jodi Mead","ID","Boise State University","Standard Grant","Leland Jameson","06/30/2018","$270,000.00","John Bradford","jmead@boisestate.edu","1910 UNIVERSITY DR","BOISE","ID","837250001","2084261574","MPS","1271","8396, 8609, 9150, 9263","$0.00","An accurate representation of the Earth's subsurface is needed to manage natural resources such as groundwater and to monitor pollutants such as those from industrial landfills.  Geophysical exploration techniques are non-invasive strategies for imaging the subsurface.  In these approaches, electric fields are induced into the subsurface and the subsequent decay response is measured.  These measurements are converted into information about the subsurface by combining them with a physical model in an inversion methodology.  It is often the case that these problems are mathematically ill-posed because the measurements and mathematical model provide inconsistent or incomplete information.  This project will provide a new method of electromagnetic geophysical characterization that combines complex resistivity and ground-penetrating radar measurements, integrating material properties across a vast range of frequency bands: 102 - 109 Hz.  This range of information will be combined in a joint inversion that offers more observational information than is traditionally used to image the subsurface.  We will accommodate inconsistent information by appropriately weighting measurements and models with experimental statistics.  The algorithms developed under this project are computationally efficient and can be used with large data sets or complex mathematical models because they are grounded in modern numerical linear algebra techniques.  <br/><br/>Regularizing solutions for ill-posed linear inverse problems have been widely studied with respect to the impact of the choice and relevant weighting of applied regularizers. Yet, in the context of the solution of ill-posed nonlinear inverse problems the impact of stabilizing a Jacobian inversion within a Newton update, which effectively regularizes the solution, appears to be less well-appreciated.  In addition, Lagrange parameters that connect one or more models and data for joint or multiple inversion, and control the relationship between components of an inversion process, may be chosen in a somewhat ad-hoc manner.  The computational cost of generating a convergent sequence of solutions in the linear framework limits serious consideration of most linear approaches in the nonlinear framework. This project transforms the solution of relevant nonlinear problems by applying techniques that appropriately include physically based modeling constraints, and choosing regularization parameters based on underlying noise statistics in data.  This methodology opens efficient avenues for incorporating uncertainty in solutions of nonlinear problems by emphasizing solution techniques that permit analysis of the propagation of intrinsic measurement and numerical error through the solution process.  Thus the underlying computational algorithms have the potential for significant impact beyond the specifics of this project."
"1418936","Kinetic Approaches for Multi-Scale Problems in Quantum Chemistry and Seismology","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","06/29/2014","Xu Yang","CA","University of California-Santa Barbara","Standard Grant","Leland Jameson","06/30/2018","$293,266.00","","xuyang@math.ucsb.edu","3227 CHEADLE HALL","SANTA BARBARA","CA","931060001","8058934188","MPS","1271","8396, 8609, 9263","$0.00","This research project concerns an interdisciplinary topic that is of general interest to both computational mathematicians and scientists from other areas. This project will lead to quantitative understanding of the optical properties of crystals and in imaging Earth rock structures clearly, simply, and quickly. More importantly, these new methods will provide important advances in computational mathematics, leading to practical benefit in the fields of materials science and seismology. <br/><br/>This work is motivated by recent work on Frozen Gaussian Approximation (FGA) and its unique interpretation of high-frequency waves that suggests many new numerical techniques embedded. State-of-the-art numerical methods will be developed and used in various applications. The work will have impact in a wide range of problems of significant interest in physical sciences and industry. Specifically, the project investigates the following topics: 1. To theoretically study the FGA method in more depth. This includes analyzing the asymptotic accuracy of FGA, and integrating it with the Tailored Finite Point/Cell Method (TFPM/TFCM).  2. To study the dynamics of electrons in crystals by computing the Schrdinger equation with a lattice potential in both the Lagrangian and Eulerian approaches.  3. To compute seismic wave propagation in heterogeneous or non-lateral media based on FGA and its combination with TFPM/TFCM.  4. To survey the practical performance of developed numerical methods on, e.g., tsunami and earthquake models."
"1418723","Efficient and highly accurate solvers for integral equations on surfaces with edges and corners","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","08/15/2014","James Bremer","CA","University of California-Davis","Standard Grant","Leland Jameson","07/31/2017","$90,000.00","","bremer@math.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1271","9263","$0.00","This project concerns the computer modeling of physical phenomena. In particular, the PI seeks to accurately model the behavior of electromagnetic and acoustic waves. This topic has a long history, and many important results have been previously achieved, but we currently lack the ability to accurately model situations involving complex geometry. This project seeks to address these difficulties by combing several observations from pure mathematics with new engineering approaches. The end result of this project will be tools which allow for the accurate modeling of electromagnetic and acoustic waves. These tools will be applicable to many problems, but the PI is particularly interested in applying them to integrated circuit analysis and biomechanical simulations (for instance, vesicle flows).<br/><br/>Many of the partial differential equations of mathematical physics can be profitably reformulated as integral equations. Such methods have important applications to problems in electrodynamics, fluid dynamics and elasticity. However, most applications involve domains with singularity, and it is notoriously difficult to achieve high-accuracy and efficiency when solving integral equations on such domains. In principle, it appears that there should be no difficulty in solving a large class of integral equations given on surfaces with edge and corner singularities in a brute-force fashion. Many important boundary value problems in mathematical physics can be formulated using integral operators which are invertible and well-conditioned on spaces of square integral functions. Galerkin discretizations of such operators converge and are as well-conditioned as the underlying operator. It follows from these observations that, assuming all aspects of discretization are correctly handled, simply representing solutions locally with polynomial basis functions on a sufficiently dense mesh will result in highly accurate approximations. However, several difficult problems arise in practice with this brute-force approach; chief among them are: (1) dense meshes lead to excessively large linear systems that even modern O(N) fast solvers are inadequate to address; (2) representing singularities near edges is best done with highly anisotropic meshes which cause difficulties for currently available discretization techniques and fast solvers; (3) evaluating the entries of coefficient matrices to high accuracy, which involves estimating singular and ""nearly"" singular integrals, is quite challenging in general and substantially more so near corner and edge regions. The goal of this project is to develop highly-accurate fast robust solvers for integral equations on surfaces with edge and corner singularities which overcome these difficulties and achieve high-accuracy and efficiency. It will do so without the use of a priori asymptotic estimates (which are not available in many cases of interest). The project consists of a three-phased approach: (1) The PI will implement a highly-accurate and very robust ""brute-force"" procedure; (2) several tools, including local fast adaptive mesh generators and operator compression techniques will be deployed in order to accelerate the brute-force solver; (3) finally, efficient numerically precomputed quadrature formulae, which characterize the singularities of solutions and serve as a substitutes for a priori asymptotic estimates, will be constructed using the accelerated solver."
"1418624","Non-Iterative Multi-Physics Domain Decomposition Method for the Navier-Stokes-Darcy Model","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","10/31/2016","Xiaoming He","MO","Missouri University of Science and Technology","Standard Grant","Leland Jameson","08/31/2017","$95,500.00","Mingzhen Wei","hex@mst.edu","300 W 12TH ST","ROLLA","MO","654096506","5733414134","MPS","1271","8396, 8609, 9150, 9251, 9263","$0.00","The Navier-Stokes-Darcy model is crucial to accurately describe many important real world problems, such as the flow problems in vuggy/fractured porous media, groundwater systems in karst aquifers, interaction between surface and subsurface flows, and industrial filtrations. The efficiency of the numerical methods to solve this model is very important for the large-scale applications. But most of the related published works focus on a simpler model, which is not accurate for many applications since it does not consider several crucial realistic factors. This project is to study a more realistic Navier-Stokes-Darcy model and design an efficient numerical method to solve it in parallel. The completion of this project will provide large advances in the study of the coupled fluid flow and porous media flow. The basic idea and framework of this project have potential to be applied to other interesting multi-physics coupling problems. This project provides undergraduate and graduate students many valuable training opportunities in the development of new methods and packages, mathematical analysis, and engineering applications. The methods under development will be applied to simulate the flow in vuggy/fractured porous media. The investigators plan to disseminate the methods and software packages to more engineers and scientists by presenting the work in professional conferences and colloquia and organizing special sessions. <br/><br/>The goal of this project is to carry out a systematic research on the development, analysis, and application of a non-iterative multi-physics domain decomposition method for the Navier-Stokes-Darcy model with the Beavers-Joseph (BJ) interface condition and defective boundary conditions. The proposed non-iterative multi-physics domain decomposition method enables us to efficiently and simultaneously deal with the significant difficulties arising from the interaction among the aforementioned crucial realistic factors. The key ideas include a non-iterative algorithm with a direct prediction on the interface, a multi-physics decomposition with Robin conditions arising from the three physical interface conditions which include the BJ condition, and the Lagrange multiplier method to deal with the defective boundary condition. The major difficulty is the inherent interaction of the nonlinear advection, BJ condition, parameters in the Robin-Robin decomposition, and the saddle point problem arising from the Lagrange multiplier method."
"1418689","Highly efficient and accurate numerical schemes  for nonlinear gradient flows with energy stability","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","07/22/2014","Cheng Wang","MA","University of Massachusetts, Dartmouth","Standard Grant","Leland Jameson","07/31/2018","$189,998.00","","cwang1@umassd.edu","285 OLD WESTPORT RD","NORTH DARTMOUTH","MA","027472356","5089998953","MPS","1271","8396, 8609, 9263","$0.00","The goal of this project is to study efficient and robust tools to numerically simulate certain nonlinear gradient flows. These numerical schemes will yield highly efficient solvers to study the complicated long-time dynamics of different models in physics, materials engineering, and biological sciences. This work is expected to have a direct and immediate impact on many scientific disciplines. The large time scale simulation of these nonlinear gradient flows is vital for understanding phase transformations of materials at the atomic and nanometer scales, the complex processes in biological growth and development, and the complicated topological change involved in two-phase flows, etc. Some numerical schemes developed by the PI have been efficiently applied in large scale, multi-discipline scientific projects; a collaborative part of the project will leverage expertise in angiogenesis analysis of tumor growth simulation and is expected to lead to significant impact in the medical sciences community. The PI will make the computational tools available in the public domain so that researchers will have direct access to some of the developed algorithms.  Through work in the project, a graduate student will receive extensive training in high-performance scientific computing, numerical mathematics, and modeling. <br/><br/>In the proposed gradient flow models, the physical energy can be decomposed into purely convex and concave parts.  The PI considers convex splitting (CS) numerical schemes, including both the 1st and 2nd order accurate splittings in time, and finite difference, finite element and pseudospectral approximations in space. A major challenge is designing truly efficient solvers for these highly nonlinear CS schemes. For example, a direct nonlinear multigrid solver can be applied. As an alternate approach, linear iterative algorithms are proposed in this work, and a contraction mapping property is expected for these linear iteration based solvers. In other words, although the formulated CS scheme is nonlinear, a linear iteration based algorithm can be used to approximate solutions of this highly nonlinear system with a geometric convergence rate. A detailed comparison between a nonlinear solver and linear iteration algorithm will also be conducted."
"1419108","A parallel algorithmic framework for flexible time discretization adaptive Cartesian grids","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","07/03/2014","Donna Calhoun","ID","Boise State University","Standard Grant","Leland Jameson","08/31/2018","$194,987.00","","donnacalhoun@boisestate.edu","1910 UNIVERSITY DR","BOISE","ID","837250001","2084261574","MPS","1271","9150, 9263","$0.00","Accurately predicting the weather, understanding global climate change, designing novel materials, developing means of exploiting energy resources, and modeling the effects of natural hazards increasingly rely on our ability to efficiently solve mathematical equations on large scale computing platforms. To exploit the emerging computing power now available on multi-core desktop machines as well as at local and national supercomputing centers, numerical algorithms originally designed to run on a single computing processor (e.g. a CPU) must often be redesigned to operate efficiently (or ""scale"") in a supercomputing environment with thousands of processors. This project focuses on redesigning a particular class of numerical methods that dynamically allocate computing resources to spatial regions of a computational domain where a simulation is most demanding.  For example, such methods would place many more grid points (e.g. pixels) at a burning flame front, but leave the empty space in an industrial burner only coarsely resolved.  Or, to accurately track a thin filament of volcanic ash, an adaptive method will update high resolution regions of the simulation domain to follow the ash plume as it meanders through the atmosphere, but will not waste computational resources in areas of the globe where no ash has arrived. Many such adaptive methods now show modest scalability in a multi-processor environment, but we propose a new software paradigm which will allow these ""adaptive refinement mesh"" methods to scale efficiently to ever larger numbers of computing processors as well as enable domain scientists to more easily incorporate complex numerical algorithms into a high performance software frameworks. Successful achievement of project goals will enable researchers to take advantage of the national investment in supercomputing centers and to make progress towards providing solutions to grand challenge problems. As a particular demonstration of our adaptive mesh paradigm, we will produce high resolution simulations of volcanic ash transport in the atmosphere. Such simulations are critical for predicting aviation hazards associated with volcanic eruptions.<br/><br/>Single step, single stage multi-rate schemes are routinely used for solving partial differential equations on adaptively refined meshes. However, such methods are usually limited to second order accuracy or may suffer from operator splitting errors. Higher order temporal discretizations involving multiple stages or complex coupling strategies are considerably more difficult to incorporate into existing adaptive mesh software frameworks. The PI proposes a highly scalable algorithmic framework that simplifies the task of implementing sophisticated time stepping strategies into adaptive Cartesian mesh methods. The PI anticipates providing functionality that allows the user to describe their temporal strategy in a natural, method-of-lines setting. This will require designing an efficient, scalable data pipeline that provides a vectorized view of spatial data distributed across adaptively refined meshes and processors. Emphasis will be focused on explicit multi-stage Runge-Kutta methods for hyperbolic and parabolic conservation laws. Targeted applications of this work include the application of the theory of multi-rate methods for ODEs to the method-of-lines setting, the implementation of multi-rate, explicit Runge-Kutta-Chebyshev methods for reaction diffusion equations in an adaptive framework, and a demonstration of the effectiveness of the proposed framework on modeling dispersion of airborne volcanic ash in the atmosphere. The work will be done using the ForestClaw software platform, developed by the PI and her collaborator C. Burstedde (Univ. of Bonn, Germany)."
"1419029","Development of superconvergent hybridizable discontinuous Galerkin methods and mixed methods for Korteweg-de Vries type equations","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","07/18/2016","Bo Dong","MA","University of Massachusetts, Dartmouth","Continuing Grant","Leland Jameson","07/31/2018","$129,939.00","","bdong@umassd.edu","285 OLD WESTPORT RD","NORTH DARTMOUTH","MA","027472356","5089998953","MPS","1271","9263","$0.00","The project focuses on developing novel numerical methods for simulating the Korteweg-de Vries (KdV) type equations, that model phenomena in areas such as fluid mechanics, nonlinear optics, acoustics, and plasma physics. For example, the KdV equation has been used in the modeling of shallow water waves and the study of Tsunami waves. The new numerical tools developed under this project will provide scientists with a better understanding of theoretically unresolved issues on the mathematical properties of solutions to KdV type equations. Furthermore, the proposed project will provide accurate and efficient numerical algorithms for the simulation of nonlinear dispersive wave propagation in various applications. These proposed research topics will have a positive impact across the mathematical sciences and have significant applications in many scientific areas that rely on the study of non-linear phenomena. This project will involve undergraduate and graduate students and focus on involving student from groups traditionally underrepresented in the sciences. By working on the project, the students will benefit from novel ideas for new algorithm design, approaches for rigorous mathematical analysis, and advanced skills in implementation.<br/><br/>The objective of the project is to devise and analyze the first superconvergent hybridizable discontinuous Galerkin (HDG) methods and hybridized mixed methods for solving the KdV equations and their multidimensional generalizations.  The proposed project includes a comprehensive coverage of new algorithm design that is backed up by solid analysis and made practical by efficient implementation. The P.I. proposes to carry out a detailed study of superconvergent HDG methods and hybridized mixed methods for KdV type problems in the following steps: First, the P.I. will develop novel HDG methods and hybridized mixed methods for stationary third-order linear equations, focusing on the discretization of the third-order differential operator. Superconvergence properties of the approximations will be computationally and analytically investigated. Second, the P.I. would like to solve the third-order KdV equations by using implicit schemes for time discretization to avoid extremely small time steps and developing new HDG methods and hybridized mixed methods for spatial discretization. Error analysis will be carried out, and superconvergence and conservativity properties will be studied. Third, the P.I. plans to extend these superconvergent methods to multidimensional KdV type equations such as the Kadomtsev-Petviashvili equation, and the hybridization technique will make the methods efficiently implementable in multiple dimensions."
"1418994","Numerical Analysis and Methods for Simulating Moving Interfaces and Controlling Shape","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","07/31/2014","Shawn Walker","LA","Louisiana State University","Standard Grant","Leland Jameson","07/31/2018","$153,520.00","","walker@math.lsu.edu","202 HIMES HALL","BATON ROUGE","LA","708030001","2255782760","MPS","1271","8396, 8609, 9150, 9263","$0.00","Understanding physical laws and developing mathematical models to make predictions enables advancement of modern technology and industrial processes.  Moreover, optimization of these processes allows for better and more efficient designs.  The direct impacts to society are in general cost savings, more robust infrastructure, and better quality of life. The goal of this research project (see  technical description) is to target a specific set of physical/industrial processes that involve moving  boundaries or interfaces (e.g. the surface of a melting ice-cube in water is an interface between the solid  ""ice"" phase and the liquid ""water"" phase). Examples include growing solid phases in a desired shape from a liquid melt (controlled crystal  growth), modeling liquid crystals (display technology), micro-fluidics that control small droplets of liquid  by electric fields (useful in the bio-medical field), and self-assembly of particles and polymer chains on  deformable membrane surfaces (design of materials).  The research will enable better design, optimization, and control of these systems.  In addition, the project will create new methods for creating computer models of complex shapes that efficiently capture moving boundaries.  This is known as grid generation, which is a basic tool used in  many areas, from computer graphics (for creating life-like animations) all the way to commercial  product design (engineering design of structures such as bridges).  Grid generation is still an expensive task in terms of man-hours and money.  Any computational tools developed by this research will be made available to the public through online resources.  Furthermore, undergraduate and graduate students will benefit from a special course developed by the PI on computational free boundary problems.  Lastly, the PI is guiding middle school students in science fair projects motivated by this research.   <br/><br/>The proposed research will develop numerical analysis tools and methods for simulating moving interface and multi-physics problems and for optimal control of free boundaries.  We will take  advantage of variational/finite element methods, stability/energy estimates, shape differentiation, and  automatic meshing technology in the following projects: (A) develop and analyze new discrete  formulations of the Stefan problem (phase change/solidification) in 2-D and 3-D and explore well posedness questions of the time-dependent  interface motion (in a finite element method (FEM) setting); (B) develop a new FEM for liquid crystals  with provable stability/convergence properties and explore dynamics and equilibrium configurations; (C)  develop PDE-based optimal control techniques and numerics for directing and controlling shape; (D)  develop stable and accurate numerical methods (FEM) for simulating geometric objects that obey the no  self-penetration condition; and (E) create new mesh generation methods to handle 3-D problems in a  robust and automatic way with attention given to parallel implementation issues.  This project will advance the fundamental theory of free boundary problems by investigating well-posedness of time dependent domain-deforming problems and optimal control of shape.  It will further the development of methods for multi-physics problems with coupling to non-linear interfacial physics, such as anisotropic surface tension.  It will create novel numerical analysis and efficient methods for simulating dynamic curves and surfaces that respect *no self-penetration*, i.e. that enforce the excluded volume  constraint."
"1460368","Hybrid Adaptive Numerical Methods and Computational Software for Biological Fluid-Structure Interaction","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","09/03/2014","Boyce Griffith","NC","University of North Carolina at Chapel Hill","Continuing Grant","Junping Wang","08/31/2015","$54,819.00","","boyceg@email.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275995023","9199663411","MPS","1271","9263","$0.00","Building upon his earlier work in developing parallel and adaptive immersed boundary (IB) methods for simulating fluid-structure interaction (FSI), in this project, the investigator aims to construct a new hybrid FSI methodology which incorporates features of both the IB method and the immersed interface (II) method.  The IB method is a broadly-useful approach to FSI which has been applied to diverse problems in biological fluid dynamics.  Although the IB method has been demonstrated to be a useful approach to such problems, it is generally only first-order accurate, and fine spatial grids are therefore required to obtain resolved numerical simulations.  The II method is an IB-like approach to FSI which yields second-order accuracy for certain problems, but which is currently limited to thin elastic interfaces which are closed (i.e., which do not have free edges).  The hybrid FSI methodology of this project will incorporate features of both the IB and II methods to obtain high-order accuracy for both ""thick"" and ""thin"" elastic bodies, including thin elastic interfaces with free edges.  We believe that the basic version of the methodology will be the first IB-like method to achieve full second-order accuracy for thick elastic bodies such as the muscular walls of the heart, and that the extended version of the methodology will be the first II-like method to treat interfaces with free edges, such as the thin leaflets of the cardiac valves.  These new methods will be used to simulate cardiovascular flows, especially the fluid dynamics of the aortic heart valve.<br/><br/>Problems in which a fluid flow interacts with an elastic structure, such as the writhing and coiling of DNA or, as addressed within this project, blood flow in the heart and vessels, are ubiquitous in engineering, biology, and medicine.  The immersed boundary (IB) method is a broadly-useful approach to such problems which was introduced to enable the computer simulation of the fluid dynamics of the heart and its valves.  Indeed, cardiovascular applications have motivated much work to develop mathematical and computational methods for FSI, and the large and growing number of patients suffering from cardiovascular diseases (80 million people in the United States, approximately 30% of the population), such as coronary heart disease (16.8 million people) or heart failure (5.7 million people), make such applications increasingly important.  This project aims to develop an improved version of the IB method which will improve the accuracy of the methodology, possibly leading to significantly more realistic simulations of cardiovascular dynamics.  Because the IB approach is widely useful, and because the software implementing the methods of this project will be freely distributed, the potential impact of this work is quite broad, possibly affecting studies which aim to address basic scientific questions (e.g., the fluid-structure interactions which result in the beating of cilia within the oviduct or respiratory tract) to studies which aim to improve the design of medical therapies and devices (e.g., prosthetic cardiac valves or treatments for heart failure)."
"1347511","Sixth Annual Graduate Student Mini-conference on Computational Mathematics, March 7-8, 2014","DMS","COMPUTATIONAL MATHEMATICS","04/01/2014","04/17/2014","Lizette Zietsman","VA","Virginia Polytechnic Institute and State University","Standard Grant","Junping Wang","03/31/2015","$8,966.00","Jeffrey Borggaard","lzietsma@vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1271","7556, 9263","$0.00","This award supports participation in the Sixth Annual Graduate Student Mini-conference on Computational Mathematics, held at Virginia Tech on March 7-8, 2014.  The conference series engages researchers and graduate students from a number of research universities in the southeastern Atlantic region of the United States. The primary objective of this meeting is to provide a venue for graduate students to present their research in either poster (interactive) or short presentation formats. The conference also includes a plenary speaker from one of the participating universities.  This conference series exposes students to modern research directions in a wide range of areas in computational mathematics including, but not limited to: turbulent flow, large eddy simulation, optimal control, groundwater flows, magnetohydrodynamics, numerical approximation for stochastic partial differential equations, model reduction methods, fluid-structure interaction, and climate modeling.  The meetings lay the groundwork for future collaborations in these important research areas.<br/><br/>For most of the student participants, this mini-conference provides their first presentation experience within a supportive atmosphere and provides valuable exposure to a large network of researchers with the same or related research interests.  The mini-conference has always attracted participants of different backgrounds and nationalities. Underrepresented groups, including women and minorities, are strongly encouraged to participate.<br/><br/>Conference web site:  www.icam.vt.edu/Conferences/johnfest_2014/"
"1415152","Early-Career and Student Support for the XIX Householder Symposium, June 8-13, 2014","DMS","COMPUTATIONAL MATHEMATICS","06/01/2014","09/08/2014","Ilse C.F. Ipsen","NC","North Carolina State University","Standard Grant","Junping Wang","05/31/2016","$20,000.00","James Nagy","ipsen@ncsu.edu","2601 WOLF VILLAGE WAY","RALEIGH","NC","27607","9195152444","MPS","1271","7556, 9263","$0.00","This project supports the participation of USA-based early career scientists and Ph.D. students at the Householder Symposium XIX on Numerical Linear Algebra, a mathematical discipline that is the basis for many computational methods in science and engineering.  The Symposium will take place on 8-13 June 2014, in Spa, Belgium.  It is the nineteenth in a series, previously called the Gatlinburg Symposia, and named after the founder Alston S. Householder, one of the pioneers in numerical linear algebra.  The Symposium is very informal, with the intermingling of early-career and established researchers a priority. Attendance is by invitation only, and each attendee is given the opportunity to present a talk or poster.  The fifteenth Householder Award for the best Ph.D. thesis in numerical linear algebra since 1 January 2011 will be awarded.  The project will ensure the attendance of well qualified Ph.D. students at US institutions, as well as early-career scientists who received their Ph.D. degree after 1 January 2011 and are currently working in the US.   <br/><br/>The Symposium places an emphasis on broad relevance, and includes presentations on the following topics: Krylov space methods for linear systems and eigenvalue problems, computation of matrix functions, nonlinear eigenvalue problems, tensor algorithms and analysis, domain decomposition and multilevel methods, algorithms for structured matrices, randomized algorithms; as well as applications to: Optimization, differential equations, signal and image processing, control theory, electronic structure calculations, data analysis, information retrieval, and engineering.  The support of this project ensures the attendance of well qualified applied mathematics Ph.D. students from US institutions, as well as early US career scientists who received their Ph.D.'s after January 1, 2011.  Support of this group will have a positive impact on the continued strong competitiveness of the US in this crucial discipline, with its connections to a large number of scientific computing topics. Individual benefits for early career researchers who have attended a Householder Symposium include: Career advice from established researchers, enhanced visibility in the community, ideas for REU programs, and several NSF funded research projects that arose directly from collaborations established or advice received at the Householder Symposium.  Funding for participants takes into consideration participant diversity at the Symposium. More information can be found in http://sites.uclouvain.be/HHXIX/"
"1419003","Collaborative Research: Construction, Analysis, Implementation and Application of New Efficient Exponential Integrators","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","09/01/2014","08/22/2014","Adrian Sandu","VA","Virginia Polytechnic Institute and State University","Standard Grant","Leland Jameson","08/31/2018","$250,000.00","","sandu@cs.vt.edu","300 TURNER ST NW","BLACKSBURG","VA","240603359","5402315281","MPS","1271, 8069","7433, 8084, 9263","$0.00","As the scale and complexity of scientific and engineering problems grow, computer simulations become a necessary and integral part of the vast majority of research endeavors. An ability to create a computer model of a process under investigation, whether it comes from physics, economics, biology or some other field, provides not only significant cost savings for a study, but also brings insights inaccessible through experimental procedures alone. The growing complexity with which we describe phenomena of interest requires increasingly more sophisticated computer models. In particular, it is important to be able to simulate many complex processes over very long times, which is a computationally intensive and challenging task. This project is focused on developing new computational methods that allow simulating and studying time evolving phenomena from a wide range of scientific and engineering disciplines over long time intervals of interest.  The mathematical and computer tools created during this project will enable researchers to study problems at a scale and complexity not possible with currently available computational tools.<br/><br/>This project will advance the state-of-the-art in both the theory and practice of time discretization methods. In the course of the project a complete theoretical framework and high performance implementations of the new generation of exponential time integrators will be developed. The new methods will significantly improve computational efficiency of numerical models in many important areas of science and engineering, and will enable simulations at a scale and complexity that are not currently possible. The research will advance core numerical analysis through the development and study of new classes of exponential propagation iterative (EPI) methods such as split, hybrid, partitioned, and Krylov-based techniques. In addition, specialized efficient schemes will be designed and optimized for a wide range of problems. The theoretical work will be complemented by the creation of a mathematical software package that will provide high quality implementations of the most efficient exponential integrators to the broad scientific community.<br/>"
"1418706","Collaborative Research: Fluctuating Hydrodynamics of Suspensions of Rigid Bodies","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","06/29/2014","Aleksandar Donev","NY","New York University","Standard Grant","Leland Jameson","06/30/2017","$252,190.00","","ad139@nyu.edu","70 WASHINGTON SQ S","NEW YORK","NY","100121019","2129982121","MPS","1271","8396, 8609, 9263","$0.00","Over the last decade there has been rapid progress in the manufacturing and design of materials and devices that employ small-scale active particles to produce novel physical behaviours such as self-organizing flows (e.g., active colloidal suspensions), or to perform specific tasks such as cargo transport (e.g., targeted drug delivery). While much progress has been made experimentally, theoretical and computational modelling lags behind, due to the difficulty in designing suitable numerical algorithms and the lack of public-domain codes capable of capturing the complex multi-physics of active propulsion. In this work we develop novel computational methods for simulating active-particles suspended in fluid, and implement the developed techniques in the public-domain code IBAMR, therefore making them available to applied researchers in physics and engineering. A specific distinguishing aspect of the work is the consistent inclusion of the random Brownian motion necessarily present when dealing with small-scale flows due to the small numbers of molecules involved in the process. Such stochastic effects are important in flows at micro and nano scales typical of nano- and micro-fluidic and microelectromechanical devices, novel materials such as nanofluids, and biological systems such as lipid membranes, Brownian molecular motors, and nanopores. We therefore expect the work to have a broad range of applications in science and engineering, beyond the specific research goals detailed below. The scientific component of this project will be supplemented by an educational and outreach component, including the development and enrichment of new graduate courses, such as Coarse Grained Modeling of Materials, which will include training in statistical mechanics, applied stochastic analysis, fluid dynamics, and high-performance computing.<br/><br/>This collaborative project focuses on computational methods for problems involving Brownian rigid and semi-rigid structures immersed in a fluid. Examples include colloidal particles, polymer chains, and macromolecules in a solvent. We aim to develop novel methods for fluid-structure coupling at small Reynolds numbers that consistently include the effects of thermal fluctuations. At small scales, the motion of immersed structures is driven by thermal fluctuations, giving rise to Brownian motion strongly affected by hydrodynamic effects. We plan to develop methods that couple an immersed-boundary Lagrangian representation of rigid bodies to a fluctuating finite-volume fluid solver. Unlike commonly-used methods based on Green's functions, we rely on an explicit-fluid fluctuating hydrodynamics formulation in which we add a stochastic stress tensor to the usual viscous stress tensor. We will handle complex rigid (e.g., synthetic nanorods) and semi-rigid (e.g., short DNA segments) bodies by composing each structure from a collection of spherical particles constrained to move (semi)rigidly. The underlying fluctuating hydrodynamics formulation automatically ensures the correct translational and rotational Brownian motion. The novel methods developed in this project will build upon prior work by the PIs and enable simulations of the long-time diffusive (Brownian) dynamics of the immersed structures. In particular, we will develop, implement, and apply computational methods that: (1) do not employ time splitting and are thus suitable for the steady Stokes (viscous-dominated or low Reynolds number) regime; (2) strictly enforce the rigidity constraint; and, (3) ensure fluctuation-dissipation balance in the overdamped limit even in the presence of nontrivial boundary conditions."
"1440016","AWM Workshops and Noether Lecture 2015, January  10-13, 2015; March 14-18, 2015","DMS","INFRASTRUCTURE PROGRAM, APPLIED MATHEMATICS, TOPOLOGY, COMPUTATIONAL MATHEMATICS","08/01/2014","07/19/2014","Ruth Charney","RI","Association for Women in Mathematics, Inc.","Standard Grant","Christopher Stark","07/31/2015","$42,325.00","Brooke Shipley, Fengyan Li","charney@brandeis.edu","201 Charles St","Providence","RI","029042213","4014554086","MPS","1260, 1266, 1267, 1271","7556, 9263","$0.00","This proposal from the Association for Women in Mathematics (AWM) includes two workshops for early-career women, the first at the Joint Mathematics Meeting (JMM) in San Antonio, January 10-13, 2015, and the second at the SIAM Conference on Computational Science and Engineering (CSE) in Salt Lake City, March 14-18, 2015. It also includes a named lectureship, the Emmy Noether Lecture, to be held at the San Antonio JMM meeting. The workshops are designed to create sustainable networks, encourage mentoring relationships, and promote research collaborations in key fields of mathematics.  The Noether Lecture highlights outstanding contributions by a female mathematician and provides an inspiring role model to more junior women.  As such, these events address some of the key issues that frequently inhibit women's participation in the field. <br/><br/><br/>Each workshop will focus on an area of mathematics of current interest. The 2015 AWM-JMM workshop will focus on homotopy theory. Homotopy theory, an area of algebraic topology, has close ties to algebraic geometry, geometric topology, number theory, representation theory, and mathematical physics.  It is a highly active area and has seen significant progress in recent years.  The 2015 AWM-SIAM workshop will focus on mathematical modeling and high-performance computing for multiphysics and multiscale problems.  Advances in science and engineering - in disciplines ranging from health to energy and the environment to defense - rely on predictive measurement and analysis of multiscale multiphysics systems.  Topics to be addressed at the workshop include physical modeling, mathematical analysis, numerical analysis, algorithms, implementation, performance, and scalability. <br/><br/>https://sites.google.com/site/awmmath/programs/workshops"
"1419103","Multivariate splines in algebra, analysis, and combinatorics","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","09/05/2016","Amos Ron","WI","University of Wisconsin-Madison","Continuing Grant","Leland Jameson","08/31/2019","$410,000.00","","amos@cs.wisc.edu","21 N PARK ST STE 6301","MADISON","WI","537151218","6082623822","MPS","1271","9263","$0.00","This research project is at the interface among multiple sub-disciplines of mathematics: analysis, approximation theory, and data representation on the one hand; combinatorics and algebra on the other hand. The research involves the construction of a new spline class. Past mathematical research on the theory and practice of spline functions led to some of the most significant contributions of the mathematical community to science and technology. Splines have become indispensable tools in computer-aided design and manufacturing of cars and airplanes, in the production of printers' typesets, in automated cartography, in the production of movies, and in many other areas, often concealed at the core of elaborate software packages. This project will merge knowledge and skills from disparate areas of mathematics to provide new multivariate spline constructions as well as new theoretical results in algebra and geometry.<br/><br/>Spline functions are piecewise-polynomials in one or several variables. Zonotopal algebra is a mathematical methodology that encodes combinatorial and geometric properties in rich algebraic and analytic structures. It presently handles the special polytope known as a zonotope and its dual hyperplane arrangement. At its core one finds the spline theory known as box splines, splines that are defined over zonotopes, arguably the most successful spline theory in several variables. Zonotopal algebra and its associated box splines are connected to a myriad of topics inside and outside mathematics, including approximation, wavelets, subdivision, matroids, graphs, algebraic geometry and more. There is evidence that zonotopal algebra should have a pair of spline constructs: box splines and another spline class over the dual geometry. This project aims to recover this additional class, in particular through extending zonotopal algebra to a class of polytopes that are invariant under group actions. Embedding such invariance into zonotopal algebra and understanding the correct algebraic structures and spline constructions over these non-commutative geometries is another goal of this project."
"1418779","Multiscale Basis Dictionaries and Best Bases for Data Analysis on Graphs and Networks","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","07/18/2016","Naoki Saito","CA","University of California-Davis","Continuing Grant","Leland Jameson","07/31/2018","$475,000.00","","saito@math.ucdavis.edu","1850 RESEARCH PARK DR, STE 300","DAVIS","CA","956186153","5307547700","MPS","1271","9263","$0.00","In recent years, the advent of new sensors, measurement technologies, and social network infrastructure has provided huge opportunities to visualize complicated interconnected network structures and record data of interest at various locations in such networks. Consequently, there is an explosion of interest and demand to analyze such data and make inferences, predictions, and diagnostics. Examples of such data include, but are not limited to: biology and medicine (e.g., blood flow rates in a network of blood vessels); computer and social sciences (e.g., information flows in social networks); electrical engineering (e.g., sensor networks); hydrology and geology (e.g., river flow measurements in a ramified river network); and civil engineering (e.g., traffic flow on a road network). The investigator and his team will develop mathematical and computational tools referred to as ""multiscale basis dictionaries"" on a given graph, which will have a positive impact in solving practical data analysis problems on graphs and networks in diverse fields as listed above. In particular, these dictionaries will be able to capture subtle features discriminating anomalous events from normal events on graphs, which may shed light on underlying causes of such anomalies. Students engaged in this project will be trained to be the next generation of interdisciplinary scientists who have deep knowledge in one area yet have open mind to the other areas and try to actively seek collaborations with domain experts. Such an attitude and a perspective will be indispensable for their future career, either in academia or in industry.<br/><br/>The goal of this project is to develop above-mentioned multiscale basis dictionaries and best bases selected from such dictionaries for graphs and networks, and demonstrate the usefulness by examining their performance on a variety of data analysis tasks on graphs and networks such as compression, denoising, semi-supervised learning, and anomaly detection. Mathematical and computational tools for analyzing such datasets, particularly for those on directed graphs, have not been well developed. For more conventional data supported on simple Euclidean domains and data sampled on regular lattices, harmonic analysis tools such as Fourier and wavelet transforms as well as multiscale basis dictionaries, e.g., wavelet packets and local trigonometric transforms, have a proven track record of success. This project can be viewed as the continuing effort of the investigator to transfer and extend these computational harmonic analysis tools from the realm of regular lattices and simple Euclidean domains to more general graph domains. The multiscale basis dictionaries for graphs including a complete Haar-Walsh basis dictionary will certainly enrich the current collection of data analysis tools on such domains because these dictionaries contain a huge number of possible bases from which one can quickly select a basis most suitable for a given task via the best-basis selection algorithm. In particular, any addition of mathematical and computational tools for data analysis on directed graphs is well rewarded since there are comparably few tools available despite their practical importance. This is partly because so many classes of directed graphs exist, and consequently, there has been confusion over the definitions of graph Laplacian matrices. Instead, this project provides a new viewpoint: on a directed graph, the connectivity between any two vertices are not a local concept; rather it is a global concept. Finding a shortest path connecting a given pair of vertices provides critical information on a directed graph. To utilize such information fully, spectral analysis of the distance matrices and the associated integral operators on a directed graph is performed using the singular value decomposition instead of analyzing the graph Laplacians using the eigendecomposition."
"1419053","Collaborative Research: Phase-field models, algorithms and simulations for multiphase complex fluids","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","08/26/2014","Jie Shen","IN","Purdue University","Standard Grant","Leland Jameson","08/31/2017","$149,998.00","","shen@math.purdue.edu","2550 NORTHWESTERN AVE STE 1900","WEST LAFAYETTE","IN","479061332","7654941055","MPS","1271","8396, 8609, 9263","$0.00","Mixtures of two or more immiscible viscous and/or complex fluid components are widely used in many science and engineering applications, in particular, in designing advanced materials involving polymers, composites, gels, liquid crystals, etc. It is expected that the proposed models and numerical methods/simulations will contribute to a better understanding of the complex physical and mathematical issues related to multiphase complex fluids, and provide valuable information for the design of advanced materials and on the rheological and hydrodynamic properties of complex fluids. The proposed research will also provide valuable opportunities for undergraduate and graduate students to engage in interdisciplinary research with strong ties to biological and engineering material systems, to learn critical skills of computational and applied mathematics, and to develop state-of-the-art numerical tools for science and engineering applications.<br/><br/>Flows of multiphase complex fluid mixtures usually involve the coupling of microstructures, interfacial morphology and macroscopic hydrodynamics. The complexity of these nonlinear couplings presents many mathematical challenges for modeling and algorithm development, numerical analysis and implementation. The proposed research aims at overcoming these challenges to design efficient and accurate numerical algorithms for nonlinear multiphase complex fluid systems that couple the microstructure, moving material interfaces and hydrodynamics. Very few efforts have been made to address these numerical challenges. This project will result in numerical schemes which satisfy discrete energy dissipation laws, and which allow large time steps and controllable error and capture the interfacial dynamics accurately. In addition, the developed predictive tools and numerical simulations will extend the applicability of mathematical analysis and numerical codes to physical problems of current interest, and contribute to a better understanding of pressing science and engineering applications."
"1347163","CBMS Conference: Algorithms for solving elliptic PDEs on modern computers---fast direct solvers, randomized methods, and high order discretizations,","DMS","INFRASTRUCTURE PROGRAM, COMPUTATIONAL MATHEMATICS","03/01/2014","02/14/2014","Alexander Barnett","NH","Dartmouth College","Standard Grant","Jennifer Pearl","02/28/2015","$37,968.00","Leslie Greengard, Adrianna Gillman","ahb@math.dartmouth.edu","7 LEBANON ST","HANOVER","NH","037552170","6036463007","MPS","1260, 1271","7556, 9150, 9263","$0.00","The award supports an NSF/CBMS regional conference that will be held at Dartmouth College in June of 2014 on the topic of so called ""fast methods"" for numerically solving linear partial differential equations (PDEs). This field of research has recently seen very rapid progress, and the purpose of the conference is to introduce recent results to junior researchers, and to engage them with current research. In particular, the conference will describe (1) new ""direct"" (as opposed to ""iterative"") solvers that in a single sweep construct an approximate solution to the equation under consideration, (2) new randomized methods for accelerating certain linear algebraic computations, and (3) the interplay between direct solvers and high-order discretization techniques that allow the solution of PDEs to ten digits of accuracy or more, even on complicated geometries.<br/><br/>The broader motivation for the research to be discussed during the conference is the growing importance of computational simulations of physical phenomena in science and engineering. The ability to perform rapid virtual experiments rather than real (expensive) ones is a key driver of technological progress. While more powerful computers are important, development of efficient numerical algorithms that run well on modern hardware is equally crucial. The CBMS conference supported will disseminate knowledge about newly developed powerful algorithms that are designed from the ground up to minimize communication, which is emerging as the most important computational bottleneck. The conference will in addition to traditional lectures also give participants an opportunity for hands-on computational experimentation using tutorial style software. The lectures will be recorded and will be posted online along with the lecture slides, and the tutorial software. A monograph will be produced to create an integrated set of resources that will maximize the impact and reach of the conference.  This project will be jointly supported by the Infrastructure Program and the Computational Mathematics Program within the Division of Mathematical Sciences."
"1418882","Multiple preconditioners for saddle-point and other problems","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","08/03/2016","Daniel Szyld","PA","Temple University","Continuing Grant","Leland Jameson","08/31/2017","$150,000.00","","szyld@temple.edu","1801 N BROAD ST","PHILADELPHIA","PA","191226003","2157077547","MPS","1271","9263","$0.00","There are many problems in science and engineering where one deals with two completely different phenomena, but these two phenomena influence one another (or one influences the other). One example of this situation is fluid flow along two different kinds of media. One flow is free, as in water in a river, and the other is constrained by some porous media, as the fluid seeping through below the river bed. When one tries to describe these physical phenomena, one needs to use very different equations for each of the two, in addition to taking into account what happens in their interphase. In this project, the PI shall study the solution of systems of equations of the kind just described. The PI shall combine solution techniques developed for each of the two phenomena, so that the resulting combination gives us an efficient computational tool.<br/><br/>Among the phenomena to which the PI wants to contribute better modeling and solution techniques is the so-called coupled Stokes-Darcy flow, with the differential equations discretized by finite elements, and in particular by discontinuous Galerkin in the Darcy region. The PI proposes to use new exact and inexact constraint-type (indefinite) preconditioners. The PI will study spectral (and field-of-values) equivalence bounds, which should provide mesh independence bounds on the GMRES convergence for these problems of saddle-point type. Multipreconditioning is another tool that the PI wants to develop for the solution of saddle-point and other problems. In the case of Multipreconditioned GMRES, the PI proposes to provide convergence analysis through the use of multivariate polynomials. In this approach, at each iteration, the method computes in an implicit manner the optimal combination of two preconditioners. For certain saddle-point problems this dynamical combination preconditioning is expected to be more efficient than either of the two original preconditioners."
"1418784","Numerical Analysis of Selected Variational and Quasi-variational Inequalities","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","08/07/2014","Abner Salgado","TN","University of Tennessee Knoxville","Standard Grant","Leland Jameson","07/31/2017","$134,265.00","","asalgad1@utk.edu","1331 CIR PARK DR","Knoxville","TN","379163801","8659743466","MPS","1271","9150, 9263","$0.00","Unilateral and constrained phenomena are ubiquitous in science and engineering. The quantities that govern a physical process are often subject to constraints: Be it of mechanical, physical, thermodynamical or practical nature. Examples of these can be impenetrability conditions (two bodies cannot be at the same place at the same time), the fact that mass cannot be negative and that entropy cannot decrease or simply the fact that we are not able to produce more than a fixed amount of forcing. In addition, many of these constraints might depend on the quantities of interest themselves (e.g. friction). One final example is mechanical contact which is, in fact, the only mechanism through which we can exert any mechanical action on another body. These examples show that the derivation of realistic models for the description of these phenomena is of fundamental importance in applications. However these models will be, as a rule, nonlinear. The development and analysis of numerical techniques for approximating the solution of these problems is of practical relevance.<br/><br/>The models that govern constrained phenomena carry the name of variational and quasi-variational inequalities. This proposal aims at the study of numerical techniques for a collection of variational and quasi-variational inequalities which have not been considered before and to which the classical ideas and techniques do not apply. They include: variational inequalities for nonlocal operators, evolution problems for degenerate parabolic equations and the general study of time discretization techniques for quasi-variational inequalities. The numerical methods that will result from this project will be of interest to a wide range of practitioners, since the proposed problems have a wide range of applications. For instance, obstacle problems with fractional diffusion appear in control theory, fluid mechanics and even finance; the degenerate parabolic equations that will be considered find applications in imaging and materials science; the prototypical example of a quasi-variational inequality is friction, but they also arise in game theory - when trying to find Nash points - and in control theory when dealing with impulse controls."
"1418961","Collaborative Research:  Efficient High-Order Parallel Algorithms for Large-Scale Photonics Simulation","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","08/11/2016","Andreas Kloeckner","IL","University of Illinois at Urbana-Champaign","Continuing Grant","Leland Jameson","07/31/2018","$209,999.00","","andreask@illinois.edu","506 S WRIGHT ST","URBANA","IL","618013620","2173332187","MPS","1271","9263","$0.00","The focus of this research project is to develop efficient, accurate, and scalable computational techniques and provide much-needed simulation design tools for the photonics industry. More and more of modern life is based on fast and cheap communication. The transmission of information by electrical circuitry is limited in latency by power concerns and in bandwidth by cost. Photonic circuits virtually eliminate these constraints and provide a way to make high-bandwidth, low-latency interconnects that are, in many applications, far superior to their electric counterparts. But photonic circuits are extremely challenging to design. Indeed, the current state-of-art integrated photonic circuits chip contains only hundreds of photonic components due to the lack of efficient and reliable tools for the design of integrated photonic circuits. The difficulty here is that the design of integrated photonic devices requires accurate simulation of propagating electromagnetic waves which, in turn, requires extremely large numbers of unknowns even for modest accuracy in a volume discretization.  The tools developed by this project will address these important challenges.<br/><br/>The fundamental mathematical model for most photonics applications consists of Maxwell's equations with complex, structured material coefficients under wide variation of feature length scales. At computational scales feasible for a design engineer, existing techniques are too inaccurate for the design of complex photonic devices. This inaccuracy/inefficiency trade-off severely limits the usefulness of simulation in a design feedback loop. It further represents an impediment to the rapid development of the photonics industry since design iteration through manufacturing is typically very expensive and can take months of turn-around time on a single design. Lifting this inefficiency constraint is challenging, and it has the potential to play a pivotal role in the development of ambitious photonic circuits and devices. The investigators propose to develop the following techniques to overcome the obstacles encountered in practical, large-scale photonics simulation.  (1) Modularization of photonic device simulation via boundary integral equation methods. Specifically, the so-called mode calculation will be converted to a nonlinear eigenvalue problem of boundary integral equations, and the so-called propagation problem will be converted to a standard scattering problem and then solved via boundary integral equation methods. (2) Extension of the QBX method (""Quadrature by Expansion""), a general-purpose, high order quadrature scheme to treat three-dimensional problems with domains having corners, edges and multi-scale structures for accurate photonics simulation. (3) Seamless combination of the QBX method with a novel variant of the Fast Multipole Method (FMM) to solve integral equations in a matrix-free form with near optimal operation and storage requirements."
"1461679","Conference on Mathematical Sciences Challenges in Quantum Information","DMS","COMPUTATIONAL MATHEMATICS, ANALYSIS PROGRAM","12/01/2014","12/01/2014","Lorenza Viola","NH","Dartmouth College","Standard Grant","Leland Jameson","11/30/2016","$36,068.00","","Lorenza.Viola@Dartmouth.edu","7 LEBANON ST","HANOVER","NH","037552170","6036463007","MPS","1271, 1281","7203, 7556, 9150, 9263","$0.00","This award supports participation in a conference on mathematical sciences challenges in quantum information science, held 12-13 February 2015 in Arlington, Virginia.  The field of quantum information (QI) explores the theoretical, experimental, and technological areas covering the use and implications of quantum mechanics for communication and computation purposes.  While conceptually all of the diverse aspects that QI encompasses arise from the seemingly simple appreciation of information as a physical resource, its consequences have the potential to revolutionize computation, communication, and ideas about complexity. In particular, QI promises exponentially more efficient quantum algorithms for solving hard combinatorial problems and for simulating the behavior of complex quantum systems, along with unprecedented opportunities for secure communication and new perspectives to tackle a range of problems in physics, biology, applied mathematics, engineering, and computer sciences. As a result, QI is currently a thriving and naturally interdisciplinary research area, and one of critical strategic relevance to 21st century's science and technology. <br/><br/>As the QI field is entering a new stage of maturity, further progress is increasingly relying on the use of diverse advanced mathematical techniques. In turn, new areas of mathematics are being identified, which offer the potential for major future advances in the field. This award supports a one-and-a-half-day conference that brings together leading researchers working across a broad spectrum of problems in QI theory, with the goals of (i) highlighting the centrality and importance of mathematical tools; and (ii) assessing outstanding mathematical challenges and formulating a vision for the future. A distinguished group of speakers will highlight the latest research developments and open problems in topics, including quantum channels and quantum error correction theory, quantum marginals, random quantum states and dynamics, many-body quantum systems, and quantum control. Panel discussions and breakout sessions are intended to identify new promising directions for investigations. The expected broad impacts of the proposed conference are twofold. On the one hand, the conference facilitates collaborative exploration between academic and federal agency scientists of the short- to mid-term mathematical challenges in advancing the field of QI -- with a written report being anticipated as an output of the conference. On the other hand, the conference serves as a vehicle to enlist mathematically-oriented scientists to bring their ideas to bear on accessible fundamental problems in QI, and to educate the next generation of scientists working in this very important field."
"1419044","Data Assimilation, Noise Models, and Dimensional Reduction, with Applications","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","08/11/2014","Alexandre Chorin","CA","University of California-Berkeley","Standard Grant","Leland Jameson","07/31/2018","$405,345.00","","chorin@math.berkeley.edu","1608 4TH ST STE 201","BERKELEY","CA","947101749","5106433891","MPS","1271","9263","$0.00","There are many problems in science where one must draw conclusions and make predictions on the basis of imperfect models supplemented by noisy and/or incomplete data; examples are weather forecasting, climate prediction, economics, and materials science. The goal of the project is to make carrying out these task more efficient and accurate, and extend the methodology to new situations. In earlier work we showed how to optimize the task of finding the best conclusions. Under the new proposal we will look for new, more efficient ways to estimate margins of error, and also use the estimates of the error to improve the conclusions. The same technology can be used to improve the models themselves on the basis of data; we will try to expand this to problems where the data are qualitative, for example, devise methods that will eventually allow an engineer to translate observations about when an engine runs smoothly into precise quantitative information that can be used for design. Finally, we will extend these methods to problems where the mathematical models are unreliable, not because of incomplete knowledge or of experimental error, but because the computing power available to solve them is insufficient; in such cases, our methods would make it possible to use statistical methods to enhance the accuracy of the computations.<br/><br/>The proposal has several thrusts. (i) One of the main obstacles to further improvement of data assimilation tools is the difficulty in deriving realistic noise models, in particular because the noise depends on the signal and is therefore hard to separate from it. It is proposed to perform this separation by using tools from statistical physics, in particular recent generalizations of the Mori-Zwanzig formalism. (ii) The problem of estimating the noise in noisy models and the problem of deriving reduced models of complex dynamics are very similar; this remark can be used to develop data assimilation methods for qualitative data. This requires creating maps between data sets and parameter sets in the models, which becomes feasible through the use of reduced dynamics, to be derived by the methods in the present proposal. (iii) One can interpret the missing components of the solution in underresolved numerical dynamics as an added noise. This remark suggests that these missing components can be estimated from numerical or experimental data, so as to enhance the accuracy of the numerical solutions. It is proposed to develop methods for doing so. (iv) It is proposed to develop reduced descriptions in a variety of areas of scientific computation. The applications of this work will include problems from geophysics, stochastic control, robotics, combustion, and fluid mechanics."
"1361197","FRG: Collaborative Research: Developing Mathematical Algorithms for Adaptive, Geodesic Mesh MHD for use in Astrophysics and Space Physics","DMS","OFFICE OF MULTIDISCIPLINARY AC, COMPUTATIONAL MATHEMATICS, ","07/01/2014","06/27/2014","Dinshaw Balsara","IN","University of Notre Dame","Standard Grant","Leland Jameson","06/30/2017","$281,163.00","","dbalsara@nd.edu","940 Grace Hall","NOTRE DAME","IN","465565708","5746317432","MPS","1253, 1271, 1798","1206, 1616, 9263","$0.00","Simulation tools for astrophysical and space physics systems share a set of common requirements: they need to robustly simulate magnetohydrodynamic (MHD) flows around spherical bodies with high accuracy. This multidisciplinary project will develop algorithms from applied mathematics for robust, highly accurate non-relativistic MHD on geodesic meshes. In the past few years new schemes for simulating conservation laws with truly multi-dimensional divergence free approximate Riemann solvers for applications have been developed. Currently, these Riemann solvers are only available for two-dimensional rectangular structured meshes for MHD. This project will employ a geodesic mesh to provide the best possible coverage for simulations of magnetohydrodynamic flows around spherical bodies and to incorporate Delaunay triangulation to achieve high accuracy. Divergence-free formulations of vector fields can be found on these triangular meshes. <br/><br/><br/>Simulation tools for astrophysical and space physics systems share a set of common requirements: they need to robustly simulate magnetohydrodynamic (MHD) flows around spherical bodies with high accuracy. Building a computational framework, based on shared needs in space physics and astrophysics, will unleash important synergies between these two allied fields of study. The MHD equations are a combination of the Navier-Stokes equations for fluid dynamics and Maxwell's equations for electromagnetism.  Thus, the MHD equations require numerical solvers that incorporate the hydrodynamic fluid motion and enforce the divergence free magnetic field, i.e. no magnetic monopoles, requirements on the geometric domain approximated by a polygonal mesh. The nature of the MHD equations closely couples solution methodologies to the underlying mesh, making it necessary to develop new algorithms for the divergence-free reconstruction of the magnetic field on novel mesh structures. Additionally, the MHD system is formulated as a system of conservation laws. With a traditional conservation law, the fluxes can be evolved on a dimension-by-dimension basis. The fact that different flux components are coupled in an involution-constrained system also makes a case for multidimensional upwinding based on multidimensional Riemann solvers. Such solver strategies are again intimately coupled to the mesh structure."
"1418833","Conference on the Foundations of Computational Mathematics 2014","DMS","ALGEBRA,NUMBER THEORY,AND COM, APPLIED MATHEMATICS, TOPOLOGY, COMPUTATIONAL MATHEMATICS","12/01/2014","11/25/2014","Agnes Szanto","NC","North Carolina State University","Standard Grant","Junping Wang","11/30/2015","$25,000.00","","aszanto@ncsu.edu","2601 Wolf Village Way","Raleigh","NC","276957514","9195152444","MPS","1264, 1266, 1267, 1271","7556, 9263","$0.00","This award supports increased participation of US researchers at the Conference on the Foundations of Computational Mathematics (FoCM), hosted by the Universidad de la Republica in Montevideo, between December 11 - 20, 2014. The conference, organized by the Society for the Foundations of Computational Mathematics, is eighth in a sequence that commenced with the Park City (1995), Rio de Janeiro (1997), Oxford (1999), Minneapolis (2002), Santander (2005), Hong Kong (2008), and Budapest (2011) FoCM meetings. The NSF funding predominantly supports participation of qualified US-based graduate students, postdocs, and recent Ph.D. recipients in the conference, either as workshop speakers or as poster presenters, promoting their integration into the research community. The conference's diverse activities -- distinguished plenary talks, specialized workshops, poster session, and informal interactions between participants -- all integrate front-line research with mentoring and training opportunities for young participants, helping them develop networks that include the world's leading researchers. Central to the overall conference organization is the organizer's continuing commitment to the tradition that FoCM conferences welcome and integrate newcomers into the research community. <br/><br/>The FoCM conference offers an effective forum for the dissemination of research results across diverse fields within computational mathematics, capitalizing on a rapid evolution in our understanding of the interplay between mathematics and computation. The organizers and committed participants of the conference have a history of intellectual and service contributions to the foundations of computational mathematics. The conference program includes 17 plenary speakers who are among the leading researchers in the field; the organizers of the 19 workshops are chosen to include both established senior and talented junior researchers, their potential, diversity and creativity promising a vibrant scientific program. Many of the confirmed plenary speakers and workshop organizers at FoCM are from the US, reflecting the vitality of US based research in a variety of disciplines within computational mathematics. The award will advance the international exposure of research conducted in the US, and will have long-term effects in catalyzing new collaborations between US and international participants of the conference.<br/><br/>Conference web site:  www.fing.edu.uy/eventos/focm2014/"
"1418962","Modeling Nanoscale Fluid-Solid Interfaces","DMS","COMPUTATIONAL MATHEMATICS","07/15/2014","06/27/2017","Nikolaos Voulgarakis","WA","Washington State University","Standard Grant","Leland Jameson","06/30/2018","$151,170.00","","n.voulgarakis@wsu.edu","280 Lighty","PULLMAN","WA","991641060","5093359661","MPS","1271","8396, 8609, 9263","$0.00","Computational modeling of fluid-solid interfaces at the macroscale has long since crossed the boundary of pure academic interest to become an essential tool for industry. Sectors such as aerospace, automotive, medical, and chemical manufacturing routinely design, test, and optimize their products using one of the many inexpensive software packages that are now available. Unfortunately, these macroscale methodologies are not applicable at the nanoscale. The forces dominating the fluid-solid interfacial processes at the nanoscale are quite different and more complex than those at the macroscale. The ability to model such nanoscale interactions could find broad use in nanotechnology, biotechnology, and materials science, three of the primary areas of Federal strategic interest. Understanding, for example, how the size of some types of nanoparticles affect their reactivity with fluidic environments can lead to the production of significantly more effective materials for energy storage and conversion (batteries). The goal of this project is to develop mathematical models capable of describing nanoscale fluid-solid interfaces, culminating in the development of open-source software for general-purpose simulations of nanoscale motion and energy transfer. Similar to macroscale systems, where automotive engineers, for instance, use software packages to design cars, it is expected that this project will grow to become an essential computational design tool for nanoscale motion that will help scientists create nanomachines capable of delivering drugs to pathogenic cells, for example. <br/><br/>This research project aims to develop a multiscale-multiphysics methodology to simulate nanoscale fluid-solid interfacial processes and their effects on nanoscale motion. The specific objectives of the project are: (1) The development of a novel field-particle methodology, which will concurrently describe three of the most dominant forces at the nanoscale: thermal fluctuations, viscosity, and surface tension. With this approach, structures and fluctuating fluid are coupled directly through excluded volume effects and hydrodynamic forces without the imposition of boundary conditions. (2) The coupling of first-principles kinetic Monte Carlo methods with fluctuating hydrodynamics to describe nanoscale energy transduction and transfer from heterogeneous catalytic surfaces to the surrounding fluid environment. (3) To overcome computational efficiency challenges typically present in multiscale multi-domain approaches by the integration of algorithms in a large-scale parallel code for nanoscale computational fluid dynamics. This method will be applied to study how energy released from catalytic nanostructures can be utilized to achieve controlled nanoscale motion. Although the primary focus of this research is controlled nanoscale motion, the algorithms and software developed are expected to have an important multidisciplinary impact to other topics, such as nanocatalysis and drug delivery. The method developed in this project will be transformative to current fluid-solid interaction methodologies since it makes the realization of active and dynamic interfacial processes possible without the implementation of boundary conditions or computationally expensive techniques for tracking interfaces. Hydrophobic forces, wetting-dewetting phenomena, stochastic electrokinetics, and other out-of-thermodynamic equilibrium transfer processes between solids and fluids will be incorporated into traditional computational fluid dynamics (CFD) methodologies. The research results will be a rational extension of CFD to the nanoscale where interfacial and stochastic processes cannot be ignored. The large-scale parallel software for nanoscale motion will be released to the scientific community and is expected to help pave the way toward advanced computer-based approaches for designing moving nano structures."
"1418971","Sparse Principal Component Analysis via the Sparsest Element in a Subspace","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","08/28/2014","Paul Hand","MA","Massachusetts Institute of Technology","Standard Grant","Junping Wang","11/30/2014","$133,789.00","","p.hand@northeastern.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","MPS","1271","9263","$0.00","Sparse principal component analysis (PCA) is a technique that allows biologists and other scientists to interpret experimental data in terms of very few variables. For example, it can help identify which among thousands of genes are important in distinguishing different types of cancer. In order for scientists and engineers to select the best algorithm for finding sparse principal components, it is important to have a theoretical understanding of the performance of many algorithms under a realistic data model. Most existing theoretical understanding focuses on the simple case where there is a single component that happens to be sparse. The proposed work will introduce a new model in which there are multiple components, of which one is sparse. For a special case of this more realistic model, the proposed work attempts to understand if there are any conditions under which sophisticated convex programs are provably better than very simple algorithms. Either outcome would be informative in helping researchers decide between the many algorithms for sparse PCA. <br/> <br/>In this project, sparse PCA will be studied from the perspective of finding the sparsest element in a subspace. This perspective is motivated by a multispike data model, which the PI calls a sparse-dense model. Under this model, the infinite data limit of sparse PCA becomes the sparsest element problem, which is nontrivial. The objective of this research is to understand the computational-statistical tradeoff in finding the sparsest element in a subspace under the sparse-dense model. The PI would like to determine if there is a scaling gap between the information theoretic limit and the best performance by a computationally efficient algorithm. Ultimately, we would like to understand when sophisticated convex methods are provably better than simple thresholding methods. This objective will be explored by semidefinite relaxations, polynomial optimization, and reductions to the planted clique problem."
"1418724","Highly Scalable Algorithms and Solvers for Eigen-Problems: Unconstrained Optimization and Multiple Power Iterations","DMS","COMPUTATIONAL MATHEMATICS","07/01/2014","06/29/2014","Yin Zhang","TX","William Marsh Rice University","Standard Grant","Leland Jameson","06/30/2018","$240,000.00","","yzhang@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271","9263","$0.00","In today's big-data era, many organizations are facing the challenge of making sense of or use of massive datasets collected or flowing in at unprecedented rates.  The first step is often to reduce the size of data to a manageable level by extracting essence and removing redundancy.  Many techniques for data reduction and information extraction rely on so-called ""principal component analysis"" which requires intensive mathematical calculations. As data size keeps growing fast, such intensive computations need to be carried out on high-performance parallel computers that are able to execute a large number of independent tasks simultaneously. Currently, bottlenecks have appeared in commonly used mathematical methods that prevent big tasks from being broken up into enough independent small pieces to be quickly handled in parallel. In other words, the current mathematical methods have encountered difficulty in scalability. To break through the bottlenecks, this scalability issues must be attacked by devising new methodologies.  This project proposes a few new approaches of higher scalability. Preliminary experiments have demonstrated clear promises, offering multi-fold speedups on a wide class of problems even on commodity computers.  Careful theoretical and experimental investigations will be carried out in this project to fully develop the proposed methodologies.<br/><br/>Computing a relatively large number of principal eigenpairs or singular pairs of large-scale matrices (or data sets) is a fundamental computational problem with wide-ranging applications, especially in today's big-data information era.  Fast-increasing problem sizes and ever-evolving computer architectures have posed new algorithmic challenges. A constant challenge is to reach for higher algorithm concurrency in order to solve critical application problems on massively parallel computers.  Currently, the main bottleneck to high scalability lies in the combined tasks of Rayleigh-Ritz and orthogonalization (RR/Orth, in short) that are heavily used by most state-of-the-art eigensolvers.  The proposed research is to explore new strategies for developing highly parallel and scalable algorithms.  A key idea is to reduce the use of RR/Orth operations in exchange for operations of higher concurrency.  One approach makes use of unconstrained optimization formulations without orthogonality constraint so that, in principle, reasonable unconstrained optimization algorithms can be used without needing RR/Orth operations; another approach utilizes a simple but embarrassingly parallel procedure called multi-power method (MPM). Preliminary theoretical and numerical results are presented to demonstrate the potential of these approaches. In particular, the MPM approach has been empirically shown to achieve an ""optimal performance"" under reasonable conditions. It remains challenging to attain robustness and efficiency levels comparable to those of state-of-the-art eigensolvers."
"1419018","Algebraic Geometry: Computations and Applications","DMS","ALGEBRA,NUMBER THEORY,AND COM, COMPUTATIONAL MATHEMATICS","08/15/2014","06/19/2018","Bernd Sturmfels","CA","University of California-Berkeley","Continuing Grant","Leland Jameson","07/31/2019","$400,001.00","","bernd@math.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947101749","5106433891","MPS","1264, 1271","9263","$0.00","Linear algebra and its numerical methods are the foundation of scientific computing and inform a wide range of research in applied mathematics. Algebraic geometry is the geometry of non-linear algebra; its primary objects are sets described by collections of polynomials in several variables. Such sets, which include varieties and semialgebraic sets, arise in many contexts, notably in optimization, statistics, finance, and quantitative biology. Recent advances, both in theory and in computing resources, have enabled researchers to extend some of the techniques available for linear models to non-linear models. This project develops new methodologies based on algebraic geometry for both symbolic and numerical computing to tackle problems arising in such applications. <br/><br/>Intermediate steps between linear and non-linear algebra include piecewise-linear algebra (in particular, the max-plus algebra and tropical geometry) and multilinear algebra (in particular, the study of tensors and their decompositions). This project builds on these connections. Its four main themes are maximum likelihood geometry, Euclidean distance optimization, convex algebraic geometry, and classical moduli spaces. Concrete goals include the determination of the maximum likelihood degrees of determinantal varieties, the semialgebraic characterization of matrices with bounded nonnegative rank, the development of sum of squares relaxations for the Euclidean distance degree problem, and a geometric characterization of Gram spectrahedra. The design and implementation of novel algorithms for moduli spaces, for example for del Pezzo surfaces, forms a bridge to the research communities in core algebraic geometry."
"1418918","Collaborative Research: Efficient High-Order Parallel Algorithms for Large-Scale Photonics Simulation","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","08/03/2016","Shidong Jiang","NJ","New Jersey Institute of Technology","Continuing Grant","Leland Jameson","07/31/2018","$150,000.00","","shidong.jiang@njit.edu","University Heights","Newark","NJ","071021982","9735965275","MPS","1271","9263","$0.00","The focus of this research project is to develop efficient, accurate, and scalable computational techniques and provide much-needed simulation design tools for the photonics industry. More and more of modern life is based on fast and cheap communication. The transmission of information by electrical circuitry is limited in latency by power concerns and in bandwidth by cost. Photonic circuits virtually eliminate these constraints and provide a way to make high-bandwidth, low-latency interconnects that are, in many applications, far superior to their electric counterparts. But photonic circuits are extremely challenging to design. Indeed, the current state-of-art integrated photonic circuits chip contains only hundreds of photonic components due to the lack of efficient and reliable tools for the design of integrated photonic circuits. The difficulty here is that the design of integrated photonic devices requires accurate simulation of propagating electromagnetic waves which, in turn, requires extremely large numbers of unknowns even for modest accuracy in a volume discretization.  The tools developed by this project will address these important challenges.<br/><br/>The fundamental mathematical model for most photonics applications consists of Maxwell's equations with complex, structured material coefficients under wide variation of feature length scales. At computational scales feasible for a design engineer, existing techniques are too inaccurate for the design of complex photonic devices. This inaccuracy/inefficiency trade-off severely limits the usefulness of simulation in a design feedback loop. It further represents an impediment to the rapid development of the photonics industry since design iteration through manufacturing is typically very expensive and can take months of turn-around time on a single design. Lifting this inefficiency constraint is challenging, and it has the potential to play a pivotal role in the development of ambitious photonic circuits and devices. The investigators propose to develop the following techniques to overcome the obstacles encountered in practical, large-scale photonics simulation.  (1) Modularization of photonic device simulation via boundary integral equation methods. Specifically, the so-called mode calculation will be converted to a nonlinear eigenvalue problem of boundary integral equations, and the so-called propagation problem will be converted to a standard scattering problem and then solved via boundary integral equation methods. (2) Extension of the QBX method (""Quadrature by Expansion""), a general-purpose, high order quadrature scheme to treat three-dimensional problems with domains having corners, edges and multi-scale structures for accurate photonics simulation. (3) Seamless combination of the QBX method with a novel variant of the Fast Multipole Method (FMM) to solve integral equations in a matrix-free form with near optimal operation and storage requirements."
"1418960","Numerical methods for non-Newtonian fluid structure interaction problems","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","07/29/2014","Hyesuk Lee","SC","Clemson University","Standard Grant","Leland Jameson","07/31/2018","$206,248.00","","hklee@clemson.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","MPS","1271","8396, 8609, 9150, 9263","$0.00","This research project is focused on the stable and efficient numerical approximation of equations describing the flow of a non-Newtonian fluid interacting with an elastic structure. Such fluid flows are ubiquitous in our everyday lives, from the flow of blood in our bodies to flow in industrial processes such as pharmaceutical blending and microfluidics. This theoretical investigation will provide a solid foundation for the further development of numerical algorithms for such systems. The research project will also broaden the mathematical basis for the numerical simulation of non-Newtonian fluid flow in physically realistic settings. <br/><br/>The goal of this research project is the development and analysis of stable and efficient numerical schemes for non-Newtonian fluid structure interactions. There has been significant mathematical research for Newtonian fluid flow problems, but to date few investigations of non-Newtonian flows. The systems studied in this project involve coupled domains representing multi-physics behavior. This increases the numerical complexity as both stress and velocity must be resolved in the domains, and the strong interaction between the governing equations requires solution algorithms that achieve optimal convergence rates for efficiency while splitting the operators. Because of the large number of unknowns required to compute an accurate approximation of non-Newtonian (in particular viscoelastic) fluids, there is a need to develop efficient solvers for these problems. This research will contribute to the development and rigorous analysis of stability and accuracy properties of numerical methods for non-Newtonian fluid structure interactions."
"1418838","Collaborative Research: Determining Forms and Data Assimilation with Stochastic Data","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","08/15/2014","Hakima Bessaih","WY","University of Wyoming","Standard Grant","Leland Jameson","07/31/2018","$139,587.00","","hbessaih@fiu.edu","1000 E. University Avenue","Laramie","WY","820712000","3077665320","MPS","1271","9150, 9263","$0.00","Among the numerous factors that make accurate weather forecasting a challenge is initialization. The finite number of data collecting instruments distributed about the globe and atmosphere give only an incomplete description of the state of the weather at any instant. Yet there is a wealth of such data over an extended time period in the past, which when combined with certain mathematical models can provide a more complete description. The general procedure for this is called data assimilation. It provides more accurate starting values for computer simulations of the weather going into the future.  This award funds research into a new method of data assimilation that is flexible enough to be combined with a variety of simulation techniques. The work will concern both the implementation of this method, and the effect of measurement errors in the data, which inevitably occur. The assimilation process leads to another mathematical model that can be used to cleanse the past data of such noise. In this second direction, the goal is not to obtain a higher resolution condition for starting a simulation for the future, but to reconstruct a highly resolved version over the time period of the original data. This has natural applications in voice and pattern recognition. <br/><br/>The research team will implement a recent data assimilation algorithm based on feedback control for several important physical systems. The approach can be applied with a variety of determining parameters, such as nodal values, and finite volume elements. The team will also carry out computations with a new determining form based on feedback control. The steady states of this ordinary differential equation are precisely the finite-dimensional projections of trajectories in the global attractor. The team will study both computationally and analytically the effects of stochastic perturbations in the data. In the case of the data assimilation algorithm, the plan is to quantify and control the effect of the noise on the highly resolved state to be used in a subsequent direct numerical simulation. In the case of the determining form, the idea is to use its evolutionary process to remove the noise from the data itself."
"1418495","Improved Monte Carlo methods for high dimensional sums and integrals","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","08/09/2014","Mark Huber","CA","Claremont McKenna College","Standard Grant","Leland Jameson","07/31/2018","$121,336.00","","mhuber@cmc.edu","500 E. Ninth St.","Claremont","CA","917115929","9096077085","MPS","1271","9263","$0.00","A ""Monte Carlo"" algorithm is a computational method that makes random decisions as it runs. Monte Carlo algorithms (dating back to the Manhattan Project) have been an invaluable tool in physics, computer science, statistics, and many other fields, and have become an indispensable part of the modern computing toolkit. Monte Carlo methods enable approximations of integrals that would otherwise remain out of reach. This project will develop and analyze new types of Monte Carlo algorithms, with the ultimate goal of better approximation for high-dimensional integrals and sums. These methods will assist in finding statistical objects such as the maximum likelihood estimator and exact p values, in performing model section using Bayesian statistics, and in building approximation algorithms for provably hard problems that arise in computer science. Many existing Monte Carlo algorithms deliver point estimates, but fail to give provably good error bounds on these estimates. Recent work of the principal investigator shows that in some instances, it is possible to build algorithms where the error only depends on the algorithm, and not on the particular problem under consideration. For example, a new method called the Paired Product Estimator gives a fast method for estimating integrals in high dimension where the error of the estimate is precisely bounded. These bounds are independent of the distribution being sampled from. The project purpose is to further develop and analyze these algorithms to improve both their theoretical and practical efficiency. These will be foundational methods, and should find use as a general tool for researchers in many different fields. <br/><br/>This project will develop several new methodologies in Monte Carlo simulation. The first algorithm is for estimating the mean of a Bernoulli random variable. This is an essential step in Monte Carlo algorithms for estimating exact p values in statistics and for estimating integrals using acceptance rejection. By employing a method of Huber, it is possible to create an estimate such that the relative error in the estimate is independent of the value of the mean. Computer experiments indicate the method is fast; in this part of the project, Huber will try to show that this method is provably close to the optimal in the number of samples needed to obtain such an estimate. A new protocol for sampling from Markov random fields such as the Ising model will also be developed, partially recursive acceptance rejection. By creating a tree of possible labels for a graph and then carefully pruning this (exponentially large) tree, it is possible to sample from the Ising model (at temperatures above the critical temperature) using only a polynomial (even linear) number of steps. Such models can usually be written as Gibbs distributions, and often finding the normalizing constant (called the partition function) for these distributions is computationally very difficult. One part of the project will be refining an idea called the Paired Product Estimator, that approximates the partition function in a provably fast way: now the goal is to make the method practically efficient as well."
"1464525","Sparse Principal Component Analysis via the Sparsest Element in a Subspace","DMS","COMPUTATIONAL MATHEMATICS","10/01/2014","10/30/2014","Paul Hand","TX","William Marsh Rice University","Standard Grant","Leland Jameson","08/31/2018","$133,789.00","","p.hand@northeastern.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","MPS","1271","9263","$0.00","Sparse principal component analysis (PCA) is a technique that allows biologists and other scientists to interpret experimental data in terms of very few variables. For example, it can help identify which among thousands of genes are important in distinguishing different types of cancer. In order for scientists and engineers to select the best algorithm for finding sparse principal components, it is important to have a theoretical understanding of the performance of many algorithms under a realistic data model. Most existing theoretical understanding focuses on the simple case where there is a single component that happens to be sparse. The proposed work will introduce a new model in which there are multiple components, of which one is sparse. For a special case of this more realistic model, the proposed work attempts to understand if there are any conditions under which sophisticated convex programs are provably better than very simple algorithms. Either outcome would be informative in helping researchers decide between the many algorithms for sparse PCA. <br/> <br/>In this project, sparse PCA will be studied from the perspective of finding the sparsest element in a subspace. This perspective is motivated by a multispike data model, which the PI calls a sparse-dense model. Under this model, the infinite data limit of sparse PCA becomes the sparsest element problem, which is nontrivial. The objective of this research is to understand the computational-statistical tradeoff in finding the sparsest element in a subspace under the sparse-dense model. The PI would like to determine if there is a scaling gap between the information theoretic limit and the best performance by a computationally efficient algorithm. Ultimately, we would like to understand when sophisticated convex methods are provably better than simple thresholding methods. This objective will be explored by semidefinite relaxations, polynomial optimization, and reductions to the planted clique problem."
"1418869","RUI: Knotting transitions in physical systems","DMS","TOPOLOGY, COMPUTATIONAL MATHEMATICS","09/01/2014","07/27/2014","Eric Rawdon","MN","University of St. Thomas","Standard Grant","Leland Jameson","08/31/2018","$190,492.00","","ejrawdon@stthomas.edu","2115 Summit Avenue","St. Paul","MN","551051096","6519626038","MPS","1267, 1271","9229, 9263","$0.00","From microscopic DNA to massive solar flares, string-like objects are replete in nature at every scale. These objects can be entangled and transition between different types of knots. Sometimes nature needs to eliminate this knotting, such as when enzymes called type II topoisomerases cut and reattach strands of DNA to release interlinking during replication.  These type II topoisomerases are targets for some chemotherapy drugs, as well as the antibiotic Cipro which is used to treat anthrax poisoning.  At other times, knotting is created for a purpose, such as in the folding of some proteins into their functional knotted native state.  While the exact function of the knotting in these proteins is unknown, determining the function could make it possible to manipulate proteins or design new proteins for medical applications. At still other times, changes in knotting are the product of natural deterioration.  For example, as sub-atomic glueball particles decay through their lifetimes, they change between different types of knots.  Indeed, knotting in nature is a dynamic process and the transitions between different types of knots reveal properties of the physical systems.  In this project, the PI, a multi-disciplinary group of collaborators, and undergraduate students study knotting transitions for topoisomerase II, proteins, and glueballs to gain insights into the role of knotting in these systems. This project has broad educational objectives. Several undergraduate students will be supported directly by the grant. They will be trained by the PI and contribute to the projects, gaining both content knowledge and experience in the research process.  The students will participate in professional meetings and disseminate their findings in talks and posters. These research experiences are essential in training the next generation of science and mathematics educators, researchers, and practitioners. To reach a wide-audience, the PI will continue to be active in giving presentations to students, non-specialists, and multi-disciplinary audiences. The results will be published in mathematics and science journals. The PI will organize interdisciplinary conference sessions to bring together scientists from traditionally disparate fields and create new interdisciplinary collaborations with researchers across the world. In addition, the research results, data, and software generated as a part of this grant will be made publicly available via the world wide web.<br/><br/>While the mathematical study of knotting has focused traditionally on closed loops, much of the knotting in nature occurs in objects with free ends (i.e. open chains). This project will establish a firm understanding of open knotted structures, including knotted substructures within open chains and closed loops. This knowledge will be applied to classify the knotting in proteins and the data will be made publicly available. Relationships between knotting, geometric structure, and the amino acid sequence in knotted proteins will be determined to establish the function of the knotting in knotted proteins. Modeling knotting transitions due to the action of type II topoisomerases will lead to a better understanding of their effectiveness in untangling DNA strands. A similar analysis will be used to determine how subatomic glueballs decay through knotting. Together, these projects will reveal fundamental insights into knotting in nature.  More specifically, the main objectives of this grant are to 1) decompose complicated knots into their essential elements, 2) reveal the function of knotting in knotted proteins, 3) determine where type II topoisomerases perform their cutting and reattaching action, and 4) understand the decaying process in glueballs. A combination of new and established models and computer applications will be used to analyze these physical systems.  In addition to the scientific goals, this project has broad educational objectives. Several undergraduate students will be supported directly by the grant. They will be trained by the PI and contribute to the projects, gaining both content knowledge and experience in the research process.  The students will participate in professional meetings and disseminate their findings in talks and posters. These research experiences are essential in training the next generation of science and mathematics educators, researchers, and practitioners. To reach a wide-audience, the PI will continue to be active in giving presentations to students, non-specialists, and multi-disciplinary audiences. The results will be published in mathematics and science journals. The PI will organize interdisciplinary conference sessions to bring together scientists from traditionally disparate fields and create new interdisciplinary collaborations with researchers across the world. In addition, the research results, data, and software generated as a part of this grant will be made publicly available via the world wide web."
"1418928","Collaborative Research: Determining Forms and Data Assimilation with Stochastic Data","DMS","COMPUTATIONAL MATHEMATICS","08/15/2014","08/15/2014","Eric Olson","NV","Board of Regents, NSHE, obo University of Nevada, Reno","Standard Grant","Leland Jameson","07/31/2018","$119,784.00","","ejolson@unr.edu","1664 North Virginia Street","Reno","NV","895570001","7757844040","MPS","1271","9150, 9263","$0.00","Among the numerous factors that make accurate weather forecasting a challenge is initialization. The finite number of data collecting instruments distributed about the globe and atmosphere give only an incomplete description of the state of the weather at any instant. Yet there is a wealth of such data over an extended time period in the past, which when combined with certain mathematical models can provide a more complete description. The general procedure for this is called data assimilation. It provides more accurate starting values for computer simulations of the weather going into the future.  This award funds research into a new method of data assimilation that is flexible enough to be combined with a variety of simulation techniques. The work will concern both the implementation of this method, and the effect of measurement errors in the data, which inevitably occur. The assimilation process leads to another mathematical model that can be used to cleanse the past data of such noise. In this second direction, the goal is not to obtain a higher resolution condition for starting a simulation for the future, but to reconstruct a highly resolved version over the time period of the original data. This has natural applications in voice and pattern recognition. <br/><br/>The research team will implement a recent data assimilation algorithm based on feedback control for several important physical systems. The approach can be applied with a variety of determining parameters, such as nodal values, and finite volume elements. The team will also carry out computations with a new determining form based on feedback control. The steady states of this ordinary differential equation are precisely the finite-dimensional projections of trajectories in the global attractor. The team will study both computationally and analytically the effects of stochastic perturbations in the data. In the case of the data assimilation algorithm, the plan is to quantify and control the effect of the noise on the highly resolved state to be used in a subsequent direct numerical simulation. In the case of the determining form, the idea is to use its evolutionary process to remove the noise from the data itself."
"1348721","CAREER: Practical Compressive Signal Processing","DMS","COMPUTATIONAL MATHEMATICS, Division Co-Funding: CAREER","06/01/2014","05/20/2014","Deanna Needell","CA","Claremont McKenna College","Standard Grant","Leland Jameson","09/30/2017","$413,527.00","","deanna@math.ucla.edu","500 E. Ninth St.","Claremont","CA","917115929","9096077085","MPS","1271, 8048","1045, 9263","$0.00","A ""signal"" is any data set that one would like to acquire, for example, an image, a large block of data, or an audio clip. One can imagine asking how quickly one would need to sample an audio clip so that from those samples alone, the audio clip could be accurately recovered. Would you need to sample every nanosecond, every millisecond, or every second? Compressive Signal Processing (CSP) shows that the important information in many signals can be obtained and recovered from far fewer samples than traditionally thought. The applications of CSP are widespread and include imaging (medical, hyperspectral, microscopy, biological), analog-to-information conversion, radar, large scale information synthesis, geophysical data analysis, computational biology, and many more. Although these applications are astounding, there has been a disconnect between the theoretical work in CSP and the use of CSP in practical settings. The goals of this project will bridge this gap by providing methods and analysis for CSP that apply to real-world signals and settings. Such work will lead to decreased scan time in MRI, reduced cost and energy consumption in computing infrastructures, improved detection of diseased crops from hyperspectral images, increased accuracy in radar, and improved compression and analysis in many other large-data applications. In addition, this project will involve students at all levels and introduce them to rigorous scientific research. The PI actively recruits members from under-represented populations, and will continue to promote diversity through her own research and outreach programs.<br/><br/>Early CSP models restrict the class of signals to those compressible in a very specific sense (sparse with respect to an orthonormal or incoherent basis). One goal of this project is to relax this restriction to allow for signals actually encountered in practice, such as those sparse in redundant, coherent, and highly overcomplete dictionaries. We will utilize both greedy approaches and optimization-based methods, tailored to specific dictionaries of interest, as well as more general methods for arbitrary bases. In addition, this project will develop adaptive CSP sampling schemes, where measurements of the signal are designed ""on the fly,"" as they are being taken. Traditional measurement schemes ignore this information, while adaptive schemes have the potential to significantly reduce reconstruction error, number of measurements, and computation time. We will identify optimal measurement strategies for constrained and unconstrained settings, and analyze how much one can actually gain from adaptivity from an information theoretic point of view. The project will also involve work in ""one-bit CSP"", a new and exciting branch of CSP which handles extreme (and often more realistic) quantization. We will draw on sub-linear methods, where large errors appear naturally, and also use optimization based techniques along with adaptive quantization thresholds to reduce the recovery error below the best possible for non-adaptive quantization. In studying these topics, the research will bridge a large gap in the theory of CSP and provide a unified framework for both practitioners and researchers."
"1418787","Dynamical Systems on Tensor Approximations","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","07/27/2014","Martin Mohlenkamp","OH","Ohio University","Standard Grant","Leland Jameson","07/31/2017","$210,000.00","Todd Young","mohlenka@ohio.edu","108 CUTLER HL","ATHENS","OH","457012979","7405932857","MPS","1271","9263","$0.00","Functions of many variables arise in numerous mathematical, statistical, and scientific problems; a particularly notable example is the multiparticle Schrodinger equation in quantum mechanics. The effort required to compute in a straightforward way with such functions grows extremely rapidly as the number of variables increases, and soon becomes prohibitive. Mathematical methods have been developed that in some cases allow one to compute without this rapid growth, but crucial parts of the method are poorly understood and unreliable. This project seeks to understand and then fix these crucial parts. Students will be actively involved in the project and so learn mathematics and how to conduct mathematical research; they will also develop skills in writing, presenting seminars and posters, and software development and usage.  <br/><br/>A mathematical study will be conducted on the approximation of tensors using sums of separable tensors and the approximation of multivariate functions using sums of separable functions.  The objectives are to understand (1) how such approximations behave and (2) how such approximations can be effectively computed. The method is to consider iterative tensor approximation algorithms as dynamical systems to probe the set of sum-of-separable tensors and to understand the behavior of the algorithm within this set.  The approximation of tensors by sums of separable tensors enables a promising computational paradigm for bypassing the curse of dimensionality when working with functions of many variables. This project addresses a bottleneck, in understanding and in computation, that prevents the computational paradigm from achieving its full potential."
"1419100","Fast algorithms for large-scale nonlinear algebraic eigenproblems","DMS","COMPUTATIONAL MATHEMATICS, CDS&E-MSS","08/01/2014","07/27/2014","Fei Xue","LA","University of Louisiana at Lafayette","Standard Grant","Rosemary Renaut","01/31/2017","$180,000.00","","fxue@clemson.edu","104 E University Ave","Lafayette","LA","705032014","3374825811","MPS","1271, 8069","7433, 8084, 8396, 8609, 9150, 9263","$0.00","This project concerns development and analysis of new numerical algorithms for large-scale algebraic eigenproblems with nonlinearity in eigenvalues, eigenvectors, and parameters. These eigenproblems arise in electronic structure calculation, design of accelerator cavities, delay differential equations, vibration analysis of complex structures, and many more. Structure-preserving linearization techniques that have been developed recently are competitive for small or medium polynomial and rational eigenproblems, but they entail high computational costs for large-scale simulations due to the significantly enlarged dimension of linearized problems. In addition, linearization introduces considerable complications for the development of preconditioners, and it is not applicable to eigenproblems with full nonlinearity. <br/><br/>The PI shall develop novel iterative projection methods that are accurate, robust and efficient, for the solution of large-scale truly nonlinear eigenproblems. This goal can be achieved in part by exploration of special properties of different types of nonlinear eigenproblems that enable solution strategies similar to those for linear eigenproblems.  This investigation is focused on ( 1) new preconditioned eigensolvers, including conjugate-gradient-like and minimal-residual-like methods, for efficient solution of a large number of extreme and interior eigenvalues of problems with nonlinearity in eigenvalues, with and without the variational principle;  (2) fast inexact Newton-like methods to solve parameter-dependent degenerate eigenproblem for the study of (in)stabilities of dynamical systems;  (3) efficient algorithms for solving eigenproblems with nonlinearity in eigenvectors arising from condensed matter physics and electronic structure calculation. The research will develop a systematic and unified treatment of mathematical theory and development of numerical software."
"1418750","High Order Schemes for Hyperbolic and Convection-dominated Problems","DMS","COMPUTATIONAL MATHEMATICS","08/01/2014","05/24/2016","Chi-Wang Shu","RI","Brown University","Continuing Grant","Leland Jameson","07/31/2018","$387,839.00","","chi-wang_shu@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","MPS","1271","8396, 8609, 9150, 9263","$0.00","In this project, the PI will perform research in the algorithm design and analysis of high order numerical methods.  These algorithms are used to solve scientific and engineering problems arising from diverse application fields such as aerospace engineering, semi-conductor device design, astrophysics, and biological problems.  Even with today's fast computers, it is still essential to design efficient and reliable algorithms which can be used to obtain accurate solutions to these application problems.  The broader impacts resulting from the proposed activity will be a suite of powerful computational tools, suitable for various applications mentioned above.  These tools are expected to make positive contributions to computer simulations of the complicated solution structure in these applications. <br/><br/>The algorithms to be investigated include the finite difference and finite volume weighted  essentially non-oscillatory (WENO) schemes and discontinuous Galerkin finite element methods, for  solving hyperbolic and other convection dominated partial differential equations (PDEs). While the emphasis of this project is on algorithm design and analysis, close attention will be paid to applications. Topics of proposed investigations will include the study on high order accurate bound-preserving  algorithms and applications, an inverse Lax-Wendroff procedure for high order numerical boundary  conditions for finite difference schemes on rectangular meshes when the physical boundary is not  aligned with the meshes, WENO schemes with subcell resolution for nonconservative problems,  Lagrangian type finite volume schemes for multi-material flows, energy-conserving discontinuous  Galerkin methods for long time simulation of wave problems, efficient discontinuous Galerkin methods  for front propagation problems with obstacles, superconvergence analysis of discontinuous Galerkin  methods and its applications in adaptive computation, simple WENO limiters for discontinuous Galerkin  methods in unstructured meshes for problems with strong shocks, multi-scale methods based on the  discontinuous Galerkin framework, analysis and numerical solutions for traffic and pedestrian flow  models, turbulence simulation in cosmology, and study on aggregation and coordinated movement in  computational biology.  Problems in applications will motivate the design of new algorithms or new  features in existing algorithms; mathematics tools are used to analyze these algorithms to give  guidelines for their applicability and limitations; practical considerations including parallel  implementation issues are addressed to make the algorithms competitive in large scale calculations; and  collaborations with engineers and other applied scientists enable the efficient application of these new  algorithms or new features in existing algorithms."
"1419077","Developing Novel Numerical Methods for Flow and Transport in Porous Media","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","08/22/2014","Jiangguo Liu","CO","Colorado State University","Standard Grant","Leland Jameson","08/31/2018","$119,999.00","","liu@math.colostate.edu","601 S Howes St","Fort Collins","CO","805232002","9704916355","MPS","1271","8396, 8609, 9263","$0.00","Flow and transport in porous media arise from a wide variety of real world problems such as oil recovery, groundwater contaminant remediation, CO2 sequestration, wildfire, magma transport through the Earth crust, and viral protein trafficking inside host cells.  All these problems have tremendous economic, environmental, medical, and social significance.  Mathematical modeling and computer simulations will provide efficient and inexpensive tools to enhance our abilities in understanding, predicting, and controlling the aforementioned problems.  Scientific challenges abound in the modeling and simulations of flow and transport, due to the heterogeneity and anisotropy of the media, multiple spatial and temporal scales, and uncertainty in these processes.  This research project aims at developing a new class of efficient and robust numerical methods for coupled flow and transport problems.  These methods will be implemented as computer software modules that can be used for a broad range of scientific computing tasks.  This project will also provide hands-on training opportunities for graduate students, especially those from underrepresented groups.<br/><br/>Specifically, this project focuses on development of novel finite element methods for solving coupled flow and transport problems in porous media.  This will be accomplished by combining the weak Galerkin (WG) and Eulerian-Lagrangian approaches.  The WG approach establishes a new type of approximations of differential operators by using degrees of freedom in element interiors as well as those on mesh skeleton.  WG finite element schemes could offer preferred features such as local conservation and symmetric positive-definite discrete linear systems.  The Eulerian-Lagrangian approach efficiently utilizes the flow information and produces small temporal truncation errors.  This enables robust long-time simulations of transport problems.  By incorporating these two approaches, the PI and collaborators will design, analyze, and implement a family of new finite element methods for solving the Darcy equation, convection-dominated transport equations, the miscible displacement problem, and two-phase flow problems.  These new methods overcome disadvantages of existing methods but maintain those well-received advantages, for example, local conservation.  Software modules (a Matlab toolbox and C++ libraries based on PETSc) will be developed to put these new methods into practical uses.  PhD students will be trained through this project to gain integrated capabilities of mathematical modeling and algorithm development for challenging real world problems."
"1518925","Problems in mathematical foundations of adaptive finite element methods","DMS","COMPUTATIONAL MATHEMATICS","09/01/2014","03/11/2015","Alan Demlow","TX","Texas A&M University","Standard Grant","Leland Jameson","08/31/2017","$166,752.00","","demlow@math.tamu.edu","400 Harvey Mitchell Pkwy South","College Station","TX","778454375","9798626777","MPS","1271","9150, 9263","$0.00","A posteriori error estimates and adaptive finite element methods (AFEM) are widely-used tools for solving partial differential equations (PDEs) arising in science and engineering applications.  A posteriori estimates provide computable bounds on discretization errors, while AFEM are efficient solution techniques which accurately reflect solution properties via automatic local mesh grading.  The goals of this project are to better understand the mathematical underpinnings of AFEM and to provide new a posteriori error estimates and adaptive algorithms in several specific application areas.  A major part of the project is devoted to development and analysis of a posteriori error estimates and AFEM for PDEs on surfaces.  Specific projects concern Eulerian formulations of parabolic PDEs on evolving surfaces, solution of elliptic PDEs on surfaces for which the only available information is a discrete approximation, and elliptic eigenvalue problems.  Another emphasis is fine properties of FEM, in particular the development of a priori and a posteriori error estimates in nonstandard norms.  The PI will develop new a priori error estimates in such norms on the types of highly graded meshes typically seen in practice, prove new a posteriori maximum-norm bounds for elliptic interface problems, and integrate similar error analysis into his study of surface eigenvalue problems. <br/><br/>A wide variety of applications in science and engineering give rise to partial differential equations (PDEs) which must be solved in order to obtain accurate predictions about the physical world.  PDEs are typically solved approximately on computers in modern applications, and there is a tradeoff between the quality of the approximate solution and the investment of computational resources.  The PI will study the mathematical underpinnings of adaptive algorithms which automatically generate more accurate solutions while efficiently employing the computing power at hand.  Part of the project is aimed at enriching mathematical understanding of existing algorithms, and part to developing new and mathematically well-justified adaptive algorithms for various applications."
